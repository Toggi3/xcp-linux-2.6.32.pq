diff --git a/drivers/net/Kconfig b/drivers/net/Kconfig
--- a/drivers/net/Kconfig
+++ b/drivers/net/Kconfig
@@ -2547,6 +2547,31 @@
 	  To compile this driver as a module, choose M here: the module
 	  will be called cxgb3.
 
+config CHELSIO_T4_DEPENDS
+        tristate
+        depends on PCI && INET
+        default y
+
+config CHELSIO_T4
+        tristate "Chelsio Communications T4 Ethernet support"
+        depends on CHELSIO_T4_DEPENDS
+        select FW_LOADER
+        select MDIO
+        help
+          This driver supports Chelsio T4-based gigabit and 10Gb Ethernet
+          adapters.
+
+          For general information about Chelsio and our products, visit
+          our website at <http://www.chelsio.com>.
+
+          For customer support, please visit our customer support page at
+          <http://www.chelsio.com/support.htm>.
+
+          Please send feedback to <linux-bugs@chelsio.com>.
+
+          To compile this driver as a module choose M here; the module
+          will be called cxgb4.
+
 config EHEA
 	tristate "eHEA Ethernet support"
 	depends on IBMEBUS && INET && SPARSEMEM
diff --git a/drivers/net/Makefile b/drivers/net/Makefile
--- a/drivers/net/Makefile
+++ b/drivers/net/Makefile
@@ -19,6 +19,7 @@
 obj-$(CONFIG_IP1000) += ipg.o
 obj-$(CONFIG_CHELSIO_T1) += chelsio/
 obj-$(CONFIG_CHELSIO_T3) += cxgb3/
+obj-$(CONFIG_CHELSIO_T4) += cxgb4/
 obj-$(CONFIG_EHEA) += ehea/
 obj-$(CONFIG_CAN) += can/
 obj-$(CONFIG_BONDING) += bonding/
diff --git a/drivers/net/cxgb4/Makefile b/drivers/net/cxgb4/Makefile
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/Makefile
@@ -0,0 +1,72 @@
+# Makefile for Chelsio T4 cxgb4 driver.
+# Copyright (c) 2006-2009 Chelsio Communications, Inc.
+SHELL = /bin/bash
+
+# The top-level makefile defines required variables and flags.
+ifneq ($(shell [[ $(MAKELEVEL) -ge 1 ]] && echo 1),1)
+  $(error Please use the top-level Makefile to build this driver)
+endif
+
+# Includes.
+EXTRA_CFLAGS += $(FLAGS)
+EXTRA_CFLAGS += -I$(srcdir)/include
+EXTRA_CFLAGS += -I$(KSRC)/drivers/net/bonding
+EXTRA_CFLAGS += -I$(KSRC)/include
+EXTRA_CFLAGS += -I.
+
+CFILES  = cxgb4_main.c t4_hw.c sge.c trace.c
+ifneq ($(disable_offload),1)
+  CFILES += l2t.c
+endif
+
+ifeq ($(enable_bypass),1)
+  CFILES +=  bypass.c bypass_sysfs.c
+  EXTRA_CFLAGS += -DCONFIG_CHELSIO_BYPASS
+endif
+
+TARGET  = cxgb4.o
+CLEAN_FILES := $(wildcard *.c)
+CLEAN_FILES := $(CLEAN_FILES:.c=.o)
+
+lib_path     := $(PREFIX)/lib/modules/$(utsrelease)
+module_path   = updates/kernel/drivers/net/cxgb4
+install_path := $(lib_path)/$(module_path)
+
+driver := $(TARGET:.o=.ko)
+
+ifneq ($(modulesymfile),)
+  override symverfile = symverfile="$(topdir)/$(modulesymfile) \
+                                    -o $(drvdir)/$(modulesymfile)"
+else
+  override symverfile =
+endif
+
+obj-m := $(TARGET)
+$(TARGET:.o=)-objs := $(CFILES:.c=.o)
+
+.SUFFIXES:
+.SUFFIXES: .c .o
+
+.PHONY: default
+default: build
+
+.PHONY: build
+build:
+	@$(MAKE) $(symverfile) -C $(KOBJ) SUBDIRS=$(shell pwd) modules
+
+.PHONY: install
+install:
+	@install -D $(verbose) -m 644 $(driver) $(install_path)/$(driver)
+
+.PHONY: uninstall
+uninstall:
+	@-if [ -n "$(verbose)" ]; then \
+	    echo "Removing $(install_path)/$(driver)";\
+	  fi;\
+	  /bin/rm -f $(install_path)/$(driver) 2>/dev/null;
+
+.PHONY: clean
+clean:
+	-/bin/rm -rf $(driver) $(TARGET) $(TARGET:.o=.mod.c) \
+	             $(TARGET:.o=.mod.o) $(CLEAN_FILES) \
+		     .*cmd .tmp_versions *.symvers Makefile.xen modules.order
diff --git a/drivers/net/cxgb4/adapter.h b/drivers/net/cxgb4/adapter.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/adapter.h
@@ -0,0 +1,794 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/* This file should not be included directly.  Include common.h instead. */
+
+#ifndef __T4_ADAPTER_H__
+#define __T4_ADAPTER_H__
+
+#include <linux/pci.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/timer.h>
+#include <linux/cache.h>
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#include <linux/toedev.h>
+#endif
+#include <linux/bitops.h>
+#include <linux/mutex.h>
+#include <linux/list.h>
+#include <linux/netdevice.h>
+#include <asm/io.h>
+#include "cxgb4_ofld.h"
+#include "cxgb4_compat.h"
+#include "t4_regs_values.h"
+
+#ifdef T4_TRACE
+# include "trace.h"
+# define NTRACEBUFS 8
+#endif
+
+#define CH_ERR(adap, fmt, ...)   dev_err(adap->pdev_dev, fmt, ## __VA_ARGS__)
+#define CH_WARN(adap, fmt, ...)  dev_warn(adap->pdev_dev, fmt, ## __VA_ARGS__)
+#define CH_ALERT(adap, fmt, ...) dev_alert(adap->pdev_dev, fmt, ## __VA_ARGS__)
+
+#define CH_WARN_RATELIMIT(adap, fmt, ...)  do {\
+	if (printk_ratelimit()) \
+		dev_warn(adap->pdev_dev, fmt, ## __VA_ARGS__); \
+} while (0)
+
+/*
+ * More powerful macro that selectively prints messages based on msg_enable.
+ * For info and debugging messages.
+ */
+#define CH_MSG(adapter, level, category, fmt, ...) do { \
+	if ((adapter)->msg_enable & NETIF_MSG_##category) \
+		dev_printk(KERN_##level, adapter->pdev_dev, fmt, \
+			   ## __VA_ARGS__); \
+} while (0)
+
+#ifdef DEBUG
+# define CH_DBG(adapter, category, fmt, ...) \
+	CH_MSG(adapter, DEBUG, category, fmt, ## __VA_ARGS__)
+#else
+# define CH_DBG(adapter, category, fmt, ...)
+#endif
+
+#define CH_DUMP_MBOX(adap, mbox, data_reg, size) \
+	CH_MSG(adap, INFO, MBOX, \
+	       "mbox %u: %llx %llx %llx %llx %llx %llx %llx %llx\n", (mbox), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 8), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 16), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 24), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 32), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 40), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 48), \
+	       (unsigned long long)t4_read_reg64(adap, data_reg + 56));
+
+/* Additional NETIF_MSG_* categories */
+#define NETIF_MSG_MBOX 0x4000000
+#define NETIF_MSG_MMIO 0x8000000
+
+enum {
+	MAX_ETH_QSETS = 32,           /* # of Ethernet Tx/Rx queue sets */
+	MAX_OFLD_QSETS = 32,          /* # of offload Tx/Rx queue sets */
+	MAX_CTRL_QUEUES = NCHAN,      /* # of control Tx queues */
+	MAX_RDMA_QUEUES = NCHAN,      /* # of streaming RDMA Rx queues */
+	MAX_ISCSI_QUEUES = NCHAN,     /* # of streaming iSCSI Rx queues */
+};
+
+/*
+ * We need to size various arrays and bitmaps to be able to use Ingress and
+ * Egress Queue IDs (minus the base starting Ingress/Egress Queue IDs) to
+ * index into those arrays/bitmaps.
+ *
+ * The maximum number of Egress Queue IDs is determined by the maximum number
+ * of Ethernet "Queue Sets" which we support plus Control, Offload "Queue
+ * Sets", RDMA and iSCSI RX Queues.  The maximum number of Ingress Queue IDs
+ * is also determined by the maximum number of Ethernet "Queue Sets" plus
+ * Offload RX Queues, the Asynchronous Firmware Event Queue and the Forwarded
+ * Interrupt Queue.
+ *
+ * Each Ethernet "Queue Set" requires one Ingress Queue for RX Packet Ingress
+ * Event notifications and two Egress Queues for a Free List and an Ethernet
+ * TX list (remember that a Free List is really an Egress Queue since it
+ * contains pointer to host side buffers which the host send to the hardware)
+ * The same is true for the Offload "Queue Sets".  And the RDMA and iSCSI RX
+ * Queues also have Free Lists, so we need to count those in the Egress Queue
+ * count Each Offload "Queue Set" has one Ingress and one Egress Queue.
+ */
+enum {
+	INGQ_EXTRAS = 2,	/* firmware event queue and */
+				/*   forwarded interrupts */
+	MAX_EGRQ = MAX_ETH_QSETS*2 + MAX_OFLD_QSETS*2
+		   + MAX_CTRL_QUEUES
+		   + MAX_RDMA_QUEUES + MAX_ISCSI_QUEUES,
+	MAX_INGQ = MAX_ETH_QSETS + MAX_OFLD_QSETS
+		   + MAX_RDMA_QUEUES + MAX_ISCSI_QUEUES
+		   + INGQ_EXTRAS,
+};
+
+struct adapter;
+struct vlan_group;
+struct sge_eth_rxq;
+struct sge_rspq;
+
+struct port_info {
+	struct adapter *adapter;
+	struct vlan_group *vlan_grp;
+	struct sge_eth_rxq *qs;       /* first Rx queue for this port */
+	u16    viid;
+	s16    xact_addr_filt;        /* index of exact MAC address filter */
+	u16    rss_size;              /* size of VI's RSS table slice */
+	s8     mdio_addr;
+	u8     port_type;
+	u8     mod_type;
+	u8     port_id;
+	u8     tx_chan;
+	u8     lport;                 /* associated offload logical port */
+	u8     rx_offload;            /* CSO, etc */
+	u8     nqsets;                /* # of qsets */
+	u8     first_qset;            /* index of first qset */
+	struct link_config link_cfg;
+	struct port_stats stats_base;
+};
+
+/* port_info.rx_offload flags */
+enum {
+	RX_CSO = 1 << 0,
+};
+
+struct work_struct;
+struct dentry;
+struct proc_dir_entry;
+
+enum {                                 /* adapter flags */
+	FULL_INIT_DONE     = (1 << 0),
+	USING_MSI          = (1 << 1),
+	USING_MSIX         = (1 << 2),
+	QUEUES_BOUND       = (1 << 3),
+	FW_OK              = (1 << 4),
+	RSS_TNLALLLOOKUP   = (1 << 5),
+	USING_SOFT_PARAMS  = (1 << 6),
+	MASTER_PF          = (1 << 7),
+	BYPASS_DROP        = (1 << 8),
+	FW_OFLD_CONN       = (1 << 9),
+};
+
+struct rx_sw_desc;
+
+struct sge_fl {                     /* SGE free-buffer queue state */
+	unsigned int avail;         /* # of available Rx buffers */
+	unsigned int pend_cred;     /* new buffers since last FL DB ring */
+	unsigned int cidx;          /* consumer index */
+	unsigned int pidx;          /* producer index */
+	unsigned long alloc_failed; /* # of times buffer allocation failed */
+	unsigned long large_alloc_failed;
+	unsigned long starving;
+	/* RO fields */
+	unsigned int cntxt_id;      /* SGE relative QID for the free list */
+	unsigned int size;          /* capacity of free list */
+	struct rx_sw_desc *sdesc;   /* address of SW Rx descriptor ring */
+	__be64 *desc;               /* address of HW Rx descriptor ring */
+	dma_addr_t addr;            /* bus address of HW ring start */
+};
+
+/* A packet gather list */
+struct pkt_gl {
+	union {
+		skb_frag_t frags[MAX_SKB_FRAGS];
+		struct sk_buff *skbs[MAX_SKB_FRAGS];
+	} /*UNNAMED*/;
+	void *va;                         /* virtual address of first byte */
+	unsigned int nfrags;              /* # of fragments */
+	unsigned int tot_len;             /* total length of fragments */
+	bool useskbs;                     /* use skbs for fragments */
+};
+
+typedef int (*rspq_handler_t)(struct sge_rspq *q, const __be64 *rsp,
+			      const struct pkt_gl *gl);
+
+struct sge_rspq {                   /* state for an SGE response queue */
+	struct napi_struct napi;
+	const __be64 *cur_desc;     /* current descriptor in queue */
+	unsigned int cidx;          /* consumer index */
+	u8 gen;                     /* current generation bit */
+	u8 intr_params;             /* interrupt holdoff parameters */
+	u8 next_intr_params;        /* holdoff params for next interrupt */
+	u8 pktcnt_idx;              /* interrupt packet threshold */
+	u8 uld;                     /* ULD handling this queue */
+	u8 idx;                     /* queue index within its group */
+	int offset;                 /* offset into current Rx buffer */
+	u16 cntxt_id;               /* SGE relative QID for the response Q */
+	u16 abs_id;                 /* absolute SGE id for the response q */
+	__be64 *desc;               /* address of HW response ring */
+	dma_addr_t phys_addr;       /* physical address of the ring */
+	unsigned int iqe_len;       /* entry size */
+	unsigned int size;          /* capacity of response queue */
+	struct adapter *adapter;
+	struct net_device *netdev;  /* associated net device */
+	rspq_handler_t handler;
+};
+
+struct sge_eth_stats {              /* Ethernet queue statistics */
+	unsigned long pkts;         /* # of ethernet packets */
+	unsigned long lro_pkts;     /* # of LRO super packets */
+	unsigned long lro_merged;   /* # of wire packets merged by LRO */
+	unsigned long rx_cso;       /* # of Rx checksum offloads */
+	unsigned long vlan_ex;      /* # of Rx VLAN extractions */
+	unsigned long rx_drops;     /* # of packets dropped due to no mem */
+};
+
+struct sge_eth_rxq {                /* a SW Ethernet Rx queue */
+	struct sge_rspq rspq;
+	struct sge_fl fl;
+	bool useskbs;               /* one ingress packet per skb FL buffer */
+	struct sge_eth_stats stats;
+} ____cacheline_aligned_in_smp;
+
+struct sge_ofld_stats {             /* offload queue statistics */
+	unsigned long pkts;         /* # of packets */
+	unsigned long imm;          /* # of immediate-data packets */
+	unsigned long an;           /* # of asynchronous notifications */
+	unsigned long nomem;        /* # of responses deferred due to no mem */
+};
+
+struct sge_ofld_rxq {               /* a SW offload Rx queue */
+	struct sge_rspq rspq;
+	struct sge_fl fl;
+	bool useskbs;
+	struct sge_ofld_stats stats;
+} ____cacheline_aligned_in_smp;
+
+struct tx_desc {
+	__be64 flit[8];
+};
+
+struct tx_sw_desc;
+
+struct eth_coalesce {
+	unsigned int idx;
+	unsigned int len;
+	unsigned int flits;
+	unsigned int max;
+	unsigned char *ptr;
+	unsigned char type;
+	bool ison;
+	bool intr;
+};
+
+struct sge_txq {
+	unsigned int  in_use;       /* # of in-use Tx descriptors */
+	unsigned int  size;         /* # of descriptors */
+	unsigned int  cidx;         /* SW consumer index */
+	unsigned int  pidx;         /* producer index */
+	unsigned long txp;          /* # of transmitted requests */
+	unsigned long stops;        /* # of times q has been stopped */
+	unsigned long restarts;     /* # of queue restarts */
+	unsigned int  cntxt_id;     /* SGE relative QID for the Tx Q */
+	struct tx_desc *desc;       /* address of HW Tx descriptor ring */
+	struct tx_sw_desc *sdesc;   /* address of SW Tx descriptor ring */
+	struct eth_coalesce coalesce;
+	struct sge_qstat *stat;     /* queue status entry */
+	dma_addr_t    phys_addr;    /* physical address of the ring */
+	spinlock_t db_lock;
+	int db_disabled;
+	unsigned short db_pidx;
+};
+
+struct sge_eth_txq {                /* state for an SGE Ethernet Tx queue */
+	struct sge_txq q;
+	struct netdev_queue *txq;   /* associated netdev TX queue */
+	unsigned long tso;          /* # of TSO requests */
+	unsigned long tx_cso;       /* # of Tx checksum offloads */
+	unsigned long vlan_ins;     /* # of Tx VLAN insertions */
+	unsigned long mapping_err;  /* # of I/O MMU packet mapping errors */
+	unsigned long coal_wr;      /* # of coalesce WR */
+	unsigned long coal_pkts;    /* # of coalesced packets */
+} ____cacheline_aligned_in_smp;
+
+struct sge_ofld_txq {               /* state for an SGE offload Tx queue */
+	struct sge_txq q;
+	struct adapter *adap;
+	struct sk_buff_head sendq;  /* list of backpressured packets */
+	struct tasklet_struct qresume_tsk; /* restarts the queue */
+	u8 full;                    /* the Tx ring is full */
+	unsigned long mapping_err;  /* # of I/O MMU packet mapping errors */
+} ____cacheline_aligned_in_smp;
+
+struct sge_ctrl_txq {               /* state for an SGE control Tx queue */
+	struct sge_txq q;
+	struct adapter *adap;
+	struct sk_buff_head sendq;  /* list of backpressured packets */
+	struct tasklet_struct qresume_tsk; /* restarts the queue */
+	u8 full;                    /* the Tx ring is full */
+} ____cacheline_aligned_in_smp;
+
+struct sge {
+	/*
+	 * Keep all the Tx queues before the Rx queues so we can tell easily
+	 * what egr_map entries point at.
+	 */
+	struct sge_eth_txq ethtxq[MAX_ETH_QSETS];
+	struct sge_ofld_txq ofldtxq[MAX_OFLD_QSETS];
+	struct sge_ctrl_txq ctrlq[MAX_CTRL_QUEUES];
+
+	struct sge_eth_rxq ethrxq[MAX_ETH_QSETS];
+	struct sge_ofld_rxq ofldrxq[MAX_OFLD_QSETS];
+	struct sge_ofld_rxq rdmarxq[MAX_RDMA_QUEUES];
+	struct sge_ofld_rxq iscsirxq[MAX_ISCSI_QUEUES];
+	struct sge_rspq fw_evtq ____cacheline_aligned_in_smp;
+
+	struct sge_rspq intrq ____cacheline_aligned_in_smp;
+	spinlock_t intrq_lock;
+
+	u16 max_ethqsets;           /* # of available Ethernet queue sets */
+	u16 ethqsets;               /* # of active Ethernet queue sets */
+	u16 ethtxq_rover;           /* Tx queue to clean up next */
+	u16 ofldqsets;              /* # of active offload queue sets */
+	u16 rdmaqs;                 /* # of available RDMA Rx queues */
+	u16 niscsiq;                /* # of available iSCSI Rx queues */
+	u16 ofld_rxq[MAX_OFLD_QSETS];
+	u16 rdma_rxq[NCHAN];
+	u16 iscsi_rxq[NCHAN];
+	u16 timer_val[SGE_NTIMERS];
+	u8 counter_val[SGE_NCOUNTERS];
+	u32 fl_pg_order;            /* large page allocation size */
+        u32 stat_len;               /* length of status page at ring end */
+        u32 pktshift;               /* padding between CPL & packet data */
+        u32 fl_align;               /* response queue message alignment */
+	u32 fl_starve_thres;        /* Free List starvation threshold */
+	unsigned int starve_thres;
+	u8 idma_state[2];
+	unsigned int egr_start;
+	unsigned int ingr_start;
+	void *egr_map[MAX_EGRQ];    /* qid->queue egress queue map */
+	struct sge_rspq *ingr_map[MAX_INGQ]; /* qid->queue ingress queue map */
+	DECLARE_BITMAP(starving_fl, MAX_EGRQ);
+	DECLARE_BITMAP(txq_maperr, MAX_EGRQ);
+	DECLARE_BITMAP(blocked_fl, MAX_EGRQ);
+	struct timer_list rx_timer; /* refills starving FLs */
+	struct timer_list tx_timer; /* checks Tx queues */
+};
+
+#define for_each_ethrxq(sge, i) for (i = 0; i < (sge)->ethqsets; i++)
+#define for_each_ofldrxq(sge, i) for (i = 0; i < (sge)->ofldqsets; i++)
+#define for_each_rdmarxq(sge, i) for (i = 0; i < (sge)->rdmaqs; i++)
+#define for_each_iscsirxq(sge, i) for (i = 0; i < (sge)->niscsiq; i++)
+
+struct l2t_data;
+struct filter_info;
+
+struct t4_os_lock {
+	spinlock_t lock;
+};
+
+struct t4_os_list {
+	struct list_head list;
+};	
+
+struct adapter {
+	void __iomem *regs;
+	struct pci_dev *pdev;
+	struct device *pdev_dev;
+	unsigned long registered_device_map;
+	unsigned long flags;
+
+	const char *name;
+	unsigned int mbox;
+	unsigned int pf;
+	int msg_enable;
+
+	struct adapter_params params;
+	struct cxgb4_virt_res vres;
+	unsigned int swintr;
+
+	unsigned int wol;
+
+	struct {
+		unsigned short vec;
+		char desc[IFNAMSIZ + 10];
+	} msix_info[MAX_INGQ + 1];
+
+#ifdef T4_TRACE
+	struct trace_buf *tb[NTRACEBUFS];
+#endif
+
+	/* T4 modules */
+	struct sge sge;
+
+	struct net_device *port[MAX_NPORTS];
+	u8 chan_map[NCHAN];                   /* channel -> port map */
+
+	unsigned int filter_mode;
+	struct filter_info *filters;
+	unsigned int l2t_start;
+	unsigned int l2t_end;
+	struct l2t_data *l2t;
+
+	void *uld_handle[CXGB4_ULD_MAX];
+	struct list_head list_node;
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	struct toedev tdev;
+#endif
+	struct tid_info tids;
+	void **tid_release_head;
+	spinlock_t tid_release_lock;
+	struct work_struct tid_release_task;
+	struct work_struct db_full_task;
+	struct work_struct db_drop_task;
+
+	struct timer_list bypass_watchdog;
+	int bypass_watchdog_timeout;
+	int bypass_failover_mode;
+	int bypass_watchdog_lock;
+
+#ifdef CONFIG_PCI_IOV
+	struct delayed_work vf_monitor_task;
+	unsigned int vf_monitor_mask;
+#endif
+
+	struct dentry *debugfs_root;
+	void *dma_virt;
+	dma_addr_t dma_phys;
+
+	struct proc_dir_entry *proc_root;
+
+	spinlock_t mdio_lock;
+	spinlock_t stats_lock;
+	spinlock_t work_lock;
+
+	/* support for single-threading access to adapter mailbox registers */
+	struct t4_os_lock mbox_lock;
+	struct t4_os_list mbox_list;
+
+	struct mutex user_mutex;
+
+	int tx_coal;
+
+	spinlock_t win0_lock ____cacheline_aligned_in_smp;
+};
+
+/**
+ * t4_read_reg - read a HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ *
+ * Returns the 32-bit value of the given HW register.
+ */
+static inline u32 t4_read_reg(adapter_t *adapter, u32 reg_addr)
+{
+	u32 val = readl(adapter->regs + reg_addr);
+
+	CH_DBG(adapter, MMIO, "read register 0x%x value 0x%x\n", reg_addr,
+	       val);
+	return val;
+}
+
+/**
+ * t4_write_reg - write a HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ * @val: the value to write
+ *
+ * Write a 32-bit value into the given HW register.
+ */
+static inline void t4_write_reg(adapter_t *adapter, u32 reg_addr, u32 val)
+{
+	CH_DBG(adapter, MMIO, "setting register 0x%x to 0x%x\n", reg_addr,
+	       val);
+	writel(val, adapter->regs + reg_addr);
+}
+
+#ifndef readq
+static inline u64 readq(const volatile void __iomem *addr)
+{
+	return readl(addr) + ((u64)readl(addr + 4) << 32);
+}
+
+static inline void writeq(u64 val, volatile void __iomem *addr)
+{
+	writel(val, addr);
+	writel(val >> 32, addr + 4);
+}
+#endif
+
+/**
+ * t4_read_reg64 - read a 64-bit HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ *
+ * Returns the 64-bit value of the given HW register.
+ */
+static inline u64 t4_read_reg64(adapter_t *adapter, u32 reg_addr)
+{
+	u64 val = readq(adapter->regs + reg_addr);
+
+	CH_DBG(adapter, MMIO, "64-bit read register %#x value %#llx\n",
+	       reg_addr, (unsigned long long)val);
+	return val;
+}
+
+/**
+ * t4_write_reg64 - write a 64-bit HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ * @val: the value to write
+ *
+ * Write a 64-bit value into the given HW register.
+ */
+static inline void t4_write_reg64(adapter_t *adapter, u32 reg_addr, u64 val)
+{
+	CH_DBG(adapter, MMIO, "setting register %#x to %#llx\n", reg_addr,
+	       (unsigned long long)val);
+	writeq(val, adapter->regs + reg_addr);
+}
+
+/**
+ * t4_os_pci_write_cfg4 - 32-bit write to PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: the value to write
+ *
+ * Write a 32-bit value into the given register in PCI config space.
+ */
+static inline void t4_os_pci_write_cfg4(adapter_t *adapter, int reg, u32 val)
+{
+	pci_write_config_dword(adapter->pdev, reg, val);
+}
+
+/**
+ * t4_os_pci_read_cfg4 - read a 32-bit value from PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: where to store the value read
+ *
+ * Read a 32-bit value from the given register in PCI config space.
+ */
+static inline void t4_os_pci_read_cfg4(adapter_t *adapter, int reg, u32 *val)
+{
+	pci_read_config_dword(adapter->pdev, reg, val); 
+}
+
+/**
+ * t4_os_pci_write_cfg2 - 16-bit write to PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: the value to write
+ *
+ * Write a 16-bit value into the given register in PCI config space.
+ */
+static inline void t4_os_pci_write_cfg2(adapter_t *adapter, int reg, u16 val)
+{
+	pci_write_config_word(adapter->pdev, reg, val);
+}
+
+/**
+ * t4_os_pci_read_cfg2 - read a 16-bit value from PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: where to store the value read
+ *
+ * Read a 16-bit value from the given register in PCI config space.
+ */
+static inline void t4_os_pci_read_cfg2(adapter_t *adapter, int reg, u16 *val)
+{
+	pci_read_config_word(adapter->pdev, reg, val); 
+}
+
+/**
+ * t4_os_find_pci_capability - lookup a capability in the PCI capability list
+ * @adapter: the adapter
+ * @cap: the capability
+ *
+ * Return the address of the given capability within the PCI capability list.
+ */
+static inline int t4_os_find_pci_capability(adapter_t *adapter, int cap)
+{
+	return pci_find_capability(adapter->pdev, cap);
+}
+
+/**
+ * t4_os_pci_save_state - save PCI config state
+ * @adapter: the adapter
+ *
+ * Save the state of PCI config space.
+ */
+static inline int t4_os_pci_save_state(adapter_t *adapter)
+{
+	return pci_save_state(adapter->pdev);
+}
+
+/**
+ * t4_os_pci_restore_state - restore PCI config state
+ * @adapter: the adapter
+ *
+ * Restore previously saved PCI config space.
+ */
+static inline int t4_os_pci_restore_state(adapter_t *adapter)
+{
+	return pci_restore_state(adapter->pdev);
+}
+
+/**
+ * t4_os_set_hw_addr - store a port's MAC address in SW
+ * @adapter: the adapter
+ * @port_idx: the port index
+ * @hw_addr: the Ethernet address
+ *
+ * Store the Ethernet address of the given port in SW.  Called by the common
+ * code when it retrieves a port's Ethernet address from EEPROM.
+ */
+static inline void t4_os_set_hw_addr(adapter_t *adapter, int port_idx,
+				     u8 hw_addr[])
+{
+	memcpy(adapter->port[port_idx]->dev_addr, hw_addr, ETH_ALEN);
+	memcpy(adapter->port[port_idx]->perm_addr, hw_addr, ETH_ALEN);
+}
+
+/**
+ * netdev2pinfo - return the port_info structure associated with a net_device
+ * @dev: the netdev
+ *
+ * Return the struct port_info associated with a net_device
+ */
+static inline struct port_info *netdev2pinfo(const struct net_device *dev)
+{
+	return netdev_priv(dev);
+}
+
+/**
+ * adap2pinfo - return the port_info of a port
+ * @adap: the adapter
+ * @idx: the port index
+ *
+ * Return the port_info structure for the port of the given index.
+ */
+static inline struct port_info *adap2pinfo(struct adapter *adap, int idx)
+{
+	return netdev_priv(adap->port[idx]);
+}
+
+/**
+ * netdev2adap - return the adapter structure associated with a net_device
+ * @dev: the netdev
+ *
+ * Return the struct adapter associated with a net_device
+ */
+static inline struct adapter *netdev2adap(const struct net_device *dev)
+{
+	return netdev2pinfo(dev)->adapter;
+}
+
+/**
+ * t4_os_lock_init - initialize spinlock
+ * @lock: the spinlock
+ */
+static inline void t4_os_lock_init(struct t4_os_lock *lock)
+{
+	spin_lock_init(&lock->lock);
+}
+
+/**
+ * t4_os_trylock - try to acquire a spinlock
+ * @lock: the spinlock
+ *
+ * Returns 1 if successful and 0 otherwise.
+ */
+static inline int t4_os_trylock(struct t4_os_lock *lock)
+{
+	return spin_trylock_bh(&lock->lock);
+}
+
+/**
+ * t4_os_lock - spin until lock is acquired
+ * @lock: the spinlock
+ */
+static inline void t4_os_lock(struct t4_os_lock *lock)
+{
+	spin_lock_bh(&lock->lock);
+}
+
+/**
+ * t4_os_unlock - unlock a spinlock
+ * @lock: the spinlock
+ */
+static inline void t4_os_unlock(struct t4_os_lock *lock)
+{
+	spin_unlock_bh(&lock->lock);
+}
+
+/**
+ * t4_os_init_list_head - initialize 
+ * @head: head of list to initialize [to empty]
+ */
+static inline void t4_os_init_list_head(struct t4_os_list *head)
+{
+	INIT_LIST_HEAD(&head->list);
+}
+
+static inline struct t4_os_list *t4_os_list_first_entry(struct t4_os_list *head)
+{
+	return list_first_entry(&head->list, struct t4_os_list, list);
+}
+
+/**
+ * t4_os_atomic_add_tail - Enqueue list element atomically onto list
+ * @new: the entry to be addded to the queue
+ * @head: current head of the linked list
+ * @lock: lock to use to guarantee atomicity
+ */
+static inline void t4_os_atomic_add_tail(struct t4_os_list *new,
+					 struct t4_os_list *head,
+					 struct t4_os_lock *lock)
+{
+	t4_os_lock(lock);
+	list_add_tail(&new->list, &head->list);
+	t4_os_unlock(lock);
+}
+
+/**
+ * t4_os_atomic_list_del - Dequeue list element atomically from list
+ * @entry: the entry to be remove/dequeued from the list.
+ * @lock: the spinlock
+ */
+static inline void t4_os_atomic_list_del(struct t4_os_list *entry,
+					 struct t4_os_lock *lock)
+{
+	t4_os_lock(lock);
+	list_del(&entry->list);
+	t4_os_unlock(lock);
+}
+
+#ifndef CONFIG_CHELSIO_T4_OFFLOAD
+static inline void t4_db_full(struct adapter *adap) {}
+static inline void t4_db_dropped(struct adapter *adap) {}
+#endif
+
+#define OFFLOAD_DEVMAP_BIT 15
+
+#define tdev2adap(d) container_of(d, struct adapter, tdev)
+#define cdev2adap(d) container_of(d, struct adapter, cdev)
+
+void t4_os_portmod_changed(const struct adapter *adap, int port_id);
+void t4_os_link_changed(struct adapter *adap, int port_id, int link_stat);
+
+void *t4_alloc_mem(size_t size);
+void t4_free_mem(void *addr);
+
+void t4_free_sge_resources(struct adapter *adap);
+void t4_free_ofld_rxqs(struct adapter *adap, int n, struct sge_ofld_rxq *q);
+irq_handler_t t4_intr_handler(struct adapter *adap);
+int t4_eth_xmit(struct sk_buff *skb, struct net_device *dev);
+int t4_ethrx_handler(struct sge_rspq *q, const __be64 *rsp,
+		     const struct pkt_gl *gl);
+int t4_mgmt_tx(adapter_t *adap, struct sk_buff *skb);
+int t4_ofld_send(struct adapter *adap, struct sk_buff *skb);
+int t4_sge_alloc_rxq(struct adapter *adap, struct sge_rspq *iq, bool fwevtq,
+		     struct net_device *dev, int intr_idx,
+		     struct sge_fl *fl, rspq_handler_t hnd, int cong);
+int t4_sge_alloc_eth_txq(struct adapter *adap, struct sge_eth_txq *txq,
+			 struct net_device *dev, struct netdev_queue *netdevq,
+			 unsigned int iqid);
+int t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,
+			  struct net_device *dev, unsigned int iqid,
+			  unsigned int cmplqid);
+int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,
+			  struct net_device *dev, unsigned int iqid);
+irqreturn_t t4_sge_intr_msix(int irq, void *cookie);
+int t4_sge_init(struct adapter *adap);
+void t4_sge_start(struct adapter *adap);
+void t4_sge_stop(struct adapter *adap);
+int t4_sge_coalesce_handler(struct adapter *adap, struct sge_eth_txq *q);
+extern int dbfifo_int_thresh;
+#endif /* __T4_ADAPTER_H__ */
diff --git a/drivers/net/cxgb4/common.h b/drivers/net/cxgb4/common.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/common.h
@@ -0,0 +1,586 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __CHELSIO_COMMON_H
+#define __CHELSIO_COMMON_H
+
+#include "t4_hw.h"
+
+#define GLBL_INTR_MASK (F_CIM | F_MPS | F_PL | F_PCIE | F_MC | F_EDC0 | \
+		F_EDC1 | F_LE | F_TP | F_MA | F_PM_TX | F_PM_RX | F_ULP_RX | \
+		F_CPL_SWITCH | F_SGE | F_ULP_TX)
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+enum {
+	MAX_NPORTS     = 4,     /* max # of ports */
+	SERNUM_LEN     = 24,    /* Serial # length */
+	EC_LEN         = 16,    /* E/C length */
+	ID_LEN         = 16,    /* ID length */
+	PN_LEN         = 16,    /* Part Number length */
+	MACADDR_LEN    = 12,    /* MAC Address length */
+};
+
+enum { MEM_EDC0, MEM_EDC1, MEM_MC };
+
+enum {
+	MEMWIN0_APERTURE = 2048,
+	MEMWIN0_BASE     = 0x1b800,
+	MEMWIN1_APERTURE = 32768,
+	MEMWIN1_BASE     = 0x28000,
+	MEMWIN2_APERTURE = 65536,
+	MEMWIN2_BASE     = 0x30000,
+};
+
+enum dev_master { MASTER_CANT, MASTER_MAY, MASTER_MUST };
+
+enum dev_state { DEV_STATE_UNINIT, DEV_STATE_INIT, DEV_STATE_ERR };
+
+enum {
+	PAUSE_RX      = 1 << 0,
+	PAUSE_TX      = 1 << 1,
+	PAUSE_AUTONEG = 1 << 2
+};
+
+#define FW_VERSION_MAJOR 1
+#define FW_VERSION_MINOR 4
+#define FW_VERSION_MICRO 23
+
+struct port_stats {
+	u64 tx_octets;            /* total # of octets in good frames */
+	u64 tx_frames;            /* all good frames */
+	u64 tx_bcast_frames;      /* all broadcast frames */
+	u64 tx_mcast_frames;      /* all multicast frames */
+	u64 tx_ucast_frames;      /* all unicast frames */
+	u64 tx_error_frames;      /* all error frames */
+
+	u64 tx_frames_64;         /* # of Tx frames in a particular range */
+	u64 tx_frames_65_127;
+	u64 tx_frames_128_255;
+	u64 tx_frames_256_511;
+	u64 tx_frames_512_1023;
+	u64 tx_frames_1024_1518;
+	u64 tx_frames_1519_max;
+
+	u64 tx_drop;              /* # of dropped Tx frames */
+	u64 tx_pause;             /* # of transmitted pause frames */
+	u64 tx_ppp0;              /* # of transmitted PPP prio 0 frames */
+	u64 tx_ppp1;              /* # of transmitted PPP prio 1 frames */
+	u64 tx_ppp2;              /* # of transmitted PPP prio 2 frames */
+	u64 tx_ppp3;              /* # of transmitted PPP prio 3 frames */
+	u64 tx_ppp4;              /* # of transmitted PPP prio 4 frames */
+	u64 tx_ppp5;              /* # of transmitted PPP prio 5 frames */
+	u64 tx_ppp6;              /* # of transmitted PPP prio 6 frames */
+	u64 tx_ppp7;              /* # of transmitted PPP prio 7 frames */
+
+	u64 rx_octets;            /* total # of octets in good frames */
+	u64 rx_frames;            /* all good frames */
+	u64 rx_bcast_frames;      /* all broadcast frames */
+	u64 rx_mcast_frames;      /* all multicast frames */
+	u64 rx_ucast_frames;      /* all unicast frames */
+	u64 rx_too_long;          /* # of frames exceeding MTU */
+	u64 rx_jabber;            /* # of jabber frames */
+	u64 rx_fcs_err;           /* # of received frames with bad FCS */
+	u64 rx_len_err;           /* # of received frames with length error */
+	u64 rx_symbol_err;        /* symbol errors */
+	u64 rx_runt;              /* # of short frames */
+
+	u64 rx_frames_64;         /* # of Rx frames in a particular range */
+	u64 rx_frames_65_127;
+	u64 rx_frames_128_255;
+	u64 rx_frames_256_511;
+	u64 rx_frames_512_1023;
+	u64 rx_frames_1024_1518;
+	u64 rx_frames_1519_max;
+
+	u64 rx_pause;             /* # of received pause frames */
+	u64 rx_ppp0;              /* # of received PPP prio 0 frames */
+	u64 rx_ppp1;              /* # of received PPP prio 1 frames */
+	u64 rx_ppp2;              /* # of received PPP prio 2 frames */
+	u64 rx_ppp3;              /* # of received PPP prio 3 frames */
+	u64 rx_ppp4;              /* # of received PPP prio 4 frames */
+	u64 rx_ppp5;              /* # of received PPP prio 5 frames */
+	u64 rx_ppp6;              /* # of received PPP prio 6 frames */
+	u64 rx_ppp7;              /* # of received PPP prio 7 frames */
+
+	u64 rx_ovflow0;           /* drops due to buffer-group 0 overflows */
+	u64 rx_ovflow1;           /* drops due to buffer-group 1 overflows */
+	u64 rx_ovflow2;           /* drops due to buffer-group 2 overflows */
+	u64 rx_ovflow3;           /* drops due to buffer-group 3 overflows */
+	u64 rx_trunc0;            /* buffer-group 0 truncated packets */
+	u64 rx_trunc1;            /* buffer-group 1 truncated packets */
+	u64 rx_trunc2;            /* buffer-group 2 truncated packets */
+	u64 rx_trunc3;            /* buffer-group 3 truncated packets */
+};
+
+struct lb_port_stats {
+	u64 octets;
+	u64 frames;
+	u64 bcast_frames;
+	u64 mcast_frames;
+	u64 ucast_frames;
+	u64 error_frames;
+
+	u64 frames_64;
+	u64 frames_65_127;
+	u64 frames_128_255;
+	u64 frames_256_511;
+	u64 frames_512_1023;
+	u64 frames_1024_1518;
+	u64 frames_1519_max;
+
+	u64 drop;
+
+	u64 ovflow0;
+	u64 ovflow1;
+	u64 ovflow2;
+	u64 ovflow3;
+	u64 trunc0;
+	u64 trunc1;
+	u64 trunc2;
+	u64 trunc3;
+};
+
+struct tp_tcp_stats {
+	u32 tcpOutRsts;
+	u64 tcpInSegs;
+	u64 tcpOutSegs;
+	u64 tcpRetransSegs;
+};
+
+struct tp_usm_stats {
+	u32 frames;
+	u32 drops;
+	u64 octets;
+};
+
+struct tp_fcoe_stats {
+	u32 framesDDP;
+	u32 framesDrop;
+	u64 octetsDDP;
+};
+
+struct tp_err_stats {
+	u32 macInErrs[4];
+	u32 hdrInErrs[4];
+	u32 tcpInErrs[4];
+	u32 tnlCongDrops[4];
+	u32 ofldChanDrops[4];
+	u32 tnlTxDrops[4];
+	u32 ofldVlanDrops[4];
+	u32 tcp6InErrs[4];
+	u32 ofldNoNeigh;
+	u32 ofldCongDefer;
+};
+
+struct tp_proxy_stats {
+	u32 proxy[4];
+};
+
+struct tp_cpl_stats {
+	u32 req[4];
+	u32 rsp[4];
+};
+
+struct tp_rdma_stats {
+	u32 rqe_dfr_mod;
+	u32 rqe_dfr_pkt;
+};
+
+struct tp_params {
+	unsigned int ntxchan;        /* # of Tx channels */
+	unsigned int tre;            /* log2 of core clocks per TP tick */
+	unsigned int dack_re;        /* DACK timer resolution */
+	unsigned int la_mask;        /* what events are recorded by TP LA */
+	unsigned short tx_modq[NCHAN];  /* channel to modulation queue map */
+};
+
+struct vpd_params {
+	unsigned int cclk;
+	u8 ec[EC_LEN + 1];
+	u8 sn[SERNUM_LEN + 1];
+	u8 id[ID_LEN + 1];
+	u8 pn[PN_LEN + 1];
+	u8 na[MACADDR_LEN + 1];
+};
+
+struct pci_params {
+	unsigned int   vpd_cap_addr;
+	unsigned short speed;
+	unsigned char  width;
+};
+
+/*
+ * Firmware device log.
+ */
+struct devlog_params {
+	u32 memtype;			/* which memory (EDC0, EDC1, MC) */
+	u32 start;			/* start of log in firmware memory */
+	u32 size;			/* size of log */
+};
+
+struct adapter_params {
+	struct tp_params  tp;
+	struct vpd_params vpd;
+	struct pci_params pci;
+	struct devlog_params devlog;
+
+	unsigned int sf_size;             /* serial flash size in bytes */
+	unsigned int sf_nsec;             /* # of flash sectors */
+
+	unsigned int fw_vers;
+	unsigned int tp_vers;
+
+	unsigned short mtus[NMTUS];
+	unsigned short a_wnd[NCCTRL_WIN];
+	unsigned short b_wnd[NCCTRL_WIN];
+
+	unsigned int mc_size;             /* MC memory size */
+	unsigned int nfilters;            /* size of filter region */
+
+	unsigned int cim_la_size;
+
+	unsigned char nports;             /* # of ethernet ports */
+	unsigned char portvec;
+	unsigned char rev;                /* chip revision */
+	unsigned char offload;
+
+	unsigned char bypass;
+
+	unsigned int ofldq_wr_cred;
+};
+
+enum {					    /* chip revisions */
+	T4_REV_A  = 0,
+};
+
+struct trace_params {
+	u32 data[TRACE_LEN / 4];
+	u32 mask[TRACE_LEN / 4];
+	unsigned short snap_len;
+	unsigned short min_len;
+	unsigned char skip_ofst;
+	unsigned char skip_len;
+	unsigned char invert;
+	unsigned char port;
+};
+
+struct link_config {
+	unsigned short supported;        /* link capabilities */
+	unsigned short advertising;      /* advertised capabilities */
+	unsigned short requested_speed;  /* speed user has requested */
+	unsigned short speed;            /* actual link speed */
+	unsigned char  requested_fc;     /* flow control user has requested */
+	unsigned char  fc;               /* actual link flow control */
+	unsigned char  autoneg;          /* autonegotiating? */
+	unsigned char  link_ok;          /* link up? */
+};
+
+#include "adapter.h"
+
+#ifndef PCI_VENDOR_ID_CHELSIO
+# define PCI_VENDOR_ID_CHELSIO 0x1425
+#endif
+
+#define for_each_port(adapter, iter) \
+	for (iter = 0; iter < (adapter)->params.nports; ++iter)
+
+static inline int is_offload(const struct adapter *adap)
+{
+	return adap->params.offload;
+}
+
+static inline int is_bypass(const adapter_t *adap)
+{
+	return adap->params.bypass;
+}
+
+static inline int is_bypass_device(int device)
+{
+	/* XXX - this should be set based upon device capabilities */
+	switch(device) {
+#ifdef CONFIG_CHELSIO_BYPASS
+		case 0x440b:
+		case 0x440c:
+			return 1;
+#endif
+
+		default:
+			return 0;
+	}
+}
+
+static inline unsigned int core_ticks_per_usec(const struct adapter *adap)
+{
+	return adap->params.vpd.cclk / 1000;
+}
+
+static inline unsigned int us_to_core_ticks(const struct adapter *adap,
+					    unsigned int us)
+{
+	return (us * adap->params.vpd.cclk) / 1000;
+}
+
+static inline unsigned int core_ticks_to_us(const struct adapter *adapter,
+					    unsigned int ticks)
+{
+	/* add Core Clock / 2 to round ticks to nearest uS */
+	return ((ticks * 1000 + adapter->params.vpd.cclk/2) /
+		adapter->params.vpd.cclk);
+}
+
+static inline unsigned int dack_ticks_to_usec(const struct adapter *adap,
+					      unsigned int ticks)
+{
+	return (ticks << adap->params.tp.dack_re) / core_ticks_per_usec(adap);
+}
+
+void t4_set_reg_field(struct adapter *adap, unsigned int addr, u32 mask, u32 val);
+int t4_wait_op_done_val(struct adapter *adapter, int reg, u32 mask, int polarity,
+			int attempts, int delay, u32 *valp);
+
+static inline int t4_wait_op_done(struct adapter *adapter, int reg, u32 mask,
+				  int polarity, int attempts, int delay)
+{
+	return t4_wait_op_done_val(adapter, reg, mask, polarity, attempts,
+				   delay, NULL);
+}
+
+int t4_wr_mbox_meat(struct adapter *adap, int mbox, const void *cmd, int size,
+		    void *rpl, bool sleep_ok);
+
+static inline int t4_wr_mbox(struct adapter *adap, int mbox, const void *cmd,
+			     int size, void *rpl)
+{
+	return t4_wr_mbox_meat(adap, mbox, cmd, size, rpl, true);
+}
+
+static inline int t4_wr_mbox_ns(struct adapter *adap, int mbox, const void *cmd,
+				int size, void *rpl)
+{
+	return t4_wr_mbox_meat(adap, mbox, cmd, size, rpl, false);
+}
+
+void t4_read_indirect(struct adapter *adap, unsigned int addr_reg,
+		      unsigned int data_reg, u32 *vals, unsigned int nregs,
+		      unsigned int start_idx);
+void t4_write_indirect(struct adapter *adap, unsigned int addr_reg,
+		       unsigned int data_reg, const u32 *vals,
+		       unsigned int nregs, unsigned int start_idx);
+
+void t4_hw_pci_read_cfg4(adapter_t *adapter, int reg, u32 *val);
+
+struct fw_filter_wr;
+
+void t4_intr_enable(struct adapter *adapter);
+void t4_intr_disable(struct adapter *adapter);
+void t4_intr_clear(struct adapter *adapter);
+int t4_slow_intr_handler(struct adapter *adapter);
+
+int t4_hash_mac_addr(const u8 *addr);
+int t4_link_start(struct adapter *adap, unsigned int mbox, unsigned int port,
+		  struct link_config *lc);
+int t4_restart_aneg(struct adapter *adap, unsigned int mbox, unsigned int port);
+int t4_seeprom_read(struct adapter *adapter, u32 addr, u32 *data);
+int t4_seeprom_write(struct adapter *adapter, u32 addr, u32 data);
+int t4_eeprom_ptov(unsigned int phys_addr, unsigned int fn, unsigned int sz);
+int t4_seeprom_wp(struct adapter *adapter, int enable);
+int t4_get_raw_vpd_params(struct adapter *adapter, struct vpd_params *p);
+int t4_get_vpd_params(struct adapter *adapter, struct vpd_params *p);
+int t4_read_flash(struct adapter *adapter, unsigned int addr, unsigned int nwords,
+		  u32 *data, int byte_oriented);
+int t4_load_fw(struct adapter *adapter, const u8 *fw_data, unsigned int size);
+int t4_load_boot(struct adapter *adap, u8 *boot_data,
+                 unsigned int boot_addr, unsigned int size);
+unsigned int t4_flash_cfg_addr(struct adapter *adapter);
+int t4_load_cfg(struct adapter *adapter, const u8 *cfg_data, unsigned int size);
+int t4_get_fw_version(struct adapter *adapter, u32 *vers);
+int t4_get_tp_version(struct adapter *adapter, u32 *vers);
+int t4_check_fw_version(struct adapter *adapter);
+int t4_init_hw(struct adapter *adapter, u32 fw_params);
+int t4_prep_adapter(struct adapter *adapter, bool reset);
+int t4_port_init(struct adapter *adap, int mbox, int pf, int vf);
+int t4_reinit_adapter(struct adapter *adap);
+void t4_fatal_err(struct adapter *adapter);
+void t4_db_full(struct adapter *adapter);
+void t4_db_dropped(struct adapter *adapter);
+int t4_set_trace_filter(struct adapter *adapter, const struct trace_params *tp,
+			int filter_index, int enable);
+void t4_get_trace_filter(struct adapter *adapter, struct trace_params *tp,
+			 int filter_index, int *enabled);
+int t4_config_rss_range(struct adapter *adapter, int mbox, unsigned int viid,
+			int start, int n, const u16 *rspq, unsigned int nrspq);
+int t4_config_glbl_rss(struct adapter *adapter, int mbox, unsigned int mode,
+		       unsigned int flags);
+int t4_config_vi_rss(struct adapter *adapter, int mbox, unsigned int viid,
+		     unsigned int flags, unsigned int defq);
+int t4_read_rss(struct adapter *adapter, u16 *entries);
+void t4_read_rss_key(struct adapter *adapter, u32 *key);
+void t4_write_rss_key(struct adapter *adap, const u32 *key, int idx);
+void t4_read_rss_pf_config(struct adapter *adapter, unsigned int index, u32 *valp);
+void t4_write_rss_pf_config(struct adapter *adapter, unsigned int index, u32 val);
+void t4_read_rss_vf_config(struct adapter *adapter, unsigned int index,
+			   u32 *vfl, u32 *vfh);
+void t4_write_rss_vf_config(struct adapter *adapter, unsigned int index,
+			    u32 vfl, u32 vfh);
+u32 t4_read_rss_pf_map(struct adapter *adapter);
+void t4_write_rss_pf_map(struct adapter *adapter, u32 pfmap);
+u32 t4_read_rss_pf_mask(struct adapter *adapter);
+void t4_write_rss_pf_mask(struct adapter *adapter, u32 pfmask);
+int t4_mps_set_active_ports(struct adapter *adap, unsigned int port_mask);
+void t4_pmtx_get_stats(struct adapter *adap, u32 cnt[], u64 cycles[]);
+void t4_pmrx_get_stats(struct adapter *adap, u32 cnt[], u64 cycles[]);
+void t4_read_cimq_cfg(struct adapter *adap, u16 *base, u16 *size, u16 *thres);
+int t4_read_cim_ibq(struct adapter *adap, unsigned int qid, u32 *data, size_t n);
+int t4_read_cim_obq(struct adapter *adap, unsigned int qid, u32 *data, size_t n);
+int t4_cim_read(struct adapter *adap, unsigned int addr, unsigned int n,
+		unsigned int *valp);
+int t4_cim_write(struct adapter *adap, unsigned int addr, unsigned int n,
+		 const unsigned int *valp);
+int t4_cim_ctl_read(struct adapter *adap, unsigned int addr, unsigned int n,
+		    unsigned int *valp);
+int t4_cim_read_la(struct adapter *adap, u32 *la_buf, unsigned int *wrptr);
+void t4_cim_read_pif_la(struct adapter *adap, u32 *pif_req, u32 *pif_rsp,
+		unsigned int *pif_req_wrptr, unsigned int *pif_rsp_wrptr);
+void t4_cim_read_ma_la(struct adapter *adap, u32 *ma_req, u32 *ma_rsp);
+int t4_mc_read(struct adapter *adap, u32 addr, __be32 *data, u64 *parity);
+int t4_edc_read(struct adapter *adap, int idx, u32 addr, __be32 *data, u64 *parity);
+int t4_mem_read(struct adapter *adap, int mtype, u32 addr, u32 size,
+		__be32 *data);
+
+int t4_mem_win_read(struct adapter *adap, u32 addr, __be32 *data);
+int t4_mem_win_write(struct adapter *adap, u32 addr, __be32 *data);
+int t4_mem_win_read_len(struct adapter *adap, u32 addr, __be32 *data, int len);
+int t4_memory_read(struct adapter *adap, int mtype, u32 addr, u32 size,
+		__be32 *data);
+int t4_memory_write(struct adapter *adap, int mtype, u32 addr, u32 size,
+		__be32 *data);
+
+void t4_get_port_stats(struct adapter *adap, int idx, struct port_stats *p);
+void t4_get_port_stats_offset(struct adapter *adap, int idx,
+		struct port_stats *stats,
+		struct port_stats *offset);
+void t4_get_lb_stats(struct adapter *adap, int idx, struct lb_port_stats *p);
+void t4_clr_port_stats(struct adapter *adap, int idx);
+
+void t4_read_mtu_tbl(struct adapter *adap, u16 *mtus, u8 *mtu_log);
+void t4_read_cong_tbl(struct adapter *adap, u16 incr[NMTUS][NCCTRL_WIN]);
+void t4_read_pace_tbl(struct adapter *adap, unsigned int pace_vals[NTX_SCHED]);
+void t4_get_tx_sched(struct adapter *adap, unsigned int sched, unsigned int *kbps,
+		     unsigned int *ipg);
+void t4_tp_wr_bits_indirect(struct adapter *adap, unsigned int addr,
+			    unsigned int mask, unsigned int val);
+void t4_tp_read_la(struct adapter *adap, u64 *la_buf, unsigned int *wrptr);
+void t4_tp_get_err_stats(struct adapter *adap, struct tp_err_stats *st);
+void t4_tp_get_proxy_stats(struct adapter *adap, struct tp_proxy_stats *st);
+void t4_tp_get_cpl_stats(struct adapter *adap, struct tp_cpl_stats *st);
+void t4_tp_get_rdma_stats(struct adapter *adap, struct tp_rdma_stats *st);
+void t4_get_usm_stats(struct adapter *adap, struct tp_usm_stats *st);
+void t4_tp_get_tcp_stats(struct adapter *adap, struct tp_tcp_stats *v4,
+			 struct tp_tcp_stats *v6);
+void t4_get_fcoe_stats(struct adapter *adap, unsigned int idx,
+		       struct tp_fcoe_stats *st);
+void t4_load_mtus(struct adapter *adap, const unsigned short *mtus,
+		  const unsigned short *alpha, const unsigned short *beta);
+
+void t4_ulprx_read_la(struct adapter *adap, u32 *la_buf);
+
+int t4_set_sched_bps(struct adapter *adap, int sched, unsigned int kbps);
+int t4_set_sched_ipg(struct adapter *adap, int sched, unsigned int ipg);
+int t4_set_pace_tbl(struct adapter *adap, const unsigned int *pace_vals,
+		    unsigned int start, unsigned int n);
+void t4_get_chan_txrate(struct adapter *adap, u64 *nic_rate, u64 *ofld_rate);
+int t4_set_filter_mode(struct adapter *adap, unsigned int mode_map);
+void t4_mk_filtdelwr(unsigned int ftid, struct fw_filter_wr *wr, int qid);
+
+void t4_wol_magic_enable(struct adapter *adap, unsigned int port, const u8 *addr);
+int t4_wol_pat_enable(struct adapter *adap, unsigned int port, unsigned int map,
+		      u64 mask0, u64 mask1, unsigned int crc, bool enable);
+
+int t4_fw_hello(struct adapter *adap, unsigned int mbox, unsigned int evt_mbox,
+		enum dev_master master, enum dev_state *state);
+int t4_fw_bye(struct adapter *adap, unsigned int mbox);
+int t4_fw_reset(struct adapter *adap, unsigned int mbox, int reset);
+int t4_fw_halt(struct adapter *adap, unsigned int mbox, int force);
+int t4_fw_restart(struct adapter *adap, unsigned int mbox, int reset);
+int t4_fw_upgrade(struct adapter *adap, unsigned int mbox,
+		  const u8 *fw_data, unsigned int size, int force);
+int t4_fw_config_file(struct adapter *adap, unsigned int mbox,
+		      unsigned int mtype, unsigned int maddr,
+		      u32 *finiver, u32 *finicsum, u32 *cfcsum);
+int t4_fixup_host_params(struct adapter *adap, unsigned int page_size,
+			 unsigned int cache_line_size);
+int t4_fw_initialize(struct adapter *adap, unsigned int mbox);
+int t4_query_params(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int nparams, const u32 *params,
+		    u32 *val);
+int t4_set_params(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		  unsigned int vf, unsigned int nparams, const u32 *params,
+		  const u32 *val);
+int t4_cfg_pfvf(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		unsigned int vf, unsigned int txq, unsigned int txq_eth_ctrl,
+		unsigned int rxqi, unsigned int rxq, unsigned int tc,
+		unsigned int vi, unsigned int cmask, unsigned int pmask,
+		unsigned int exactf, unsigned int rcaps, unsigned int wxcaps);
+int t4_alloc_vi_func(struct adapter *adap, unsigned int mbox,
+		     unsigned int port, unsigned int pf, unsigned int vf,
+		     unsigned int nmac, u8 *mac, unsigned int *rss_size,
+		     unsigned int portfunc, unsigned int idstype);
+int t4_alloc_vi(struct adapter *adap, unsigned int mbox, unsigned int port,
+		unsigned int pf, unsigned int vf, unsigned int nmac, u8 *mac,
+		unsigned int *rss_size);
+int t4_free_vi(struct adapter *adap, unsigned int mbox,
+	       unsigned int pf, unsigned int vf,
+	       unsigned int viid);
+int t4_set_rxmode(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		  int mtu, int promisc, int all_multi, int bcast, int vlanex,
+		  bool sleep_ok);
+int t4_alloc_mac_filt(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		      bool free, unsigned int naddr, const u8 **addr, u16 *idx,
+		      u64 *hash, bool sleep_ok);
+int t4_change_mac(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		  int idx, const u8 *addr, bool persist, bool add_smt);
+int t4_set_addr_hash(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		     bool ucast, u64 vec, bool sleep_ok);
+int t4_enable_vi(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		 bool rx_en, bool tx_en);
+int t4_identify_port(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		     unsigned int nblinks);
+int t4_mdio_rd(struct adapter *adap, unsigned int mbox, unsigned int phy_addr,
+	       unsigned int mmd, unsigned int reg, unsigned int *valp);
+int t4_mdio_wr(struct adapter *adap, unsigned int mbox, unsigned int phy_addr,
+	       unsigned int mmd, unsigned int reg, unsigned int val);
+int t4_iq_start_stop(struct adapter *adap, unsigned int mbox, bool start,
+		     unsigned int pf, unsigned int vf, unsigned int iqid,
+		     unsigned int fl0id, unsigned int fl1id);
+int t4_iq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+	       unsigned int vf, unsigned int iqtype, unsigned int iqid,
+	       unsigned int fl0id, unsigned int fl1id);
+int t4_eth_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		   unsigned int vf, unsigned int eqid);
+int t4_ctrl_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int eqid);
+int t4_ofld_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int eqid);
+int t4_sge_ctxt_rd(struct adapter *adap, unsigned int mbox, unsigned int cid,
+		   enum ctxt_type ctype, u32 *data);
+int t4_sge_ctxt_rd_bd(struct adapter *adap, unsigned int cid, enum ctxt_type ctype,
+		      u32 *data);
+int t4_sge_ctxt_flush(struct adapter *adap, unsigned int mbox);
+int t4_handle_fw_rpl(struct adapter *adap, const __be64 *rpl);
+int t4_fwaddrspace_write(struct adapter *adap, unsigned int mbox, u32 addr, u32 val);
+
+#ifdef __cplusplus
+}
+#endif
+#endif /* __CHELSIO_COMMON_H */
diff --git a/drivers/net/cxgb4/cxgb4_compat.h b/drivers/net/cxgb4/cxgb4_compat.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/cxgb4_compat.h
@@ -0,0 +1,189 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/*
+ * This file is used to allow the driver to be compiled under multiple
+ * versions of Linux with as few obtrusive in-line #ifdef's as possible.
+ */
+
+#ifndef __CXGB4_COMPAT_H
+#define __CXGB4_COMPAT_H
+
+#include <linux/version.h>
+
+/*
+ * Set a /proc node's module owner field.
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30)
+#define SET_PROC_NODE_OWNER(_p, _owner) \
+	do { (_p)->owner = (_owner); } while (0)
+#else
+#define SET_PROC_NODE_OWNER(_p, _owner) \
+	do { } while (0)
+#endif
+
+/*
+ * Collect up to maxaddrs worth of a netdevice's unicast addresses, starting
+ * at a specified offset within the list, into an array of addrss pointers and
+ * return the number collected.
+ */
+static inline unsigned int collect_netdev_uc_list_addrs(const struct net_device *dev,
+							const u8 **addr,
+							unsigned int offset,
+							unsigned int maxaddrs)
+{
+	unsigned int index = 0;
+	unsigned int naddr = 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,31)
+	const struct dev_addr_list *da;
+
+	for (da = dev->uc_list; da && naddr < maxaddrs; da = da->next)
+		if (index++ >= offset)
+			addr[naddr++] = da->dmi_addr;
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34)
+	const struct netdev_hw_addr *ha;
+
+	list_for_each_entry(ha, &dev->uc.list, list)
+		if (index++ >= offset) {
+			addr[naddr++] = ha->addr;
+			if (naddr >= maxaddrs)
+				break;
+		}
+#else
+	const struct netdev_hw_addr *ha;
+
+	netdev_for_each_uc_addr(ha, dev)
+		if (index++ >= offset) {
+			addr[naddr++] = ha->addr;
+			if (naddr >= maxaddrs)
+				break;
+		}
+#endif
+	return naddr;
+}
+
+/*
+ * Collect up to maxaddrs worth of a netdevice's multicast addresses, starting
+ * at a specified offset within the list, into an array of addrss pointers and
+ * return the number collected.
+ */
+static inline unsigned int collect_netdev_mc_list_addrs(const struct net_device *dev,
+							const u8 **addr,
+							unsigned int offset,
+							unsigned int maxaddrs)
+{
+	unsigned int index = 0;
+	unsigned int naddr = 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34)
+	const struct dev_addr_list *da;
+
+	for (da = dev->mc_list; da && naddr < maxaddrs; da = da->next)
+		if (index++ >= offset)
+			addr[naddr++] = da->dmi_addr;
+#elif LINUX_VERSION_CODE == KERNEL_VERSION(2,6,34)
+	const struct dev_mc_list *mclist;
+
+	netdev_for_each_mc_addr(mclist, dev)
+		if (index++ >= offset) {
+			addr[naddr++] = mclist->dmi_addr;
+			if (naddr >= maxaddrs)
+				break;
+		}
+#else
+	const struct netdev_hw_addr *ha;
+
+	netdev_for_each_mc_addr(ha, dev)
+		if (index++ >= offset) {
+			addr[naddr++] = ha->addr;
+			if (naddr >= maxaddrs)
+				break;
+		}
+#endif
+	return naddr;
+}
+
+/**
+ *      struct vlan_priority_tci_mapping - vlan egress priority mappings
+ *      @priority: skb priority
+ *      @vlan_qos: vlan priority: (skb->priority << 13) & 0xE000
+ *      @next: pointer to next struct
+ */
+struct vlan_priority_tci_mapping {
+        u32                                     priority;
+        u16                                     vlan_qos;
+        struct vlan_priority_tci_mapping        *next;
+};
+
+struct vlan_dev_info {
+        unsigned int                            nr_ingress_mappings;
+        u32                                     ingress_priority_map[8];
+        unsigned int                            nr_egress_mappings;
+        struct vlan_priority_tci_mapping        *egress_priority_map[16];
+
+        u16                                     vlan_id;
+        u16                                     flags;
+
+        struct net_device                       *real_dev;
+        unsigned char                           real_dev_addr[ETH_ALEN];
+
+        struct proc_dir_entry                   *dent;
+        unsigned long                           cnt_inc_headroom_on_tx;
+        unsigned long                           cnt_encap_on_xmit;
+        struct vlan_rx_stats                    *vlan_rx_stats;
+};
+
+static inline struct vlan_dev_info *vlan_dev_info(const struct net_device *dev)
+{
+        return netdev_priv(dev);
+}
+
+#ifndef NIPQUAD
+#define NIPQUAD(addr) \
+	((unsigned char *)&addr)[0], \
+	((unsigned char *)&addr)[1], \
+	((unsigned char *)&addr)[2], \
+	((unsigned char *)&addr)[3]
+#endif
+
+#ifndef NIPQUAD_FMT
+#define NIPQUAD_FMT "%u.%u.%u.%u"
+#endif
+
+/*
+ * Number of online CPUs on the host
+ */
+static inline unsigned int online_cpus(void)
+{
+	unsigned int numcpus = num_online_cpus();
+
+	/*
+	 * TODO: CONFIG_XEN is not sufficient to differentiate. Distribution kernels
+	 * can be with CONFIG_XEN defined.
+	 */
+#if defined(CONFIG_XEN) || defined(CONFIG_CRASH_DUMP)
+	/*
+	 * In Xen, num_online_cpus() returns the number of physical CPUs during
+	 * boot and 4 (maximum for Dom0) during non-boot.
+	 */
+	if (numcpus > 4)
+		numcpus = 4;
+#endif
+	return numcpus;
+}
+
+#ifndef PORT_DA
+#define PORT_DA 0x05
+#endif
+#ifndef PORT_OTHER
+#define PORT_OTHER 0xff
+#endif
+
+#endif  /* !__CXGB4_COMPAT_H */
diff --git a/drivers/net/cxgb4/cxgb4_ctl_defs.h b/drivers/net/cxgb4/cxgb4_ctl_defs.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/cxgb4_ctl_defs.h
@@ -0,0 +1,151 @@
+/*
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef _CXGB4_OFLD_CTL_DEFS_H
+#define _CXGB4_OFLD_CTL_DEFS_H
+
+#include <linux/types.h>
+
+enum {
+	GET_MAX_OUTSTANDING_WR = 0,
+	GET_TX_MAX_CHUNK       = 1,
+	GET_MTUS               = 6,
+	GET_WR_LEN             = 7,
+	GET_DDP_PARAMS         = 9,
+
+	ULP_ISCSI_GET_PARAMS   = 10,
+	ULP_ISCSI_SET_PARAMS   = 11,
+
+	RDMA_GET_PARAMS        = 12,
+	RDMA_CQ_OP             = 13,
+	RDMA_CQ_SETUP          = 14,
+	RDMA_CQ_DISABLE        = 15,
+
+	GET_PORT_SCHED         = 21,
+	GET_NUM_QUEUES         = 22,
+	GET_CHAN_MAP	       = 23,
+	GET_PORT_ARRAY         = 24,
+
+	FAILOVER		= 30,
+	FAILOVER_DONE		= 31,
+	FAILOVER_CLEAR		= 32,
+	FAILOVER_ACTIVE_SLAVE	= 33,
+	FAILOVER_PORT_DOWN	= 34,
+	FAILOVER_PORT_UP	= 35,
+	FAILOVER_PORT_RELEASE	= 36,
+	FAILOVER_BOND_DOWN	= 38,
+	FAILOVER_BOND_UP	= 39,
+};
+
+/*
+ * Structure used to describe a TID range.  Valid TIDs are [base, base+num).
+ */
+struct tid_range {
+	unsigned int base;   /* first TID */
+	unsigned int num;    /* number of TIDs in range */
+};
+
+/*
+ * Structure used to request the size and contents of the MTU table.
+ */
+struct mtutab {
+	unsigned int size;          /* # of entries in the MTU table */
+	const unsigned short *mtus; /* the MTU table values */
+};
+
+struct adap_ports {
+	unsigned int nports;          /* number of ports on this adapter */
+	struct net_device *lldevs[4]; /* Max number of ports is 4 */
+};
+
+struct port_array {
+        unsigned int nports;          /* number of ports on this adapter */
+        struct net_device **lldevs;   /* points to array of net_devices */
+};
+
+struct net_device;
+
+/* Structure used to request a port's offload scheduler */
+struct port_sched {
+	struct net_device *dev;          /* the net_device */
+	int sched;                       /* associated scheduler */
+};
+
+struct bond_ports {
+	unsigned int port;
+	unsigned int nports;            /* number of ports on this adapter */
+	unsigned int ports[4];          /* Max number of ports is 4 */
+};
+
+struct pci_dev;
+
+/*
+ * Structure used to request the TCP DDP parameters.
+ */
+struct ddp_params {
+	unsigned int llimit;     /* TDDP region start address */
+	unsigned int ulimit;     /* TDDP region end address */
+	unsigned int tag_mask;   /* TDDP tag mask */
+	struct pci_dev *pdev;
+};
+
+/*
+ * Structure used to return information to the iscsi layer.
+ */
+struct ulp_iscsi_info {
+	unsigned int	offset;
+	unsigned int	llimit;
+	unsigned int	ulimit;
+	unsigned int	tagmask;
+	unsigned char	pgsz_factor[4];
+	unsigned int	max_rxsz;
+	unsigned int	max_txsz;
+	struct pci_dev	*pdev;
+};
+
+/*
+ * Structure used to return information to the RDMA layer.
+ */
+struct rdma_info {
+	unsigned int tpt_base;   /* TPT base address */
+	unsigned int tpt_top;	 /* TPT last entry address */
+	unsigned int pbl_base;   /* PBL base address */
+	unsigned int pbl_top;	 /* PBL last entry address */
+	unsigned int rqt_base;   /* RQT base address */
+	unsigned int rqt_top;	 /* RQT last entry address */
+	unsigned int udbell_len; /* user doorbell region length */
+	resource_size_t udbell_physbase; /* user doorbell physical start addr */
+	void __iomem *kdb_addr;  /* kernel doorbell register address */
+	struct pci_dev *pdev;    /* associated PCI device */
+};
+
+/*
+ * Structure used to request an operation on an RDMA completion queue.
+ */
+struct rdma_cq_op {
+	unsigned int id;
+	unsigned int op;
+	unsigned int credits;
+};
+
+/*
+ * Structure used to setup RDMA completion queues.
+ */
+struct rdma_cq_setup {
+	unsigned int id;
+	unsigned long long base_addr;
+	unsigned int size;
+	unsigned int credits;
+	unsigned int credit_thres;
+	unsigned int ovfl_mode;
+};
+
+struct toedev;
+
+#endif /* _CXGB4_OFLD_CTL_DEFS_H */
diff --git a/drivers/net/cxgb4/cxgb4_main.c b/drivers/net/cxgb4/cxgb4_main.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/cxgb4_main.c
@@ -0,0 +1,10931 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/dma-mapping.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/mii.h>
+#include <linux/sockios.h>
+#include <linux/workqueue.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/debugfs.h>
+#include <linux/rtnetlink.h>
+#include <linux/firmware.h>
+#include <linux/log2.h>
+#include <linux/sched.h>
+#include <linux/string_helpers.h>
+#include <linux/sort.h>
+#include <linux/aer.h>
+#include <linux/vmalloc.h>
+#include <linux/notifier.h>
+#include <linux/bitmap.h>
+#include <linux/mutex.h>
+#include <linux/crc32.h>
+#include <net/neighbour.h>
+#include <net/netevent.h>
+#include <asm/uaccess.h>
+
+#include "common.h"
+#include "cxgbtool.h"
+#include "t4_regs.h"
+#include "t4_regs_values.h"
+#include "t4_msg.h"
+#include "t4_tcb.h"
+#include "t4fw_interface.h"
+
+#ifdef CONFIG_CHELSIO_BYPASS
+#include "bypass.h"
+#include "bypass_sysfs.h"
+#endif
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#include "l2t.h"
+#include "cxgb4_ofld.h"
+#endif
+
+#if defined(BOND_SUPPORT)
+#include "../bonding/bonding.h"
+#endif
+#undef DRV_VERSION
+
+#define DRV_VERSION "1.1.1.0-xen"
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#define DRV_DESC "Chelsio T4 Offload Network Driver"
+#else
+#define DRV_DESC "Chelsio T4 Non-Offload Network Driver"
+#endif
+
+/*
+ * Max interrupt hold-off timer value in us.  Queues fall back to this value
+ * under extreme memory pressure so it's largish to give the system time to
+ * recover.
+ */
+#define MAX_SGE_TIMERVAL 200U
+
+enum {
+	/*
+	 * Physical Function provisioning constants.
+	 */
+	PFRES_NVI = 4,			/* # of Virtual Interfaces */
+	PFRES_NETHCTRL = 128,		/* # of EQs used for ETH or CTRL Qs */
+	PFRES_NIQFLINT = 128,		/* # of ingress Qs/w Free List(s)/intr */
+	PFRES_NEQ = 256,		/* # of egress queues */
+	PFRES_NIQ = 0,			/* # of ingress queues */
+	PFRES_TC = 0,			/* PCI-E traffic class */
+	PFRES_NEXACTF = 128,		/* # of exact MPS filters */
+
+	PFRES_R_CAPS = FW_CMD_CAP_PF,
+	PFRES_WX_CAPS = FW_CMD_CAP_PF,
+
+#ifdef CONFIG_PCI_IOV
+	/*
+	 * Virtual Function provisioning constants.  We need two extra Ingress
+	 * Queues with Interrupt capability to serve as the VF's Firmware
+	 * Event Queue and Forwarded Interrupt Queue (when using MSI mode) --
+	 * neither will have Free Lists associated with them).  For each
+	 * Ethernet/Control Egress Queue and for each Free List, we need an
+	 * Egress Context.
+	 */
+	VFRES_NPORTS = 1,		/* # of "ports" per VF */
+	VFRES_NQSETS = 2,		/* # of "Queue Sets" per VF */
+
+	VFRES_NVI = VFRES_NPORTS,	/* # of Virtual Interfaces */
+	VFRES_NETHCTRL = VFRES_NQSETS,	/* # of EQs used for ETH or CTRL Qs */
+	VFRES_NIQFLINT = VFRES_NQSETS+2,/* # of ingress Qs/w Free List(s)/intr */
+	VFRES_NEQ = VFRES_NQSETS*2,	/* # of egress queues */
+	VFRES_NIQ = 0,			/* # of non-fl/int ingress queues */
+	VFRES_TC = 0,			/* PCI-E traffic class */
+	VFRES_NEXACTF = 16,		/* # of exact MPS filters */
+
+	VFRES_R_CAPS = FW_CMD_CAP_DMAQ|FW_CMD_CAP_VF|FW_CMD_CAP_PORT,
+	VFRES_WX_CAPS = FW_CMD_CAP_DMAQ|FW_CMD_CAP_VF,
+#endif
+};
+
+/*
+ * Provide a Port Access Rights Mask for the specified PF/VF.  This is very
+ * static and likely not to be useful in the long run.  We really need to
+ * implement some form of persistent configuration which the firmware
+ * controls.
+ */
+static unsigned int pfvfres_pmask(struct adapter *adapter,
+				  unsigned int pf, unsigned int vf)
+{
+	unsigned int portn, portvec;
+
+	/*
+	 * Give PF's access to all of the ports.
+	 */
+	if (vf == 0)
+		return M_FW_PFVF_CMD_PMASK;
+
+	/*
+	 * For VFs, we'll assign them access to the ports based purely on the
+	 * PF.  We assign active ports in order, wrapping around if there are
+	 * fewer active ports than PFs: e.g. active port[pf % nports].
+	 * Unfortunately the adapter's port_info structs haven't been
+	 * initialized yet so we have to compute this.
+	 */
+	if (adapter->params.nports == 0)
+		return 0;
+
+	portn = pf % adapter->params.nports;
+	portvec = adapter->params.portvec;
+	for (;;) {
+		/*
+		 * Isolate the lowest set bit in the port vector.  If we're at
+		 * the port number that we want, return that as the pmask.
+		 * otherwise mask that bit out of the port vector and
+		 * decrement our port number ...
+		 */
+		unsigned int pmask = portvec ^ (portvec & (portvec-1));
+		if (portn == 0)
+			return pmask;
+		portn--;
+		portvec &= ~pmask;
+	}
+	/*NOTREACHED*/
+}
+
+enum {
+	MAX_TXQ_ENTRIES      = 16384,
+	MAX_CTRL_TXQ_ENTRIES = 1024,
+	MAX_RSPQ_ENTRIES     = 16384,
+	MAX_RX_BUFFERS       = 16384,
+	MIN_TXQ_ENTRIES      = 32,
+	MIN_CTRL_TXQ_ENTRIES = 32,
+	MIN_RSPQ_ENTRIES     = 128,
+	MIN_FL_ENTRIES       = 16
+};
+
+#ifdef CONFIG_PCI_IOV
+enum {
+	VF_MONITOR_PERIOD = 4 * HZ,
+};
+#endif
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Host shadow copy of ingress filter entry.  This is in host native format
+ * and doesn't match the ordering or bit order, etc. of the hardware or the
+ * firmware command.
+ */
+struct filter_entry {
+	/*
+	 * Administrative fields for filter.
+	 */
+	u32 valid:1;		/* filter allocated and valid */
+	u32 locked:1;		/* filter is administratively locked */
+	u32 pending:1;		/* filter action is pending firmware reply */
+	u32 smtidx:8;		/* Source MAC Table index for smac */
+	struct l2t_entry *l2t;	/* Layer Two Table entry for dmac */
+
+	/*
+	 * The filter itself.  Most of this is a straight copy of information
+	 * provided by the extended ioctl().  Some fields are translated to
+	 * internal forms -- for instance the Ingress Queue ID passed in from
+	 * the ioctl() is translated into the Absolute Ingress Queue ID.
+	 */
+	struct ch_filter_specification fs;
+};
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+#define PORT_MASK ((1 << MAX_NPORTS) - 1)
+
+#define DFLT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK | \
+			 NETIF_MSG_TIMER | NETIF_MSG_IFDOWN | NETIF_MSG_IFUP |\
+			 NETIF_MSG_RX_ERR | NETIF_MSG_TX_ERR)
+
+#define EEPROM_MAGIC 0x38E2F10C
+
+#ifndef CHELSIO_T4_DIAGS
+#define CH_DEVICE(devid) { PCI_VDEVICE(CHELSIO, devid), 0 }
+#else
+/*
+ * Include PCI Device IDs for both PF4 and PF0 so our PCI probe() routine is
+ * called for both.  Normally we'll manage the adapter via PF4 but for some
+ * diagnostic purposes we need the use PF0.
+ */
+#define CH_DEVICE(devid) \
+	{ PCI_VDEVICE(CHELSIO, devid), 0 }, \
+	{ PCI_VDEVICE(CHELSIO, (devid & 0xf0ff)), 0 }
+#endif
+
+static struct pci_device_id cxgb4_pci_tbl[] = {
+	CH_DEVICE(0xa000),  /* PE10K FPGA */
+	CH_DEVICE(0x4400),  /* T440-dbg */
+	CH_DEVICE(0x4401),  /* T420-cr */
+	CH_DEVICE(0x4402),  /* T422-cr */
+	CH_DEVICE(0x4403),  /* T440-cr */
+	CH_DEVICE(0x4404),  /* T420-bch */
+	CH_DEVICE(0x4405),  /* T440-bch */
+	CH_DEVICE(0x4406),  /* T440-ch */
+	CH_DEVICE(0x4407),  /* T420-so */
+	CH_DEVICE(0x4408),  /* T420-cx */
+	CH_DEVICE(0x4409),  /* T420-bt */
+	CH_DEVICE(0x440a),  /* T404-bt */
+	CH_DEVICE(0x440b),  /* B420-sr */
+	CH_DEVICE(0x440c),  /* B404-bt */
+	CH_DEVICE(0x440d),  /* T480-cr */
+	CH_DEVICE(0x440e),  /* T440-LP-cr */
+	CH_DEVICE(0x4480),  /* Custom T480-cr */
+	CH_DEVICE(0x4481),  /* Custom T440-cr */
+	{ 0, }
+};
+
+#define FW_FNAME "cxgb4/t4fw.bin"
+#define FW_CFNAME "cxgb4/t4-config.txt"
+
+MODULE_DESCRIPTION(DRV_DESC);
+MODULE_AUTHOR("Chelsio Communications");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_VERSION(DRV_VERSION);
+MODULE_DEVICE_TABLE(pci, cxgb4_pci_tbl);
+MODULE_FIRMWARE(FW_FNAME);
+
+#ifdef CHELSIO_T4_DIAGS
+/*
+ * The master PF is normally PF4 but can be changed to PF0 via the attach_pf0
+ * module parameter.  Note that PF0 does have extra privileges and can access
+ * all the other PFs' VPDs and the entire EEPROM which the other PFs cannot.
+ * This functionality is vital for diagnostics which needs access to the entire
+ * EEPROM.
+ */
+static int attach_pf0;
+
+module_param(attach_pf0, bool, 0644);
+MODULE_PARM_DESC(attach_pf0, "Attach to Master Physical Function 0");
+#endif
+
+/*
+ * Normally we're willing to become the firmware's Master PF but will be happy
+ * if another PF has already become the Master and initialized the adapter.
+ * Setting "force_init" will cause this driver to forcibly establish itself as
+ * the Master PF and initialize the adapter.
+ */
+static uint force_init = 0;
+
+module_param(force_init, uint, 0644);
+MODULE_PARM_DESC(force_init, "Forcibly become Master PF and initialize adapter");
+
+static uint use_bd;
+module_param(use_bd, uint, 0644);
+MODULE_PARM_DESC(use_bd, "Force using the bd interface");
+
+/*
+ * Normally if the firmware we connect to has Configuration File support, we
+ * use that and only fall back to the old Driver-based initialization if the
+ * Configuration File fails for some reason.  If force_old_init is set, then
+ * we'll always use the old Driver-based initialization sequence.
+ */
+static uint force_old_init = 0;
+
+module_param(force_old_init, uint, 0644);
+MODULE_PARM_DESC(force_old_init, "Force old initialization sequence");
+
+/*
+ * Default message set for the interfaces.  This can be changed after the
+ * driver is loaded via "ethtool -s ethX msglvl N".
+ */
+static int dflt_msg_enable = DFLT_MSG_ENABLE;
+
+module_param(dflt_msg_enable, int, 0644);
+MODULE_PARM_DESC(dflt_msg_enable, "Chelsio T4 default message enable bitmap");
+
+/*
+ * The driver uses the best interrupt scheme available on a platform in the
+ * order MSI-X, MSI, legacy INTx interrupts.  This parameter determines which
+ * of these schemes the driver may consider as follows:
+ *
+ * msi = 2: choose from among all three options
+ * msi = 1: only consider MSI and INTx interrupts
+ * msi = 0: force INTx interrupts
+ */
+static int msi = 2;
+
+module_param(msi, int, 0644);
+MODULE_PARM_DESC(msi, "whether to use MSI-X, MSI or INTx");
+
+/*
+ * Queue interrupt hold-off timer values.  Queues default to the first of these
+ * upon creation.
+ */
+static unsigned int intr_holdoff[SGE_NTIMERS - 1] = { 5, 10, 20, 50, 100 };
+
+module_param_array(intr_holdoff, uint, NULL, 0644);
+MODULE_PARM_DESC(intr_holdoff, "values for queue interrupt hold-off timers "
+		 "<int0>[,...,<int4>] in microseconds");
+
+static unsigned int intr_cnt[SGE_NCOUNTERS - 1] = { 4, 8, 16 };
+
+module_param_array(intr_cnt, uint, NULL, 0644);
+MODULE_PARM_DESC(intr_cnt,
+		 "thresholds <int1>[,...,<int3>] for queue interrupt packet "
+		 "counters");
+
+/*
+ * Use skb's for ingress packets.
+ */
+static int rx_useskbs = 1;
+
+module_param(rx_useskbs, int, 0644);
+MODULE_PARM_DESC(rx_useskbs, "one ingress packet per skb Free List buffer");
+
+static int tx_coal = 1;
+
+module_param(tx_coal, int, 0644);
+MODULE_PARM_DESC(tx_coal, "use tx WR coalescing, if set to 2, coalescing "
+		 " will be used most of the time improving packets per "
+		 " second troughput but affecting latency");
+
+/*
+ * Normally we tell the chip to deliver Ingress Packets into our DMA buffers
+ * offset by 2 bytes in order to have the IP headers line up on 4-byte
+ * boundaries.  This is a requirement for many architectures which will throw
+ * a machine check fault if an attempt is made to access one of the 4-byte IP
+ * header fields on a non-4-byte boundary.  And it's a major performance issue
+ * even on some architectures which allow it like some implementations of the
+ * x86 ISA.  However, some architectures don't mind this and for some ery
+ * edge-case performance sensitive applications (like forwarding large volumes
+ * of small packets), setting this DMA offset to 0 will decrease the number of
+ * PCI-E Bus transfers enough to measurably affect performance.
+ */
+static int rx_dma_offset = 2;
+
+module_param(rx_dma_offset, int, 0644);
+MODULE_PARM_DESC(rx_dma_offset, "Offset of RX packets into DMA buffers -- "
+		 " legal values 2 (default) and 0");
+
+static int vf_acls;
+
+#ifdef CONFIG_PCI_IOV
+module_param(vf_acls, bool, 0644);
+MODULE_PARM_DESC(vf_acls, "if set enable virtualization L2 ACL enforcement");
+
+static unsigned int num_vf[4];
+
+module_param_array(num_vf, uint, NULL, 0644);
+MODULE_PARM_DESC(num_vf, "number of VFs for each of PFs 0-3");
+#endif
+
+/*
+ * If fw_attach is 0 the driver will not connect to FW.  This is intended only
+ * for FW debugging.  fw_attach must be 1 for normal operation.
+ */
+static int fw_attach = 1;
+
+module_param(fw_attach, int, 0644);
+MODULE_PARM_DESC(fw_attach, "whether to connect to FW");
+
+int dbfifo_int_thresh = 10; /* 10 == 640 entry threshold */
+module_param(dbfifo_int_thresh, int, 0644);
+MODULE_PARM_DESC(dbfifo_int_thresh, "doorbell fifo interrupt threshold");
+
+int dbfifo_drain_delay = 1000; /* usecs to sleep while draining the dbfifo */
+module_param(dbfifo_drain_delay, int, 0644);
+MODULE_PARM_DESC(dbfifo_drain_delay, 
+		 "usecs to sleep while draining the dbfifo");
+
+static int allow_nonroot_filters = 0;
+module_param(allow_nonroot_filters, int, 0644);
+MODULE_PARM_DESC(allow_nonroot_filters,
+		 "Allow nonroot access to filters (default = 0)");
+
+/*
+ * TX Queue select used to determine what algorithm to use for selecting TX
+ * queue. Select between the kernel provided function (select_queue=0) or user
+ * cxgb_select_queue function (select_queue=1)
+ *
+ * Default: select_queue=0
+ */
+static int select_queue = 0;
+module_param(select_queue, int, 0644);
+MODULE_PARM_DESC(select_queue, "Select between kernel provided method of "
+		 "selecting or driver method of selecting TX queue. Default "
+		 "is kernel method.");
+
+/*
+ * The filter TCAM has a fixed portion and a variable portion.  The fixed
+ * portion can match on source/destination IP IPv4/IPv6 addresses and TCP/UDP
+ * ports.  The variable portion is 36 bits which can include things like Exact
+ * Match MAC Index (9 bits), Ether Type (16 bits), IP Protocol (8 bits),
+ * [Inner] VLAN Tag (17 bits), etc. which, if all were somehow selected, would
+ * far exceed the 36-bit budget for this "compressed" header portion of the
+ * filter.  Thus, we have a scarce resource which must be carefully managed.
+ *
+ * By default we set this up to mostly match the set of filter matching
+ * capabilities of T3 but with accommodations for some of T4's more
+ * interesting features:
+ *
+ *   { IP Fragment (1), MPS Match Type (3), IP Protocol (8),
+ *     VNIC ID (17), Port (3), FCoE (1) }
+ */
+enum {
+	TP_VLAN_PRI_MAP_DEFAULT = HW_TPL_FR_MT_PR_OV_P_FC,
+	TP_VLAN_PRI_MAP_FIRST = S_FCOE,
+	TP_VLAN_PRI_MAP_LAST = S_FRAGMENTATION,
+};
+
+static unsigned int tp_vlan_pri_map = TP_VLAN_PRI_MAP_DEFAULT;
+
+module_param(tp_vlan_pri_map, uint, 0644);
+MODULE_PARM_DESC(tp_vlan_pri_map, "global compressed filter configuration");
+
+static struct dentry *cxgb4_debugfs_root;
+static struct proc_dir_entry *cxgb4_proc_root;
+
+static LIST_HEAD(adapter_list);
+static DEFINE_MUTEX(uld_mutex);
+static struct cxgb4_uld_info ulds[CXGB4_ULD_MAX];
+static const char *uld_str[] = { "RDMA", "iSCSI", "TOE" };
+
+static struct workqueue_struct *workq;
+
+/**
+ *	link_report - show link status and link speed/duplex
+ *	@dev: the port whose settings are to be reported
+ *
+ *	Shows the link status, speed, and duplex of a port.
+ */
+static void link_report(struct net_device *dev)
+{
+	if (!netif_carrier_ok(dev))
+		printk(KERN_INFO "%s: link down\n", dev->name);
+	else {
+		static const char *fc[] = { "no", "Rx", "Tx", "Tx/Rx" };
+
+		const char *s = "10Mbps";
+		const struct port_info *p = netdev_priv(dev);
+
+		switch (p->link_cfg.speed) {
+		case SPEED_10000:
+			s = "10Gbps";
+			break;
+		case SPEED_1000:
+			s = "1000Mbps";
+			break;
+		case SPEED_100:
+			s = "100Mbps";
+			break;
+		}
+
+		printk(KERN_INFO "%s: link up, %s, full-duplex, %s PAUSE\n",
+		       dev->name, s, fc[p->link_cfg.fc]);
+	}
+}
+
+/**
+ *	t4_os_link_changed - handle link status changes
+ *	@adapter: the adapter associated with the link change
+ *	@port_id: the port index whose link status has changed
+ *	@link_stat: the new status of the link
+ *
+ *	This is the OS-dependent handler for link status changes.  The OS
+ *	neutral handler takes care of most of the processing for these events,
+ *	then calls this handler for any OS-specific processing.
+ */
+void t4_os_link_changed(struct adapter *adapter, int port_id, int link_stat)
+{
+	struct net_device *dev = adapter->port[port_id];
+
+	/* Skip changes from disabled ports. */
+	if (netif_running(dev) && link_stat != netif_carrier_ok(dev)) {
+		if (link_stat)
+			netif_carrier_on(dev);
+		else
+			netif_carrier_off(dev);
+
+		link_report(dev);
+	}
+}
+
+/**
+ *	t4_os_portmod_changed - handle port module changes
+ *	@adap: the adapter associated with the module change
+ *	@port_id: the port index whose module status has changed
+ *
+ *	This is the OS-dependent handler for port module changes.  It is
+ *	invoked when a port module is removed or inserted for any OS-specific
+ *	processing.
+ */
+void t4_os_portmod_changed(const struct adapter *adap, int port_id)
+{
+	static const char *mod_str[] = {
+		NULL, "LR", "SR", "ER", "TWINAX", "active TWINAX", "LRM"
+	};
+
+	const struct net_device *dev = adap->port[port_id];
+	const struct port_info *pi = netdev_priv(dev);
+
+	if (pi->mod_type == FW_PORT_MOD_TYPE_NONE)
+		printk(KERN_INFO "%s: port module unplugged\n", dev->name);
+	else if (pi->mod_type < ARRAY_SIZE(mod_str))
+		printk(KERN_INFO "%s: %s port module inserted\n", dev->name,
+		       mod_str[pi->mod_type]);
+	else if (pi->mod_type == FW_PORT_MOD_TYPE_NOTSUPPORTED)
+		printk(KERN_INFO "%s: unsupported optical port module "
+		 	"inserted\n", dev->name);
+	else if (pi->mod_type == FW_PORT_MOD_TYPE_UNKNOWN)
+		printk(KERN_INFO "%s: unknown port module inserted, forcing "
+		       "TWINAX\n", dev->name);
+	else if (pi->mod_type == FW_PORT_MOD_TYPE_ERROR)
+		printk(KERN_INFO "%s: transceiver module error\n", dev->name);
+	else
+		printk(KERN_INFO "%s: unknown module type %d inserted\n",
+		       dev->name, pi->mod_type);
+}
+
+/*
+ * Configure the exact and hash address filters to handle a port's multicast
+ * and secondary unicast MAC addresses.
+ */
+static int set_addr_filters(const struct net_device *dev, bool sleep)
+{
+	u64 mhash = 0;
+	u64 uhash = 0;
+	bool free = true;
+	unsigned int offset, naddr;
+	const u8 *addr[7];
+	int ret;
+	const struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	/* first do the secondary unicast addresses */
+	for (offset = 0; ; offset += naddr) {
+		naddr = collect_netdev_uc_list_addrs(dev, addr, offset,
+						     ARRAY_SIZE(addr));
+		if (naddr == 0)
+			break;
+
+		ret = t4_alloc_mac_filt(adapter, adapter->mbox, pi->viid, free,
+					naddr, addr, NULL, &uhash, sleep);
+		if (ret < 0)
+			return ret;
+		free = false;
+	}
+
+	/* next set up the multicast addresses */
+	for (offset = 0; ; offset += naddr) {
+		naddr = collect_netdev_mc_list_addrs(dev, addr, offset,
+						     ARRAY_SIZE(addr));
+		if (naddr == 0)
+			break;
+
+		ret = t4_alloc_mac_filt(adapter, adapter->mbox, pi->viid, free,
+					naddr, addr, NULL, &mhash, sleep);
+		if (ret < 0)
+			return ret;
+		free = false;
+	}
+
+	return t4_set_addr_hash(adapter, adapter->mbox, pi->viid, uhash != 0,
+				uhash | mhash, sleep);
+}
+
+/*
+ * Set Rx properties of a port, such as promiscruity, address filters, and MTU.
+ * If @mtu is -1 it is left unchanged.
+ */
+static int set_rxmode(struct net_device *dev, int mtu, bool sleep_ok)
+{
+	int ret;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	ret = set_addr_filters(dev, sleep_ok);
+	if (ret == 0)
+		ret = t4_set_rxmode(adapter, adapter->mbox, pi->viid, mtu,
+				    (dev->flags & IFF_PROMISC) ? 1 : 0,
+				    (dev->flags & IFF_ALLMULTI) ? 1 : 0,
+				    1, -1, sleep_ok);
+	return ret;
+}
+
+static void cxgb_set_rxmode(struct net_device *dev)
+{
+	/* unfortunately we can't return errors to the stack */
+	set_rxmode(dev, -1, false);
+}
+
+/**
+ *	link_start - enable a port
+ *	@dev: the port to enable
+ *
+ *	Performs the MAC and PHY actions needed to enable a port.
+ */
+static int link_start(struct net_device *dev)
+{
+	int ret;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	/*
+	 * We do not set address filters and promiscuity here, the stack does
+	 * that step explicitly.
+	 */
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+	ret = t4_set_rxmode(adapter, adapter->mbox, pi->viid, dev->mtu, 1,
+			    -1, -1, -1, true);
+#else
+	ret = t4_set_rxmode(adapter, adapter->mbox, pi->viid, dev->mtu, -1,
+			    -1, -1, -1, true);
+#endif
+	if (ret == 0) {
+		ret = t4_change_mac(adapter, adapter->mbox, pi->viid,
+				    pi->xact_addr_filt, dev->dev_addr, true,
+				    true);
+		if (ret >= 0) {
+			pi->xact_addr_filt = ret;
+			ret = 0;
+		}
+	}
+	if (ret == 0)
+		ret = t4_link_start(adapter, adapter->mbox, pi->tx_chan,
+				    &pi->link_cfg);
+	if (ret == 0)
+		ret = t4_enable_vi(adapter, adapter->mbox, pi->viid, true,
+				   true);
+	return ret;
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Clear a filter and release any of its resources that we own.  This also
+ * clears the filter's "pending" status.
+ */
+static void clear_filter(struct adapter *adap, struct filter_entry *f)
+{
+	/*
+	 * If the new or old filter have loopback rewriteing rules then we'll
+	 * need to free any existing Layer Two Table (L2T) entries of the old
+	 * filter rule.  The firmware will handle freeing up any Source MAC
+	 * Table (SMT) entries used for rewriting Source MAC Addresses in
+	 * loopback rules.
+	 */
+	if (f->l2t)
+		cxgb4_l2t_release(f->l2t);
+
+	/*
+	 * The zeroing of the filter rule below clears the filter valid,
+	 * pending, locked flags, l2t pointer, etc. so it's all we need for
+	 * this operation.
+	 */
+	memset(f, 0, sizeof(*f));
+}
+
+/*
+ * Handle a filter write/deletion reply.
+ */
+static void filter_rpl(struct adapter *adap, const struct cpl_set_tcb_rpl *rpl)
+{
+	unsigned int idx = GET_TID(rpl);
+
+	if (idx >= adap->tids.ftid_base &&
+	    (idx -= adap->tids.ftid_base) <
+			(adap->tids.nftids + adap->tids.nsftids)) {
+		unsigned int ret = G_COOKIE(rpl->cookie);
+		struct filter_entry *f = &adap->tids.ftid_tab[idx];
+
+		if (ret == FW_FILTER_WR_FLT_DELETED) {
+			/*
+			 * Clear the filter when we get confirmation from the
+			 * hardware that the filter has been deleted.
+			 */
+			clear_filter(adap, f);
+		} else if (ret == FW_FILTER_WR_SMT_TBL_FULL) {
+			CH_ERR(adap, "filter %u setup failed due to full SMT\n",
+			       idx);
+			clear_filter(adap, f);
+		} else if (ret == FW_FILTER_WR_FLT_ADDED) {
+			f->smtidx = (be64_to_cpu(rpl->oldval) >> 24) & 0xff;
+			f->pending = 0;  /* asynchronous setup completed */
+			f->valid = 1;
+		} else {
+			/*
+			 * Something went wrong.  Issue a warning about the
+			 * problem and clear everything out.
+			 */
+			CH_ERR(adap, "filter %u setup failed with error %u\n",
+			       idx, ret);
+			clear_filter(adap, f);
+		}
+	}
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/*
+ * Response queue handler for the FW event queue.
+ */
+static int fwevtq_handler(struct sge_rspq *q, const __be64 *rsp,
+			  const struct pkt_gl *gl)
+{
+	u8 opcode = ((const struct rss_header *)rsp)->opcode;
+
+	rsp++;                                          /* skip RSS header */
+
+	if (likely(opcode == CPL_SGE_EGR_UPDATE)) {
+		const struct cpl_sge_egr_update *p = (void *)rsp;
+		unsigned int qid = G_EGR_QID(ntohl(p->opcode_qid));
+		struct sge_txq *txq;
+
+		txq = q->adapter->sge.egr_map[qid - q->adapter->sge.egr_start];
+		if ((u8 *)txq < (u8 *)q->adapter->sge.ofldtxq) {
+			struct sge_eth_txq *eq;
+
+			eq = container_of(txq, struct sge_eth_txq, q);
+			t4_sge_coalesce_handler(q->adapter, eq);	
+		} else {
+			struct sge_ofld_txq *oq;
+
+			txq->restarts++;
+			oq = container_of(txq, struct sge_ofld_txq, q);
+			tasklet_schedule(&oq->qresume_tsk);
+		}
+	} else if (opcode == CPL_FW6_MSG || opcode == CPL_FW4_MSG) {
+		const struct cpl_fw6_msg *p = (void *)rsp;
+
+		if (p->type == FW6_TYPE_CMD_RPL)
+			t4_handle_fw_rpl(q->adapter, p->data);
+	}
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	else if (opcode == CPL_L2T_WRITE_RPL) {
+		const struct cpl_l2t_write_rpl *p = (void *)rsp;
+
+		do_l2t_write_rpl(q->adapter, p);
+	} else if (opcode == CPL_SET_TCB_RPL) {
+		const struct cpl_set_tcb_rpl *p = (void *)rsp;
+
+		filter_rpl(q->adapter, p);
+	}
+#endif
+	else
+		CH_ERR(q->adapter, "unexpected CPL %#x on FW event queue\n",
+		       opcode);
+	return 0;
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/**
+ *	uldrx_handler - response queue handler for ULD queues
+ *	@q: the response queue that received the packet
+ *	@rsp: the response queue descriptor holding the offload message
+ *	@gl: the gather list of packet fragments
+ *
+ *	Deliver an ingress offload packet to a ULD.  All processing is done by
+ *	the ULD, we just maintain statistics.
+ */
+static int uldrx_handler(struct sge_rspq *q, const __be64 *rsp,
+			 const struct pkt_gl *gl)
+{
+	struct sge_ofld_rxq *rxq = container_of(q, struct sge_ofld_rxq, rspq);
+
+	if (ulds[q->uld].rx_handler(q->adapter->uld_handle[q->uld], rsp, gl)) {
+		rxq->stats.nomem++;
+		return -1;
+	}
+	if (gl == NULL)
+		rxq->stats.imm++;
+	else if (gl == CXGB4_MSG_AN)
+		rxq->stats.an++;
+	else
+		rxq->stats.pkts++;
+	return 0;
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+static void cxgb_disable_msi(struct adapter *adapter)
+{
+	if (adapter->flags & USING_MSIX) {
+		pci_disable_msix(adapter->pdev);
+		adapter->flags &= ~USING_MSIX;
+	} else if (adapter->flags & USING_MSI) {
+		pci_disable_msi(adapter->pdev);
+		adapter->flags &= ~USING_MSI;
+	}
+}
+
+/*
+ * Interrupt handler for non-data events used with MSI-X.
+ */
+static irqreturn_t t4_nondata_intr(int irq, void *cookie)
+{
+	struct adapter *adap = cookie;
+
+	u32 v = t4_read_reg(adap, MYPF_REG(A_PL_PF_INT_CAUSE));
+	if (v & F_PFSW) {
+		adap->swintr = 1;
+		t4_write_reg(adap, MYPF_REG(A_PL_PF_INT_CAUSE), v);
+	}
+	if (adap->flags & MASTER_PF)
+		t4_slow_intr_handler(adap);
+	return IRQ_HANDLED;
+}
+
+/*
+ * Name the MSI-X interrupts.
+ */
+static void name_msix_vecs(struct adapter *adap)
+{
+	int i, j, msi_idx = 2, n = sizeof(adap->msix_info[0].desc);
+
+	/* non-data interrupts */
+	snprintf(adap->msix_info[0].desc, n, "%s", adap->name);
+
+	/* FW events */
+	snprintf(adap->msix_info[1].desc, n, "%s-FWeventq", adap->name);
+
+	/* Ethernet queues */
+	for_each_port(adap, j) {
+		struct net_device *d = adap->port[j];
+		const struct port_info *pi = netdev_priv(d);
+
+		for (i = 0; i < pi->nqsets; i++, msi_idx++)
+			snprintf(adap->msix_info[msi_idx].desc, n,
+				 "%s (queue %d)", d->name, i);
+	}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	/* offload queues */
+	for_each_ofldrxq(&adap->sge, i)
+		snprintf(adap->msix_info[msi_idx++].desc, n, "%s-ofld%d",
+			 adap->name, i);
+	for_each_rdmarxq(&adap->sge, i)
+		snprintf(adap->msix_info[msi_idx++].desc, n, "%s-rdma%d",
+			 adap->name, i);
+	for_each_iscsirxq(&adap->sge, i)
+		snprintf(adap->msix_info[msi_idx++].desc, n, "%s-iSCSI%d",
+			 adap->name, i);
+#endif
+}
+
+static int request_msix_queue_irqs(struct adapter *adap)
+{
+	struct sge *s = &adap->sge;
+	int err, ethqidx, msi = 2;
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	int ofldqidx = 0, rdmaqidx = 0, iscsiqidx = 0;
+#endif
+
+	err = request_irq(adap->msix_info[1].vec, t4_sge_intr_msix, 0,
+			  adap->msix_info[1].desc, &s->fw_evtq);
+	if (err)
+		return err;
+
+	for_each_ethrxq(s, ethqidx) {
+		err = request_irq(adap->msix_info[msi].vec, t4_sge_intr_msix, 0,
+				  adap->msix_info[msi].desc,
+				  &s->ethrxq[ethqidx].rspq);
+		if (err)
+			goto unwind;
+		msi++;
+	}
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	for_each_ofldrxq(s, ofldqidx) {
+		err = request_irq(adap->msix_info[msi].vec, t4_sge_intr_msix, 0,
+				  adap->msix_info[msi].desc,
+				  &s->ofldrxq[ofldqidx].rspq);
+		if (err)
+			goto unwind;
+		msi++;
+	}
+	for_each_rdmarxq(s, rdmaqidx) {
+		err = request_irq(adap->msix_info[msi].vec, t4_sge_intr_msix, 0,
+				  adap->msix_info[msi].desc,
+				  &s->rdmarxq[rdmaqidx].rspq);
+		if (err)
+			goto unwind;
+		msi++;
+	}
+	for_each_iscsirxq(s, iscsiqidx) {
+		err = request_irq(adap->msix_info[msi].vec, t4_sge_intr_msix, 0,
+				  adap->msix_info[msi].desc,
+				  &s->iscsirxq[iscsiqidx].rspq);
+		if (err)
+			goto unwind;
+		msi++;
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+	return 0;
+
+unwind:
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	while (--iscsiqidx >= 0)
+		free_irq(adap->msix_info[--msi].vec,
+			 &s->iscsirxq[iscsiqidx].rspq);
+	while (--rdmaqidx >= 0)
+		free_irq(adap->msix_info[--msi].vec,
+			 &s->rdmarxq[rdmaqidx].rspq);
+	while (--ofldqidx >= 0)
+		free_irq(adap->msix_info[--msi].vec,
+			 &s->ofldrxq[ofldqidx].rspq);
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+	while (--ethqidx >= 0)
+		free_irq(adap->msix_info[--msi].vec, &s->ethrxq[ethqidx].rspq);
+	free_irq(adap->msix_info[1].vec, &s->fw_evtq);
+	return err;
+}
+
+static void free_msix_queue_irqs(struct adapter *adap)
+{
+	int i, msi = 2;
+	struct sge *s = &adap->sge;
+
+	free_irq(adap->msix_info[1].vec, &s->fw_evtq);
+	for_each_ethrxq(s, i)
+		free_irq(adap->msix_info[msi++].vec, &s->ethrxq[i].rspq);
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	for_each_ofldrxq(s, i)
+		free_irq(adap->msix_info[msi++].vec, &s->ofldrxq[i].rspq);
+	for_each_rdmarxq(s, i)
+		free_irq(adap->msix_info[msi++].vec, &s->rdmarxq[i].rspq);
+	for_each_iscsirxq(s, i)
+		free_irq(adap->msix_info[msi++].vec, &s->iscsirxq[i].rspq);
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+}
+
+/**
+ *	setup_rss - configure RSS
+ *	@adapter: the adapter
+ *
+ *	Sets up RSS to distribute packets to multiple receive queues.  We
+ *	configure the RSS CPU lookup table to distribute to the number of HW
+ *	receive queues, and the response queue lookup table to narrow that
+ *	down to the response queues actually configured for each port.
+ *	We always configure the RSS mapping for all ports since the mapping
+ *	table has plenty of entries.
+ */
+static int setup_rss(struct adapter *adapter)
+{
+	int pidx;
+
+	for_each_port(adapter, pidx) {
+		struct port_info *pi = adap2pinfo(adapter, pidx);
+		struct sge_eth_rxq *rxq = &adapter->sge.ethrxq[pi->first_qset];
+		u16 rss[MAX_INGQ];
+		int qs, err;
+
+		for (qs = 0; qs < pi->nqsets; qs++)
+			rss[qs] = rxq[qs].rspq.abs_id;
+
+		err = t4_config_rss_range(adapter, adapter->mbox, pi->viid,
+					  0, pi->rss_size, rss, pi->nqsets);
+		/*
+		 * If Tunnel All Lookup isn't specified in the global RSS
+		 * Configuration, then we need to specify a default Ingress
+		 * Queue for any ingress packets which aren't hashed.  We'll
+		 * use our first ingress queue ...
+		 */
+		if (!err)
+			err = t4_config_vi_rss(adapter, adapter->mbox, pi->viid,
+					F_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN |
+					F_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN |
+					F_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN |
+					F_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN |
+					F_FW_RSS_VI_CONFIG_CMD_UDPEN,
+					rss[0]);
+		if (err)
+			return err;
+	}
+	return 0;
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Return the channel of the ingress queue with the given qid.
+ */
+static unsigned int rxq_to_chan(const struct sge *p, unsigned int qid)
+{
+	qid -= p->ingr_start;
+	return netdev2pinfo(p->ingr_map[qid]->netdev)->tx_chan;
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/*
+ * Wait until all NAPI handlers are descheduled.
+ */
+static void quiesce_rx(struct adapter *adap)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(adap->sge.ingr_map); i++) {
+		struct sge_rspq *q = adap->sge.ingr_map[i];
+
+		if (q && q->handler)
+			napi_disable(&q->napi);
+	}
+}
+
+/*
+ * Enable NAPI scheduling and interrupt generation for all Rx queues.
+ */
+static void enable_rx(struct adapter *adap)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(adap->sge.ingr_map); i++) {
+		struct sge_rspq *q = adap->sge.ingr_map[i];
+
+		if (!q)
+			continue;
+		if (q->handler)
+			napi_enable(&q->napi);
+		/* 0-increment GTS to start the timer and enable interrupts */
+		t4_write_reg(adap, MYPF_REG(A_SGE_PF_GTS),
+			     V_SEINTARM(q->intr_params) |
+			     V_INGRESSQID(q->cntxt_id));
+	}
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+static int alloc_ofld_rxqs(struct adapter *adap, struct sge_ofld_rxq *q,
+			   unsigned int nq, unsigned int per_chan, int msi_idx,
+			   u16 *ids)
+{
+	int i, err;
+
+	for (i = 0; i < nq; i++, q++) {
+		if (msi_idx > 0)
+			msi_idx++;
+		err = t4_sge_alloc_rxq(adap, &q->rspq, false,
+				       adap->port[i / per_chan],
+				       msi_idx, &q->fl, uldrx_handler, 0);
+		if (err)
+			return err;
+		memset(&q->stats, 0, sizeof(q->stats));
+		if (ids)
+			ids[i] = q->rspq.abs_id;
+	}
+	return 0;
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/**
+ *	setup_sge_queues - configure SGE Tx/Rx/response queues
+ *	@adap: the adapter
+ *
+ *	Determines how many sets of SGE queues to use and initializes them.
+ *	We support multiple queue sets per port if we have MSI-X, otherwise
+ *	just one queue set per port.
+ */
+static int setup_sge_queues(struct adapter *adap)
+{
+	int err, msi_idx, i, j;
+	struct sge *s = &adap->sge;
+
+	bitmap_zero(s->starving_fl, MAX_EGRQ);
+	bitmap_zero(s->txq_maperr, MAX_EGRQ);
+
+	if (adap->flags & USING_MSIX)
+		msi_idx = 1;         /* vector 0 is for non-queue interrupts */
+	else {
+		err = t4_sge_alloc_rxq(adap, &s->intrq, false, adap->port[0], 0,
+				       NULL, NULL, -1);
+		if (err)
+			return err;
+		msi_idx = -((int)s->intrq.abs_id + 1);
+	}
+
+	err = t4_sge_alloc_rxq(adap, &s->fw_evtq, true, adap->port[0],
+			       msi_idx, NULL, fwevtq_handler, -1);
+	if (err) {
+freeout:	t4_free_sge_resources(adap);
+		return err;
+	}
+
+	for_each_port(adap, i) {
+		struct net_device *dev = adap->port[i];
+		struct port_info *pi = netdev_priv(dev);
+		struct sge_eth_rxq *q = &s->ethrxq[pi->first_qset];
+		struct sge_eth_txq *t = &s->ethtxq[pi->first_qset];
+
+		for (j = 0; j < pi->nqsets; j++, q++) {
+			if (msi_idx > 0)
+				msi_idx++;
+			err = t4_sge_alloc_rxq(adap, &q->rspq, false, dev,
+					       msi_idx, &q->fl,
+					       t4_ethrx_handler,
+					       1 << pi->tx_chan);
+			if (err)
+				goto freeout;
+			q->rspq.idx = j;
+			memset(&q->stats, 0, sizeof(q->stats));
+		}
+		for (j = 0; j < pi->nqsets; j++, t++) {
+			err = t4_sge_alloc_eth_txq(adap, t, dev,
+					netdev_get_tx_queue(dev, j),
+					s->fw_evtq.cntxt_id);
+			if (err)
+				goto freeout;
+		}
+	}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	j = s->ofldqsets / adap->params.nports; /* ofld queues per channel */
+	for_each_ofldrxq(s, i) {
+		err = t4_sge_alloc_ofld_txq(adap, &s->ofldtxq[i],
+					    adap->port[i / j],
+					    s->fw_evtq.cntxt_id);
+		if (err)
+			goto freeout;
+	}
+
+#define ALLOC_OFLD_RXQS(firstq, nq, per_chan, ids) do { \
+	err = alloc_ofld_rxqs(adap, firstq, nq, per_chan, msi_idx, ids); \
+	if (err) \
+		goto freeout; \
+	if (msi_idx > 0) \
+		msi_idx += nq; \
+} while (0)
+
+	ALLOC_OFLD_RXQS(s->ofldrxq, s->ofldqsets, j, s->ofld_rxq);
+	ALLOC_OFLD_RXQS(s->rdmarxq, s->rdmaqs, 1, s->rdma_rxq);
+	ALLOC_OFLD_RXQS(s->iscsirxq, s->niscsiq, 1, s->iscsi_rxq);
+
+#undef ALLOC_OFLD_RXQS
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+	for_each_port(adap, i) {
+		/*
+		 * Note that ->rdmarxq[i].rspq.cntxt_id below is 0 if we don't
+		 * have RDMA queues, and that's the right value.
+		 */
+		err = t4_sge_alloc_ctrl_txq(adap, &s->ctrlq[i], adap->port[i],
+					    s->fw_evtq.cntxt_id,
+					    s->rdmarxq[i].rspq.cntxt_id);
+		if (err)
+			goto freeout;
+
+	}
+
+	t4_write_reg(adap, A_MPS_TRC_RSS_CONTROL,
+		     V_RSSCONTROL(netdev2pinfo(adap->port[0])->tx_chan) |
+		     V_QUEUENUMBER(s->ethrxq[0].rspq.abs_id));
+	return 0;
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+static int setup_loopback(struct adapter *adap)
+{
+	int i, err;
+	u8 mac0[] = { 0, 0, 0, 0, 0, 0 };
+
+	for_each_port(adap, i) {
+		err = t4_change_mac(adap, adap->mbox, adap2pinfo(adap, i)->viid,
+				    -1, mac0, true, false);
+		if (err)
+			return err;
+	}
+	return 0;
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/*
+ * Returns 0 if new FW was successfully loaded, a positive errno if a load was
+ * started but failed, and a negative errno if flash load couldn't start, an
+ * incompatible or lower revision firmware version was found, etc.
+ */
+static int upgrade_fw(struct adapter *adap)
+{
+	int ret;
+	u32 fw_ver;
+	const struct fw_hdr *fw_hdr;
+	const struct firmware *fw;
+	struct device *dev = adap->pdev_dev;
+
+	ret = request_firmware(&fw, FW_FNAME, dev);
+	if (ret < 0) {
+		dev_err(dev, "unable to load firmware image " FW_FNAME
+			", error %d\n", ret);
+		return ret;
+	}
+
+	fw_hdr = (const struct fw_hdr *)fw->data;
+	fw_ver = ntohl(fw_hdr->fw_ver);
+	if (G_FW_HDR_FW_VER_MAJOR(fw_ver) != FW_VERSION_MAJOR) {
+		ret = -EINVAL;              /* wrong major version, won't do */
+		goto out;
+	}
+
+	/*
+	 * If the flash FW is unusable or we found something newer, load it.
+	 */
+	if (G_FW_HDR_FW_VER_MAJOR(adap->params.fw_vers) != FW_VERSION_MAJOR ||
+	    fw_ver > adap->params.fw_vers) {
+		dev_info(dev, "upgrading firmware ...\n");
+		ret = t4_fw_upgrade(adap, adap->mbox, fw->data, fw->size,
+				    /*force=*/false);
+		if (!ret)
+			dev_info(dev, "firmware successfully upgraded to "
+				 FW_FNAME " (%d.%d.%d.%d)\n",
+				 G_FW_HDR_FW_VER_MAJOR(fw_ver),
+				 G_FW_HDR_FW_VER_MINOR(fw_ver),
+				 G_FW_HDR_FW_VER_MICRO(fw_ver),
+				 G_FW_HDR_FW_VER_BUILD(fw_ver));
+		else
+			dev_err(dev, "firmware upgrade failed! err=%d\n", -ret);
+	} else {
+		/*
+		 * Tell our caller that we didn't upgrade the firmware.
+		 */
+		ret = -EINVAL;
+	}
+
+out:
+	release_firmware(fw);
+	return ret;
+}
+
+/*
+ * Allocate a chunk of memory using kmalloc or, if that fails, vmalloc.
+ * The allocated memory is cleared.
+ */
+void *t4_alloc_mem(size_t size)
+{
+	void *p = kmalloc(size, GFP_KERNEL);
+
+	if (!p)
+		p = vmalloc(size);
+	if (p)
+		memset(p, 0, size);
+	return p;
+}
+
+/*
+ * Free memory allocated through alloc_mem().
+ */
+void t4_free_mem(void *addr)
+{
+	if (is_vmalloc_addr(addr))
+		vfree(addr);
+	else
+		kfree(addr);
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Send a Work Request to write the filter at a specified index.  We construct
+ * a Firmware Filter Work Request to have the work done and put the indicated
+ * filter into "pending" mode which will prevent any further actions against
+ * it till we get a reply from the firmware on the completion status of the
+ * request.
+ */
+static int set_filter_wr(struct adapter *adapter, int fidx)
+{
+	struct filter_entry *f = &adapter->tids.ftid_tab[fidx];
+	struct sk_buff *skb;
+	struct fw_filter_wr *fwr;
+	unsigned int ftid;
+
+	/*
+	 * If the new filter requires loopback Destination MAC and/or VLAN
+	 * rewriting then we need to allocate a Layer 2 Table (L2T) entry for
+	 * the filter.
+	 */
+	if (f->fs.newdmac || f->fs.newvlan) {
+		/* allocate L2T entry for new filter */
+		f->l2t = t4_l2t_alloc_switching(adapter->l2t);
+		if (f->l2t == NULL)
+			return -EAGAIN;
+		if (t4_l2t_set_switching(adapter, f->l2t, f->fs.vlan,
+					 f->fs.eport, f->fs.dmac)) {
+			cxgb4_l2t_release(f->l2t);
+			f->l2t = NULL;
+			return -ENOMEM;
+		}
+	}
+
+	ftid = adapter->tids.ftid_base + fidx;
+
+	skb = alloc_skb(sizeof *fwr, GFP_KERNEL | __GFP_NOFAIL);
+	fwr = (struct fw_filter_wr *)__skb_put(skb, sizeof *fwr);
+	memset(fwr, 0, sizeof *fwr);
+
+	/*
+	 * It would be nice to put most of the following in t4_hw.c but most
+	 * of the work is translating the cxgbtool ch_filter_specification
+	 * into the Work Request and the definition of that structure is
+	 * currently in cxgbtool.h which isn't appropriate to pull into the
+	 * common code.  We may eventually try to come up with a more neutral
+	 * filter specification structure but for now it's easiest to simply
+	 * put this fairly direct code in line ...
+	 */
+	fwr->op_pkd = htonl(V_FW_WR_OP(FW_FILTER_WR));
+	fwr->len16_pkd = htonl(V_FW_WR_LEN16(sizeof *fwr/16));
+	fwr->tid_to_iq =
+		htonl(V_FW_FILTER_WR_TID(ftid) |
+		      V_FW_FILTER_WR_RQTYPE(f->fs.type) |
+		      V_FW_FILTER_WR_NOREPLY(0) |
+		      V_FW_FILTER_WR_IQ(f->fs.iq));
+	fwr->del_filter_to_l2tix =
+		htonl(V_FW_FILTER_WR_RPTTID(f->fs.rpttid) |
+		      V_FW_FILTER_WR_DROP(f->fs.action == FILTER_DROP) |
+		      V_FW_FILTER_WR_DIRSTEER(f->fs.dirsteer) |
+		      V_FW_FILTER_WR_MASKHASH(f->fs.maskhash) |
+		      V_FW_FILTER_WR_DIRSTEERHASH(f->fs.dirsteerhash) |
+		      V_FW_FILTER_WR_LPBK(f->fs.action == FILTER_SWITCH) |
+		      V_FW_FILTER_WR_DMAC(f->fs.newdmac) |
+		      V_FW_FILTER_WR_SMAC(f->fs.newsmac) |
+		      V_FW_FILTER_WR_INSVLAN(f->fs.newvlan == VLAN_INSERT ||
+					     f->fs.newvlan == VLAN_REWRITE) |
+		      V_FW_FILTER_WR_RMVLAN(f->fs.newvlan == VLAN_REMOVE ||
+					    f->fs.newvlan == VLAN_REWRITE) |
+		      V_FW_FILTER_WR_HITCNTS(f->fs.hitcnts) |
+		      V_FW_FILTER_WR_TXCHAN(f->fs.eport) |
+		      V_FW_FILTER_WR_PRIO(f->fs.prio) |
+		      V_FW_FILTER_WR_L2TIX(f->l2t ? f->l2t->idx : 0));
+	fwr->ethtype = htons(f->fs.val.ethtype);
+	fwr->ethtypem = htons(f->fs.mask.ethtype);
+	fwr->frag_to_ovlan_vldm =
+		     (V_FW_FILTER_WR_FRAG(f->fs.val.frag) |
+		      V_FW_FILTER_WR_FRAGM(f->fs.mask.frag) |
+		      V_FW_FILTER_WR_IVLAN_VLD(f->fs.val.ivlan_vld) |
+		      V_FW_FILTER_WR_OVLAN_VLD(f->fs.val.ovlan_vld) |
+		      V_FW_FILTER_WR_IVLAN_VLDM(f->fs.mask.ivlan_vld) |
+		      V_FW_FILTER_WR_OVLAN_VLDM(f->fs.mask.ovlan_vld));
+	fwr->smac_sel = 0;
+	fwr->rx_chan_rx_rpl_iq =
+		htons(V_FW_FILTER_WR_RX_CHAN(0) |
+		      V_FW_FILTER_WR_RX_RPL_IQ(adapter->sge.fw_evtq.abs_id));
+	fwr->maci_to_matchtypem =
+		htonl(V_FW_FILTER_WR_MACI(f->fs.val.macidx) |
+		      V_FW_FILTER_WR_MACIM(f->fs.mask.macidx) |
+		      V_FW_FILTER_WR_FCOE(f->fs.val.fcoe) |
+		      V_FW_FILTER_WR_FCOEM(f->fs.mask.fcoe) |
+		      V_FW_FILTER_WR_PORT(f->fs.val.iport) |
+		      V_FW_FILTER_WR_PORTM(f->fs.mask.iport) |
+		      V_FW_FILTER_WR_MATCHTYPE(f->fs.val.matchtype) |
+		      V_FW_FILTER_WR_MATCHTYPEM(f->fs.mask.matchtype));
+	fwr->ptcl = f->fs.val.proto;
+	fwr->ptclm = f->fs.mask.proto;
+	fwr->ttyp = f->fs.val.tos;
+	fwr->ttypm = f->fs.mask.tos;
+	fwr->ivlan = htons(f->fs.val.ivlan);
+	fwr->ivlanm = htons(f->fs.mask.ivlan);
+	fwr->ovlan = htons(f->fs.val.ovlan);
+	fwr->ovlanm = htons(f->fs.mask.ovlan);
+	memcpy(fwr->lip, f->fs.val.lip, sizeof fwr->lip);
+	memcpy(fwr->lipm, f->fs.mask.lip, sizeof fwr->lipm);
+	memcpy(fwr->fip, f->fs.val.fip, sizeof fwr->fip);
+	memcpy(fwr->fipm, f->fs.mask.fip, sizeof fwr->fipm);
+	fwr->lp = htons(f->fs.val.lport);
+	fwr->lpm = htons(f->fs.mask.lport);
+	fwr->fp = htons(f->fs.val.fport);
+	fwr->fpm = htons(f->fs.mask.fport);
+	if (f->fs.newsmac)
+		memcpy(fwr->sma, f->fs.smac, sizeof fwr->sma);
+
+	/*
+	 * Mark the filter as "pending" and ship off the Filter Work Request.
+	 * When we get the Work Request Reply we'll clear the pending status.
+	 */
+	f->pending = 1;
+	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+	t4_ofld_send(adapter, skb);
+	return 0;
+}
+
+/*
+ * Delete the filter at a specified index.
+ */
+static int del_filter_wr(struct adapter *adapter, int fidx)
+{
+	struct filter_entry *f = &adapter->tids.ftid_tab[fidx];
+	struct sk_buff *skb;
+	struct fw_filter_wr *fwr;
+	unsigned int len, ftid;
+
+	len = sizeof *fwr;
+	ftid = adapter->tids.ftid_base + fidx;
+
+	skb = alloc_skb(len, GFP_KERNEL | __GFP_NOFAIL);
+	fwr = (struct fw_filter_wr *)__skb_put(skb, len);
+	t4_mk_filtdelwr(ftid, fwr, adapter->sge.fw_evtq.abs_id);
+
+	/*
+	 * Mark the filter as "pending" and ship off the Filter Work Request.
+	 * When we get the Work Request Reply we'll clear the pending status.
+	 */
+	f->pending = 1;
+	t4_mgmt_tx(adapter, skb);
+	return 0;
+}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+static u16 cxgb_select_queue(struct net_device *dev, struct sk_buff *skb)
+{
+	int txq = skb_rx_queue_recorded(skb) ? skb_get_rx_queue(skb) :
+					smp_processor_id();
+
+	if (select_queue) {
+		while (unlikely(txq >= dev->real_num_tx_queues))
+			txq -= dev->real_num_tx_queues;
+
+		return txq;
+	}
+
+	return skb_tx_hash(dev, skb);
+}
+
+static struct net_device_stats *cxgb_get_stats(struct net_device *dev)
+{
+	struct port_stats stats;
+	struct port_info *p = netdev_priv(dev);
+	struct adapter *adapter = p->adapter;
+	struct net_device_stats *ns = &dev->stats;
+
+	spin_lock(&adapter->stats_lock);
+	t4_get_port_stats_offset(adapter, p->tx_chan, &stats,
+				 &p->stats_base);
+	spin_unlock(&adapter->stats_lock);
+
+	ns->tx_bytes   = stats.tx_octets;
+	ns->tx_packets = stats.tx_frames;
+	ns->rx_bytes   = stats.rx_octets;
+	ns->rx_packets = stats.rx_frames;
+	ns->multicast  = stats.rx_mcast_frames;
+
+	/* detailed rx_errors */
+	ns->rx_length_errors = stats.rx_jabber + stats.rx_too_long +
+			       stats.rx_runt;
+	ns->rx_over_errors   = 0;
+	ns->rx_crc_errors    = stats.rx_fcs_err;
+	ns->rx_frame_errors  = stats.rx_symbol_err;
+	ns->rx_fifo_errors   = stats.rx_ovflow0 + stats.rx_ovflow1 +
+			       stats.rx_ovflow2 + stats.rx_ovflow3 +
+			       stats.rx_trunc0 + stats.rx_trunc1 +
+			       stats.rx_trunc2 + stats.rx_trunc3;
+	ns->rx_missed_errors = 0;
+
+	/* detailed tx_errors */
+	ns->tx_aborted_errors   = 0;
+	ns->tx_carrier_errors   = 0;
+	ns->tx_fifo_errors      = 0;
+	ns->tx_heartbeat_errors = 0;
+	ns->tx_window_errors    = 0;
+
+	ns->tx_errors = stats.tx_error_frames;
+	ns->rx_errors = stats.rx_symbol_err + stats.rx_fcs_err +
+		ns->rx_length_errors + stats.rx_len_err + ns->rx_fifo_errors;
+	return ns;
+}
+
+/*
+ * Implementation of ethtool operations.
+ */
+
+static u32 get_msglevel(struct net_device *dev)
+{
+	return netdev2adap(dev)->msg_enable;
+}
+
+static void set_msglevel(struct net_device *dev, u32 val)
+{
+	netdev2adap(dev)->msg_enable = val;
+}
+
+static char stats_strings[][ETH_GSTRING_LEN] = {
+	"TxOctetsOK         ",
+	"TxFramesOK         ",
+	"TxBroadcastFrames  ",
+	"TxMulticastFrames  ",
+	"TxUnicastFrames    ",
+	"TxErrorFrames      ",
+
+	"TxFrames64         ",
+	"TxFrames65To127    ",
+	"TxFrames128To255   ",
+	"TxFrames256To511   ",
+	"TxFrames512To1023  ",
+	"TxFrames1024To1518 ",
+	"TxFrames1519ToMax  ",
+
+	"TxFramesDropped    ",
+	"TxPauseFrames      ",
+	"TxPPP0Frames       ",
+	"TxPPP1Frames       ",
+	"TxPPP2Frames       ",
+	"TxPPP3Frames       ",
+	"TxPPP4Frames       ",
+	"TxPPP5Frames       ",
+	"TxPPP6Frames       ",
+	"TxPPP7Frames       ",
+
+	"RxOctetsOK         ",
+	"RxFramesOK         ",
+	"RxBroadcastFrames  ",
+	"RxMulticastFrames  ",
+	"RxUnicastFrames    ",
+
+	"RxFramesTooLong    ",
+	"RxJabberErrors     ",
+	"RxFCSErrors        ",
+	"RxLengthErrors     ",
+	"RxSymbolErrors     ",
+	"RxRuntFrames       ",
+
+	"RxFrames64         ",
+	"RxFrames65To127    ",
+	"RxFrames128To255   ",
+	"RxFrames256To511   ",
+	"RxFrames512To1023  ",
+	"RxFrames1024To1518 ",
+	"RxFrames1519ToMax  ",
+
+	"RxPauseFrames      ",
+	"RxPPP0Frames       ",
+	"RxPPP1Frames       ",
+	"RxPPP2Frames       ",
+	"RxPPP3Frames       ",
+	"RxPPP4Frames       ",
+	"RxPPP5Frames       ",
+	"RxPPP6Frames       ",
+	"RxPPP7Frames       ",
+
+	"RxBG0FramesDropped ",
+	"RxBG1FramesDropped ",
+	"RxBG2FramesDropped ",
+	"RxBG3FramesDropped ",
+	"RxBG0FramesTrunc   ",
+	"RxBG1FramesTrunc   ",
+	"RxBG2FramesTrunc   ",
+	"RxBG3FramesTrunc   ",
+
+	"TSO                ",
+	"TxCsumOffload      ",
+	"RxCsumGood         ",
+	"VLANextractions    ",
+	"VLANinsertions     ",
+	"GROPackets         ",
+	"GROMerged          ",
+};
+
+static int get_sset_count(struct net_device *dev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return ARRAY_SIZE(stats_strings);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+#define T4_REGMAP_SIZE (160 * 1024)
+
+static int get_regs_len(struct net_device *dev)
+{
+	return T4_REGMAP_SIZE;
+}
+
+static int get_eeprom_len(struct net_device *dev)
+{
+	return EEPROMSIZE;
+}
+
+static void get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)
+{
+	struct adapter *adapter = netdev2adap(dev);
+
+	strcpy(info->driver, KBUILD_MODNAME);
+	strcpy(info->version, DRV_VERSION);
+	strcpy(info->bus_info, pci_name(adapter->pdev));
+
+	if (!adapter->params.fw_vers)
+		strcpy(info->fw_version, "N/A");
+	else
+		snprintf(info->fw_version, sizeof(info->fw_version),
+			"%u.%u.%u.%u, TP %u.%u.%u.%u",
+			G_FW_HDR_FW_VER_MAJOR(adapter->params.fw_vers),
+			G_FW_HDR_FW_VER_MINOR(adapter->params.fw_vers),
+			G_FW_HDR_FW_VER_MICRO(adapter->params.fw_vers),
+			G_FW_HDR_FW_VER_BUILD(adapter->params.fw_vers),
+			G_FW_HDR_FW_VER_MAJOR(adapter->params.tp_vers),
+			G_FW_HDR_FW_VER_MINOR(adapter->params.tp_vers),
+			G_FW_HDR_FW_VER_MICRO(adapter->params.tp_vers),
+			G_FW_HDR_FW_VER_BUILD(adapter->params.tp_vers));
+}
+
+static void get_strings(struct net_device *dev, u32 stringset, u8 *data)
+{
+	if (stringset == ETH_SS_STATS)
+		memcpy(data, stats_strings, sizeof(stats_strings));
+}
+
+/*
+ * port stats maintained per queue of the port.  They should be in the same
+ * order as in stats_strings above.
+ */
+struct queue_port_stats {
+	u64 tso;
+	u64 tx_csum;
+	u64 rx_csum;
+	u64 vlan_ex;
+	u64 vlan_ins;
+	u64 lro_pkts;
+	u64 lro_merged;
+};
+
+static void collect_sge_port_stats(const struct adapter *adap,
+		struct port_info *p, struct queue_port_stats *s)
+{
+	int i;
+	const struct sge_eth_txq *tx = &adap->sge.ethtxq[p->first_qset];
+	const struct sge_eth_rxq *rx = &adap->sge.ethrxq[p->first_qset];
+
+	memset(s, 0, sizeof(*s));
+	for (i = 0; i < p->nqsets; i++, rx++, tx++) {
+		s->tso += tx->tso;
+		s->tx_csum += tx->tx_cso;
+		s->rx_csum += rx->stats.rx_cso;
+		s->vlan_ex += rx->stats.vlan_ex;
+		s->vlan_ins += tx->vlan_ins;
+		s->lro_pkts += rx->stats.lro_pkts;
+		s->lro_merged += rx->stats.lro_merged;
+	}
+}
+
+/* clear port-related stats maintained by the port's associated queues */
+static void clear_sge_port_stats(struct adapter *adap, struct port_info *p)
+{
+	int i;
+	struct sge_eth_txq *tx = &adap->sge.ethtxq[p->first_qset];
+	struct sge_eth_rxq *rx = &adap->sge.ethrxq[p->first_qset];
+
+	for (i = 0; i < p->nqsets; i++, rx++, tx++) {
+		memset(&rx->stats, 0, sizeof(rx->stats));
+		tx->tso = 0;
+		tx->tx_cso = 0;
+		tx->vlan_ins = 0;
+		tx->coal_wr = 0;
+		tx->coal_pkts = 0;
+		rx->stats.lro_pkts = 0;
+		rx->stats.lro_merged = 0;
+	}
+}
+
+static void get_stats(struct net_device *dev, struct ethtool_stats *stats,
+		      u64 *data)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	t4_get_port_stats_offset(adapter, pi->tx_chan,
+				 (struct port_stats *)data,
+				 &pi->stats_base);
+
+	data += sizeof(struct port_stats) / sizeof(u64);
+	collect_sge_port_stats(adapter, pi, (struct queue_port_stats *)data);
+}
+
+/*
+ * Return a version number to identify the type of adapter.  The scheme is:
+ * - bits 0..9: chip version
+ * - bits 10..15: chip revision
+ * - bits 16..23: register dump version
+ */
+static inline unsigned int mk_adap_vers(const struct adapter *ap)
+{
+	return 4 | (ap->params.rev << 10) | (1 << 16);
+}
+
+static void reg_block_dump(struct adapter *ap, void *buf, unsigned int start,
+			   unsigned int end)
+{
+	u32 *p = buf + start;
+
+	for ( ; start <= end; start += sizeof(u32))
+		*p++ = t4_read_reg(ap, start);
+}
+
+static void get_regs(struct net_device *dev, struct ethtool_regs *regs,
+		     void *buf)
+{
+	static const unsigned int reg_ranges[] = {
+		0x1008, 0x1108,
+		0x1180, 0x11b4,
+		0x11fc, 0x123c,
+		0x1300, 0x173c,
+		0x1800, 0x18fc,
+		0x3000, 0x30d8,
+		0x30e0, 0x5924,
+		0x5960, 0x59d4,
+		0x5a00, 0x5af8,
+		0x6000, 0x6098,
+		0x6100, 0x6150,
+		0x6200, 0x6208,
+		0x6240, 0x6248,
+		0x6280, 0x6338,
+		0x6370, 0x638c,
+		0x6400, 0x643c,
+		0x6500, 0x6524,
+		0x6a00, 0x6a38,
+		0x6a60, 0x6a78,
+		0x6b00, 0x6b84,
+		0x6bf0, 0x6c84,
+		0x6cf0, 0x6d84,
+		0x6df0, 0x6e84,
+		0x6ef0, 0x6f84,
+		0x6ff0, 0x7084,
+		0x70f0, 0x7184,
+		0x71f0, 0x7284,
+		0x72f0, 0x7384,
+		0x73f0, 0x7450,
+		0x7500, 0x7530,
+		0x7600, 0x761c,
+		0x7680, 0x76cc,
+		0x7700, 0x7798,
+		0x77c0, 0x77fc,
+		0x7900, 0x79fc,
+		0x7b00, 0x7c38,
+		0x7d00, 0x7efc,
+		0x8dc0, 0x8e1c,
+		0x8e30, 0x8e78,
+		0x8ea0, 0x8f6c,
+		0x8fc0, 0x9074,
+		0x90fc, 0x90fc,
+		0x9400, 0x9458,
+		0x9600, 0x96bc,
+		0x9800, 0x9808,
+		0x9820, 0x983c,
+		0x9850, 0x9864,
+		0x9c00, 0x9c6c,
+		0x9c80, 0x9cec,
+		0x9d00, 0x9d6c,
+		0x9d80, 0x9dec,
+		0x9e00, 0x9e6c,
+		0x9e80, 0x9eec,
+		0x9f00, 0x9f6c,
+		0x9f80, 0x9fec,
+		0xd004, 0xd03c,
+		0xdfc0, 0xdfe0,
+		0xe000, 0xea7c,
+		0xf000, 0x11190,
+		0x19040, 0x1906c,
+		0x19078, 0x19080,
+		0x1908c, 0x19124,
+		0x19150, 0x191b0,
+		0x191d0, 0x191e8,
+		0x19238, 0x1924c,
+		0x193f8, 0x19474,
+		0x19490, 0x194f8,
+		0x19800, 0x19f30,
+		0x1a000, 0x1a06c,
+		0x1a0b0, 0x1a120,
+		0x1a128, 0x1a138,
+		0x1a190, 0x1a1c4,
+		0x1a1fc, 0x1a1fc,
+		0x1e040, 0x1e04c,
+		0x1e284, 0x1e28c,
+		0x1e2c0, 0x1e2c0,
+		0x1e2e0, 0x1e2e0,
+		0x1e300, 0x1e384,
+		0x1e3c0, 0x1e3c8,
+		0x1e440, 0x1e44c,
+		0x1e684, 0x1e68c,
+		0x1e6c0, 0x1e6c0,
+		0x1e6e0, 0x1e6e0,
+		0x1e700, 0x1e784,
+		0x1e7c0, 0x1e7c8,
+		0x1e840, 0x1e84c,
+		0x1ea84, 0x1ea8c,
+		0x1eac0, 0x1eac0,
+		0x1eae0, 0x1eae0,
+		0x1eb00, 0x1eb84,
+		0x1ebc0, 0x1ebc8,
+		0x1ec40, 0x1ec4c,
+		0x1ee84, 0x1ee8c,
+		0x1eec0, 0x1eec0,
+		0x1eee0, 0x1eee0,
+		0x1ef00, 0x1ef84,
+		0x1efc0, 0x1efc8,
+		0x1f040, 0x1f04c,
+		0x1f284, 0x1f28c,
+		0x1f2c0, 0x1f2c0,
+		0x1f2e0, 0x1f2e0,
+		0x1f300, 0x1f384,
+		0x1f3c0, 0x1f3c8,
+		0x1f440, 0x1f44c,
+		0x1f684, 0x1f68c,
+		0x1f6c0, 0x1f6c0,
+		0x1f6e0, 0x1f6e0,
+		0x1f700, 0x1f784,
+		0x1f7c0, 0x1f7c8,
+		0x1f840, 0x1f84c,
+		0x1fa84, 0x1fa8c,
+		0x1fac0, 0x1fac0,
+		0x1fae0, 0x1fae0,
+		0x1fb00, 0x1fb84,
+		0x1fbc0, 0x1fbc8,
+		0x1fc40, 0x1fc4c,
+		0x1fe84, 0x1fe8c,
+		0x1fec0, 0x1fec0,
+		0x1fee0, 0x1fee0,
+		0x1ff00, 0x1ff84,
+		0x1ffc0, 0x1ffc8,
+		0x20000, 0x2002c,
+		0x20100, 0x2013c,
+		0x20190, 0x201c8,
+		0x20200, 0x20318,
+		0x20400, 0x20528,
+		0x20540, 0x20614,
+		0x21000, 0x21040,
+		0x2104c, 0x21060,
+		0x210c0, 0x210ec,
+		0x21200, 0x21268,
+		0x21270, 0x21284,
+		0x212fc, 0x21388,
+		0x21400, 0x21404,
+		0x21500, 0x21518,
+		0x2152c, 0x2153c,
+		0x21550, 0x21554,
+		0x21600, 0x21600,
+		0x21608, 0x21628,
+		0x21630, 0x2163c,
+		0x21700, 0x2171c,
+		0x21780, 0x2178c,
+		0x21800, 0x21c38,
+		0x21c80, 0x21d7c,
+		0x21e00, 0x21e04,
+		0x22000, 0x2202c,
+		0x22100, 0x2213c,
+		0x22190, 0x221c8,
+		0x22200, 0x22318,
+		0x22400, 0x22528,
+		0x22540, 0x22614,
+		0x23000, 0x23040,
+		0x2304c, 0x23060,
+		0x230c0, 0x230ec,
+		0x23200, 0x23268,
+		0x23270, 0x23284,
+		0x232fc, 0x23388,
+		0x23400, 0x23404,
+		0x23500, 0x23518,
+		0x2352c, 0x2353c,
+		0x23550, 0x23554,
+		0x23600, 0x23600,
+		0x23608, 0x23628,
+		0x23630, 0x2363c,
+		0x23700, 0x2371c,
+		0x23780, 0x2378c,
+		0x23800, 0x23c38,
+		0x23c80, 0x23d7c,
+		0x23e00, 0x23e04,
+		0x24000, 0x2402c,
+		0x24100, 0x2413c,
+		0x24190, 0x241c8,
+		0x24200, 0x24318,
+		0x24400, 0x24528,
+		0x24540, 0x24614,
+		0x25000, 0x25040,
+		0x2504c, 0x25060,
+		0x250c0, 0x250ec,
+		0x25200, 0x25268,
+		0x25270, 0x25284,
+		0x252fc, 0x25388,
+		0x25400, 0x25404,
+		0x25500, 0x25518,
+		0x2552c, 0x2553c,
+		0x25550, 0x25554,
+		0x25600, 0x25600,
+		0x25608, 0x25628,
+		0x25630, 0x2563c,
+		0x25700, 0x2571c,
+		0x25780, 0x2578c,
+		0x25800, 0x25c38,
+		0x25c80, 0x25d7c,
+		0x25e00, 0x25e04,
+		0x26000, 0x2602c,
+		0x26100, 0x2613c,
+		0x26190, 0x261c8,
+		0x26200, 0x26318,
+		0x26400, 0x26528,
+		0x26540, 0x26614,
+		0x27000, 0x27040,
+		0x2704c, 0x27060,
+		0x270c0, 0x270ec,
+		0x27200, 0x27268,
+		0x27270, 0x27284,
+		0x272fc, 0x27388,
+		0x27400, 0x27404,
+		0x27500, 0x27518,
+		0x2752c, 0x2753c,
+		0x27550, 0x27554,
+		0x27600, 0x27600,
+		0x27608, 0x27628,
+		0x27630, 0x2763c,
+		0x27700, 0x2771c,
+		0x27780, 0x2778c,
+		0x27800, 0x27c38,
+		0x27c80, 0x27d7c,
+		0x27e00, 0x27e04
+	};
+
+	int i;
+	struct adapter *ap = netdev2adap(dev);
+
+	regs->version = mk_adap_vers(ap);
+
+	memset(buf, 0, T4_REGMAP_SIZE);
+	for (i = 0; i < ARRAY_SIZE(reg_ranges); i += 2)
+		reg_block_dump(ap, buf, reg_ranges[i], reg_ranges[i + 1]);
+}
+
+static int restart_autoneg(struct net_device *dev)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	if (!netif_running(dev))
+		return -EAGAIN;
+	if (pi->link_cfg.autoneg != AUTONEG_ENABLE)
+		return -EINVAL;
+	t4_restart_aneg(adapter, adapter->mbox, pi->tx_chan);
+	return 0;
+}
+
+static int identify_port(struct net_device *dev, u32 data)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	if (data == 0)
+		data = 2;     /* default to 2 seconds */
+
+	return t4_identify_port(adapter, adapter->mbox, pi->viid, data * 5);
+}
+
+static unsigned int from_fw_linkcaps(unsigned int caps)
+{
+	unsigned int v = 0;
+
+	if (caps & FW_PORT_CAP_SPEED_100M)
+		v |= SUPPORTED_100baseT_Full;
+	if (caps & FW_PORT_CAP_SPEED_1G)
+		v |= SUPPORTED_1000baseT_Full;
+	if (caps & FW_PORT_CAP_SPEED_10G)
+		v |= SUPPORTED_10000baseT_Full;
+	if (caps & FW_PORT_CAP_ANEG)
+		v |= SUPPORTED_Autoneg;
+	return v;
+}
+
+static unsigned int to_fw_linkcaps(unsigned int caps)
+{
+	unsigned int v = 0;
+
+	if (caps & ADVERTISED_100baseT_Full)
+		v |= FW_PORT_CAP_SPEED_100M;
+	if (caps & ADVERTISED_1000baseT_Full)
+		v |= FW_PORT_CAP_SPEED_1G;
+	if (caps & ADVERTISED_10000baseT_Full)
+		v |= FW_PORT_CAP_SPEED_10G;
+	return v;
+}
+
+static int get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	const struct port_info *p = netdev_priv(dev);
+
+	cmd->supported = from_fw_linkcaps(p->link_cfg.supported);
+	cmd->advertising = from_fw_linkcaps(p->link_cfg.advertising);
+	cmd->speed = netif_carrier_ok(dev) ? p->link_cfg.speed : -1;
+	cmd->duplex = DUPLEX_FULL;
+
+	if (p->port_type == FW_PORT_TYPE_BT_SGMII ||
+	    p->port_type == FW_PORT_TYPE_BT_XAUI ||
+	    p->port_type == FW_PORT_TYPE_BT_XFI)
+		cmd->port = PORT_TP;
+	else if (p->port_type == FW_PORT_TYPE_FIBER_XFI ||
+		 p->port_type == FW_PORT_TYPE_FIBER_XAUI)
+		cmd->port = PORT_FIBRE;
+	else if (p->port_type == FW_PORT_TYPE_SFP) {
+		if (p->mod_type == FW_PORT_MOD_TYPE_LR ||
+		    p->mod_type == FW_PORT_MOD_TYPE_SR ||
+		    p->mod_type == FW_PORT_MOD_TYPE_ER ||
+		    p->mod_type == FW_PORT_MOD_TYPE_LRM)
+			cmd->port = PORT_FIBRE;
+		else if (p->mod_type == FW_PORT_MOD_TYPE_TWINAX_PASSIVE ||
+			 p->mod_type == FW_PORT_MOD_TYPE_TWINAX_ACTIVE)
+			cmd->port = PORT_DA;
+		else
+			cmd->port = PORT_OTHER;
+	}else
+		cmd->port = PORT_OTHER;
+
+	cmd->phy_address = p->mdio_addr < 32 ? p->mdio_addr : 0;
+	cmd->transceiver = p->mdio_addr < 32 ? XCVR_EXTERNAL : XCVR_INTERNAL;
+	cmd->autoneg = p->link_cfg.autoneg;
+	cmd->maxtxpkt = 0;
+	cmd->maxrxpkt = 0;
+	return 0;
+}
+
+static unsigned int speed_to_caps(int speed)
+{
+	if (speed == SPEED_100)
+		return FW_PORT_CAP_SPEED_100M;
+	if (speed == SPEED_1000)
+		return FW_PORT_CAP_SPEED_1G;
+	if (speed == SPEED_10000)
+		return FW_PORT_CAP_SPEED_10G;
+	return 0;
+}
+
+static int set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	unsigned int cap;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	struct link_config *lc = &pi->link_cfg;
+
+	if (cmd->duplex != DUPLEX_FULL)     /* only full-duplex supported */
+		return -EINVAL;
+
+	if (!(lc->supported & FW_PORT_CAP_ANEG)) {
+		/*
+		 * PHY offers a single speed.  See if that's what's
+		 * being requested.
+		 */
+		if (cmd->autoneg == AUTONEG_DISABLE &&
+		    (lc->supported & speed_to_caps(cmd->speed)))
+				return 0;
+		return -EINVAL;
+	}
+
+	if (cmd->autoneg == AUTONEG_DISABLE) {
+		cap = speed_to_caps(cmd->speed);
+
+		if (!(lc->supported & cap) || cmd->speed == SPEED_1000 ||
+		    cmd->speed == SPEED_10000)
+			return -EINVAL;
+		lc->requested_speed = cap;
+		lc->advertising = 0;
+	} else {
+		cap = to_fw_linkcaps(cmd->advertising);
+		if (!(lc->supported & cap))
+			return -EINVAL;
+		lc->requested_speed = 0;
+		lc->advertising = cap | FW_PORT_CAP_ANEG;
+	}
+	lc->autoneg = cmd->autoneg;
+
+	if (netif_running(dev))
+		return t4_link_start(adapter, adapter->mbox, pi->tx_chan, lc);
+	return 0;
+}
+
+static void get_pauseparam(struct net_device *dev,
+			   struct ethtool_pauseparam *epause)
+{
+	struct port_info *p = netdev_priv(dev);
+
+	epause->autoneg = (p->link_cfg.requested_fc & PAUSE_AUTONEG) != 0;
+	epause->rx_pause = (p->link_cfg.fc & PAUSE_RX) != 0;
+	epause->tx_pause = (p->link_cfg.fc & PAUSE_TX) != 0;
+}
+
+static int set_pauseparam(struct net_device *dev,
+			  struct ethtool_pauseparam *epause)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	struct link_config *lc = &pi->link_cfg;
+
+	if (epause->autoneg == AUTONEG_DISABLE)
+		lc->requested_fc = 0;
+	else if (lc->supported & FW_PORT_CAP_ANEG)
+		lc->requested_fc = PAUSE_AUTONEG;
+	else
+		return -EINVAL;
+
+	if (epause->rx_pause)
+		lc->requested_fc |= PAUSE_RX;
+	if (epause->tx_pause)
+		lc->requested_fc |= PAUSE_TX;
+	if (netif_running(dev))
+		return t4_link_start(adapter, adapter->mbox, pi->tx_chan, lc);
+	return 0;
+}
+
+static u32 get_rx_csum(struct net_device *dev)
+{
+	struct port_info *p = netdev_priv(dev);
+
+	return p->rx_offload & RX_CSO;
+}
+
+static int set_rx_csum(struct net_device *dev, u32 data)
+{
+	struct port_info *p = netdev_priv(dev);
+
+	if (data)
+		p->rx_offload |= RX_CSO;
+	else
+		p->rx_offload &= ~RX_CSO;
+	return 0;
+}
+
+static void get_sge_param(struct net_device *dev, struct ethtool_ringparam *e)
+{
+	const struct port_info *pi = netdev_priv(dev);
+	const struct sge *s = &pi->adapter->sge;
+
+	e->rx_max_pending = MAX_RX_BUFFERS;
+	e->rx_mini_max_pending = MAX_RSPQ_ENTRIES;
+	e->rx_jumbo_max_pending = 0;
+	e->tx_max_pending = MAX_TXQ_ENTRIES;
+
+	e->rx_pending = s->ethrxq[pi->first_qset].fl.size - 8;
+	e->rx_mini_pending = s->ethrxq[pi->first_qset].rspq.size;
+	e->rx_jumbo_pending = 0;
+	e->tx_pending = s->ethtxq[pi->first_qset].q.size;
+}
+
+static int set_sge_param(struct net_device *dev, struct ethtool_ringparam *e)
+{
+	int i;
+	const struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	struct sge *s = &adapter->sge;
+
+	if (e->rx_pending > MAX_RX_BUFFERS || e->rx_jumbo_pending ||
+	    e->tx_pending > MAX_TXQ_ENTRIES ||
+	    e->rx_mini_pending > MAX_RSPQ_ENTRIES ||
+	    e->rx_mini_pending < MIN_RSPQ_ENTRIES ||
+	    e->rx_pending < MIN_FL_ENTRIES || e->tx_pending < MIN_TXQ_ENTRIES)
+		return -EINVAL;
+
+	if (adapter->flags & FULL_INIT_DONE)
+		return -EBUSY;
+
+	for (i = 0; i < pi->nqsets; ++i) {
+		s->ethtxq[pi->first_qset + i].q.size = e->tx_pending;
+		s->ethrxq[pi->first_qset + i].fl.size = e->rx_pending + 8;
+		s->ethrxq[pi->first_qset + i].rspq.size = e->rx_mini_pending;
+	}
+	return 0;
+}
+
+static int closest_timer(const struct sge *s, int time)
+{
+	int i, delta, match = 0, min_delta = INT_MAX;
+
+	for (i = 0; i < ARRAY_SIZE(s->timer_val); i++) {
+		delta = time - s->timer_val[i];
+		if (delta < 0)
+			delta = -delta;
+		if (delta < min_delta) {
+			min_delta = delta;
+			match = i;
+		}
+	}
+	return match;
+}
+
+static int closest_thres(const struct sge *s, int thres)
+{
+	int i, delta, match = 0, min_delta = INT_MAX;
+
+	for (i = 0; i < ARRAY_SIZE(s->counter_val); i++) {
+		delta = thres - s->counter_val[i];
+		if (delta < 0)
+			delta = -delta;
+		if (delta < min_delta) {
+			min_delta = delta;
+			match = i;
+		}
+	}
+	return match;
+}
+
+/*
+ * Return a queue's interrupt hold-off time in us.  0 means no timer.
+ */
+static unsigned int qtimer_val(const struct adapter *adap,
+			       const struct sge_rspq *q)
+{
+	unsigned int idx = G_QINTR_TIMER_IDX(q->intr_params);
+
+	return idx < SGE_NTIMERS ? adap->sge.timer_val[idx] : 0;
+}
+
+/**
+ *	set_rspq_intr_params - set a queue's interrupt holdoff parameters
+ *	@q: the Rx queue
+ *	@us: the hold-off time in us, or 0 to disable timer
+ *	@cnt: the hold-off packet count, or 0 to disable counter
+ *
+ *	Sets an Rx queue's interrupt hold-off time and packet count.  At least
+ *	one of the two needs to be enabled for the queue to generate interrupts.
+ */
+static int set_rspq_intr_params(struct sge_rspq *q,
+				unsigned int us, unsigned int cnt)
+{
+	struct adapter *adap = q->adapter;
+
+	if ((us | cnt) == 0)
+		cnt = 1;
+
+	if (cnt) {
+		int err;
+		u32 v, new_idx;
+
+		new_idx = closest_thres(&adap->sge, cnt);
+		if (q->desc && q->pktcnt_idx != new_idx) {
+			/* the queue has already been created, update it */
+			v = V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DMAQ) |
+			    V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DMAQ_IQ_INTCNTTHRESH) |
+			    V_FW_PARAMS_PARAM_YZ(q->cntxt_id);
+			err = t4_set_params(adap, adap->mbox, adap->pf, 0, 1,
+					    &v, &new_idx);
+			if (err)
+				return err;
+		}
+		q->pktcnt_idx = new_idx;
+	}
+
+	us = us == 0 ? X_TIMERREG_RESTART_COUNTER : closest_timer(&adap->sge, us);
+	q->intr_params = V_QINTR_TIMER_IDX(us) | V_QINTR_CNT_EN(cnt > 0);
+	return 0;
+}
+
+
+/**
+ *	set_rx_intr_params - set a net devices's RX interrupt holdoff paramete!
+ *	@dev: the network device
+ *	@us: the hold-off time in us, or 0 to disable timer
+ *	@cnt: the hold-off packet count, or 0 to disable counter
+ *
+ *	Set the RX interrupt hold-off parameters for a network device.
+ */
+static int set_rx_intr_params(struct net_device *dev,
+                              unsigned int us, unsigned int cnt)
+{
+	int i, err;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adap = pi->adapter;
+	struct sge_eth_rxq *q = &adap->sge.ethrxq[pi->first_qset];
+
+	for (i = 0; i < pi->nqsets; i++, q++) {
+		err = set_rspq_intr_params(&q->rspq, us, cnt);
+		if (err)
+			return err;
+	}
+	return 0;
+}
+
+static int set_coalesce(struct net_device *dev,
+			struct ethtool_coalesce *coalesce)
+{
+	return set_rx_intr_params(dev, coalesce->rx_coalesce_usecs,
+				  coalesce->rx_max_coalesced_frames);
+}
+
+static int get_coalesce(struct net_device *dev, struct ethtool_coalesce *c)
+{
+	const struct port_info *pi = netdev_priv(dev);
+	const struct adapter *adap = pi->adapter;
+	const struct sge_rspq *rq = &adap->sge.ethrxq[pi->first_qset].rspq;
+
+	c->rx_coalesce_usecs = qtimer_val(adap, rq);
+	c->rx_max_coalesced_frames = (rq->intr_params & F_QINTR_CNT_EN) ?
+		adap->sge.counter_val[rq->pktcnt_idx] : 0;
+	return 0;
+}
+
+/*
+ * The next two routines implement eeprom read/write from physical addresses.
+ */
+static int eeprom_rd_phys(struct adapter *adap, unsigned int phys_addr, u32 *v)
+{
+	int vaddr = t4_eeprom_ptov(phys_addr, adap->pf, EEPROMPFSIZE);
+
+	return vaddr < 0 ? vaddr : t4_seeprom_read(adap, vaddr, v);
+}
+
+static int eeprom_wr_phys(struct adapter *adap, unsigned int phys_addr, u32 v)
+{
+	int vaddr = t4_eeprom_ptov(phys_addr, adap->pf, EEPROMPFSIZE);
+
+	return vaddr < 0 ? vaddr : t4_seeprom_write(adap, vaddr, v);
+}
+
+static int get_eeprom(struct net_device *dev, struct ethtool_eeprom *e,
+		      u8 *data)
+{
+	int i, err = 0;
+	struct adapter *adapter = netdev2adap(dev);
+
+	u8 *buf = t4_alloc_mem(EEPROMSIZE);
+	if (!buf)
+		return -ENOMEM;
+
+	e->magic = EEPROM_MAGIC;
+	for (i = e->offset & ~3; !err && i < e->offset + e->len; i += 4)
+		err = eeprom_rd_phys(adapter, i, (u32 *)&buf[i]);
+
+	if (!err)
+		memcpy(data, buf + e->offset, e->len);
+	t4_free_mem(buf);
+	return err;
+}
+
+static int set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom,
+		      u8 *data)
+{
+	u8 *buf;
+	int err = 0;
+	u32 aligned_offset, aligned_len, *p;
+	struct adapter *adapter = netdev2adap(dev);
+
+	if (eeprom->magic != EEPROM_MAGIC)
+		return -EINVAL;
+
+	aligned_offset = eeprom->offset & ~3;
+	aligned_len = (eeprom->len + (eeprom->offset & 3) + 3) & ~3;
+
+	if (adapter->pf > 0) {
+		u32 start = 1024 + adapter->pf * EEPROMPFSIZE;
+
+		if (aligned_offset < start ||
+		    aligned_offset + aligned_len > start + EEPROMPFSIZE)
+			return -EPERM;
+	}
+
+	if (aligned_offset != eeprom->offset || aligned_len != eeprom->len) {
+		/*
+		 * RMW possibly needed for first or last words.
+		 */
+		buf = t4_alloc_mem(aligned_len);
+		if (!buf)
+			return -ENOMEM;
+		err = eeprom_rd_phys(adapter, aligned_offset, (u32 *)buf);
+		if (!err && aligned_len > 4)
+			err = eeprom_rd_phys(adapter,
+					     aligned_offset + aligned_len - 4,
+					     (u32 *)&buf[aligned_len - 4]);
+		if (err)
+			goto out;
+		memcpy(buf + (eeprom->offset & 3), data, eeprom->len);
+	} else
+		buf = data;
+
+	err = t4_seeprom_wp(adapter, 0);
+	if (err)
+		goto out;
+
+	for (p = (u32 *)buf; !err && aligned_len; aligned_len -= 4, p++) {
+		err = eeprom_wr_phys(adapter, aligned_offset, *p);
+		aligned_offset += 4;
+	}
+
+	if (!err)
+		err = t4_seeprom_wp(adapter, 1);
+out:
+	if (buf != data)
+		t4_free_mem(buf);
+	return err;
+}
+
+#define WOL_SUPPORTED (WAKE_BCAST | WAKE_MAGIC)
+#define BCAST_CRC 0xa0ccc1a6
+
+static void get_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	wol->supported = WAKE_BCAST | WAKE_MAGIC;
+	wol->wolopts = netdev2adap(dev)->wol;
+	memset(&wol->sopass, 0, sizeof(wol->sopass));
+}
+
+static int set_wol(struct net_device *dev, struct ethtool_wolinfo *wol)
+{
+	int err = 0;
+	struct port_info *pi = netdev_priv(dev);
+
+	if (wol->wolopts & ~WOL_SUPPORTED)
+		return -EINVAL;
+	t4_wol_magic_enable(pi->adapter, pi->tx_chan,
+			    (wol->wolopts & WAKE_MAGIC) ? dev->dev_addr : NULL);
+	if (wol->wolopts & WAKE_BCAST) {
+		err = t4_wol_pat_enable(pi->adapter, pi->tx_chan, 0xfe, ~0ULL,
+					~0ULL, 0, false);
+		if (!err)
+			err = t4_wol_pat_enable(pi->adapter, pi->tx_chan, 1,
+						~6ULL, ~0ULL, BCAST_CRC, true);
+	} else
+		t4_wol_pat_enable(pi->adapter, pi->tx_chan, 0, 0, 0, 0, false);
+	return err;
+}
+
+#define TSO_FLAGS (NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_TSO_ECN)
+
+static int set_tso(struct net_device *dev, u32 value)
+{
+	if (value)
+		dev->features |= TSO_FLAGS;
+	else
+		dev->features &= ~TSO_FLAGS;
+	return 0;
+}
+
+static struct ethtool_ops cxgb_ethtool_ops = {
+	.get_settings      = get_settings,
+	.set_settings      = set_settings,
+	.get_drvinfo       = get_drvinfo,
+	.get_msglevel      = get_msglevel,
+	.set_msglevel      = set_msglevel,
+	.get_ringparam     = get_sge_param,
+	.set_ringparam     = set_sge_param,
+	.get_coalesce      = get_coalesce,
+	.set_coalesce      = set_coalesce,
+	.get_eeprom_len    = get_eeprom_len,
+	.get_eeprom        = get_eeprom,
+	.set_eeprom        = set_eeprom,
+	.get_pauseparam    = get_pauseparam,
+	.set_pauseparam    = set_pauseparam,
+	.get_rx_csum       = get_rx_csum,
+	.set_rx_csum       = set_rx_csum,
+	.set_tx_csum       = ethtool_op_set_tx_ipv6_csum,
+	.set_sg            = ethtool_op_set_sg,
+	.get_link          = ethtool_op_get_link,
+	.get_strings       = get_strings,
+	.phys_id           = identify_port,
+	.nway_reset        = restart_autoneg,
+	.get_sset_count    = get_sset_count,
+	.get_ethtool_stats = get_stats,
+	.get_regs_len      = get_regs_len,
+	.get_regs          = get_regs,
+	.get_wol           = get_wol,
+	.set_wol           = set_wol,
+	.set_tso           = set_tso,
+};
+
+/*
+ * debugfs support
+ */
+
+/*
+ * Read a vector of numbers from a user space buffer.  Each number must be
+ * between min and max inclusive and in the given base.
+ */
+static int rd_usr_int_vec(const char __user *buf, size_t usr_len, int vec_len,
+			  unsigned long *vals, unsigned long min,
+			  unsigned long max, int base)
+{
+	size_t l;
+	unsigned long v;
+	char c, word[68], *end;
+
+	while (usr_len) {
+		/* skip whitespace to beginning of next word */
+		while (usr_len) {
+			if (get_user(c, buf))
+				return -EFAULT;
+			if (!isspace(c))
+				break;
+			usr_len--;
+			buf++;
+		}
+
+		if (!usr_len)
+			break;
+		if (!vec_len)
+			return -EINVAL;              /* too many numbers */
+
+		/* get next word (possibly going beyond its end) */
+		l = min(usr_len, sizeof(word) - 1);
+		if (copy_from_user(word, buf, l))
+			return -EFAULT;
+		word[l] = '\0';
+
+		v = simple_strtoul(word, &end, base);
+		l = end - word;
+		if (!l)
+			return -EINVAL;              /* catch embedded '\0's */
+		if (*end && !isspace(*end))
+			return -EINVAL;
+		/*
+		 * Complain if we encountered a too long sequence of digits.
+		 * The most we can consume in one iteration is for a 64-bit
+		 * number in binary.  Too bad simple_strtoul doesn't catch
+		 * overflows.
+		 */
+		if (l > 64)
+			return -EINVAL;
+		if (v < min || v > max)
+			return -ERANGE;
+		*vals++ = v;
+		vec_len--;
+		usr_len -= l;
+		buf += l;
+	}
+	if (vec_len)
+		return -EINVAL;                      /* not enough numbers */
+	return 0;
+}
+
+/*
+ * generic seq_file support for showing a table of size rows x width.
+ */
+struct seq_tab {
+	int (*show)(struct seq_file *seq, void *v, int idx);
+	unsigned int rows;        /* # of entries */
+	unsigned char width;      /* size in bytes of each entry */
+	unsigned char skip_first; /* whether the first line is a header */
+	char data[0];             /* the table data */
+};
+
+static void *seq_tab_get_idx(struct seq_tab *tb, loff_t pos)
+{
+	pos -= tb->skip_first;
+	return pos >= tb->rows ? NULL : &tb->data[pos * tb->width];
+}
+
+static void *seq_tab_start(struct seq_file *seq, loff_t *pos)
+{
+	struct seq_tab *tb = seq->private;
+
+	if (tb->skip_first && *pos == 0)
+		return SEQ_START_TOKEN;
+
+	return seq_tab_get_idx(tb, *pos);
+}
+
+static void *seq_tab_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	v = seq_tab_get_idx(seq->private, *pos + 1);
+	if (v)
+		++*pos;
+	return v;
+}
+
+static void seq_tab_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int seq_tab_show(struct seq_file *seq, void *v)
+{
+	const struct seq_tab *tb = seq->private;
+
+	/*
+	 * index is bogus when v isn't within data, eg when it's
+	 * SEQ_START_TOKEN, but that's OK
+	 */
+	return tb->show(seq, v, ((char *)v - tb->data) / tb->width);
+}
+
+static const struct seq_operations seq_tab_ops = {
+	.start = seq_tab_start,
+	.next  = seq_tab_next,
+	.stop  = seq_tab_stop,
+	.show  = seq_tab_show
+};
+
+struct seq_tab *seq_open_tab(struct file *f, unsigned int rows,
+			     unsigned int width, unsigned int have_header,
+			     int (*show)(struct seq_file *seq, void *v, int i))
+{
+	struct seq_tab *p;
+
+	p = __seq_open_private(f, &seq_tab_ops, sizeof(*p) + rows * width);
+	if (p) {
+		p->show = show;
+		p->rows = rows;
+		p->width = width;
+		p->skip_first = have_header != 0;
+	}
+	return p;
+}
+
+/*
+ * Trim the size of a seq_tab to the supplied number of rows.  The opration is
+ * irreversible.
+ */
+static int seq_tab_trim(struct seq_tab *p, unsigned int new_rows)
+{
+	if (new_rows > p->rows)
+		return -EINVAL;
+	p->rows = new_rows;
+	return 0;
+}
+
+static int cim_la_show(struct seq_file *seq, void *v, int idx)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "Status   Data      PC     LS0Stat  LS0Addr "
+			 "            LS0Data\n");
+	else {
+		const u32 *p = v;
+
+		seq_printf(seq,
+			"  %02x   %x%07x %x%07x %08x %08x %08x%08x%08x%08x\n",
+			(p[0] >> 4) & 0xff, p[0] & 0xf, p[1] >> 4, p[1] & 0xf,
+			p[2] >> 4, p[2] & 0xf, p[3], p[4], p[5], p[6], p[7]);
+	}
+	return 0;
+}
+
+static int cim_la_show_3in1(struct seq_file *seq, void *v, int idx)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "Status   Data      PC\n");
+	else {
+		const u32 *p = v;
+
+		seq_printf(seq, "  %02x   %08x %08x\n", p[5] & 0xff, p[6],
+			   p[7]);
+		seq_printf(seq, "  %02x   %02x%06x %02x%06x\n",
+			   (p[3] >> 8) & 0xff, p[3] & 0xff, p[4] >> 8,
+			   p[4] & 0xff, p[5] >> 8);
+		seq_printf(seq, "  %02x   %x%07x %x%07x\n", (p[0] >> 4) & 0xff,
+			   p[0] & 0xf, p[1] >> 4, p[1] & 0xf, p[2] >> 4);
+	}
+	return 0;
+}
+
+static int cim_la_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	unsigned int cfg;
+	struct seq_tab *p;
+	struct adapter *adap = inode->i_private;
+
+	ret = t4_cim_read(adap, A_UP_UP_DBG_LA_CFG, 1, &cfg);
+	if (ret)
+		return ret;
+
+	p = seq_open_tab(file, adap->params.cim_la_size / 8, 8 * sizeof(u32), 1,
+		cfg & F_UPDBGLACAPTPCONLY ? cim_la_show_3in1 : cim_la_show);
+	if (!p)
+		return -ENOMEM;
+
+	ret = t4_cim_read_la(adap, (u32 *)p->data, NULL);
+	if (ret)
+		seq_release_private(inode, file);
+	return ret;
+}
+
+static const struct file_operations cim_la_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_la_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int cim_pif_la_show(struct seq_file *seq, void *v, int idx)
+{
+	const u32 *p = v;
+
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "Cntl ID DataBE   Addr                 Data\n");
+	else if (idx < CIM_PIFLA_SIZE)
+		seq_printf(seq, " %02x  %02x  %04x  %08x %08x%08x%08x%08x\n",
+			   (p[5] >> 22) & 0xff, (p[5] >> 16) & 0x3f,
+			   p[5] & 0xffff, p[4], p[3], p[2], p[1], p[0]);
+	else {
+		if (idx == CIM_PIFLA_SIZE)
+			seq_puts(seq, "\nCntl ID               Data\n");
+		seq_printf(seq, " %02x  %02x %08x%08x%08x%08x\n",
+			   (p[4] >> 6) & 0xff, p[4] & 0x3f,
+			   p[3], p[2], p[1], p[0]);
+	}
+	return 0;
+}
+
+static int cim_pif_la_open(struct inode *inode, struct file *file)
+{
+	struct seq_tab *p;
+	struct adapter *adap = inode->i_private;
+
+	p = seq_open_tab(file, 2 * CIM_PIFLA_SIZE, 6 * sizeof(u32), 1,
+			 cim_pif_la_show);
+	if (!p)
+		return -ENOMEM;
+
+	t4_cim_read_pif_la(adap, (u32 *)p->data,
+			   (u32 *)p->data + 6 * CIM_PIFLA_SIZE, NULL, NULL);
+	return 0;
+}
+
+static const struct file_operations cim_pif_la_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_pif_la_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int cim_ma_la_show(struct seq_file *seq, void *v, int idx)
+{
+	const u32 *p = v;
+
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "\n");
+	else if (idx < CIM_MALA_SIZE)
+		seq_printf(seq, "%02x%08x%08x%08x%08x\n",
+			   p[4], p[3], p[2], p[1], p[0]);
+	else {
+		if (idx == CIM_MALA_SIZE)
+			seq_puts(seq,
+				 "\nCnt ID Tag UE       Data       RDY VLD\n");
+		seq_printf(seq, "%3u %2u  %x   %u %08x%08x  %u   %u\n",
+			   (p[2] >> 10) & 0xff, (p[2] >> 7) & 7,
+			   (p[2] >> 3) & 0xf, (p[2] >> 2) & 1,
+			   (p[1] >> 2) | ((p[2] & 3) << 30),
+			   (p[0] >> 2) | ((p[1] & 3) << 30), (p[0] >> 1) & 1,
+			   p[0] & 1);
+	}
+	return 0;
+}
+
+static int cim_ma_la_open(struct inode *inode, struct file *file)
+{
+	struct seq_tab *p;
+	struct adapter *adap = inode->i_private;
+
+	p = seq_open_tab(file, 2 * CIM_MALA_SIZE, 5 * sizeof(u32), 1,
+			 cim_ma_la_show);
+	if (!p)
+		return -ENOMEM;
+
+	t4_cim_read_ma_la(adap, (u32 *)p->data,
+			  (u32 *)p->data + 5 * CIM_MALA_SIZE);
+	return 0;
+}
+
+static const struct file_operations cim_ma_la_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_ma_la_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int cim_qcfg_show(struct seq_file *seq, void *v)
+{
+	static const char *qname[] = {
+		"TP0", "TP1", "ULP", "SGE0", "SGE1", "NC-SI",
+		"ULP0", "ULP1", "ULP2", "ULP3", "SGE", "NC-SI"
+	};
+
+	int i;
+	u16 base[CIM_NUM_IBQ + CIM_NUM_OBQ];
+	u16 size[CIM_NUM_IBQ + CIM_NUM_OBQ];
+	u16 thres[CIM_NUM_IBQ];
+	u32 obq_wr[2 * CIM_NUM_OBQ], *wr = obq_wr;
+	u32 stat[4 * (CIM_NUM_IBQ + CIM_NUM_OBQ)], *p = stat;
+	struct adapter *adap = seq->private;
+
+	i = t4_cim_read(adap, A_UP_IBQ_0_RDADDR, ARRAY_SIZE(stat), stat);
+	if (!i)
+		i = t4_cim_read(adap, A_UP_OBQ_0_REALADDR, ARRAY_SIZE(obq_wr),
+				obq_wr);
+	if (i)
+		return i;
+
+	t4_read_cimq_cfg(adap, base, size, thres);
+
+	seq_printf(seq,
+		   "Queue  Base  Size Thres RdPtr WrPtr  SOP  EOP Avail\n");
+	for (i = 0; i < CIM_NUM_IBQ; i++, p += 4)
+		seq_printf(seq, "%5s %5x %5u %4u %6x  %4x %4u %4u %5u\n",
+			   qname[i], base[i], size[i], thres[i],
+			   G_IBQRDADDR(p[0]), G_IBQWRADDR(p[1]),
+			   G_QUESOPCNT(p[3]), G_QUEEOPCNT(p[3]),
+			   G_QUEREMFLITS(p[2]) * 16);
+	for ( ; i < CIM_NUM_IBQ + CIM_NUM_OBQ; i++, p += 4, wr += 2)
+		seq_printf(seq, "%5s %5x %5u %11x  %4x %4u %4u %5u\n",
+			   qname[i], base[i], size[i],
+			   G_QUERDADDR(p[0]) & 0x3fff, wr[0] - base[i],
+			   G_QUESOPCNT(p[3]), G_QUEEOPCNT(p[3]),
+			   G_QUEREMFLITS(p[2]) * 16);
+	return 0;
+}
+
+static int cim_qcfg_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cim_qcfg_show, inode->i_private);
+}
+
+static const struct file_operations cim_qcfg_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_qcfg_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+static int cimq_show(struct seq_file *seq, void *v, int idx)
+{
+	const u32 *p = v;
+
+	seq_printf(seq, "%#06x: %08x %08x %08x %08x\n", idx * 16, p[0], p[1],
+		   p[2], p[3]);
+	return 0;
+}
+
+static int cim_ibq_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct seq_tab *p;
+	unsigned int qid = (uintptr_t)inode->i_private & 7;
+	struct adapter *adap = inode->i_private - qid;
+
+	p = seq_open_tab(file, CIM_IBQ_SIZE, 4 * sizeof(u32), 0, cimq_show);
+	if (!p)
+		return -ENOMEM;
+
+	ret = t4_read_cim_ibq(adap, qid, (u32 *)p->data, CIM_IBQ_SIZE * 4);
+	if (ret < 0)
+		seq_release_private(inode, file);
+	else
+		ret = 0;
+	return ret;
+}
+
+static const struct file_operations cim_ibq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_ibq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int cim_obq_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct seq_tab *p;
+	unsigned int qid = (uintptr_t)inode->i_private & 7;
+	struct adapter *adap = inode->i_private - qid;
+
+	p = seq_open_tab(file, 6 * CIM_IBQ_SIZE, 4 * sizeof(u32), 0, cimq_show);
+	if (!p)
+		return -ENOMEM;
+
+	ret = t4_read_cim_obq(adap, qid, (u32 *)p->data, 6 * CIM_IBQ_SIZE * 4);
+	if (ret < 0)
+		seq_release_private(inode, file);
+	else {
+		seq_tab_trim(p, ret / 4);
+		ret = 0;
+	}
+	return ret;
+}
+
+static const struct file_operations cim_obq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cim_obq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+struct field_desc {
+	const char *name;
+	unsigned int start;
+	unsigned int width;
+};
+
+static void field_desc_show(struct seq_file *seq, u64 v,
+			    const struct field_desc *p)
+{
+	char buf[32];
+	int line_size = 0;
+
+	while (p->name) {
+		u64 mask = (1ULL << p->width) - 1;
+		int len = scnprintf(buf, sizeof(buf), "%s: %llu", p->name,
+				    ((unsigned long long)v >> p->start) & mask);
+
+		if (line_size + len >= 79) {
+			line_size = 8;
+			seq_puts(seq, "\n        ");
+		}
+		seq_printf(seq, "%s ", buf);
+		line_size += len + 1;
+		p++;
+	}
+	seq_putc(seq, '\n');
+}
+
+static struct field_desc tp_la0[] = {
+	{ "RcfOpCodeOut", 60, 4 },
+	{ "State", 56, 4 },
+	{ "WcfState", 52, 4 },
+	{ "RcfOpcSrcOut", 50, 2 },
+	{ "CRxError", 49, 1 },
+	{ "ERxError", 48, 1 },
+	{ "SanityFailed", 47, 1 },
+	{ "SpuriousMsg", 46, 1 },
+	{ "FlushInputMsg", 45, 1 },
+	{ "FlushInputCpl", 44, 1 },
+	{ "RssUpBit", 43, 1 },
+	{ "RssFilterHit", 42, 1 },
+	{ "Tid", 32, 10 },
+	{ "InitTcb", 31, 1 },
+	{ "LineNumber", 24, 7 },
+	{ "Emsg", 23, 1 },
+	{ "EdataOut", 22, 1 },
+	{ "Cmsg", 21, 1 },
+	{ "CdataOut", 20, 1 },
+	{ "EreadPdu", 19, 1 },
+	{ "CreadPdu", 18, 1 },
+	{ "TunnelPkt", 17, 1 },
+	{ "RcfPeerFin", 16, 1 },
+	{ "RcfReasonOut", 12, 4 },
+	{ "TxCchannel", 10, 2 },
+	{ "RcfTxChannel", 8, 2 },
+	{ "RxEchannel", 6, 2 },
+	{ "RcfRxChannel", 5, 1 },
+	{ "RcfDataOutSrdy", 4, 1 },
+	{ "RxDvld", 3, 1 },
+	{ "RxOoDvld", 2, 1 },
+	{ "RxCongestion", 1, 1 },
+	{ "TxCongestion", 0, 1 },
+	{ NULL }
+};
+
+static int tp_la_show(struct seq_file *seq, void *v, int idx)
+{
+	const u64 *p = v;
+
+	field_desc_show(seq, *p, tp_la0);
+	return 0;
+}
+
+static int tp_la_show2(struct seq_file *seq, void *v, int idx)
+{
+	const u64 *p = v;
+
+	if (idx)
+		seq_putc(seq, '\n');
+	field_desc_show(seq, p[0], tp_la0);
+	if (idx < (TPLA_SIZE / 2 - 1) || p[1] != ~0ULL)
+		field_desc_show(seq, p[1], tp_la0);
+	return 0;
+}
+
+static int tp_la_show3(struct seq_file *seq, void *v, int idx)
+{
+	static struct field_desc tp_la1[] = {
+		{ "CplCmdIn", 56, 8 },
+		{ "CplCmdOut", 48, 8 },
+		{ "ESynOut", 47, 1 },
+		{ "EAckOut", 46, 1 },
+		{ "EFinOut", 45, 1 },
+		{ "ERstOut", 44, 1 },
+		{ "SynIn", 43, 1 },
+		{ "AckIn", 42, 1 },
+		{ "FinIn", 41, 1 },
+		{ "RstIn", 40, 1 },
+		{ "DataIn", 39, 1 },
+		{ "DataInVld", 38, 1 },
+		{ "PadIn", 37, 1 },
+		{ "RxBufEmpty", 36, 1 },
+		{ "RxDdp", 35, 1 },
+		{ "RxFbCongestion", 34, 1 },
+		{ "TxFbCongestion", 33, 1 },
+		{ "TxPktSumSrdy", 32, 1 },
+		{ "RcfUlpType", 28, 4 },
+		{ "Eread", 27, 1 },
+		{ "Ebypass", 26, 1 },
+		{ "Esave", 25, 1 },
+		{ "Static0", 24, 1 },
+		{ "Cread", 23, 1 },
+		{ "Cbypass", 22, 1 },
+		{ "Csave", 21, 1 },
+		{ "CPktOut", 20, 1 },
+		{ "RxPagePoolFull", 18, 2 },
+		{ "RxLpbkPkt", 17, 1 },
+		{ "TxLpbkPkt", 16, 1 },
+		{ "RxVfValid", 15, 1 },
+		{ "SynLearned", 14, 1 },
+		{ "SetDelEntry", 13, 1 },
+		{ "SetInvEntry", 12, 1 },
+		{ "CpcmdDvld", 11, 1 },
+		{ "CpcmdSave", 10, 1 },
+		{ "RxPstructsFull", 8, 2 },
+		{ "EpcmdDvld", 7, 1 },
+		{ "EpcmdFlush", 6, 1 },
+		{ "EpcmdTrimPrefix", 5, 1 },
+		{ "EpcmdTrimPostfix", 4, 1 },
+		{ "ERssIp4Pkt", 3, 1 },
+		{ "ERssIp6Pkt", 2, 1 },
+		{ "ERssTcpUdpPkt", 1, 1 },
+		{ "ERssFceFipPkt", 0, 1 },
+		{ NULL }
+	};
+	static struct field_desc tp_la2[] = {
+		{ "CplCmdIn", 56, 8 },
+		{ "MpsVfVld", 55, 1 },
+		{ "MpsPf", 52, 3 },
+		{ "MpsVf", 44, 8 },
+		{ "SynIn", 43, 1 },
+		{ "AckIn", 42, 1 },
+		{ "FinIn", 41, 1 },
+		{ "RstIn", 40, 1 },
+		{ "DataIn", 39, 1 },
+		{ "DataInVld", 38, 1 },
+		{ "PadIn", 37, 1 },
+		{ "RxBufEmpty", 36, 1 },
+		{ "RxDdp", 35, 1 },
+		{ "RxFbCongestion", 34, 1 },
+		{ "TxFbCongestion", 33, 1 },
+		{ "TxPktSumSrdy", 32, 1 },
+		{ "RcfUlpType", 28, 4 },
+		{ "Eread", 27, 1 },
+		{ "Ebypass", 26, 1 },
+		{ "Esave", 25, 1 },
+		{ "Static0", 24, 1 },
+		{ "Cread", 23, 1 },
+		{ "Cbypass", 22, 1 },
+		{ "Csave", 21, 1 },
+		{ "CPktOut", 20, 1 },
+		{ "RxPagePoolFull", 18, 2 },
+		{ "RxLpbkPkt", 17, 1 },
+		{ "TxLpbkPkt", 16, 1 },
+		{ "RxVfValid", 15, 1 },
+		{ "SynLearned", 14, 1 },
+		{ "SetDelEntry", 13, 1 },
+		{ "SetInvEntry", 12, 1 },
+		{ "CpcmdDvld", 11, 1 },
+		{ "CpcmdSave", 10, 1 },
+		{ "RxPstructsFull", 8, 2 },
+		{ "EpcmdDvld", 7, 1 },
+		{ "EpcmdFlush", 6, 1 },
+		{ "EpcmdTrimPrefix", 5, 1 },
+		{ "EpcmdTrimPostfix", 4, 1 },
+		{ "ERssIp4Pkt", 3, 1 },
+		{ "ERssIp6Pkt", 2, 1 },
+		{ "ERssTcpUdpPkt", 1, 1 },
+		{ "ERssFceFipPkt", 0, 1 },
+		{ NULL }
+	};
+	const u64 *p = v;
+
+	if (idx)
+		seq_putc(seq, '\n');
+	field_desc_show(seq, p[0], tp_la0);
+	if (idx < (TPLA_SIZE / 2 - 1) || p[1] != ~0ULL)
+		field_desc_show(seq, p[1], (p[0] & BIT(17)) ? tp_la2 : tp_la1);
+	return 0;
+}
+
+static int tp_la_open(struct inode *inode, struct file *file)
+{
+	struct seq_tab *p;
+	struct adapter *adap = inode->i_private;
+
+	switch (G_DBGLAMODE(t4_read_reg(adap, A_TP_DBG_LA_CONFIG))) {
+	case 2:
+		p = seq_open_tab(file, TPLA_SIZE / 2, 2 * sizeof(u64), 0,
+				 tp_la_show2);
+		break;
+	case 3:
+		p = seq_open_tab(file, TPLA_SIZE / 2, 2 * sizeof(u64), 0,
+				 tp_la_show3);
+		break;
+	default:
+		p = seq_open_tab(file, TPLA_SIZE, sizeof(u64), 0, tp_la_show);
+	}
+	if (!p)
+		return -ENOMEM;
+
+	t4_tp_read_la(adap, (u64 *)p->data, NULL);
+	return 0;
+}
+
+static ssize_t tp_la_write(struct file *file, const char __user *buf,
+			   size_t count, loff_t *pos)
+{
+	int err;
+	char s[32];
+	unsigned long val;
+	size_t size = min(sizeof(s) - 1, count);
+	struct adapter *adap = file->f_path.dentry->d_inode->i_private;
+
+	if (copy_from_user(s, buf, size))
+		return -EFAULT;
+	s[size] = '\0';
+	err = strict_strtoul(s, 0, &val);
+	if (err)
+		return err;
+	if (val > 0xffff)
+		return -EINVAL;
+	adap->params.tp.la_mask = val << 16;
+	t4_set_reg_field(adap, A_TP_DBG_LA_CONFIG, 0xffff0000U,
+			 adap->params.tp.la_mask);
+	return count;
+}
+
+static const struct file_operations tp_la_fops = {
+	.owner   = THIS_MODULE,
+	.open    = tp_la_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private,
+	.write   = tp_la_write
+};
+
+static int ulprx_la_show(struct seq_file *seq, void *v, int idx)
+{
+	const u32 *p = v;
+
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "      Pcmd        Type   Message"
+			 "                Data\n");
+	else
+		seq_printf(seq, "%08x%08x  %4x  %08x  %08x%08x%08x%08x\n",
+			   p[1], p[0], p[2], p[3], p[7], p[6], p[5], p[4]);
+	return 0;
+}
+
+static int ulprx_la_open(struct inode *inode, struct file *file)
+{
+	struct seq_tab *p;
+	struct adapter *adap = inode->i_private;
+
+	p = seq_open_tab(file, ULPRX_LA_SIZE, 8 * sizeof(u32), 1,
+			 ulprx_la_show);
+	if (!p)
+		return -ENOMEM;
+
+	t4_ulprx_read_la(adap, (u32 *)p->data);
+	return 0;
+}
+
+static const struct file_operations ulprx_la_fops = {
+	.owner   = THIS_MODULE,
+	.open    = ulprx_la_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+/*
+ * Format a value in a unit that differs from the value's native unit by the
+ * given factor.
+ */
+static char *unit_conv(char *buf, size_t len, unsigned int val,
+		       unsigned int factor)
+{
+	unsigned int rem = val % factor;
+
+	if (rem == 0)
+		snprintf(buf, len, "%u", val / factor);
+	else {
+		while (rem % 10 == 0)
+			rem /= 10;
+		snprintf(buf, len, "%u.%u", val / factor, rem);
+	}
+	return buf;
+}
+
+static int clk_show(struct seq_file *seq, void *v)
+{
+	char buf[32];
+	struct adapter *adap = seq->private;
+	unsigned int cclk_ps = 1000000000 / adap->params.vpd.cclk;  /* in ps */
+	u32 res = t4_read_reg(adap, A_TP_TIMER_RESOLUTION);
+	unsigned int tre = G_TIMERRESOLUTION(res);
+	unsigned int dack_re = G_DELAYEDACKRESOLUTION(res);
+	unsigned long long tp_tick_us = (cclk_ps << tre) / 1000000; /* in us */
+
+	seq_printf(seq, "Core clock period: %s ns\n",
+		   unit_conv(buf, sizeof(buf), cclk_ps, 1000));
+	seq_printf(seq, "TP timer tick: %s us\n",
+		   unit_conv(buf, sizeof(buf), (cclk_ps << tre), 1000000));
+	seq_printf(seq, "TCP timestamp tick: %s us\n",
+		   unit_conv(buf, sizeof(buf),
+			     (cclk_ps << G_TIMESTAMPRESOLUTION(res)), 1000000));
+	seq_printf(seq, "DACK tick: %s us\n",
+		   unit_conv(buf, sizeof(buf), (cclk_ps << dack_re), 1000000));
+	seq_printf(seq, "DACK timer: %u us\n",
+		   ((cclk_ps << dack_re) / 1000000) *
+		   t4_read_reg(adap, A_TP_DACK_TIMER));
+	seq_printf(seq, "Retransmit min: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_RXT_MIN));
+	seq_printf(seq, "Retransmit max: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_RXT_MAX));
+	seq_printf(seq, "Persist timer min: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_PERS_MIN));
+	seq_printf(seq, "Persist timer max: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_PERS_MAX));
+	seq_printf(seq, "Keepalive idle timer: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_KEEP_IDLE));
+	seq_printf(seq, "Keepalive interval: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_KEEP_INTVL));
+	seq_printf(seq, "Initial SRTT: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_INIT_SRTT));
+	seq_printf(seq, "FINWAIT2 timer: %llu us\n",
+		   tp_tick_us * t4_read_reg(adap, A_TP_FINWAIT2_TIMER));
+
+	return 0;
+}
+
+static int clk_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, clk_show, inode->i_private);
+}
+
+static const struct file_operations clk_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = clk_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * Firmware Device Log dump.
+ * =========================
+ */
+const char *devlog_level_strings[] = {
+	[FW_DEVLOG_LEVEL_EMERG]		= "EMERG",
+	[FW_DEVLOG_LEVEL_CRIT]		= "CRIT",
+	[FW_DEVLOG_LEVEL_ERR]		= "ERR",
+	[FW_DEVLOG_LEVEL_NOTICE]	= "NOTICE",
+	[FW_DEVLOG_LEVEL_INFO]		= "INFO",
+	[FW_DEVLOG_LEVEL_DEBUG]		= "DEBUG"
+};
+
+const char *devlog_facility_strings[] = {
+	[FW_DEVLOG_FACILITY_CORE]	= "CORE",
+	[FW_DEVLOG_FACILITY_SCHED]	= "SCHED",
+	[FW_DEVLOG_FACILITY_TIMER]	= "TIMER",
+	[FW_DEVLOG_FACILITY_RES]	= "RES",
+	[FW_DEVLOG_FACILITY_HW]		= "HW",
+	[FW_DEVLOG_FACILITY_FLR]	= "FLR",
+	[FW_DEVLOG_FACILITY_DMAQ]	= "DMAQ",
+	[FW_DEVLOG_FACILITY_PHY]	= "PHY",
+	[FW_DEVLOG_FACILITY_MAC]	= "MAC",
+	[FW_DEVLOG_FACILITY_PORT]	= "PORT",
+	[FW_DEVLOG_FACILITY_VI]		= "VI",
+	[FW_DEVLOG_FACILITY_FILTER]	= "FILTER",
+	[FW_DEVLOG_FACILITY_ACL]	= "ACL",
+	[FW_DEVLOG_FACILITY_TM]		= "TM",
+	[FW_DEVLOG_FACILITY_QFC]	= "QFC",
+	[FW_DEVLOG_FACILITY_DCB]	= "DCB",
+	[FW_DEVLOG_FACILITY_ETH]	= "ETH",
+	[FW_DEVLOG_FACILITY_OFLD]	= "OFLD",
+	[FW_DEVLOG_FACILITY_RI]		= "RI",
+	[FW_DEVLOG_FACILITY_ISCSI]	= "ISCSI",
+	[FW_DEVLOG_FACILITY_FCOE]	= "FCOE",
+	[FW_DEVLOG_FACILITY_FOISCSI]	= "FOISCSI",
+	[FW_DEVLOG_FACILITY_FOFCOE]	= "FOFCOE"
+};
+
+/*
+ * Information gathered by Device Log Open routine for the display routine.
+ */
+struct devlog_info {
+	unsigned int nentries;		/* number of entries in log[] */
+	unsigned int first;		/* first [temporal] entry in log[] */
+	struct fw_devlog_e log[0];	/* Firmware Device Log */
+};
+
+/*
+ * Dump a Firmaware Device Log entry.
+ */
+static int devlog_show(struct seq_file *seq, void *v)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_printf(seq, "%10s  %15s  %8s  %8s  %s\n",
+			   "Seq#", "Tstamp", "Level", "Facility", "Message");
+	else {
+		struct devlog_info *dinfo = seq->private;
+		int fidx = (uintptr_t)v - 2;
+		unsigned long index;
+		struct fw_devlog_e *e;
+
+		/*
+		 * Get a pointer to the log entry to display.  Skip unused log
+		 * entries.
+		 */
+		index = dinfo->first + fidx;
+		if (index >= dinfo->nentries)
+			index -= dinfo->nentries;
+		e = &dinfo->log[index];
+		if (e->timestamp == 0)
+			return 0;
+
+		/*
+		 * Print the message.  This depends on the firmware using
+		 * exactly the same formating strings as the kernel so we may
+		 * eventually have to put a format interpreter in here ...
+		 */
+		seq_printf(seq, "%10d  %15llu  %8s  %8s  ",
+			   e->seqno, e->timestamp,
+			   (e->level < ARRAY_SIZE(devlog_level_strings)
+			    ? devlog_level_strings[e->level]
+			    : "UNKNOWN"),
+			   (e->facility < ARRAY_SIZE(devlog_facility_strings)
+			    ? devlog_facility_strings[e->facility]
+			    : "UNKNOWN"));
+		seq_printf(seq, e->fmt, e->params[0], e->params[1],
+			   e->params[2], e->params[3], e->params[4],
+			   e->params[5], e->params[6], e->params[7]);
+	}
+
+	return 0;
+}
+
+/*
+ * Sequential File Operations for Device Log.
+ */
+static inline void *devlog_get_idx(struct devlog_info *dinfo, loff_t pos)
+{
+	if (pos > dinfo->nentries)
+		return NULL;
+
+	return (void *)(uintptr_t)(pos + 1);
+}
+
+static void *devlog_start(struct seq_file *seq, loff_t *pos)
+{
+	struct devlog_info *dinfo = seq->private;
+
+	return (*pos
+		? devlog_get_idx(dinfo, *pos)
+		: SEQ_START_TOKEN);
+}
+
+static void *devlog_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	struct devlog_info *dinfo = seq->private;
+
+	(*pos)++;
+	return devlog_get_idx(dinfo, *pos);
+}
+
+static void devlog_stop(struct seq_file *seq, void *v)
+{
+}
+
+static const struct seq_operations devlog_seq_ops = {
+	.start = devlog_start,
+	.next  = devlog_next,
+	.stop  = devlog_stop,
+	.show  = devlog_show
+};
+
+/*
+ * Set up for reading the firmware's device log.  We read the entire log here
+ * and then display it incrementally in devlog_show().
+ */
+static int devlog_open(struct inode *inode, struct file *file)
+{
+	struct adapter *adap = inode->i_private;
+	struct devlog_params *dparams = &adap->params.devlog;
+	struct devlog_info *dinfo;
+	unsigned int index;
+	u64 ftstamp;
+	int ret;
+
+	/*
+	 * If we don't know where the log is we can't do anything.
+	 */
+	if (dparams->start == 0)
+		return -ENXIO;
+
+	/*
+	 * Allocate the space to read in the firmware's device log and set up
+	 * for the iterated call to our display function.
+	 */
+	dinfo = __seq_open_private(file, &devlog_seq_ops,
+				   sizeof *dinfo + dparams->size);
+	if (dinfo == NULL)
+		return -ENOMEM;
+
+	/*
+	 * Record the basic log buffer information and read in the raw log.
+	 */
+	dinfo->nentries = (dparams->size / sizeof (struct fw_devlog_e));
+	dinfo->first = 0;
+	ret = t4_memory_read(adap, dparams->memtype, dparams->start,
+			  dparams->size, (__be32 *)dinfo->log);
+	if (ret) {
+		seq_release_private(inode, file);
+		return ret;
+	}
+
+	/*
+	 * Translate log multi-byte integral elements into host native format
+	 * and determine where the first entry in the log is.
+	 */
+	for (ftstamp = ~0ULL, index = 0; index < dinfo->nentries; index++) {
+		struct fw_devlog_e *e = &dinfo->log[index];
+		int i;
+
+		if (e->timestamp == 0)
+			continue;
+
+		e->timestamp = be64_to_cpu(e->timestamp);
+		e->seqno = be32_to_cpu(e->seqno);
+		for (i = 0; i < 8; i++)
+			e->params[i] = be32_to_cpu(e->params[i]);
+
+		if (e->timestamp < ftstamp) {
+			ftstamp = e->timestamp;
+			dinfo->first = index;
+		}
+	}
+
+	return 0;
+}
+
+static const struct file_operations devlog_fops = {
+	.owner   = THIS_MODULE,
+	.open    = devlog_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int mbox_show(struct seq_file *seq, void *v)
+{
+	static const char *owner[] = { "none", "FW", "driver", "unknown" };
+
+	int i;
+	unsigned int mbox = (uintptr_t)seq->private & 7;
+	struct adapter *adap = seq->private - mbox;
+	void __iomem *addr = adap->regs + PF_REG(mbox, A_CIM_PF_MAILBOX_DATA);
+	void __iomem *ctrl = addr + MBOX_LEN;
+
+	i = G_MBOWNER(readl(ctrl));
+	seq_printf(seq, "mailbox owned by %s\n\n", owner[i]);
+
+	for (i = 0; i < MBOX_LEN; i += 8)
+		seq_printf(seq, "%016llx\n",
+			   (unsigned long long)readq(addr + i));
+	return 0;
+}
+
+static int mbox_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mbox_show, inode->i_private);
+}
+
+static ssize_t mbox_write(struct file *file, const char __user *buf,
+			  size_t count, loff_t *pos)
+{
+	int i;
+	char c = '\n', s[256];
+	unsigned long long data[8];
+	const struct inode *ino;
+	unsigned int mbox;
+	struct adapter *adap;
+	void __iomem *addr;
+	void __iomem *ctrl;
+
+	if (count > sizeof(s) - 1 || !count)
+		return -EINVAL;
+	if (copy_from_user(s, buf, count))
+		return -EFAULT;
+	s[count] = '\0';
+
+	if (sscanf(s, "%llx %llx %llx %llx %llx %llx %llx %llx%c", &data[0],
+		   &data[1], &data[2], &data[3], &data[4], &data[5], &data[6],
+		   &data[7], &c) < 8 || c != '\n')
+		return -EINVAL;
+
+	ino = file->f_path.dentry->d_inode;
+	mbox = (uintptr_t)ino->i_private & 7;
+	adap = ino->i_private - mbox;
+	addr = adap->regs + PF_REG(mbox, A_CIM_PF_MAILBOX_DATA);
+	ctrl = addr + MBOX_LEN;
+
+	if (G_MBOWNER(readl(ctrl)) != X_MBOWNER_PL)
+		return -EBUSY;
+
+	for (i = 0; i < 8; i++)
+		writeq(data[i], addr + 8 * i);
+
+	writel(F_MBMSGVALID | V_MBOWNER(X_MBOWNER_FW), ctrl);
+	return count;
+}
+
+static const struct file_operations mbox_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mbox_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = mbox_write
+};
+
+static int mem_open(struct inode *inode, struct file *file)
+{
+	file->private_data = inode->i_private;
+	return 0;
+}
+
+static ssize_t mem_read(struct file *file, char __user *buf, size_t count,
+			loff_t *ppos)
+{
+	u32 memoffset;
+	loff_t pos = *ppos;
+	loff_t avail = file->f_path.dentry->d_inode->i_size;
+	unsigned int mem = (uintptr_t)file->private_data & 3;
+	struct adapter *adap = file->private_data - mem;
+	__be32 *data;
+
+	if (pos < 0)
+		return -EINVAL;
+	if (pos >= avail)
+		return 0;
+	if (count > avail - pos)
+		count = avail - pos;
+
+	/* Offset into the region of memory which is being accessed
+	 * MEM_EDC0 = 0
+	 * MEM_EDC1 = 1
+	 * MEM_MC   = 2
+	 */
+	memoffset = (mem * ( 5 * 1024 * 1024));
+
+	while (count) {
+		size_t len;
+		int ret, ofst;
+
+		data = t4_alloc_mem(MEMWIN0_APERTURE);
+		if (!data)
+			return -ENOMEM;
+
+		ret = t4_mem_win_read(adap, (pos + memoffset), data);
+		if (ret) {
+			t4_free_mem(data);
+			return ret;
+		}
+
+		ofst = pos % MEMWIN0_APERTURE;
+		len = min(count, (size_t) MEMWIN0_APERTURE - ofst);
+		if (copy_to_user(buf, (u8 *)data + ofst, len)) {
+	                t4_free_mem(data);
+			return -EFAULT;
+		}
+
+		buf += len;
+		pos += len;
+		count -= len;
+	        t4_free_mem(data);
+	}
+	count = pos - *ppos;
+	*ppos = pos;
+	return count;
+}
+
+static const struct file_operations mem_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mem_open,
+	.read    = mem_read,
+	.llseek  = default_llseek,
+};
+
+static ssize_t flash_read(struct file *file, char __user *buf, size_t count,
+			  loff_t *ppos)
+{
+	loff_t pos = *ppos;
+	loff_t avail = file->f_path.dentry->d_inode->i_size;
+	struct adapter *adap = file->private_data;
+
+	if (pos < 0)
+		return -EINVAL;
+	if (pos >= avail)
+		return 0;
+	if (count > avail - pos)
+		count = avail - pos;
+
+	while (count) {
+		size_t len;
+		int ret, ofst;
+		u8 data[256];
+
+		ofst = pos & 3;
+		len = min(count + ofst, sizeof(data));
+		ret = t4_read_flash(adap, pos - ofst, (len + 3) / 4,
+				    (u32 *)data, 1);
+		if (ret)
+			return ret;
+
+		len -= ofst;
+		if (copy_to_user(buf, data + ofst, len))
+			return -EFAULT;
+
+		buf += len;
+		pos += len;
+		count -= len;
+	}
+	count = pos - *ppos;
+	*ppos = pos;
+	return count;
+}
+
+static const struct file_operations flash_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mem_open,
+	.read    = flash_read,
+};
+
+static inline void tcamxy2valmask(u64 x, u64 y, u8 *addr, u64 *mask)
+{
+	*mask = x | y;
+	y = cpu_to_be64(y);
+	memcpy(addr, (char *)&y + 2, ETH_ALEN);
+}
+
+static int mps_tcam_show(struct seq_file *seq, void *v)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "Idx  Ethernet address     Mask     Vld Ports PF"
+			 "  VF Repl P0 P1 P2 P3  ML\n");
+	else {
+		u64 mask;
+		u8 addr[ETH_ALEN];
+		struct adapter *adap = seq->private;
+		unsigned int idx = (uintptr_t)v - 2;
+		u64 tcamy = t4_read_reg64(adap, MPS_CLS_TCAM_Y_L(idx));
+		u64 tcamx = t4_read_reg64(adap, MPS_CLS_TCAM_X_L(idx));
+		u32 cls_lo = t4_read_reg(adap, MPS_CLS_SRAM_L(idx));
+		u32 cls_hi = t4_read_reg(adap, MPS_CLS_SRAM_H(idx));
+
+		if (tcamx & tcamy) {
+			seq_printf(seq, "%3u         -\n", idx);
+			goto out;
+		}
+
+		tcamxy2valmask(tcamx, tcamy, addr, &mask);
+		seq_printf(seq, "%3u %02x:%02x:%02x:%02x:%02x:%02x %012llx"
+			   "%3c   %#x%4u%4d%4c%4u%3u%3u%3u %#x\n",
+			   idx, addr[0], addr[1], addr[2], addr[3], addr[4],
+			   addr[5], (unsigned long long)mask,
+			   (cls_lo & F_SRAM_VLD) ? 'Y' : 'N', G_PORTMAP(cls_hi),
+			   G_PF(cls_lo),
+			   (cls_lo & F_VF_VALID) ? G_VF(cls_lo) : -1,
+			   (cls_lo & F_REPLICATE) ? 'Y' : 'N',
+			   G_SRAM_PRIO0(cls_lo), G_SRAM_PRIO1(cls_lo),
+			   G_SRAM_PRIO2(cls_lo), G_SRAM_PRIO3(cls_lo),
+			   (cls_lo >> S_MULTILISTEN0) & 0xf);
+	}
+out:	return 0;
+}
+
+static inline void *mps_tcam_get_idx(loff_t pos)
+{
+	return (pos <= NUM_MPS_CLS_SRAM_L_INSTANCES 
+		? (void *)(uintptr_t)(pos + 1)
+		: NULL);
+}
+
+static void *mps_tcam_start(struct seq_file *seq, loff_t *pos)
+{
+	return *pos ? mps_tcam_get_idx(*pos) : SEQ_START_TOKEN;
+}
+
+static void *mps_tcam_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	++*pos;
+	return mps_tcam_get_idx(*pos);
+}
+
+static void mps_tcam_stop(struct seq_file *seq, void *v)
+{
+}
+
+static const struct seq_operations mps_tcam_seq_ops = {
+	.start = mps_tcam_start,
+	.next  = mps_tcam_next,
+	.stop  = mps_tcam_stop,
+	.show  = mps_tcam_show
+};
+
+static int mps_tcam_open(struct inode *inode, struct file *file)
+{
+	int res = seq_open(file, &mps_tcam_seq_ops);
+
+	if (!res) {
+		struct seq_file *seq = file->private_data;
+		seq->private = inode->i_private;
+	}
+	return res;
+}
+
+static const struct file_operations mps_tcam_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mps_tcam_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int sge_qinfo_show(struct seq_file *seq, void *v)
+{
+	struct adapter *adap = seq->private;
+	int eth_entries = DIV_ROUND_UP(adap->sge.ethqsets, 4);
+	int toe_entries = DIV_ROUND_UP(adap->sge.ofldqsets, 4);
+	int rdma_entries = DIV_ROUND_UP(adap->sge.rdmaqs, 4);
+	int iscsi_entries = DIV_ROUND_UP(adap->sge.niscsiq, 4);
+	int ctrl_entries = DIV_ROUND_UP(MAX_CTRL_QUEUES, 4);
+	int i, r = (uintptr_t)v - 1;
+
+	if (r)
+		seq_putc(seq, '\n');
+
+#define S3(fmt_spec, s, v) \
+	seq_printf(seq, "%-12s", s); \
+	for (i = 0; i < n; ++i) \
+		seq_printf(seq, " %16" fmt_spec, v); \
+		seq_putc(seq, '\n');
+#define S(s, v) S3("s", s, v)
+#define T(s, v) S3("u", s, tx[i].v)
+#define R(s, v) S3("u", s, rx[i].v)
+
+	if (r < eth_entries) {
+		const struct sge_eth_rxq *rx = &adap->sge.ethrxq[r * 4];
+		const struct sge_eth_txq *tx = &adap->sge.ethtxq[r * 4];
+		int n = min(4, adap->sge.ethqsets - 4 * r);
+
+		S("QType:", "Ethernet");
+		S("Interface:",
+		  rx[i].rspq.netdev ? rx[i].rspq.netdev->name : "N/A");
+		T("TxQ ID:", q.cntxt_id);
+		T("TxQ CIDX:", q.cidx);
+		T("TxQ PIDX:", q.pidx);
+		T("TxQ size:", q.size);
+		T("TxQ inuse:", q.in_use);
+		R("RspQ ID:", rspq.abs_id);
+		R("RspQ size:", rspq.size);
+		R("RspQE size:", rspq.iqe_len);
+		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
+		S3("u", "Intr pktcnt:",
+		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
+		R("FL ID:", fl.cntxt_id);
+		R("FL size:", fl.size - 8);
+		R("FL avail:", fl.avail);
+	} else if ((r -= eth_entries) < toe_entries) {
+		const struct sge_ofld_rxq *rx = &adap->sge.ofldrxq[r * 4];
+		const struct sge_ofld_txq *tx = &adap->sge.ofldtxq[r * 4];
+		int n = min(4, adap->sge.ofldqsets - 4 * r);
+
+		S("QType:", "TOE");
+		T("TxQ ID:", q.cntxt_id);
+		T("TxQ CIDX:", q.cidx);
+		T("TxQ PIDX:", q.pidx);
+		T("TxQ size:", q.size);
+		T("TxQ inuse:", q.in_use);
+		R("RspQ ID:", rspq.abs_id);
+		R("RspQ size:", rspq.size);
+		R("RspQE size:", rspq.iqe_len);
+		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
+		S3("u", "Intr pktcnt:",
+		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
+		R("FL ID:", fl.cntxt_id);
+		R("FL size:", fl.size - 8);
+		R("FL avail:", fl.avail);
+	} else if ((r -= toe_entries) < rdma_entries) {
+		const struct sge_ofld_rxq *rx = &adap->sge.rdmarxq[r * 4];
+		int n = min(4, adap->sge.rdmaqs - 4 * r);
+
+		S("QType:", "RDMA");
+		R("RspQ ID:", rspq.abs_id);
+		R("RspQ size:", rspq.size);
+		R("RspQE size:", rspq.iqe_len);
+		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
+		S3("u", "Intr pktcnt:",
+		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
+		R("FL ID:", fl.cntxt_id);
+		R("FL size:", fl.size - 8);
+		R("FL avail:", fl.avail);
+	} else if ((r -= rdma_entries) < iscsi_entries) {
+		const struct sge_ofld_rxq *rx = &adap->sge.iscsirxq[r * 4];
+		int n = min(4, adap->sge.niscsiq - 4 * r);
+
+		S("QType:", "iSCSI");
+		R("RspQ ID:", rspq.abs_id);
+		R("RspQ size:", rspq.size);
+		R("RspQE size:", rspq.iqe_len);
+		S3("u", "Intr delay:", qtimer_val(adap, &rx[i].rspq));
+		S3("u", "Intr pktcnt:",
+		   adap->sge.counter_val[rx[i].rspq.pktcnt_idx]);
+		R("FL ID:", fl.cntxt_id);
+		R("FL size:", fl.size - 8);
+		R("FL avail:", fl.avail);
+	} else if ((r -= iscsi_entries) < ctrl_entries) {
+		const struct sge_ctrl_txq *tx = &adap->sge.ctrlq[r * 4];
+		int n = min(4, adap->params.nports - 4 * r);
+
+		S("QType:", "Control");
+		T("TxQ ID:", q.cntxt_id);
+		T("TxQ CIDX:", q.cidx);
+		T("TxQ PIDX:", q.pidx);
+		T("TxQ size:", q.size);
+		T("TxQ inuse:", q.in_use);
+	} else if ((r -= ctrl_entries) == 0) {
+		const struct sge_rspq *evtq = &adap->sge.fw_evtq;
+
+		seq_printf(seq, "%-12s %16s\n", "QType:", "FW event queue");
+		seq_printf(seq, "%-12s %16u\n", "RspQ ID:", evtq->abs_id);
+		seq_printf(seq, "%-12s %16u\n", "Intr delay:",
+			   qtimer_val(adap, evtq));
+		seq_printf(seq, "%-12s %16u\n", "Intr pktcnt:",
+			   adap->sge.counter_val[evtq->pktcnt_idx]);
+	}
+#undef R
+#undef T
+#undef S
+#undef S3
+	return 0;
+}
+
+static int sge_queue_entries(const struct adapter *adap)
+{
+	return DIV_ROUND_UP(adap->sge.ethqsets, 4) +
+	       DIV_ROUND_UP(adap->sge.ofldqsets, 4) +
+	       DIV_ROUND_UP(adap->sge.rdmaqs, 4) +
+	       DIV_ROUND_UP(adap->sge.niscsiq, 4) +
+	       DIV_ROUND_UP(MAX_CTRL_QUEUES, 4) + 1;
+}
+
+static void *sge_queue_start(struct seq_file *seq, loff_t *pos)
+{
+	int entries = sge_queue_entries(seq->private);
+
+	return *pos < entries ? (void *)((uintptr_t)*pos + 1) : NULL;
+}
+
+static void sge_queue_stop(struct seq_file *seq, void *v)
+{
+}
+
+static void *sge_queue_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	int entries = sge_queue_entries(seq->private);
+
+	++*pos;
+	return *pos < entries ? (void *)((uintptr_t)*pos + 1) : NULL;
+}
+
+static const struct seq_operations sge_qinfo_seq_ops = {
+	.start = sge_queue_start,
+	.next  = sge_queue_next,
+	.stop  = sge_queue_stop,
+	.show  = sge_qinfo_show
+};
+
+static int sge_qinfo_open(struct inode *inode, struct file *file)
+{
+	int res = seq_open(file, &sge_qinfo_seq_ops);
+
+	if (!res) {
+		struct seq_file *seq = file->private_data;
+		seq->private = inode->i_private;
+	}
+	return res;
+}
+
+static const struct file_operations sge_qinfo_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = sge_qinfo_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int intr_holdoff_show(struct seq_file *seq, void *v)
+{
+
+	struct adapter *adap = seq->private;
+	struct sge *s = &adap->sge;
+	int i;
+
+	for (i=0; i < SGE_NTIMERS; i ++)
+		seq_printf(seq, "%u ", s->timer_val[i]);
+	seq_printf(seq, "\n");
+
+	return 0;
+}
+
+static int intr_holdoff_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, intr_holdoff_show, inode->i_private);
+}
+
+static const struct file_operations intr_holdoff_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = intr_holdoff_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int intr_cnt_show(struct seq_file *seq, void *v)
+{
+
+	struct adapter *adap = seq->private;
+	struct sge *s = &adap->sge;
+	int i;
+
+	for (i=0; i < SGE_NCOUNTERS; i ++)
+		seq_printf(seq, "%u ", s->counter_val[i]);
+	seq_printf(seq, "\n");
+
+	return 0;
+}
+
+static int intr_cnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, intr_cnt_show, inode->i_private);
+}
+
+static const struct file_operations intr_cnt_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = intr_cnt_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int blocked_fl_open(struct inode *inode, struct file *file)
+{
+	file->private_data = inode->i_private;
+	return 0;
+}
+
+static ssize_t blocked_fl_read(struct file *filp, char __user *ubuf,
+			       size_t count, loff_t *ppos)
+{
+	int len;
+	const struct adapter *adap = filp->private_data;
+	char buf[(MAX_EGRQ + 3) / 4 + MAX_EGRQ / 32 + 2]; /* includes ,/\n/\0 */
+
+	len = bitmap_scnprintf(buf, sizeof(buf) - 1, adap->sge.blocked_fl,
+			       MAX_EGRQ);
+	len += sprintf(buf + len, "\n");
+	return simple_read_from_buffer(ubuf, count, ppos, buf, len);
+}
+
+static ssize_t blocked_fl_write(struct file *filp, const char __user *ubuf,
+				size_t count, loff_t *ppos)
+{
+	int err;
+	DECLARE_BITMAP(t, MAX_EGRQ);
+	struct adapter *adap = filp->private_data;
+
+	err = bitmap_parse_user(ubuf, count, t, MAX_EGRQ);
+	if (err)
+		return err;
+
+	bitmap_copy(adap->sge.blocked_fl, t, MAX_EGRQ);
+	return count;
+}
+
+static const struct file_operations blocked_fl_fops = {
+	.owner   = THIS_MODULE,
+	.open    = blocked_fl_open,
+	.read    = blocked_fl_read,
+	.write   = blocked_fl_write,
+	.llseek  = generic_file_llseek,
+};
+
+#define DMABUF 1
+#if DMABUF
+
+#define DMABUF_SZ (64 * 1024)
+
+static ssize_t dma_read(struct file *file, char __user *buf, size_t count,
+			loff_t *ppos)
+{
+	const struct adapter *adap = file->private_data;
+
+	return simple_read_from_buffer(buf, count, ppos, adap->dma_virt,
+				       DMABUF_SZ);
+}
+
+static ssize_t dma_write(struct file *file, const char __user *buf,
+			 size_t count, loff_t *ppos)
+{
+	const struct adapter *adap = file->private_data;
+	loff_t pos = *ppos;
+	size_t avail = DMABUF_SZ;
+
+	file->f_path.dentry->d_inode->i_size = avail;
+	if (pos < 0)
+		return -EINVAL;
+	if (pos >= avail)
+		return 0;
+	if (count > avail - pos)
+		count = avail - pos;
+	if (copy_from_user(adap->dma_virt + pos, buf, count))
+		return -EFAULT;
+	*ppos = pos + count;
+	return count;
+}
+
+static const struct file_operations dma_debugfs_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mem_open,
+	.read    = dma_read,
+	.write   = dma_write
+};
+#endif
+
+#ifdef T4_TRACE
+static void __devinit alloc_trace_bufs(struct adapter *adap)
+{
+	int i;
+	char s[32];
+
+	for (i = 0; i < ARRAY_SIZE(adap->tb); ++i) {
+		sprintf(s, "sge_q%d", i);
+		adap->tb[i] = t4_trace_alloc(adap->debugfs_root, s, 512);
+	}
+}
+
+static void free_trace_bufs(struct adapter *adap)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(adap->tb); ++i)
+		t4_trace_free(adap->tb[i]);
+}
+#else
+# define alloc_trace_bufs(adapter)
+# define free_trace_bufs(adapter)
+#endif
+
+static void __devinit set_debugfs_file_size(struct dentry *de, loff_t size)
+{
+	if (!IS_ERR(de) && de->d_inode)
+		de->d_inode->i_size = size;
+}
+
+static void __devinit add_debugfs_mem(struct adapter *adap, const char *name,
+				      unsigned int idx, unsigned int size_mb)
+{
+	struct dentry *de;
+
+	de = debugfs_create_file(name, S_IRUSR, adap->debugfs_root,
+				 (void *)adap + idx, &mem_debugfs_fops);
+	set_debugfs_file_size(de, size_mb << 20);
+}
+
+struct cxgb4_debugfs_entry {
+	const char *name;
+	const struct file_operations *ops;
+	mode_t mode;
+	unsigned char data;
+};
+
+static int __devinit setup_debugfs(struct adapter *adap)
+{
+	static struct cxgb4_debugfs_entry debugfs_files[] = {
+		{ "blocked_fl", &blocked_fl_fops, S_IRUSR | S_IWUSR, 0 },
+		{ "cim_la", &cim_la_fops, S_IRUSR, 0 },
+		{ "cim_pif_la", &cim_pif_la_fops, S_IRUSR, 0 },
+		{ "cim_ma_la", &cim_ma_la_fops, S_IRUSR, 0 },
+		{ "cim_qcfg", &cim_qcfg_fops, S_IRUSR, 0 },
+		{ "clk", &clk_debugfs_fops, S_IRUSR, 0 },
+		{ "devlog", &devlog_fops, S_IRUSR, 0 },
+		{ "mbox0", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 0 },
+		{ "mbox1", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 1 },
+		{ "mbox2", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 2 },
+		{ "mbox3", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 3 },
+		{ "mbox4", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 4 },
+		{ "mbox5", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 5 },
+		{ "mbox6", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 6 },
+		{ "mbox7", &mbox_debugfs_fops, S_IRUSR | S_IWUSR, 7 },
+		{ "mps_tcam", &mps_tcam_debugfs_fops, S_IRUSR, 0 },
+		{ "ibq_tp0",  &cim_ibq_fops, S_IRUSR, 0 },
+		{ "ibq_tp1",  &cim_ibq_fops, S_IRUSR, 1 },
+		{ "ibq_ulp",  &cim_ibq_fops, S_IRUSR, 2 },
+		{ "ibq_sge0", &cim_ibq_fops, S_IRUSR, 3 },
+		{ "ibq_sge1", &cim_ibq_fops, S_IRUSR, 4 },
+		{ "ibq_ncsi", &cim_ibq_fops, S_IRUSR, 5 },
+		{ "obq_ulp0", &cim_obq_fops, S_IRUSR, 0 },
+		{ "obq_ulp1", &cim_obq_fops, S_IRUSR, 1 },
+		{ "obq_ulp2", &cim_obq_fops, S_IRUSR, 2 },
+		{ "obq_ulp3", &cim_obq_fops, S_IRUSR, 3 },
+		{ "obq_sge",  &cim_obq_fops, S_IRUSR, 4 },
+		{ "obq_ncsi", &cim_obq_fops, S_IRUSR, 5 },
+		{ "sge_qinfo", &sge_qinfo_debugfs_fops, S_IRUSR, 0 },
+		{ "tp_la", &tp_la_fops, S_IRUSR, 0 },
+		{ "ulprx_la", &ulprx_la_fops, S_IRUSR, 0 },
+		{ "intr_holdoff", &intr_holdoff_debugfs_fops, S_IRUSR, 0 },
+		{ "intr_cnt", &intr_cnt_debugfs_fops, S_IRUSR, 0 },
+	};
+
+	int i;
+	struct dentry *de;
+
+	if (!adap->debugfs_root)
+		return -1;
+
+	/* debugfs support is best effort */
+	for (i = 0; i < ARRAY_SIZE(debugfs_files); i++)
+		de = debugfs_create_file(debugfs_files[i].name,
+				debugfs_files[i].mode, adap->debugfs_root,
+				(void *)adap + debugfs_files[i].data,
+				debugfs_files[i].ops);
+
+	i = t4_read_reg(adap, A_MA_TARGET_MEM_ENABLE);
+	if (i & F_EDRAM0_ENABLE)
+		add_debugfs_mem(adap, "edc0", MEM_EDC0, 5);
+	if (i & F_EDRAM1_ENABLE)
+		add_debugfs_mem(adap, "edc1", MEM_EDC1, 5);
+	if (i & F_EXT_MEM_ENABLE)
+		add_debugfs_mem(adap, "mc", MEM_MC,
+			G_EXT_MEM_SIZE(t4_read_reg(adap, A_MA_EXT_MEMORY_BAR)));
+
+	de = debugfs_create_file("flash", S_IRUSR, adap->debugfs_root, adap,
+				 &flash_debugfs_fops);
+	set_debugfs_file_size(de, adap->params.sf_size);
+
+#if DMABUF
+	adap->dma_virt = dma_alloc_coherent(adap->pdev_dev, DMABUF_SZ,
+					    &adap->dma_phys,
+				      GFP_KERNEL);
+	if (adap->dma_virt) {
+		printk("DMA buffer at bus address %#llx, virtual 0x%p\n",
+			(unsigned long long)adap->dma_phys, adap->dma_virt);
+		de = debugfs_create_file("dmabuf", 0644, adap->debugfs_root,
+					 adap, &dma_debugfs_fops);
+		set_debugfs_file_size(de, DMABUF_SZ);
+	}
+#endif
+
+	alloc_trace_bufs(adap);
+	return 0;
+}
+
+/*
+ * /proc support
+ */
+
+#define DEFINE_SIMPLE_PROC_FILE(name) \
+static int name##_open(struct inode *inode, struct file *file) \
+{ \
+	return single_open(file, name##_show, PDE(inode)->data); \
+} \
+static const struct file_operations name##_proc_fops = { \
+	.owner   = THIS_MODULE, \
+	.open    = name##_open, \
+	.read    = seq_read, \
+	.llseek  = seq_lseek, \
+	.release = single_release \
+}
+
+static int sge_stats_show(struct seq_file *seq, void *v)
+{
+	struct adapter *adap = seq->private;
+	int eth_entries = DIV_ROUND_UP(adap->sge.ethqsets, 4);
+	int toe_entries = DIV_ROUND_UP(adap->sge.ofldqsets, 4);
+	int rdma_entries = DIV_ROUND_UP(adap->sge.rdmaqs, 4);
+	int iscsi_entries = DIV_ROUND_UP(adap->sge.niscsiq, 4);
+	int ctrl_entries = DIV_ROUND_UP(MAX_CTRL_QUEUES, 4);
+	int i, r = (uintptr_t)v - 1;
+
+	if (r)
+		seq_putc(seq, '\n');
+
+#define S3(fmt_spec, s, v) \
+	seq_printf(seq, "%-12s", s); \
+	for (i = 0; i < n; ++i) \
+		seq_printf(seq, " %16" fmt_spec, v); \
+		seq_putc(seq, '\n');
+#define S(s, v) S3("s", s, v)
+#define T3(fmt_spec, s, v) S3(fmt_spec, s, tx[i].v)
+#define T(s, v) T3("lu", s, v)
+#define R3(fmt_spec, s, v) S3(fmt_spec, s, qs[i].v)
+#define R(s, v) R3("lu", s, v)
+
+	if (r < eth_entries) {
+		const struct sge_eth_rxq *qs = &adap->sge.ethrxq[r * 4];
+		const struct sge_eth_txq *tx = &adap->sge.ethtxq[r * 4];
+		int n = min(4, adap->sge.ethqsets - 4 * r);
+
+		S("QType:", "Ethernet");
+		S("Interface:",
+		  qs[i].rspq.netdev ? qs[i].rspq.netdev->name : "N/A");
+		R("RxPackets:", stats.pkts);
+		R("RxCSO:", stats.rx_cso);
+		R("VLANxtract:", stats.vlan_ex);
+		R("LROmerged:", stats.lro_merged);
+		R("LROpackets:", stats.lro_pkts);
+		R("RxDrops:", stats.rx_drops);
+		T("TSO:", tso);
+		T("TxCSO:", tx_cso);
+		T("VLANins:", vlan_ins);
+		T("TxQFull:", q.stops);
+		T("TxQRestarts:", q.restarts);
+		T("TxMapErr:", mapping_err);
+		T("TxCoalWR:", coal_wr);
+		T("TxCoalPkt:", coal_pkts);
+		R("FLAllocErr:", fl.alloc_failed);
+		R("FLLrgAlcErr:", fl.large_alloc_failed);
+		R("FLstarving:", fl.starving);
+	} else if ((r -= eth_entries) < toe_entries) {
+		const struct sge_ofld_rxq *qs = &adap->sge.ofldrxq[r * 4];
+		const struct sge_ofld_txq *tx = &adap->sge.ofldtxq[r * 4];
+		int n = min(4, adap->sge.ofldqsets - 4 * r);
+
+		S("QType:", "TOE");
+		R("RxPackets:", stats.pkts);
+		R("RxImmPkts:", stats.imm);
+		R("RxNoMem:", stats.nomem);
+		T("TxPkts:", q.txp);
+		T("TxQFull:", q.stops);
+		T("TxQRestarts:", q.restarts);
+		T("TxMapErr:", mapping_err);
+		R("FLAllocErr:", fl.alloc_failed);
+		R("FLLrgAlcErr:", fl.large_alloc_failed);
+		R("FLstarving:", fl.starving);
+	} else if ((r -= toe_entries) < rdma_entries) {
+		const struct sge_ofld_rxq *qs = &adap->sge.rdmarxq[r * 4];
+		int n = min(4, adap->sge.rdmaqs - 4 * r);
+
+		S("QType:", "RDMA");
+		R("RxPackets:", stats.pkts);
+		R("RxImmPkts:", stats.imm);
+		R("RxAN:", stats.an);
+		R("RxNoMem:", stats.nomem);
+		R("FLAllocErr:", fl.alloc_failed);
+		R("FLstarving:", fl.starving);
+	} else if ((r -= rdma_entries) < iscsi_entries) {
+		const struct sge_ofld_rxq *qs = &adap->sge.iscsirxq[r * 4];
+		int n = min(4, adap->sge.niscsiq - 4 * r);
+
+		S("QType:", "iSCSI");
+		R("RxPackets:", stats.pkts);
+		R("RxImmPkts:", stats.imm);
+		R("RxNoMem:", stats.nomem);
+		R("FLAllocErr:", fl.alloc_failed);
+		R("FLstarving:", fl.starving);
+	} else if ((r -= iscsi_entries) < ctrl_entries) {
+		const struct sge_ctrl_txq *tx = &adap->sge.ctrlq[r * 4];
+		int n = min(4, MAX_CTRL_QUEUES - 4 * r);
+
+		S("QType:", "Control");
+		T("TxPkts:", q.txp);
+		T("TxQFull:", q.stops);
+		T("TxQRestarts:", q.restarts);
+	} else if ((r -= ctrl_entries) == 0) {
+		seq_printf(seq, "%-12s %16s\n", "QType:", "FW event queue");
+	}
+#undef R
+#undef T
+#undef S
+#undef R3
+#undef T3
+#undef S3
+	return 0;
+}
+
+static const struct seq_operations sge_stats_seq_ops = {
+	.start = sge_queue_start,
+	.next  = sge_queue_next,
+	.stop  = sge_queue_stop,
+	.show  = sge_stats_show
+};
+
+static int sge_stats_open(struct inode *inode, struct file *file)
+{
+	int res = seq_open(file, &sge_stats_seq_ops);
+
+	if (!res) {
+		struct seq_file *seq = file->private_data;
+		seq->private = PDE(inode)->data;
+	}
+	return res;
+}
+
+static const struct file_operations sge_stats_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = sge_stats_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release,
+};
+
+static int sched_show(struct seq_file *seq, void *v)
+{
+	int i;
+	unsigned int map, kbps, ipg, mode;
+	unsigned int pace_tab[NTX_SCHED];
+	struct adapter *adap = seq->private;
+
+	map = t4_read_reg(adap, A_TP_TX_MOD_QUEUE_REQ_MAP);
+	mode = G_TIMERMODE(t4_read_reg(adap, A_TP_MOD_CONFIG));
+	t4_read_pace_tbl(adap, pace_tab);
+
+	seq_printf(seq, "Scheduler  Mode   Channel  Rate (Kbps)   "
+		      "Class IPG (0.1 ns)   Flow IPG (us)\n");
+	for (i = 0; i < NTX_SCHED; ++i, map >>= 2) {
+		t4_get_tx_sched(adap, i, &kbps, &ipg);
+		seq_printf(seq, "    %u      %-5s     %u     ", i,
+			   (mode & (1 << i)) ? "flow" : "class", map & 3);
+		if (kbps)
+			seq_printf(seq, "%9u     ", kbps);
+		else
+			seq_puts(seq, " disabled     ");
+
+		if (ipg)
+			seq_printf(seq, "%13u        ", ipg);
+		else
+			seq_puts(seq, "     disabled        ");
+
+		if (pace_tab[i])
+			seq_printf(seq, "%10u\n", pace_tab[i]);
+		else
+			seq_puts(seq, "  disabled\n");
+	}
+	return 0;
+}
+
+static int sched_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, sched_show, PDE(inode)->data);
+}
+
+static const struct file_operations sched_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = sched_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+static int lb_stats_show(struct seq_file *seq, void *v)
+{
+	static const char *stat_name[] = {
+		"OctetsOK:", "FramesOK:", "BcastFrames:", "McastFrames:",
+		"UcastFrames:", "ErrorFrames:", "Frames64:", "Frames65To127:",
+		"Frames128To255:", "Frames256To511:", "Frames512To1023:",
+		"Frames1024To1518:", "Frames1519ToMax:", "FramesDropped:",
+		"BG0FramesDropped:", "BG1FramesDropped:", "BG2FramesDropped:",
+		"BG3FramesDropped:", "BG0FramesTrunc:", "BG1FramesTrunc:",
+		"BG2FramesTrunc:", "BG3FramesTrunc:"
+	};
+
+	int i, j;
+	u64 *p0, *p1;
+	struct lb_port_stats s[2];
+
+	memset(s, 0, sizeof(s));
+
+	for (i = 0; i < 4; i += 2) {
+		t4_get_lb_stats(seq->private, i, &s[0]);
+		t4_get_lb_stats(seq->private, i + 1, &s[1]);
+
+		p0 = &s[0].octets;
+		p1 = &s[1].octets;
+		seq_printf(seq, "%s                       Loopback %u          "
+			   " Loopback %u\n", i == 0 ? "" : "\n", i, i + 1);
+
+		for (j = 0; j < ARRAY_SIZE(stat_name); j++)
+			seq_printf(seq, "%-17s %20llu %20llu\n", stat_name[j],
+				   (unsigned long long)*p0++,
+				   (unsigned long long)*p1++);
+	}
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(lb_stats);
+
+static int pm_stats_show(struct seq_file *seq, void *v)
+{
+	static const char *pm_stats[] = {
+		"Read:", "Write bypass:", "Write mem:", "Flush:", "FIFO wait:"
+	};
+
+	int i;
+	u32 tx_cnt[PM_NSTATS], rx_cnt[PM_NSTATS];
+	u64 tx_cyc[PM_NSTATS], rx_cyc[PM_NSTATS];
+	struct adapter *adap = seq->private;
+
+	t4_pmtx_get_stats(adap, tx_cnt, tx_cyc);
+	t4_pmrx_get_stats(adap, rx_cnt, rx_cyc);
+
+	seq_puts(seq, "                Tx count            Tx cycles    "
+		 "Rx count            Rx cycles\n");
+	for (i = 0; i < PM_NSTATS; i++)
+		seq_printf(seq, "%-13s %10u %20llu  %10u %20llu\n",
+			   pm_stats[i], tx_cnt[i],
+			   (unsigned long long)tx_cyc[i], rx_cnt[i],
+			   (unsigned long long)rx_cyc[i]);
+	return 0;
+}
+
+static int pm_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pm_stats_show, PDE(inode)->data);
+}
+
+static ssize_t pm_stats_clear(struct file *file, const char __user *buf,
+			      size_t count, loff_t *pos)
+{
+	struct adapter *adap = PDE(file->f_path.dentry->d_inode)->data;
+
+	t4_write_reg(adap, A_PM_RX_STAT_CONFIG, 0);
+	t4_write_reg(adap, A_PM_TX_STAT_CONFIG, 0);
+	return count;
+}
+
+static const struct file_operations pm_stats_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = pm_stats_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = pm_stats_clear
+};
+
+static int tcp_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_tcp_stats v4, v6;
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_tp_get_tcp_stats(adap, &v4, &v6);
+	spin_unlock(&adap->stats_lock);
+
+	seq_puts(seq,
+		 "                                IP                 IPv6\n");
+	seq_printf(seq, "OutRsts:      %20u %20u\n",
+		   v4.tcpOutRsts, v6.tcpOutRsts);
+	seq_printf(seq, "InSegs:       %20llu %20llu\n",
+		   (unsigned long long)v4.tcpInSegs,
+		   (unsigned long long)v6.tcpInSegs);
+	seq_printf(seq, "OutSegs:      %20llu %20llu\n",
+		   (unsigned long long)v4.tcpOutSegs,
+		   (unsigned long long)v6.tcpOutSegs);
+	seq_printf(seq, "RetransSegs:  %20llu %20llu\n",
+		   (unsigned long long)v4.tcpRetransSegs,
+		   (unsigned long long)v6.tcpRetransSegs);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(tcp_stats);
+
+static int tp_err_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_err_stats stats;
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_tp_get_err_stats(adap, &stats);
+	spin_unlock(&adap->stats_lock);
+
+	seq_puts(seq, "                 channel 0  channel 1  channel 2  "
+		      "channel 3\n");
+	seq_printf(seq, "macInErrs:      %10u %10u %10u %10u\n",
+		   stats.macInErrs[0], stats.macInErrs[1], stats.macInErrs[2],
+		   stats.macInErrs[3]);
+	seq_printf(seq, "hdrInErrs:      %10u %10u %10u %10u\n",
+		   stats.hdrInErrs[0], stats.hdrInErrs[1], stats.hdrInErrs[2],
+		   stats.hdrInErrs[3]);
+	seq_printf(seq, "tcpInErrs:      %10u %10u %10u %10u\n",
+		   stats.tcpInErrs[0], stats.tcpInErrs[1], stats.tcpInErrs[2],
+		   stats.tcpInErrs[3]);
+	seq_printf(seq, "tcp6InErrs:     %10u %10u %10u %10u\n",
+		   stats.tcp6InErrs[0], stats.tcp6InErrs[1],
+		   stats.tcp6InErrs[2], stats.tcp6InErrs[3]);
+	seq_printf(seq, "tnlCongDrops:   %10u %10u %10u %10u\n",
+		   stats.tnlCongDrops[0], stats.tnlCongDrops[1],
+		   stats.tnlCongDrops[2], stats.tnlCongDrops[3]);
+	seq_printf(seq, "tnlTxDrops:     %10u %10u %10u %10u\n",
+		   stats.tnlTxDrops[0], stats.tnlTxDrops[1],
+		   stats.tnlTxDrops[2], stats.tnlTxDrops[3]);
+	seq_printf(seq, "ofldVlanDrops:  %10u %10u %10u %10u\n",
+		   stats.ofldVlanDrops[0], stats.ofldVlanDrops[1],
+		   stats.ofldVlanDrops[2], stats.ofldVlanDrops[3]);
+	seq_printf(seq, "ofldChanDrops:  %10u %10u %10u %10u\n\n",
+		   stats.ofldChanDrops[0], stats.ofldChanDrops[1],
+		   stats.ofldChanDrops[2], stats.ofldChanDrops[3]);
+	seq_printf(seq, "ofldNoNeigh:    %u\nofldCongDefer:  %u\n",
+		   stats.ofldNoNeigh, stats.ofldCongDefer);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(tp_err_stats);
+
+static int cpl_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_cpl_stats stats;
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_tp_get_cpl_stats(adap, &stats);
+	spin_unlock(&adap->stats_lock);
+
+	seq_puts(seq, "                 channel 0  channel 1  channel 2  "
+		      "channel 3\n");
+	seq_printf(seq, "CPL requests:   %10u %10u %10u %10u\n",
+		   stats.req[0], stats.req[1], stats.req[2], stats.req[3]);
+	seq_printf(seq, "CPL responses:  %10u %10u %10u %10u\n",
+		   stats.rsp[0], stats.rsp[1], stats.rsp[2], stats.rsp[3]);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(cpl_stats);
+
+static int rdma_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_rdma_stats stats;
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_tp_get_rdma_stats(adap, &stats);
+	spin_unlock(&adap->stats_lock);
+
+	seq_printf(seq, "NoRQEModDefferals: %u\n", stats.rqe_dfr_mod);
+	seq_printf(seq, "NoRQEPktDefferals: %u\n", stats.rqe_dfr_pkt);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(rdma_stats);
+
+static int fcoe_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_fcoe_stats stats[4];
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_get_fcoe_stats(adap, 0, &stats[0]);
+	t4_get_fcoe_stats(adap, 1, &stats[1]);
+	t4_get_fcoe_stats(adap, 2, &stats[2]);
+	t4_get_fcoe_stats(adap, 3, &stats[3]);
+	spin_unlock(&adap->stats_lock);
+
+	seq_puts(seq, "                   channel 0        channel 1        "
+		      "channel 2        channel 3\n");
+	seq_printf(seq, "octetsDDP:  %16llu %16llu %16llu %16llu\n",
+		   stats[0].octetsDDP, stats[1].octetsDDP, stats[2].octetsDDP,
+		   stats[3].octetsDDP);
+	seq_printf(seq, "framesDDP:  %16u %16u %16u %16u\n", stats[0].framesDDP,
+		   stats[1].framesDDP, stats[2].framesDDP, stats[3].framesDDP);
+	seq_printf(seq, "framesDrop: %16u %16u %16u %16u\n",
+		   stats[0].framesDrop, stats[1].framesDrop,
+		   stats[2].framesDrop, stats[3].framesDrop);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(fcoe_stats);
+
+static int ddp_stats_show(struct seq_file *seq, void *v)
+{
+	struct tp_usm_stats stats;
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_get_usm_stats(adap, &stats);
+	spin_unlock(&adap->stats_lock);
+
+	seq_printf(seq, "Frames: %u\n", stats.frames);
+	seq_printf(seq, "Octets: %llu\n", (unsigned long long)stats.octets);
+	seq_printf(seq, "Drops:  %u\n", stats.drops);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(ddp_stats);
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Filter Table.
+ */
+
+/*
+ * Global filter infor used to decode individual filter entries.  This is
+ * grabbed once when we open the /proc filters node to save endlessly
+ * rereading the registers.
+ */
+struct filters_global_info {
+	struct adapter *adapter;
+	u32 tp_vlan_pri_map;
+	u32 tp_ingress_config;
+};
+
+static void filters_show_ipaddr(struct seq_file *seq,
+				int type, u8 *addr, u8 *addrm)
+{
+	int noctets, octet;
+
+	seq_puts(seq, " ");
+	if (type == 0) {
+		noctets = 4;
+		seq_printf(seq, "%48s", " ");
+	} else
+		noctets = 16;
+
+	for (octet = 0; octet < noctets; octet++)
+		seq_printf(seq, "%02x", addr[octet]);
+	seq_puts(seq, "/");
+	for (octet = 0; octet < noctets; octet++)
+		seq_printf(seq, "%02x", addrm[octet]);
+}
+
+static int get_filter_count(struct adapter *adapter, unsigned int fidx, u64 *c);
+
+static int filters_show(struct seq_file *seq, void *v)
+{
+	struct filters_global_info *gconf = seq->private;
+	u32 fconf = gconf->tp_vlan_pri_map;
+	u32 tpiconf = gconf->tp_ingress_config;
+	int i;
+
+	if (v == SEQ_START_TOKEN) {
+		seq_puts(seq, " Idx                 Hits");
+		for (i = TP_VLAN_PRI_MAP_FIRST; i <= TP_VLAN_PRI_MAP_LAST; i++) {
+			switch (fconf & (1 << i)) {
+			    case 0:
+				/* compressed filter field not enabled */
+				break;
+
+			    case F_FCOE:
+				seq_puts(seq, " FCoE");
+				break;
+
+			    case F_PORT:
+				seq_puts(seq, " Port");
+				break;
+
+			    case F_VNIC_ID:
+				if ((tpiconf & F_VNIC) == 0)
+					seq_puts(seq, "     vld:oVLAN");
+				else
+					seq_puts(seq, "   VFvld:PF:VF");
+				break;
+
+			    case F_VLAN:
+				seq_puts(seq, "     vld:iVLAN");
+				break;
+
+			    case F_TOS:
+				seq_puts(seq, "   TOS");
+				break;
+
+			    case F_PROTOCOL:
+				seq_puts(seq, "  Prot");
+				break;
+
+			    case F_ETHERTYPE:
+				seq_puts(seq, "   EthType");
+				break;
+
+			    case F_MACMATCH:
+				seq_puts(seq, "  MACIdx");
+				break;
+
+			    case F_MPSHITTYPE:
+				seq_puts(seq, " MPS");
+				break;
+
+			    case F_FRAGMENTATION:
+				seq_puts(seq, " Frag");
+				break;
+			}
+		}
+		seq_printf(seq, " %65s %65s %9s %9s %s\n",
+			   "LIP", "FIP", "LPORT", "FPORT", "Action");
+	} else {
+		int fidx = (uintptr_t)v - 2;
+		struct filter_entry *f = &gconf->adapter->tids.ftid_tab[fidx];
+
+		/* if this entry isn't filled in just return */
+		if (!f->valid)
+			return 0;
+
+		/*
+		 * Filter index.
+		 */
+		seq_printf(seq, "%4d", fidx);
+
+		if (f->fs.hitcnts) {
+			u64 hitcnt;
+			int ret;
+
+			ret = get_filter_count(gconf->adapter, fidx, &hitcnt);
+			if (ret)
+				seq_printf(seq, " %20s", "hits={ERROR}");
+			else
+				seq_printf(seq, " %20llu", hitcnt);
+		} else
+			seq_printf(seq, " %20s", "Disabled");
+
+		/*
+		 * Compressed header portion of filter.
+		 */
+		for (i = TP_VLAN_PRI_MAP_FIRST; i <= TP_VLAN_PRI_MAP_LAST; i++) {
+			switch (fconf & (1 << i)) {
+			    case 0:
+				/* compressed filter field not enabled */
+				break;
+
+			    case F_FCOE:
+				seq_printf(seq, "  %1d/%1d",
+					   f->fs.val.fcoe, f->fs.mask.fcoe);
+				break;
+
+			    case F_PORT:
+				seq_printf(seq, "  %1d/%1d",
+					   f->fs.val.iport, f->fs.mask.iport);
+				break;
+
+			    case F_VNIC_ID:
+				if ((tpiconf & F_VNIC) == 0)
+				  seq_printf(seq, " %1d:%04x/%1d:%04x",
+					     f->fs.val.ovlan_vld,
+					     f->fs.val.ovlan,
+					     f->fs.mask.ovlan_vld,
+					     f->fs.mask.ovlan);
+				else
+				  seq_printf(seq, " %1d:%1x:%02x/%1d:%1x:%02x",
+					     f->fs.val.ovlan_vld,
+					     (f->fs.val.ovlan >> 7) & 0x7,
+					     f->fs.val.ovlan & 0x7f,
+					     f->fs.mask.ovlan_vld,
+					     (f->fs.mask.ovlan >> 7) & 0x7,
+					     f->fs.mask.ovlan & 0x7f);
+				break;
+
+			    case F_VLAN:
+				seq_printf(seq, " %1d:%04x/%1d:%04x",
+					   f->fs.val.ivlan_vld,
+					   f->fs.val.ivlan,
+					   f->fs.mask.ivlan_vld,
+					   f->fs.mask.ivlan);
+				break;
+
+			    case F_TOS:
+				seq_printf(seq, " %02x/%02x",
+					   f->fs.val.tos, f->fs.mask.tos);
+				break;
+
+			    case F_PROTOCOL:
+				seq_printf(seq, " %02x/%02x",
+					   f->fs.val.proto, f->fs.mask.proto);
+				break;
+
+			    case F_ETHERTYPE:
+				seq_printf(seq, " %04x/%04x",
+					   f->fs.val.ethtype, f->fs.mask.ethtype);
+				break;
+
+			    case F_MACMATCH:
+				seq_printf(seq, " %03x/%03x",
+					   f->fs.val.macidx, f->fs.mask.macidx);
+				break;
+
+			    case F_MPSHITTYPE:
+				seq_printf(seq, " %1x/%1x",
+					   f->fs.val.matchtype,
+					   f->fs.mask.matchtype);
+				break;
+
+			    case F_FRAGMENTATION:
+				seq_printf(seq, "  %1d/%1d",
+					   f->fs.val.frag, f->fs.mask.frag);
+				break;
+			}
+		}
+
+		/*
+		 * Fixed portion of filter.
+		 */
+		filters_show_ipaddr(seq, f->fs.type,
+				    f->fs.val.lip, f->fs.mask.lip);
+		filters_show_ipaddr(seq, f->fs.type,
+				    f->fs.val.fip, f->fs.mask.fip);
+		seq_printf(seq, " %04x/%04x %04x/%04x",
+			   f->fs.val.lport, f->fs.mask.lport,
+			   f->fs.val.fport, f->fs.mask.fport);
+
+		/*
+		 * Variable length filter action.
+		 */
+		if (f->fs.action == FILTER_DROP)
+			seq_puts(seq, " Drop");
+		else if (f->fs.action == FILTER_SWITCH) {
+			seq_printf(seq, " Switch: port=%d", f->fs.eport);
+			if (f->fs.newdmac)
+				seq_printf(seq,
+					   ", dmac=%02x:%02x:%02x:%02x:%02x:%02x"
+					   ", l2tidx=%d",
+					   f->fs.dmac[0], f->fs.dmac[1],
+					   f->fs.dmac[2], f->fs.dmac[3],
+					   f->fs.dmac[4], f->fs.dmac[5],
+					   f->l2t->idx);
+			if (f->fs.newsmac)
+				seq_printf(seq,
+					   ", smac=%02x:%02x:%02x:%02x:%02x:%02x"
+					   ", smtidx=%d",
+					   f->fs.smac[0], f->fs.smac[1],
+					   f->fs.smac[2], f->fs.smac[3],
+					   f->fs.smac[4], f->fs.smac[5],
+					   f->smtidx);
+			if (f->fs.newvlan == VLAN_REMOVE)
+				seq_printf(seq, ", vlan=none");
+			else if (f->fs.newvlan == VLAN_INSERT)
+				seq_printf(seq, ", vlan=insert(%x)",
+					   f->fs.vlan);
+			else if (f->fs.newvlan == VLAN_REWRITE)
+				seq_printf(seq, ", vlan=rewrite(%x)",
+					   f->fs.vlan);
+		} else {
+			seq_puts(seq, " Pass: Q=");
+			if (f->fs.dirsteer == 0) {
+				seq_puts(seq, "RSS");
+				if (f->fs.maskhash)
+					seq_puts(seq, "(TCB=hash)");
+			} else {
+				seq_printf(seq, "%d", f->fs.iq);
+				if (f->fs.dirsteerhash == 0)
+					seq_puts(seq, "(QID)");
+				else
+					seq_puts(seq, "(hash)");
+			}
+		}
+		if (f->fs.prio)
+			seq_puts(seq, " Prio");
+		if (f->fs.rpttid)
+			seq_puts(seq, " RptTID");
+		seq_puts(seq, "\n");
+	}
+	return 0;
+}
+
+static inline void *filters_get_idx(struct adapter *adapter, loff_t pos)
+{
+	if (pos > (adapter->tids.nftids + adapter->tids.nsftids))
+		return NULL;
+
+	return (void *)(uintptr_t)(pos + 1);
+}
+
+static void *filters_start(struct seq_file *seq, loff_t *pos)
+{
+	struct filters_global_info *gconf = seq->private;
+
+	return (*pos
+		? filters_get_idx(gconf->adapter, *pos)
+		: SEQ_START_TOKEN);
+}
+
+static void *filters_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	struct filters_global_info *gconf = seq->private;
+
+	(*pos)++;
+	return filters_get_idx(gconf->adapter, *pos);
+}
+
+static void filters_stop(struct seq_file *seq, void *v)
+{
+}
+
+static const struct seq_operations filters_seq_ops = {
+	.start = filters_start,
+	.next  = filters_next,
+	.stop  = filters_stop,
+	.show  = filters_show
+};
+
+static int filters_open(struct inode *inode, struct file *file)
+{
+	struct adapter *adapter = PDE(inode)->data;
+	struct filters_global_info *gconf =
+		__seq_open_private(file, &filters_seq_ops, sizeof *gconf);
+
+	if (gconf == NULL)
+		return -ENOMEM;
+	if (adapter->tids.nftids == 0 ||
+	    adapter->tids.ftid_tab == NULL)
+		return -EOPNOTSUPP;
+
+	gconf->adapter = adapter;
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 &gconf->tp_vlan_pri_map, 1,
+			 A_TP_VLAN_PRI_MAP);
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 &gconf->tp_ingress_config, 1,
+			 A_TP_INGRESS_CONFIG);
+
+	return 0;
+}
+
+static const struct file_operations filters_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = filters_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+static int tx_rate_show(struct seq_file *seq, void *v)
+{
+	u64 nrate[NCHAN], orate[NCHAN];
+	struct adapter *adap = seq->private;
+
+	t4_get_chan_txrate(adap, nrate, orate);
+	seq_puts(seq, "              channel 0   channel 1   channel 2   "
+		 "channel 3\n");
+	seq_printf(seq, "NIC B/s:     %10llu  %10llu  %10llu  %10llu\n",
+		   (unsigned long long)nrate[0], (unsigned long long)nrate[1],
+		   (unsigned long long)nrate[2], (unsigned long long)nrate[3]);
+	seq_printf(seq, "Offload B/s: %10llu  %10llu  %10llu  %10llu\n",
+		   (unsigned long long)orate[0], (unsigned long long)orate[1],
+		   (unsigned long long)orate[2], (unsigned long long)orate[3]);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(tx_rate);
+
+/*
+ * RSS Table.
+ */
+
+static int rss_show(struct seq_file *seq, void *v, int idx)
+{
+	u16 *entry = v;
+
+	seq_printf(seq, "%4d:  %4u  %4u  %4u  %4u  %4u  %4u  %4u  %4u\n",
+		   idx * 8, entry[0], entry[1], entry[2], entry[3], entry[4],
+		   entry[5], entry[6], entry[7]);
+	return 0;
+}
+
+static int rss_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	struct seq_tab *p;
+
+	p = seq_open_tab(file, RSS_NENTRIES / 8, 8 * sizeof(u16), 0, rss_show);
+	if (!p)
+		return -ENOMEM;
+	ret = t4_read_rss(PDE(inode)->data, (u16 *)p->data);
+	if (ret)
+		seq_release_private(inode, file);
+	return ret;
+}
+
+static const struct file_operations rss_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = rss_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+/*
+ * RSS Configuration.
+ */
+
+/*
+ * Small utility function to return the strings "yes" or "no" if the supplied
+ * argument is non-zero.
+ */
+static const char *yesno(int x)
+{
+	static const char *yes = "yes";
+	static const char *no = "no";
+	return x ? yes : no;
+}
+
+static int rss_config_show(struct seq_file *seq, void *v)
+{
+	struct adapter *adapter = seq->private;
+	static const char *keymode[] = {
+		"global",
+		"global and per-VF scramble",
+		"per-PF and per-VF scramble",
+		"per-VF and per-VF scramble",
+	};
+	u32 rssconf;
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG);
+	seq_printf(seq, "TP_RSS_CONFIG: %#x\n", rssconf);
+	seq_printf(seq, "  Tnl4TupEnIpv6: %3s\n", yesno(rssconf & F_TNL4TUPENIPV6));
+	seq_printf(seq, "  Tnl2TupEnIpv6: %3s\n", yesno(rssconf & F_TNL2TUPENIPV6));
+	seq_printf(seq, "  Tnl4TupEnIpv4: %3s\n", yesno(rssconf & F_TNL4TUPENIPV4));
+	seq_printf(seq, "  Tnl2TupEnIpv4: %3s\n", yesno(rssconf & F_TNL2TUPENIPV4));
+	seq_printf(seq, "  TnlTcpSel:     %3s\n", yesno(rssconf & F_TNLTCPSEL));
+	seq_printf(seq, "  TnlIp6Sel:     %3s\n", yesno(rssconf & F_TNLIP6SEL));
+	seq_printf(seq, "  TnlVrtSel:     %3s\n", yesno(rssconf & F_TNLVRTSEL));
+	seq_printf(seq, "  TnlMapEn:      %3s\n", yesno(rssconf & F_TNLMAPEN));
+	seq_printf(seq, "  OfdHashSave:   %3s\n", yesno(rssconf & F_OFDHASHSAVE));
+	seq_printf(seq, "  OfdVrtSel:     %3s\n", yesno(rssconf & F_OFDVRTSEL));
+	seq_printf(seq, "  OfdMapEn:      %3s\n", yesno(rssconf & F_OFDMAPEN));
+	seq_printf(seq, "  OfdLkpEn:      %3s\n", yesno(rssconf & F_OFDLKPEN));
+	seq_printf(seq, "  Syn4TupEnIpv6: %3s\n", yesno(rssconf & F_SYN4TUPENIPV6));
+	seq_printf(seq, "  Syn2TupEnIpv6: %3s\n", yesno(rssconf & F_SYN2TUPENIPV6));
+	seq_printf(seq, "  Syn4TupEnIpv4: %3s\n", yesno(rssconf & F_SYN4TUPENIPV4));
+	seq_printf(seq, "  Syn2TupEnIpv4: %3s\n", yesno(rssconf & F_SYN2TUPENIPV4));
+	seq_printf(seq, "  Syn4TupEnIpv6: %3s\n", yesno(rssconf & F_SYN4TUPENIPV6));
+	seq_printf(seq, "  SynIp6Sel:     %3s\n", yesno(rssconf & F_SYNIP6SEL));
+	seq_printf(seq, "  SynVrt6Sel:    %3s\n", yesno(rssconf & F_SYNVRTSEL));
+	seq_printf(seq, "  SynMapEn:      %3s\n", yesno(rssconf & F_SYNMAPEN));
+	seq_printf(seq, "  SynLkpEn:      %3s\n", yesno(rssconf & F_SYNLKPEN));
+	seq_printf(seq, "  ChnEn:         %3s\n", yesno(rssconf & F_CHANNELENABLE));
+	seq_printf(seq, "  PrtEn:         %3s\n", yesno(rssconf & F_PORTENABLE));
+	seq_printf(seq, "  TnlAllLkp:     %3s\n", yesno(rssconf & F_TNLALLLOOKUP));
+	seq_printf(seq, "  VrtEn:         %3s\n", yesno(rssconf & F_VIRTENABLE));
+	seq_printf(seq, "  CngEn:         %3s\n", yesno(rssconf & F_CONGESTIONENABLE));
+	seq_printf(seq, "  HashToeplitz:  %3s\n", yesno(rssconf & F_HASHTOEPLITZ));
+	seq_printf(seq, "  Udp4En:        %3s\n", yesno(rssconf & F_UDPENABLE));
+	seq_printf(seq, "  Disable:       %3s\n", yesno(rssconf & F_DISABLE));
+
+	seq_puts(seq, "\n");
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG_TNL);
+	seq_printf(seq, "TP_RSS_CONFIG_TNL: %#x\n", rssconf);
+	seq_printf(seq, "  MaskSize:      %3d\n", G_MASKSIZE(rssconf));
+	seq_printf(seq, "  MaskFilter:    %3d\n", G_MASKFILTER(rssconf));
+	seq_printf(seq, "  UseWireCh:     %3s\n", yesno(rssconf & F_USEWIRECH));
+
+	seq_puts(seq, "\n");
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG_OFD);
+	seq_printf(seq, "TP_RSS_CONFIG_OFD: %#x\n", rssconf);
+	seq_printf(seq, "  MaskSize:      %3d\n", G_MASKSIZE(rssconf));
+	seq_printf(seq, "  RRCplMapEn:    %3s\n", yesno(rssconf & F_RRCPLMAPEN));
+	seq_printf(seq, "  RRCplQueWidth: %3d\n", G_RRCPLQUEWIDTH(rssconf));
+
+	seq_puts(seq, "\n");
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG_SYN);
+	seq_printf(seq, "TP_RSS_CONFIG_SYN: %#x\n", rssconf);
+	seq_printf(seq, "  MaskSize:      %3d\n", G_MASKSIZE(rssconf));
+	seq_printf(seq, "  UseWireCh:     %3s\n", yesno(rssconf & F_USEWIRECH));
+
+	seq_puts(seq, "\n");
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG_VRT);
+	seq_printf(seq, "TP_RSS_CONFIG_VRT: %#x\n", rssconf);
+	seq_printf(seq, "  VfRdRg:        %3s\n", yesno(rssconf & F_VFRDRG));
+	seq_printf(seq, "  VfRdEn:        %3s\n", yesno(rssconf & F_VFRDEN));
+	seq_printf(seq, "  VfPerrEn:      %3s\n", yesno(rssconf & F_VFPERREN));
+	seq_printf(seq, "  KeyPerrEn:     %3s\n", yesno(rssconf & F_KEYPERREN));
+	seq_printf(seq, "  DisVfVlan:     %3s\n", yesno(rssconf & F_DISABLEVLAN));
+	seq_printf(seq, "  EnUpSwt:       %3s\n", yesno(rssconf & F_ENABLEUP0));
+	seq_printf(seq, "  HashDelay:     %3d\n", G_HASHDELAY(rssconf));
+	seq_printf(seq, "  VfWrAddr:      %3d\n", G_VFWRADDR(rssconf));
+	seq_printf(seq, "  KeyMode:       %s\n", keymode[G_KEYMODE(rssconf)]);
+	seq_printf(seq, "  VfWrEn:        %3s\n", yesno(rssconf & F_VFWREN));
+	seq_printf(seq, "  KeyWrEn:       %3s\n", yesno(rssconf & F_KEYWREN));
+	seq_printf(seq, "  KeyWrAddr:     %3d\n", G_KEYWRADDR(rssconf));
+
+	seq_puts(seq, "\n");
+
+	rssconf = t4_read_reg(adapter, A_TP_RSS_CONFIG_CNG);
+	seq_printf(seq, "TP_RSS_CONFIG_CNG: %#x\n", rssconf);
+	seq_printf(seq, "  ChnCount3:     %3s\n", yesno(rssconf & F_CHNCOUNT3));
+	seq_printf(seq, "  ChnCount2:     %3s\n", yesno(rssconf & F_CHNCOUNT2));
+	seq_printf(seq, "  ChnCount1:     %3s\n", yesno(rssconf & F_CHNCOUNT1));
+	seq_printf(seq, "  ChnCount0:     %3s\n", yesno(rssconf & F_CHNCOUNT0));
+	seq_printf(seq, "  ChnUndFlow3:   %3s\n", yesno(rssconf & F_CHNUNDFLOW3));
+	seq_printf(seq, "  ChnUndFlow2:   %3s\n", yesno(rssconf & F_CHNUNDFLOW2));
+	seq_printf(seq, "  ChnUndFlow1:   %3s\n", yesno(rssconf & F_CHNUNDFLOW1));
+	seq_printf(seq, "  ChnUndFlow0:   %3s\n", yesno(rssconf & F_CHNUNDFLOW0));
+	seq_printf(seq, "  RstChn3:       %3s\n", yesno(rssconf & F_RSTCHN3));
+	seq_printf(seq, "  RstChn2:       %3s\n", yesno(rssconf & F_RSTCHN2));
+	seq_printf(seq, "  RstChn1:       %3s\n", yesno(rssconf & F_RSTCHN1));
+	seq_printf(seq, "  RstChn0:       %3s\n", yesno(rssconf & F_RSTCHN0));
+	seq_printf(seq, "  UpdVld:        %3s\n", yesno(rssconf & F_UPDVLD));
+	seq_printf(seq, "  Xoff:          %3s\n", yesno(rssconf & F_XOFF));
+	seq_printf(seq, "  UpdChn3:       %3s\n", yesno(rssconf & F_UPDCHN3));
+	seq_printf(seq, "  UpdChn2:       %3s\n", yesno(rssconf & F_UPDCHN2));
+	seq_printf(seq, "  UpdChn1:       %3s\n", yesno(rssconf & F_UPDCHN1));
+	seq_printf(seq, "  UpdChn0:       %3s\n", yesno(rssconf & F_UPDCHN0));
+	seq_printf(seq, "  Queue:         %3d\n", G_QUEUE(rssconf));
+
+	return 0;
+}
+
+static int rss_config_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, rss_config_show, PDE(inode)->data);
+}
+
+static const struct file_operations rss_config_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = rss_config_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * RSS Secret Key.
+ */
+
+static int rss_key_show(struct seq_file *seq, void *v)
+{
+	u32 key[10];
+
+	t4_read_rss_key(seq->private, key);
+	seq_printf(seq, "%08x%08x%08x%08x%08x%08x%08x%08x%08x%08x\n",
+		   key[9], key[8], key[7], key[6], key[5], key[4], key[3],
+		   key[2], key[1], key[0]);
+	return 0;
+}
+
+static int rss_key_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, rss_key_show, PDE(inode)->data);
+}
+
+static inline unsigned int hex2val(char c)
+{
+	return isdigit(c) ? c - '0' : tolower(c) - 'a' + 10;
+}
+
+static ssize_t rss_key_write(struct file *file, const char __user *buf,
+			     size_t count, loff_t *pos)
+{
+	int i, j;
+	u32 key[10];
+	char s[100], *p;
+	struct adapter *adap = PDE(file->f_path.dentry->d_inode)->data;
+
+	if (count > sizeof(s) - 1)
+		return -EINVAL;
+	if (copy_from_user(s, buf, count))
+		return -EFAULT;
+	for (i = count; i > 0 && isspace(s[i - 1]); i--)
+		;
+	s[i] = '\0';
+
+	for (p = s, i = 9; i >= 0; i--) {
+		key[i] = 0;
+		for (j = 0; j < 8; j++, p++) {
+			if (!isxdigit(*p))
+				return -EINVAL;
+			key[i] = (key[i] << 4) | hex2val(*p);
+		}
+	}
+
+	t4_write_rss_key(adap, key, -1);
+	return count;
+}
+
+static const struct file_operations rss_key_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = rss_key_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = rss_key_write
+};
+
+/*
+ * PF RSS Configuration.
+ */
+
+struct rss_pf_conf {
+	u32 rss_pf_map;
+	u32 rss_pf_mask;
+	u32 rss_pf_config;
+};
+
+static int rss_pf_config_show(struct seq_file *seq, void *v, int idx)
+{
+	struct rss_pf_conf *pfconf;
+
+	if (v == SEQ_START_TOKEN) {
+		/* use the 0th entry to dump the PF Map Index Size */
+		pfconf = seq->private + offsetof(struct seq_tab, data);
+		seq_printf(seq, "PF Map Index Size = %d\n\n",
+			   G_LKPIDXSIZE(pfconf->rss_pf_map));
+
+		seq_puts(seq, "     RSS              PF   VF    Hash Tuple Enable         Default\n");
+		seq_puts(seq, "     Enable       IPF Mask Mask  IPv6      IPv4      UDP   Queue\n");
+		seq_puts(seq, " PF  Map Chn Prt  Map Size Size  Four Two  Four Two  Four  Ch1  Ch0\n");
+	} else {
+		#define G_PFnLKPIDX(map, n) \
+			(((map) >> S_PF1LKPIDX*(n)) & M_PF0LKPIDX)
+		#define G_PFnMSKSIZE(mask, n) \
+			(((mask) >> S_PF1MSKSIZE*(n)) & M_PF1MSKSIZE)
+
+		pfconf = v;
+		seq_printf(seq, "%3d  %3s %3s %3s  %3d  %3d  %3d   %3s %3s   %3s %3s   %3s  %3d  %3d\n",
+			   idx,
+			   yesno(pfconf->rss_pf_config & F_MAPENABLE),
+			   yesno(pfconf->rss_pf_config & F_CHNENABLE),
+			   yesno(pfconf->rss_pf_config & F_PRTENABLE),
+			   G_PFnLKPIDX(pfconf->rss_pf_map, idx),
+			   G_PFnMSKSIZE(pfconf->rss_pf_mask, idx),
+			   G_IVFWIDTH(pfconf->rss_pf_config),
+			   yesno(pfconf->rss_pf_config & F_IP6FOURTUPEN),
+			   yesno(pfconf->rss_pf_config & F_IP6TWOTUPEN),
+			   yesno(pfconf->rss_pf_config & F_IP4FOURTUPEN),
+			   yesno(pfconf->rss_pf_config & F_IP4TWOTUPEN),
+			   yesno(pfconf->rss_pf_config & F_UDPFOURTUPEN),
+			   G_CH1DEFAULTQUEUE(pfconf->rss_pf_config),
+			   G_CH0DEFAULTQUEUE(pfconf->rss_pf_config));
+
+		#undef G_PFnLKPIDX
+		#undef G_PFnMSKSIZE
+	}
+	return 0;
+}
+
+static int rss_pf_config_open(struct inode *inode, struct file *file)
+{
+	struct adapter *adapter = PDE(inode)->data;
+	struct seq_tab *p;
+	u32 rss_pf_map, rss_pf_mask;
+	struct rss_pf_conf *pfconf;
+	int pf;
+
+	p = seq_open_tab(file, 8, sizeof(*pfconf), 1, rss_pf_config_show);
+	if (!p)
+		return -ENOMEM;
+
+	pfconf = (struct rss_pf_conf *)p->data;
+	rss_pf_map = t4_read_rss_pf_map(adapter);
+	rss_pf_mask = t4_read_rss_pf_mask(adapter);
+	for (pf = 0; pf < 8; pf++) {
+		pfconf[pf].rss_pf_map = rss_pf_map;
+		pfconf[pf].rss_pf_mask = rss_pf_mask;
+		t4_read_rss_pf_config(adapter, pf, &pfconf[pf].rss_pf_config);
+	}
+	return 0;
+}
+
+static const struct file_operations rss_pf_config_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = rss_pf_config_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+/*
+ * VF RSS Configuration.
+ */
+
+struct rss_vf_conf {
+	u32 rss_vf_vfl;
+	u32 rss_vf_vfh;
+};
+
+static int rss_vf_config_show(struct seq_file *seq, void *v, int idx)
+{
+	if (v == SEQ_START_TOKEN) {
+		seq_puts(seq, "     RSS                     Hash Tuple Enable\n");
+		seq_puts(seq, "     Enable   IVF  Dis  Enb  IPv6      IPv4      UDP    Def  Secret Key\n");
+		seq_puts(seq, " VF  Chn Prt  Map  VLAN  uP  Four Two  Four Two  Four   Que  Idx       Hash\n");
+	} else {
+		struct rss_vf_conf *vfconf = v;
+		seq_printf(seq, "%3d  %3s %3s  %3d   %3s %3s   %3s %3s   %3s %3s   %3s  %4d  %3d %#10x\n",
+			   idx,
+			   yesno(vfconf->rss_vf_vfh & F_VFCHNEN),
+			   yesno(vfconf->rss_vf_vfh & F_VFPRTEN),
+			   G_VFLKPIDX(vfconf->rss_vf_vfh),
+			   yesno(vfconf->rss_vf_vfh & F_VFVLNEX),
+			   yesno(vfconf->rss_vf_vfh & F_VFUPEN),
+			   yesno(vfconf->rss_vf_vfh & F_VFIP4FOURTUPEN),
+			   yesno(vfconf->rss_vf_vfh & F_VFIP6TWOTUPEN),
+			   yesno(vfconf->rss_vf_vfh & F_VFIP4FOURTUPEN),
+			   yesno(vfconf->rss_vf_vfh & F_VFIP4TWOTUPEN),
+			   yesno(vfconf->rss_vf_vfh & F_ENABLEUDPHASH),
+			   G_DEFAULTQUEUE(vfconf->rss_vf_vfh),
+			   G_KEYINDEX(vfconf->rss_vf_vfh),
+			   vfconf->rss_vf_vfl);
+	}
+	return 0;
+}
+
+static int rss_vf_config_open(struct inode *inode, struct file *file)
+{
+	struct adapter *adapter = PDE(inode)->data;
+	struct seq_tab *p;
+	struct rss_vf_conf *vfconf;
+	int vf;
+
+	p = seq_open_tab(file, 128, sizeof(*vfconf), 1, rss_vf_config_show);
+	if (!p)
+		return -ENOMEM;
+
+	vfconf = (struct rss_vf_conf *)p->data;
+	for (vf = 0; vf < 128; vf++) {
+		t4_read_rss_vf_config(adapter, vf, &vfconf[vf].rss_vf_vfl,
+				      &vfconf[vf].rss_vf_vfh);
+	}
+	return 0;
+}
+
+static const struct file_operations rss_vf_config_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = rss_vf_config_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release_private
+};
+
+static int mtutab_show(struct seq_file *seq, void *v)
+{
+	u16 mtus[NMTUS];
+	struct adapter *adap = seq->private;
+
+	spin_lock(&adap->stats_lock);
+	t4_read_mtu_tbl(adap, mtus, NULL);
+	spin_unlock(&adap->stats_lock);
+
+	seq_printf(seq, "%u %u %u %u %u %u %u %u %u %u %u %u %u %u %u %u\n",
+		   mtus[0], mtus[1], mtus[2], mtus[3], mtus[4], mtus[5],
+		   mtus[6], mtus[7], mtus[8], mtus[9], mtus[10], mtus[11],
+		   mtus[12], mtus[13], mtus[14], mtus[15]);
+	return 0;
+}
+
+static int mtutab_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtutab_show, PDE(inode)->data);
+}
+
+static ssize_t mtutab_write(struct file *file, const char __user *buf,
+			    size_t count, loff_t *pos)
+{
+	int i;
+	unsigned long mtus[NMTUS];
+	struct adapter *adap = PDE(file->f_path.dentry->d_inode)->data;
+
+	/* Require min MTU of 81 to accommodate SACK */
+	i = rd_usr_int_vec(buf, count, NMTUS, mtus, 81, MAX_MTU, 10);
+	if (i)
+		return i;
+
+	/* MTUs must be in ascending order */
+	for (i = 1; i < NMTUS; ++i)
+		if (mtus[i] < mtus[i - 1])
+			return -EINVAL;
+
+	/* can't change the MTU table if offload is in use */
+	mutex_lock(&uld_mutex);
+	for (i = 0; i < CXGB4_ULD_MAX; i++)
+		if (adap->uld_handle[i]) {
+			mutex_unlock(&uld_mutex);
+			return -EBUSY;
+		}
+
+	for (i = 0; i < NMTUS; ++i)
+		adap->params.mtus[i] = mtus[i];
+	t4_load_mtus(adap, adap->params.mtus, adap->params.a_wnd,
+		     adap->params.b_wnd);
+	mutex_unlock(&uld_mutex);
+	return count;
+}
+
+static const struct file_operations mtutab_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mtutab_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = mtutab_write
+};
+
+static int cctrl_tbl_show(struct seq_file *seq, void *v)
+{
+	static const char *dec_fac[] = {
+		"0.5", "0.5625", "0.625", "0.6875", "0.75", "0.8125", "0.875",
+		"0.9375" };
+
+	int i;
+	u16 incr[NMTUS][NCCTRL_WIN];
+	struct adapter *adap = seq->private;
+
+	t4_read_cong_tbl(adap, incr);
+
+	for (i = 0; i < NCCTRL_WIN; ++i) {
+		seq_printf(seq, "%2d: %4u %4u %4u %4u %4u %4u %4u %4u\n", i,
+			   incr[0][i], incr[1][i], incr[2][i], incr[3][i],
+			   incr[4][i], incr[5][i], incr[6][i], incr[7][i]);
+		seq_printf(seq, "%8u %4u %4u %4u %4u %4u %4u %4u %5u %s\n",
+			   incr[8][i], incr[9][i], incr[10][i], incr[11][i],
+			   incr[12][i], incr[13][i], incr[14][i], incr[15][i],
+			   adap->params.a_wnd[i],
+			   dec_fac[adap->params.b_wnd[i]]);
+	}
+	return 0;
+}
+
+static int cctrl_tbl_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cctrl_tbl_show, PDE(inode)->data);
+}
+
+static const struct file_operations cctrl_tbl_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = cctrl_tbl_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+struct mem_desc {
+	unsigned int base;
+	unsigned int limit;
+	unsigned int idx;
+};
+
+static int mem_desc_cmp(const void *a, const void *b)
+{
+	return ((const struct mem_desc *)a)->base -
+	       ((const struct mem_desc *)b)->base;
+}
+
+static void mem_region_show(struct seq_file *seq, const char *name,
+			    unsigned int from, unsigned int to)
+{
+	char buf[40];
+
+	string_get_size((u64)to - from + 1, STRING_UNITS_2, buf, sizeof(buf));
+	seq_printf(seq, "%-15s %#x-%#x [%s]\n", name, from, to, buf);
+}
+
+static int meminfo_show(struct seq_file *seq, void *v)
+{
+	static const char *memory[] = { "EDC0:", "EDC1:", "MC:" };
+	static const char *region[] = {
+		"DBQ contexts:", "IMSG contexts:", "FLM cache:", "TCBs:",
+		"Pstructs:", "Timers:", "Rx FL:", "Tx FL:", "Pstruct FL:",
+		"Tx payload:", "Rx payload:", "LE hash:", "iSCSI region:",
+		"TDDP region:", "TPT region:", "STAG region:", "RQ region:",
+		"RQUDP region:", "PBL region:", "TXPBL region:", "ULPRX state:",
+		"ULPTX state:", "On-chip queues:"
+	};
+
+	int i, n;
+	u32 lo, hi;
+	struct mem_desc avail[3];
+	struct mem_desc mem[ARRAY_SIZE(region) + 3];      /* up to 3 holes */
+	struct mem_desc *md = mem;
+	struct adapter *adap = seq->private;
+
+	for (i = 0; i < ARRAY_SIZE(mem); i++) {
+		mem[i].limit = 0;
+		mem[i].idx = i;
+	}
+
+	/* Find and sort the populated memory ranges */
+	i = 0;
+	lo = t4_read_reg(adap, A_MA_TARGET_MEM_ENABLE);
+	if (lo & F_EDRAM0_ENABLE) {
+		hi = t4_read_reg(adap, A_MA_EDRAM0_BAR);
+		avail[i].base = G_EDRAM0_BASE(hi) << 20;
+		avail[i].limit = avail[i].base + (G_EDRAM0_SIZE(hi) << 20);
+		avail[i].idx = 0;
+		i++;
+	}
+	if (lo & F_EDRAM1_ENABLE) {
+		hi = t4_read_reg(adap, A_MA_EDRAM1_BAR);
+		avail[i].base = G_EDRAM1_BASE(hi) << 20;
+		avail[i].limit = avail[i].base + (G_EDRAM1_SIZE(hi) << 20);
+		avail[i].idx = 1;
+		i++;
+	}
+	if (lo & F_EXT_MEM_ENABLE) {
+		hi = t4_read_reg(adap, A_MA_EXT_MEMORY_BAR);
+		avail[i].base = G_EXT_MEM_BASE(hi) << 20;
+		avail[i].limit = avail[i].base + (G_EXT_MEM_SIZE(hi) << 20);
+		avail[i].idx = 2;
+		i++;
+	}
+	if (!i)                                    /* no memory available */
+		return 0;
+	sort(avail, i, sizeof(struct mem_desc), mem_desc_cmp, NULL);
+
+	(md++)->base = t4_read_reg(adap, A_SGE_DBQ_CTXT_BADDR);
+	(md++)->base = t4_read_reg(adap, A_SGE_IMSG_CTXT_BADDR);
+	(md++)->base = t4_read_reg(adap, A_SGE_FLM_CACHE_BADDR);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_TCB_BASE);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_MM_BASE);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_TIMER_BASE);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_MM_RX_FLST_BASE);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_MM_TX_FLST_BASE);
+	(md++)->base = t4_read_reg(adap, A_TP_CMM_MM_PS_FLST_BASE);
+
+	/* the next few have explicit upper bounds */
+	md->base = t4_read_reg(adap, A_TP_PMM_TX_BASE);
+	md->limit = md->base - 1 +
+		    t4_read_reg(adap, A_TP_PMM_TX_PAGE_SIZE) *
+		    G_PMTXMAXPAGE(t4_read_reg(adap, A_TP_PMM_TX_MAX_PAGE));
+	md++;
+
+	md->base = t4_read_reg(adap, A_TP_PMM_RX_BASE);
+	md->limit = md->base - 1 +
+		    t4_read_reg(adap, A_TP_PMM_RX_PAGE_SIZE) *
+		    G_PMRXMAXPAGE(t4_read_reg(adap, A_TP_PMM_RX_MAX_PAGE));
+	md++;
+
+	if (t4_read_reg(adap, A_LE_DB_CONFIG) & F_HASHEN) {
+		hi = t4_read_reg(adap, A_LE_DB_TID_HASHBASE) / 4;
+		md->base = t4_read_reg(adap, A_LE_DB_HASH_TID_BASE);
+		md->limit = (adap->tids.ntids - hi) * 16 + md->base - 1;
+	} else {
+		md->base = 0;
+		md->idx = ARRAY_SIZE(region);  /* hide it */
+	}
+	md++;
+
+#define ulp_region(reg) \
+	md->base = t4_read_reg(adap, A_ULP_ ## reg ## _LLIMIT);\
+	(md++)->limit = t4_read_reg(adap, A_ULP_ ## reg ## _ULIMIT)
+
+	ulp_region(RX_ISCSI);
+	ulp_region(RX_TDDP);
+	ulp_region(TX_TPT);
+	ulp_region(RX_STAG);
+	ulp_region(RX_RQ);
+	ulp_region(RX_RQUDP);
+	ulp_region(RX_PBL);
+	ulp_region(TX_PBL);
+#undef ulp_region
+
+	md->base = t4_read_reg(adap, A_ULP_RX_CTX_BASE);
+	md->limit = md->base + adap->tids.ntids - 1;
+	md++;
+	md->base = t4_read_reg(adap, A_ULP_TX_ERR_TABLE_BASE);
+	md->limit = md->base + adap->tids.ntids - 1;
+	md++;
+
+	md->base = adap->vres.ocq.start;
+	if (adap->vres.ocq.size)
+		md->limit = md->base + adap->vres.ocq.size - 1;
+	else
+		md->idx = ARRAY_SIZE(region);  /* hide it */
+	md++;
+
+	/* add any address-space holes, there can be up to 3 */
+	for (n = 0; n < i - 1; n++)
+		if (avail[n].limit < avail[n + 1].base)
+			(md++)->base = avail[n].limit;
+	if (avail[n].limit)
+		(md++)->base = avail[n].limit;
+
+	n = md - mem;
+	sort(mem, n, sizeof(struct mem_desc), mem_desc_cmp, NULL);
+
+	for (lo = 0; lo < i; lo++)
+		mem_region_show(seq, memory[avail[lo].idx], avail[lo].base,
+				avail[lo].limit - 1);
+
+	seq_putc(seq, '\n');
+	for (i = 0; i < n; i++) {
+		if (mem[i].idx >= ARRAY_SIZE(region))
+			continue;                        /* skip holes */
+		if (!mem[i].limit)
+			mem[i].limit = i < n - 1 ? mem[i + 1].base - 1 : ~0;
+		mem_region_show(seq, region[mem[i].idx], mem[i].base,
+				mem[i].limit);
+	}
+
+	seq_putc(seq, '\n');
+	lo = t4_read_reg(adap, A_CIM_SDRAM_BASE_ADDR);
+	hi = t4_read_reg(adap, A_CIM_SDRAM_ADDR_SIZE) + lo - 1;
+	mem_region_show(seq, "uP RAM:", lo, hi);
+
+	lo = t4_read_reg(adap, A_CIM_EXTMEM2_BASE_ADDR);
+	hi = t4_read_reg(adap, A_CIM_EXTMEM2_ADDR_SIZE) + lo - 1;
+	mem_region_show(seq, "uP Extmem2:", lo, hi);
+
+	lo = t4_read_reg(adap, A_TP_PMM_RX_MAX_PAGE);
+	seq_printf(seq, "\n%u Rx pages of size %uKiB for %u channels\n",
+		   G_PMRXMAXPAGE(lo),
+		   t4_read_reg(adap, A_TP_PMM_RX_PAGE_SIZE) >> 10,
+		   (lo & F_PMRXNUMCHN) ? 2 : 1);
+
+	lo = t4_read_reg(adap, A_TP_PMM_TX_MAX_PAGE);
+	hi = t4_read_reg(adap, A_TP_PMM_TX_PAGE_SIZE);
+	seq_printf(seq, "%u Tx pages of size %u%ciB for %u channels\n",
+		   G_PMTXMAXPAGE(lo),
+		   hi >= (1 << 20) ? (hi >> 20) : (hi >> 10),
+		   hi >= (1 << 20) ? 'M' : 'K', 1 << G_PMTXNUMCHN(lo));
+	seq_printf(seq, "%u p-structs\n\n",
+		   t4_read_reg(adap, A_TP_CMM_MM_MAX_PSTRUCT));
+
+	for (i = 0; i < 4; i++) {
+		lo = t4_read_reg(adap, A_MPS_RX_PG_RSV0 + i * 4);
+		seq_printf(seq, "Port %d using %u pages out of %u allocated\n",
+			   i, G_USED(lo), G_ALLOC(lo));
+	}
+	for (i = 0; i < 4; i++) {
+		lo = t4_read_reg(adap, A_MPS_RX_PG_RSV4 + i * 4);
+		seq_printf(seq,
+			   "Loopback %d using %u pages out of %u allocated\n",
+			   i, G_USED(lo), G_ALLOC(lo));
+	}
+	return 0;
+}
+
+static int meminfo_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, meminfo_show, PDE(inode)->data);
+}
+
+static const struct file_operations meminfo_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = meminfo_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+static int mps_trc_show(struct seq_file *seq, void *v)
+{
+	int enabled, i;
+	struct trace_params tp;
+	struct adapter *adap = seq->private;
+	unsigned int trcidx = (uintptr_t)adap & 3;
+
+	adap = (void *)adap - trcidx;
+	t4_get_trace_filter(adap, &tp, trcidx, &enabled);
+	if (!enabled) {
+		seq_puts(seq, "tracer is disabled\n");
+		return 0;
+	}
+
+	if (tp.skip_ofst * 8 >= TRACE_LEN) {
+		dev_err(adap->pdev_dev, "illegal trace pattern skip offset\n");
+		return -EINVAL;
+	}
+	if (tp.port < 8) {
+		i = adap->chan_map[tp.port & 3];
+		if (i >= MAX_NPORTS) {
+			dev_err(adap->pdev_dev, "tracer %u is assigned "
+				"to non-existing port\n", trcidx);
+			return -EINVAL;
+		}
+		seq_printf(seq, "tracer is capturing %s %s, ",
+			   adap->port[i]->name, tp.port < 4 ? "Rx" : "Tx");
+	} else
+		seq_printf(seq, "tracer is capturing loopback %d, ",
+			   tp.port - 8);
+	seq_printf(seq, "snap length: %u, min length: %u\n", tp.snap_len,
+		   tp.min_len);
+	seq_printf(seq, "packets captured %smatch filter\n",
+		   tp.invert ? "do not " : "");
+
+	if (tp.skip_ofst) {
+		seq_puts(seq, "filter pattern: ");
+		for (i = 0; i < tp.skip_ofst * 2; i += 2)
+			seq_printf(seq, "%08x%08x", tp.data[i], tp.data[i + 1]);
+		seq_putc(seq, '/');
+		for (i = 0; i < tp.skip_ofst * 2; i += 2)
+			seq_printf(seq, "%08x%08x", tp.mask[i], tp.mask[i + 1]);
+		seq_puts(seq, "@0\n");
+	}
+
+	seq_puts(seq, "filter pattern: ");
+	for (i = tp.skip_ofst * 2; i < TRACE_LEN / 4; i += 2)
+		seq_printf(seq, "%08x%08x", tp.data[i], tp.data[i + 1]);
+	seq_putc(seq, '/');
+	for (i = tp.skip_ofst * 2; i < TRACE_LEN / 4; i += 2)
+		seq_printf(seq, "%08x%08x", tp.mask[i], tp.mask[i + 1]);
+	seq_printf(seq, "@%u\n", (tp.skip_ofst + tp.skip_len) * 8);
+	return 0;
+}
+
+static int mps_trc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mps_trc_show, PDE(inode)->data);
+}
+
+static unsigned int xdigit2int(unsigned char c)
+{
+	return isdigit(c) ? c - '0' : tolower(c) - 'a' + 10;
+}
+
+#define TRC_PORT_NONE 0xff
+
+/*
+ * Set an MPS trace filter.  Syntax is:
+ *
+ * disable
+ *
+ * to disable tracing, or
+ *
+ * interface [snaplen=<val>] [minlen=<val>] [not] [<pattern>]...
+ *
+ * where interface is one of rxN, txN, or loopbackN, N = 0..3, and pattern
+ * has the form
+ *
+ * <pattern data>[/<pattern mask>][@<anchor>]
+ *
+ * Up to 2 filter patterns can be specified.  If 2 are supplied the first one
+ * must be anchored at 0.  An omited mask is taken as a mask of 1s, an omitted
+ * anchor is taken as 0.
+ */
+static ssize_t mps_trc_write(struct file *file, const char __user *buf,
+			     size_t count, loff_t *pos)
+{
+	int i, j, enable;
+	u32 *data, *mask;
+	struct trace_params tp;
+	char *s, *p, *word, *end;
+	struct adapter *adap = PDE(file->f_path.dentry->d_inode)->data;
+	unsigned int trcidx = (uintptr_t)adap & 3;
+
+	adap = (void *)adap - trcidx;
+
+	/*
+	 * Don't accept input more than 1K, can't be anything valid except lots
+	 * of whitespace.  Well, use less.
+	 */
+	if (count > 1024)
+		return -EFBIG;
+	p = s = kzalloc(count + 1, GFP_USER);
+	if (!s)
+		return -ENOMEM;
+	if (copy_from_user(s, buf, count)) {
+		count = -EFAULT;
+		goto out;
+	}
+
+	if (s[count - 1] == '\n')
+		s[count - 1] = '\0';
+
+	enable = strcmp("disable", s) != 0;
+	if (!enable)
+		goto apply;
+
+	memset(&tp, 0, sizeof(tp));
+	tp.port = TRC_PORT_NONE;
+	i = 0;                                      /* counts pattern nibbles */
+
+	while (p) {
+		while (isspace(*p))
+			p++;
+		word = strsep(&p, " ");
+		if (!*word)
+			break;
+
+		if (!strncmp(word, "snaplen=", 8)) {
+			j = simple_strtoul(word + 8, &end, 10);
+			if (*end || j > 9600) {
+inval:				count = -EINVAL;
+				goto out;
+			}
+			tp.snap_len = j;
+			continue;
+		}
+		if (!strncmp(word, "minlen=", 7)) {
+			j = simple_strtoul(word + 7, &end, 10);
+			if (*end || j > M_TFMINPKTSIZE)
+				goto inval;
+			tp.min_len = j;
+			continue;
+		}
+		if (!strcmp(word, "not")) {
+			tp.invert = !tp.invert;
+			continue;
+		}
+		if (!strncmp(word, "loopback", 8) && tp.port == TRC_PORT_NONE) {
+			if (word[8] < '0' || word[8] > '3' || word[9])
+				goto inval;
+			tp.port = word[8] - '0' + 8;
+			continue;
+		}
+		if (!strncmp(word, "tx", 2) && tp.port == TRC_PORT_NONE) {
+			if (word[2] < '0' || word[2] > '3' || word[3])
+				goto inval;
+			tp.port = word[2] - '0' + 4;
+			if (adap->chan_map[tp.port & 3] >= MAX_NPORTS)
+				goto inval;
+			continue;
+		}
+		if (!strncmp(word, "rx", 2) && tp.port == TRC_PORT_NONE) {
+			if (word[2] < '0' || word[2] > '3' || word[3])
+				goto inval;
+			tp.port = word[2] - '0';
+			if (adap->chan_map[tp.port] >= MAX_NPORTS)
+				goto inval;
+			continue;
+		}
+		if (!isxdigit(*word))
+			goto inval;
+
+		/* we have found a trace pattern */
+		if (i) {                            /* split pattern */
+			if (tp.skip_len)            /* too many splits */
+				goto inval;
+			tp.skip_ofst = i / 16;
+		}
+
+		data = &tp.data[i / 8];
+		mask = &tp.mask[i / 8];
+		j = i;
+
+		while (isxdigit(*word)) {
+			if (i >= TRACE_LEN * 2) {
+				count = -EFBIG;
+				goto out;
+			}
+			*data = (*data << 4) + xdigit2int(*word++);
+			if (++i % 8 == 0)
+				data++;
+		}
+		if (*word == '/') {
+			word++;
+			while (isxdigit(*word)) {
+				if (j >= i)         /* mask longer than data */
+					goto inval;
+				*mask = (*mask << 4) + xdigit2int(*word++);
+				if (++j % 8 == 0)
+					mask++;
+			}
+			if (i != j)                 /* mask shorter than data */
+				goto inval;
+		} else {                            /* no mask, use all 1s */
+			for ( ; i - j >= 8; j += 8)
+				*mask++ = 0xffffffff;
+			if (i % 8)
+				*mask = (1 << (i % 8) * 4) - 1;
+		}
+		if (*word == '@') {
+			j = simple_strtoul(word + 1, &end, 10);
+			if (*end && *end != '\n')
+				goto inval;
+			if (j & 7)          /* doesn't start at multiple of 8 */
+				goto inval;
+			j /= 8;
+			if (j < tp.skip_ofst)     /* overlaps earlier pattern */
+				goto inval;
+			if (j - tp.skip_ofst > 31)            /* skip too big */
+				goto inval;
+			tp.skip_len = j - tp.skip_ofst;
+		}
+		if (i % 8) {
+			*data <<= (8 - i % 8) * 4;
+			*mask <<= (8 - i % 8) * 4;
+			i = (i + 15) & ~15;         /* 8-byte align */
+		}
+	}
+
+	if (tp.port == TRC_PORT_NONE)
+		goto inval;
+
+#if 0
+	if (tp.port < 8)
+		printk("tracer is capturing %s %s, ",
+			adap->port[adap->chan_map[tp.port & 3]]->name,
+			tp.port < 4 ? "Rx" : "Tx");
+	else
+		printk("tracer is capturing loopback %u, ", tp.port - 8);
+	printk("snap length: %u, min length: %u\n", tp.snap_len, tp.min_len);
+	printk("packets captured %smatch filter\n", tp.invert ? "do not " : "");
+
+	if (tp.skip_ofst) {
+		printk("filter pattern: ");
+		for (i = 0; i < tp.skip_ofst * 2; i += 2)
+			printk("%08x%08x", tp.data[i], tp.data[i + 1]);
+		printk("/");
+		for (i = 0; i < tp.skip_ofst * 2; i += 2)
+			printk("%08x%08x", tp.mask[i], tp.mask[i + 1]);
+		printk("@0\n");
+	}
+
+	printk("filter pattern: ");
+	for (i = tp.skip_ofst * 2; i < TRACE_LEN / 4; i += 2)
+		printk("%08x%08x", tp.data[i], tp.data[i + 1]);
+	printk("/");
+	for (i = tp.skip_ofst * 2; i < TRACE_LEN / 4; i += 2)
+		printk("%08x%08x", tp.mask[i], tp.mask[i + 1]);
+	printk("@%u\n", (tp.skip_ofst + tp.skip_len) * 8);
+#endif
+
+apply:
+	i = t4_set_trace_filter(adap, &tp, trcidx, enable);
+	if (i)
+		count = i;
+out:
+	kfree(s);
+	return count;
+}
+
+static const struct file_operations mps_trc_proc_fops = {
+	.owner   = THIS_MODULE,
+	.open    = mps_trc_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+	.write   = mps_trc_write
+};
+
+static int tid_info_show(struct seq_file *seq, void *v)
+{
+	struct adapter *adap = seq->private;
+	const struct tid_info *t = &adap->tids;
+
+	if (t4_read_reg(adap, A_LE_DB_CONFIG) & F_HASHEN) {
+		unsigned int sb = t4_read_reg(adap, A_LE_DB_SERVER_INDEX) / 4;
+
+		if (sb)
+			seq_printf(seq, "TID range: 0..%u/%u..%u", sb - 1,
+				   t4_read_reg(adap, A_LE_DB_TID_HASHBASE) / 4,
+				   t->ntids - 1);
+		else if (adap->flags & FW_OFLD_CONN)
+			seq_printf(seq, "TID range: %u..%u/%u..%u", t->aftid_base,
+				   t->aftid_end,
+				   t4_read_reg(adap, A_LE_DB_TID_HASHBASE) / 4,
+				   t->ntids - 1);
+		else
+			seq_printf(seq, "TID range: %u..%u",
+				   t4_read_reg(adap, A_LE_DB_TID_HASHBASE) / 4,
+				   t->ntids - 1);
+	} else
+		seq_printf(seq, "TID range: 0..%u", t->ntids - 1);
+	seq_printf(seq, ", in use: %u\n", atomic_read(&t->tids_in_use));
+	seq_printf(seq, "STID range: %u..%u, in use: %u\n", t->stid_base,
+		   t->stid_base + t->nstids - 1, t->stids_in_use);
+	seq_printf(seq, "ATID range: 0..%u, in use: %u\n", t->natids - 1,
+		   t->atids_in_use);
+	seq_printf(seq, "FTID range: %u..%u\n", t->ftid_base,
+		   t->ftid_base + t->nftids - 1);
+	if (t->nsftids)
+		seq_printf(seq, "SFTID range: %u..%u\n", t->sftid_base,
+			   t->sftid_base + t->nsftids - 1);
+	seq_printf(seq, "HW TID usage: %u IP users, %u IPv6 users\n",
+		   t4_read_reg(adap, A_LE_DB_ACT_CNT_IPV4),
+		   t4_read_reg(adap, A_LE_DB_ACT_CNT_IPV6));
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(tid_info);
+
+static int uld_show(struct seq_file *seq, void *v)
+{
+	int i;
+	struct adapter *adap = seq->private;
+
+	for (i = 0; i < CXGB4_ULD_MAX; i++)
+		if (adap->uld_handle[i])
+			seq_printf(seq, "%s: %s\n", uld_str[i], ulds[i].name);
+	return 0;
+}
+
+DEFINE_SIMPLE_PROC_FILE(uld);
+
+enum {
+	ADAP_NEED_L2T  = 1 << 0,
+	ADAP_NEED_OFLD = 1 << 1,
+	ADAP_NEED_FILT = 1 << 2,
+};
+
+struct cxgb4_proc_entry {
+	const char *name;
+	mode_t mode;
+	unsigned int req;      /* adapter requirements to create this file */
+	unsigned char data;    /* data passed in low bits of adapter pointer */
+	const struct file_operations *fops;
+};
+
+static struct cxgb4_proc_entry proc_files[] = {
+	{ "cctrl", 0444, 0, 0, &cctrl_tbl_proc_fops },
+	{ "cpl_stats", 0444, 0, 0, &cpl_stats_proc_fops },
+	{ "ddp_stats", 0444, ADAP_NEED_OFLD, 0, &ddp_stats_proc_fops },
+	{ "fcoe_stats", 0444, 0, 0, &fcoe_stats_proc_fops },
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	{ "filters", 0444, 0, 0, &filters_proc_fops },
+#endif
+	{ "hw_sched", 0444, 0, 0, &sched_proc_fops },
+#if defined(CONFIG_PROC_FS) && defined(CONFIG_CHELSIO_T4_OFFLOAD)
+	{ "l2t", 0444, ADAP_NEED_L2T, 0, &t4_l2t_proc_fops },
+#endif
+	{ "lb_stats", 0444, 0, 0, &lb_stats_proc_fops },
+	{ "meminfo", 0444, 0, 0, &meminfo_proc_fops },
+	{ "path_mtus", 0644, 0, 0, &mtutab_proc_fops },
+	{ "pm_stats", 0644, 0, 0, &pm_stats_proc_fops },
+	{ "qstats", 0444, 0, 0, &sge_stats_proc_fops },
+	{ "rdma_stats", 0444, ADAP_NEED_OFLD, 0, &rdma_stats_proc_fops },
+	{ "rss", 0444, 0, 0, &rss_proc_fops },
+	{ "rss_config", 0444, 0, 0, &rss_config_proc_fops },
+	{ "rss_key", 0600, 0, 0, &rss_key_proc_fops },
+	{ "rss_pf_config", 0444, 0, 0, &rss_pf_config_proc_fops },
+	{ "rss_vf_config", 0444, 0, 0, &rss_vf_config_proc_fops },
+	{ "tcp_stats", 0444, 0, 0, &tcp_stats_proc_fops },
+	{ "tids", 0444, ADAP_NEED_OFLD, 0, &tid_info_proc_fops },
+	{ "tp_err_stats", 0444, 0, 0, &tp_err_stats_proc_fops },
+	{ "trace0", 0644, 0, 0, &mps_trc_proc_fops },
+	{ "trace1", 0644, 0, 1, &mps_trc_proc_fops },
+	{ "trace2", 0644, 0, 2, &mps_trc_proc_fops },
+	{ "trace3", 0644, 0, 3, &mps_trc_proc_fops },
+	{ "tx_rate", 0444, 0, 0, &tx_rate_proc_fops },
+	{ "uld", 0444, 0, 0, &uld_proc_fops },
+};
+
+static int __devinit setup_proc(struct adapter *adap,
+				struct proc_dir_entry *dir)
+{
+	int i, created;
+	struct proc_dir_entry *p;
+	int ofld = is_offload(adap);
+
+	if (!dir)
+		return -EINVAL;
+
+	/* If we can create any of the entries we do. */
+	for (created = i = 0; i < ARRAY_SIZE(proc_files); ++i) {
+		unsigned int req = proc_files[i].req;
+
+		if ((req & ADAP_NEED_OFLD) && !ofld)
+			continue;
+		if ((req & ADAP_NEED_L2T) && !adap->l2t)
+			continue;
+
+		p = proc_create_data(proc_files[i].name, proc_files[i].mode,
+				     dir, proc_files[i].fops,
+				     (void *)adap + proc_files[i].data);
+		if (p)
+			created++;
+	}
+
+	return created;
+}
+
+static void cleanup_proc(struct adapter *adap, struct proc_dir_entry *dir)
+{
+	int i;
+	int ofld = is_offload(adap);
+
+	for (i = 0; i < ARRAY_SIZE(proc_files); ++i) {
+		unsigned int req = proc_files[i].req;
+
+		if ((req & ADAP_NEED_OFLD) && !ofld)
+			continue;
+		if ((req & ADAP_NEED_L2T) && !adap->l2t)
+			continue;
+
+		remove_proc_entry(proc_files[i].name, dir);
+	}
+}
+
+/*
+ * offload upper-layer driver support
+ */
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#include <net/offload.h>
+#include "cxgb4_ctl_defs.h"
+
+/*
+ * Allocate an active-open TID and set it to the supplied value.
+ */
+int cxgb4_alloc_atid(struct tid_info *t, void *data)
+{
+	int atid = -1;
+
+	spin_lock_bh(&t->atid_lock);
+	if (t->afree) {
+		union aopen_entry *p = t->afree;
+
+		atid = p - t->atid_tab;
+		t->afree = p->next;
+		p->data = data;
+		t->atids_in_use++;
+	}
+	spin_unlock_bh(&t->atid_lock);
+	return atid;
+}
+EXPORT_SYMBOL(cxgb4_alloc_atid);
+
+/*
+ * Release an active-open TID.
+ */
+void cxgb4_free_atid(struct tid_info *t, unsigned int atid)
+{
+	union aopen_entry *p = &t->atid_tab[atid];
+
+	spin_lock_bh(&t->atid_lock);
+	p->next = t->afree;
+	t->afree = p;
+	t->atids_in_use--;
+	spin_unlock_bh(&t->atid_lock);
+}
+EXPORT_SYMBOL(cxgb4_free_atid);
+
+/*
+ * Allocate a server TID and set it to the supplied value.
+ */
+int cxgb4_alloc_stid(struct tid_info *t, int family, void *data)
+{
+	int stid;
+
+	spin_lock_bh(&t->stid_lock);
+	if (family == PF_INET) {
+		stid = find_first_zero_bit(t->stid_bmap, t->nstids);
+		if (stid < t->nstids)
+			__set_bit(stid, t->stid_bmap);
+		else
+			stid = -1;
+	} else {
+		stid = bitmap_find_free_region(t->stid_bmap, t->nstids, 2);
+		if (stid < 0)
+			stid = -1;
+	}
+	if (stid >= 0) {
+		t->stid_tab[stid].data = data;
+		stid += t->stid_base;
+		t->stids_in_use++;
+	}
+	spin_unlock_bh(&t->stid_lock);
+	return stid;
+}
+EXPORT_SYMBOL(cxgb4_alloc_stid);
+
+/*
+ * Allocate a server filter TID and set it to the supplied value.
+ */
+int cxgb4_alloc_sftid(struct tid_info *t, int family, void *data)
+{
+	int stid;
+
+	spin_lock_bh(&t->stid_lock);
+	if (family == PF_INET) {
+		stid = find_next_zero_bit(t->stid_bmap,
+			 t->nstids + t->nsftids, t->nstids);
+		if (stid < (t->nstids + t->nsftids))
+			__set_bit(stid, t->stid_bmap);
+		else
+			stid = -1;
+	} else {
+			stid = -1;
+	}
+	if (stid >= 0) {
+		t->stid_tab[stid].data = data;
+		stid += t->stid_base;
+		t->stids_in_use++;
+	}
+	spin_unlock_bh(&t->stid_lock);
+	return stid;
+}
+EXPORT_SYMBOL(cxgb4_alloc_sftid);
+
+/*
+ * Release a server TID.
+ */
+void cxgb4_free_stid(struct tid_info *t, unsigned int stid, int family)
+{
+	stid -= t->stid_base;
+	spin_lock_bh(&t->stid_lock);
+	if (family == PF_INET)
+		__clear_bit(stid, t->stid_bmap);
+	else
+		bitmap_release_region(t->stid_bmap, stid, 2);
+	t->stid_tab[stid].data = NULL;
+	t->stids_in_use--;
+	spin_unlock_bh(&t->stid_lock);
+}
+EXPORT_SYMBOL(cxgb4_free_stid);
+
+/*
+ * Populate a TID_RELEASE WR.  Caller must properly size the skb.
+ */
+static void mk_tid_release(struct sk_buff *skb, unsigned int chan,
+			   unsigned int tid)
+{
+	struct cpl_tid_release *req;
+
+	set_wr_txq(skb, CPL_PRIORITY_SETUP, chan);
+	req = (struct cpl_tid_release *)__skb_put(skb, sizeof(*req));
+	INIT_TP_WR_MIT_CPL(req, CPL_TID_RELEASE, tid);
+}
+
+/*
+ * Queue a TID release request and if necessary schedule a work queue to
+ * process it.
+ */
+static void cxgb4_queue_tid_release(struct tid_info *t, unsigned int chan,
+			     unsigned int tid)
+{
+	void **p = &t->tid_tab[tid];
+	struct adapter *adap = container_of(t, struct adapter, tids);
+
+	spin_lock_bh(&adap->tid_release_lock);
+	*p = adap->tid_release_head;
+	/* Low 2 bits encode the Tx channel number */
+	adap->tid_release_head = (void **)((uintptr_t)p | chan);
+	if (!*p)
+		queue_work(workq, &adap->tid_release_task);
+	spin_unlock_bh(&adap->tid_release_lock);
+}
+
+/*
+ * Process the list of pending TID release requests.
+ */
+static void process_tid_release_list(struct work_struct *work)
+{
+	struct sk_buff *skb;
+	struct adapter *adap;
+
+	adap = container_of(work, struct adapter, tid_release_task);
+
+	spin_lock_bh(&adap->tid_release_lock);
+	while (adap->tid_release_head) {
+		void **p = adap->tid_release_head;
+		unsigned int chan = (uintptr_t)p & 3;
+		p = (void *)p - chan;
+
+		adap->tid_release_head = *p;
+		*p = NULL;
+		spin_unlock_bh(&adap->tid_release_lock);
+
+		while (!(skb = alloc_skb(sizeof(struct cpl_tid_release),
+					 GFP_KERNEL)))
+			yield();
+
+		mk_tid_release(skb, chan, p - adap->tids.tid_tab);
+		t4_ofld_send(adap, skb);
+		spin_lock_bh(&adap->tid_release_lock);
+	}
+	spin_unlock_bh(&adap->tid_release_lock);
+}
+
+/*
+ * Release a TID and inform HW.  If we are unable to allocate the release
+ * message we defer to a work queue.
+ */
+void cxgb4_remove_tid(struct tid_info *t, unsigned int chan, unsigned int tid)
+{
+	struct sk_buff *skb;
+	struct adapter *adap = container_of(t, struct adapter, tids);
+
+	WARN_ON(tid >= t->ntids);
+
+	if (t->tid_tab[tid]) {
+		t->tid_tab[tid] = NULL;
+		atomic_dec(&t->tids_in_use);
+	}
+
+	skb = alloc_skb(sizeof(struct cpl_tid_release), GFP_ATOMIC);
+	if (likely(skb)) {
+		mk_tid_release(skb, chan, tid);
+		t4_ofld_send(adap, skb);
+	} else
+		cxgb4_queue_tid_release(t, chan, tid);
+}
+EXPORT_SYMBOL(cxgb4_remove_tid);
+
+/*
+ * Allocate and initialize the TID tables.  Returns 0 on success.
+ */
+static int tid_init(struct tid_info *t)
+{
+	size_t size;
+	unsigned int stid_bmap_size;
+	unsigned int natids = t->natids;
+
+	stid_bmap_size = BITS_TO_LONGS(t->nstids + t->nsftids);
+	size = t->ntids * sizeof(*t->tid_tab) +
+	       natids * sizeof(*t->atid_tab) +
+	       t->nstids * sizeof(*t->stid_tab) +
+	       t->nsftids * sizeof(*t->stid_tab) +
+	       stid_bmap_size * sizeof(long) +
+	       t->nftids * sizeof(*t->ftid_tab) +
+	       t->nsftids * sizeof(*t->ftid_tab);
+
+	t->tid_tab = t4_alloc_mem(size);
+	if (!t->tid_tab)
+		return -ENOMEM;
+
+	t->atid_tab = (union aopen_entry *)&t->tid_tab[t->ntids];
+	t->stid_tab = (struct serv_entry *)&t->atid_tab[natids];
+	t->stid_bmap = (unsigned long *)&t->stid_tab[t->nstids + t->nsftids];
+	t->ftid_tab = (struct filter_entry *)&t->stid_bmap[stid_bmap_size];
+	spin_lock_init(&t->stid_lock);
+	spin_lock_init(&t->atid_lock);
+
+	t->stids_in_use = 0;
+	t->afree = NULL;
+	t->atids_in_use = 0;
+	atomic_set(&t->tids_in_use, 0);
+
+	/* Setup the free list for atid_tab and clear the stid bitmap. */
+	if (natids) {
+		while (--natids)
+			t->atid_tab[natids - 1].next = &t->atid_tab[natids];
+		t->afree = t->atid_tab;
+	}
+	bitmap_zero(t->stid_bmap, t->nstids + t->nsftids);
+	return 0;
+}
+
+/**
+ *	cxgb4_create_server - create an IP server
+ *	@dev: the device
+ *	@stid: the server TID
+ *	@sip: local IP address to bind server to
+ *	@sport: the server's TCP port
+ *	@queue: queue to direct messages from this server to
+ *
+ *	Create an IP server for the given port and address.
+ *	Returns <0 on error and one of the %NET_XMIT_* values on success.
+ */
+int cxgb4_create_server(const struct net_device *dev, unsigned int stid,
+			__be32 sip, __be16 sport, unsigned int queue)
+{
+	unsigned int chan;
+	struct sk_buff *skb;
+	struct adapter *adap;
+	struct cpl_pass_open_req *req;
+
+	skb = alloc_skb(sizeof(*req), GFP_KERNEL);
+	if (!skb)
+		return -ENOMEM;
+
+	adap = netdev2adap(dev);
+	req = (struct cpl_pass_open_req *)__skb_put(skb, sizeof(*req));
+	INIT_TP_WR(req, 0);
+	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_PASS_OPEN_REQ, stid));
+	req->local_port = sport;
+	req->peer_port = htons(0);
+	req->local_ip = sip;
+	req->peer_ip = htonl(0);
+	chan = rxq_to_chan(&adap->sge, queue);
+	req->opt0 = cpu_to_be64(V_TX_CHAN(chan));
+	req->opt1 = cpu_to_be64(V_CONN_POLICY(CPL_CONN_POLICY_ASK) |
+				F_SYN_RSS_ENABLE | V_SYN_RSS_QUEUE(queue));
+	return t4_mgmt_tx(adap, skb);
+}
+EXPORT_SYMBOL(cxgb4_create_server);
+
+/**
+ *	cxgb4_create_server6 - create an IPv6 server
+ *	@dev: the device
+ *	@stid: the server TID
+ *	@sip: local IPv6 address to bind server to
+ *	@sport: the server's TCP port
+ *	@queue: queue to direct messages from this server to
+ *
+ *	Create an IPv6 server for the given port and address.
+ *	Returns <0 on error and one of the %NET_XMIT_* values on success.
+ */
+int cxgb4_create_server6(const struct net_device *dev, unsigned int stid,
+			 const struct in6_addr *sip, __be16 sport,
+			 unsigned int queue)
+{
+	unsigned int chan;
+	struct sk_buff *skb;
+	struct adapter *adap;
+	struct cpl_pass_open_req6 *req;
+
+	skb = alloc_skb(sizeof(*req), GFP_KERNEL);
+	if (!skb)
+		return -ENOMEM;
+
+	adap = netdev2adap(dev);
+	req = (struct cpl_pass_open_req6 *)__skb_put(skb, sizeof(*req));
+	INIT_TP_WR(req, 0);
+	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_PASS_OPEN_REQ6, stid));
+	req->local_port = sport;
+	req->peer_port = htons(0);
+	req->local_ip_hi = *(__be64 *)(sip->s6_addr);
+	req->local_ip_lo = *(__be64 *)(sip->s6_addr + 8);
+	req->peer_ip_hi = cpu_to_be64(0);
+	req->peer_ip_lo = cpu_to_be64(0);
+	chan = rxq_to_chan(&adap->sge, queue);
+	req->opt0 = cpu_to_be64(V_TX_CHAN(chan));
+	req->opt1 = cpu_to_be64(V_CONN_POLICY(CPL_CONN_POLICY_ASK) |
+				F_SYN_RSS_ENABLE | V_SYN_RSS_QUEUE(queue));
+	return t4_mgmt_tx(adap, skb);
+}
+EXPORT_SYMBOL(cxgb4_create_server6);
+
+int cxgb4_remove_server(const struct net_device *dev, unsigned int stid, unsigned int queue, bool ipv6)
+{
+        struct sk_buff *skb;
+        struct adapter *adap;
+        struct cpl_close_listsvr_req *req;
+
+        skb = alloc_skb(sizeof(*req), GFP_KERNEL);
+        if (!skb)
+                return -ENOMEM;
+
+        adap = netdev2adap(dev);
+        req = (struct cpl_close_listsvr_req *)__skb_put(skb, sizeof(*req));
+        INIT_TP_WR(req, 0);
+        OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_CLOSE_LISTSRV_REQ, stid));
+	req->reply_ctrl = htons(V_NO_REPLY(0) | (ipv6 ? V_LISTSVR_IPV6(1) : V_LISTSVR_IPV6(0)) | V_QUEUENO(queue));
+        return t4_mgmt_tx(adap, skb);
+}
+EXPORT_SYMBOL(cxgb4_remove_server);
+
+int cxgb4_clip_get(const struct net_device *dev, const struct in6_addr *lip)
+{
+	struct adapter *adap;
+        struct fw_clip_cmd c;
+
+	adap = netdev2adap(dev);
+        memset(&c, 0, sizeof(c));
+        c.op_to_write = htonl(V_FW_CMD_OP(FW_CLIP_CMD) |
+                            F_FW_CMD_REQUEST | F_FW_CMD_WRITE);
+        c.alloc_to_len16 = htonl(F_FW_CLIP_CMD_ALLOC | FW_LEN16(c));
+        *(__be64 *)&c.ip_hi = *(__be64 *)(lip->s6_addr);
+        *(__be64 *)&c.ip_lo = *(__be64 *)(lip->s6_addr + 8);
+        return t4_wr_mbox_meat(adap, adap->mbox, &c, sizeof(c), &c, false);
+}
+EXPORT_SYMBOL(cxgb4_clip_get);
+
+int cxgb4_clip_release(const struct net_device *dev, const struct in6_addr *lip)
+{
+	struct adapter *adap;
+        struct fw_clip_cmd c;
+
+	adap = netdev2adap(dev);
+        memset(&c, 0, sizeof(c));
+        c.op_to_write = htonl(V_FW_CMD_OP(FW_CLIP_CMD) |
+                            F_FW_CMD_REQUEST | F_FW_CMD_READ);
+        c.alloc_to_len16 = htonl(F_FW_CLIP_CMD_FREE | FW_LEN16(c));
+        *(__be64 *)&c.ip_hi = *(__be64 *)(lip->s6_addr);
+        *(__be64 *)&c.ip_lo = *(__be64 *)(lip->s6_addr + 8);
+        return t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);
+}
+EXPORT_SYMBOL(cxgb4_clip_release);
+
+/**
+ *	cxgb4_setup_ddpbuf - set up a DDP buffer
+ *	@pdev: the PCI device
+ *	@bus_addr: bus addresses of pages making up the buffer
+ *	@naddr: number of entries in @bus_addr
+ *	@tid: connection id associated with the buffer
+ *	@tag: HW buffer tag
+ *	@len: buffer length
+ *	@pg_ofst: buffer start offset into first page
+ *	@color: buffer generation
+ *
+ *	Sets up a buffer for direct data placement.
+ */
+int cxgb4_setup_ddpbuf(struct pci_dev *pdev, const dma_addr_t *bus_addr,
+		       unsigned int naddr, unsigned int tid, unsigned int tag,
+		       unsigned int len, unsigned int pg_ofst,
+		       unsigned int color)
+{
+	__be64 w0, w1;
+	unsigned int n;
+	struct adapter *adap = pci_get_drvdata(pdev);
+	volatile void __iomem *addr = adap->regs + MEMWIN0_BASE;
+
+	w0 = cpu_to_be64(F_PPOD_VALID | V_PPOD_TID(tid) | V_PPOD_TAG(tag) |
+			 V_PPOD_COLOR(color));
+	w1 = cpu_to_be64(V_PPOD_LEN(len) | V_PPOD_OFST(pg_ofst));
+	n = tag * sizeof(struct pagepod) + adap->vres.ddp.start;
+
+	spin_lock(&adap->win0_lock);
+
+	/*
+	 * Move window to first pagepod.  (Read back MA register to ensure
+	 * that changes propagate before we attempt to use the new values.)
+	 */
+	t4_write_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0), n);
+	t4_read_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0));
+
+	for ( ; naddr; naddr -= PPOD_PAGES) {
+		writeq(w0, addr);
+		writeq(w1, addr + 8);
+		writeq(0, addr + 16);
+		addr += 24;
+
+		n = min(naddr, PPOD_PAGES + 1);
+		for ( ; n; n--, addr += 8, bus_addr++)
+			writeq(cpu_to_be64(*bus_addr), addr);
+		if (naddr <= PPOD_PAGES) {
+			for ( ; naddr <= PPOD_PAGES; naddr++, addr += 8)
+				writeq(0, addr);
+			break;
+		}
+		bus_addr--;
+	}
+	t4_read_reg(adap, MEMWIN0_BASE);   /* flush */
+	spin_unlock(&adap->win0_lock);
+	return 0;
+}
+EXPORT_SYMBOL(cxgb4_setup_ddpbuf);
+
+#define __DBG_MEMWIN_ISCSI_PAGEPOD__
+#define MAX_PPOD_PER_PCIE_MEMWIN   (MEMWIN0_APERTURE/sizeof(struct pagepod))
+/**
+ *	cxgb4_setup_iscsi_pagepod - set up iscsi DDP pagepods
+ *	@pdev: the PCI device
+ *	@ppod_hdr: the pagepod header settings
+ *	@bus_addr: bus addresses of pages making up the buffer
+ *	@naddr: number of entries in @bus_addr
+ *	@idx: pagepod index
+ *	@max: # of pagepod to be written
+ *
+ *	Sets up iscsi pagepods for iscsi direct data placement.
+ */
+int cxgb4_setup_iscsi_pagepod(struct pci_dev *pdev, void *ppod_hdr,
+				dma_addr_t *bus_addr, unsigned int naddr,
+				unsigned int idx, unsigned int max)
+{
+	struct pagepod *ppod = (struct pagepod *)ppod_hdr;
+	struct adapter *adap = pci_get_drvdata(pdev);
+	unsigned int pidx = 0;
+	int memwin_loop = (max + MAX_PPOD_PER_PCIE_MEMWIN - 1) /
+				MAX_PPOD_PER_PCIE_MEMWIN;
+	int i, j, k;
+	int err = 0;
+
+	if (!bus_addr || !naddr) {
+		printk(KERN_ERR "%s: bus addr 0x%p, naddr %u INVALID.\n",
+			__func__, bus_addr, naddr);
+		return -EINVAL;
+	}
+
+	if ((max * 4) < naddr) {
+		printk(KERN_ERR "%s: max ppod %u * 4 < # of bus_addr %u.\n",
+			__func__, max, naddr);
+		return -EINVAL;
+	}
+
+	spin_lock(&adap->win0_lock);
+
+	for (i = 0; i < memwin_loop; i++) {
+		unsigned int addr = MEMWIN0_BASE;
+                unsigned int ppod_start = idx * sizeof(struct pagepod) +
+					adap->vres.iscsi.start;
+		unsigned int nppod = max < MAX_PPOD_PER_PCIE_MEMWIN ?
+					max : MAX_PPOD_PER_PCIE_MEMWIN;
+#ifdef __DBG_MEMWIN_ISCSI_PAGEPOD__
+		unsigned int pidx_s = pidx;
+		unsigned int addr_s = addr;
+#endif
+
+		/*
+		 * Move window to pagepod start.  (Read back MA register to
+		 * ensure that changes propagate before we attempt to use the
+		 * new values.)
+		 */
+		t4_write_reg(adap,
+			PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0),
+			ppod_start);
+		t4_read_reg(adap,
+			PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0));
+
+		max -= nppod;
+                idx += nppod;
+
+		for (j = 0; j < nppod; j++) {
+			/* ppod header */
+			t4_write_reg64(adap, addr,
+					ppod->vld_tid_pgsz_tag_color);
+			addr += 8;
+			t4_write_reg64(adap, addr, ppod->len_offset);
+			addr += 8;
+			t4_write_reg64(adap, addr, 0ULL);
+			addr += 8;
+
+			for (k = 0; k < (PPOD_PAGES + 1);
+				k++, pidx++, addr+=8) {
+				if (pidx < naddr) {
+					if (!(bus_addr[pidx])) {
+						printk(KERN_ERR
+						"%s: bus addr %u INVALID.\n",
+						__func__, pidx);
+						err = -EINVAL;
+						goto done;
+					}
+					t4_write_reg64(adap, addr,
+						cpu_to_be64(bus_addr[pidx]));
+				} else 
+					t4_write_reg64(adap, addr, 0ULL);
+			}	
+			pidx--;
+		}
+
+		/* flush */
+		t4_read_reg(adap, MEMWIN0_BASE);
+
+#ifdef __DBG_MEMWIN_ISCSI_PAGEPOD__
+		/* read it back */
+		for (j = 0; j < nppod; j++) {
+			u64 val;
+
+			/* ppod header */
+			val = t4_read_reg64(adap, addr_s);
+			if (val != ppod->vld_tid_pgsz_tag_color) {
+				printk(KERN_ERR
+				"%s: loop %d, nppod %d, vld 0x%llx/0x%llx.\n",
+				__func__, i, j, val,
+				ppod->vld_tid_pgsz_tag_color);
+				err = -EIO;
+			}
+			addr_s += 8;
+
+			val = t4_read_reg64(adap, addr_s);
+			if (val != ppod->len_offset) {
+				printk(KERN_ERR
+				"%s: loop %d, nppod %d, len 0x%llx/0x%llx.\n",
+				__func__, i, j, val, ppod->len_offset);
+				err = -EIO;
+			}
+			addr_s += 8;
+
+			val = t4_read_reg64(adap, addr_s);
+			if (val) {
+				printk(KERN_ERR
+				"%s: loop %d, nppod %d, rsvd 0x%llx.\n",
+				__func__, i, j, val);
+				err = -EIO;
+			}
+			addr_s += 8;
+
+			for (k = 0; k < (PPOD_PAGES + 1);
+				k++, pidx_s++, addr_s += 8) {
+				val = t4_read_reg64(adap, addr_s);
+				if (pidx_s < naddr) {
+					val = be64_to_cpu(val);
+					if (val != bus_addr[pidx_s]) {
+						printk(KERN_ERR
+						"%s: loop %d, nppod %d, "
+						"page %d, pidx %u, 0x%llx"
+						" != 0x%llx.\n",
+						__func__, i, j, k, pidx_s,
+						val, bus_addr[pidx_s]);
+						err = -EIO;
+					}
+				} else if (val) {
+					printk(KERN_ERR
+					"%s: loop %d, nppod %d, page %d, "
+					"pidx %u/%u, 0x%llx non-zero.\n",
+					__func__, i, j, k, pidx_s, naddr, val);
+					err = -EIO;
+				}
+			}
+			pidx_s--;
+
+			if (err < 0)
+				goto done;
+		}
+#endif
+	}
+
+done:
+	spin_unlock(&adap->win0_lock);
+	return err;
+}
+EXPORT_SYMBOL(cxgb4_setup_iscsi_pagepod);
+
+/**
+ *	cxgb4_clear_iscsi_pagepod - clear iscsi DDP pagepods
+ *	@pdev: the PCI device
+ *	@idx: pagepod index
+ *	@max: # of pagepod to be cleared
+ *
+ *	clear settings for iscsi direct data placement.
+ */
+int cxgb4_clear_iscsi_pagepod(struct pci_dev *pdev,
+				unsigned int idx, unsigned int max)
+{
+	struct adapter *adap = pci_get_drvdata(pdev);
+	int memwin_loop = (max + MAX_PPOD_PER_PCIE_MEMWIN - 1) /
+				MAX_PPOD_PER_PCIE_MEMWIN;
+	int i, j;
+	int err = 0;
+
+	spin_lock(&adap->win0_lock);
+
+	for (i = 0; i < memwin_loop; i++) {
+		unsigned int addr = MEMWIN0_BASE;
+                unsigned int ppod_start = idx * sizeof(struct pagepod) +
+					adap->vres.iscsi.start;
+		unsigned int nppod = max < MAX_PPOD_PER_PCIE_MEMWIN ?
+					max : MAX_PPOD_PER_PCIE_MEMWIN;
+#ifdef __DBG_MEMWIN_ISCSI_PAGEPOD__
+		unsigned int addr_s = addr;
+#endif
+
+		/*
+		 * Move window to pagepod start.  (Read back MA register to
+		 * ensure that changes propagate before we attempt to use the
+		 * new values.)
+		 */
+		t4_write_reg(adap,
+			PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0),
+			ppod_start);
+		t4_read_reg(adap,
+			PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0));
+
+		max -= nppod;
+                idx += nppod;
+
+		for (j = 0; j < nppod; j++, addr += 64)
+			t4_write_reg64(adap, addr, 0ULL);
+
+		/* flush */
+		t4_read_reg(adap, MEMWIN0_BASE);
+
+#ifdef __DBG_MEMWIN_ISCSI_PAGEPOD__
+		/* read it back */
+		for (j = 0; j < nppod; j++, addr_s += 64) {
+			u64 val;
+
+			val = t4_read_reg64(adap, addr_s);
+			if (val) {
+				printk(KERN_ERR
+				"%s: loop %d, nppod %d, clr 0x%llx non-zero.\n",
+				__func__, i, j, val);
+				err = -EIO;
+				goto done;
+			}
+		}
+#endif
+	}
+
+done:
+	spin_unlock(&adap->win0_lock);
+	return err;
+}
+EXPORT_SYMBOL(cxgb4_clear_iscsi_pagepod);
+
+static ssize_t reg_attr_show(struct device *d, char *buf, int reg, int shift,
+			     unsigned int mask)
+{
+	ssize_t len;
+	unsigned int v;
+	struct adapter *adap = netdev2adap(to_net_dev(d));
+
+	/* Synchronize with ioctls that may shut down the device */
+	mutex_lock(&adap->user_mutex);
+	v = t4_read_reg(adap, reg);
+	len = sprintf(buf, "%u\n", (v >> shift) & mask);
+	mutex_unlock(&adap->user_mutex);
+	return len;
+}
+
+static ssize_t reg_attr_store(struct device *d, const char *buf, size_t len,
+			      int reg, int shift, unsigned int mask,
+			      unsigned int min_val, unsigned int max_val)
+{
+	char *endp;
+	unsigned int val;
+	struct adapter *adap;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf || val < min_val || val > max_val)
+		return -EINVAL;
+
+	adap = netdev2adap(to_net_dev(d));
+	mutex_lock(&adap->user_mutex);
+	t4_set_reg_field(adap, reg, mask << shift, val << shift);
+	mutex_unlock(&adap->user_mutex);
+	return len;
+}
+
+#define T4_REG_SHOW(name, reg, shift, mask) \
+static ssize_t show_##name(struct device *d, struct device_attribute *attr, \
+			   char *buf) \
+{ \
+	return reg_attr_show(d, buf, reg, shift, mask); \
+}
+
+#define T4_REG_STORE(name, reg, shift, mask, min_val, max_val) \
+static ssize_t store_##name(struct device *d, struct device_attribute *attr, \
+			    const char *buf, size_t len) \
+{ \
+	return reg_attr_store(d, buf, len, reg, shift, mask, min_val, max_val); \
+}
+
+#define T4_ATTR(name, reg, shift, mask, min_val, max_val) \
+T4_REG_SHOW(name, reg, shift, mask) \
+T4_REG_STORE(name, reg, shift, mask, min_val, max_val) \
+static DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+T4_ATTR(tcp_retries1, A_TP_SHIFT_CNT, S_RXTSHIFTMAXR1, M_RXTSHIFTMAXR1, 3, 15);
+T4_ATTR(tcp_retries2, A_TP_SHIFT_CNT, S_RXTSHIFTMAXR2, M_RXTSHIFTMAXR2, 0, 15);
+T4_ATTR(tcp_syn_retries, A_TP_SHIFT_CNT, S_SYNSHIFTMAX, M_SYNSHIFTMAX, 0, 15);
+T4_ATTR(tcp_keepalive_probes, A_TP_SHIFT_CNT, S_KEEPALIVEMAXR2,
+	M_KEEPALIVEMAXR2, 1, 15);
+
+static ssize_t timer_attr_show(struct device *d, char *buf, int reg)
+{
+	ssize_t len;
+	unsigned int v, tps;
+	struct adapter *adap = netdev2adap(to_net_dev(d));
+
+	/* Synchronize with ioctls that may shut down the device */
+	mutex_lock(&adap->user_mutex);
+	v = t4_read_reg(adap, reg);
+	tps = (adap->params.vpd.cclk * 1000) >> adap->params.tp.tre;
+	len = sprintf(buf, "%u\n", v / tps);
+	mutex_unlock(&adap->user_mutex);
+	return len;
+}
+
+static ssize_t timer_attr_store(struct device *d, const char *buf, size_t len,
+				int reg, unsigned int min_val,
+				unsigned int max_val)
+{
+	char *endp;
+	unsigned int val, tps;
+	struct adapter *adap;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	adap = netdev2adap(to_net_dev(d));
+	tps = (adap->params.vpd.cclk * 1000) >> adap->params.tp.tre;
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf || val * tps < min_val || val * tps > max_val)
+		return -EINVAL;
+
+	mutex_lock(&adap->user_mutex);
+	t4_write_reg(adap, reg, val * tps);
+	mutex_unlock(&adap->user_mutex);
+	return len;
+}
+
+#define T4_TIMER_REG_SHOW(name, reg) \
+static ssize_t show_##name(struct device *d, struct device_attribute *attr, \
+			   char *buf) \
+{ \
+	return timer_attr_show(d, buf, reg); \
+}
+
+#define T4_TIMER_REG_STORE(name, reg, min_val, max_val) \
+static ssize_t store_##name(struct device *d, struct device_attribute *attr, \
+			    const char *buf, size_t len) \
+{ \
+	return timer_attr_store(d, buf, len, reg, min_val, max_val); \
+}
+
+#define T4_TIMER_ATTR(name, reg, min_val, max_val) \
+T4_TIMER_REG_SHOW(name, reg) \
+T4_TIMER_REG_STORE(name, reg, min_val, max_val) \
+static DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+T4_TIMER_ATTR(tcp_keepalive_time, A_TP_KEEP_IDLE, 0, M_KEEPALIVEIDLE);
+T4_TIMER_ATTR(tcp_keepalive_intvl, A_TP_KEEP_INTVL, 0, M_KEEPALIVEINTVL);
+T4_TIMER_ATTR(tcp_finwait2_timeout, A_TP_FINWAIT2_TIMER, 0, M_FINWAIT2TIME);
+
+static struct attribute *offload_attrs[] = {
+	&dev_attr_tcp_retries1.attr,
+	&dev_attr_tcp_retries2.attr,
+	&dev_attr_tcp_syn_retries.attr,
+	&dev_attr_tcp_keepalive_probes.attr,
+	&dev_attr_tcp_keepalive_time.attr,
+	&dev_attr_tcp_keepalive_intvl.attr,
+	&dev_attr_tcp_finwait2_timeout.attr,
+	NULL
+};
+
+static struct attribute_group offload_attr_group = { .attrs = offload_attrs };
+
+/*
+ * Dummy handler for Rx offload packets in case an offload module hasn't
+ * registered yet.  We can get such packets in response to setting up filters.
+ * We just drop them.
+ */
+static int rx_offload_blackhole(struct toedev *dev, struct sk_buff **skbs,
+				int n)
+{
+	while (n--)
+		dev_kfree_skb_any(skbs[n]);
+	return 0;
+}
+
+/*
+ * Other dummy ops usually overwritten by offload modules when they attach.
+ */
+static int dummy_can_offload(struct toedev *dev, struct sock *sk)
+{
+	return 0;
+}
+
+static int dummy_connect(struct toedev *dev, struct sock *sk,
+			 struct net_device *edev)
+{
+	return -1;
+}
+
+static void dummy_neigh_update(struct toedev *dev, struct neighbour *neigh)
+{
+}
+
+static void set_dummy_ops(struct toedev *dev)
+{
+	dev->can_offload  = dummy_can_offload;
+	dev->connect      = dummy_connect;
+	dev->neigh_update = dummy_neigh_update;
+	dev->recv         = rx_offload_blackhole;
+}
+
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+static ssize_t show_cclk(struct device *d, struct device_attribute *attr,
+			 char *buf)
+{
+	ssize_t len;
+	struct adapter *adap = netdev2adap(to_net_dev(d));
+	char temp[32];
+	unsigned int cclk_ps = 1000000000 / adap->params.vpd.cclk;  /* in ps */
+
+	/*
+	 * Display the core clock in units of ns, the same way it is
+	 * displayed in debugfs.
+	 */
+	len = sprintf(buf, "Core clock period: %s ns\n",
+		   unit_conv(temp, sizeof(temp), cclk_ps, 1000));
+
+	return len;
+}
+
+#define T4_DISPLAY_ATTR(name) \
+static DEVICE_ATTR(name, S_IRUGO, show_##name, NULL)
+
+T4_DISPLAY_ATTR(cclk);
+
+static struct attribute *t4_attrs[] = {
+	&dev_attr_cclk.attr,
+	NULL
+};
+
+static struct attribute_group t4_attr_group = { .attrs = t4_attrs };
+
+/**
+ *	cxgb4_best_mtu - find the entry in the MTU table closest to an MTU
+ *	@mtus: the HW MTU table
+ *	@mtu: the target MTU
+ *	@idx: index of selected entry in the MTU table
+ *
+ *	Returns the index and the value in the HW MTU table that is closest to
+ *	but does not exceed @mtu, unless @mtu is smaller than any value in the
+ *	table, in which case that smallest available value is selected.
+ */
+unsigned int cxgb4_best_mtu(const unsigned short *mtus, unsigned short mtu,
+			    unsigned int *idx)
+{
+	unsigned int i = 0;
+
+	while (i < NMTUS - 1 && mtus[i + 1] <= mtu)
+		++i;
+	if (idx)
+		*idx = i;
+	return mtus[i];
+}
+EXPORT_SYMBOL(cxgb4_best_mtu);
+
+/**
+ *	cxgb4_port_chan - get the HW channel of a port
+ *	@dev: the net device for the port
+ *
+ *	Return the HW Tx channel of the given port.
+ */
+unsigned int cxgb4_port_chan(const struct net_device *dev)
+{
+	return netdev2pinfo(dev)->tx_chan;
+}
+EXPORT_SYMBOL(cxgb4_port_chan);
+
+unsigned int cxgb4_dbfifo_count(const struct net_device *dev, int lpfifo)
+{
+	struct adapter *adap = netdev2adap(dev);
+	u32 v;
+
+	v = t4_read_reg(adap, A_SGE_DBFIFO_STATUS);
+	return lpfifo ? G_LP_COUNT(v) : G_HP_COUNT(v);
+}
+EXPORT_SYMBOL(cxgb4_dbfifo_count);
+
+/**
+ *	cxgb4_port_viid - get the VI id of a port
+ *	@dev: the net device for the port
+ *
+ *	Return the VI id of the given port.
+ */
+unsigned int cxgb4_port_viid(const struct net_device *dev)
+{
+	return netdev2pinfo(dev)->viid;
+}
+EXPORT_SYMBOL(cxgb4_port_viid);
+
+/**
+ *	cxgb4_port_idx - get the index of a port
+ *	@dev: the net device for the port
+ *
+ *	Return the index of the given port.
+ */
+unsigned cxgb4_port_idx(const struct net_device *dev)
+{
+	return netdev2pinfo(dev)->port_id;
+}
+EXPORT_SYMBOL(cxgb4_port_idx);
+
+void cxgb4_get_tcp_stats(struct pci_dev *pdev, struct tp_tcp_stats *v4,
+                        struct tp_tcp_stats *v6)
+{
+        struct adapter *adap = pci_get_drvdata(pdev);
+
+        spin_lock(&adap->stats_lock);
+        t4_tp_get_tcp_stats(adap, v4, v6);
+        spin_unlock(&adap->stats_lock);
+}
+EXPORT_SYMBOL(cxgb4_get_tcp_stats);
+
+/**
+ *	cxgb4_netdev_by_hwid - return the net device of a HW port
+ *	@pdev: identifies the adapter
+ *	@id: the HW port id
+ *
+ *	Return the net device associated with the interface with the given HW
+ *	id.
+ */
+struct net_device *cxgb4_netdev_by_hwid(struct pci_dev *pdev, unsigned int id)
+{
+	const struct adapter *adap = pci_get_drvdata(pdev);
+
+	if (!adap || id >= NCHAN)
+		return NULL;
+	id = adap->chan_map[id];
+	return id < MAX_NPORTS ? adap->port[id] : NULL;
+}
+EXPORT_SYMBOL(cxgb4_netdev_by_hwid);
+
+/**
+ *	cxgb4_root_dev - get the root net_device of a physical net_device
+ *	@dev: the net device
+ *	@vlan: the VLAN id or -1 if we shouldn't traverse VLAN devices
+ *
+ *	Return the root bonding or VLAN device for the given device.
+ *	It assumes that VLAN devices are layered on top of bonding devices.
+ */
+struct net_device *cxgb4_root_dev(struct net_device *dev, int vlan)
+{
+	struct vlan_group *grp = netdev2pinfo(dev)->vlan_grp;
+
+#if defined(BOND_SUPPORT)
+	while (dev->master) {
+		dev = dev->master;
+		grp = ((const struct bonding *)netdev_priv(dev))->vlgrp;
+	}
+#endif
+	if (vlan >= 0)
+		dev = grp ? vlan_group_get_device(grp, vlan) : NULL;
+	return dev;
+}
+EXPORT_SYMBOL(cxgb4_root_dev);
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+void cxgb4_iscsi_init(struct net_device *dev, unsigned int tag_mask,
+		      const unsigned int *pgsz_order)
+{
+	struct adapter *adap = netdev2adap(dev);
+
+	t4_write_reg(adap, A_ULP_RX_ISCSI_TAGMASK, tag_mask);
+	t4_write_reg(adap, A_ULP_RX_ISCSI_PSZ, V_HPZ0(pgsz_order[0]) |
+		     V_HPZ1(pgsz_order[1]) | V_HPZ2(pgsz_order[2]) |
+		     V_HPZ3(pgsz_order[3]));
+}
+EXPORT_SYMBOL(cxgb4_iscsi_init);
+
+int cxgb4_wr_mbox(struct net_device *dev, const void *cmd,
+		  int size, void *rpl)
+{
+	struct adapter *adap = netdev2adap(dev);
+
+	return t4_wr_mbox(adap, adap->mbox, cmd, size, rpl);
+}
+EXPORT_SYMBOL(cxgb4_wr_mbox);
+
+int cxgb4_flush_eq_cache(struct net_device *dev)
+{
+	struct adapter *adap = netdev2adap(dev);
+
+	return t4_sge_ctxt_flush(adap, adap->mbox);
+}
+EXPORT_SYMBOL(cxgb4_flush_eq_cache);
+
+static int read_eq_indices(struct adapter *adap, u16 qid, u16 *pidx, u16 *cidx)
+{
+	u32 addr = t4_read_reg(adap, A_SGE_DBQ_CTXT_BADDR) + 24 * qid + 8;
+	__be64 indices;
+	int ret;
+
+	ret = t4_mem_win_read_len(adap, addr, (__be32 *)&indices, 8);
+	if (!ret) {
+		indices = be64_to_cpu(indices);
+		*cidx = (indices >> 25) & 0xffff;
+		*pidx = (indices >> 9) & 0xffff;
+	}
+	return ret;
+}
+
+int cxgb4_sync_txq_pidx(struct net_device *dev, u16 qid, u16 pidx,
+			u16 size)
+{
+	struct adapter *adap = netdev2adap(dev);
+	u16 hw_pidx, hw_cidx;
+	int ret;
+
+	ret = read_eq_indices(adap, qid, &hw_pidx, &hw_cidx);
+	if (ret)
+		goto out;
+
+	if (pidx != hw_pidx) {
+		u16 delta;
+
+		if (pidx >= hw_pidx)
+			delta = pidx - hw_pidx;
+		else
+			delta = size - hw_pidx + pidx;
+		wmb();
+		t4_write_reg(adap, MYPF_REG(A_SGE_PF_KDOORBELL),
+			     V_QID(qid) | V_PIDX(delta));
+	}
+out:
+	return ret;
+}
+EXPORT_SYMBOL(cxgb4_sync_txq_pidx);
+
+void cxgb4_disable_db_coalescing(struct net_device *dev)
+{
+	struct adapter *adap;
+
+	adap = netdev2adap(dev);
+	t4_set_reg_field(adap, A_SGE_DOORBELL_CONTROL, F_NOCOALESCE, F_NOCOALESCE);
+}
+EXPORT_SYMBOL(cxgb4_disable_db_coalescing);
+
+void cxgb4_enable_db_coalescing(struct net_device *dev)
+{
+	struct adapter *adap;
+
+	adap = netdev2adap(dev);
+	t4_set_reg_field(adap, A_SGE_DOORBELL_CONTROL, F_NOCOALESCE, 0);
+}
+EXPORT_SYMBOL(cxgb4_enable_db_coalescing);
+
+static struct pci_driver cxgb4_driver;
+
+static void check_neigh_update(struct neighbour *neigh)
+{
+	const struct device *parent = NULL;
+	struct net_device *netdev = neigh->dev;
+#if defined(BOND_SUPPORT)
+	struct bonding *bond;
+#endif
+
+	if (netdev->priv_flags & IFF_802_1Q_VLAN)
+		netdev = vlan_dev_real_dev(netdev);
+#if defined(BOND_SUPPORT)
+	if (netdev->flags & IFF_MASTER) {
+		bond = (struct bonding *)netdev_priv(netdev);
+		/* We select the first child since we can only bond
+		 * offload devices belonging to the same adapter.
+		 */
+		read_lock(&bond->lock);
+		if (bond->first_slave)
+			netdev = bond->first_slave->dev;
+		else
+			netdev = NULL;
+		read_unlock(&bond->lock);
+	}
+#endif
+
+	if (netdev)
+		parent = netdev->dev.parent;
+
+	if (parent && parent->driver == &cxgb4_driver.driver)
+		t4_l2t_update(dev_get_drvdata(parent), neigh);
+}
+
+static int netevent_cb(struct notifier_block *nb, unsigned long event,
+		       void *data)
+{
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE :
+		check_neigh_update(data);
+		break;
+	case NETEVENT_PMTU_UPDATE:
+	case NETEVENT_REDIRECT:
+	default:
+		break;
+	}
+	return 0;
+}
+
+static bool netevent_registered;
+static struct notifier_block cxgb4_netevent_nb = {
+	.notifier_call = netevent_cb
+};
+
+static void uld_attach(struct adapter *adap, unsigned int uld)
+{
+	void *handle;
+	struct cxgb4_lld_info lli;
+	unsigned int s_qpp;
+	unsigned short i;
+	u32 filt_mode;
+
+	if (!is_offload(adap))
+		return;
+
+	lli.pdev = adap->pdev;
+	lli.pf = adap->pf;
+	lli.l2t = adap->l2t;
+	lli.tids = &adap->tids;
+	lli.ports = adap->port;
+	lli.vr = &adap->vres;
+	lli.mtus = adap->params.mtus;
+	if (uld == CXGB4_ULD_RDMA) {
+		lli.rxq_ids = adap->sge.rdma_rxq;
+		lli.nrxq = adap->sge.rdmaqs;
+	} else if (uld == CXGB4_ULD_ISCSI) {
+		lli.rxq_ids = adap->sge.iscsi_rxq;
+		lli.nrxq = adap->sge.niscsiq;
+	} else if (uld == CXGB4_ULD_TOE) {
+		lli.rxq_ids = adap->sge.ofld_rxq;
+		lli.nrxq = adap->sge.ofldqsets;
+	}
+	lli.ntxq = adap->sge.ofldqsets;
+	lli.nchan = adap->params.nports;
+	lli.nports = adap->params.nports;
+	lli.wr_cred = adap->params.ofldq_wr_cred;
+	lli.adapter_type = adap->params.rev;
+	lli.iscsi_iolen = G_MAXRXDATA(t4_read_reg(adap, A_TP_PARA_REG2));
+	s_qpp = (S_QUEUESPERPAGEPF0 +
+		 (S_QUEUESPERPAGEPF1 - S_QUEUESPERPAGEPF0) * adap->pf);
+	lli.udb_density =
+		1 << ((t4_read_reg(adap, A_SGE_EGRESS_QUEUES_PER_PAGE_PF)
+		       >> s_qpp) & M_QUEUESPERPAGEPF0);
+	lli.ucq_density =
+		1 << ((t4_read_reg(adap, A_SGE_INGRESS_QUEUES_PER_PAGE_PF)
+		       >> s_qpp) & M_QUEUESPERPAGEPF0);
+	t4_read_indirect(adap, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 &filt_mode, 1, A_TP_VLAN_PRI_MAP);
+	lli.filt_mode = filt_mode;
+	
+	for (i = 0; i < NCHAN; i++)
+		lli.tx_modq[i] = adap->params.tp.tx_modq[i];
+	lli.gts_reg = adap->regs + MYPF_REG(A_SGE_PF_GTS);
+	lli.db_reg = adap->regs + MYPF_REG(A_SGE_PF_KDOORBELL);
+	lli.fw_vers = adap->params.fw_vers;
+	lli.dbfifo_int_thresh = dbfifo_int_thresh;
+	lli.sge_ingpadboundary = adap->sge.fl_align;
+	lli.sge_pktshift = adap->sge.pktshift;
+	lli.sge_egrstatuspagesize = adap->sge.stat_len;
+	lli.enable_fw_ofld_conn = adap->flags & FW_OFLD_CONN;
+
+	handle = ulds[uld].add(&lli);
+	if (IS_ERR(handle)) {
+		CH_WARN(adap, "could not attach to the %s driver, error %ld\n",
+			uld_str[uld], PTR_ERR(handle));
+		return;
+	}
+
+	adap->uld_handle[uld] = handle;
+
+	if (!netevent_registered) {
+		register_netevent_notifier(&cxgb4_netevent_nb);
+		netevent_registered = true;
+	}
+
+	if (adap->flags & FULL_INIT_DONE)
+		ulds[uld].state_change(handle, CXGB4_STATE_UP);
+}
+
+static void attach_ulds(struct adapter *adap)
+{
+	unsigned int i;
+
+	mutex_lock(&uld_mutex);
+	list_add_tail(&adap->list_node, &adapter_list);
+	for (i = 0; i < CXGB4_ULD_MAX; i++)
+		if (ulds[i].add)
+			uld_attach(adap, i);
+	mutex_unlock(&uld_mutex);
+}
+
+static void detach_ulds(struct adapter *adap)
+{
+	unsigned int i;
+
+	mutex_lock(&uld_mutex);
+	list_del(&adap->list_node);
+	for (i = 0; i < CXGB4_ULD_MAX; i++)
+		if (adap->uld_handle[i]) {
+			ulds[i].state_change(adap->uld_handle[i],
+					     CXGB4_STATE_DETACH);
+			adap->uld_handle[i] = NULL;
+		}
+	if (netevent_registered && list_empty(&adapter_list)) {
+		unregister_netevent_notifier(&cxgb4_netevent_nb);
+		netevent_registered = false;
+	}
+	mutex_unlock(&uld_mutex);
+}
+
+static void notify_rdma_uld(struct adapter *adap, enum cxgb4_control cmd)
+{
+	mutex_lock(&uld_mutex);
+	if (adap->uld_handle[CXGB4_ULD_RDMA])
+		ulds[CXGB4_ULD_RDMA].control(adap->uld_handle[CXGB4_ULD_RDMA],
+					     cmd);
+	mutex_unlock(&uld_mutex);
+}
+
+static void drain_db_fifo(struct adapter *adap, int usecs)
+{
+	u32 v;
+
+	do {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout(usecs_to_jiffies(usecs));
+		v = t4_read_reg(adap, A_SGE_DBFIFO_STATUS);
+		if (G_LP_COUNT(v) == 0 && G_HP_COUNT(v) == 0)
+			break;
+	} while (1);
+}
+
+static void disable_txq_db(struct sge_txq *q)
+{
+	spin_lock_irq(&q->db_lock);
+	q->db_disabled = 1;
+	spin_unlock_irq(&q->db_lock);
+}
+
+static void enable_txq_db(struct sge_txq *q)
+{
+	spin_lock_irq(&q->db_lock);
+	q->db_disabled = 0;
+	spin_unlock_irq(&q->db_lock);
+}
+
+static void disable_dbs(struct adapter *adap)
+{
+	int i;
+
+	for_each_ethrxq(&adap->sge, i)
+		disable_txq_db(&adap->sge.ethtxq[i].q);
+	for_each_ofldrxq(&adap->sge, i)
+		disable_txq_db(&adap->sge.ofldtxq[i].q);
+	for_each_port(adap, i)
+		disable_txq_db(&adap->sge.ctrlq[i].q);
+}
+
+static void enable_dbs(struct adapter *adap)
+{
+	int i;
+
+	for_each_ethrxq(&adap->sge, i)
+		enable_txq_db(&adap->sge.ethtxq[i].q);
+	for_each_ofldrxq(&adap->sge, i)
+		enable_txq_db(&adap->sge.ofldtxq[i].q);
+	for_each_port(adap, i)
+		enable_txq_db(&adap->sge.ctrlq[i].q);
+}
+
+static void sync_txq_pidx(struct adapter *adap, struct sge_txq *q)
+{
+	u16 hw_pidx, hw_cidx;
+	int ret;
+
+	spin_lock_bh(&q->db_lock);
+	ret = read_eq_indices(adap, (u16)q->cntxt_id, &hw_pidx, &hw_cidx);
+	if (ret)
+		goto out;
+	if (q->db_pidx != hw_pidx) {
+		u16 delta;
+
+		if (q->db_pidx >= hw_pidx)
+			delta = q->db_pidx - hw_pidx;
+		else
+			delta = q->size - hw_pidx + q->db_pidx;
+		wmb();
+		t4_write_reg(adap, MYPF_REG(A_SGE_PF_KDOORBELL),
+			     V_QID(q->cntxt_id) | V_PIDX(delta));
+	}
+out:
+	q->db_disabled = 0;
+	spin_unlock_bh(&q->db_lock);
+	if (ret)
+		CH_WARN(adap, "DB drop recovery failed.\n");
+}
+
+static void recover_all_queues(struct adapter *adap)
+{
+	int i;
+
+	for_each_ethrxq(&adap->sge, i)
+		sync_txq_pidx(adap, &adap->sge.ethtxq[i].q);
+	for_each_ofldrxq(&adap->sge, i)
+		sync_txq_pidx(adap, &adap->sge.ofldtxq[i].q);
+	for_each_port(adap, i)
+		sync_txq_pidx(adap, &adap->sge.ctrlq[i].q);
+}
+
+static void process_db_full(struct work_struct *work)
+{
+	struct adapter *adap;
+
+	adap = container_of(work, struct adapter, db_full_task);
+
+	notify_rdma_uld(adap, CXGB4_CONTROL_DB_FULL);
+	drain_db_fifo(adap, dbfifo_drain_delay);
+	t4_set_reg_field(adap, A_SGE_INT_ENABLE3,
+			 F_DBFIFO_HP_INT | F_DBFIFO_LP_INT,
+			 F_DBFIFO_HP_INT | F_DBFIFO_LP_INT);
+	notify_rdma_uld(adap, CXGB4_CONTROL_DB_EMPTY);
+}
+
+static void process_db_drop(struct work_struct *work)
+{
+	struct adapter *adap;
+
+	adap = container_of(work, struct adapter, db_drop_task);
+
+	t4_set_reg_field(adap, A_SGE_DOORBELL_CONTROL, F_DROPPED_DB, 0);
+	disable_dbs(adap);
+	notify_rdma_uld(adap, CXGB4_CONTROL_DB_DROP);
+	drain_db_fifo(adap, 1);
+	recover_all_queues(adap);
+	enable_dbs(adap);
+}
+
+void t4_db_full(struct adapter *adap)
+{
+	t4_set_reg_field(adap, A_SGE_INT_ENABLE3,
+			 F_DBFIFO_HP_INT | F_DBFIFO_LP_INT, 0);
+	queue_work(workq, &adap->db_full_task);
+}
+
+void t4_db_dropped(struct adapter *adap)
+{
+	queue_work(workq, &adap->db_drop_task);
+}
+
+static void notify_ulds(struct adapter *adap, enum cxgb4_state new_state)
+{
+	unsigned int i;
+
+	mutex_lock(&uld_mutex);
+	for (i = 0; i < CXGB4_ULD_MAX; i++)
+		if (adap->uld_handle[i])
+			ulds[i].state_change(adap->uld_handle[i], new_state);
+	mutex_unlock(&uld_mutex);
+}
+
+/**
+ *	cxgb4_register_uld - register an upper-layer driver
+ *	@type: the ULD type
+ *	@p: the ULD methods
+ *
+ *	Registers an upper-layer driver with this driver and notifies the ULD
+ *	about any presently available devices that support its type.  Returns
+ *	%-EBUSY if a ULD of the same type is already registered.
+ */
+int cxgb4_register_uld(enum cxgb4_uld type, const struct cxgb4_uld_info *p)
+{
+	int ret = 0;
+	struct adapter *adap;
+
+	if (type >= CXGB4_ULD_MAX)
+		return -EINVAL;
+	mutex_lock(&uld_mutex);
+	if (ulds[type].add) {
+		ret = -EBUSY;
+		goto out;
+	}
+	ulds[type] = *p;
+	list_for_each_entry(adap, &adapter_list, list_node)
+		uld_attach(adap, type);
+out:	mutex_unlock(&uld_mutex);
+	return ret;
+}
+EXPORT_SYMBOL(cxgb4_register_uld);
+
+/**
+ *	cxgb4_unregister_uld - unregister an upper-layer driver
+ *	@type: the ULD type
+ *
+ *	Unregisters an existing upper-layer driver.
+ */
+int cxgb4_unregister_uld(enum cxgb4_uld type)
+{
+	struct adapter *adap;
+
+	if (type >= CXGB4_ULD_MAX)
+		return -EINVAL;
+	mutex_lock(&uld_mutex);
+	list_for_each_entry(adap, &adapter_list, list_node)
+		adap->uld_handle[type] = NULL;
+	ulds[type].add = NULL;
+	mutex_unlock(&uld_mutex);
+	return 0;
+}
+EXPORT_SYMBOL(cxgb4_unregister_uld);
+
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/**
+ *	cxgb_up - enable the adapter
+ *	@adap: adapter being enabled
+ *
+ *	Called when the first port is enabled, this function performs the
+ *	actions necessary to make an adapter operational, such as completing
+ *	the initialization of HW modules, and enabling interrupts.
+ *
+ *	Must be called with the rtnl lock held.
+ */
+static int cxgb_up(struct adapter *adap)
+{
+	int err;
+
+	err = setup_sge_queues(adap);
+	if (err)
+		goto out;
+	err = setup_rss(adap);
+	if (err)
+		goto freeq;
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	if (is_offload(adap))
+		setup_loopback(adap);
+#endif
+
+	if (adap->flags & USING_MSIX) {
+		name_msix_vecs(adap);
+		err = request_irq(adap->msix_info[0].vec, t4_nondata_intr, 0,
+				  adap->msix_info[0].desc, adap);
+		if (err)
+			goto irq_err;
+
+		err = request_msix_queue_irqs(adap);
+		if (err) {
+			free_irq(adap->msix_info[0].vec, adap);
+			goto irq_err;
+		}
+	} else {
+		err = request_irq(adap->pdev->irq, t4_intr_handler(adap),
+				  (adap->flags & USING_MSI) ? 0 : IRQF_SHARED,
+				  adap->name, adap);
+		if (err)
+			goto irq_err;
+	}
+
+	enable_rx(adap);
+	t4_sge_start(adap);
+	t4_intr_enable(adap);
+	adap->flags |= FULL_INIT_DONE;
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	notify_ulds(adap, CXGB4_STATE_UP);
+#endif
+ out:
+	return err;
+ irq_err:
+	CH_ERR(adap, "request_irq failed, err %d\n", err);
+ freeq:
+	t4_free_sge_resources(adap);
+	goto out;
+}
+
+/*
+ * Release resources when all the ports and offloading have been stopped.
+ */
+static void cxgb_down(struct adapter *adapter)
+{
+	t4_intr_disable(adapter);
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	cancel_work_sync(&adapter->tid_release_task);
+	cancel_work_sync(&adapter->db_full_task);
+	cancel_work_sync(&adapter->db_drop_task);
+#endif
+
+	if (adapter->flags & USING_MSIX) {
+		free_msix_queue_irqs(adapter);
+		free_irq(adapter->msix_info[0].vec, adapter);
+	} else
+		free_irq(adapter->pdev->irq, adapter);
+	quiesce_rx(adapter);
+	t4_sge_stop(adapter);
+	t4_free_sge_resources(adapter);
+	adapter->flags &= ~FULL_INIT_DONE;
+}
+
+static int cxgb_open(struct net_device *dev)
+{
+	int err;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	/*
+	 * If we don't have a connection to the firmware there's nothing we
+	 * can do.
+	 */
+	if (!(adapter->flags & FW_OK))
+		return -ENXIO;
+
+	netif_carrier_off(dev);
+
+	if (!(adapter->flags & FULL_INIT_DONE)) {
+		err = cxgb_up(adapter);
+		if (err < 0)
+			return err;
+	}
+
+	err = link_start(dev);
+	if (err)
+		return err;
+
+	netif_tx_start_all_queues(dev);
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	if (test_bit(OFFLOAD_DEVMAP_BIT, &adapter->registered_device_map))
+		netdev_set_offload(dev);
+#endif
+	return 0;
+}
+
+static int cxgb_close(struct net_device *dev)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	netif_tx_stop_all_queues(dev);
+	netif_carrier_off(dev);
+	return t4_enable_vi(adapter, adapter->mbox, pi->viid, false, false);
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+
+static int toe_ctl(struct toedev *tdev, unsigned int req, void *data)
+{
+	return -EOPNOTSUPP;
+}
+
+static void __devinit setup_offload(struct adapter *adapter)
+{
+	struct toedev *tdev = &adapter->tdev;
+
+	init_offload_dev(tdev);
+	set_dummy_ops(tdev);
+	tdev->nlldev = adapter->params.nports;
+	tdev->lldev = adapter->port;
+	tdev->ctl = toe_ctl;
+	tdev->ttid = TOE_ID_CHELSIO_T4;
+}
+
+#define ERR(fmt, ...) do {\
+	printk(KERN_ERR "%s: " fmt "\n", dev->name, ## __VA_ARGS__); \
+	return -EINVAL; \
+} while (0)
+
+/*
+ * Perform device independent validation of offload policy.
+ */
+static int validate_offload_policy(const struct net_device *dev,
+				   const struct ofld_policy_file *f,
+				   size_t len)
+{
+	int i, inst;
+	const u32 *p;
+	const struct ofld_prog_inst *pi;
+
+	/*
+	 * We validate the following:
+	 * - Program sizes match what's in the header
+	 * - Branch targets are within the program
+	 * - Offsets do not step outside struct offload_req
+	 * - Outputs are valid
+	 */
+	printk(KERN_DEBUG "version %u, program length %zu bytes, alternate "
+	       "program length %zu bytes\n", f->vers,
+	       f->prog_size * sizeof(*pi), f->opt_prog_size * sizeof(*p));
+
+	if (sizeof(*f) + (f->nrules + 1) * sizeof(struct offload_settings) +
+	    f->prog_size * sizeof(*pi) + f->opt_prog_size * sizeof(*p) != len)
+		ERR("bad offload policy length %zu", len);
+
+	if (f->output_everything >= 0 && f->output_everything > f->nrules)
+		ERR("illegal output_everything %d in header",
+		    f->output_everything);
+
+	pi = f->prog;
+
+	for (i = 0; i < f->prog_size; i++, pi++) {
+		if (pi->offset < 0 ||
+		    pi->offset >= sizeof(struct offload_req) / 4)
+			ERR("illegal offset %d at instruction %d", pi->offset,
+			    i);
+		if (pi->next[0] < 0 && -pi->next[0] > f->nrules)
+			ERR("illegal output %d at instruction %d",
+			    -pi->next[0], i);
+		if (pi->next[1] < 0 && -pi->next[1] > f->nrules)
+			ERR("illegal output %d at instruction %d",
+			    -pi->next[1], i);
+		if (pi->next[0] > 0 && pi->next[0] >= f->prog_size)
+			ERR("illegal branch target %d at instruction %d",
+			    pi->next[0], i);
+		if (pi->next[1] > 0 && pi->next[1] >= f->prog_size)
+			ERR("illegal branch target %d at instruction %d",
+			    pi->next[1], i);
+	}
+
+	p = (const u32 *)pi;
+
+	for (inst = i = 0; i < f->opt_prog_size; inst++) {
+		unsigned int off = *p & 0xffff, nvals = *p >> 16;
+
+		if (off >= sizeof(struct offload_req) / 4)
+			ERR("illegal offset %u at opt instruction %d",
+			    off, inst);
+		if ((int32_t)p[1] < 0 && -p[1] > f->nrules)
+			ERR("illegal output %d at opt instruction %d",
+			    -p[1], inst);
+		if ((int32_t)p[2] < 0 && -p[2] > f->nrules)
+			ERR("illegal output %d at opt instruction %d",
+			    -p[2], inst);
+		if ((int32_t)p[1] > 0 && p[1] >= f->opt_prog_size)
+			ERR("illegal branch target %d at opt instruction %d",
+			    p[1], inst);
+		if ((int32_t)p[2] > 0 && p[2] >= f->opt_prog_size)
+			ERR("illegal branch target %d at opt instruction %d",
+			    p[2], inst);
+		p += 4 + nvals;
+		i += 4 + nvals;
+		if (i > f->opt_prog_size)
+			ERR("too many values %u for opt instruction %d",
+			    nvals, inst);
+	}
+
+	return 0;
+}
+
+#undef ERR
+
+static int validate_policy_settings(const struct net_device *dev,
+				    struct adapter *adap,
+				    const struct ofld_policy_file *f)
+{
+	int i, nchan = adap->params.nports;
+	const u32 *op = (const u32 *)&f->prog[f->prog_size];
+	const struct offload_settings *s = (void *)&op[f->opt_prog_size];
+
+	for (i = 0; i <= f->nrules; i++, s++) {
+		if (s->cong_algo > 3) {
+			printk(KERN_ERR "%s: illegal congestion algorithm %d\n",
+			       dev->name, s->cong_algo);
+			return -EINVAL;
+		}
+		if (s->rssq >= adap->sge.ofldqsets) {
+			printk(KERN_ERR "%s: illegal RSS queue %d\n", dev->name,
+			       s->rssq);
+			return -EINVAL;
+		}
+		if (s->sched_class >= NTX_SCHED / nchan) {
+			printk(KERN_ERR "%s: illegal scheduling class %d\n",
+			       dev->name, s->sched_class);
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+/*
+ * driver-specific ioctl support
+ */
+
+/* clear statistics for the given Ethernet Tx and Rx queues */
+static void clear_ethq_stats(struct sge *p, unsigned int idx)
+{
+	struct sge_eth_rxq *rxq = &p->ethrxq[idx];
+	struct sge_eth_txq *txq = &p->ethtxq[idx];
+
+	memset(&rxq->stats, 0, sizeof(rxq->stats));
+	rxq->fl.alloc_failed = rxq->fl.large_alloc_failed = 0;
+	rxq->fl.starving = 0;
+
+	txq->tso = txq->tx_cso = txq->vlan_ins = 0;
+	txq->q.stops = txq->q.restarts = 0;
+	txq->mapping_err = 0;
+}
+
+/* clear statistics for the Ethernet queues associated with the given port */
+static void clear_port_qstats(struct adapter *adap, const struct port_info *pi)
+{
+	int i;
+
+	for (i = 0; i < pi->nqsets; i++)
+		clear_ethq_stats(&adap->sge, pi->first_qset + i);
+}
+
+/**
+ *	t4_get_desc - dump an SGE descriptor for debugging purposes
+ *	@p: points to the sge structure for the adapter
+ *	@category: the type of queue
+ *	@qid: the absolute SGE QID of the specific queue within the category
+ *	@idx: the descriptor index in the queue
+ *	@data: where to dump the descriptor contents
+ *
+ *	Dumps the contents of a HW descriptor of an SGE queue.  Returns the
+ *	size of the descriptor or a negative error.
+ */
+static int get_qdesc(const struct sge *p, int category, unsigned int qid,
+		     unsigned int idx, unsigned char *data)
+{
+	int i, len = sizeof(struct tx_desc);
+
+	/*
+	 * For Tx queues allow reading the status entry too.
+	 */
+	if (category == SGE_QTYPE_TX_ETH) {
+		const struct sge_eth_txq *q = p->ethtxq;
+
+		for (i = 0; i < ARRAY_SIZE(p->ethtxq); i++, q++)
+			if (q->q.cntxt_id == qid && q->q.desc &&
+			    idx <= q->q.size) {
+				memcpy(data, &q->q.desc[idx], len);
+				return len;
+			}
+	}
+	if (category == SGE_QTYPE_TX_OFLD) {
+		const struct sge_ofld_txq *q = p->ofldtxq;
+
+		for (i = 0; i < ARRAY_SIZE(p->ofldtxq); i++, q++)
+			if (q->q.cntxt_id == qid && q->q.desc &&
+			    idx <= q->q.size) {
+				memcpy(data, &q->q.desc[idx], len);
+				return len;
+			}
+	}
+	if (category == SGE_QTYPE_TX_CTRL) {
+		const struct sge_ctrl_txq *q = p->ctrlq;
+
+		for (i = 0; i < ARRAY_SIZE(p->ctrlq); i++, q++)
+			if (q->q.cntxt_id == qid && q->q.desc &&
+			    idx <= q->q.size) {
+				memcpy(data, &q->q.desc[idx], len);
+				return len;
+			}
+	}
+	if (category == SGE_QTYPE_FL) {
+		const struct sge_fl *q;
+
+		qid -= p->egr_start;
+		q = qid >= ARRAY_SIZE(p->egr_map) ? NULL : p->egr_map[qid];
+		if (q && q >= &p->ethrxq[0].fl && idx < q->size) {
+			*(__be64 *)data = q->desc[idx];
+			return sizeof(u64);
+		}
+	}
+	if (category == SGE_QTYPE_RSP) {
+		const struct sge_rspq *q;
+
+		qid -= p->ingr_start;
+		q = qid >= ARRAY_SIZE(p->ingr_map) ? NULL : p->ingr_map[qid];
+		if (q && idx < q->size) {
+			len = q->iqe_len;
+			idx *= len / sizeof(u64);
+			memcpy(data, &q->desc[idx], len);
+			return len;
+		}
+	}
+	return -EINVAL;
+}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+/*
+ * Return an error number if the indicated filter isn't writable ...
+ */
+static int writable_filter(struct filter_entry *f)
+{
+	if (f->locked)
+		return -EPERM;
+	if (f->pending)
+		return -EBUSY;
+
+	return 0;
+}
+
+/*
+ * Delete the filter at the specified index (if valid).  The checks for all
+ * the common problems with doing this like the filter being locked, currently
+ * pending in another operation, etc.
+ */
+static int delete_filter(struct adapter *adapter, unsigned int fidx)
+{
+	struct filter_entry *f;
+	int ret;
+
+	if (fidx >= adapter->tids.nftids + adapter->tids.nsftids)
+		return -EINVAL;
+
+	f = &adapter->tids.ftid_tab[fidx];
+	ret = writable_filter(f);
+	if (ret)
+		return ret;
+	if (f->valid)
+		return del_filter_wr(adapter, fidx);
+
+	return 0;
+}
+
+/*
+ * Retrieve the packet count for the specified filter.
+ */
+static int get_filter_count(struct adapter *adapter, unsigned int fidx, u64 *c)
+{
+	struct filter_entry *f;
+	int tcbaddr;
+	int tcb_base;
+
+	if (fidx >= adapter->tids.nftids)
+		return -EINVAL;
+
+	f = &adapter->tids.ftid_tab[fidx];
+	if (!f->valid)
+		return -EINVAL;
+
+	tcb_base = t4_read_reg(adapter, A_TP_CMM_TCB_BASE);
+	tcbaddr = tcb_base + ((fidx + adapter->tids.ftid_base) * TCB_SIZE);
+	t4_write_reg(adapter, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0),
+	    tcbaddr);
+	t4_read_reg(adapter, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 0));
+	*c = be64_to_cpu(t4_read_reg64(adapter, MEMWIN0_BASE + 16));
+
+	return 0;
+}
+
+int cxgb4_create_server_filter(const struct net_device *dev, unsigned int stid,
+			__be32 sip, __be16 sport, unsigned int queue)
+{
+	int ret;
+	struct filter_entry *f;
+	struct adapter *adap;
+	int i;
+	u8 *val;
+
+	adap = netdev2adap(dev);
+
+	/* Adjust stid to correct filter index */
+	stid -= adap->tids.nstids;
+	stid += adap->tids.nftids;
+
+	/*
+	 * Check to make sure the filter requested is writable ...
+	 */
+	f = &adap->tids.ftid_tab[stid];
+	ret = writable_filter(f);
+	if (ret)
+		return ret;
+
+	/*
+	 * Clear out any old resources being used by the filter before
+	 * we start constructing the new filter.
+	 */
+	if (f->valid)
+		clear_filter(adap, f);
+
+	/* Clear out filter specifications */
+	memset(&f->fs, 0, sizeof(struct ch_filter_specification));
+	f->fs.val.lport = cpu_to_be16(sport);
+	f->fs.mask.lport  = ~0;
+	val = (u8 *)&sip;
+	if ((val[0] | val[1] | val[2] | val[3]) != 0)
+		for (i = 0; i < 4; i++) {
+			f->fs.val.lip[i] = val[i];
+			f->fs.mask.lip[i] = ~0;
+		}
+
+	f->fs.dirsteer = 1;
+	f->fs.iq = queue;
+	/* Mark filter as locked */
+	f->locked = 1;
+	f->fs.rpttid = 1;
+
+	ret = set_filter_wr(adap, stid);
+	if (ret) {
+		clear_filter(adap, f);
+		return ret;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(cxgb4_create_server_filter);
+
+int cxgb4_remove_server_filter(const struct net_device *dev, unsigned int stid,
+					 unsigned int queue, bool ipv6)
+{
+	int ret;
+	struct filter_entry *f;
+	struct adapter *adap;
+
+	adap = netdev2adap(dev);
+
+	/* Adjust stid to correct filter index */
+	stid -= adap->tids.nstids;
+	stid += adap->tids.nftids;
+
+	f = &adap->tids.ftid_tab[stid];
+	/* Unlock the filter */
+	f->locked = 0;
+
+	ret = delete_filter(adap, stid);
+	if(ret)
+		return ret;
+
+	return 0;
+}
+EXPORT_SYMBOL(cxgb4_remove_server_filter);
+
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+#ifdef CONFIG_CHELSIO_BYPASS
+/*
+ * Retrieve a list of bypass ports.
+ */
+static int get_bypass_ports(struct adapter *adapter, 
+				struct ch_bypass_ports *cba)
+{
+	const struct net_device *dev;
+	int i = 0;
+
+	for_each_port(adapter, i) {
+		dev = adapter->port[i];
+		strncpy(cba->ba_if[i].if_name, dev->name, IFNAMSIZ);
+	}
+	cba->port_count = adapter->params.nports;
+
+	return 0;
+}
+#endif
+
+/*
+ * Simple predicate to vet incoming Chelsio ioctl() parameters to make sure
+ * they are either not set (value < 0) or within the indicated range.
+ */
+static int in_range(int val, int lo, int hi)
+{
+	return val < 0 || (val <= hi && val >= lo);
+}
+
+static int cxgb_extension_ioctl(struct net_device *dev, void __user *useraddr)
+{
+	int ret;
+	u32 cmd;
+	struct adapter *adapter = netdev2adap(dev);
+
+	if (copy_from_user(&cmd, useraddr, sizeof(cmd)))
+		return -EFAULT;
+
+	switch (cmd) {
+	case CHELSIO_SETREG: {
+		struct ch_reg edata;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.addr & 3) != 0 ||
+		    edata.addr >= pci_resource_len(adapter->pdev, 0))
+			return -EINVAL;
+		writel(edata.val, adapter->regs + edata.addr);
+		break;
+	}
+	case CHELSIO_GETREG: {
+		struct ch_reg edata;
+
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.addr & 3) != 0 ||
+		    edata.addr >= pci_resource_len(adapter->pdev, 0))
+			return -EINVAL;
+		edata.val = readl(adapter->regs + edata.addr);
+		if (copy_to_user(useraddr, &edata, sizeof(edata)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_GET_SGE_CTXT: {
+		struct ch_mem_range t;
+		u32 buf[SGE_CTXT_SIZE / 4];
+
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (t.len < SGE_CTXT_SIZE || t.addr > M_CTXTQID)
+			return -EINVAL;
+
+		if (t.mem_id == CNTXT_TYPE_RSP || t.mem_id == CNTXT_TYPE_CQ)
+			ret = CTXT_INGRESS;
+		else if (t.mem_id == CNTXT_TYPE_EGRESS)
+			ret = CTXT_EGRESS;
+		else if (t.mem_id == CNTXT_TYPE_FL)
+			ret = CTXT_FLM;
+		else if (t.mem_id == CNTXT_TYPE_CONG)
+			ret = CTXT_CNM;
+		else
+			return -EINVAL;
+
+		if ((adapter->flags & FW_OK) && !use_bd)
+			ret = t4_sge_ctxt_rd(adapter, adapter->mbox, t.addr,
+					     ret, buf);
+		else
+			ret = t4_sge_ctxt_rd_bd(adapter, t.addr, ret, buf);
+		if (ret)
+			return ret;
+
+		t.version = mk_adap_vers(adapter);
+		if (copy_to_user(useraddr + sizeof(t), buf, SGE_CTXT_SIZE) ||
+		    copy_to_user(useraddr, &t, sizeof(t)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_GET_SGE_DESC2: {
+		unsigned char buf[128];
+		struct ch_mem_range edesc;
+
+		if (copy_from_user(&edesc, useraddr, sizeof(edesc)))
+			return -EFAULT;
+		/*
+		 * Upper 8 bits of mem_id is the queue type, the rest the qid.
+		 */
+		ret = get_qdesc(&adapter->sge, edesc.mem_id >> 24,
+				edesc.mem_id & 0xffffff, edesc.addr, buf);
+		if (ret < 0)
+			return ret;
+		if (edesc.len < ret)
+			return -EINVAL;
+
+		edesc.len = ret;
+		edesc.version = mk_adap_vers(adapter);
+		if (copy_to_user(useraddr + sizeof(edesc), buf, edesc.len) ||
+		    copy_to_user(useraddr, &edesc, sizeof(edesc)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_SET_QSET_PARAMS: {
+		struct sge_eth_rxq *rq;
+		struct sge_eth_txq *tq;
+		struct ch_qset_params t;
+		const struct port_info *pi = netdev_priv(dev);
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (t.qset_idx >= pi->nqsets)
+			return -EINVAL;
+		if (t.txq_size[1] >= 0 || t.txq_size[2] >= 0 ||
+		    t.fl_size[1] >= 0 || t.cong_thres >= 0 || t.polling >= 0)
+			return -EINVAL;
+		if (//!in_range(t.intr_lat, 0, M_NEWTIMER) ||
+		    //!in_range(t.cong_thres, 0, 255) ||
+		    !in_range(t.txq_size[0], MIN_TXQ_ENTRIES,
+			      MAX_TXQ_ENTRIES) ||
+		    !in_range(t.fl_size[0], MIN_FL_ENTRIES, MAX_RX_BUFFERS) ||
+		    !in_range(t.rspq_size, MIN_RSPQ_ENTRIES, MAX_RSPQ_ENTRIES))
+			return -EINVAL;
+
+		if (t.lro > 0)
+			return -EINVAL;
+
+		if ((adapter->flags & FULL_INIT_DONE) &&
+		    (t.rspq_size >= 0 || t.fl_size[0] >= 0 ||
+		     t.txq_size[0] >= 0))
+			return -EBUSY;
+
+		tq = &adapter->sge.ethtxq[t.qset_idx + pi->first_qset];
+		rq = &adapter->sge.ethrxq[t.qset_idx + pi->first_qset];
+
+		if (t.rspq_size >= 0)
+			rq->rspq.size = t.rspq_size;
+		if (t.fl_size[0] >= 0)
+			rq->fl.size = t.fl_size[0] + 8; /* need an empty desc */
+		if (t.txq_size[0] >= 0)
+			tq->q.size = t.txq_size[0];
+		if (t.intr_lat >= 0)
+			rq->rspq.intr_params =
+				V_QINTR_TIMER_IDX(closest_timer(&adapter->sge, t.intr_lat));
+		break;
+	}
+	case CHELSIO_GET_QSET_PARAMS: {
+		struct sge_eth_rxq *rq;
+		struct sge_eth_txq *tq;
+		struct ch_qset_params t;
+		const struct port_info *pi = netdev_priv(dev);
+
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (t.qset_idx >= pi->nqsets)
+			return -EINVAL;
+
+		tq = &adapter->sge.ethtxq[t.qset_idx + pi->first_qset];
+		rq = &adapter->sge.ethrxq[t.qset_idx + pi->first_qset];
+		t.rspq_size   = rq->rspq.size;
+		t.txq_size[0] = tq->q.size;
+		t.txq_size[1] = 0;
+		t.txq_size[2] = 0;
+		t.fl_size[0]  = rq->fl.size - 8; /* sub unused descriptor */
+		t.fl_size[1]  = 0;
+		t.polling     = 1;
+		t.lro         = ((dev->features & NETIF_F_GRO) != 0);
+		t.intr_lat    = qtimer_val(adapter, &rq->rspq);
+		t.cong_thres  = 0;
+
+		if (adapter->flags & USING_MSIX)
+			t.vector = adapter->msix_info[pi->first_qset +
+						      t.qset_idx + 2].vec;
+		else
+			t.vector = adapter->pdev->irq;
+
+		if (copy_to_user(useraddr, &t, sizeof(t)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_SET_QSET_NUM: {
+		struct ch_reg edata;
+		struct port_info *pi = netdev_priv(dev);
+		unsigned int i, first_qset = 0, other_qsets;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (adapter->flags & FULL_INIT_DONE)
+			return -EBUSY;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+
+		/*
+		 * Check legitimate range for number of Queue Sets.  We need
+		 * at least one Queue Set and we can't have more that
+		 * MAX_ETH_QSETS.  (Note that the incoming value from User
+		 * Space is an unsigned 32-bit value.  Since that includes
+		 * 0xffff == (u32)-1, if we depend solely on the test below
+		 * for "edata.val + other_qsets > adapter->sge.max_ethqsets",
+		 * then we'll miss such bad values because of wrap-around
+		 * arithmetic.)
+		 */
+		if (edata.val < 1 || edata.val > MAX_ETH_QSETS)
+			return -EINVAL;
+
+		other_qsets = adapter->sge.ethqsets - pi->nqsets;
+		if (edata.val + other_qsets > adapter->sge.max_ethqsets ||
+		    edata.val > pi->rss_size)
+			return -EINVAL;
+		pi->nqsets = edata.val;
+		dev->real_num_tx_queues = edata.val;
+		adapter->sge.ethqsets = other_qsets + pi->nqsets;
+
+		for_each_port(adapter, i)
+			if (adapter->port[i]) {
+				pi = adap2pinfo(adapter, i);
+				pi->first_qset = first_qset;
+				first_qset += pi->nqsets;
+			}
+		break;
+	}
+	case CHELSIO_GET_QSET_NUM: {
+		struct ch_reg edata;
+		struct port_info *pi = netdev_priv(dev);
+
+		edata.cmd = CHELSIO_GET_QSET_NUM;
+		edata.val = pi->nqsets;
+		if (copy_to_user(useraddr, &edata, sizeof(edata)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_LOAD_FW: {
+		u8 *fw_data;
+		struct ch_mem_range t;
+
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (!t.len)
+			return -EINVAL;
+
+		fw_data = t4_alloc_mem(t.len);
+		if (!fw_data)
+			return -ENOMEM;
+
+		if (copy_from_user(fw_data, useraddr + sizeof(t), t.len)) {
+			t4_free_mem(fw_data);
+			return -EFAULT;
+		}
+
+		/*
+		 * If the adapter has been fully initialized then we'll go
+		 * ahead and try to get the firmware's cooperation in
+		 * upgrading to the new firmware image otherwise we'll try to
+		 * do the entire job from the host ... and we always "force"
+		 * the operation in this path.
+		 */
+		ret = t4_fw_upgrade(adapter,
+				    ((adapter->flags & FULL_INIT_DONE)
+				     ? adapter->mbox
+				     : M_PCIE_FW_MASTER + 1),
+				    fw_data, t.len, /*force=*/true);
+		t4_free_mem(fw_data);
+		if (ret)
+			return ret;
+		break;
+	}
+	case CHELSIO_LOAD_BOOT: {
+		u8 *boot_data;
+		struct ch_mem_range t;
+		unsigned int pcie_pf_exprom_ofst, offset;
+
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		/*
+		 * Check if user selected a valid PF index or offset
+		 * mem_id:	type of access 0: PF index, 1: offset
+		 * addr: 	pf index or offset
+		 */
+		if (t.mem_id == 0) {
+			/*
+			 * Flash boot image to the offset defined by the PFs
+			 * EXPROM_OFST defined in the serial configuration file.
+			 * Read PCIE_PF_EXPROM_OFST register
+		 	 */
+
+			/*
+			 * Check PF index
+			 */
+			if (t.addr > 7 || t.addr < 0) {
+				CH_ERR(adapter, "PF index is too small/large\n");
+				return EFAULT;
+			}
+
+			pcie_pf_exprom_ofst = t4_read_reg(adapter,
+					PF_REG(t.addr, A_PCIE_PF_EXPROM_OFST));
+			offset = G_OFFSET(pcie_pf_exprom_ofst);
+
+		} else if (t.mem_id == 1) {
+			/*
+			 * Flash boot image to offset specified by the user.
+			 */
+			offset = G_OFFSET(t.addr);
+
+		} else
+			return -EINVAL;
+
+		/*
+		 * If a length of 0 is supplied that implies the desire to
+		 * clear the FLASH area associated with the option ROM
+		 */
+		if (t.len == 0)
+			ret = t4_load_boot(adapter, NULL, offset, 0);
+		else {
+			boot_data = t4_alloc_mem(t.len);
+			if (!boot_data)
+				return -ENOMEM;
+
+			if (copy_from_user(boot_data, useraddr + sizeof(t),
+						t.len)) {
+				t4_free_mem(boot_data);
+				return -EFAULT;
+			}
+
+			ret = t4_load_boot(adapter, boot_data, offset, t.len);
+			t4_free_mem(boot_data);
+		}
+		if (ret)
+			return ret;
+		break;
+	}
+        case CHELSIO_LOAD_CFG: {
+                u8 *cfg_data;
+		struct struct_load_cfg t;
+		
+
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+                if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		/*
+		 * If a length of 0 is supplied that implies the desire to
+		 * clear the FLASH area associated with the Firmware
+		 * Configuration File.
+		 */
+		if (t.len == 0)
+			ret = t4_load_cfg(adapter, NULL, 0);
+		else {
+			cfg_data = t4_alloc_mem(t.len);
+			if (!cfg_data)
+				return -ENOMEM;
+
+			if (copy_from_user(cfg_data, useraddr + sizeof(t), t.len)) {
+				t4_free_mem(cfg_data);
+				return -EFAULT;
+			}
+			ret = t4_load_cfg(adapter, cfg_data, t.len);
+			t4_free_mem(cfg_data);
+		}
+		if (ret)
+			return ret;
+		break;
+        }
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	case CHELSIO_SET_FILTER: {
+		u32 fconf, iconf;
+		struct ch_filter t;
+		unsigned int fidx, iq;
+		struct filter_entry *f;
+
+		/*
+		 * Vet the filter specification against our hardware filter
+		 * configuration and capabilities.
+		 */
+
+		if (!allow_nonroot_filters && !capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (adapter->tids.nftids == 0 ||
+		    adapter->tids.ftid_tab == NULL)
+			return -EOPNOTSUPP;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;  /* can still change nfilters */
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		if (t.filter_id >= adapter->tids.nftids)
+			return -E2BIG;
+
+		/*
+		 * Check for unconfigured fields being used.
+		 */
+		t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+				 &fconf, 1, A_TP_VLAN_PRI_MAP);
+		t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+				 &iconf, 1, A_TP_INGRESS_CONFIG);
+
+		#define S(_field) \
+			(t.fs.val._field || t.fs.mask._field)
+		#define U(_mask, _field) \
+			(!(fconf & (_mask)) && S(_field))
+
+		if (U(F_FCOE, fcoe) || U(F_PORT, iport) || U(F_TOS, tos) ||
+		    U(F_ETHERTYPE, ethtype) || U(F_MACMATCH, macidx) ||
+		    U(F_MPSHITTYPE, matchtype) || U(F_FRAGMENTATION, frag) ||
+		    U(F_PROTOCOL, proto) ||
+		    U(F_VNIC_ID, pfvf_vld) ||
+		    U(F_VNIC_ID, ovlan_vld) ||
+		    U(F_VLAN, ivlan_vld))
+			return -EOPNOTSUPP;
+
+		/*
+		 * T4 inconveniently uses the same 17 bits for both the Outer
+		 * VLAN Tag and PF/VF/VFvld fields based on F_VNIC being set
+		 * in TP_INGRESS_CONFIG.  Hense the somewhat crazy checks
+		 * below.  Additionally, since the T4 firmware interface also
+		 * carries that overlap, we need to translate any PF/VF
+		 * specification into that internal format below.
+		 */
+		if (S(pfvf_vld) && S(ovlan_vld))
+			return -EOPNOTSUPP;
+		if ((S(pfvf_vld) && !(iconf & F_VNIC)) ||
+		    (S(ovlan_vld) && (iconf & F_VNIC)))
+			return -EOPNOTSUPP;
+		if (t.fs.val.pf > 0x7 || t.fs.val.vf > 0x7f)
+			return -ERANGE;
+		t.fs.mask.pf &= 0x7;
+		t.fs.mask.vf &= 0x7f;
+
+		#undef S
+		#undef U
+
+		/*
+		 * If the user is requesting that the filter action loop
+		 * matching packets back out one of our ports, make sure that
+		 * the egress port is in range.
+		 */
+		if (t.fs.action == FILTER_SWITCH &&
+		    t.fs.eport >= adapter->params.nports)
+			return -ERANGE;
+
+		/*
+		 * Don't allow various trivially obvious bogus out-of-range
+		 * values ...
+		 */
+		if (t.fs.val.iport >= adapter->params.nports)
+			return -ERANGE;
+
+		/*
+		 * T4 doesn't support removing VLAN Tags for loop back
+		 * filters.
+		 */
+		if (t.fs.action == FILTER_SWITCH &&
+		    (t.fs.newvlan == VLAN_REMOVE ||
+		     t.fs.newvlan == VLAN_REWRITE))
+			return -EOPNOTSUPP;
+
+		/*
+		 * If the user has requested steering matching Ingress Packets
+		 * to a specific Queue Set, we need to make sure it's in range
+		 * for the port and map that into the Absolute Queue ID of the
+		 * Queue Set's Response Queue.
+		 */
+		if (!t.fs.dirsteer) {
+			if (t.fs.iq)
+				return -EINVAL;
+			iq = 0;
+		} else {
+			struct port_info *pi = netdev_priv(dev);
+
+			/*
+			 * If the iq id is greater than the number of qsets,
+			 * then assume it is an absolute qid.
+			 */
+			if (t.fs.iq < pi->nqsets)
+				iq = adapter->sge.ethrxq[pi->first_qset +
+							 t.fs.iq].rspq.abs_id;
+			else
+				iq = t.fs.iq;
+		}
+
+		/*
+		 * IPv6 filters occupy four slots and must be aligned on
+		 * four-slot boundaries.  IPv4 filters only occupy a single
+		 * slot and have no alignment requirements but writing a new
+		 * IPv4 filter into the middle of an existing IPv6 filter
+		 * requires clearing the old IPv6 filter.
+		 */
+		if (t.fs.type == 0) { /* IPv4 */
+			/*
+			 * If our IPv4 filter isn't being written to a
+			 * multiple of four filter index and there's an IPv6
+			 * filter at the multiple of 4 base slot, then we need
+			 * to delete that IPv6 filter ...
+			 */
+			fidx = t.filter_id & ~0x3;
+			if (fidx != t.filter_id &&
+			    adapter->tids.ftid_tab[fidx].fs.type) {
+				ret = delete_filter(adapter, fidx);
+				if (ret)
+					return ret;
+			}
+		} else { /* IPv6 */
+			/*
+			 * Ensure that the IPv6 filter is aligned on a
+			 * multiple of 4 boundary.
+			 */
+			if (t.filter_id & 0x3)
+				return -EINVAL;
+
+			/*
+			 * Check all except the base overlapping IPv4 filter
+			 * slots.
+			 */
+			for (fidx = t.filter_id+1; fidx < t.filter_id+4; fidx++) {
+				ret = delete_filter(adapter, fidx);
+				if (ret)
+					return ret;
+			}
+		}
+
+		/*
+		 * Check to make sure the filter requested is writable ...
+		 */
+		f = &adapter->tids.ftid_tab[t.filter_id];
+		ret = writable_filter(f);
+		if (ret)
+			return ret;
+
+		/*
+		 * Clear out any old resources being used by the filter before
+		 * we start constructing the new filter.
+		 */
+		if (f->valid)
+			clear_filter(adapter, f);
+
+		/*
+		 * Convert the filter specification into our internal format.
+		 * We copy the PF/VF specification into the Outer VLAN field
+		 * here so the rest of the code -- including the interface to
+		 * the firmware -- doesn't have to constantly do these checks.
+		 */
+		f->fs = t.fs;
+		f->fs.iq = iq;
+		if (iconf & F_VNIC) {
+			f->fs.val.ovlan = (t.fs.val.pf << 7) | t.fs.val.vf;
+			f->fs.mask.ovlan = (t.fs.mask.pf << 7) | t.fs.mask.vf;
+			f->fs.val.ovlan_vld = t.fs.val.pfvf_vld;
+			f->fs.mask.ovlan_vld = t.fs.mask.pfvf_vld;
+		}
+
+		/*
+		 * Attempt to set the filter.  If we don't succeed, we clear
+		 * it and return the failure.
+		 */
+		ret = set_filter_wr(adapter, t.filter_id);
+		if (ret) {
+			clear_filter(adapter, f);
+			return ret;
+		}
+
+		break;
+	}
+	case CHELSIO_DEL_FILTER: {
+		struct ch_filter fs;
+
+		if (!allow_nonroot_filters && !capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (adapter->tids.nftids == 0 ||
+		    adapter->tids.ftid_tab == NULL)
+			return -EOPNOTSUPP;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;  /* can still change nfilters */
+		if (copy_from_user(&fs, useraddr, sizeof(fs)))
+			return -EFAULT;
+
+		/*
+		 * Make sure this is a valid filter and that we can delete it.
+		 */
+		if (fs.filter_id >= adapter->tids.nftids)
+			return -E2BIG;
+
+		ret = delete_filter(adapter, fs.filter_id);
+		if(ret)
+			return ret;
+		break;
+	}
+	case CHELSIO_GET_FILTER_COUNT: {
+		struct ch_filter_count count;
+
+		if (copy_from_user(&count, useraddr, sizeof(count)))
+			return -EFAULT;
+		if (adapter->tids.nftids == 0 ||
+		    adapter->tids.ftid_tab == NULL)
+			return -EOPNOTSUPP;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;  /* can still change nfilters */
+
+		if (count.filter_id >= adapter->tids.nftids)
+			return -E2BIG;
+
+		ret = get_filter_count(adapter, count.filter_id,
+					&count.pkt_count);
+
+		if (copy_to_user(useraddr, &count, sizeof(count)))
+			return -EFAULT;
+		break;
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+#ifdef CONFIG_CHELSIO_BYPASS
+	case CHELSIO_GET_BYPASS_PORTS: {
+		struct ch_bypass_ports cbp;
+
+		if (!is_bypass(adapter))
+			return -EINVAL;
+
+		get_bypass_ports(adapter, &cbp);
+
+		if (copy_to_user(useraddr, &cbp, sizeof(cbp)))
+			return -EFAULT;
+		break;
+	}
+#endif
+	case CHELSIO_CLEAR_STATS: {
+		struct ch_reg edata;
+		struct port_info *pi = netdev_priv(dev);
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.val & STATS_QUEUE) && edata.addr != -1 &&
+		    edata.addr >= pi->nqsets)
+			return -EINVAL;
+		if (edata.val & STATS_PORT) {
+			t4_get_port_stats(adapter, pi->tx_chan,
+					  &pi->stats_base);
+			clear_sge_port_stats(adapter, pi);
+		}
+		if (edata.val & STATS_QUEUE) {
+			if (edata.addr == -1)
+				clear_port_qstats(adapter, pi);
+			else
+				clear_ethq_stats(&adapter->sge,
+						 pi->first_qset + edata.addr);
+		}
+		break;
+	}
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#if 0
+	case CHELSIO_DEVUP:
+		if (!is_offload(adapter))
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		return activate_offload(&adapter->tdev);
+#endif
+	case CHELSIO_SET_HW_SCHED: {
+		struct ch_hw_sched t;
+		unsigned int ticks_per_usec = core_ticks_per_usec(adapter);
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (t.sched >= NTX_SCHED || !in_range(t.mode, 0, 1) ||
+		    !in_range(t.channel, 0, NCHAN-1) ||
+		    !in_range(t.kbps, 0, 10000000) ||
+		    !in_range(t.class_ipg, 0, 10000 * 65535 / ticks_per_usec) ||
+		    !in_range(t.flow_ipg, 0,
+			      dack_ticks_to_usec(adapter, 0x7ff)))
+			return -EINVAL;
+
+		if (t.kbps >= 0) {
+			ret = t4_set_sched_bps(adapter, t.sched, t.kbps);
+			if (ret < 0)
+				return ret;
+		}
+		if (t.class_ipg >= 0)
+			t4_set_sched_ipg(adapter, t.sched, t.class_ipg);
+		if (t.flow_ipg >= 0) {
+			ret = t4_set_pace_tbl(adapter, &t.flow_ipg, t.sched, 1);
+			if (ret < 0)
+				return ret;
+		}
+		if (t.mode >= 0) {
+			int bit = 1 << (S_TIMERMODE + t.sched);
+
+			t4_set_reg_field(adapter, A_TP_MOD_CONFIG,
+					 bit, t.mode ? bit : 0);
+		}
+		if (t.channel >= 0) {
+			t4_set_reg_field(adapter, A_TP_TX_MOD_QUEUE_REQ_MAP,
+					 3 << (2 * t.sched),
+					 t.channel << (2 * t.sched));
+			adapter->params.tp.tx_modq[t.channel] = t.sched;
+		}
+		if (t.weight >= 0) {
+			if (t.sched <= 3)
+				t4_set_reg_field(adapter, A_TP_TX_MOD_QUEUE_WEIGHT0,
+					0xFF << (8 * t.sched),
+					t.weight << (8 * t.sched));
+			if(t.sched > 3)
+				t4_set_reg_field(adapter, A_TP_TX_MOD_QUEUE_WEIGHT1,
+					0xFF << (8 * (t.sched - 4)),
+					t.weight << (8 * (t.sched - 4)));
+		}
+		break;
+	}
+	case CHELSIO_SET_OFLD_POLICY: {
+		struct ch_mem_range t;
+		struct ofld_policy_file *opf;
+		struct cxgb4_uld_info *toe_uld = &ulds[CXGB4_ULD_TOE];
+		void *toe_handle = adapter->uld_handle[CXGB4_ULD_TOE];
+
+		if (!test_bit(OFFLOAD_DEVMAP_BIT,
+			      &adapter->registered_device_map))
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (!toe_uld->control || !TOEDEV(dev))
+			return -EOPNOTSUPP;
+
+		/* len == 0 removes any existing policy */
+		if (t.len == 0) {
+			toe_uld->control(toe_handle,
+					 CXGB4_CONTROL_SET_OFFLOAD_POLICY,
+					 NULL);
+			break;
+		}
+
+		opf = t4_alloc_mem(t.len);
+		if (!opf)
+			return -ENOMEM;
+
+		if (copy_from_user(opf, useraddr + sizeof(t), t.len)) {
+			t4_free_mem(opf);
+			return -EFAULT;
+		}
+
+		ret = validate_offload_policy(dev, opf, t.len);
+		if (!ret)
+			ret = validate_policy_settings(dev, adapter, opf);
+		if (!ret)
+			ret = toe_uld->control(toe_handle,
+					       CXGB4_CONTROL_SET_OFFLOAD_POLICY,
+					       opf);
+		t4_free_mem(opf);
+		return ret;
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+	default:
+		return -EOPNOTSUPP;
+	}
+	return 0;
+}
+
+/*
+ * net_device operations
+ */
+
+/* IEEE 802.3 specified MDIO devices */
+enum {
+	MDIO_DEV_PMA_PMD = 1,
+	MDIO_DEV_VEND2   = 31
+};
+
+static int cxgb_ioctl(struct net_device *dev, struct ifreq *req, int cmd)
+{
+	int ret = 0, mmd;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	struct mii_ioctl_data *data = (struct mii_ioctl_data *)&req->ifr_data;
+
+	switch (cmd) {
+	case SIOCGMIIPHY:
+		data->phy_id = pi->mdio_addr;
+		break;
+	case SIOCGMIIREG: {
+		u32 val;
+
+		if (pi->link_cfg.supported & FW_PORT_CAP_SPEED_10G) {
+			mmd = data->phy_id >> 8;
+			if (!mmd)
+				mmd = MDIO_DEV_PMA_PMD;
+			else if (mmd > MDIO_DEV_VEND2)
+				return -EINVAL;
+
+			ret = t4_mdio_rd(adapter, adapter->mbox,
+					 data->phy_id & 0x1f, mmd,
+					 data->reg_num, &val);
+		} else
+			ret = t4_mdio_rd(adapter, adapter->mbox,
+					 data->phy_id & 0x1f, 0,
+					 data->reg_num & 0x1f, &val);
+		if (!ret)
+			data->val_out = val;
+		break;
+	}
+	case SIOCSMIIREG:
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (pi->link_cfg.supported & FW_PORT_CAP_SPEED_10G) {
+			mmd = data->phy_id >> 8;
+			if (!mmd)
+				mmd = MDIO_DEV_PMA_PMD;
+			else if (mmd > MDIO_DEV_VEND2)
+				return -EINVAL;
+
+			ret = t4_mdio_wr(adapter, adapter->mbox,
+					 data->phy_id & 0x1f, mmd,
+					 data->reg_num, data->val_in);
+		} else
+			ret = t4_mdio_wr(adapter, adapter->mbox,
+					 data->phy_id & 0x1f, 0,
+					 data->reg_num & 0x1f, data->val_in);
+		break;
+
+	case SIOCCHIOCTL:
+		return cxgb_extension_ioctl(dev, (void *)req->ifr_data);
+	default:
+		return -EOPNOTSUPP;
+	}
+	return ret;
+}
+
+static int cxgb_change_mtu(struct net_device *dev, int new_mtu)
+{
+	int ret;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	if (new_mtu < 81)         /* accommodate SACK */
+		return -EINVAL;
+	ret = t4_set_rxmode(adapter, adapter->mbox, pi->viid, new_mtu, -1, -1,
+			    -1, -1, true);
+	if (!ret)
+		dev->mtu = new_mtu;
+	return ret;
+}
+
+static int cxgb_set_mac_addr(struct net_device *dev, void *p)
+{
+	int ret;
+	struct sockaddr *addr = p;
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EINVAL;
+
+	ret = t4_change_mac(adapter, adapter->mbox, pi->viid,
+			    pi->xact_addr_filt, addr->sa_data, true, true);
+	if (ret < 0)
+		return ret;
+
+	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+	pi->xact_addr_filt = ret;
+	return 0;
+}
+
+static void vlan_rx_register(struct net_device *dev, struct vlan_group *grp)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+
+	pi->vlan_grp = grp;
+	t4_set_rxmode(adapter, adapter->mbox, pi->viid, -1, -1, -1, -1,
+		      grp != NULL, 0);
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void cxgb_netpoll(struct net_device *dev)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adap = pi->adapter;
+
+	if (adap->flags & USING_MSIX) {
+		int i;
+		struct sge_eth_rxq *rx = &adap->sge.ethrxq[pi->first_qset];
+
+		for (i = pi->nqsets; i; i--, rx++)
+			t4_sge_intr_msix(0, &rx->rspq);
+	} else
+		t4_intr_handler(adap)(0, adap);
+}
+#endif
+
+void t4_fatal_err(struct adapter *adap)
+{
+	t4_set_reg_field(adap, A_SGE_CONTROL, F_GLOBALENABLE, 0);
+	t4_intr_disable(adap);
+	dev_alert(adap->pdev_dev, "encountered fatal error, adapter stopped\n");
+}
+
+/*
+ * Return the specified PCI-E Configuration Space register from our Physical
+ * Function.  We try first via a Firmware LDST Command since we prefer to let
+ * the firmware own all of these registers, but if that fails we go for it
+ * directly ourselves.
+ */
+static u32 t4_read_pcie_cfg4(struct adapter *adap, int reg)
+{
+	struct fw_ldst_cmd ldst_cmd;
+	u32 val;
+	int ret;
+
+	/*
+	 * Construct and send the Firmware LDST Command to retrieve the
+	 * specified PCI-E Configuration Space register.
+	 */
+	memset(&ldst_cmd, 0, sizeof(ldst_cmd));
+	ldst_cmd.op_to_addrspace =
+		htonl(V_FW_CMD_OP(FW_LDST_CMD) |
+		      F_FW_CMD_REQUEST |
+		      F_FW_CMD_READ |
+		      V_FW_LDST_CMD_ADDRSPACE(FW_LDST_ADDRSPC_FUNC_PCIE));
+	ldst_cmd.cycles_to_len16 = htonl(FW_LEN16(ldst_cmd));
+	ldst_cmd.u.pcie.select_naccess = V_FW_LDST_CMD_NACCESS(1);
+	ldst_cmd.u.pcie.ctrl_to_fn =
+		(F_FW_LDST_CMD_LC | V_FW_LDST_CMD_FN(adap->pf));
+	ldst_cmd.u.pcie.r = reg;
+	ret = t4_wr_mbox(adap, adap->mbox, &ldst_cmd, sizeof(ldst_cmd),
+			 &ldst_cmd);
+
+	/*
+	 * If the LDST Command suucceeded, exctract the returned register
+	 * value.  Otherwise read it directly ourself.
+	 */
+	if (ret == 0)
+		val = ntohl(ldst_cmd.u.pcie.data[0]);
+	else
+		t4_hw_pci_read_cfg4(adap, reg, &val);
+
+	return val;
+}
+
+static void setup_memwin(struct adapter *adap)
+{
+	u32 bar0;
+
+	/*
+	 * Truncation intentional: we only read the bottom 32-bits of the
+	 * 64-bit BAR0/BAR1 ...  We use the hardware backdoor mechanism to
+	 * read BAR0 instead of using pci_resource_start() because we could be
+	 * operating from within a Virtual Machine which is trapping our
+	 * accesses to our Configuration Space and we need to set up the PCI-E
+	 * Memory Window decoders with the actual addresses which will be
+	 * coming across the PCI-E link.
+	 */
+	bar0 = t4_read_pcie_cfg4(adap, PCI_BASE_ADDRESS_0);
+	bar0 &= PCI_BASE_ADDRESS_MEM_MASK;
+
+	/*
+	 * Set up memory window for accessing adapter memory ranges.  (Read
+	 * back MA register to ensure that changes propagate before we attempt
+	 * to use the new values.)
+	 */
+	t4_write_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_BASE_WIN, 0),
+		     (bar0 + MEMWIN0_BASE) | V_BIR(0) |
+		     V_WINDOW(ilog2(MEMWIN0_APERTURE) - 10));
+	t4_write_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_BASE_WIN, 1),
+		     (bar0 + MEMWIN1_BASE) | V_BIR(0) |
+		     V_WINDOW(ilog2(MEMWIN1_APERTURE) - 10));
+	t4_write_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_BASE_WIN, 2),
+		     (bar0 + MEMWIN2_BASE) | V_BIR(0) |
+		     V_WINDOW(ilog2(MEMWIN2_APERTURE) - 10));
+	t4_read_reg(adap, PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_BASE_WIN, 2));
+}
+
+static void setup_memwin_rdma(struct adapter *adap)
+{
+	if (adap->vres.ocq.size) {
+		u32 start;
+		unsigned int sz_kb;
+
+		start = t4_read_pcie_cfg4(adap, PCI_BASE_ADDRESS_2);
+		start &= PCI_BASE_ADDRESS_MEM_MASK;
+		start += OCQ_WIN_OFFSET(adap->pdev, &adap->vres);
+		sz_kb = roundup_pow_of_two(adap->vres.ocq.size) >> 10;
+
+		/*
+		 * Set up RDMA memory window for accessing adapter memory
+		 * ranges.  (Read back MA register to ensure that changes
+		 * propagate before we attempt to use the new values.)
+		 */
+		t4_write_reg(adap,
+			     PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_BASE_WIN, 3),
+			     start | V_BIR(1) | V_WINDOW(ilog2(sz_kb)));
+		t4_write_reg(adap,
+			     PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 3),
+			     adap->vres.ocq.start);
+		t4_read_reg(adap,
+			    PCIE_MEM_ACCESS_REG(A_PCIE_MEM_ACCESS_OFFSET, 3));
+	}
+}
+
+/*
+ * Max # of ATIDs.  The absolute HW max is 16K but we keep it lower.
+ */
+#define MAX_ATIDS 8192U
+
+/*
+ * Phase 0 of initialization: contact FW, obtain config, perform basic init.
+ *
+ * If the firmware we're dealing with has Configuration File support, then
+ * we use that to perform all configuration -- either using the configuration
+ * file stored in flash on the adapter or using a filesystem-local file
+ * if available.
+ *
+ * If we don't have configuration file support in the firmware, then we'll
+ * have to set things up the old fashioned way with hard-coded register
+ * writes and firmware commands ...
+ */
+
+/*
+ * Tweak configuration based on module parameters, etc.  Most of these have
+ * defaults assigned to them by Firmware Configuration Files (if we're using
+ * them) but need to be explicitly set if we're using hard-coded
+ * initialization.  But even in the case of using Firmware Configuration
+ * Files, we'd like to expose the ability to change these via module
+ * parameters so these are essentially common tweaks/settings for
+ * Configuration Files and hard-coded initialization ...
+ */
+static int adap_init0_tweaks(struct adapter *adapter)
+{
+	/*
+	 * Fix up various Host-Dependent Parameters like Page Size, Cache
+	 * Line Size, etc.  The firmware default is for a 4KB Page Size and
+	 * 64B Cache Line Size ...
+	 */
+	t4_fixup_host_params(adapter, PAGE_SIZE, L1_CACHE_BYTES);
+
+	/*
+	 * Process module parameters which affect early initialization.
+	 */
+	if (rx_dma_offset != 2 && rx_dma_offset != 0) {
+		dev_err(&adapter->pdev->dev,
+			"Ignoring illegal rx_dma_offset=%d, using 2\n",
+			rx_dma_offset);
+		rx_dma_offset = 2;
+	}
+	t4_set_reg_field(adapter, A_SGE_CONTROL,
+			 V_PKTSHIFT(M_PKTSHIFT),
+			 V_PKTSHIFT(rx_dma_offset));
+
+	return 0;
+}
+
+/*
+ * Attempt to initialize the adapter via a Firmware Configuration File.
+ */
+static int adap_init0_config(struct adapter *adapter, int reset)
+{
+	struct fw_caps_config_cmd caps_cmd;
+	const struct firmware *cf;
+	unsigned long mtype = 0, maddr = 0;
+	u32 finiver, finicsum, cfcsum;
+	int ret, using_flash;
+
+	/*
+	 * Reset device if necessary.
+	 */
+	if (reset) {
+		ret = t4_fw_reset(adapter, adapter->mbox,
+				  F_PIORSTMODE | F_PIORST);
+		if (ret < 0)
+			goto bye;
+	}
+
+	/*
+	 * If we have a T4 configuration file under /lib/firmware/cxgb4/,
+	 * then use that.  Otherwise, use the configuration file stored
+	 * in the adapter flash ...
+	 */
+	ret = request_firmware(&cf, FW_CFNAME, adapter->pdev_dev);
+	if (ret < 0) {
+		using_flash = 1;
+		mtype = FW_MEMTYPE_CF_FLASH;
+		maddr = t4_flash_cfg_addr(adapter);
+	} else {
+		u32 params[7], val[7];
+
+		using_flash = 0;
+		if (cf->size >= FLASH_CFG_MAX_SIZE)
+			ret = -ENOMEM;
+		else {
+			params[0] = (V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DEV) |
+			     V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DEV_CF));
+			ret = t4_query_params(adapter, adapter->mbox,
+					      adapter->pf, 0, 1, params, val);
+			if (ret == 0) {
+				/*
+				 * For t4_memory_write() below addresses and
+				 * sizes have to be in terms of multiples of 4
+				 * bytes.  So, if the Configuration File isn't
+				 * a multiple of 4 bytes in length we'll have
+				 * to write that out separately since we can't
+				 * guarantee that the bytes following the
+				 * residual byte in the buffer returned by
+				 * request_firmware() are zeroed out ...
+				 */
+				size_t resid = cf->size & 0x3;
+				size_t size = cf->size & ~0x3;
+				__be32 *data = (__be32 *)cf->data;
+
+				mtype = G_FW_PARAMS_PARAM_Y(val[0]);
+				maddr = G_FW_PARAMS_PARAM_Z(val[0]) << 16;
+
+				ret = t4_memory_write(adapter, mtype, maddr,
+						      size, data);
+				if (ret == 0 && resid != 0) {
+					union {
+						__be32 word;
+						char buf[4];
+					} last;
+					int i;
+
+					last.word = data[size >> 2];
+					for (i = resid; i < 4; i++)
+						last.buf[i] = 0;
+					ret = t4_memory_write(adapter, mtype,
+							      maddr + size,
+							      4, &last.word);
+				}
+			}
+		}
+
+		release_firmware(cf);
+		if (ret)
+			goto bye;
+	}
+
+	/*
+	 * Issue a Capability Configuration command to the firmware to get it
+	 * to parse the Configuration File.  We don't use t4_fw_config_file()
+	 * because we want the ability to modify various features after we've
+	 * processed the configuration file ...
+	 */
+	memset(&caps_cmd, 0, sizeof(caps_cmd));
+	caps_cmd.op_to_write =
+		htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+		      F_FW_CMD_REQUEST |
+		      F_FW_CMD_READ);
+	caps_cmd.cfvalid_to_len16 =
+		htonl(F_FW_CAPS_CONFIG_CMD_CFVALID |
+		      V_FW_CAPS_CONFIG_CMD_MEMTYPE_CF(mtype) |
+		      V_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF(maddr >> 16) |
+		      FW_LEN16(caps_cmd));
+	ret = t4_wr_mbox(adapter, adapter->mbox, &caps_cmd, sizeof(caps_cmd),
+			 &caps_cmd);
+	if (ret < 0)
+		goto bye;
+
+	finiver = ntohl(caps_cmd.finiver);
+	finicsum = ntohl(caps_cmd.finicsum);
+	cfcsum = ntohl(caps_cmd.cfcsum);
+	if (finicsum != cfcsum)
+		dev_warn(adapter->pdev_dev, "Configuration File checksum mismatch: "
+			 " [fini] csum=%#x, computed csum=%#x\n",
+			 finicsum, cfcsum);
+
+#ifndef CONFIG_CHELSIO_T4_OFFLOAD
+	/*
+	 * If we're a pure NIC driver then disable all offloading facilities.
+	 * This will allow the firmware to optimize aspects of the hardware
+	 * configuration which will result in improved performance.
+	 */
+	caps_cmd.toecaps = 0;
+	caps_cmd.iscsicaps = 0;
+	caps_cmd.rdmacaps = 0;
+	caps_cmd.fcoecaps = 0;
+#endif
+
+	/*
+	 * And now tell the firmware to use the configuration we just loaded.
+	 */
+	caps_cmd.op_to_write =
+		htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+		      F_FW_CMD_REQUEST |
+		      F_FW_CMD_WRITE);
+	caps_cmd.cfvalid_to_len16 = htonl(FW_LEN16(caps_cmd));
+	ret = t4_wr_mbox(adapter, adapter->mbox, &caps_cmd, sizeof(caps_cmd),
+			 NULL);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Tweak configuration based on system architecture, module
+	 * parameters, etc.
+	 */
+	ret = adap_init0_tweaks(adapter);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * And finally tell the firmware to initialize itself using the
+	 * parameters from the Configuration File.
+	 */
+	ret = t4_fw_initialize(adapter, adapter->mbox);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Return successfully and note that we're operating with parameters
+	 * not supplied by the driver, rather than from hard-wired
+	 * initialization constants burried in the driver.
+	 */
+	adapter->flags |= USING_SOFT_PARAMS;
+	dev_info(adapter->pdev_dev, "Successfully configured using Firmware "
+		 "Configuration File %s, version %#x, computed checksum %#x\n",
+		 (using_flash
+		  ? "in device FLASH"
+		  : "/lib/firmware/" FW_CFNAME),
+		 finiver, cfcsum);
+	return 0;
+
+	/*
+	 * Something bad happened.  Return the error ...  (If the "error"
+	 * is that there's no Configuration File on the adapter we don't
+	 * want to issue a warning since this is fairly common.)
+	 */
+bye:
+	if (ret != -ENOENT)
+		dev_warn(adapter->pdev_dev, "Configuration file error %d\n",
+			 -ret);
+	return ret;
+}
+
+/*
+ * Attempt to initialize the adapter via hard-coded, driver supplied
+ * parameters ...
+ */
+static int adap_init0_no_config(struct adapter *adapter, int reset)
+{
+	struct sge *s = &adapter->sge;
+	struct fw_caps_config_cmd caps_cmd;
+	u32 v;
+	int i, ret;
+
+	/*
+	 * Reset device if necessary
+	 */
+	if (reset) {
+		ret = t4_fw_reset(adapter, adapter->mbox,
+				  F_PIORSTMODE | F_PIORST);
+		if (ret < 0)
+			goto bye;
+	}
+
+	/*
+	 * Get device capabilities and select which we'll be using.
+	 */
+	memset(&caps_cmd, 0, sizeof(caps_cmd));
+	caps_cmd.op_to_write = htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+				     F_FW_CMD_REQUEST | F_FW_CMD_READ);
+	caps_cmd.cfvalid_to_len16 = htonl(FW_LEN16(caps_cmd));
+	ret = t4_wr_mbox(adapter, adapter->mbox, &caps_cmd, sizeof(caps_cmd),
+			 &caps_cmd);
+	if (ret < 0)
+		goto bye;
+
+#ifndef CONFIG_CHELSIO_T4_OFFLOAD
+	/*
+	 * If we're a pure NIC driver then disable all offloading facilities.
+	 * This will allow the firmware to optimize aspects of the hardware
+	 * configuration which will result in improved performance.
+	 */
+	caps_cmd.toecaps = 0;
+	caps_cmd.iscsicaps = 0;
+	caps_cmd.rdmacaps = 0;
+	caps_cmd.fcoecaps = 0;
+#endif
+
+	if (caps_cmd.niccaps & htons(FW_CAPS_CONFIG_NIC_VM)) {
+		if (!vf_acls)
+			caps_cmd.niccaps ^= htons(FW_CAPS_CONFIG_NIC_VM);
+		else
+			caps_cmd.niccaps = htons(FW_CAPS_CONFIG_NIC_VM);
+	} else if (vf_acls) {
+		CH_ERR(adapter, "virtualization ACLs not supported");
+		goto bye;
+	}
+	caps_cmd.op_to_write = htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+			      F_FW_CMD_REQUEST | F_FW_CMD_WRITE);
+	ret = t4_wr_mbox(adapter, adapter->mbox, &caps_cmd, sizeof(caps_cmd), NULL);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Tweak configuration based on system architecture, module
+	 * parameters, etc.
+	 */
+	ret = adap_init0_tweaks(adapter);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Select RSS Global Mode we want to use.  We use "Basic Virtual"
+	 * mode which maps each Virtual Interface to its own section of
+	 * the RSS Table and we turn on all map and hash enables ...
+	 */
+	adapter->flags |= RSS_TNLALLLOOKUP;
+	ret = t4_config_glbl_rss(adapter, adapter->mbox,
+				 FW_RSS_GLB_CONFIG_CMD_MODE_BASICVIRTUAL,
+				 F_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN |
+				 F_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ |
+				 ((adapter->flags & RSS_TNLALLLOOKUP) ?
+					F_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP : 0));
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Set up our own fundamental resource provisioning ...
+	 */
+	ret = t4_cfg_pfvf(adapter, adapter->mbox, adapter->pf, 0,
+			  PFRES_NEQ, PFRES_NETHCTRL,
+			  PFRES_NIQFLINT, PFRES_NIQ,
+			  PFRES_TC, PFRES_NVI,
+			  M_FW_PFVF_CMD_CMASK,
+			  pfvfres_pmask(adapter, adapter->pf, 0),
+			  PFRES_NEXACTF,
+			  PFRES_R_CAPS, PFRES_WX_CAPS);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Perform low level SGE initialization.  We need to do this before we
+	 * send the firmware the INITIALIZE command because that will cause
+	 * any other PF Drivers which are waiting for the Master
+	 * Initialization to proceed forward.
+	 */
+	for (i = 0; i < SGE_NTIMERS - 1; i++)
+		s->timer_val[i] = min(intr_holdoff[i], MAX_SGE_TIMERVAL);
+	s->timer_val[SGE_NTIMERS - 1] = MAX_SGE_TIMERVAL;
+	s->counter_val[0] = 1;
+	for (i = 1; i < SGE_NCOUNTERS; i++)
+		s->counter_val[i] = min(intr_cnt[i - 1], M_THRESHOLD_0);
+	t4_sge_init(adapter);
+
+#ifdef CONFIG_PCI_IOV
+	/*
+	 * Provision resource limits for Virtual Functions.  We currently
+	 * grant them all the same static resource limits except for the Port
+	 * Access Rights Mask which we're assigning based on the PF.  All of
+	 * the static provisioning stuff for both the PF and VF really needs
+	 * to be managed in a persistent manner for each device which the
+	 * firmware controls.
+	 */
+	{
+		int pf, vf;
+
+		for (pf = 0; pf < ARRAY_SIZE(num_vf); pf++) {
+			if (num_vf[pf] <= 0)
+				continue;
+
+			/* VF numbering starts at 1! */
+			for (vf = 1; vf <= num_vf[pf]; vf++) {
+				ret = t4_cfg_pfvf(adapter, adapter->mbox,
+						  pf, vf,
+						  VFRES_NEQ, VFRES_NETHCTRL,
+						  VFRES_NIQFLINT, VFRES_NIQ,
+						  VFRES_TC, VFRES_NVI,
+						  M_FW_PFVF_CMD_CMASK,
+						  pfvfres_pmask(adapter, pf, vf),
+						  VFRES_NEXACTF,
+						  VFRES_R_CAPS, VFRES_WX_CAPS);
+				if (ret < 0)
+					dev_warn(adapter->pdev_dev, "failed to "
+						 "provision pf/vf=%d/%d; "
+						 "err=%d\n", pf, vf, ret);
+			}
+		}
+	}
+#endif
+
+	/*
+	 * Set up the default filter mode.  Later we'll want to implement this
+	 * via a firmware command, etc. ...  This needs to be done before the
+	 * firmare initialization command ...  If the selected set of fields
+	 * isn't equal to the default value, we'll need to make sure that the
+	 * field selections will fit in the 36-bit budget.
+	 */
+	if (tp_vlan_pri_map != TP_VLAN_PRI_MAP_DEFAULT) {
+		int i, bits = 0;
+
+		for (i = TP_VLAN_PRI_MAP_FIRST; i <= TP_VLAN_PRI_MAP_LAST; i++)
+			switch (tp_vlan_pri_map & (1 << i)) {
+			    case 0:
+				/* compressed filter field not enabled */
+				break;
+
+			    case F_FCOE:          bits +=  1; break;
+			    case F_PORT:          bits +=  3; break;
+			    case F_VNIC_ID:       bits += 17; break;
+			    case F_VLAN:          bits += 17; break;
+			    case F_TOS:           bits +=  8; break;
+			    case F_PROTOCOL:      bits +=  8; break;
+			    case F_ETHERTYPE:     bits += 16; break;
+			    case F_MACMATCH:      bits +=  9; break;
+			    case F_MPSHITTYPE:    bits +=  3; break;
+			    case F_FRAGMENTATION: bits +=  1; break;
+			}
+
+		if (bits > 36) {
+			CH_ERR(adapter, "tp_vlan_pri_map=%#x needs %d bits > 36;"
+			       " using %#x\n", tp_vlan_pri_map, bits,
+			       TP_VLAN_PRI_MAP_DEFAULT);
+			tp_vlan_pri_map = TP_VLAN_PRI_MAP_DEFAULT;
+		}
+	}
+	v = tp_vlan_pri_map;
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &v, 1, A_TP_VLAN_PRI_MAP);
+
+	/*
+	 * We need Five Tuple Lookup mode to be set in TP_GLOBAL_CONFIG order
+	 * to support any of the compressed filter fields above.  Newer
+	 * versions of the firmware do this automatically but it doesn't hurt
+	 * to set it here.  Meanwhile, we do _not_ need to set Lookup Every
+	 * Packet in TP_INGRESS_CONFIG to support matching non-TCP packets
+	 * since the firmware automatically turns this on and off when we have
+	 * a non-zero number of filters active (since it does have a
+	 * performance impact).
+	 */
+	if (tp_vlan_pri_map)
+		t4_set_reg_field(adapter, A_TP_GLOBAL_CONFIG,
+				 V_FIVETUPLELOOKUP(M_FIVETUPLELOOKUP),
+				 V_FIVETUPLELOOKUP(M_FIVETUPLELOOKUP));
+
+	/*
+	 * Tweak some settings.
+	 */
+	t4_write_reg(adapter, A_TP_SHIFT_CNT, V_SYNSHIFTMAX(6) |
+		     V_RXTSHIFTMAXR1(4) | V_RXTSHIFTMAXR2(15) |
+		     V_PERSHIFTBACKOFFMAX(8) | V_PERSHIFTMAX(8) |
+		     V_KEEPALIVEMAXR1(4) | V_KEEPALIVEMAXR2(9));
+
+	/*
+	 * Get basic stuff going by issuing the Firmware Initialize command.
+	 * Note that this _must_ be after all PFVF commands ...
+	 */
+	ret = t4_fw_initialize(adapter, adapter->mbox);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Return successfully!
+	 */
+	dev_info(adapter->pdev_dev, "Successfully configured using built-in "
+		 "driver parameters\n");
+	return 0;
+
+	/*
+	 * Something bad happened.  Return the error ...
+	 */
+bye:
+	return ret;
+}
+
+static int adap_init0(struct adapter *adap)
+{
+	int ret;
+	u32 v, port_vec;
+	enum dev_state state;
+	u32 params[7], val[7];
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	struct fw_caps_config_cmd caps_cmd;
+#endif
+	struct fw_devlog_cmd devlog_cmd;
+	u32 devlog_meminfo;
+	int j;
+	int reset = 1;
+
+	if (!fw_attach)
+		return 0;
+
+	/*
+	 * Contact FW, advertising Master capability (and potentially forcing
+	 * ourselves as the Master PF if our module parameter force_init is
+	 * set).
+	 */
+	ret = t4_fw_hello(adap, adap->mbox, adap->mbox,
+			  force_init ? MASTER_MUST : MASTER_MAY,
+			  &state);
+	if (ret < 0) {
+		CH_ERR(adap, "could not connect to FW, error %d\n", -ret);
+		return ret;
+	}
+	if (ret == adap->mbox)
+		adap->flags |= MASTER_PF;
+	if (force_init && state == DEV_STATE_INIT)
+		state = DEV_STATE_UNINIT;
+
+	/*
+	 * If we're the Master PF Driver and the device is uninitialized,
+	 * then let's consider upgrading the firmware ...  (We always want
+	 * to check the firmware version number in order to A. get it for
+	 * later reporting and B. to warn if the currently loaded firmware
+	 * is excessively mismatched relative to the driver.)
+	 */
+	ret = t4_check_fw_version(adap);
+	if ((adap->flags & MASTER_PF) && state != DEV_STATE_INIT) {
+		if (ret == -EINVAL || ret > 0) {
+			if (upgrade_fw(adap) >= 0) {
+				/*
+				 * Note that the chip was reset as part of the
+				 * firmware upgrade so we don't reset it again
+				 * below and grab the new firmware version.
+				 */
+				reset = 0;
+				ret = t4_check_fw_version(adap);
+			}
+		}
+		if (ret < 0)
+			return ret;
+	}
+
+	/*
+	 * Grab VPD parameters.  This should be done after we establish a
+	 * connection to the firmware since some of the VPD parameters
+	 * (notably the Core Clock frequency) are retrieved via requests to
+	 * the firmware.  On the other hand, we need these fairly early on
+	 * so we do this right after getting ahold of the firmware.
+	 */
+	ret = t4_get_vpd_params(adap, &adap->params.vpd);
+	if (ret < 0)
+		goto bye;
+
+	/*
+	 * Read firmware device log parameters.  We really need to find a way
+	 * to get these parameters initialized with some default values (which
+	 * are likely to be correct) for the case where we either don't
+	 * attache to the firmware or it's crashed when we probe the adapter.
+	 * That way we'll still be able to perform early firmware startup
+	 * debugging ...  If the request to get the Firmware's Device Log
+	 * parameters fails, we'll live so we don't make that a fatal error.
+	 */
+	memset(&devlog_cmd, 0, sizeof devlog_cmd);
+	devlog_cmd.op_to_write = htonl(V_FW_CMD_OP(FW_DEVLOG_CMD) |
+				       F_FW_CMD_REQUEST | F_FW_CMD_READ);
+	devlog_cmd.retval_len16 = htonl(FW_LEN16(devlog_cmd));
+	ret = t4_wr_mbox(adap, adap->mbox, &devlog_cmd, sizeof(devlog_cmd),
+			 &devlog_cmd);
+	if (ret == 0) {
+		devlog_meminfo = ntohl(devlog_cmd.memtype_devlog_memaddr16_devlog);
+		adap->params.devlog.memtype = G_FW_DEVLOG_CMD_MEMTYPE_DEVLOG(devlog_meminfo);
+		adap->params.devlog.start = G_FW_DEVLOG_CMD_MEMADDR16_DEVLOG(devlog_meminfo) << 4;
+		adap->params.devlog.size = ntohl(devlog_cmd.memsize_devlog);
+	}
+
+	/*
+	 * Find out what ports are available to us.  Note that we need to do
+	 * this before calling adap_init0_no_config() since it needs nports
+	 * and portvec ...
+	 */
+	v =
+	    V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DEV) |
+	    V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DEV_PORTVEC);
+	ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 1, &v, &port_vec);
+	if (ret < 0)
+		goto bye;
+
+#ifdef CHELSIO_T4_DIAGS
+	/*
+	 * If attach_pf0 is specified we can only access a single port because
+	 * the default configuration only provisions a single Virtual Interface
+	 * for PF0. So we whack the Port Vector bitmask to only include the
+	 * lowest available port number.
+	 */
+	if (attach_pf0)
+		port_vec ^= (port_vec & (port_vec - 1));
+#endif
+
+	adap->params.nports = hweight32(port_vec);
+	adap->params.portvec = port_vec;
+
+	/*
+	 * If the firmware is initialized already (and we're not forcing a
+	 * master initialization), note that we're living with existing
+	 * adapter parameters.  Otherwise, it's time to try initializing the
+	 * adapter ...
+	 */
+	if (state == DEV_STATE_INIT) {
+		dev_info(adap->pdev_dev, "Coming up as %s: "
+			 "Adapter already initialized\n",
+			 adap->flags & MASTER_PF ? "MASTER" : "SLAVE");
+		adap->flags |= USING_SOFT_PARAMS;
+	} else {
+		dev_info(adap->pdev_dev, "Coming up as MASTER: "
+			 "Initializing adapter\n");
+		if (force_old_init)
+			ret = adap_init0_no_config(adap, reset);
+		else {
+			/*
+			 * Find out whether we're dealing with a version of
+			 * the firmware which has configuration file support.
+			 */
+			params[0] = (V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DEV) |
+				     V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DEV_CF));
+			ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 1,
+					      params, val);
+
+			/*
+			 * If the firmware doesn't support Configuration
+			 * Files, use the old Driver-based, hard-wired
+			 * initialization.  Otherwise, try using the
+			 * Configuration File support and fall back to the
+			 * Driver-based initialization if there's no
+			 * Configuration File found.
+			 */
+			if (ret < 0)
+				ret = adap_init0_no_config(adap, reset);
+			else {
+				/*
+				 * The firmware provides us with a memory
+				 * buffer where we can load a Configuration
+				 * File from the host if we want to override
+				 * the Configuration File in flash.
+				 */
+
+				ret = adap_init0_config(adap, reset);
+				if (ret == -ENOENT) {
+					dev_info(adap->pdev_dev,
+					    "No Configuration File present "
+					    "on adapter.  Using hard-wired "
+					    "configuration parameters.\n");
+					ret = adap_init0_no_config(adap, reset);
+				}
+			}
+		}
+		if (ret < 0) {
+			CH_ERR(adap, "could not initialize adapter, error %d\n",
+			       -ret);
+			goto bye;
+		}
+	}
+
+	/*
+	 * If we're living with non-hard-coded parameters (either from a
+	 * Firmware Configuration File or values programmed by a different PF
+	 * Driver), give the SGE code a chance to pull in anything that it
+	 * needs ...  Note that this must be called after we retrieve our VPD
+	 * parameters in order to know how to convert core ticks to seconds.
+	 */
+	if (adap->flags & USING_SOFT_PARAMS) {
+		ret = t4_sge_init(adap);
+		if (ret < 0)
+			goto bye;
+	}
+
+#ifdef CONFIG_CHELSIO_BYPASS
+	if (is_bypass_device(adap->pdev->device))
+		adap->params.bypass = 1;
+#endif
+
+	/*
+	 * Grab some of our basic fundamental operating parameters.
+	 */
+#define FW_PARAM_DEV(param) \
+	V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DEV) | \
+	V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DEV_##param)
+
+#define FW_PARAM_PFVF(param) \
+	V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_PFVF) | \
+	V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_PFVF_##param)|  \
+	V_FW_PARAMS_PARAM_Y(0) | \
+	V_FW_PARAMS_PARAM_Z(0)
+
+	params[0] = FW_PARAM_PFVF(EQ_START);
+	params[1] = FW_PARAM_PFVF(L2T_START);
+	params[2] = FW_PARAM_PFVF(L2T_END);
+	params[3] = FW_PARAM_PFVF(FILTER_START);
+	params[4] = FW_PARAM_PFVF(FILTER_END);
+	params[5] = FW_PARAM_PFVF(IQFLINT_START);
+	ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 6, params, val);
+	if (ret < 0)
+		goto bye;
+	adap->sge.egr_start = val[0];
+	adap->l2t_start = val[1];
+	adap->l2t_end = val[2];
+	adap->tids.ftid_base = val[3];
+	adap->tids.nftids = val[4] - val[3] + 1;
+	adap->sge.ingr_start = val[5];
+
+	/* query params related to active filter region */
+	params[0] = FW_PARAM_PFVF(ACTIVE_FILTER_START);
+	params[1] = FW_PARAM_PFVF(ACTIVE_FILTER_END);
+	ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 2, params, val);
+	/* If Active filter size is set we enable establishing
+	 * offload connection through firmware work request
+	 */
+	if ((val[0] != val[1]) && (ret >= 0)) {
+		adap->flags |= FW_OFLD_CONN;
+		adap->tids.aftid_base = val[0];
+		adap->tids.aftid_end = val[1];
+	}
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	/*
+	 * Get device capabilities so we can determine what resources we need
+	 * to manage.
+	 */
+	memset(&caps_cmd, 0, sizeof(caps_cmd));
+	caps_cmd.op_to_write = htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+				     F_FW_CMD_REQUEST | F_FW_CMD_READ);
+	caps_cmd.cfvalid_to_len16 = htonl(FW_LEN16(caps_cmd));
+	ret = t4_wr_mbox(adap, adap->mbox, &caps_cmd, sizeof(caps_cmd),
+			 &caps_cmd);
+	if (ret < 0)
+		goto bye;
+
+	if (caps_cmd.toecaps) {
+		/* query offload-related parameters */
+		params[0] = FW_PARAM_DEV(NTID);
+		params[1] = FW_PARAM_PFVF(SERVER_START);
+		params[2] = FW_PARAM_PFVF(SERVER_END);
+		params[3] = FW_PARAM_PFVF(TDDP_START);
+		params[4] = FW_PARAM_PFVF(TDDP_END);
+		params[5] = FW_PARAM_DEV(FLOWC_BUFFIFO_SZ);
+		ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 6,
+				      params, val);
+		if (ret < 0)
+			goto bye;
+		adap->tids.ntids = val[0];
+		adap->tids.natids = min(adap->tids.ntids / 2, MAX_ATIDS);
+		adap->tids.stid_base = val[1];
+		adap->tids.nstids = val[2] - val[1] + 1;
+		/*
+		 * Setup server filter region. Divide the availble filter
+		 * region into two parts. Regular filters get 1/3rd and server
+		 * filters get 2/3rd part. This is only enabled if workarond
+		 * path is enabled.
+		 * 1. For regular filters.
+		 * 2. Server filter: This are special filters which are used
+		 * to redirect SYN packets to offload queue.
+		 */
+		if (adap->flags & FW_OFLD_CONN && !is_bypass(adap)) {
+			adap->tids.sftid_base = adap->tids.ftid_base +
+					DIV_ROUND_UP(adap->tids.nftids, 3);
+			adap->tids.nsftids = adap->tids.nftids -
+					 DIV_ROUND_UP(adap->tids.nftids, 3);
+			adap->tids.nftids = adap->tids.sftid_base -
+						adap->tids.ftid_base;
+		}
+		adap->vres.ddp.start = val[3];
+		adap->vres.ddp.size = val[4] - val[3] + 1;
+		adap->params.ofldq_wr_cred = val[5];
+		adap->params.offload = 1;
+	}
+	if (caps_cmd.rdmacaps) {
+		params[0] = FW_PARAM_PFVF(STAG_START);
+		params[1] = FW_PARAM_PFVF(STAG_END);
+		params[2] = FW_PARAM_PFVF(RQ_START);
+		params[3] = FW_PARAM_PFVF(RQ_END);
+		params[4] = FW_PARAM_PFVF(PBL_START);
+		params[5] = FW_PARAM_PFVF(PBL_END);
+		ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 6,
+				      params, val);
+		if (ret < 0)
+			goto bye;
+		adap->vres.stag.start = val[0];
+		adap->vres.stag.size = val[1] - val[0] + 1;
+		adap->vres.rq.start = val[2];
+		adap->vres.rq.size = val[3] - val[2] + 1;
+		adap->vres.pbl.start = val[4];
+		adap->vres.pbl.size = val[5] - val[4] + 1;
+
+		params[0] = FW_PARAM_PFVF(SQRQ_START);
+		params[1] = FW_PARAM_PFVF(SQRQ_END);
+		params[2] = FW_PARAM_PFVF(CQ_START);
+		params[3] = FW_PARAM_PFVF(CQ_END);
+		params[4] = FW_PARAM_PFVF(OCQ_START);
+		params[5] = FW_PARAM_PFVF(OCQ_END);
+		ret = t4_query_params(adap, 0, 0, 0, 6, params, val);
+		if (ret < 0)
+			goto bye;
+		adap->vres.qp.start = val[0];
+		adap->vres.qp.size = val[1] - val[0] + 1;
+		adap->vres.cq.start = val[2];
+		adap->vres.cq.size = val[3] - val[2] + 1;
+		adap->vres.ocq.start = val[4];
+		adap->vres.ocq.size = val[5] - val[4] + 1;
+	}
+	if (caps_cmd.iscsicaps) {
+		params[0] = FW_PARAM_PFVF(ISCSI_START);
+		params[1] = FW_PARAM_PFVF(ISCSI_END);
+		ret = t4_query_params(adap, adap->mbox, adap->pf, 0, 2,
+				      params, val);
+		if (ret < 0)
+			goto bye;
+		adap->vres.iscsi.start = val[0];
+		adap->vres.iscsi.size = val[1] - val[0] + 1;
+	}
+
+#undef FW_PARAM_PFVF
+#undef FW_PARAM_DEV
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+	/*
+	 * These are finalized by FW initialization, load their values now.
+	 */
+	v = t4_read_reg(adap, A_TP_TIMER_RESOLUTION);
+	adap->params.tp.tre = G_TIMERRESOLUTION(v);
+	adap->params.tp.dack_re = G_DELAYEDACKRESOLUTION(v);
+	t4_read_mtu_tbl(adap, adap->params.mtus, NULL);
+	t4_load_mtus(adap, adap->params.mtus, adap->params.a_wnd,
+		     adap->params.b_wnd);
+
+	/* MODQ_REQ_MAP defaults to setting queues 0-3 to chan 0-3 */
+	for (j=0; j< NCHAN; j++)
+		adap->params.tp.tx_modq[j] = j;
+	
+	adap->flags |= FW_OK;
+	return 0;
+
+	/*
+	 * Something bad happened.  If a command timed out or failed with EIO
+	 * FW does not operate within its spec or something catastrophic
+	 * happened to HW/FW, stop issuing commands.
+	 */
+bye:
+	if (ret != -ETIMEDOUT && ret != -EIO)
+		t4_fw_bye(adap, adap->mbox);
+	return ret;
+}
+
+static inline bool is_10g_port(const struct link_config *lc)
+{
+	return (lc->supported & FW_PORT_CAP_SPEED_10G) != 0;
+}
+
+static inline void init_rspq(struct adapter *adap, struct sge_rspq *q,
+			     unsigned int us, unsigned int cnt,
+			     unsigned int size, unsigned int iqe_size)
+{
+	q->adapter = adap;
+	set_rspq_intr_params(q, us, cnt);
+	q->iqe_len = iqe_size;
+	q->size = size;
+}
+
+/*
+ * Perform default configuration of DMA queues depending on the number and type
+ * of ports we found and the number of available CPUs.  Most settings can be
+ * modified by the admin prior to actual use.
+ */
+static void __devinit cfg_queues(struct adapter *adap)
+{
+	struct sge *s = &adap->sge;
+	int i, q10g = 0, n10g = 0, qidx = 0;
+
+	for_each_port(adap, i)
+		n10g += is_10g_port(&adap2pinfo(adap, i)->link_cfg);
+
+	/*
+	 * We default to 1 queue per non-10G port and up to # of cores queues
+	 * per 10G port.
+	 */
+	if (n10g)
+		q10g = (MAX_ETH_QSETS - (adap->params.nports - n10g)) / n10g;
+	if (q10g > online_cpus())
+		q10g = online_cpus();
+
+	for_each_port(adap, i) {
+		struct port_info *pi = adap2pinfo(adap, i);
+
+		pi->first_qset = qidx;
+		pi->nqsets = is_10g_port(&pi->link_cfg) ? q10g : 1;
+		if (pi->nqsets > pi->rss_size)
+			pi->nqsets = pi->rss_size;
+		qidx += pi->nqsets;
+	}
+
+	s->ethqsets = qidx;
+	s->max_ethqsets = qidx;   /* MSI-X may lower it later */
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	if (is_offload(adap)) {
+		/*
+		 * For offload we use 1 queue/channel if all ports are up to 1G,
+		 * otherwise we divide all available queues amongst the channels
+		 * capped by the number of available cores.
+		 */
+		if (n10g) {
+			i = min_t(int, ARRAY_SIZE(s->ofldrxq),
+				  online_cpus());
+			s->ofldqsets = roundup(i, adap->params.nports);
+		} else
+			s->ofldqsets = adap->params.nports;
+		/* For RDMA one Rx queue per channel suffices */
+		s->rdmaqs = adap->params.nports;
+#ifdef SCSI_CXGB4_ISCSI
+		s->niscsiq = adap->params.nports;
+#endif
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+	for (i = 0; i < ARRAY_SIZE(s->ethrxq); i++) {
+		struct sge_eth_rxq *r = &s->ethrxq[i];
+
+		init_rspq(adap, &r->rspq, 5, 10, 1024, 64);
+		r->useskbs = rx_useskbs;
+		r->fl.size = (r->useskbs ? 1024 : 72);
+	}
+
+	for (i = 0; i < ARRAY_SIZE(s->ethtxq); i++)
+		s->ethtxq[i].q.size = 1024;
+
+	for (i = 0; i < ARRAY_SIZE(s->ctrlq); i++)
+		s->ctrlq[i].q.size = 512;
+
+	for (i = 0; i < ARRAY_SIZE(s->ofldtxq); i++)
+		s->ofldtxq[i].q.size = 1024;
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	for (i = 0; i < ARRAY_SIZE(s->ofldrxq); i++) {
+		struct sge_ofld_rxq *r = &s->ofldrxq[i];
+
+		init_rspq(adap, &r->rspq, 5, 1, 1024, 64);
+		r->rspq.uld = CXGB4_ULD_TOE;
+		r->useskbs = 0;
+		r->fl.size = 72;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(s->rdmarxq); i++) {
+		struct sge_ofld_rxq *r = &s->rdmarxq[i];
+
+		init_rspq(adap, &r->rspq, 5, 1, 511, 64);
+		r->rspq.uld = CXGB4_ULD_RDMA;
+		r->useskbs = 0;
+		r->fl.size = 72;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(s->iscsirxq); i++) {
+		struct sge_ofld_rxq *r = &s->iscsirxq[i];
+
+		init_rspq(adap, &r->rspq, 5, 1, 1024, 64);
+		r->rspq.uld = CXGB4_ULD_ISCSI;
+		r->useskbs = 0;
+		r->fl.size = 72;
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+	init_rspq(adap, &s->fw_evtq, 0, 1, 1024, 64);
+	init_rspq(adap, &s->intrq, 0, 1, 2 * MAX_INGQ, 64);
+}
+
+/*
+ * Interrupt handler used to check if MSI/MSI-X works on this platform.
+ */
+static irqreturn_t __devinit check_intr_handler(int irq, void *data)
+{
+	struct adapter *adap = data;
+
+	adap->swintr = 1;
+	t4_write_reg(adap, MYPF_REG(A_PL_PF_INT_CAUSE), F_PFSW);
+	t4_read_reg(adap, MYPF_REG(A_PL_PF_INT_CAUSE));          /* flush */
+	return IRQ_HANDLED;
+}
+
+static void __devinit check_msi(struct adapter *adap)
+{
+	int vec;
+
+	vec = (adap->flags & USING_MSI) ? adap->pdev->irq :
+					  adap->msix_info[0].vec;
+
+	if (request_irq(vec, check_intr_handler, 0, adap->name, adap))
+		return;
+
+	adap->swintr = 0;
+	t4_write_reg(adap, MYPF_REG(A_PL_PF_INT_ENABLE), F_PFSW);
+	t4_write_reg(adap, MYPF_REG(A_PL_PF_CTL), F_SWINT);
+	msleep(10);
+	t4_write_reg(adap, MYPF_REG(A_PL_PF_INT_ENABLE), 0);
+	free_irq(vec, adap);
+
+	if (!adap->swintr) {
+		const char *s = (adap->flags & USING_MSI) ? "MSI" : "MSI-X";
+
+		cxgb_disable_msi(adap);
+		dev_info(adap->pdev_dev,
+			 "the kernel believes that %s is available on this "
+			 "platform\nbut the driver's %s test has failed.  "
+			 "Proceeding with INTx interrupts.\n", s, s);
+	}
+}
+
+/*
+ * Reduce the number of Ethernet queues across all ports to at most n.
+ * n provides at least one queue per port.
+ */
+static void __devinit reduce_ethqs(struct adapter *adap, int n)
+{
+	int i;
+	struct port_info *pi;
+
+	while (n < adap->sge.ethqsets)
+		for_each_port(adap, i) {
+			pi = adap2pinfo(adap, i);
+			if (pi->nqsets > 1) {
+				pi->nqsets--;
+				adap->sge.ethqsets--;
+				if (adap->sge.ethqsets <= n)
+					break;
+			}
+		}
+
+	n = 0;
+	for_each_port(adap, i) {
+		pi = adap2pinfo(adap, i);
+		pi->first_qset = n;
+		n += pi->nqsets;
+	}
+}
+
+/* 2 MSI-X vectors needed for the FW queue and non-data interrupts */
+#define EXTRA_VECS 2
+
+static int __devinit cxgb_enable_msix(struct adapter *adap)
+{
+	int ofld_need = 0;
+	int i, err, want, need;
+	struct sge *s = &adap->sge;
+	unsigned int nchan = adap->params.nports;
+	struct msix_entry entries[MAX_INGQ + 1];
+
+	for (i = 0; i < ARRAY_SIZE(entries); ++i)
+		entries[i].entry = i;
+
+	want = s->max_ethqsets + EXTRA_VECS;
+	if (is_offload(adap)) {
+		want += s->rdmaqs + s->ofldqsets + s->niscsiq;
+		/* need nchan for each possible ULD */
+		ofld_need = 2 * nchan;
+#ifdef SCSI_CXGB4_ISCSI
+		ofld_need += nchan;
+#endif
+	}
+	need = adap->params.nports + EXTRA_VECS + ofld_need;
+
+	while ((err = pci_enable_msix(adap->pdev, entries, want)) >= need)
+		want = err;
+
+	if (!err) {
+		/*
+		 * Distribute available vectors to the various queue groups.
+		 * Every group gets its minimum requirement and NIC gets top
+		 * priority for leftovers.
+		 */
+		i = want - EXTRA_VECS - ofld_need;
+		if (i < s->max_ethqsets) {
+			s->max_ethqsets = i;
+			if (i < s->ethqsets)
+				reduce_ethqs(adap, i);
+		}
+		if (is_offload(adap)) {
+			/* after NIC leftovers go to TOE */
+			i = want - EXTRA_VECS - s->max_ethqsets;
+			i -= ofld_need - nchan;
+			s->ofldqsets = (i / nchan) * nchan;  /* round down */
+		}
+		for (i = 0; i < want; ++i)
+			adap->msix_info[i].vec = entries[i].vector;
+	} else if (err > 0) {
+		pci_disable_msix(adap->pdev);
+		dev_info(adap->pdev_dev, 
+			 "only %d MSI-X vectors left, not using MSI-X\n", err);
+	}
+	return err;
+}
+
+#undef EXTRA_VECS
+
+static void __devinit print_port_info(adapter_t *adap)
+{
+	static const char *base[] = {
+		"Fiber_XFI",
+		"Fiber_XAUI",
+		"BT_SGMII",
+		"BT_XFI",
+		"BT_XAUI",
+		"KX4",
+		"CX4",
+		"KX",
+		"KR",
+		"SFP",
+		"BP_AP",
+		"BP4_AP",
+	};
+
+	int i;
+	char buf[80];
+
+	for_each_port(adap, i) {
+		struct net_device *dev = adap->port[i];
+		const struct port_info *pi = netdev_priv(dev);
+		char *bufp = buf;
+
+		if (!test_bit(i, &adap->registered_device_map))
+			continue;
+
+		if (pi->link_cfg.supported & FW_PORT_CAP_SPEED_100M)
+			bufp += sprintf(bufp, "100/");
+		if (pi->link_cfg.supported & FW_PORT_CAP_SPEED_1G)
+			bufp += sprintf(bufp, "1000/");
+		if (pi->link_cfg.supported & FW_PORT_CAP_SPEED_10G)
+			bufp += sprintf(bufp, "10G/");
+		if (bufp != buf)
+			--bufp;
+		if (pi->port_type < ARRAY_SIZE(base))
+			sprintf(bufp, "BASE-%s", base[pi->port_type]);
+		else
+			sprintf(bufp, "BASE-UNKNOWN[%d]", pi->port_type);
+
+		printk(KERN_INFO "%s: Chelsio %s rev %d %s %sNIC PCIe x%d%s, %s capable\n",
+		       dev->name, adap->params.vpd.id, adap->params.rev,
+		       buf, is_offload(adap) ? "R" : "", adap->params.pci.width,
+		       (adap->flags & USING_MSIX) ? " MSI-X" :
+		       (adap->flags & USING_MSI) ? " MSI" : "",
+		       is_offload(adap) ? "Offload" : "non-Offload");
+
+		printk(KERN_INFO "%s: S/N: %s, P/N: %s\n", adap->name,
+		       adap->params.vpd.sn, adap->params.vpd.pn);
+	}
+}
+
+#ifdef CONFIG_PCI_IOV
+/**
+ *	vf_monitor - monitor VFs for potential problems
+ *	@work: the adapter's vf_monitor_task
+ *
+ *	VFs can get into trouble in various ways so we monitor them to see if
+ *	they need to be kicked, reset, etc.
+ */
+static void vf_monitor(struct work_struct *work)
+{
+	struct adapter *adapter = container_of(work, struct adapter,
+					       vf_monitor_task.work);
+	struct pci_dev *pdev;
+	u32 pcie_cdebug;
+	unsigned int reqfn;
+	const unsigned int vf_offset = 8;
+	const unsigned int vf_stride = 4;
+	unsigned int vfdevfn, pf, vf;
+	struct pci_dev *vfdev;
+	int pos, i;
+	u16 control;
+
+	/*
+	 * Read the PCI-E Debug Register to see if it's hanging with a
+	 * Request Valid condition.  But we check it several times to be
+	 * Absolutely Sure since we can see the PCI-E block being busy
+	 * transiently during normal operation.
+	 */
+	for (i = 0; i < 4; i++) {
+		t4_write_reg(adapter, A_PCIE_CDEBUG_INDEX, 0x3c003c);
+		pcie_cdebug = t4_read_reg(adapter, A_PCIE_CDEBUG_DATA_HIGH);
+		if ((pcie_cdebug & 0x100) == 0)
+			goto reschedule_vf_monitor;
+	}
+
+	/*
+	 * We're not prepared to deal with anything other than a VF.
+	 */
+	pdev = adapter->pdev;
+	reqfn = (pcie_cdebug >> 24) & 0xff;
+	if (reqfn < vf_offset) {
+		dev_info(&pdev->dev, "vf_monitor: hung ReqFn %d is a PF!\n",
+			 reqfn);
+		goto reschedule_vf_monitor;
+	}
+
+	/*
+	 * Grab a handle on the VF's PCI State.
+	 */
+	pf = (reqfn - vf_offset) & (vf_stride - 1);
+	vf = ((reqfn - vf_offset) & ~(vf_stride - 1))/vf_stride + 1;
+	vfdevfn = PCI_SLOT(pdev->devfn) + reqfn;
+	vfdev = pci_get_slot(pdev->bus, vfdevfn);
+	if (vfdev == NULL) {
+		dev_info(&pdev->dev, "vf_monitor: can't find PF%d/VF%d",
+			 pf, vf);
+		goto reschedule_vf_monitor;
+	}
+
+	/*
+	 * Now that we have a handle on the VF which is hung, we need to
+	 * mask and re-enable its interrupts, reset it and then disable its
+	 * interrupts again.
+	 */
+	pos = pci_find_capability(vfdev, PCI_CAP_ID_MSIX);
+	if (!pos) {
+		dev_err(&pdev->dev, "vf_monitor: can't find MSI-X PF%d/VF%d\n",
+			pf, vf);
+		goto drop_vfdev_reference;
+	}
+	pci_read_config_word(vfdev, pos+PCI_MSIX_FLAGS, &control);
+	if (control & PCI_MSIX_FLAGS_ENABLE) {
+		dev_info(&pdev->dev, "vf_monitor: MSI-X already enabled PF%d/VF%d\n",
+			 pf, vf);
+		goto drop_vfdev_reference;
+	}
+	pci_write_config_word(vfdev, pos+PCI_MSIX_FLAGS,
+			      control |
+			      PCI_MSIX_FLAGS_ENABLE |
+			      PCI_MSIX_FLAGS_MASKALL);
+	pci_reset_function(vfdev);
+	pci_write_config_word(vfdev, pos+PCI_MSIX_FLAGS, control);
+	dev_warn(&pdev->dev, "vf_monitor: reset hung PF%d/VF%d\n", pf, vf);
+
+drop_vfdev_reference:
+	/*
+	 * Drop reference to the VF's CI State.
+	 */
+	pci_dev_put(vfdev);
+
+reschedule_vf_monitor:
+	/*
+	 * Set up for the next time we need to check things ...
+	 */
+	schedule_delayed_work(&adapter->vf_monitor_task, VF_MONITOR_PERIOD);
+}
+#endif
+
+#ifdef HAVE_NET_DEVICE_OPS
+static const struct net_device_ops cxgb4_netdev_ops = {
+	.ndo_open             = cxgb_open,
+	.ndo_stop             = cxgb_close,
+	.ndo_start_xmit       = t4_eth_xmit,
+	.ndo_select_queue     = cxgb_select_queue,
+	.ndo_get_stats        = cxgb_get_stats,
+	.ndo_set_rx_mode      = cxgb_set_rxmode,
+	.ndo_set_mac_address  = cxgb_set_mac_addr,
+	.ndo_validate_addr    = eth_validate_addr,
+	.ndo_do_ioctl         = cxgb_ioctl,
+	.ndo_change_mtu       = cxgb_change_mtu,
+	.ndo_vlan_rx_register = vlan_rx_register,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller  = cxgb_netpoll,
+#endif
+};
+#endif
+
+#define VLAN_FEAT (NETIF_F_SG | NETIF_F_IP_CSUM | TSO_FLAGS | \
+		   NETIF_F_IPV6_CSUM | NETIF_F_HIGHDMA)
+
+static int __devinit init_one(struct pci_dev *pdev,
+			      const struct pci_device_id *ent)
+{
+	static int version_printed;
+
+	int func;
+	int i, err, pci_using_dac = 0;
+	struct adapter *adapter = NULL;
+	struct port_info *pi;
+
+	if (!version_printed) {
+		printk(KERN_INFO "%s - version %s\n", DRV_DESC, DRV_VERSION);
+		++version_printed;
+	}
+
+	err = pci_request_regions(pdev, KBUILD_MODNAME);
+	if (err) {
+		/* Just info, some other driver may have claimed the device. */
+		dev_info(&pdev->dev, "cannot obtain PCI resources\n");
+		return err;
+	}
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "cannot enable PCI device\n");
+		goto out_release_regions;
+	}
+
+	pci_enable_pcie_error_reporting(pdev);
+
+	if (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {
+		pci_using_dac = 1;
+		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));
+		if (err) {
+			dev_err(&pdev->dev, "unable to obtain 64-bit DMA for "
+				"coherent allocations\n");
+			goto out_disable_device;
+		}
+	} else if ((err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) != 0) {
+		dev_err(&pdev->dev, "no usable DMA configuration\n");
+		goto out_disable_device;
+	}
+
+	pci_set_master(pdev);
+
+	adapter = kzalloc(sizeof(*adapter), GFP_KERNEL);
+	if (!adapter) {
+		err = -ENOMEM;
+		goto out_disable_device;
+	}
+	adapter->regs = pci_ioremap_bar(pdev, 0);
+	if (!adapter->regs) {
+		dev_err(&pdev->dev, "cannot map device registers\n");
+		err = -ENOMEM;
+		goto out_free_adapter;
+	}
+
+	/*
+	 * We control everything via a single PF (which we refer to as the
+	 * "Master PF").  This Master PF is identifed with a special PCI
+	 * Device ID separate from the "normal" NIC Device IDs so for the most
+	 * part we just advertise that we want to be hooked up with the
+	 * Unified PF and everything works out.
+	 *
+	 * However, note that the "PE10K" FPGA is very annoying since both of
+	 * its two Physical Functions have the same Device ID so we need to
+	 * explcitly skip working with any PF other than Master PF, which we
+	 * hardwire to PF0.  This means that we have to undo all the I/O
+	 * mapping, etc.  once we get here and discover that we're actually
+	 * dealing with PF1.  Hopefully the next FPGA will use different PCI
+	 * Device IDs for each of the PFs.
+	 *
+	 * Note that we use the PL_WHOAMI register to figure out to which PF
+	 * we're actually attached rather than PCI_FUNC(pdev->devfn).  We do
+	 * this because we could be operating within a Virtual Machine where,
+	 * say, PF4 has been inserted via some form of "PCI Pass Through"
+	 * resulting in the VM PCI Device having a completely different PCI
+	 * Function Number, say, PF0.  However, there are many communications
+	 * with the firmware (and the hardware) where we need to use the
+	 * actual Physical Function Number and we can get this from the
+	 * PL_WHOAMI register ...  (We directly use readl() rather than
+	 * t4_read_reg() because the latter inline function could attempt to
+	 * access partially set up state in the adapter data structure.)
+	 */
+	func = G_SOURCEPF(readl(adapter->regs + A_PL_WHOAMI));
+	if (pdev->device == 0xa000 && func != 0) {
+		err = 0;
+		goto out_unmap_bar;
+	}
+
+#ifdef CHELSIO_T4_DIAGS
+	/*
+	 * The PCI Device ID Table includes PCI Device IDs both for PF4 and
+	 * for PF0 (which is the same ID used for PF1..3).  So we'll get
+	 * called for PF0..3 as well as PF4.  If the module parameter
+	 * "attach_pf0" is specified, then we want to continue forward only
+	 * with PF0 and ingnore the rest.  If attach_pf0 is not specified,
+	 * then we want to continue forward only with PF4.
+	 */
+	if ((attach_pf0 && func != 0) ||
+	    (!attach_pf0 && func != 4)) {
+		err = 0;
+		goto out_unmap_bar;
+	}
+
+	if (attach_pf0)
+		dev_info(&pdev->dev, "Attaching to pci func %d\n", func);
+#endif
+
+	adapter->pdev = pdev;
+	adapter->pdev_dev = &pdev->dev;
+	adapter->name = pci_name(pdev);
+	adapter->mbox = func;
+	adapter->pf = func;
+	adapter->msg_enable = dflt_msg_enable;
+	memset(adapter->chan_map, 0xff, sizeof(adapter->chan_map));
+	bitmap_zero(adapter->sge.blocked_fl, MAX_EGRQ);
+
+	spin_lock_init(&adapter->mdio_lock);
+	spin_lock_init(&adapter->win0_lock);
+	spin_lock_init(&adapter->work_lock);
+	spin_lock_init(&adapter->stats_lock);
+	spin_lock_init(&adapter->tid_release_lock);
+	t4_os_lock_init(&adapter->mbox_lock);
+	mutex_init(&adapter->user_mutex);
+	INIT_LIST_HEAD(&adapter->mbox_list.list);
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	INIT_WORK(&adapter->tid_release_task, process_tid_release_list);
+	INIT_WORK(&adapter->db_full_task, process_db_full);
+	INIT_WORK(&adapter->db_drop_task, process_db_drop);
+#endif
+
+	err = t4_prep_adapter(adapter, false);
+	if (err)
+		goto out_unmap_bar;
+	setup_memwin(adapter);
+	err = adap_init0(adapter);
+	setup_memwin_rdma(adapter);
+	if (err)
+		dev_err(&pdev->dev, "Adapter initialization failed, error %d.  "
+			"Continuing in debug mode\n", -err);
+
+	adapter->tx_coal = tx_coal;
+
+	for_each_port(adapter, i) {
+		struct net_device *netdev;
+
+		netdev = alloc_etherdev_mq(sizeof(struct port_info),
+					   MAX_ETH_QSETS);
+		if (!netdev) {
+			err = -ENOMEM;
+			goto out_free_dev;
+		}
+
+		SET_NETDEV_DEV(netdev, &pdev->dev);
+
+		adapter->port[i] = netdev;
+		pi = netdev_priv(netdev);
+		pi->adapter = adapter;
+		pi->xact_addr_filt = -1;
+		pi->rx_offload = RX_CSO;
+		pi->port_id = i;
+		netdev->irq = pdev->irq;
+
+		netdev->features |= NETIF_F_SG | TSO_FLAGS;
+		netdev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+		if (pci_using_dac)
+			netdev->features |= NETIF_F_HIGHDMA;
+
+#ifdef CONFIG_CXGB4_GRO
+		netdev->features |= NETIF_F_GRO;
+#endif
+		netdev->features |= NETIF_F_HW_VLAN_TX | NETIF_F_HW_VLAN_RX;
+		netdev->vlan_features = netdev->features & VLAN_FEAT;
+#ifdef HAVE_NET_DEVICE_OPS
+		netdev->netdev_ops = &cxgb4_netdev_ops;
+#else
+		netdev->vlan_rx_register = vlan_rx_register;
+
+		netdev->open = cxgb_open;
+		netdev->stop = cxgb_close;
+		netdev->hard_start_xmit = t4_eth_xmit;
+		netdev->get_stats = cxgb_get_stats;
+		netdev->select_queue = cxgb_select_queue;
+		netdev->set_rx_mode = cxgb_set_rxmode;
+		netdev->do_ioctl = cxgb_ioctl;
+		netdev->change_mtu = cxgb_change_mtu;
+		netdev->set_mac_address = cxgb_set_mac_addr;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+		netdev->poll_controller = cxgb_netpoll;
+#endif
+#endif
+		SET_ETHTOOL_OPS(netdev, &cxgb_ethtool_ops);
+	}
+
+	pci_set_drvdata(pdev, adapter);
+
+	if (adapter->flags & FW_OK) {
+		err = t4_port_init(adapter, adapter->mbox, adapter->pf, 0);
+		if (err)
+			goto out_free_dev;
+	} else if (adapter->params.nports == 1) {
+		/*
+		 * If we don't have a connection to the firmware -- either
+		 * because of an error of because fw_attach=0 was specified --
+		 * grab the raw VPD parameters so we can set the proper MAC
+		 * Address on the debug network interface that we've created.
+		 */
+		u8 hw_addr[ETH_ALEN];
+		u8 *na = adapter->params.vpd.na;
+
+		err = t4_get_raw_vpd_params(adapter, &adapter->params.vpd);
+		if (!err) {
+			for (i = 0; i < ETH_ALEN; i++)
+				hw_addr[i] = (hex2val(na[2*i + 0]) * 16 +
+					      hex2val(na[2*i + 1]));
+			t4_os_set_hw_addr(adapter, 0, hw_addr);
+		}
+	}
+
+	cfg_queues(adapter);  // XXX move after we know interrupt type
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	adapter->l2t = t4_init_l2t(adapter->l2t_start, adapter->l2t_end);
+	if (!adapter->l2t) {
+		/* We tolerate a lack of L2T, giving up some functionality */
+		dev_warn(&pdev->dev, "could not allocate L2T, continuing\n");
+		adapter->params.offload = 0;
+	}
+
+	if (is_offload(adapter) && tid_init(&adapter->tids) < 0) {
+		dev_warn(&pdev->dev, "could not allocate TID table, "
+			 "continuing\n");
+		adapter->params.offload = 0;
+	}
+
+	if (is_offload(adapter)) {
+		__set_bit(OFFLOAD_DEVMAP_BIT,
+			&adapter->registered_device_map);
+	}
+#endif /* CONFIG_CHELSIO_T4_OFFLOAD */
+
+#ifdef CONFIG_CHELSIO_BYPASS
+	/*
+	 * We need to call the Bypass Adapter's setup routine very early on in
+	 * order to set the current and failure modes correctly.  These will
+	 * be set to the failover mode of the previous incarnation of the
+	 * driver.  This early call also means that these are reported
+	 * correctly via the interface even though the interfaces haven't been
+	 * brought up yet.
+	 */
+	if (is_bypass(adapter))
+		t4_bypass_setup(adapter);
+#endif
+
+	/*
+	 * See what interrupts we'll be using.  Note that we need to enable
+	 * our interrupts before we register the network devices since certain
+	 * installations can have the network devices setup for automatic
+	 * configuration.  When that happens, we can get a Port Link Status
+	 * message from the firmware on our Asynchronous Firmware Event Queue
+	 * and end up losing the interrupt.
+	 */
+	if (msi > 1 && cxgb_enable_msix(adapter) == 0)
+		adapter->flags |= USING_MSIX;
+	else if (msi > 0 && pci_enable_msi(pdev) == 0)
+		adapter->flags |= USING_MSI;
+	if (adapter->flags & (USING_MSIX | USING_MSI))
+		check_msi(adapter);
+
+	/*
+	 * The card is now ready to go.  If any errors occur during device
+	 * registration we do not fail the whole card but rather proceed only
+	 * with the ports we manage to register successfully.  However we must
+	 * register at least one net device.
+	 */
+	for_each_port(adapter, i) {
+		pi = adap2pinfo(adapter, i);
+		adapter->port[i]->dev_id = pi->tx_chan;
+		adapter->port[i]->real_num_tx_queues = pi->nqsets;
+
+		err = register_netdev(adapter->port[i]);
+		if (err)
+			dev_warn(&pdev->dev,
+				 "cannot register net device %s, skipping\n",
+				 adapter->port[i]->name);
+		else {
+			/*
+			 * Change the name we use for messages to the name of
+			 * the first successfully registered interface.
+			 */
+			if (!adapter->registered_device_map)
+				adapter->name = adapter->port[i]->name;
+
+			__set_bit(i, &adapter->registered_device_map);
+			adapter->chan_map[pi->tx_chan] = i;
+		}
+	}
+	if (!adapter->registered_device_map) {
+		dev_err(&pdev->dev, "could not register any net devices\n");
+		goto out_disable_interrupts;
+	}
+
+	if (cxgb4_proc_root) {
+		adapter->proc_root = proc_mkdir(pci_name(pdev),
+						cxgb4_proc_root);
+		if (!adapter->proc_root)
+			dev_warn(&pdev->dev,
+				 "could not create /proc directory");
+		else
+			setup_proc(adapter, adapter->proc_root);
+	}
+
+	if (cxgb4_debugfs_root) {
+		adapter->debugfs_root = debugfs_create_dir(pci_name(pdev),
+							   cxgb4_debugfs_root);
+		setup_debugfs(adapter);
+	}
+
+	/*
+	 * Setup sysfs
+	 */
+	for_each_port(adapter, i)
+		if (sysfs_create_group(&adapter->port[i]->dev.kobj,
+				       &t4_attr_group))
+			dev_warn(&pdev->dev,
+				 "cannot create sysfs t4_attr_group net device "
+				 "%s\n", adapter->port[i]->name);
+
+#ifdef CONFIG_CHELSIO_BYPASS
+	if (is_bypass(adapter))
+		bypass_sysfs_create(adapter);
+#endif
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+	setup_offload(adapter);
+
+	if (is_offload(adapter)) {
+		attach_ulds(adapter);
+	}
+#endif
+
+	print_port_info(adapter);
+
+#ifdef CONFIG_PCI_IOV
+	/*
+	 * Loop accross PF0-3 to see if any VFs need to be instantiated.
+	 */
+	for (func = 0; func < ARRAY_SIZE(num_vf); func++) {
+		struct pci_dev *pf;
+
+		if (num_vf[func] <= 0)
+			continue;
+
+		pf = pci_get_slot(pdev->bus,
+				  PCI_DEVFN(PCI_SLOT(pdev->devfn),
+					    func));
+		if (pf == NULL) {
+			dev_warn(&pdev->dev, "failed to find PF%d; not"
+				 " enabling %d virtual functions\n",
+				 func, num_vf[func]);
+			continue;
+		}
+		err = pci_enable_sriov(pf, num_vf[func]);
+		if (err < 0)
+			dev_warn(&pf->dev, "failed to instantiate %d"
+				 " virtual functions; err=%d\n", num_vf[func],
+				 err);
+		else {
+			dev_info(&pf->dev,
+				 "instantiated %u virtual functions\n",
+				 num_vf[func]);
+			adapter->vf_monitor_mask |= 1U << func;
+		}
+		pci_dev_put(pf);
+	}
+
+	/*
+	 * If we instantiated any VFs, set up and start recurrant task to
+	 * monitor the state of the VFs.
+	 */
+	if (adapter->vf_monitor_mask) {
+		INIT_DELAYED_WORK(&adapter->vf_monitor_task, vf_monitor);
+		schedule_delayed_work(&adapter->vf_monitor_task,
+				      VF_MONITOR_PERIOD);
+	}
+#endif
+
+	return 0;
+
+	/*
+	 * Non-standard returns ...
+	 */
+ out_disable_interrupts:
+	cxgb_disable_msi(adapter);
+
+ out_free_dev:
+	t4_free_mem(adapter->l2t);
+	for_each_port(adapter, i)
+		if (adapter->port[i]) {
+			pi = netdev_priv(adapter->port[i]);
+			if (pi->viid != 0)
+				t4_free_vi(adapter, adapter->mbox, adapter->pf,
+					   0, pi->viid);
+			free_netdev(adapter->port[i]);
+		}
+	if (adapter->flags & FW_OK)
+		t4_fw_bye(adapter, adapter->mbox);
+ out_unmap_bar:
+	iounmap(adapter->regs);
+ out_free_adapter:
+	kfree(adapter);
+ out_disable_device:
+	pci_disable_device(pdev);
+ out_release_regions:
+	pci_release_regions(pdev);
+	pci_set_drvdata(pdev, NULL);
+	return err;
+}
+
+static void __devexit remove_one(struct pci_dev *pdev)
+{
+	struct adapter *adapter = pci_get_drvdata(pdev);
+
+#ifdef CONFIG_PCI_IOV
+	/*
+	 * Tear down VF Monitoring.
+	 * Note: Check to see if adapter is defined, because the not all
+	 * PFs go through the full init process (i.e. adapter is NULL).
+	 */
+	if (adapter && adapter->vf_monitor_mask)
+		cancel_delayed_work_sync(&adapter->vf_monitor_task);
+
+	/*
+	 * Loop accross PF0-3 to see if any VFs need to be uninstantiated.
+	 */
+	{
+		int func;
+
+		for (func = 0; func < 4; func++) {
+			struct pci_dev *pf;
+
+			if (num_vf[func] <= 0)
+				continue;
+
+			pf = pci_get_slot(pdev->bus,
+					  PCI_DEVFN(PCI_SLOT(pdev->devfn),
+						    func));
+			if (pf == NULL) {
+				dev_warn(&pdev->dev, "failed to find PF%d; not"
+					 " disabling %d virtual functions\n",
+					 func, num_vf[func]);
+				continue;
+			}
+			pci_disable_sriov(pf);
+			pci_dev_put(pf);
+		}
+	}
+#endif
+
+	if (adapter) {
+		int i;
+
+#ifdef CONFIG_CHELSIO_BYPASS
+		/*
+		 * We call the Bypass Adapter's shutdown logic here,
+		 * redundantly with same call in cxgb_down().  We do this
+		 * because the interface may never have been brought up but
+		 * the adapter's failover mode may have been set to a new
+		 * value ...
+		 */
+		if (is_bypass(adapter)) {
+			t4_bypass_shutdown(adapter);
+			bypass_sysfs_remove(adapter);
+		}
+#endif
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+		if (is_offload(adapter))
+			detach_ulds(adapter);
+#endif
+
+		for_each_port(adapter, i)
+			if (test_bit(i, &adapter->registered_device_map))
+				unregister_netdev(adapter->port[i]);
+
+		if (adapter->proc_root) {
+			cleanup_proc(adapter, adapter->proc_root);
+			remove_proc_entry(pci_name(pdev), cxgb4_proc_root);
+		}
+
+		/*
+		 * Remove sysfs group
+		 */
+		for_each_port(adapter, i)
+			sysfs_remove_group(&adapter->port[i]->dev.kobj,
+					   &t4_attr_group);
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+		/*
+		 * If we allocated filters, free up state associated with any
+		 * valid filters ...
+		 */
+		if (adapter->tids.ftid_tab) {
+			struct filter_entry *f = &adapter->tids.ftid_tab[0];
+			for (i = 0; i < (adapter->tids.nftids +
+					adapter->tids.nsftids); i++, f++)
+				if (f->valid)
+					clear_filter(adapter, f);
+		}
+#endif
+
+		if (adapter->flags & FULL_INIT_DONE)
+			cxgb_down(adapter);
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+		t4_free_mem(adapter->l2t);
+#endif
+		t4_free_mem(adapter->tids.tid_tab);
+		t4_free_mem(adapter->filters);
+		cxgb_disable_msi(adapter);
+
+		for_each_port(adapter, i)
+			if (adapter->port[i]) {
+				struct port_info *pi = adap2pinfo(adapter, i);
+				if (pi->viid != 0)
+					t4_free_vi(adapter, adapter->mbox,
+						   adapter->pf, 0, pi->viid);
+				free_netdev(adapter->port[i]);
+			}
+
+		if (adapter->flags & FW_OK)
+			t4_fw_bye(adapter, adapter->mbox);
+
+		if (adapter->debugfs_root) {
+			free_trace_bufs(adapter);
+#if DMABUF
+			dma_free_coherent(adapter->pdev_dev, DMABUF_SZ,
+					  adapter->dma_virt, adapter->dma_phys);
+#endif
+			debugfs_remove_recursive(adapter->debugfs_root);
+		}
+
+		iounmap(adapter->regs);
+		kfree(adapter);
+		pci_disable_device(pdev);
+		pci_release_regions(pdev);
+		pci_set_drvdata(pdev, NULL);
+	} else if (PCI_FUNC(pdev->devfn) > 0)
+		pci_release_regions(pdev);
+}
+
+static struct pci_driver cxgb4_driver = {
+	.name     = KBUILD_MODNAME,
+	.id_table = cxgb4_pci_tbl,
+	.probe    = init_one,
+	.remove   = __devexit_p(remove_one),
+};
+
+#define DRV_PROC_NAME "driver/" KBUILD_MODNAME
+
+static int __init cxgb4_init_module(void)
+{
+	int ret;
+
+	workq = create_singlethread_workqueue("cxgb4");
+	if (!workq)
+		return -ENOMEM;
+
+	/* Debugfs support is optional, just warn if this fails */
+	cxgb4_debugfs_root = debugfs_create_dir(KBUILD_MODNAME, NULL);
+	if (!cxgb4_debugfs_root)
+		pr_warning("could not create debugfs entry, continuing\n");
+
+#ifdef CONFIG_PROC_FS
+	cxgb4_proc_root = proc_mkdir(DRV_PROC_NAME, NULL);
+	if (!cxgb4_proc_root)
+		pr_warning("could not create /proc driver directory, "
+			   "continuing\n");
+#endif
+
+	ret = pci_register_driver(&cxgb4_driver);
+	if (ret < 0) {
+		remove_proc_entry(DRV_PROC_NAME, NULL);
+		debugfs_remove(cxgb4_debugfs_root);
+	}
+	return ret;
+}
+
+static void __exit cxgb4_cleanup_module(void)
+{
+	pci_unregister_driver(&cxgb4_driver);
+	remove_proc_entry(DRV_PROC_NAME, NULL);
+	debugfs_remove(cxgb4_debugfs_root);  /* NULL ok */
+	flush_workqueue(workq);
+	destroy_workqueue(workq);
+}
+
+module_init(cxgb4_init_module);
+module_exit(cxgb4_cleanup_module);
diff --git a/drivers/net/cxgb4/cxgb4_ofld.h b/drivers/net/cxgb4/cxgb4_ofld.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/cxgb4_ofld.h
@@ -0,0 +1,279 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2009-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __CXGB4_OFLD_H
+#define __CXGB4_OFLD_H
+
+#include <linux/cache.h>
+#include <linux/spinlock.h>
+#include <linux/skbuff.h>
+#include "l2t.h"
+#include <asm/atomic.h>
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD
+#include <net/offload.h>
+#endif
+
+/* CPL message priority levels */
+enum {
+	CPL_PRIORITY_DATA     = 0,  /* data messages */
+	CPL_PRIORITY_SETUP    = 1,  /* connection setup messages */
+	CPL_PRIORITY_TEARDOWN = 0,  /* connection teardown messages */
+	CPL_PRIORITY_LISTEN   = 1,  /* listen start/stop messages */
+	CPL_PRIORITY_ACK      = 1,  /* RX ACK messages */
+	CPL_PRIORITY_CONTROL  = 1   /* control messages */
+};
+
+#define INIT_TP_WR(w, tid) do { \
+	(w)->wr.wr_hi = htonl(V_FW_WR_OP(FW_TP_WR) | \
+			      V_FW_WR_IMMDLEN(sizeof(*w) - sizeof(w->wr))); \
+	(w)->wr.wr_mid = htonl(V_FW_WR_LEN16(DIV_ROUND_UP(sizeof(*w), 16)) | \
+			       V_FW_WR_FLOWID(tid)); \
+	(w)->wr.wr_lo = cpu_to_be64(0); \
+} while (0)
+
+#define INIT_TP_WR_MIT_CPL(w, cpl, tid) do { \
+	INIT_TP_WR(w, tid); \
+	OPCODE_TID(w) = htonl(MK_OPCODE_TID(cpl, tid)); \
+} while (0)
+
+#define INIT_ULPTX_WR(w, wrlen, atomic, tid) do { \
+	(w)->wr.wr_hi = htonl(V_FW_WR_OP(FW_ULPTX_WR) | V_FW_WR_ATOMIC(atomic)); \
+	(w)->wr.wr_mid = htonl(V_FW_WR_LEN16(DIV_ROUND_UP(wrlen, 16)) | \
+			       V_FW_WR_FLOWID(tid)); \
+	(w)->wr.wr_lo = cpu_to_be64(0); \
+} while (0)
+
+/* Special asynchronous notification message */
+#define CXGB4_MSG_AN ((void *)1)
+
+struct serv_entry {
+	void *data;
+};
+
+union aopen_entry {
+	void *data;
+	union aopen_entry *next;
+};
+
+/*
+ * Holds the size, base address, free list start, etc of the TID, server TID,
+ * and active-open TID tables.  The tables themselves are allocated dynamically.
+ */
+struct tid_info {
+	void **tid_tab;
+	unsigned int ntids;
+
+	struct serv_entry *stid_tab;
+	unsigned long *stid_bmap;
+	unsigned int nstids;
+	unsigned int stid_base;
+
+	union aopen_entry *atid_tab;
+	unsigned int natids;
+
+	struct filter_entry *ftid_tab;
+	unsigned int nftids;
+	unsigned int ftid_base;
+	unsigned int aftid_base;
+	unsigned int aftid_end;
+	/* Server filter region */
+	unsigned int sftid_base;
+	unsigned int nsftids;
+
+	/*
+	 * The following members are accessed R/W so we put them in their own
+	 * cache line.  STIDs are used sparingly, we let them share the line.
+	 */
+	spinlock_t atid_lock ____cacheline_aligned_in_smp;
+	union aopen_entry *afree;
+	unsigned int atids_in_use;
+
+	spinlock_t stid_lock;
+	unsigned int stids_in_use;
+
+	atomic_t tids_in_use;
+};
+
+static inline void *lookup_tid(const struct tid_info *t, unsigned int tid)
+{
+	return tid < t->ntids ? t->tid_tab[tid] : NULL;
+}
+
+static inline void *lookup_atid(const struct tid_info *t, unsigned int atid)
+{
+	return atid < t->natids ? t->atid_tab[atid].data : NULL;
+}
+
+static inline void *lookup_stid(const struct tid_info *t, unsigned int stid)
+{
+	stid -= t->stid_base;
+	return stid < (t->nstids + t->nsftids) ? t->stid_tab[stid].data : NULL;
+}
+
+static inline void cxgb4_insert_tid(struct tid_info *t, void *data,
+				    unsigned int tid)
+{
+	t->tid_tab[tid] = data;
+	atomic_inc(&t->tids_in_use);
+}
+
+int cxgb4_alloc_atid(struct tid_info *t, void *data);
+int cxgb4_alloc_stid(struct tid_info *t, int family, void *data);
+int cxgb4_alloc_sftid(struct tid_info *t, int family, void *data);
+void cxgb4_free_atid(struct tid_info *t, unsigned int atid);
+void cxgb4_free_stid(struct tid_info *t, unsigned int stid, int family);
+
+void *cxgb_alloc_mem(unsigned long size);
+void cxgb4_remove_tid(struct tid_info *t, unsigned int qid, unsigned int tid);
+
+struct in6_addr;
+
+int cxgb4_create_server(const struct net_device *dev, unsigned int stid,
+			__be32 sip, __be16 sport, unsigned int queue);
+int cxgb4_create_server_filter(const struct net_device *dev, unsigned int stid,
+			__be32 sip, __be16 sport, unsigned int queue);
+int cxgb4_create_server6(const struct net_device *dev, unsigned int stid,
+			 const struct in6_addr *sip, __be16 sport,
+			 unsigned int queue);
+int cxgb4_remove_server(const struct net_device *dev, unsigned int stid,
+			unsigned int queue, bool ipv6);
+int cxgb4_remove_server_filter(const struct net_device *dev, unsigned int stid,
+			unsigned int queue, bool ipv6);
+int cxgb4_clip_get(const struct net_device *dev, const struct in6_addr *lip);
+int cxgb4_clip_release(const struct net_device *dev, const struct in6_addr *lip);
+
+static inline void set_wr_txq(struct sk_buff *skb, int prio, int queue)
+{
+	skb_set_queue_mapping(skb, (queue << 1) | prio);
+}
+
+enum cxgb4_uld {
+	CXGB4_ULD_RDMA,
+	CXGB4_ULD_ISCSI,
+	CXGB4_ULD_TOE,
+	CXGB4_ULD_MAX
+};
+
+enum cxgb4_state {
+	CXGB4_STATE_UP,
+	CXGB4_STATE_START_RECOVERY,
+	CXGB4_STATE_DOWN,
+	CXGB4_STATE_DETACH
+};
+
+enum cxgb4_control {
+	CXGB4_CONTROL_SET_OFFLOAD_POLICY,
+	CXGB4_CONTROL_DB_FULL,
+	CXGB4_CONTROL_DB_EMPTY,
+	CXGB4_CONTROL_DB_DROP,
+};
+
+struct pci_dev;
+struct l2t_data;
+struct net_device;
+struct pkt_gl;
+
+struct cxgb4_range {
+	unsigned int start;
+	unsigned int size;
+};
+
+struct cxgb4_virt_res {                      /* virtualized HW resources */
+	struct cxgb4_range ddp;
+	struct cxgb4_range iscsi;
+	struct cxgb4_range stag;
+	struct cxgb4_range rq;
+	struct cxgb4_range pbl;
+	struct cxgb4_range qp;
+	struct cxgb4_range cq;
+	struct cxgb4_range ocq;
+};
+
+#define OCQ_WIN_OFFSET(pdev, vres) \
+	(pci_resource_len((pdev), 2) - roundup_pow_of_two((vres)->ocq.size))
+
+/*
+ * Block of information the LLD provides to ULDs attaching to a device.
+ */
+struct cxgb4_lld_info {
+	struct pci_dev *pdev;                /* associated PCI device */
+	struct l2t_data *l2t;                /* L2 table */
+	struct tid_info *tids;               /* TID table */
+	struct net_device **ports;           /* device ports */
+	const struct cxgb4_virt_res *vr;     /* assorted HW resources */
+	const unsigned short *mtus;          /* MTU table */
+	const unsigned short *rxq_ids;       /* the ULD's Rx queue ids */
+	unsigned short nrxq;                 /* # of Rx queues */
+	unsigned short ntxq;                 /* # of Tx queues */
+	unsigned char nchan:4;               /* # of channels */
+	unsigned char nports:4;              /* # of ports */
+	unsigned char wr_cred;               /* WR 16-byte credits */
+	unsigned char adapter_type;          /* type of adapter */
+	unsigned char fw_api_ver;            /* FW API version */
+	unsigned int fw_vers;                /* FW version */
+	unsigned int iscsi_iolen;            /* iSCSI max I/O length */
+	unsigned short udb_density;          /* # of user DB/page */
+	unsigned short ucq_density;          /* # of user CQs/page */
+	unsigned short filt_mode;            /* filter optional components */
+	unsigned short tx_modq[NCHAN]; 	     /* maps each tx channel to a scheduler queue */
+	void __iomem *gts_reg;               /* address of GTS register */
+	void __iomem *db_reg;                /* address of kernel doorbell */
+	int dbfifo_int_thresh;		     /* doorbell fifo int threshold */
+	unsigned int sge_ingpadboundary;     /* SGE ingress padding boundary */
+	unsigned int sge_pktshift;   	     /* Padding between CPL and packet Data */
+	unsigned int sge_egrstatuspagesize;  /* SGE egress status page size */
+	unsigned int pf;                     /* Physical Function we're using */
+	bool enable_fw_ofld_conn;	     /* Enable connection through fw WR */
+};
+
+struct cxgb4_uld_info {
+	const char *name;
+	void *(*add)(const struct cxgb4_lld_info *p);
+	int (*rx_handler)(void *handle, const __be64 *rsp,
+			  const struct pkt_gl *gl);
+	int (*state_change)(void *handle, enum cxgb4_state new_state);
+	int (*control)(void *handle, enum cxgb4_control control, ...);
+};
+
+int cxgb4_register_uld(enum cxgb4_uld type, const struct cxgb4_uld_info *p);
+int cxgb4_unregister_uld(enum cxgb4_uld type);
+int cxgb4_ofld_send(struct net_device *dev, struct sk_buff *skb);
+unsigned int cxgb4_dbfifo_count(const struct net_device *dev, int lpfifo);
+unsigned int cxgb4_port_chan(const struct net_device *dev);
+unsigned int cxgb4_port_viid(const struct net_device *dev);
+unsigned int cxgb4_port_idx(const struct net_device *dev);
+struct net_device *cxgb4_netdev_by_hwid(struct pci_dev *pdev, unsigned int id);
+struct net_device *cxgb4_root_dev(struct net_device *dev, int vlan);
+unsigned int cxgb4_best_mtu(const unsigned short *mtus, unsigned short mtu,
+			    unsigned int *idx);
+void cxgb4_get_tcp_stats(struct pci_dev *pdev, struct tp_tcp_stats *v4,
+                        struct tp_tcp_stats *v6);
+void cxgb4_iscsi_init(struct net_device *dev, unsigned int tag_mask,
+		      const unsigned int *pgsz_order);
+int cxgb4_setup_ddpbuf(struct pci_dev *pdev, const dma_addr_t *bus_addr,
+		       unsigned int naddr, unsigned int tid, unsigned int tag,
+		       unsigned int len, unsigned int pg_ofst,
+		       unsigned int color);
+int cxgb4_setup_iscsi_pagepod(struct pci_dev *pdev, void *ppod_hdr,
+			dma_addr_t *bus_addr, unsigned int naddr,
+			unsigned int idx, unsigned int max);
+int cxgb4_clear_iscsi_pagepod(struct pci_dev *pdev, unsigned int idx,
+			unsigned int max);
+int cxgb4_wr_mbox(struct net_device *dev, const void *cmd, int size, void *rpl);
+int cxgb4_sync_txq_pidx(struct net_device *dev, u16 qid, u16 pidx, u16 size);
+int cxgb4_flush_eq_cache(struct net_device *dev);
+void cxgb4_disable_db_coalescing(struct net_device *dev);
+void cxgb4_enable_db_coalescing(struct net_device *dev);
+
+#define TOM_DATA(dev) (*(struct tom_data **)&(dev)->l4opt)
+
+#endif  /* !__CXGB4_OFLD_H */
diff --git a/drivers/net/cxgb4/cxgbtool.h b/drivers/net/cxgb4/cxgbtool.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/cxgbtool.h
@@ -0,0 +1,438 @@
+/*
+ * This file is part of the Chelsio NIC management interface.
+ *
+ * Copyright (C) 2003-2011 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+#ifndef __CXGBTOOL_H__
+#define __CXGBTOOL_H__
+
+#define SIOCCHIOCTL SIOCDEVPRIVATE
+
+enum {
+	CHELSIO_SETREG			= 1024,
+	CHELSIO_GETREG			= 1025,
+	CHELSIO_SETTPI                  = 1026,
+	CHELSIO_GETTPI                  = 1027,
+	CHELSIO_DEVUP			= 1028,
+	CHELSIO_GETMTUTAB		= 1029,
+	CHELSIO_SETMTUTAB		= 1030,
+	CHELSIO_GETMTU			= 1031,
+	CHELSIO_SET_PM			= 1032,
+	CHELSIO_GET_PM			= 1033,
+	CHELSIO_SET_TCAM		= 1035,
+	CHELSIO_READ_TCAM_WORD		= 1037,
+	CHELSIO_GET_MEM			= 1038,
+	CHELSIO_GET_SGE_CONTEXT		= 1039,
+	CHELSIO_GET_SGE_DESC		= 1040,
+	CHELSIO_LOAD_FW			= 1041,
+	CHELSIO_SET_TRACE_FILTER        = 1044,
+	CHELSIO_SET_QSET_PARAMS		= 1045,
+	CHELSIO_GET_QSET_PARAMS		= 1046,
+	CHELSIO_SET_QSET_NUM		= 1047,
+	CHELSIO_GET_QSET_NUM		= 1048,
+	CHELSIO_SET_PKTSCHED		= 1049,
+	CHELSIO_SET_HW_SCHED		= 1051,
+	CHELSIO_LOAD_BOOT		= 1054,
+	CHELSIO_CLEAR_STATS             = 1055,
+	CHELSIO_GET_UP_LA		= 1056,
+	CHELSIO_GET_UP_IOQS		= 1057,
+	CHELSIO_GET_TRACE_FILTER	= 1058,
+	CHELSIO_GET_SGE_CTXT            = 1059,
+	CHELSIO_GET_SGE_DESC2		= 1060,
+
+	CHELSIO_SET_OFLD_POLICY         = 1062,
+
+	CHELSIO_SET_FILTER		= 1063,
+	CHELSIO_DEL_FILTER		= 1064,
+	CHELSIO_GET_PKTSCHED            = 1065,
+	CHELSIO_LOAD_CFG                = 1066,
+	CHELSIO_REG_DUMP		= 1067,
+	CHELSIO_GET_FILTER_COUNT	= 1068,
+	CHELSIO_GET_BYPASS_PORTS	= 1069,
+
+#if 0 /* Unsupported */
+	CHELSIO_SETTPI			= 1026,
+	CHELSIO_GETTPI			= 1027,
+	CHELSIO_GET_TCAM		= 1034,
+	CHELSIO_GET_TCB			= 1036,
+	CHELSIO_GET_PROTO		= 1042,
+	CHELSIO_SET_PROTO		= 1043,
+#endif
+};
+
+/* statistics categories */
+enum {
+	STATS_PORT  = 1 << 1,
+	STATS_QUEUE = 1 << 2,
+};
+
+/* queue types for "qdesc" command */
+enum {
+	SGE_QTYPE_TX_ETH = 1,
+	SGE_QTYPE_TX_OFLD,
+	SGE_QTYPE_TX_CTRL,
+	SGE_QTYPE_FL,
+	SGE_QTYPE_RSP,
+};
+
+struct ch_reg {
+	uint32_t cmd;
+	uint32_t addr;
+	uint32_t val;
+};
+
+struct ch_cntxt {
+	uint32_t cmd;
+	uint32_t cntxt_type;
+	uint32_t cntxt_id;
+	uint32_t data[4];
+};
+
+/* context types */
+enum {
+	CNTXT_TYPE_EGRESS,
+	CNTXT_TYPE_FL,
+	CNTXT_TYPE_RSP,
+	CNTXT_TYPE_CQ,
+	CNTXT_TYPE_CONG
+};
+
+struct ch_desc {
+	uint32_t cmd;
+	uint32_t queue_num;
+	uint32_t idx;
+	uint32_t size;
+	uint8_t  data[128];
+};
+
+struct ch_mem_range {
+	uint32_t cmd;
+	uint32_t mem_id;
+	uint32_t addr;
+	uint32_t len;
+	uint32_t version;
+	uint8_t  buf[0];
+};
+
+struct struct_load_cfg {
+	uint32_t cmd;
+	uint32_t len;
+	uint8_t  buf[0];
+};
+
+/* ch_mem_range.mem_id values */
+enum {
+	MEM_CM,
+	MEM_PMRX,
+	MEM_PMTX,
+	MEM_FLASH
+};
+
+struct ch_qset_params {
+	uint32_t cmd;
+	uint32_t qset_idx;
+	int32_t  txq_size[3];
+	int32_t  rspq_size;
+	int32_t  fl_size[2];
+	int32_t  intr_lat;
+	int32_t  polling;
+	int32_t  lro;
+	int32_t  cong_thres;
+	int32_t  vector;
+	int32_t  qnum;
+};
+
+struct ch_pktsched_params {
+	uint32_t cmd;
+	uint8_t  sched;
+	uint8_t  idx;
+	uint8_t  min;
+	uint8_t  max;
+	uint8_t  binding;
+};
+
+enum {
+	PKTSCHED_PORT = 0,
+	PKTSCHED_TUNNELQ = 1,
+};
+
+struct ch_hw_sched {
+	uint32_t cmd;
+	uint8_t  sched;
+	int8_t   mode;
+	int8_t   channel;
+	int8_t   weight;
+	int32_t  kbps;
+	int32_t  class_ipg;
+	int32_t  flow_ipg;
+};
+
+/*
+ * Defined bit width of user definable filter tuples
+ */
+#define ETHTYPE_BITWIDTH 16
+#define FRAG_BITWIDTH 1
+#define MACIDX_BITWIDTH 9
+#define FCOE_BITWIDTH 1
+#define IPORT_BITWIDTH 3
+#define MATCHTYPE_BITWIDTH 3
+#define PROTO_BITWIDTH 8
+#define TOS_BITWIDTH 8
+#define PF_BITWIDTH 8
+#define VF_BITWIDTH 8
+#define IVLAN_BITWIDTH 16
+#define OVLAN_BITWIDTH 16
+
+/*
+ * Filter matching rules.  These consist of a set of ingress packet field
+ * (value, mask) tuples.  The associated ingress packet field matches the
+ * tuple when ((field & mask) == value).  (Thus a wildcard "don't care" field
+ * rule can be constructed by specifying a tuple of (0, 0).)  A filter rule
+ * matches an ingress packet when all of the individual individual field
+ * matching rules are true.
+ *
+ * Partial field masks are always valid, however, while it may be easy to
+ * understand their meanings for some fields (e.g. IP address to match a
+ * subnet), for others making sensible partial masks is less intuitive (e.g.
+ * MPS match type) ...
+ *
+ * Most of the following data structures are modeled on T4 capabilities.
+ * Drivers for earlier chips use the subsets which make sense for those chips.
+ * We really need to come up with a hardware-independent mechanism to
+ * represent hardware filter capabilities ...
+ */
+struct ch_filter_tuple {
+	/*
+	 * Compressed header matching field rules.  The TP_VLAN_PRI_MAP
+	 * register selects which of these fields will participate in the
+	 * filter match rules -- up to a maximum of 36 bits.  Because
+	 * TP_VLAN_PRI_MAP is a global register, all filters must use the same
+	 * set of fields.
+	 */
+	uint32_t ethtype:ETHTYPE_BITWIDTH;	/* Ethernet type */
+	uint32_t frag:FRAG_BITWIDTH;		/* IP fragmentation header */
+	uint32_t ivlan_vld:1;			/* inner VLAN valid */
+	uint32_t ovlan_vld:1;			/* outer VLAN valid */
+	uint32_t pfvf_vld:1;			/* PF/VF valid */
+	uint32_t macidx:MACIDX_BITWIDTH;	/* exact match MAC index */
+	uint32_t fcoe:FCOE_BITWIDTH;		/* FCoE packet */
+	uint32_t iport:IPORT_BITWIDTH;		/* ingress port */
+	uint32_t matchtype:MATCHTYPE_BITWIDTH;	/* MPS match type */
+	uint32_t proto:PROTO_BITWIDTH;		/* protocol type */
+	uint32_t tos:TOS_BITWIDTH;		/* TOS/Traffic Type */
+	uint32_t pf:PF_BITWIDTH;		/* PCI-E PF ID */
+	uint32_t vf:VF_BITWIDTH;		/* PCI-E VF ID */
+	uint32_t ivlan:IVLAN_BITWIDTH;		/* inner VLAN */
+	uint32_t ovlan:OVLAN_BITWIDTH;		/* outer VLAN */
+
+	/*
+	 * Uncompressed header matching field rules.  These are always
+	 * available for field rules.
+	 */
+	uint8_t lip[16];	/* local IP address (IPv4 in [3:0]) */
+	uint8_t fip[16];	/* foreign IP address (IPv4 in [3:0]) */
+	uint16_t lport;		/* local port */
+	uint16_t fport;		/* foreign port */
+};
+
+/*
+ * A filter ioctl command.
+ */
+struct ch_filter_specification {
+	/*
+	 * Administrative fields for filter.
+	 */
+	uint32_t hitcnts:1;	/* count filter hits in TCB */
+	uint32_t prio:1;	/* filter has priority over active/server */
+
+	/*
+	 * Fundamental filter typing.  This is the one element of filter
+	 * matching that doesn't exist as a (value, mask) tuple.
+	 */
+	uint32_t type:1;	/* 0 => IPv4, 1 => IPv6 */
+
+	/*
+	 * Packet dispatch information.  Ingress packets which match the
+	 * filter rules will be dropped, passed to the host or switched back
+	 * out as egress packets.
+	 */
+	uint32_t action:2;	/* drop, pass, switch */
+
+	uint32_t rpttid:1;	/* report TID in RSS hash field */
+
+	uint32_t dirsteer:1;	/* 0 => RSS, 1 => steer to iq */
+	uint32_t iq:10;		/* ingress queue */
+
+	uint32_t maskhash:1;	/* dirsteer=0: store RSS hash in TCB */
+	uint32_t dirsteerhash:1;/* dirsteer=1: 0 => TCB contains RSS hash */
+				/*             1 => TCB contains IQ ID */
+
+	/*
+	 * Switch proxy/rewrite fields.  An ingress packet which matches a
+	 * filter with "switch" set will be looped back out as an egress
+	 * packet -- potentially with some Ethernet header rewriting.
+	 */
+	uint32_t eport:2;	/* egress port to switch packet out */
+	uint32_t newdmac:1;	/* rewrite destination MAC address */
+	uint32_t newsmac:1;	/* rewrite source MAC address */
+	uint32_t newvlan:2;	/* rewrite VLAN Tag */
+	uint8_t dmac[ETH_ALEN];	/* new destination MAC address */
+	uint8_t smac[ETH_ALEN];	/* new source MAC address */
+	uint16_t vlan;		/* VLAN Tag to insert */
+
+	/*
+	 * Filter rule value/mask pairs.
+	 */
+	struct ch_filter_tuple val;
+	struct ch_filter_tuple mask;
+};
+
+enum {
+	FILTER_PASS = 0,	/* default */
+	FILTER_DROP,
+	FILTER_SWITCH
+};
+
+enum {
+	VLAN_NOCHANGE = 0,	/* default */
+	VLAN_REMOVE,
+	VLAN_INSERT,
+	VLAN_REWRITE
+};
+
+enum {                         /* Ethernet address match types */
+	UCAST_EXACT = 0,       /* exact unicast match */
+	UCAST_HASH  = 1,       /* inexact (hashed) unicast match */
+	MCAST_EXACT = 2,       /* exact multicast match */
+	MCAST_HASH  = 3,       /* inexact (hashed) multicast match */
+	PROMISC     = 4,       /* no match but port is promiscuous */
+	HYPPROMISC  = 5,       /* port is hypervisor-promisuous + not bcast */
+	BCAST       = 6,       /* broadcast packet */
+};
+
+enum {                         /* selection of Rx queue for accepted packets */
+	DST_MODE_QUEUE,        /* queue is directly specified by filter */
+	DST_MODE_RSS_QUEUE,    /* filter specifies RSS entry containing queue */
+	DST_MODE_RSS,          /* queue selected by default RSS hash lookup */
+	DST_MODE_FILT_RSS      /* queue selected by hashing in filter-specified
+				  RSS subtable */
+};
+
+struct ch_filter {
+	uint32_t cmd;		/* common "cxgbtool" command header */
+	uint32_t filter_id;	/* the filter index to set */
+	struct ch_filter_specification fs;
+};
+
+struct ch_filter_count {
+	uint32_t cmd;		/* common "cxgbtool" command header */
+	uint32_t filter_id;	/* the filter index to retrieve count */
+	uint64_t pkt_count;	/* number of packets that matched filter */
+};
+
+#define	MAX_BA_IFS	8
+
+struct ch_bypass_ports {
+	uint32_t cmd;			/* common "cxgbtool" command header */
+	char port_count;		/* number of ports on adapter */
+	struct ba_if {
+		char if_name[16];	/* port name, e.g. "eth0" */
+	} ba_if[MAX_BA_IFS];
+};
+
+#define MAX_NMTUS 16
+
+struct ch_mtus {
+	uint32_t cmd;
+	uint32_t nmtus;
+	uint16_t mtus[MAX_NMTUS];
+};
+
+struct ch_pm {
+	uint32_t cmd;
+	uint32_t tx_pg_sz;
+	uint32_t tx_num_pg;
+	uint32_t rx_pg_sz;
+	uint32_t rx_num_pg;
+	uint32_t pm_total;
+};
+
+struct ch_tcam {
+	uint32_t cmd;
+	uint32_t tcam_size;
+	uint32_t nservers;
+	uint32_t nroutes;
+	uint32_t nfilters;
+};
+
+#define TCB_SIZE 128
+#define TCB_WORDS (TCB_SIZE / 4)
+
+struct ch_tcb {
+	uint32_t cmd;
+	uint32_t tcb_index;
+	uint32_t tcb_data[TCB_WORDS];
+};
+
+struct ch_tcam_word {
+	uint32_t cmd;
+	uint32_t addr;
+	uint32_t buf[3];
+};
+
+struct ch_trace {
+	uint32_t cmd;
+	uint32_t sip;
+	uint32_t sip_mask;
+	uint32_t dip;
+	uint32_t dip_mask;
+	uint16_t sport;
+	uint16_t sport_mask;
+	uint16_t dport;
+	uint16_t dport_mask;
+	uint32_t vlan:12;
+	uint32_t vlan_mask:12;
+	uint32_t intf:4;
+	uint32_t intf_mask:4;
+	uint8_t  proto;
+	uint8_t  proto_mask;
+	uint8_t  invert_match:1;
+	uint8_t  config_tx:1;
+	uint8_t  config_rx:1;
+	uint8_t  trace_tx:1;
+	uint8_t  trace_rx:1;
+};
+
+struct ch_up_la {
+	uint32_t cmd;
+	uint32_t stopped;
+	uint32_t idx;
+	uint32_t bufsize;
+	uint32_t la[0];
+};
+
+struct ioq_entry {
+	uint32_t ioq_cp;
+	uint32_t ioq_pp;
+	uint32_t ioq_alen;
+	uint32_t ioq_stats;
+};
+
+struct ch_up_ioqs {
+	uint32_t cmd;
+
+	uint32_t ioq_rx_enable;
+	uint32_t ioq_tx_enable;
+	uint32_t ioq_rx_status;
+	uint32_t ioq_tx_status;
+
+	uint32_t bufsize;
+	struct ioq_entry ioqs[0];
+};
+
+#endif /* __CXGBTOOL_H__ */
diff --git a/drivers/net/cxgb4/l2t.c b/drivers/net/cxgb4/l2t.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/l2t.c
@@ -0,0 +1,813 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * Written by Dimitris Michailidis (dm@chelsio.com)
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/if.h>
+#include <linux/if_vlan.h>
+#include <linux/jhash.h>
+#include <net/neighbour.h>
+#include "common.h"
+#include "l2t.h"
+#include "t4_msg.h"
+#include "t4fw_interface.h"
+#include "cxgb4_ofld.h"
+
+#define VLAN_NONE 0xfff
+
+/* identifies sync vs async L2T_WRITE_REQs */
+#define S_SYNC_WR    12
+#define V_SYNC_WR(x) ((x) << S_SYNC_WR)
+#define F_SYNC_WR    V_SYNC_WR(1)
+
+enum {
+	L2T_STATE_VALID,      /* entry is up to date */
+	L2T_STATE_STALE,      /* entry may be used but needs revalidation */
+	L2T_STATE_RESOLVING,  /* entry needs address resolution */
+	L2T_STATE_SYNC_WRITE, /* synchronous write of entry underway */
+
+	/* when state is one of the below the entry is not hashed */
+	L2T_STATE_SWITCHING,  /* entry is being used by a switching filter */
+	L2T_STATE_UNUSED      /* entry not in use */
+};
+
+struct l2t_data {
+	unsigned int l2t_start;     /* start index of our piece of the L2T */
+	unsigned int l2t_size;      /* number of entries in l2tab */
+	rwlock_t lock;
+	atomic_t nfree;             /* number of free entries */
+	struct l2t_entry *rover;    /* starting point for next allocation */
+	struct l2t_entry l2tab[0];  /* MUST BE LAST */
+};
+
+/*
+ * Module locking notes:  There is a RW lock protecting the L2 table as a
+ * whole plus a spinlock per L2T entry.  Entry lookups and allocations happen
+ * under the protection of the table lock, individual entry changes happen
+ * while holding that entry's spinlock.  The table lock nests outside the
+ * entry locks.  Allocations of new entries take the table lock as writers so
+ * no other lookups can happen while allocating new entries.  Entry updates
+ * take the table lock as readers so multiple entries can be updated in
+ * parallel.  An L2T entry can be dropped by decrementing its reference count
+ * and therefore can happen in parallel with entry allocation but no entry
+ * can change state or increment its ref count during allocation as both of
+ * these perform lookups.
+ *
+ * Note: We do not take refereces to net_devices in this module because both
+ * the TOE and the sockets already hold references to the interfaces and the
+ * lifetime of an L2T entry is fully contained in the lifetime of the TOE.
+ */
+
+static inline unsigned int vlan_prio(const struct l2t_entry *e)
+{
+	return e->vlan >> 13;
+}
+
+static inline void l2t_hold(struct l2t_data *d, struct l2t_entry *e)
+{
+	if (atomic_add_return(1, &e->refcnt) == 1)  /* 0 -> 1 transition */
+		atomic_dec(&d->nfree);
+}
+
+/*
+ * To avoid having to check address families we do not allow v4 and v6
+ * neighbors to be on the same hash chain.  We keep v4 entries in the first
+ * half of available hash buckets and v6 in the second.  We need at least two
+ * entries in our L2T for this scheme to work.
+ */
+enum {
+	L2T_MIN_HASH_BUCKETS = 2,
+};
+
+static inline unsigned int arp_hash(struct l2t_data *d, const u32 *key,
+				    int ifindex)
+{
+	unsigned int l2t_size_half = d->l2t_size / 2;
+
+	return jhash_2words(*key, ifindex, 0) % l2t_size_half;
+}
+
+static inline unsigned int ipv6_hash(struct l2t_data *d,const u32 *key,
+				     int ifindex)
+{
+	unsigned int l2t_size_half = d->l2t_size / 2;
+	u32 xor = key[0] ^ key[1] ^ key[2] ^ key[3];
+
+	return (l2t_size_half +
+		(jhash_2words(xor, ifindex, 0) % l2t_size_half));
+}
+
+static unsigned int addr_hash(struct l2t_data *d, const u32 *addr,
+			      int addr_len, int ifindex)
+{
+	return addr_len == 4 ? arp_hash(d, addr, ifindex) :
+			       ipv6_hash(d, addr, ifindex);
+}
+
+
+static inline unsigned short vlan_dev_get_egress_pri_mask(struct net_device* dev,
+                                                           u32 priority)
+{
+        struct vlan_priority_tci_mapping *mp = 
+		vlan_dev_info(dev)->egress_priority_map[(priority & 0xF)];
+ 
+        while (mp) {
+		if (mp->priority == priority) {
+			return mp->vlan_qos; 
+		}	
+		mp = mp->next;
+	}
+	return 0;
+}
+
+/*
+ * Checks if an L2T entry is for the given IP/IPv6 address.  It does not check
+ * whether the L2T entry and the address are of the same address family.
+ * Callers ensure an address is only checked against L2T entries of the same
+ * family, something made trivial by the separation of IP and IPv6 hash chains
+ * mentioned above.  Returns 0 if there's a match,
+ */
+static int addreq(const struct l2t_entry *e, const u32 *addr)
+{
+	if (e->v6)
+		return (e->addr[0] ^ addr[0]) | (e->addr[1] ^ addr[1]) |
+		       (e->addr[2] ^ addr[2]) | (e->addr[3] ^ addr[3]);
+	return e->addr[0] ^ addr[0];
+}
+
+static void neigh_replace(struct l2t_entry *e, struct neighbour *n)
+{
+	neigh_hold(n);
+	if (e->neigh)
+		neigh_release(e->neigh);
+	e->neigh = n;
+}
+
+/*
+ * Write an L2T entry.  Must be called with the entry locked.
+ * The write may be synchronous or asynchronous.
+ */
+static int write_l2e(struct adapter *adap, struct l2t_entry *e, int sync)
+{
+	struct l2t_data *d = adap->l2t;
+	unsigned int l2t_idx = e->idx + d->l2t_start;
+	struct sk_buff *skb;
+	struct cpl_l2t_write_req *req;
+
+	skb = alloc_skb(sizeof(*req), GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	req = (struct cpl_l2t_write_req *)__skb_put(skb, sizeof(*req));
+	INIT_TP_WR(req, 0);
+
+	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_L2T_WRITE_REQ,
+					l2t_idx | V_SYNC_WR(sync) |
+					V_TID_QID(adap->sge.fw_evtq.abs_id)));
+	req->params = htons(V_L2T_W_PORT(e->lport) | V_L2T_W_NOREPLY(!sync));
+	req->l2t_idx = htons(l2t_idx);
+	req->vlan = htons(e->vlan);
+	if (e->neigh && !(e->neigh->dev->flags & IFF_LOOPBACK))
+		memcpy(e->dmac, e->neigh->ha, sizeof(e->dmac));
+	memcpy(req->dst_mac, e->dmac, sizeof(req->dst_mac));
+
+	set_wr_txq(skb, CPL_PRIORITY_CONTROL, 0);
+	t4_ofld_send(adap, skb);
+
+	if (sync && e->state != L2T_STATE_SWITCHING)
+		e->state = L2T_STATE_SYNC_WRITE;
+
+	return 0;
+}
+
+/*
+ * Send packets waiting in an L2T entry's ARP queue.  Must be called with the
+ * entry locked.
+ */
+static void send_pending(struct adapter *adap, struct l2t_entry *e)
+{
+	while (e->arpq_head) {
+		struct sk_buff *skb = e->arpq_head;
+
+		e->arpq_head = skb->next;
+		skb->next = NULL;
+		t4_ofld_send(adap, skb);
+	}
+	e->arpq_tail = NULL;
+}
+
+/*
+ * Process a CPL_L2T_WRITE_RPL.  Wake up the ARP queue if it completes a
+ * synchronous L2T_WRITE.  Note that the TID in the reply is really the L2T
+ * index it refers to.
+ */
+void do_l2t_write_rpl(struct adapter *adap, const struct cpl_l2t_write_rpl *rpl)
+{
+	struct l2t_data *d = adap->l2t;
+	unsigned int tid = GET_TID(rpl);
+	unsigned int l2t_idx = tid % L2T_SIZE;
+
+	if (unlikely(rpl->status != CPL_ERR_NONE)) {
+		CH_ERR(adap,
+		       "Unexpected L2T_WRITE_RPL status %u for entry %u\n",
+		       rpl->status, l2t_idx);
+		return;
+	}
+
+	if (tid & F_SYNC_WR) {
+		struct l2t_entry *e = &d->l2tab[l2t_idx - d->l2t_start];
+
+		spin_lock(&e->lock);
+		if (e->state != L2T_STATE_SWITCHING) {
+			send_pending(adap, e);
+			e->state = (e->neigh->nud_state & NUD_STALE) ?
+					L2T_STATE_STALE : L2T_STATE_VALID;
+		}
+		spin_unlock(&e->lock);
+	}
+}
+
+/*
+ * Add a packet to an L2T entry's queue of packets awaiting resolution.
+ * Must be called with the entry's lock held.
+ */
+static inline void arpq_enqueue(struct l2t_entry *e, struct sk_buff *skb)
+{
+	skb->next = NULL;
+	if (e->arpq_head)
+		e->arpq_tail->next = skb;
+	else
+		e->arpq_head = skb;
+	e->arpq_tail = skb;
+}
+
+int cxgb4_l2t_send(struct net_device *dev, struct sk_buff *skb,
+		   struct l2t_entry *e)
+{
+	struct adapter *adap = netdev2adap(dev);
+
+again:
+	switch (e->state) {
+	case L2T_STATE_STALE:     /* entry is stale, kick off revalidation */
+		neigh_event_send(e->neigh, NULL);
+		spin_lock_bh(&e->lock);
+		if (e->state == L2T_STATE_STALE)
+			e->state = L2T_STATE_VALID;
+		spin_unlock_bh(&e->lock);
+	case L2T_STATE_VALID:     /* fast-path, send the packet on */
+		return t4_ofld_send(adap, skb);
+	case L2T_STATE_RESOLVING:
+	case L2T_STATE_SYNC_WRITE:
+		spin_lock_bh(&e->lock);
+		if (e->state != L2T_STATE_SYNC_WRITE &&
+		    e->state != L2T_STATE_RESOLVING) { // ARP already completed
+			spin_unlock_bh(&e->lock);
+			goto again;
+		}
+		arpq_enqueue(e, skb);
+		spin_unlock_bh(&e->lock);
+
+		/*
+		 * Only the first packet added to the arpq should kick off
+		 * resolution.  However, because skb allocation can fail,
+		 * we allow each packet added to the arpq to retry resolution
+		 * as a way of recovering from transient memory exhaustion.
+		 * A better way would be to use a work request to retry L2T
+		 * entries when there's no memory.
+		 */
+		if (e->state == L2T_STATE_RESOLVING &&
+		    !neigh_event_send(e->neigh, NULL)) {
+			spin_lock_bh(&e->lock);
+			if (e->state == L2T_STATE_RESOLVING && e->arpq_head)
+				write_l2e(adap, e, 1);
+			spin_unlock_bh(&e->lock);
+		}
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cxgb4_l2t_send);
+
+/*
+ * Allocate a free L2T entry.  Must be called with l2t_data.lock held.
+ */
+static struct l2t_entry *alloc_l2e(struct l2t_data *d)
+{
+	struct l2t_entry *end, *e, **p;
+
+	if (!atomic_read(&d->nfree))
+		return NULL;
+
+	/* there's definitely a free entry */
+	for (e = d->rover, end = &d->l2tab[d->l2t_size]; e != end; ++e)
+		if (atomic_read(&e->refcnt) == 0)
+			goto found;
+
+	for (e = d->l2tab; atomic_read(&e->refcnt); ++e) ;
+found:
+	d->rover = e + 1;
+	atomic_dec(&d->nfree);
+
+	/*
+	 * The entry we found may be an inactive entry that is
+	 * presently in the hash table.  We need to remove it.
+	 */
+	if (e->state < L2T_STATE_SWITCHING)
+		for (p = &d->l2tab[e->hash].first; *p; p = &(*p)->next)
+			if (*p == e) {
+				*p = e->next;
+				e->next = NULL;
+				break;
+			}
+
+	e->state = L2T_STATE_UNUSED;
+	return e;
+}
+
+/*
+ * Called when an L2T entry has no more users.  The entry is left in the hash
+ * table since it is likely to be reused but we also bump nfree to indicate
+ * that the entry can be reallocated for a different neighbor.  We also drop
+ * the existing neighbor reference in case the neighbor is going away and is
+ * waiting on our reference.
+ *
+ * Because entries can be reallocated to other neighbors once their ref count
+ * drops to 0 we need to take the entry's lock to avoid races with a new
+ * incarnation.
+ */
+static void t4_l2e_free(struct l2t_entry *e)
+{
+	struct l2t_data *d;
+
+	spin_lock_bh(&e->lock);
+	if (atomic_read(&e->refcnt) == 0) {  /* hasn't been recycled */
+		if (e->neigh) {
+			neigh_release(e->neigh);
+			e->neigh = NULL;
+		}
+		/*
+		 * Don't need to worry about the arpq, an L2T entry can't be
+		 * released if any packets are waiting for resolution as we
+		 * need to be able to communicate with the device to close a
+		 * connection.
+		 */
+	}
+	spin_unlock_bh(&e->lock);
+
+	d = container_of(e, struct l2t_data, l2tab[e->idx]);
+	atomic_inc(&d->nfree);
+}
+
+void cxgb4_l2t_release(struct l2t_entry *e)
+{
+	if (atomic_dec_and_test(&e->refcnt))
+		t4_l2e_free(e);
+}
+EXPORT_SYMBOL(cxgb4_l2t_release);
+
+/*
+ * Update an L2T entry that was previously used for the same next hop as neigh.
+ * Must be called with softirqs disabled.
+ */
+static void reuse_entry(struct l2t_entry *e, struct neighbour *neigh)
+{
+	unsigned int nud_state;
+
+	spin_lock(&e->lock);                /* avoid race with t4_l2t_free */
+	if (neigh != e->neigh)
+		neigh_replace(e, neigh);
+	nud_state = neigh->nud_state;
+	if (memcmp(e->dmac, neigh->ha, sizeof(e->dmac)) ||
+	    !(nud_state & NUD_VALID))
+		e->state = L2T_STATE_RESOLVING;
+	else if (nud_state & NUD_CONNECTED)
+		e->state = L2T_STATE_VALID;
+	else
+		e->state = L2T_STATE_STALE;
+	spin_unlock(&e->lock);
+}
+
+static inline int in_bond(int port, struct bond_ports *bond_ports)
+{
+	int i;
+
+	for (i = 0; i < bond_ports->nports; i++)
+		if (port ==  bond_ports->ports[i])
+			break;
+
+	return (i < bond_ports->nports);
+}
+
+int t4_bond_port_disable(struct net_device *dev, bool flag,
+				struct bond_ports *bond_ports)
+{
+	struct adapter *adapter = netdev2adap(dev);
+	struct port_info *pi = adap2pinfo(adapter, bond_ports->port);
+
+	return t4_enable_vi(adapter, adapter->mbox, pi->viid, flag, flag);
+}
+EXPORT_SYMBOL(t4_bond_port_disable);
+
+int t4_ports_failover(struct net_device *dev, int event,
+			struct bond_ports *bond_ports, struct l2t_data *d)
+{
+	int port = bond_ports->port, i;
+	struct adapter *adap = netdev2adap(dev);
+	struct l2t_entry *e;
+	int nports = 0, port_idx;
+
+	/* Reassign L2T entries */
+	switch (event) {
+		case FAILOVER_PORT_RELEASE:
+		case FAILOVER_PORT_DOWN:
+			read_lock_bh(&d->lock);
+			port_idx = 0;
+			nports = bond_ports->nports;
+			for ( i = 0 ; i < d->l2t_size;  ++i) {
+				for (e = d->l2tab[i].first; e; e = e->next) {
+					int newport;
+
+					if (e->lport == port && nports) {
+						newport = bond_ports->ports
+								[port_idx];
+						spin_lock_bh(&e->lock);
+						e->lport = newport;
+						write_l2e(adap, e, 0);
+						spin_unlock_bh(&e->lock);
+						port_idx = port_idx < nports - 1?
+						port_idx + 1 : 0;
+					}
+					/*
+					 * If the port is released, update
+					 * orig_smt_idx to failed over port.
+					 * There are 2 situations:
+					 * 1. Port X is the original port and
+					 * is released. {orig_smt_idx, smt_idx}
+					 * follows these steps.
+					 * {X, X} -> {X, Y} -> {Y, Y}
+					 * 2. Port Z is released, a failover
+					 * from port X had happened previously.
+					 * {orig_smt_idx, smt_idx} follows these
+					 * steps:
+					 * {X, Z} -> {Z, Z}
+					 */
+					if (event == FAILOVER_PORT_RELEASE &&
+						e->orig_lport == port) {
+						spin_lock_bh(&e->lock);
+						e->orig_lport = e->lport;
+						spin_unlock_bh(&e->lock);
+					}
+				}
+			}
+			read_unlock_bh(&d->lock);
+			break;
+		case FAILOVER_PORT_UP:
+			read_lock_bh(&d->lock);
+			for ( i = 0 ; i < d->l2t_size;  ++i) {
+				for (e = d->l2tab[i].first; e; e = e->next) {
+					if (e->orig_lport == port &&
+						in_bond(e->lport, bond_ports)) {
+						spin_lock_bh(&e->lock);
+						e->lport = port;
+						write_l2e(adap, e, 0);
+						spin_unlock_bh(&e->lock);
+					}
+				}
+			}
+			read_unlock_bh(&d->lock);
+			break;
+		case FAILOVER_ACTIVE_SLAVE:
+			read_lock_bh(&d->lock);
+			for ( i = 0 ; i < d->l2t_size;  ++i) {
+				for (e = d->l2tab[i].first; e; e = e->next) {
+					if (e->lport != port &&
+						in_bond(e->lport, bond_ports)) {
+						spin_lock_bh(&e->lock);
+						e->lport = port;
+						write_l2e(adap, e, 0);
+						spin_unlock_bh(&e->lock);
+					}
+				}
+			}
+			read_unlock_bh(&d->lock);
+			break;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(t4_ports_failover);
+
+struct l2t_entry *cxgb4_l2t_get(struct l2t_data *d, struct neighbour *neigh,
+				const struct net_device *physdev , u32 priority)
+{
+	u8 lport;
+	u16 vlan;
+	struct l2t_entry *e;
+	int addr_len = neigh->tbl->key_len;
+	u32 *addr = (u32 *)neigh->primary_key;
+	int ifidx = neigh->dev->ifindex;
+	int hash = addr_hash(d, addr, addr_len, ifidx);
+
+	if (neigh->dev->flags & IFF_LOOPBACK)
+		lport = netdev2pinfo(physdev)->tx_chan + 4;
+	else
+		lport = netdev2pinfo(physdev)->lport;
+
+	if (neigh->dev->priv_flags & IFF_802_1Q_VLAN) {
+		vlan = vlan_dev_vlan_id(neigh->dev);
+		vlan |= vlan_dev_get_egress_pri_mask(neigh->dev , priority);
+	} else
+		vlan = VLAN_NONE;
+
+	write_lock_bh(&d->lock);
+	for (e = d->l2tab[hash].first; e; e = e->next)
+		if (!addreq(e, addr) && e->ifindex == ifidx &&
+		    e->lport == lport && e->vlan == vlan) {
+			l2t_hold(d, e);
+			/*
+			 * We now have an entry that has previously been used
+			 * for this next hop.  If we are the sole owner it
+			 * may have been some time since this entry has been
+			 * maintained so we need to bring it up to date.
+			 * Otherwise the existing users have been updating it.
+			 */
+			if (atomic_read(&e->refcnt) == 1)
+				reuse_entry(e, neigh);
+			goto done;
+		}
+
+	/* Need to allocate a new entry */
+	e = alloc_l2e(d);
+	if (e) {
+		spin_lock(&e->lock);          /* avoid race with t4_l2t_free */
+		e->state = L2T_STATE_RESOLVING;
+		if (neigh->dev->flags & IFF_LOOPBACK)
+			memcpy(e->dmac, physdev->dev_addr, ETH_ALEN); 
+		memcpy(e->addr, addr, addr_len);
+		e->ifindex = ifidx;
+		e->hash = hash;
+		e->lport = lport;
+		e->orig_lport = lport;
+		e->v6 = addr_len == 16;
+		atomic_set(&e->refcnt, 1);
+		neigh_replace(e, neigh);
+		e->vlan = vlan;
+		e->next = d->l2tab[hash].first;
+		d->l2tab[hash].first = e;
+		spin_unlock(&e->lock);
+	}
+done:
+	write_unlock_bh(&d->lock);
+	return e;
+}
+EXPORT_SYMBOL(cxgb4_l2t_get);
+
+/*
+ * Called when address resolution fails for an L2T entry to handle packets
+ * on the arpq head.  If a packet specifies a failure handler it is invoked,
+ * otherwise the packet is sent to the device.
+ *
+ * XXX: maybe we should abandon the latter behavior and just require a failure
+ * handler.
+ */
+static void handle_failed_resolution(struct adapter *adap, struct sk_buff *arpq)
+{
+	while (arpq) {
+		struct sk_buff *skb = arpq;
+		const struct l2t_skb_cb *cb = L2T_SKB_CB(skb);
+
+		arpq = skb->next;
+		skb->next = NULL;
+		if (cb->arp_err_handler)
+			cb->arp_err_handler(cb->handle, skb);
+		else
+			t4_ofld_send(adap, skb);
+	}
+}
+
+/*
+ * Called when the host's neighbor layer makes a change to some entry that is
+ * loaded into the HW L2 table.
+ */
+void t4_l2t_update(struct adapter *adap, struct neighbour *neigh)
+{
+	struct l2t_entry *e;
+	struct sk_buff *arpq = NULL;
+	struct l2t_data *d = adap->l2t;
+	int addr_len = neigh->tbl->key_len;
+	u32 *addr = (u32 *) neigh->primary_key;
+	int ifidx = neigh->dev->ifindex;
+	int hash = addr_hash(d, addr, addr_len, ifidx);
+
+	read_lock_bh(&d->lock);
+	for (e = d->l2tab[hash].first; e; e = e->next)
+		if (!addreq(e, addr) && e->ifindex == ifidx) {
+			spin_lock(&e->lock);
+			if (atomic_read(&e->refcnt))
+				goto found;
+			spin_unlock(&e->lock);
+		}
+	read_unlock_bh(&d->lock);
+	return;
+
+ found:
+	read_unlock(&d->lock);
+
+	if (neigh != e->neigh)
+		neigh_replace(e, neigh);
+
+	if (e->state == L2T_STATE_RESOLVING) {
+		if (neigh->nud_state & NUD_FAILED) {
+			arpq = e->arpq_head;
+			e->arpq_head = e->arpq_tail = NULL;
+		} else if ((neigh->nud_state & (NUD_CONNECTED | NUD_STALE)) &&
+			   e->arpq_head) {
+			write_l2e(adap, e, 1);
+		}
+	} else {
+		e->state = neigh->nud_state & NUD_CONNECTED ?
+			L2T_STATE_VALID : L2T_STATE_STALE;
+		if (memcmp(e->dmac, neigh->ha, sizeof(e->dmac)))
+			write_l2e(adap, e, 0);
+	}
+
+	spin_unlock_bh(&e->lock);
+
+	if (arpq)
+		handle_failed_resolution(adap, arpq);
+}
+
+/*
+ * Allocate an L2T entry for use by a switching rule.  Such need to be
+ * explicitly freed and while busy they are not on any hash chain, so normal
+ * address resolution updates do not see them.
+ */
+struct l2t_entry *t4_l2t_alloc_switching(struct l2t_data *d)
+{
+	struct l2t_entry *e;
+
+	write_lock_bh(&d->lock);
+	e = alloc_l2e(d);
+	if (e) {
+		spin_lock(&e->lock);          /* avoid race with t4_l2t_free */
+		e->state = L2T_STATE_SWITCHING;
+		atomic_set(&e->refcnt, 1);
+		spin_unlock(&e->lock);
+	}
+	write_unlock_bh(&d->lock);
+	return e;
+}
+
+/*
+ * Sets/updates the contents of a switching L2T entry that has been allocated
+ * with an earlier call to @t4_l2t_alloc_switching.
+ */
+int t4_l2t_set_switching(struct adapter *adap, struct l2t_entry *e, u16 vlan,
+			 u8 port, u8 *eth_addr)
+{
+	e->vlan = vlan;
+	e->lport = port;
+	memcpy(e->dmac, eth_addr, ETH_ALEN);
+	return write_l2e(adap, e, 0);
+}
+
+struct l2t_data *t4_init_l2t(unsigned int l2t_start, unsigned int l2t_end)
+{
+	unsigned int l2t_size;
+	int i;
+	struct l2t_data *d;
+
+	if (l2t_start >= l2t_end || l2t_end >= L2T_SIZE)
+		return NULL;
+	l2t_size = l2t_end - l2t_start + 1;
+	if (l2t_size < L2T_MIN_HASH_BUCKETS)
+		return NULL;
+
+	d = t4_alloc_mem(sizeof(*d) + l2t_size*sizeof(struct l2t_entry));
+	if (!d)
+		return NULL;
+
+	d->l2t_start = l2t_start;
+	d->l2t_size = l2t_size;
+
+	d->rover = d->l2tab;
+	atomic_set(&d->nfree, l2t_size);
+	rwlock_init(&d->lock);
+
+	for (i = 0; i < d->l2t_size; ++i) {
+		d->l2tab[i].idx = i;
+		d->l2tab[i].state = L2T_STATE_UNUSED;
+		spin_lock_init(&d->l2tab[i].lock);
+		atomic_set(&d->l2tab[i].refcnt, 0);
+	}
+	return d;
+}
+
+#ifdef CONFIG_PROC_FS
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+static inline void *l2t_get_idx(struct seq_file *seq, loff_t pos)
+{
+	struct l2t_data *d = seq->private;
+
+	return pos >= d->l2t_size ? NULL : &d->l2tab[pos];
+}
+
+static void *l2t_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	return *pos ? l2t_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;
+}
+
+static void *l2t_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	v = l2t_get_idx(seq, *pos);
+	if (v)
+		++*pos;
+	return v;
+}
+
+static void l2t_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static char l2e_state(const struct l2t_entry *e)
+{
+	switch (e->state) {
+	case L2T_STATE_VALID: return 'V';  /* valid, fast-path entry */
+	case L2T_STATE_STALE: return 'S';  /* needs revalidation, but usable */
+	case L2T_STATE_SYNC_WRITE: return 'W';
+	case L2T_STATE_RESOLVING: return e->arpq_head ? 'A' : 'R';
+	case L2T_STATE_SWITCHING: return 'X';
+	default:
+		return 'U';
+	}
+}
+
+static int l2t_seq_show(struct seq_file *seq, void *v)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, " Idx IP address      Ethernet address  VLAN/P "
+			 "LP State Users Port\n");
+	else {
+		char ip[60];
+		struct l2t_data *d = seq->private;
+		struct l2t_entry *e = v;
+
+		spin_lock_bh(&e->lock);
+		if (e->state == L2T_STATE_SWITCHING)
+			ip[0] = '\0';
+		else if (!e->v6)
+			sprintf(ip, NIPQUAD_FMT, NIPQUAD(e->addr[0]));
+		else
+			ip[0] = '\0';  // XXX IPv6 is too long, hmm
+		seq_printf(seq, "%4u %-15s %02x:%02x:%02x:%02x:%02x:%02x %4d"
+			   " %u %2u   %c   %5u %s\n",
+			   e->idx + d->l2t_start, ip,
+			   e->dmac[0], e->dmac[1], e->dmac[2],
+			   e->dmac[3], e->dmac[4], e->dmac[5],
+			   e->vlan & VLAN_VID_MASK, vlan_prio(e), e->lport,
+			   l2e_state(e), atomic_read(&e->refcnt),
+			   e->neigh ? e->neigh->dev->name : "");
+		spin_unlock_bh(&e->lock);
+	}
+	return 0;
+}
+
+static const struct seq_operations l2t_seq_ops = {
+	.start = l2t_seq_start,
+	.next = l2t_seq_next,
+	.stop = l2t_seq_stop,
+	.show = l2t_seq_show
+};
+
+static int l2t_seq_open(struct inode *inode, struct file *file)
+{
+	int rc = seq_open(file, &l2t_seq_ops);
+
+	if (!rc) {
+		struct adapter *adap = PDE(inode)->data;
+		struct seq_file *seq = file->private_data;
+
+		seq->private = adap->l2t;
+	}
+	return rc;
+}
+
+const struct file_operations t4_l2t_proc_fops = {
+	.owner = THIS_MODULE,
+	.open = l2t_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+#endif
diff --git a/drivers/net/cxgb4/l2t.h b/drivers/net/cxgb4/l2t.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/l2t.h
@@ -0,0 +1,97 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * Written by Dimitris Michailidis (dm@chelsio.com)
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __CXGB4_L2T_H
+#define __CXGB4_L2T_H
+
+#include <linux/spinlock.h>
+#include <linux/if_ether.h>
+#include <asm/atomic.h>
+#include "cxgb4_ctl_defs.h"
+
+
+enum { L2T_SIZE = 4096 };     /* # of L2T entries */
+
+struct adapter;
+struct l2t_data;
+struct neighbour;
+struct file_operations;
+struct cpl_l2t_write_rpl;
+
+/*
+ * Each L2T entry plays multiple roles.  First of all, it keeps state for the
+ * corresponding entry of the HW L2 table and maintains a queue of offload
+ * packets awaiting address resolution.  Second, it is a node of a hash table
+ * chain, where the nodes of the chain are linked together through their next
+ * pointer.  Finally, each node is a bucket of a hash table, pointing to the
+ * first element in its chain through its first pointer.
+ */
+struct l2t_entry {
+	u16 state;                  /* entry state */
+	u16 idx;                    /* entry index within in-memory table */
+	u32 addr[4];                /* next hop IP or IPv6 address */
+	int ifindex;                /* neighbor's net_device's ifindex */
+	struct neighbour *neigh;    /* associated neighbour */
+	struct l2t_entry *first;    /* start of hash chain */
+	struct l2t_entry *next;     /* next l2t_entry on chain */
+	struct sk_buff *arpq_head;  /* queue of packets awaiting resolution */
+	struct sk_buff *arpq_tail;
+	spinlock_t lock;
+	atomic_t refcnt;            /* entry reference count */
+	u16 hash;                   /* hash bucket the entry is on */
+	u16 vlan;                   /* VLAN TCI (id: bits 0-11, prio: 13-15 */
+	u8 v6;                      /* whether entry is for IPv6 */
+	u8 lport;                   /* associated offload logical interface */
+	u8 dmac[ETH_ALEN];          /* neighbour's MAC address */
+	u8 orig_lport;
+};
+
+typedef void (*arp_err_handler_t)(void *handle, struct sk_buff *skb);
+
+/*
+ * Callback stored in an skb to handle address resolution failure.
+ */
+struct l2t_skb_cb {
+	void *handle;
+	arp_err_handler_t arp_err_handler;
+};
+
+#define L2T_SKB_CB(skb) ((struct l2t_skb_cb *)(skb)->cb)
+
+static inline void t4_set_arp_err_handler(struct sk_buff *skb, void *handle,
+					  arp_err_handler_t handler)
+{
+	L2T_SKB_CB(skb)->handle = handle;
+	L2T_SKB_CB(skb)->arp_err_handler = handler;
+}
+
+void cxgb4_l2t_release(struct l2t_entry *e);
+int cxgb4_l2t_send(struct net_device *dev, struct sk_buff *skb,
+		   struct l2t_entry *e);
+struct l2t_entry *cxgb4_l2t_get(struct l2t_data *d, struct neighbour *neigh,
+				const struct net_device *physdev , u32 priority);
+
+void t4_l2t_update(struct adapter *adap, struct neighbour *neigh);
+struct l2t_entry *t4_l2t_alloc_switching(struct l2t_data *d);
+int t4_l2t_set_switching(struct adapter *adap, struct l2t_entry *e, u16 vlan,
+			 u8 port, u8 *eth_addr);
+struct l2t_data *t4_init_l2t(unsigned int l2t_start, unsigned int l2t_end);
+void do_l2t_write_rpl(struct adapter *p, const struct cpl_l2t_write_rpl *rpl);
+
+int t4_bond_port_disable(struct net_device *dev, bool flag,
+					struct bond_ports *bond_ports);
+int t4_ports_failover(struct net_device *dev, int event,
+                              struct bond_ports *bond_ports, struct l2t_data *d);
+
+extern const struct file_operations t4_l2t_proc_fops;
+#endif  /* __CXGB4_L2T_H */
diff --git a/drivers/net/cxgb4/osdep.h b/drivers/net/cxgb4/osdep.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/osdep.h
@@ -0,0 +1,35 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __CXGB4_OSDEP_H
+#define __CXGB4_OSDEP_H
+
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/ctype.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/ethtool.h>
+
+#ifdef CONFIG_CHELSIO_T4_OFFLOAD_MODULE
+# define CONFIG_CHELSIO_T4_OFFLOAD
+#endif
+
+typedef struct adapter adapter_t;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,31)
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#define CONFIG_CXGB4_GRO 1
+#endif
+#endif
+
+#endif  /* !__CXGB4_OSDEP_H */
diff --git a/drivers/net/cxgb4/sge.c b/drivers/net/cxgb4/sge.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/sge.c
@@ -0,0 +1,3618 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <linux/dma-mapping.h>
+#include <net/ipv6.h>
+#include <net/tcp.h>
+#include "common.h"
+#include "t4_regs.h"
+#include "t4_regs_values.h"
+#include "t4_msg.h"
+#include "t4fw_interface.h"
+
+/*
+ * Rx buffer size for "packed" pages Free List buffers (multiple ingress
+ * packets packed per page buffer).  We use largish buffers if possible but
+ * settle for single pages under memory shortage.
+ */
+#if PAGE_SHIFT >= 16
+# define FL_PG_ORDER 0
+#else
+# define FL_PG_ORDER (16 - PAGE_SHIFT)
+#endif
+
+/* RX_PULL_LEN should be <= RX_COPY_THRES */
+#define RX_COPY_THRES    256
+#define RX_PULL_LEN      128
+
+/*
+ * Main body length for sk_buffs used for Rx Ethernet packets with fragments.
+ * Should be >= RX_PULL_LEN but possibly bigger to give pskb_may_pull some room.
+ */
+#define RX_PKT_SKB_LEN   512
+
+/*
+ * Max number of Tx descriptors we clean up at a time.  Should be modest as
+ * freeing skbs isn't cheap and it happens while holding locks.  We just need
+ * to free packets faster than they arrive, we eventually catch up and keep
+ * the amortized cost reasonable.  Must be >= 2 * TXQ_STOP_THRES.
+ */
+#define MAX_TX_RECLAIM 16
+
+/*
+ * Max number of Rx buffers we replenish at a time.  Again keep this modest,
+ * allocating buffers isn't cheap either.
+ */
+#define MAX_RX_REFILL 16U
+
+/*
+ * Period of the Rx queue check timer.  This timer is infrequent as it has
+ * something to do only when the system experiences severe memory shortage.
+ */
+#define RX_QCHECK_PERIOD (HZ / 2)
+
+/*
+ * Period of the Tx queue check timer.
+ */
+#define TX_QCHECK_PERIOD (HZ / 2)
+
+/*
+ * Max number of Tx descriptors to be reclaimed by the Tx timer.
+ */
+#define MAX_TIMER_TX_RECLAIM 100
+
+/*
+ * Timer index used when Rx queues encounter severe memory shortage.
+ */
+#define NOMEM_TMR_IDX (SGE_NTIMERS - 1)
+
+/*
+ * Suspend an Ethernet Tx queue with fewer available descriptors than this.
+ * This is the same as calc_tx_descs() for a TSO packet with
+ * nr_frags == MAX_SKB_FRAGS.
+ */
+#define ETHTXQ_STOP_THRES \
+	(1 + DIV_ROUND_UP((3 * MAX_SKB_FRAGS) / 2 + (MAX_SKB_FRAGS & 1), 8))
+
+/*
+ * Suspension threshold for non-Ethernet Tx queues.  We require enough room
+ * for a full sized WR.
+ */
+#define TXQ_STOP_THRES (SGE_MAX_WR_LEN / sizeof(struct tx_desc))
+
+/*
+ * Max Tx descriptor space we allow for an Ethernet packet to be inlined
+ * into a WR.
+ */
+#define MAX_IMM_TX_PKT_LEN 128
+
+/*
+ * Max size of a WR sent through a control Tx queue.
+ */
+#define MAX_CTRL_WR_LEN SGE_MAX_WR_LEN
+
+/*
+ * Currently there are two types of coalesce WR. Type 0 needs 48 bytes per
+ * packet (if one sgl is present) and type 1 needs 32 bytes. This means
+ * that type 0 can fit a maximum of 10 packets per WR and type 1 can fit
+ * 15 packets. We need to keep track of the skb pointers in a coalesce WR
+ * to be able to free those skbs when we get completions back from the FW.
+ * Allocating the maximum number of pointers in every tx desc is a waste
+ * of memory resources so we only store 2 pointers per tx desc which should
+ * be enough since a tx desc can only fit 2 packets in the best case
+ * scenario where a packet needs 32 bytes.
+ */
+#define ETH_COALESCE_PKT_NUM 15
+#define ETH_COALESCE_PKT_PER_DESC 2
+#define MAX_SKB_COALESCE_LEN 4096
+
+struct tx_eth_coal_desc {
+	struct sk_buff *skb[ETH_COALESCE_PKT_PER_DESC];
+	struct ulptx_sgl *sgl[ETH_COALESCE_PKT_PER_DESC];
+	int idx; 
+};
+
+struct tx_sw_desc {                /* SW state per Tx descriptor */
+	struct sk_buff *skb;
+	struct ulptx_sgl *sgl;
+	struct tx_eth_coal_desc coalesce;
+};
+
+struct rx_sw_desc {                /* SW state per Rx descriptor */
+	void *buf;                 /* struct page or sk_buff */
+	dma_addr_t dma_addr;
+};
+
+/*
+ * Rx buffer sizes for "useskbs" Free List buffers (one ingress packet per skb
+ * buffer).  We currently only support two sizes for 1500- and 9000-byte MTUs.
+ * We could easily support more but there doesn't seem to be much need for
+ * that ...
+ */
+#define FL_MTU_SMALL 1500
+#define FL_MTU_LARGE 9000
+
+static inline unsigned int fl_mtu_bufsize(struct adapter *adapter,
+					  unsigned int mtu)
+{
+	struct sge *s = &adapter->sge;
+
+	return ALIGN(s->pktshift + ETH_HLEN + VLAN_HLEN + mtu, s->fl_align);
+}
+
+#define FL_MTU_SMALL_BUFSIZE(adapter) fl_mtu_bufsize(adapter, FL_MTU_SMALL)
+#define FL_MTU_LARGE_BUFSIZE(adapter) fl_mtu_bufsize(adapter, FL_MTU_LARGE)
+
+/*
+ * The low bits of rx_sw_desc.dma_addr have special meaning.  The hardware
+ * uses these to specify the buffer size as an index into the SGE Free List
+ * Buffer Size register array.  We also use one bit, when the buffer isn't
+ * mapped to the hardware to indicate that the buffer isn't mapped.  (If we
+ * ever need to support more than 8 different buffer sizes, then we'll need to
+ * move that "Buffer Unmapped" flag into a separate "flags" word in the Rx
+ * Software Descriptor.)
+ */
+enum {
+	RX_BUF_FLAGS     = 0xf,   /* bottom four bits are special */
+	RX_BUF_SIZE      = 0x7,   /* bottom three bits are for buf sizes */
+	RX_UNMAPPED_BUF  = 0x8,   /* buffer is not mapped */
+
+	/*
+	 * XXX We shouldn't depend on being able to use these indices.
+	 * XXX Especially when some other Master PF has initialized the
+	 * XXX adapter or we use the Firmware Configuration File.  We
+	 * XXX should really search through the Host Buffer Size register
+	 * XXX array for the appropriately sized buffer indices.
+	 */
+	RX_SMALL_PG_BUF  = 0x0,   /* small (PAGE_SIZE) page buffer */
+	RX_LARGE_PG_BUF  = 0x1,   /* buffer large (FL_PG_ORDER) page buffer */
+
+	RX_SMALL_MTU_BUF = 0x2,   /* small MTU buffer */
+	RX_LARGE_MTU_BUF = 0x3,   /* large MTU buffer */
+};
+
+static inline dma_addr_t get_buf_addr(const struct rx_sw_desc *d)
+{
+	return d->dma_addr & ~(dma_addr_t)RX_BUF_FLAGS;
+}
+
+static inline bool is_buf_mapped(const struct rx_sw_desc *d)
+{
+	return !(d->dma_addr & RX_UNMAPPED_BUF);
+}
+
+/**
+ *	txq_avail - return the number of available slots in a Tx queue
+ *	@q: the Tx queue
+ *
+ *	Returns the number of descriptors in a Tx queue available to write new
+ *	packets.
+ */
+static inline unsigned int txq_avail(const struct sge_txq *q)
+{
+	return q->size - 1 - q->in_use;
+}
+
+/**
+ *	fl_cap - return the capacity of a free-buffer list
+ *	@fl: the FL
+ *
+ *	Returns the capacity of a free-buffer list.  The capacity is less than
+ *	the size because one descriptor needs to be left unpopulated, otherwise
+ *	HW will think the FL is empty.
+ */
+static inline unsigned int fl_cap(const struct sge_fl *fl)
+{
+	return fl->size - 8;   /* 1 descriptor = 8 buffers */
+}
+
+/**
+ *	fl_starving - return whether a Free List is starving.
+ *	@adapter: pointer to the adapter
+ *	@fl: the Free List
+ *
+ *	Tests specified Free List to see whether the number of buffers
+ *	available to the hardware has falled below our "starvation"
+ *	threshhold.
+ */
+static inline bool fl_starving(const struct adapter *adapter,
+			       const struct sge_fl *fl)
+{
+	const struct sge *s = &adapter->sge;
+
+	return fl->avail - fl->pend_cred <= s->fl_starve_thres;
+}
+
+static int map_skb(struct device *dev, const struct sk_buff *skb,
+		   dma_addr_t *addr)
+{
+	const skb_frag_t *fp, *end;
+	const struct skb_shared_info *si;
+
+	*addr = dma_map_single(dev, skb->data, skb_headlen(skb), DMA_TO_DEVICE);
+	if (dma_mapping_error(dev, *addr))
+		goto out_err;
+
+	si = skb_shinfo(skb);
+	end = &si->frags[si->nr_frags];
+
+	for (fp = si->frags; fp < end; fp++) {
+		*++addr = dma_map_page(dev, fp->page, fp->page_offset, fp->size,
+				       DMA_TO_DEVICE);
+		if (dma_mapping_error(dev, *addr))
+			goto unwind;
+	}
+	return 0;
+
+unwind:
+	while (fp-- > si->frags)
+		dma_unmap_page(dev, *--addr, fp->size, DMA_TO_DEVICE);
+
+	dma_unmap_single(dev, addr[-1], skb_headlen(skb), DMA_TO_DEVICE);
+out_err:
+	return -ENOMEM;
+}
+
+
+static void unmap_skb(struct device *dev, const struct sk_buff *skb,
+		      const dma_addr_t *addr)
+{
+	const skb_frag_t *fp, *end;
+	const struct skb_shared_info *si;
+
+	dma_unmap_single(dev, *addr++, skb_headlen(skb), DMA_TO_DEVICE);
+
+	si = skb_shinfo(skb);
+	end = &si->frags[si->nr_frags];
+	for (fp = si->frags; fp < end; fp++)
+		dma_unmap_page(dev, *addr++, fp->size, DMA_TO_DEVICE);
+}
+
+/**
+ *	deferred_unmap_destructor - unmap a packet when it is freed
+ *	@skb: the packet
+ *
+ *	This is the packet destructor used for Tx packets that need to remain
+ *	mapped until they are freed rather than until their Tx descriptors are
+ *	freed.
+ */
+static void deferred_unmap_destructor(struct sk_buff *skb)
+{
+	unmap_skb(skb->dev->dev.parent, skb, (dma_addr_t *)skb->head);
+}
+
+static void unmap_sgl(struct device *dev, const struct sk_buff *skb,
+		      const struct ulptx_sgl *sgl, const struct sge_txq *q)
+{
+	const struct ulptx_sge_pair *p;
+	unsigned int nfrags = skb_shinfo(skb)->nr_frags;
+
+	if (likely(skb_headlen(skb)))
+		dma_unmap_single(dev, be64_to_cpu(sgl->addr0), ntohl(sgl->len0),
+				 DMA_TO_DEVICE);
+	else {
+		dma_unmap_page(dev, be64_to_cpu(sgl->addr0), ntohl(sgl->len0),
+			       DMA_TO_DEVICE);
+		nfrags--;
+	}
+
+	/*
+	 * the complexity below is because of the possibility of a wrap-around
+	 * in the middle of an SGL
+	 */
+	for (p = sgl->sge; nfrags >= 2; nfrags -= 2) {
+		if (likely((u8 *)(p + 1) <= (u8 *)q->stat)) {
+unmap:			dma_unmap_page(dev, be64_to_cpu(p->addr[0]),
+				       ntohl(p->len[0]), DMA_TO_DEVICE);
+			dma_unmap_page(dev, be64_to_cpu(p->addr[1]),
+				       ntohl(p->len[1]), DMA_TO_DEVICE);
+			p++;
+		} else if ((u8 *)p == (u8 *)q->stat) {
+			p = (const struct ulptx_sge_pair *)q->desc;
+			goto unmap;
+		} else if ((u8 *)p + 8 == (u8 *)q->stat) {
+			const __be64 *addr = (const __be64 *)q->desc;
+
+			dma_unmap_page(dev, be64_to_cpu(addr[0]),
+				       ntohl(p->len[0]), DMA_TO_DEVICE);
+			dma_unmap_page(dev, be64_to_cpu(addr[1]),
+				       ntohl(p->len[1]), DMA_TO_DEVICE);
+			p = (const struct ulptx_sge_pair *)&addr[2];
+		} else {
+			const __be64 *addr = (const __be64 *)q->desc;
+
+			dma_unmap_page(dev, be64_to_cpu(p->addr[0]),
+				       ntohl(p->len[0]), DMA_TO_DEVICE);
+			dma_unmap_page(dev, be64_to_cpu(addr[0]),
+				       ntohl(p->len[1]), DMA_TO_DEVICE);
+			p = (const struct ulptx_sge_pair *)&addr[1];
+		}
+	}
+	if (nfrags) {
+		__be64 addr;
+
+		if ((u8 *)p == (u8 *)q->stat)
+			p = (const struct ulptx_sge_pair *)q->desc;
+		addr = (u8 *)p + 16 <= (u8 *)q->stat ? p->addr[0] :
+						       *(const __be64 *)q->desc;
+		dma_unmap_page(dev, be64_to_cpu(addr), ntohl(p->len[0]),
+			       DMA_TO_DEVICE);
+	}
+}
+
+/**
+ *	 need_skb_unmap - does the platform need unmapping of sk_buffs?
+ *
+ *	Returns true if the platfrom needs sk_buff unmapping.  The compiler
+ *	optimizes away unecessary code if this returns true.
+ */
+static inline int need_skb_unmap(void)
+{
+	/*
+	 * This structure is used to tell if the platfrom needs buffer
+	 * unmapping by checking if DECLARE_PCI_UNMAP_ADDR defines anything.
+	 */
+	struct dummy {
+		DECLARE_PCI_UNMAP_ADDR(addr);
+	};
+
+	return sizeof(struct dummy) != 0;
+}
+
+/**
+ *	free_tx_desc - reclaims Tx descriptors and their buffers
+ *	@adapter: the adapter
+ *	@q: the Tx queue to reclaim descriptors from
+ *	@n: the number of descriptors to reclaim
+ *	@unmap: whether the buffers should be unmapped for DMA
+ *
+ *	Reclaims Tx descriptors from an SGE Tx queue and frees the associated
+ *	Tx buffers.  Called with the Tx queue lock held.
+ */
+static void free_tx_desc(struct adapter *adap, struct sge_txq *q,
+			 unsigned int n, bool unmap)
+{
+	struct tx_sw_desc *d;
+	unsigned int cidx = q->cidx;
+	struct device *dev = adap->pdev_dev;
+	int i;
+
+	const int need_unmap = need_skb_unmap() && unmap;
+
+#ifdef T4_TRACE
+	T4_TRACE2(adap->tb[q->cntxt_id & 7],
+		  "reclaiming %u Tx descriptors at cidx %u", n, cidx);
+#endif
+	d = &q->sdesc[cidx];
+	while (n--) {
+		if (d->skb) {                       /* an SGL is present */
+			if (need_unmap)
+				unmap_sgl(dev, d->skb, d->sgl, q);
+			kfree_skb(d->skb);
+			d->skb = NULL;
+		}
+		if (d->coalesce.idx) {
+			for (i = 0; i < d->coalesce.idx; i++) {
+				if (need_unmap)
+					unmap_sgl(dev, d->coalesce.skb[i],
+						  d->coalesce.sgl[i], q);
+				kfree_skb(d->coalesce.skb[i]);
+				d->coalesce.skb[i] = NULL;
+			}
+			d->coalesce.idx = 0;
+		}
+		++d;
+		if (++cidx == q->size) {
+			cidx = 0;
+			d = q->sdesc;
+		}
+	}
+	q->cidx = cidx;
+}
+
+/*
+ * Return the number of reclaimable descriptors in a Tx queue.
+ */
+static inline int reclaimable(const struct sge_txq *q)
+{
+	int hw_cidx = ntohs(q->stat->cidx);
+	hw_cidx -= q->cidx;
+	return hw_cidx < 0 ? hw_cidx + q->size : hw_cidx;
+}
+
+/**
+ *	reclaim_completed_tx - reclaims completed Tx descriptors
+ *	@adap: the adapter
+ *	@q: the Tx queue to reclaim completed descriptors from
+ *	@unmap: whether the buffers should be unmapped for DMA
+ *
+ *	Reclaims Tx descriptors that the SGE has indicated it has processed,
+ *	and frees the associated buffers if possible.  Called with the Tx
+ *	queue locked.
+ */
+static inline void reclaim_completed_tx(struct adapter *adap, struct sge_txq *q,
+					bool unmap)
+{
+	int avail = reclaimable(q);
+
+	if (avail) {
+		/*
+		 * Limit the amount of clean up work we do at a time to keep
+		 * the Tx lock hold time O(1).
+		 */
+		if (avail > MAX_TX_RECLAIM)
+			avail = MAX_TX_RECLAIM;
+
+		free_tx_desc(adap, q, avail, unmap);
+		q->in_use -= avail;
+	}
+}
+
+static inline int get_buf_size(struct adapter *adapter,
+			       const struct rx_sw_desc *d)
+{
+	struct sge *s = &adapter->sge;
+	unsigned int rx_buf_size_idx = d->dma_addr & RX_BUF_SIZE;
+	int buf_size;
+
+	switch (rx_buf_size_idx) {
+	case RX_SMALL_PG_BUF:
+		buf_size = PAGE_SIZE;
+		break;
+
+	case RX_LARGE_PG_BUF:
+		buf_size = PAGE_SIZE << s->fl_pg_order;
+		break;
+
+	case RX_SMALL_MTU_BUF:
+		buf_size = FL_MTU_SMALL_BUFSIZE(adapter);
+		break;
+
+	case RX_LARGE_MTU_BUF:
+		buf_size = FL_MTU_LARGE_BUFSIZE(adapter);
+		break;
+
+	default:
+		BUG_ON(1);
+	}
+
+	return buf_size;
+}
+
+/**
+ *	free_rx_bufs - free the Rx buffers on an SGE free list
+ *	@adap: the adapter
+ *	@q: the SGE free list to free buffers from
+ *	@n: how many buffers to free
+ *
+ *	Release the next @n buffers on an SGE free-buffer Rx queue.   The
+ *	buffers must be made inaccessible to HW before calling this function.
+ */
+static void free_rx_bufs(struct adapter *adap, struct sge_fl *q, int n)
+{
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, fl);
+
+	while (n--) {
+		struct rx_sw_desc *d = &q->sdesc[q->cidx];
+
+#if defined(CONFIG_XEN) || defined(CONFIG_CRASH_DUMP)
+		if (likely(rxq->useskbs)) {
+#else
+		if (unlikely(rxq->useskbs)) {
+#endif
+			if (is_buf_mapped(d))
+				dma_unmap_single(adap->pdev_dev,
+						 get_buf_addr(d),
+						 get_buf_size(adap, d),
+						 PCI_DMA_FROMDEVICE);
+			kfree_skb(d->buf);
+		} else {
+			if (is_buf_mapped(d))
+				dma_unmap_page(adap->pdev_dev, get_buf_addr(d),
+					       get_buf_size(adap, d),
+					       PCI_DMA_FROMDEVICE);
+			put_page(d->buf);
+		}
+
+		d->buf = NULL;
+		if (++q->cidx == q->size)
+			q->cidx = 0;
+		q->avail--;
+	}
+}
+
+/**
+ *	unmap_rx_buf - unmap the current Rx buffer on an SGE free list
+ *	@adap: the adapter
+ *	@q: the SGE free list
+ *
+ *	Unmap the current buffer on an SGE free-buffer Rx queue.   The
+ *	buffer must be made inaccessible to HW before calling this function.
+ *
+ *	This is similar to @free_rx_bufs above but does not free the buffer.
+ *	Do note that the FL still loses any further access to the buffer.
+ */
+static void unmap_rx_buf(struct adapter *adap, struct sge_fl *q)
+{
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, fl);
+	struct rx_sw_desc *d = &q->sdesc[q->cidx];
+
+	if (unlikely(rxq->useskbs)) {
+		if (is_buf_mapped(d))
+			dma_unmap_page(adap->pdev_dev, get_buf_addr(d),
+				       get_buf_size(adap, d), PCI_DMA_FROMDEVICE);
+	} else {
+		if (is_buf_mapped(d))
+			dma_unmap_page(adap->pdev_dev, get_buf_addr(d),
+				       get_buf_size(adap, d), PCI_DMA_FROMDEVICE);
+	}
+
+	if (++q->cidx == q->size)
+		q->cidx = 0;
+	q->avail--;
+}
+
+static inline void ring_fl_db(struct adapter *adap, struct sge_fl *q)
+{
+	if (q->pend_cred >= 8) {
+		wmb();
+		t4_write_reg(adap, MYPF_REG(A_SGE_PF_KDOORBELL), F_DBPRIO |
+			     V_QID(q->cntxt_id) | V_PIDX(q->pend_cred / 8));
+		q->pend_cred &= 7;
+	}
+}
+
+static inline void set_rx_sw_desc(struct rx_sw_desc *sd, void *buf,
+				  dma_addr_t mapping)
+{
+	sd->buf = buf;
+	sd->dma_addr = mapping;      /* includes size low bits */
+}
+
+#define POISON_BUF_VAL -1
+
+static inline void poison_buf(struct page *pg, size_t sz)
+{
+#if POISON_BUF_VAL >= 0
+	memset(page_address(pg), POISON_BUF_VAL, sz);
+#endif
+}
+
+/**
+ *	refill_fl_usepages - refill an SGE Rx buffer ring with pages
+ *	@adap: the adapter
+ *	@q: the ring to refill
+ *	@n: the number of new buffers to allocate
+ *	@gfp: the gfp flags for the allocations
+ *
+ *	(Re)populate an SGE free-buffer queue with up to @n new packet buffers,
+ *	allocated with the supplied gfp flags.  The caller must assure that
+ *	@n does not exceed the queue's capacity.  If afterwards the queue is
+ *	found critically low mark it as starving in the bitmap of starving FLs.
+ *
+ *	Returns the number of buffers allocated.
+ */
+static unsigned int refill_fl_usepages(struct adapter *adap, struct sge_fl *q,
+				       int n, gfp_t gfp)
+{
+	struct sge *s = &adap->sge;
+	struct page *pg;
+	dma_addr_t mapping;
+	unsigned int cred = q->avail;
+	__be64 *d = &q->desc[q->pidx];
+	struct rx_sw_desc *sd = &q->sdesc[q->pidx];
+
+	if (test_bit(q->cntxt_id - adap->sge.egr_start, adap->sge.blocked_fl))
+		goto out;
+
+	gfp |= __GFP_NOWARN;         /* failures are expected */
+
+	if (s->fl_pg_order == 0)
+		goto alloc_small_pages;
+
+	/*
+	 * Prefer large buffers
+	 */
+	while (n) {
+		pg = alloc_pages(gfp | __GFP_COMP, s->fl_pg_order);
+		if (unlikely(!pg)) {
+			q->large_alloc_failed++;
+			break;       /* fall back to single pages */
+		}
+
+		poison_buf(pg, PAGE_SIZE << s->fl_pg_order);
+
+		mapping = dma_map_page(adap->pdev_dev, pg, 0,
+				       PAGE_SIZE << s->fl_pg_order,
+				       PCI_DMA_FROMDEVICE);
+		if (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {
+			__free_pages(pg, s->fl_pg_order);
+			goto out;   /* do not try small pages for this error */
+		}
+		mapping |= RX_LARGE_PG_BUF;
+		*d++ = cpu_to_be64(mapping);
+
+		set_rx_sw_desc(sd, pg, mapping);
+		sd++;
+
+		q->avail++;
+		if (++q->pidx == q->size) {
+			q->pidx = 0;
+			sd = q->sdesc;
+			d = q->desc;
+		}
+		n--;
+	}
+
+alloc_small_pages:
+	while (n--) {
+		pg = __netdev_alloc_page(adap->port[0], gfp);
+		if (unlikely(!pg)) {
+			q->alloc_failed++;
+			break;
+		}
+
+		poison_buf(pg, PAGE_SIZE);
+
+		mapping = dma_map_page(adap->pdev_dev, pg, 0, PAGE_SIZE,
+				       PCI_DMA_FROMDEVICE);
+		if (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {
+			netdev_free_page(adap->port[0], pg);
+			break;
+		}
+		mapping |= RX_SMALL_PG_BUF;
+		*d++ = cpu_to_be64(mapping);
+
+		set_rx_sw_desc(sd, pg, mapping);
+		sd++;
+
+		q->avail++;
+		if (++q->pidx == q->size) {
+			q->pidx = 0;
+			sd = q->sdesc;
+			d = q->desc;
+		}
+	}
+
+out:	cred = q->avail - cred;
+	q->pend_cred += cred;
+	ring_fl_db(adap, q);
+
+	if (unlikely(fl_starving(adap, q))) {
+		smp_wmb();
+		set_bit(q->cntxt_id - adap->sge.egr_start,
+			adap->sge.starving_fl);
+	}
+
+	return cred;
+}
+
+/**
+ *	refill_fl_useskbs - refill an SGE Rx buffer ring with skbs
+ *	@adap: the adapter
+ *	@q: the ring to refill
+ *	@n: the number of new buffers to allocate
+ *	@gfp: the gfp flags for the allocations
+ *
+ *	(Re)populate an SGE free-buffer queue with up to @n new packet buffers,
+ *	allocated with the supplied gfp flags.  The caller must assure that
+ *	@n does not exceed the queue's capacity.  If afterwards the queue is
+ *	found critically low mark it as starving in the bitmap of starving FLs.
+ *
+ *	Returns the number of buffers allocated.
+ */
+static unsigned int refill_fl_useskbs(struct adapter *adap, struct sge_fl *q,
+				      int n, gfp_t gfp)
+{
+	struct sge *s = &adap->sge;
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, fl);
+	struct net_device *dev = rxq->rspq.netdev;
+	unsigned int mtu = dev->mtu;
+	unsigned int buf_size, buf_size_idx, skb_size;
+	unsigned int cred = q->avail;
+	__be64 *d = &q->desc[q->pidx];
+	struct rx_sw_desc *sd = &q->sdesc[q->pidx];
+
+	gfp |= __GFP_NOWARN;         /* failures are expected */
+
+	/*
+	 * Figure out what skb buffer size and corresponding T4 SGE FL Buffer
+	 * Size index we'll be using based on the device's current MTU.  We
+	 * need to allocate an extra (FL_ALIGN-1) bytes in order to be able to
+	 * create an aligned address (and leave the bottom bits available for
+	 * our flags and buffer size indices).
+	 */
+	if (mtu <= FL_MTU_SMALL) {
+		buf_size = FL_MTU_SMALL_BUFSIZE(adap);
+		buf_size_idx = RX_SMALL_MTU_BUF;
+	} else {
+		buf_size = FL_MTU_LARGE_BUFSIZE(adap);
+		buf_size_idx = RX_LARGE_MTU_BUF;
+	}
+	skb_size = buf_size + s->fl_align - 1;
+
+	while (n--) {
+		struct sk_buff *skb = alloc_skb(skb_size, gfp);
+		void *buf_start;
+		dma_addr_t mapping;
+
+		if (unlikely(!skb))
+			goto out;
+
+		buf_start = PTR_ALIGN(skb->data, s->fl_align);
+		if (buf_start != skb->data)
+			skb_reserve(skb,
+				   (__kernel_ptrdiff_t)buf_start -
+				   (__kernel_ptrdiff_t)skb->data);
+
+		mapping = dma_map_single(adap->pdev_dev, buf_start, buf_size,
+					 PCI_DMA_FROMDEVICE);
+		if (unlikely(dma_mapping_error(adap->pdev_dev, mapping))) {
+			kfree_skb(skb);
+			goto out;
+		}
+
+		mapping |= buf_size_idx;
+		*d++ = cpu_to_be64(mapping);
+
+		set_rx_sw_desc(sd, skb, mapping);
+		sd++;
+
+		q->avail++;
+		if (++q->pidx == q->size) {
+			q->pidx = 0;
+			sd = q->sdesc;
+			d = q->desc;
+		}
+	}
+
+out:	cred = q->avail - cred;
+	q->pend_cred += cred;
+	ring_fl_db(adap, q);
+
+	if (unlikely(fl_starving(adap, q))) {
+		smp_wmb();
+		set_bit(q->cntxt_id - adap->sge.egr_start,
+			adap->sge.starving_fl);
+	}
+
+	return cred;
+}
+
+/**
+ *	refill_fl - refill an SGE Rx buffer ring with skbs
+ *	@adap: the adapter
+ *	@q: the ring to refill
+ *	@n: the number of new buffers to allocate
+ *	@gfp: the gfp flags for the allocations
+ *
+ *	(Re)populate an SGE free-buffer queue with up to @n new packet buffers,
+ *	allocated with the supplied gfp flags.  The caller must assure that
+ *	@n does not exceed the queue's capacity.  Returns the number of buffers
+ *	allocated.
+ */
+static unsigned int refill_fl(struct adapter *adap, struct sge_fl *q, int n,
+			      gfp_t gfp)
+{
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, fl);
+
+	return (unlikely(rxq->useskbs)
+		? refill_fl_useskbs(adap, q, n, gfp)
+		: refill_fl_usepages(adap, q, n, gfp));
+}
+
+static inline void __refill_fl(struct adapter *adap, struct sge_fl *fl)
+{
+	refill_fl(adap, fl, min(MAX_RX_REFILL, fl_cap(fl) - fl->avail),
+		  GFP_ATOMIC);
+}
+
+/**
+ *	alloc_ring - allocate resources for an SGE descriptor ring
+ *	@dev: the PCI device's core device
+ *	@nelem: the number of descriptors
+ *	@elem_size: the size of each descriptor
+ *	@sw_size: the size of the SW state associated with each ring element
+ *	@phys: the physical address of the allocated ring
+ *	@metadata: address of the array holding the SW state for the ring
+ *	@stat_size: extra space in HW ring for status information
+ *
+ *	Allocates resources for an SGE descriptor ring, such as Tx queues,
+ *	free buffer lists, or response queues.  Each SGE ring requires
+ *	space for its HW descriptors plus, optionally, space for the SW state
+ *	associated with each HW entry (the metadata).  The function returns
+ *	three values: the virtual address for the HW ring (the return value
+ *	of the function), the bus address of the HW ring, and the address
+ *	of the SW ring.
+ */
+static void *alloc_ring(struct device *dev, size_t nelem, size_t elem_size,
+			size_t sw_size, dma_addr_t *phys, void *metadata,
+			size_t stat_size)
+{
+	size_t len = nelem * elem_size + stat_size;
+	void *s = NULL;
+	void *p = dma_alloc_coherent(dev, len, phys, GFP_KERNEL);
+
+	if (!p)
+		return NULL;
+	if (sw_size) {
+		s = kcalloc(nelem, sw_size, GFP_KERNEL);
+
+		if (!s) {
+			dma_free_coherent(dev, len, p, *phys);
+			return NULL;
+		}
+	}
+	if (metadata)
+		*(void **)metadata = s;
+	memset(p, 0, len);
+	return p;
+}
+
+/**
+ *	sgl_len - calculates the size of an SGL of the given capacity
+ *	@n: the number of SGL entries
+ *
+ *	Calculates the number of flits needed for a scatter/gather list that
+ *	can hold the given number of entries.
+ */
+static inline unsigned int sgl_len(unsigned int n)
+{
+	/*
+	 * A Direct Scatter Gather List uses 32-bit lengths and 64-bit PCI DMA
+	 * addresses.  The DSGL Work Request starts off with a 32-bit DSGL
+	 * ULPTX header, then Length0, then Address0, then, for 1 <= i <= N,
+	 * repeated sequences of { Length[i], Length[i+1], Address[i],
+	 * Address[i+1] } (this ensures that all addresses are on 64-bit
+	 * boundaries).  If N is even, then Length[N+1] should be set to 0 and
+	 * Address[N+1] is omitted.
+	 *
+	 * The following calculation incorporates all of the above.  It's
+	 * somewhat hard to follow but, briefly: the "+2" accounts for the
+	 * first two flits which include the DSGL header, Length0 and
+	 * Address0; the "(3*(n-1))/2" covers the main body of list entries (3
+	 * flits for every pair of the remaining N) +1 if (n-1) is odd; and
+	 * finally the "+((n-1)&1)" adds the one remaining flit needed if
+	 * (n-1) is odd ...
+	 */
+	n--;
+	return (3 * n) / 2 + (n & 1) + 2;
+}
+
+/**
+ *	flits_to_desc - returns the num of Tx descriptors for the given flits
+ *	@n: the number of flits
+ *
+ *	Returns the number of Tx descriptors needed for the supplied number
+ *	of flits.
+ */
+static inline unsigned int flits_to_desc(unsigned int n)
+{
+	BUG_ON(n > SGE_MAX_WR_LEN / 8);
+	return DIV_ROUND_UP(n, 8);
+}
+
+/**
+ *	is_eth_imm - can an Ethernet packet be sent as immediate data?
+ *	@skb: the packet
+ *
+ *	Returns whether an Ethernet packet is small enough to fit as
+ *	immediate data.
+ */
+static inline int is_eth_imm(const struct sk_buff *skb)
+{
+	return skb->len <= MAX_IMM_TX_PKT_LEN - sizeof(struct cpl_tx_pkt);
+}
+
+/**
+ *	calc_tx_flits - calculate the number of flits for a packet Tx WR
+ *	@skb: the packet
+ *
+ * 	Returns the number of flits needed for a Tx WR for the given Ethernet
+ * 	packet, including the needed WR and CPL headers.
+ */
+static inline unsigned int calc_tx_flits(const struct sk_buff *skb)
+{
+	unsigned int flits;
+
+	/*
+	 * If the skb is small enough, we can pump it out as a work request
+	 * with only immediate data.  In that case we just have to have the
+	 * TX Packet header plus the skb data in the Work Request.
+	 */
+	if (is_eth_imm(skb))
+		return DIV_ROUND_UP(skb->len + sizeof(struct cpl_tx_pkt), 8);
+
+	/*
+	 * Otherwise, we're going to have to construct a Scatter gather list
+	 * of the skb body and fragments.  We also include the flits necessary
+	 * for the TX Packet Work Request and CPL.  We always have a firmware
+	 * Write Header (incorporated as part of the cpl_tx_pkt_lso and
+	 * cpl_tx_pkt structures), followed by either a TX Packet Write CPL
+	 * message or, if we're doing a Large Send Offload, an LSO CPL message
+	 * with an embeded TX Packet Write CPL message.
+	 */
+	flits = sgl_len(skb_shinfo(skb)->nr_frags + 1);
+	if (skb_shinfo(skb)->gso_size)
+		flits += (sizeof(struct fw_eth_tx_pkt_wr) +
+			  sizeof(struct cpl_tx_pkt_lso_core) +
+			  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);
+	else
+		flits += (sizeof(struct fw_eth_tx_pkt_wr) +
+			  sizeof(struct cpl_tx_pkt_core)) / sizeof(__be64);
+	return flits;
+}
+
+/**
+ *	calc_tx_descs - calculate the number of Tx descriptors for a packet
+ *	@skb: the packet
+ *
+ * 	Returns the number of Tx descriptors needed for the given Ethernet
+ * 	packet, including the needed WR and CPL headers.
+ */
+static inline unsigned int calc_tx_descs(const struct sk_buff *skb)
+{
+	return flits_to_desc(calc_tx_flits(skb));
+}
+
+/**
+ *	write_sgl - populate a scatter/gather list for a packet
+ *	@skb: the packet
+ *	@q: the Tx queue we are writing into
+ *	@sgl: starting location for writing the SGL
+ *	@end: points right after the end of the SGL
+ *	@start: start offset into skb main-body data to include in the SGL
+ *
+ *	Generates a scatter/gather list for the buffers that make up a packet.
+ *	The caller must provide adequate space for the SGL that will be written.
+ *	The SGL includes all of the packet's page fragments and the data in its
+ *	main body except for the first @start bytes.  @sgl must be 16-byte
+ *	aligned and within a Tx descriptor with available space.  @end points
+ *	write after the end of the SGL but does not account for any potential
+ *	wrap around, i.e., @end > @sgl.
+ */
+static void write_sgl(const struct sk_buff *skb, struct sge_txq *q,
+		      struct ulptx_sgl *sgl, u64 *end, unsigned int start,
+		      const dma_addr_t *addr)
+{
+	unsigned int i, len;
+	struct ulptx_sge_pair *to;
+	const struct skb_shared_info *si = skb_shinfo(skb);
+	unsigned int nfrags = si->nr_frags;
+	struct ulptx_sge_pair buf[MAX_SKB_FRAGS / 2 + 1];
+
+	len = skb_headlen(skb) - start;
+	if (likely(len)) {
+		sgl->len0 = htonl(len);
+		sgl->addr0 = cpu_to_be64(addr[0] + start);
+		nfrags++;
+	} else {
+		sgl->len0 = htonl(si->frags[0].size);
+		sgl->addr0 = cpu_to_be64(addr[1]);
+	}
+
+	sgl->cmd_nsge = htonl(V_ULPTX_CMD(ULP_TX_SC_DSGL) |
+			      V_ULPTX_NSGE(nfrags));
+	if (likely(--nfrags == 0))
+		return;
+	/*
+	 * Most of the complexity below deals with the possibility we hit the
+	 * end of the queue in the middle of writing the SGL.  For this case
+	 * only we create the SGL in a temporary buffer and then copy it.
+	 */
+	to = (u8 *)end > (u8 *)q->stat ? buf : sgl->sge;
+
+	for (i = (nfrags != si->nr_frags); nfrags >= 2; nfrags -= 2, to++) {
+		to->len[0] = cpu_to_be32(si->frags[i].size);
+		to->len[1] = cpu_to_be32(si->frags[++i].size);
+		to->addr[0] = cpu_to_be64(addr[i]);
+		to->addr[1] = cpu_to_be64(addr[++i]);
+	}
+	if (nfrags) {
+		to->len[0] = cpu_to_be32(si->frags[i].size);
+		to->len[1] = cpu_to_be32(0);
+		to->addr[0] = cpu_to_be64(addr[i + 1]);
+	}
+	if (unlikely((u8 *)end > (u8 *)q->stat)) {
+		unsigned int part0 = (u8 *)q->stat - (u8 *)sgl->sge, part1;
+
+		if (likely(part0))
+			memcpy(sgl->sge, buf, part0);
+		part1 = (u8 *)end - (u8 *)q->stat;
+		memcpy(q->desc, (u8 *)buf + part0, part1);
+		end = (void *)q->desc + part1;
+	}
+	if ((uintptr_t)end & 8)           /* 0-pad to multiple of 16 */
+		*(u64 *)end = 0;
+}
+
+/**
+ *	ring_tx_db - check and potentially ring a Tx queue's doorbell
+ *	@adap: the adapter
+ *	@q: the Tx queue
+ *	@n: number of new descriptors to give to HW
+ *
+ *	Ring the doorbel for a Tx queue.
+ */
+static inline void ring_tx_db(struct adapter *adap, struct sge_txq *q, int n)
+{
+	WARN_ON((V_QID(q->cntxt_id) | V_PIDX(n)) & F_DBPRIO);
+	wmb();            /* write descriptors before telling HW */
+	spin_lock(&q->db_lock);
+	if (!q->db_disabled) {
+		t4_write_reg(adap, MYPF_REG(A_SGE_PF_KDOORBELL),
+			     V_QID(q->cntxt_id) | V_PIDX(n));
+	}
+	q->db_pidx = q->pidx;
+	spin_unlock(&q->db_lock);
+}
+
+/**
+ * 	inline_tx_skb - inline a packet's data into Tx descriptors
+ * 	@skb: the packet
+ * 	@q: the Tx queue where the packet will be inlined
+ * 	@pos: starting position in the Tx queue where to inline the packet
+ *
+ *	Inline a packet's contents directly into Tx descriptors, starting at
+ *	the given position within the Tx DMA ring.
+ *	Most of the complexity of this operation is dealing with wrap arounds
+ *	in the middle of the packet we want to inline.
+ */
+static void inline_tx_skb(const struct sk_buff *skb, const struct sge_txq *q,
+			  void *pos)
+{
+	u64 *p;
+	int left = (void *)q->stat - pos;
+
+	if (likely(skb->len <= left)) {
+		if (likely(!skb->data_len))
+			skb_copy_from_linear_data(skb, pos, skb->len);
+		else
+			skb_copy_bits(skb, 0, pos, skb->len);
+		pos += skb->len;
+	} else {
+		skb_copy_bits(skb, 0, pos, left);
+		skb_copy_bits(skb, left, q->desc, skb->len - left);
+		pos = (void *)q->desc + (skb->len - left);
+	}
+
+	/* 0-pad to multiple of 16 */
+	p = PTR_ALIGN(pos, 8);
+	if ((uintptr_t)p & 8)
+		*p = 0;
+}
+
+/*
+ * Figure out what HW csum a packet wants and return the appropriate control
+ * bits.
+ */
+static u64 hwcsum(const struct sk_buff *skb)
+{
+	int csum_type;
+	const struct iphdr *iph = ip_hdr(skb);
+
+	if (iph->version == 4) {
+		if (iph->protocol == IPPROTO_TCP)
+			csum_type = TX_CSUM_TCPIP;
+		else if (iph->protocol == IPPROTO_UDP)
+			csum_type = TX_CSUM_UDPIP;
+		else {
+nocsum:			/*
+			 * unknown protocol, disable HW csum
+			 * and hope a bad packet is detected
+			 */
+			return F_TXPKT_L4CSUM_DIS;
+		}
+	} else {
+		/*
+		 * this doesn't work with extension headers
+		 */
+		const struct ipv6hdr *ip6h = (const struct ipv6hdr *)iph;
+
+		if (ip6h->nexthdr == IPPROTO_TCP)
+			csum_type = TX_CSUM_TCPIP6;
+		else if (ip6h->nexthdr == IPPROTO_UDP)
+			csum_type = TX_CSUM_UDPIP6;
+		else
+			goto nocsum;
+	}
+
+	if (likely(csum_type >= TX_CSUM_TCPIP))
+		return V_TXPKT_CSUM_TYPE(csum_type) |
+			V_TXPKT_IPHDR_LEN(skb_network_header_len(skb)) |
+			V_TXPKT_ETHHDR_LEN(skb_network_offset(skb) - ETH_HLEN);
+	else {
+		int start = skb_transport_offset(skb);
+
+		return V_TXPKT_CSUM_TYPE(csum_type) |
+			V_TXPKT_CSUM_START(start) |
+			V_TXPKT_CSUM_LOC(start + skb->csum_offset);
+	}
+}
+
+#if 0
+/*
+ * Returns a pointer to the Tx descriptor at the start of the 8th most recent
+ * packet.
+ */
+static struct tx_desc *wakeup_desc(const struct sge_txq *q)
+{
+	unsigned int n;
+
+	n = (q->recent & 0x0f0f0f0f) + ((q->recent >> 4) & 0x0f0f0f0f);
+	n = (n & 0x00ff00ff) + ((n >> 8) & 0x00ff00ff);
+	n = (n & 0xffff) + (n >> 16);
+	n = q->pidx >= n ? q->pidx - n : q->pidx - n + q->size;
+	return &q->desc[n];
+}
+#endif
+
+static void eth_txq_stop(struct sge_eth_txq *q)
+{
+	netif_tx_stop_queue(q->txq);
+	q->q.stops++;
+}
+
+static inline void txq_advance(struct sge_txq *q, unsigned int n)
+{
+	q->in_use += n;
+	q->pidx += n;
+	if (q->pidx >= q->size)
+		q->pidx -= q->size;
+}
+
+#define MAX_COALESCE_LEN 64000
+
+static inline int wraps_around(struct sge_txq *q, int ndesc)
+{
+	return (q->pidx + ndesc) > q->size ? 1 : 0;
+}
+
+/**
+ * 	ship_tx_pkt_coalesce_wr - finalizes and ships a coalesce WR
+ * 	@ adap: adapter structure
+ * 	@txq: tx queue
+ *
+ * 	writes the different fields of the pkts WR and sends it.
+ */
+static inline int ship_tx_pkt_coalesce_wr(struct adapter *adap, struct sge_eth_txq *txq)
+{
+	u32 wr_mid;
+	struct sge_txq *q = &txq->q;
+	struct fw_eth_tx_pkts_wr *wr;
+	unsigned int ndesc;
+
+	/* fill the pkts WR header */
+	wr = (void *)&q->desc[q->pidx];
+	wr->op_pkd = htonl(V_FW_WR_OP(FW_ETH_TX_PKTS_WR));
+
+	wr_mid = V_FW_WR_LEN16(DIV_ROUND_UP(q->coalesce.flits, 2));
+	ndesc = flits_to_desc(q->coalesce.flits);
+	
+	if (q->coalesce.intr) {
+		wr_mid |= F_FW_WR_EQUEQ | F_FW_WR_EQUIQ;
+		q->coalesce.intr = false;
+	}
+
+	wr->equiq_to_len16 = htonl(wr_mid);
+	wr->plen = cpu_to_be16(q->coalesce.len);
+	wr->npkt = q->coalesce.idx;
+	wr->r3 = 0;
+	wr->type = q->coalesce.type;
+
+	/* zero out coalesce structure members */
+	q->coalesce.idx = 0;
+	q->coalesce.flits = 0;
+	q->coalesce.len = 0;
+
+	txq_advance(q, ndesc);
+	txq->coal_wr++;
+	txq->coal_pkts += wr->npkt;
+	ring_tx_db(adap, q, ndesc);
+
+	return 1;
+}
+
+int t4_sge_coalesce_handler(struct adapter *adap, struct sge_eth_txq *eq)
+{
+	struct sge_txq *q = &eq->q;
+	int hw_cidx = ntohs(q->stat->cidx);
+	int in_use = q->pidx - hw_cidx + flits_to_desc(q->coalesce.flits);
+
+	/* in_use is what the hardware hasn't processed yet and not
+	 * the tx descriptors not yet freed */
+	if (in_use < 0)
+		in_use += q->size;
+
+	/* if the queue is stopped and half the descritors were consumed
+	 * by the hw, restart the queue */
+	if (netif_tx_queue_stopped(eq->txq) && in_use < (eq->q.size >> 1)) {
+		netif_tx_wake_queue(eq->txq);
+		eq->q.restarts++;
+	} else if (!netif_tx_queue_stopped(eq->txq) && in_use >= (eq->q.size >> 1))
+		eq->q.coalesce.intr = true;
+
+	if (eq->q.coalesce.idx && __netif_tx_trylock(eq->txq)){
+		if (eq->q.coalesce.idx)
+			ship_tx_pkt_coalesce_wr(adap, eq);
+		__netif_tx_unlock(eq->txq);
+	}
+	return 1; 
+}
+
+/**
+ * 	should_tx_packet_coalesce - decides wether to coalesce an skb or not
+ * 	@txq: tx queue where the skb is sent
+ * 	@skb: skb to be sent
+ * 	@nflits: return value for number of flits needed
+ * 	@adap: adapter structure
+ *
+ *	This function decides if a packet should be coalesced or not. We start
+ *	coalescing if half of the descriptors in a tx queue are used and stop
+ *	when the number of used descriptors falls down to one fourth of the txq.
+ */
+
+static inline int should_tx_packet_coalesce(struct sge_eth_txq *txq, struct sk_buff *skb,
+					    int *nflits, struct adapter *adap)
+{
+	struct skb_shared_info *si = skb_shinfo(skb);
+	struct sge_txq *q = &txq->q;
+	unsigned int flits, ndesc;
+	unsigned char type = 0;
+	int credits, hw_cidx = ntohs(q->stat->cidx);
+	int in_use = q->pidx - hw_cidx + flits_to_desc(q->coalesce.flits);
+
+	/* use coal WR type 1 when no frags are present */
+	type = (si->nr_frags == 0) ? 1 : 0;
+
+	if (in_use < 0)
+		in_use += q->size;
+
+	if (unlikely(type != q->coalesce.type && q->coalesce.idx))
+		ship_tx_pkt_coalesce_wr(adap, txq);
+
+	/* calculate the number of flits required for coalescing this packet
+	 * without the 2 flits of the WR header. These are added further down
+	 * if we are just starting in new PKTS WR. sgl_len doesn't account for
+	 * the possible 16 bytes alignment ULP TX commands so we do it here.
+	 */
+	flits = (sgl_len(si->nr_frags + 1) + 1) & ~1U;
+	if (type == 0)
+		flits += (sizeof(struct ulp_txpkt) +
+			  sizeof(struct ulptx_idata)) / sizeof(__be64);
+	flits += sizeof(struct cpl_tx_pkt_core) / sizeof(__be64);
+	*nflits = flits;
+
+	/* if we're using less than 64 descriptors and the tx_coal module parameter
+	 * is not equal to 2 stop coalescing and ship any pending WR */
+	if ((adap->tx_coal != 2) && in_use < 64) {
+		if (q->coalesce.idx)
+			ship_tx_pkt_coalesce_wr(adap, txq);
+		q->coalesce.ison = false;
+
+		return 0;
+	}
+
+	/* we don't bother coalescing gso packets or skb larger than 4K*/
+	if (si->gso_size || skb->len > MAX_SKB_COALESCE_LEN) {
+		if (q->coalesce.idx)
+			ship_tx_pkt_coalesce_wr(adap, txq);
+		return 0;
+	}
+
+	/* if coalescing is on, the skb is added to a pkts WR. Otherwise,
+	 * if the queue is half full we turn coalescing on but send this
+	 * skb through the normal path to request a completion interrupt.
+	 * if the queue is not half full we just send the skb through the
+	 * normal path. */
+	if (q->coalesce.ison) {
+		if (q->coalesce.idx) {
+			ndesc = DIV_ROUND_UP(q->coalesce.flits + flits, 8);
+			credits = txq_avail(q) - ndesc;
+			/* If credits are not available for this skb, send the
+			 * already coalesced skbs and let the non-coalesce pass
+			 * handle stopping the queue.
+			 */
+			if (unlikely(credits < ETHTXQ_STOP_THRES ||
+				     wraps_around(q, ndesc))) {
+				ship_tx_pkt_coalesce_wr(adap, txq);
+				return 0;
+			}
+			/* If the max coalesce len or the max WR len is reached
+			 * ship the WR and keep coalescing on.
+			 */
+			if (unlikely((q->coalesce.len + skb->len > 
+				      MAX_COALESCE_LEN) ||
+				     (q->coalesce.flits + flits >
+				      q->coalesce.max))) {
+				ship_tx_pkt_coalesce_wr(adap, txq);
+				goto new;
+			}
+			return 1;
+		} else
+			goto new;
+			
+	} else if ((adap->tx_coal == 2 && in_use > 32) ||
+		   in_use > (q->size >> 1)) {
+		/* start coalescing and arm completion interrupt */
+		q->coalesce.ison = true;
+		q->coalesce.intr = true;
+		return 0;
+	} else
+		return 0;
+
+new:
+	/* start a new pkts WR, the WR header is not filled below */
+	flits += sizeof(struct fw_eth_tx_pkts_wr) /
+			sizeof(__be64);
+	ndesc = flits_to_desc(q->coalesce.flits + flits);
+	credits = txq_avail(q) - ndesc;
+	if (unlikely((credits < ETHTXQ_STOP_THRES) || wraps_around(q, ndesc)))
+		return 0;
+	q->coalesce.flits += 2;
+	q->coalesce.type = type;
+	q->coalesce.ptr = (unsigned char *) &q->desc[q->pidx] +
+			  2 * sizeof(__be64);
+	return 1;
+}
+
+/**
+ * 	tx_do_packet_coalesce - add an skb to a coalesce WR
+ *	@txq: sge_eth_txq used send the skb
+ *	@skb: skb to be sent
+ *	@flits: flits needed for this skb
+ *	@adap: adapter structure
+ *	@pi: port_info structure
+ *	@addr: mapped address of the skb
+ *
+ *	Adds an skb to be sent as part of a coalesce WR by filling a
+ *	ulp_tx_pkt command, ulp_tx_sc_imm command, cpl message and
+ *	ulp_tx_sc_dsgl command.
+ */
+static inline int tx_do_packet_coalesce(struct sge_eth_txq *txq,
+					struct sk_buff *skb,
+					int flits, struct adapter *adap,
+					const struct port_info *pi,
+					dma_addr_t *addr)
+{
+	u64 cntrl, *end;
+	struct sge_txq *q = &txq->q;
+	struct ulp_txpkt *mc;
+	struct ulptx_idata *sc_imm;
+	struct cpl_tx_pkt_core *cpl;
+	struct tx_sw_desc *sd;
+	unsigned int idx = q->coalesce.idx, len = skb->len;
+
+	if (q->coalesce.type == 0) {
+		mc = (struct ulp_txpkt *) q->coalesce.ptr;
+		mc->cmd_dest = htonl(V_ULPTX_CMD(4) | V_ULP_TXPKT_DEST(0) |
+				V_ULP_TXPKT_FID(adap->sge.fw_evtq.cntxt_id) |
+				F_ULP_TXPKT_RO);
+		mc->len = htonl(DIV_ROUND_UP(flits, 2));
+
+		sc_imm = (struct ulptx_idata *) (mc + 1);
+		sc_imm->cmd_more = htonl(V_ULPTX_CMD(ULP_TX_SC_IMM) | F_ULP_TX_SC_MORE);
+		sc_imm->len = htonl(sizeof(*cpl));
+		end = (u64 *) mc + flits;
+		cpl = (struct cpl_tx_pkt_core *) (sc_imm + 1);
+	} else {
+		end = (u64 *) q->coalesce.ptr + flits;
+		cpl = (struct cpl_tx_pkt_core *) q->coalesce.ptr;
+	}
+
+	/* update coalesce structure for this txq */
+	q->coalesce.flits += flits;
+	q->coalesce.ptr += flits * sizeof(__be64);
+	q->coalesce.len += skb->len;
+
+	/* fill the cpl message, same as in t4_eth_xmit, this should be kept
+	 * similar to t4_eth_xmit
+	 */
+	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+		cntrl = hwcsum(skb) | F_TXPKT_IPCSUM_DIS;
+		txq->tx_cso++;
+	} else
+		cntrl = F_TXPKT_L4CSUM_DIS | F_TXPKT_IPCSUM_DIS;
+
+	if (vlan_tx_tag_present(skb)) {
+		txq->vlan_ins++;
+		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(vlan_tx_tag_get(skb));
+	}
+
+	cpl->ctrl0 = htonl(V_TXPKT_OPCODE(CPL_TX_PKT_XT) |
+			   V_TXPKT_INTF(pi->tx_chan) |
+			   V_TXPKT_PF(adap->pf));
+	cpl->pack = htons(0);
+	cpl->len = htons(len);
+	cpl->ctrl1 = cpu_to_be64(cntrl);
+
+	write_sgl(skb, q, (struct ulptx_sgl *)(cpl + 1), end, 0,
+		  addr);
+	skb_orphan(skb);
+
+	/* store pointers to the skb and the sgl used in free_tx_desc.
+	 * each tx desc can hold two pointers corresponding to the value
+	 * of ETH_COALESCE_PKT_PER_DESC */
+	sd = &q->sdesc[q->pidx + (idx >> 1)];
+	sd->coalesce.skb[idx & 1] = skb;
+	sd->coalesce.sgl[idx & 1] = (struct ulptx_sgl *)(cpl + 1);
+	sd->coalesce.idx = (idx & 1) + 1;
+
+	/* send the coaelsced work request if max reached */
+	if (++q->coalesce.idx == ETH_COALESCE_PKT_NUM)
+		ship_tx_pkt_coalesce_wr(adap, txq);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ *	t4_eth_xmit - add a packet to an Ethernet Tx queue
+ *	@skb: the packet
+ *	@dev: the egress net device
+ *
+ *	Add a packet to an SGE Ethernet Tx queue.  Runs with softirqs disabled.
+ */
+int t4_eth_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	u32 wr_mid;
+	u64 cntrl, *end;
+	int qidx, credits;
+	unsigned int flits, ndesc, cflits;
+	struct adapter *adap;
+	struct sge_eth_txq *q;
+	const struct port_info *pi;
+	struct fw_eth_tx_pkt_wr *wr;
+	struct cpl_tx_pkt_core *cpl;
+	const struct skb_shared_info *ssi;
+	dma_addr_t addr[MAX_SKB_FRAGS + 1];
+
+	/*
+	 * The chip min packet length is 10 octets but play safe and reject
+	 * anything shorter than an Ethernet header.
+	 */
+	if (unlikely(skb->len < ETH_HLEN)) {
+out_free:	dev_kfree_skb(skb);
+		return NETDEV_TX_OK;
+	}
+
+	pi = netdev_priv(dev);
+	adap = pi->adapter;
+	qidx = skb_get_queue_mapping(skb);
+	q = &adap->sge.ethtxq[qidx + pi->first_qset];
+
+	reclaim_completed_tx(adap, &q->q, true);
+
+	/* align the end fo coalesce WR to a 512 byte boundary */
+	q->q.coalesce.max = (8 - (q->q.pidx & 7)) * 8;
+
+	/* check if we can do packet coalescing */
+	if (adap->tx_coal && should_tx_packet_coalesce(q, skb, &cflits, adap)) {
+		if (unlikely(map_skb(adap->pdev_dev, skb, addr) < 0)) {
+			q->mapping_err++;
+			goto out_free;
+		}
+		return tx_do_packet_coalesce(q, skb, cflits, adap, pi, addr);
+	}
+
+	flits = calc_tx_flits(skb);
+	ndesc = flits_to_desc(flits);
+	credits = txq_avail(&q->q) - ndesc;
+
+	if (unlikely(credits < 0)) {
+		eth_txq_stop(q);
+		dev_err(adap->pdev_dev,
+			"%s: Tx ring %u full while queue awake!\n",
+			dev->name, qidx);
+		return NETDEV_TX_BUSY;
+	}
+
+	if (!is_eth_imm(skb) &&
+	    unlikely(map_skb(adap->pdev_dev, skb, addr) < 0)) {
+		q->mapping_err++;
+		goto out_free;
+	}
+
+	wr_mid = V_FW_WR_LEN16(DIV_ROUND_UP(flits, 2));
+	if (unlikely(credits < ETHTXQ_STOP_THRES)) {
+		eth_txq_stop(q);
+		wr_mid |= F_FW_WR_EQUEQ | F_FW_WR_EQUIQ;
+	}
+
+	/* request tx completion if needed for tx coalescing */
+	if (adap->tx_coal && q->q.coalesce.intr) {
+		wr_mid |= F_FW_WR_EQUEQ | F_FW_WR_EQUIQ;
+		q->q.coalesce.intr = false;
+	}
+
+	wr = (void *)&q->q.desc[q->q.pidx];
+	wr->equiq_to_len16 = htonl(wr_mid);
+	wr->r3 = cpu_to_be64(0);
+	end = (u64 *)wr + flits;
+
+	ssi = skb_shinfo(skb);
+	if (ssi->gso_size) {
+		struct cpl_tx_pkt_lso_core *lso = (void *)(wr + 1);
+		bool v6 = (ssi->gso_type & SKB_GSO_TCPV6) != 0;
+		int l3hdr_len = skb_network_header_len(skb);
+		int eth_xtra_len = skb_network_offset(skb) - ETH_HLEN;
+
+		wr->op_immdlen = htonl(V_FW_WR_OP(FW_ETH_TX_PKT_WR) |
+				     V_FW_WR_IMMDLEN(sizeof(*lso) +
+								sizeof(*cpl)));
+		lso->lso_ctrl = htonl(V_LSO_OPCODE(CPL_TX_PKT_LSO) |
+					F_LSO_FIRST_SLICE | F_LSO_LAST_SLICE |
+					V_LSO_IPV6(v6) |
+					V_LSO_ETHHDR_LEN(eth_xtra_len / 4) |
+					V_LSO_IPHDR_LEN(l3hdr_len / 4) |
+					V_LSO_TCPHDR_LEN(tcp_hdr(skb)->doff));
+		lso->ipid_ofst = htons(0);
+		lso->mss = htons(ssi->gso_size);
+		lso->seqno_offset = htonl(0);
+		lso->len = htonl(skb->len);
+		cpl = (void *)(lso + 1);
+		cntrl = V_TXPKT_CSUM_TYPE(v6 ? TX_CSUM_TCPIP6 : TX_CSUM_TCPIP) |
+			V_TXPKT_IPHDR_LEN(l3hdr_len) |
+			V_TXPKT_ETHHDR_LEN(eth_xtra_len);
+		q->tso++;
+		q->tx_cso += ssi->gso_segs;
+	} else {
+		int len;
+
+		len = is_eth_imm(skb) ? skb->len + sizeof(*cpl) : sizeof(*cpl);
+		wr->op_immdlen = htonl(V_FW_WR_OP(FW_ETH_TX_PKT_WR) |
+				       V_FW_WR_IMMDLEN(len));
+		cpl = (void *)(wr + 1);
+		if (skb->ip_summed == CHECKSUM_PARTIAL) {
+			cntrl = hwcsum(skb) | F_TXPKT_IPCSUM_DIS;
+			q->tx_cso++;
+		} else
+			cntrl = F_TXPKT_L4CSUM_DIS | F_TXPKT_IPCSUM_DIS;
+	}
+
+	if (vlan_tx_tag_present(skb)) {
+		q->vlan_ins++;
+		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(vlan_tx_tag_get(skb));
+	}
+
+	cpl->ctrl0 = htonl(V_TXPKT_OPCODE(CPL_TX_PKT_XT) |
+			   V_TXPKT_INTF(pi->tx_chan) |
+			   V_TXPKT_PF(adap->pf));
+	cpl->pack = htons(0);
+	cpl->len = htons(skb->len);
+	cpl->ctrl1 = cpu_to_be64(cntrl);
+
+#ifdef T4_TRACE
+	T4_TRACE5(adap->tb[q->q.cntxt_id & 7],
+		  "eth_xmit: ndesc %u, credits %u, pidx %u, len %u, frags %u",
+		  ndesc, credits, q->q.pidx, skb->len, ssi->nr_frags);
+#endif
+
+	if (is_eth_imm(skb)) {
+		inline_tx_skb(skb, &q->q, cpl + 1);
+		dev_kfree_skb(skb);
+	} else {
+		int last_desc;
+
+		write_sgl(skb, &q->q, (struct ulptx_sgl *)(cpl + 1), end, 0,
+			  addr);
+		skb_orphan(skb);
+
+		last_desc = q->q.pidx + ndesc - 1;
+		if (last_desc >= q->q.size)
+			last_desc -= q->q.size;
+		q->q.sdesc[last_desc].skb = skb;
+		q->q.sdesc[last_desc].sgl = (struct ulptx_sgl *)(cpl + 1);
+	}
+
+	txq_advance(&q->q, ndesc);
+
+	dev->trans_start = jiffies;    // XXX removed in newer kernels
+	ring_tx_db(adap, &q->q, ndesc);
+	return NETDEV_TX_OK;
+}
+
+/**
+ *	reclaim_completed_tx_imm - reclaim completed control-queue Tx descs
+ *	@q: the SGE control Tx queue
+ *
+ *	This is a variant of reclaim_completed_tx() that is used for Tx queues
+ *	that send only immediate data (presently just the control queues) and
+ *	thus do not have any sk_buffs to release.
+ */
+static inline void reclaim_completed_tx_imm(struct sge_txq *q)
+{
+	int hw_cidx = ntohs(q->stat->cidx);
+	int reclaim = hw_cidx - q->cidx;
+
+	if (reclaim < 0)
+		reclaim += q->size;
+
+	q->in_use -= reclaim;
+	q->cidx = hw_cidx;
+}
+
+/**
+ *	is_imm - check whether a packet can be sent as immediate data
+ *	@skb: the packet
+ *
+ *	Returns true if a packet can be sent as a WR with immediate data.
+ */
+static inline int is_imm(const struct sk_buff *skb)
+{
+	return skb->len <= MAX_CTRL_WR_LEN;
+}
+
+/**
+ *	ctrlq_check_stop - check if a control queue is full and should stop
+ *	@q: the queue
+ *	@wr: most recent WR written to the queue
+ *
+ *	Check if a control queue has become full and should be stopped.
+ *	We clean up control queue descriptors very lazily, only when we are out.
+ *	If the queue is still full after reclaiming any completed descriptors
+ *	we suspend it and have the last WR wake it up.
+ */
+static void ctrlq_check_stop(struct sge_ctrl_txq *q, struct fw_wr_hdr *wr)
+{
+	reclaim_completed_tx_imm(&q->q);
+	if (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {
+		wr->lo |= htonl(F_FW_WR_EQUEQ | F_FW_WR_EQUIQ);
+		q->q.stops++;
+		q->full = 1;
+	}
+}
+
+/**
+ *	ctrl_xmit - send a packet through an SGE control Tx queue
+ *	@q: the control queue
+ *	@skb: the packet
+ *
+ *	Send a packet through an SGE control Tx queue.  Packets sent through
+ *	a control queue must fit entirely as immediate data.
+ */
+static int ctrl_xmit(struct sge_ctrl_txq *q, struct sk_buff *skb)
+{
+	unsigned int ndesc;
+	struct fw_wr_hdr *wr;
+
+	if (unlikely(!is_imm(skb))) {
+		WARN_ON(1);
+		dev_kfree_skb(skb);
+		return NET_XMIT_DROP;
+	}
+
+	ndesc = DIV_ROUND_UP(skb->len, sizeof(struct tx_desc));
+	spin_lock(&q->sendq.lock);
+
+	if (unlikely(q->full)) {
+		skb->priority = ndesc;                  /* save for restart */
+		__skb_queue_tail(&q->sendq, skb);
+		spin_unlock(&q->sendq.lock);
+		return NET_XMIT_CN;
+	}
+
+	wr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];
+	inline_tx_skb(skb, &q->q, wr);
+
+	txq_advance(&q->q, ndesc);
+	if (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES))
+		ctrlq_check_stop(q, wr);
+
+	q->q.txp++;
+
+	ring_tx_db(q->adap, &q->q, ndesc);
+	spin_unlock(&q->sendq.lock);
+
+	kfree_skb(skb);
+	return NET_XMIT_SUCCESS;
+}
+
+
+/**
+ *	restart_ctrlq - restart a suspended control queue
+ *	@data: the control queue to restart
+ *
+ *	Resumes transmission on a suspended Tx control queue.
+ */
+static void restart_ctrlq(unsigned long data)
+{
+	struct sk_buff *skb;
+	unsigned int written = 0;
+	struct sge_ctrl_txq *q = (struct sge_ctrl_txq *)data;
+
+	spin_lock(&q->sendq.lock);
+	reclaim_completed_tx_imm(&q->q);
+	BUG_ON(txq_avail(&q->q) < TXQ_STOP_THRES);  /* q should be empty */
+
+	while ((skb = __skb_dequeue(&q->sendq)) != NULL) {
+		struct fw_wr_hdr *wr;
+		unsigned int ndesc = skb->priority;     /* previously saved */
+
+		/*
+		 * Write descriptors and free skbs outside the lock to limit
+		 * wait times.  q->full is still set so new skbs will be queued.
+		 */
+		spin_unlock(&q->sendq.lock);
+
+		wr = (struct fw_wr_hdr *)&q->q.desc[q->q.pidx];
+		inline_tx_skb(skb, &q->q, wr);
+		kfree_skb(skb);
+
+		written += ndesc;
+		txq_advance(&q->q, ndesc);
+		if (unlikely(txq_avail(&q->q) < TXQ_STOP_THRES)) {
+			unsigned long old = q->q.stops;
+
+			ctrlq_check_stop(q, wr);
+			if (q->q.stops != old) {          /* suspended anew */
+				spin_lock(&q->sendq.lock);
+				goto ringdb;
+			}
+		}
+		if (written > 16) {
+			ring_tx_db(q->adap, &q->q, written);
+			written = 0;
+		}
+		spin_lock(&q->sendq.lock);
+	}
+	q->full = 0;
+ringdb: if (written)
+		ring_tx_db(q->adap, &q->q, written);
+	spin_unlock(&q->sendq.lock);
+}
+
+/**
+ *	t4_mgmt_tx - send a management message
+ *	@adap: the adapter
+ *	@skb: the packet containing the management message
+ *
+ *	Send a management message through control queue 0.
+ */
+int t4_mgmt_tx(struct adapter *adap, struct sk_buff *skb)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = ctrl_xmit(&adap->sge.ctrlq[0], skb);
+	local_bh_enable();
+	return ret;
+}
+
+/**
+ *	is_ofld_imm - check whether a packet can be sent as immediate data
+ *	@skb: the packet
+ *
+ *	Returns true if a packet can be sent as an offload WR with immediate
+ *	data.  We currently use the same limit as for Ethernet packets.
+ */
+static inline int is_ofld_imm(const struct sk_buff *skb)
+{
+	return skb->len <= MAX_IMM_TX_PKT_LEN;
+}
+
+/**
+ *	calc_tx_flits_ofld - calculate # of flits for an offload packet
+ *	@skb: the packet
+ *
+ * 	Returns the number of flits needed for the given offload packet.
+ * 	These packets are already fully constructed and no additional headers
+ * 	will be added.
+ */
+static inline unsigned int calc_tx_flits_ofld(const struct sk_buff *skb)
+{
+	unsigned int flits, cnt;
+
+	if (is_ofld_imm(skb))
+		return DIV_ROUND_UP(skb->len, 8);
+
+	flits = skb_transport_offset(skb) / 8U;   /* headers */
+	cnt = skb_shinfo(skb)->nr_frags;
+	if (skb->tail != skb->transport_header)
+		cnt++;
+	return flits + sgl_len(cnt);
+}
+
+/**
+ *	txq_stop_maperr - stop a Tx queue due to I/O MMU exhaustion
+ *	@adap: the adapter
+ *	@q: the queue to stop
+ *
+ *	Mark a Tx queue stopped due to I/O MMU exhaustion and resulting
+ *	inability to map packets.  A periodic timer attempts to restart
+ *	queues so marked.
+ */
+static void txq_stop_maperr(struct sge_ofld_txq *q)
+{
+	q->mapping_err++;
+	q->q.stops++;
+	set_bit(q->q.cntxt_id - q->adap->sge.egr_start,
+		q->adap->sge.txq_maperr);
+}
+
+/**
+ *	ofldtxq_stop - stop an offload Tx queue that has become full
+ *	@q: the queue to stop
+ *	@skb: the packet causing the queue to become full
+ *
+ *	Stops an offload Tx queue that has become full and modifies the packet
+ *	being written to request a wakeup.
+ */
+static void ofldtxq_stop(struct sge_ofld_txq *q, struct sk_buff *skb)
+{
+	struct fw_wr_hdr *wr = (struct fw_wr_hdr *)skb->data;
+
+	wr->lo |= htonl(F_FW_WR_EQUEQ | F_FW_WR_EQUIQ);
+	q->q.stops++;
+	q->full = 1;
+}
+
+/**
+ *	service_ofldq - restart a suspended offload queue
+ *	@q: the offload queue
+ *
+ *	Services an offload Tx queue by moving packets from its packet queue
+ *	to the HW Tx ring.  The function starts and ends with the queue locked.
+ */
+static void service_ofldq(struct sge_ofld_txq *q)
+{
+	u64 *pos;
+	int credits;
+	struct sk_buff *skb;
+	unsigned int written = 0;
+	unsigned int flits, ndesc;
+
+	while ((skb = skb_peek(&q->sendq)) != NULL && !q->full) {
+		/*
+		 * We drop the lock but leave skb on sendq, thus retaining
+		 * exclusive access to the state of the queue.
+		 */
+		spin_unlock(&q->sendq.lock);
+
+		reclaim_completed_tx(q->adap, &q->q, false);
+
+		flits = skb->priority;                /* previously saved */
+		ndesc = flits_to_desc(flits);
+		credits = txq_avail(&q->q) - ndesc;
+		BUG_ON(credits < 0);
+		if (unlikely(credits < TXQ_STOP_THRES))
+			ofldtxq_stop(q, skb);
+#ifdef T4_TRACE
+		T4_TRACE5(q->adap->tb[q->q.cntxt_id & 7],
+			  "ofld_xmit: ndesc %u, pidx %u, len %u, main %u, "
+			  "frags %u", ndesc, q->q.pidx, skb->len,
+			  skb->len - skb->data_len, skb_shinfo(skb)->nr_frags);
+#endif
+		pos = (u64 *)&q->q.desc[q->q.pidx];
+		if (is_ofld_imm(skb))
+			inline_tx_skb(skb, &q->q, pos);
+		else if (map_skb(q->adap->pdev_dev, skb,
+				 (dma_addr_t *)skb->head)) {
+			txq_stop_maperr(q);
+			spin_lock(&q->sendq.lock);
+			break;
+		} else {
+			int last_desc, hdr_len = skb_transport_offset(skb);
+
+			/*
+			 * This assumes the WR headers fit within one descriptor
+			 * with room to spare.  Otherwise we need to deal with
+			 * wrap-around here.
+			 */
+			memcpy(pos, skb->data, hdr_len);
+			write_sgl(skb, &q->q, (void *)pos + hdr_len,
+				  pos + flits, hdr_len,
+				  (dma_addr_t *)skb->head);
+
+			if (need_skb_unmap()) {
+				skb->dev = q->adap->port[0];
+				skb->destructor = deferred_unmap_destructor;
+			}
+
+			last_desc = q->q.pidx + ndesc - 1;
+			if (last_desc >= q->q.size)
+				last_desc -= q->q.size;
+			q->q.sdesc[last_desc].skb = skb;
+		}
+
+		txq_advance(&q->q, ndesc);
+		written += ndesc;
+		q->q.txp++;
+		if (unlikely(written > 32)) {
+			ring_tx_db(q->adap, &q->q, written);
+			written = 0;
+		}
+
+		spin_lock(&q->sendq.lock);
+		__skb_unlink(skb, &q->sendq);
+		if (is_ofld_imm(skb))
+			kfree_skb(skb);
+	}
+	if (likely(written))
+		ring_tx_db(q->adap, &q->q, written);
+}
+
+/**
+ *	ofld_xmit - send a packet through an offload queue
+ *	@q: the Tx offload queue
+ *	@skb: the packet
+ *
+ *	Send an offload packet through an SGE offload queue.
+ */
+static int ofld_xmit(struct sge_ofld_txq *q, struct sk_buff *skb)
+{
+	skb->priority = calc_tx_flits_ofld(skb);       /* save for restart */
+	spin_lock(&q->sendq.lock);
+	__skb_queue_tail(&q->sendq, skb);
+	if (q->sendq.qlen == 1)
+		service_ofldq(q);
+	spin_unlock(&q->sendq.lock);
+	return NET_XMIT_SUCCESS;
+}
+
+/**
+ *	restart_ofldq - restart a suspended offload queue
+ *	@data: the offload queue to restart
+ *
+ *	Resumes transmission on a suspended Tx offload queue.
+ */
+static void restart_ofldq(unsigned long data)
+{
+	struct sge_ofld_txq *q = (struct sge_ofld_txq *)data;
+
+	spin_lock(&q->sendq.lock);
+	q->full = 0;            /* the queue actually is completely empty now */
+	service_ofldq(q);
+	spin_unlock(&q->sendq.lock);
+}
+
+/**
+ *	skb_txq - return the Tx queue an offload packet should use
+ *	@skb: the packet
+ *
+ *	Returns the Tx queue an offload packet should use as indicated by bits
+ *	1-15 in the packet's queue_mapping.
+ */
+static inline unsigned int skb_txq(const struct sk_buff *skb)
+{
+	return skb->queue_mapping >> 1;
+}
+
+/**
+ *	is_ctrl_pkt - return whether an offload packet is a control packet
+ *	@skb: the packet
+ *
+ *	Returns whether an offload packet should use an OFLD or a CTRL
+ *	Tx queue as indicated by bit 0 in the packet's queue_mapping.
+ */
+static inline unsigned int is_ctrl_pkt(const struct sk_buff *skb)
+{
+	return skb->queue_mapping & 1;
+}
+
+static inline int ofld_send(struct adapter *adap, struct sk_buff *skb)
+{
+	unsigned int idx = skb_txq(skb);
+
+	if (unlikely(is_ctrl_pkt(skb)))
+		return ctrl_xmit(&adap->sge.ctrlq[idx], skb);
+	return ofld_xmit(&adap->sge.ofldtxq[idx], skb);
+}
+
+/**
+ *	t4_ofld_send - send an offload packet
+ *	@adap: the adapter
+ *	@skb: the packet
+ *
+ *	Sends an offload packet.  We use the packet queue_mapping to select the
+ *	appropriate Tx queue as follows: bit 0 indicates whether the packet
+ *	should be sent as regular or control, bits 1-15 select the queue.
+ */
+int t4_ofld_send(struct adapter *adap, struct sk_buff *skb)
+{
+	int ret;
+
+	local_bh_disable();
+	ret = ofld_send(adap, skb);
+	local_bh_enable();
+	return ret;
+}
+
+/**
+ *	cxgb4_ofld_send - send an offload packet
+ *	@dev: the net device
+ *	@skb: the packet
+ *
+ *	Sends an offload packet.  This is an exported version of @t4_ofld_send,
+ *	intended for ULDs.
+ */
+int cxgb4_ofld_send(struct net_device *dev, struct sk_buff *skb)
+{
+	return t4_ofld_send(netdev2adap(dev), skb);
+}
+EXPORT_SYMBOL(cxgb4_ofld_send);
+
+/**
+ *	pktgl_to_skb_usepages - build an sk_buff from a packet gather list
+ *	@gl: the gather list
+ *	@skb_len: size of sk_buff main body if it carries fragments
+ *	@pull_len: amount of data to move to the sk_buff's main body
+ *
+ *	Builds an sk_buff from the given packet gather list.  Returns the
+ *	sk_buff or %NULL if sk_buff allocation failed.
+ */
+struct sk_buff *t4_pktgl_to_skb_usepages(const struct pkt_gl *gl,
+					 unsigned int skb_len,
+					 unsigned int pull_len)
+{
+	struct sk_buff *skb;
+	struct skb_shared_info *ssi;
+
+	/*
+	 * Below we rely on RX_COPY_THRES being less than the smallest Rx buffer
+	 * size, which is expected since buffers are at least PAGE_SIZEd.
+	 * In this case packets up to RX_COPY_THRES have only one fragment.
+	 */
+	if (gl->tot_len <= RX_COPY_THRES) {
+		skb = dev_alloc_skb(gl->tot_len);
+		if (unlikely(!skb))
+			goto out;
+
+		__skb_put(skb, gl->tot_len);
+		skb_copy_to_linear_data(skb, gl->va, gl->tot_len);
+	} else {
+		skb = dev_alloc_skb(skb_len);
+		if (unlikely(!skb))
+			goto out;
+		__skb_put(skb, pull_len);
+		skb_copy_to_linear_data(skb, gl->va, pull_len);
+
+		ssi = skb_shinfo(skb);
+		ssi->frags[0].page = gl->frags[0].page;
+		ssi->frags[0].page_offset = gl->frags[0].page_offset + pull_len;
+		ssi->frags[0].size = gl->frags[0].size - pull_len;
+		if (gl->nfrags > 1)
+			memcpy(&ssi->frags[1], &gl->frags[1],
+			       (gl->nfrags - 1) * sizeof(skb_frag_t));
+		ssi->nr_frags = gl->nfrags;
+
+		skb->len = gl->tot_len;
+		skb->data_len = skb->len - pull_len;
+		skb->truesize += skb->data_len;
+
+		/* Get a reference for the last page, we don't own it */
+		get_page(gl->frags[gl->nfrags - 1].page);
+	}
+out:	return skb;
+}
+
+
+/**
+ *	pktgl_to_skb_useskbs - build an sk_buff from a packet gather list
+ *	@gl: the gather list
+ *	@skb_len: size of sk_buff main body if it carries fragments
+ *	@pull_len: amount of data to move to the sk_buff's main body
+ *
+ *	Builds an sk_buff from the given packet gather list.  Returns the
+ *	sk_buff or %NULL if sk_buff allocation failed.
+ */
+struct sk_buff *t4_pktgl_to_skb_useskbs(const struct pkt_gl *gl,
+					unsigned int skb_len,
+					unsigned int pull_len)
+{
+	struct sk_buff *skb;
+	unsigned char *dp;
+	int frag;
+
+	/*
+	 * Ideally we'd have a reference to the adapter here so we could use
+	 * adapter->sge.pktshift but we don't have that and the base routine
+	 * t4_pktgl_to_skb() is an exported function so we can't change that
+	 * without breaking binary compatibility.  (Plus it would be annoying
+	 * to have to add a pointer to the adapter in the Packet Gather List.)
+	 * The good news is that the Ingress Padding Boundary is a power of 2
+	 * with the smallest value being 32 bytes and Packet Shift is a value
+	 * in the range [0, 7].  Thus, we can actually quickly compute the
+	 * Packet Shift with a simple binary AND.
+	 */
+	u32 pktshift = (u32)((unsigned long)gl->va & M_PKTSHIFT);
+
+	/*
+	 * If there's only one skb fragment, just return that.
+	 */
+	if (likely(gl->nfrags == 1))
+		return gl->skbs[0];
+
+	/*
+	 * There are multiple skb fragments so we need to create a single new
+	 * skb which contains all the data.  This can happen when the MTU on
+	 * an interface is increased and the ingress packet is received into
+	 * the old smaller MTU buffers which were on the receive ring at the
+	 * time of the MTU change.
+	 */
+	skb = dev_alloc_skb(gl->tot_len + pktshift);
+	if (unlikely(!skb))
+		goto out;
+
+	skb_put(skb, gl->tot_len + pktshift);
+	dp = skb->data;
+	for (frag = 0; frag < gl->nfrags; frag++) {
+		struct sk_buff *sskb = gl->skbs[frag];
+		memcpy(dp, sskb->data, sskb->len);
+		dp += sskb->len;
+		kfree_skb(sskb);
+	}
+
+out:	return skb;
+}
+
+/**
+ *	pktgl_to_skb - build an sk_buff from a packet gather list
+ *	@gl: the gather list
+ *	@skb_len: size of sk_buff main body if it carries fragments
+ *	@pull_len: amount of data to move to the sk_buff's main body
+ *
+ *	Builds an sk_buff from the given packet gather list.  Returns the
+ *	sk_buff or %NULL if sk_buff allocation failed.
+ */
+struct sk_buff *t4_pktgl_to_skb(const struct pkt_gl *gl, unsigned int skb_len,
+				unsigned int pull_len)
+{
+	return (unlikely(gl->useskbs)
+		? t4_pktgl_to_skb_useskbs(gl, skb_len, pull_len)
+		: t4_pktgl_to_skb_usepages(gl, skb_len, pull_len));
+}
+EXPORT_SYMBOL(t4_pktgl_to_skb);
+
+/**
+ *	t4_pktgl_free - free a packet gather list
+ *	@gl: the gather list
+ *
+ *	Releases the buffers of a packet gather list.
+ */
+void t4_pktgl_free(const struct pkt_gl *gl)
+{
+	int n;
+
+	if (unlikely(gl->useskbs)) {
+		for (n = 0; n < gl->nfrags; n++)
+			kfree_skb(gl->skbs[n]);
+	} else {
+		const skb_frag_t *p;
+
+		/*
+		 * We do not own the last page on the list and do not free
+		 * it.
+		 */
+		for (p = gl->frags, n = gl->nfrags - 1; n--; p++)
+			put_page(p->page);
+	}
+}
+
+#ifdef CONFIG_CXGB4_GRO
+/**
+ *	copy_frags - copy fragments from gather list into skb_shared_info
+ *	@si: destination skb shared info structure
+ *	@gl: source internal packet gather list
+ *	@offset: packet start offset in first page
+ *
+ *	Copy an internal packet gather list into a Linux skb_shared_info
+ *	structure.
+ */
+static inline void copy_frags(struct skb_shared_info *si,
+			      const struct pkt_gl *gl,
+			      unsigned int offset)
+{
+	unsigned int n;
+
+	/* usually there's just one frag */
+	si->frags[0].page = gl->frags[0].page;
+	si->frags[0].page_offset = gl->frags[0].page_offset + offset;
+	si->frags[0].size = gl->frags[0].size - offset;
+	si->nr_frags = gl->nfrags;
+
+	n = gl->nfrags - 1;
+	if (n)
+		memcpy(&si->frags[1], &gl->frags[1], n * sizeof(skb_frag_t));
+
+	/* get a reference to the last page, we don't own it */
+	get_page(gl->frags[n].page);
+}
+
+/**
+ *	do_gro - perform Generic Receive Offload ingress packet processing
+ *	@rxq: ingress RX Ethernet Queue
+ *	@gl: gather list for ingress packet
+ *	@pkt: CPL header for last packet fragment
+ *
+ *	Perform Generic Receive Offload (GRO) ingress packet processing.
+ *	We use the standard Linux GRO interfaces for this.
+ */
+static void do_gro(struct sge_eth_rxq *rxq, const struct pkt_gl *gl,
+		   const struct cpl_rx_pkt *pkt)
+{
+	struct adapter *adapter = rxq->rspq.adapter;
+	struct sge *s = &adapter->sge;
+	int ret;
+	struct sk_buff *skb;
+
+	skb = napi_get_frags(&rxq->rspq.napi);
+	if (unlikely(!skb)) {
+		t4_pktgl_free(gl);
+		rxq->stats.rx_drops++;
+		return;
+	}
+
+	copy_frags(skb_shinfo(skb), gl, s->pktshift);
+	skb->len = gl->tot_len - s->pktshift;
+	skb->data_len = skb->len;
+	skb->truesize += skb->data_len;
+	skb->ip_summed = CHECKSUM_UNNECESSARY;
+	skb_record_rx_queue(skb, rxq->rspq.idx);
+
+	if (unlikely(pkt->vlan_ex)) {
+		struct port_info *pi = netdev_priv(rxq->rspq.netdev);
+		struct vlan_group *grp = pi->vlan_grp;
+
+		rxq->stats.vlan_ex++;
+		if (likely(grp)) {
+			ret = vlan_gro_frags(&rxq->rspq.napi, grp,
+					     be16_to_cpu(pkt->vlan));
+			goto stats;
+		}
+	}
+	ret = napi_gro_frags(&rxq->rspq.napi);
+
+stats:
+	if (ret == GRO_HELD)
+		rxq->stats.lro_pkts++;
+	else if (ret == GRO_MERGED || ret == GRO_MERGED_FREE)
+		rxq->stats.lro_merged++;
+	rxq->stats.pkts++;
+	rxq->stats.rx_cso++;
+}
+#endif
+
+/*
+ * Process an MPS trace packet.  Give it an unused protocol number so it won't
+ * be delivered to anyone and send it to the stack for capture.
+ */
+static noinline int handle_trace_pkt(struct adapter *adap,
+				     const struct pkt_gl *gl)
+{
+	struct sk_buff *skb;
+	struct cpl_trace_pkt *p;
+
+	skb = t4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);
+	if (unlikely(!skb)) {
+		t4_pktgl_free(gl);
+		return 0;
+	}
+
+	p = (struct cpl_trace_pkt *)skb->data;
+	__skb_pull(skb, sizeof(*p));
+	skb_reset_mac_header(skb);
+	skb->protocol = htons(0xffff);
+	skb->dev = adap->port[0];
+	netif_receive_skb(skb);
+	return 0;
+}
+
+/**
+ *	t4_ethrx_handler - process an ingress ethernet packet
+ *	@q: the response queue that received the packet
+ *	@rsp: the response queue descriptor holding the RX_PKT message
+ *	@si: the gather list of packet fragments
+ *
+ *	Process an ingress ethernet packet and deliver it to the stack.
+ */
+int t4_ethrx_handler(struct sge_rspq *q, const __be64 *rsp,
+		     const struct pkt_gl *si)
+{
+	struct sk_buff *skb;
+	struct port_info *pi;
+	const struct cpl_rx_pkt *pkt;
+	bool csum_ok;
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);
+	struct adapter *adapter = q->adapter;
+	struct sge *s = &adapter->sge;
+
+	if (unlikely(*(u8 *)rsp == CPL_TRACE_PKT))
+		return handle_trace_pkt(q->adapter, si);
+
+	pkt = (void *)&rsp[1];
+	csum_ok = pkt->csum_calc && !pkt->err_vec;
+
+#ifdef CONFIG_CXGB4_GRO
+	/*
+	 * If this is a good TCP packet and we have Generic Receive Offload
+	 * enabled, handle the packet in the GRO path.
+	 */
+	if ((pkt->l2info & cpu_to_be32(F_RXF_TCP)) &&
+	    (q->netdev->features & NETIF_F_GRO) &&
+	    likely(si->useskbs == 0) &&
+	    csum_ok && !pkt->ip_frag) {
+		do_gro(rxq, si, pkt);
+		return 0;
+	}
+#endif
+	skb = t4_pktgl_to_skb(si, RX_PKT_SKB_LEN, RX_PULL_LEN);
+	if (unlikely(!skb)) {
+		t4_pktgl_free(si);
+		rxq->stats.rx_drops++;
+		return 0;
+	}
+
+	__skb_pull(skb, s->pktshift);      /* remove ethernet header padding */
+	skb->protocol = eth_type_trans(skb, q->netdev);
+	skb_record_rx_queue(skb, q->idx);
+	pi = netdev_priv(skb->dev);
+	rxq->stats.pkts++;
+
+	if (csum_ok && (pi->rx_offload & RX_CSO) &&
+	    (pkt->l2info & htonl(F_RXF_UDP | F_RXF_TCP))) {
+		if (!pkt->ip_frag) {
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			rxq->stats.rx_cso++;
+		} else if (pkt->l2info & htonl(F_RXF_IP)) {
+			__sum16 c = (__force __sum16)pkt->csum;
+			skb->csum = csum_unfold(c);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+			rxq->stats.rx_cso++;
+		}
+	} else
+		skb->ip_summed = CHECKSUM_NONE;
+
+	if (unlikely(pkt->vlan_ex)) {
+		struct vlan_group *grp = pi->vlan_grp;
+
+		rxq->stats.vlan_ex++;
+		if (likely(grp))
+			vlan_hwaccel_receive_skb(skb, grp, ntohs(pkt->vlan));
+		else
+			dev_kfree_skb_any(skb);
+	} else
+		netif_receive_skb(skb);
+
+	return 0;
+}
+
+/**
+ *	restore_rx_bufs - put back a packet's Rx buffers
+ *	@q: the SGE free list
+ *	@frags: number of FL buffers to restore
+ *
+ *	Puts back on an FL the Rx buffers.  The buffers have already been
+ *	unmapped and are left unmapped, we mark them so to prevent further
+ *	unmapping attempts. 
+ *
+ *	This function undoes a series of @unmap_rx_buf calls when we find out
+ *	that the current packet can't be processed right away afterall and we
+ *	need to come back to it later.  This is a very rare event and there's
+ *	no effort to make this particularly efficient.
+ */
+static void restore_rx_bufs(struct sge_fl *q, int frags)
+{
+	struct rx_sw_desc *d;
+
+	while (frags--) {
+		if (q->cidx == 0)
+			q->cidx = q->size - 1;
+		else
+			q->cidx--;
+		d = &q->sdesc[q->cidx];
+		d->dma_addr |= RX_UNMAPPED_BUF;
+		q->avail++;
+	}
+}
+
+/**
+ *	is_new_response - check if a response is newly written
+ *	@r: the response descriptor
+ *	@q: the response queue
+ *
+ *	Returns true if a response descriptor contains a yet unprocessed
+ *	response.
+ */
+static inline bool is_new_response(const struct rsp_ctrl *r,
+				   const struct sge_rspq *q)
+{
+	return (r->u.type_gen >> S_RSPD_GEN) == q->gen;
+}
+
+/**
+ *	rspq_next - advance to the next entry in a response queue
+ *	@q: the queue
+ *
+ *	Updates the state of a response queue to advance it to the next entry.
+ */
+static inline void rspq_next(struct sge_rspq *q)
+{
+	q->cur_desc = (void *)q->cur_desc + q->iqe_len;
+	if (unlikely(++q->cidx == q->size)) {
+		q->cidx = 0;
+		q->gen ^= 1;
+		q->cur_desc = q->desc;
+	}
+}
+
+/**
+ *	process_responses - process responses from an SGE response queue
+ *	@q: the ingress queue to process
+ *	@budget: how many responses can be processed in this round
+ *
+ *	Process responses from an SGE response queue up to the supplied budget.
+ *	Responses include received packets as well as control messages from FW
+ *	or HW.
+ *
+ *	Additionally choose the interrupt holdoff time for the next interrupt
+ *	on this queue.  If the system is under memory shortage use a fairly
+ *	long delay to help recovery.
+ */
+int process_responses(struct sge_rspq *q, int budget)
+{
+	int ret, rsp_type;
+	int budget_left = budget;
+	const struct rsp_ctrl *rc;
+	struct sge_eth_rxq *rxq = container_of(q, struct sge_eth_rxq, rspq);
+	struct adapter *adapter = q->adapter;
+	struct sge *s = &adapter->sge;
+
+	while (likely(budget_left)) {
+		rc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));
+		if (!is_new_response(rc, q))
+			break;
+
+		rmb();
+		rsp_type = G_RSPD_TYPE(rc->u.type_gen);
+		if (likely(rsp_type == X_RSPD_TYPE_FLBUF)) {
+			const struct rx_sw_desc *rsd;
+			u32 len = ntohl(rc->pldbuflen_qid), bufsz, frags;
+			struct pkt_gl si;
+
+			si.useskbs = rxq->useskbs;
+			if (unlikely(si.useskbs)) {
+				struct sk_buff *skb;
+
+				/*
+				 * In "use skbs" mode, we don't pack multiple
+				 * ingress packets per buffer (skb) so we
+				 * should _always_ get a "New Buffer" flags
+				 * from the SGE.  Also, since we hand the
+				 * skb's up to the host stack for it to
+				 * eventually free, we don't release the skb's
+				 * in the driver (in contrast to the "packed
+				 * page" mode where the driver needs to
+				 * release its reference on the page buffers).
+				 */
+				BUG_ON(!(len & F_RSPD_NEWBUF));
+				len = G_RSPD_LEN(len);
+				si.tot_len = len;
+
+				/* gather packet fragments */
+				for (frags = 0; len; frags++) {
+					rsd = &rxq->fl.sdesc[rxq->fl.cidx];
+					bufsz = min(get_buf_size(adapter, rsd),
+						    (int)len);
+					skb = rsd->buf;
+					skb_put(skb, bufsz);
+					si.skbs[frags] = skb;
+					len -= bufsz;
+					unmap_rx_buf(q->adapter, &rxq->fl);
+				}
+
+				si.va = si.skbs[0]->data;
+				prefetch(si.va);
+	
+				si.nfrags = frags;
+				ret = q->handler(q, q->cur_desc, &si);
+				if (unlikely(ret != 0))
+					restore_rx_bufs(&rxq->fl, frags);
+			} else {
+				skb_frag_t *fp;
+
+				if (len & F_RSPD_NEWBUF) {
+					if (likely(q->offset > 0)) {
+						free_rx_bufs(q->adapter,
+							     &rxq->fl, 1);
+						q->offset = 0;
+					}
+					len = G_RSPD_LEN(len);
+				}
+				si.tot_len = len;
+	
+				/* gather packet fragments */
+				for (frags = 0, fp = si.frags; ; frags++, fp++) {
+					rsd = &rxq->fl.sdesc[rxq->fl.cidx];
+					bufsz = min(get_buf_size(adapter, rsd),
+						    (int)len);
+					fp->page = rsd->buf;
+					fp->page_offset = q->offset;
+					fp->size = bufsz;
+					len -= bufsz;
+					if (!len)
+						break;
+					unmap_rx_buf(q->adapter, &rxq->fl);
+				}
+	
+				/*
+				 * Last buffer remains mapped so explicitly
+				 * make it coherent for CPU access.
+				 */
+				dma_sync_single_for_cpu(q->adapter->pdev_dev,
+							get_buf_addr(rsd),
+							fp->size,
+							DMA_FROM_DEVICE);
+	
+					si.va = page_address(si.frags[0].page) +
+						si.frags[0].page_offset;
+	
+				prefetch(si.va);
+	
+				si.nfrags = frags + 1;
+				ret = q->handler(q, q->cur_desc, &si);
+				if (likely(ret == 0))
+					q->offset += ALIGN(fp->size, s->fl_align);
+				else
+					restore_rx_bufs(&rxq->fl, frags);
+			}
+		} else if (likely(rsp_type == X_RSPD_TYPE_CPL)) {
+			ret = q->handler(q, q->cur_desc, NULL);
+		} else {
+			ret = q->handler(q, (const __be64 *)rc, CXGB4_MSG_AN);
+		}
+
+		if (unlikely(ret)) {
+			/* couldn't process descriptor, back off for recovery */
+			q->next_intr_params = V_QINTR_TIMER_IDX(NOMEM_TMR_IDX);
+			break;
+		}
+
+		rspq_next(q);
+		budget_left--;
+	}
+
+	if (q->offset >= 0 && rxq->fl.size - rxq->fl.avail >= 16)
+		__refill_fl(q->adapter, &rxq->fl);
+	return budget - budget_left;
+}
+
+/**
+ *	napi_rx_handler - the NAPI handler for Rx processing
+ *	@napi: the napi instance
+ *	@budget: how many packets we can process in this round
+ *
+ *	Handler for new data events when using NAPI.  This does not need any
+ *	locking or protection from interrupts as data interrupts are off at
+ *	this point and other adapter interrupts do not interfere (the latter
+ *	in not a concern at all with MSI-X as non-data interrupts then have
+ *	a separate handler).
+ */
+static int napi_rx_handler(struct napi_struct *napi, int budget)
+{
+	unsigned int params;
+	struct sge_rspq *q = container_of(napi, struct sge_rspq, napi);
+	int work_done = process_responses(q, budget);
+
+	if (likely(work_done < budget)) {
+		napi_complete(napi);
+		params = q->next_intr_params;
+		q->next_intr_params = q->intr_params;
+	} else
+		params = V_QINTR_TIMER_IDX(X_TIMERREG_UPDATE_CIDX);
+
+	t4_write_reg(q->adapter, MYPF_REG(A_SGE_PF_GTS), V_CIDXINC(work_done) |
+		     V_INGRESSQID((u32)q->cntxt_id) | V_SEINTARM(params));
+	return work_done;
+}
+
+/*
+ * The MSI-X interrupt handler for an SGE response queue.
+ */
+irqreturn_t t4_sge_intr_msix(int irq, void *cookie)
+{
+	struct sge_rspq *q = cookie;
+
+	napi_schedule(&q->napi);
+	return IRQ_HANDLED;
+}
+
+/*
+ * Process the indirect interrupt entries in the interrupt queue and kick off
+ * NAPI for each queue that has generated an entry.
+ */
+static unsigned int process_intrq(struct adapter *adap)
+{
+	unsigned int credits;
+	const struct rsp_ctrl *rc;
+	struct sge_rspq *q = &adap->sge.intrq;
+
+	spin_lock(&adap->sge.intrq_lock);
+	for (credits = 0; ; credits++) {
+		rc = (void *)q->cur_desc + (q->iqe_len - sizeof(*rc));
+		if (!is_new_response(rc, q))
+			break;
+
+		rmb();
+		if (G_RSPD_TYPE(rc->u.type_gen) == X_RSPD_TYPE_INTR) {
+			unsigned int qid = ntohl(rc->pldbuflen_qid);
+
+			qid -= adap->sge.ingr_start;
+			napi_schedule(&adap->sge.ingr_map[qid]->napi);
+		}
+
+		rspq_next(q);
+	}
+
+	t4_write_reg(adap, MYPF_REG(A_SGE_PF_GTS), V_CIDXINC(credits) |
+		     V_INGRESSQID(q->cntxt_id) | V_SEINTARM(q->intr_params));
+	spin_unlock(&adap->sge.intrq_lock);
+	return credits;
+}
+
+/*
+ * The MSI interrupt handler, which handles data events from SGE response queues
+ * as well as error and other async events as they all use the same MSI vector.
+ */
+static irqreturn_t t4_intr_msi(int irq, void *cookie)
+{
+	struct adapter *adap = cookie;
+
+	if (adap->flags & MASTER_PF)
+		t4_slow_intr_handler(adap);
+	process_intrq(adap);
+	return IRQ_HANDLED;
+}
+
+/*
+ * Interrupt handler for legacy INTx interrupts for T4-based cards.
+ * Handles data events from SGE response queues as well as error and other
+ * async events as they all use the same interrupt line.
+ */
+static irqreturn_t t4_intr_intx(int irq, void *cookie)
+{
+	struct adapter *adap = cookie;
+
+	t4_write_reg(adap, MYPF_REG(A_PCIE_PF_CLI), 0);
+	if (((adap->flags & MASTER_PF) && t4_slow_intr_handler(adap)) |
+	    process_intrq(adap))
+		return IRQ_HANDLED;
+	return IRQ_NONE;             /* probably shared interrupt */
+}
+
+/**
+ *	t4_intr_handler - select the top-level interrupt handler
+ *	@adap: the adapter
+ *
+ *	Selects the top-level interrupt handler based on the type of interrupts
+ *	(MSI-X, MSI, or INTx).
+ */
+irq_handler_t t4_intr_handler(adapter_t *adap)
+{
+	if (adap->flags & USING_MSIX)
+		return t4_sge_intr_msix;
+	if (adap->flags & USING_MSI)
+		return t4_intr_msi;
+	return t4_intr_intx;
+}
+
+/**
+ *	sge_rx_timer_cb - perform periodic maintenance of SGE Rx queues
+ *	@data: the adapter
+ *
+ *	Runs periodically from a timer to perform maintenance of SGE Rx queues.
+ *	It performs two tasks:
+ *
+ *	a) Replenishes Rx queues that have run out due to memory shortage.
+ *	Normally new Rx buffers are added as existing ones are consumed but
+ *	when out of memory a queue can become empty.  We schedule NAPI to do
+ *	the actual refill.
+ *
+ *	b) Checks that the SGE is not stuck trying to deliver packets.  This
+ *	typically indicates a programming error that has caused an Rx queue to
+ *	be exhausted.
+ */
+static void sge_rx_timer_cb(unsigned long data)
+{
+	unsigned long m;
+	unsigned int i, state, cnt[2];
+	struct adapter *adap = (struct adapter *)data;
+	struct sge *s = &adap->sge;
+
+	for (i = 0; i < ARRAY_SIZE(s->starving_fl); i++)
+		for (m = s->starving_fl[i]; m; m &= m - 1) {
+			struct sge_eth_rxq *rxq;
+			unsigned int id = __ffs(m) + i * BITS_PER_LONG;
+			struct sge_fl *fl = s->egr_map[id];
+
+			clear_bit(id, s->starving_fl);
+			smp_mb__after_clear_bit();
+
+			/*
+			 * Since we are accessing fl without a lock there's a
+			 * small probability of a false positive where we
+			 * schedule napi but the FL is no longer starving.
+			 * No biggie.
+			 */
+			if (fl_starving(adap, fl)) {
+				rxq = container_of(fl, struct sge_eth_rxq, fl);
+				if (napi_reschedule(&rxq->rspq.napi))
+					fl->starving++;
+				else
+					set_bit(id, s->starving_fl);
+			}
+		}
+
+	t4_write_reg(adap, A_SGE_DEBUG_INDEX, 13);
+	cnt[0] = t4_read_reg(adap, A_SGE_DEBUG_DATA_HIGH);
+	cnt[1] = t4_read_reg(adap, A_SGE_DEBUG_DATA_LOW);
+
+	for (i = 0; i < 2; i++)
+		if (cnt[i] >= s->starve_thres) {
+			if (s->idma_state[i] || cnt[i] == 0xffffffff)
+				continue;
+			s->idma_state[i] = 1;
+			t4_write_reg(adap, A_SGE_DEBUG_INDEX, 0);
+			m = t4_read_reg(adap, A_SGE_DEBUG_DATA_LOW) >> (i * 9);
+			state = m & 0x3f;
+			t4_write_reg(adap, A_SGE_DEBUG_INDEX, 11);
+			m = t4_read_reg(adap, A_SGE_DEBUG_DATA_LOW) >> (i * 16);
+			CH_WARN(adap, "SGE idma%u stuck in state %u, "
+				"queue %lu\n", i, state, m & 0xffff);
+		} else if (s->idma_state[i])
+			s->idma_state[i] = 0;
+
+	mod_timer(&s->rx_timer, jiffies + RX_QCHECK_PERIOD);
+}
+
+/**
+ *	send_flush_wr - send a Flush Work Request on a TX Queue
+ *	@adapter: the adapter
+ *	@txq: TX Queue to flush
+ *
+ *	Send a Flush Work Request on the indicated TX Queue with a request to
+ *	updated the Status Page of the TX Queue when the Flush Work Request
+ *	is processed.  This will allow us to determine when all of the
+ *	preceeding TX Requests have been processed.
+ */
+static void send_flush_wr(struct adapter *adapter, struct sge_eth_txq *txq)
+{
+	int  credits;
+	unsigned int ndesc;
+	struct fw_eq_flush_wr *fwr;
+	struct sk_buff *skb;
+	unsigned int len;
+
+	/*
+	 * See if there's space in the TX Queue to fit the Flush Work Request.
+	 * If not, we simply return.
+	 */
+	len = sizeof *fwr;
+	ndesc = DIV_ROUND_UP(len, sizeof(struct tx_desc));
+	credits = txq_avail(&txq->q) - ndesc;
+	if (unlikely(credits < 0))
+		return;
+
+	/*
+	 * Allocate an skb to hold the Flush Work Request and initialize it 
+	 * with the flush request.
+	 */
+	skb = alloc_skb(len, GFP_ATOMIC);
+	if (unlikely(!skb))
+		return;
+	fwr = (struct fw_eq_flush_wr *)__skb_put(skb, len);
+	memset(fwr, 0, sizeof(*fwr));
+
+	fwr->opcode = htonl(V_FW_WR_OP(FW_EQ_FLUSH_WR));
+	fwr->equiq_to_len16 = cpu_to_be32(F_FW_WR_EQUEQ |
+					  V_FW_WR_LEN16(len / 16));
+
+	/*
+	 * If the Flush Work Request fills up the TX Queue to the point where
+	 * we don't have enough room for a maximum sized TX Request, then
+	 * we need to stop further TX Requests and request that the firmware
+	 * notify us with an interrupt when it processes this request.
+	 */
+	if (unlikely(credits < ETHTXQ_STOP_THRES)) {
+		eth_txq_stop(txq);
+		fwr->equiq_to_len16 |= cpu_to_be32(F_FW_WR_EQUIQ);
+	}
+
+	/*
+	 * Copy the Flush Work Request into the TX Queue and notify the
+	 * hardware that we've given it some more to do ...
+	 */
+	inline_tx_skb(skb, &txq->q, &txq->q.desc[txq->q.pidx]);
+	txq_advance(&txq->q, ndesc);
+	ring_tx_db(adapter, &txq->q, ndesc);
+
+	/*
+	 * Free up the skb and return ...
+	 */
+	kfree_skb(skb);
+	return;
+}
+
+/**
+ *	sge_tx_timer_cb - perform periodic maintenance of SGE Tx queues
+ *	@data: the adapter
+ *
+ *	Runs periodically from a timer to perform maintenance of SGE Tx queues.
+ *	It performs two tasks:
+ *
+ *	a) Restarts offload Tx queues stopped due to I/O MMU mapping errors.
+ *
+ *	b) Reclaims completed Tx packets for the Ethernet queues.  Normally
+ *	packets are cleaned up by new Tx packets, this timer cleans up packets
+ *	when no new packets are being submitted.  This is essential for pktgen,
+ *	at least.
+ */
+static void sge_tx_timer_cb(unsigned long data)
+{
+	unsigned long m, period;
+	unsigned int i, budget;
+	struct adapter *adap = (struct adapter *)data;
+	struct sge *s = &adap->sge;
+
+	for (i = 0; i < ARRAY_SIZE(s->txq_maperr); i++)
+		for (m = s->txq_maperr[i]; m; m &= m - 1) {
+			unsigned long id = __ffs(m) + i * BITS_PER_LONG;
+			struct sge_ofld_txq *txq = s->egr_map[id];
+
+			clear_bit(id, s->txq_maperr);
+			tasklet_schedule(&txq->qresume_tsk);
+		}
+
+	budget = MAX_TIMER_TX_RECLAIM;
+	i = s->ethtxq_rover;
+	do {
+		struct sge_eth_txq *q = &s->ethtxq[i];
+
+		if (__netif_tx_trylock(q->txq)) {
+
+			if (reclaimable(&q->q)) {
+				int avail = reclaimable(&q->q);
+				if (avail > budget)
+					avail = budget;
+
+				free_tx_desc(adap, &q->q, avail, true);
+				q->q.in_use -= avail;
+
+				budget -= avail;
+				if (!budget){
+					__netif_tx_unlock(q->txq);
+					break;
+				}
+			}
+
+			/* if coalescing is on, ship the coal WR */
+			if (q->q.coalesce.idx) {
+				ship_tx_pkt_coalesce_wr(adap, q);
+				q->q.coalesce.ison = false;
+			}
+
+			/*
+			 * If the TX Queue has unreclaimed TX Descriptors and
+			 * the last time anything was sent on the associated
+			 * net device was more than 5 seconds in the past,
+			 * issue a flush request on the TX Queue in order to
+			 * get any stranded skb's off the TX Queue.
+			 */
+			if (q->q.in_use > 0 &&
+			    time_after(jiffies,
+				       q->txq->dev->trans_start + HZ * 5)) {
+				local_bh_disable();
+				send_flush_wr(adap, q);
+				local_bh_enable();
+			}
+			__netif_tx_unlock(q->txq);
+		}
+
+		i++;
+		if (i >= s->ethqsets)
+			i = 0;
+	} while (i != s->ethtxq_rover);
+	s->ethtxq_rover = i;
+	/* if we coalesce all the time, we need to run the timer more often */
+	period = (adap->tx_coal == 2) ? (TX_QCHECK_PERIOD / 20) :
+					TX_QCHECK_PERIOD;
+	
+	/*
+	 * If we found too many reclaimable packets schedule a timer in the
+	 * near future to continue where we left off.  Otherwise the next timer
+	 * will be at its normal interval.
+	 */
+	mod_timer(&s->tx_timer, jiffies + (budget ? period : 2));
+}
+
+/*
+ * @intr_idx: MSI/MSI-X vector if >=0, -(absolute qid + 1) if < 0
+ * @cong: < 0 -> no congestion feedback, >= 0 -> congestion channel map
+ */
+int t4_sge_alloc_rxq(struct adapter *adap, struct sge_rspq *iq, bool fwevtq,
+		     struct net_device *dev, int intr_idx,
+		     struct sge_fl *fl, rspq_handler_t hnd, int cong)
+{
+	int ret, flsz = 0;
+	struct fw_iq_cmd c;
+	struct sge *s = &adap->sge;
+	struct port_info *pi = netdev_priv(dev);
+
+	/* Size needs to be multiple of 16, including status entry. */
+	iq->size = roundup(iq->size, 16);
+
+	iq->desc = alloc_ring(adap->pdev_dev, iq->size, iq->iqe_len, 0,
+			      &iq->phys_addr, NULL, 0);
+	if (!iq->desc)
+		return -ENOMEM;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_IQ_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | F_FW_CMD_EXEC |
+			    V_FW_IQ_CMD_PFN(adap->pf) | V_FW_IQ_CMD_VFN(0));
+	c.alloc_to_len16 = htonl(F_FW_IQ_CMD_ALLOC | F_FW_IQ_CMD_IQSTART |
+				 (sizeof(c) / 16));
+	c.type_to_iqandstindex = htonl(V_FW_IQ_CMD_TYPE(FW_IQ_TYPE_FL_INT_CAP) |
+		V_FW_IQ_CMD_IQASYNCH(fwevtq) | V_FW_IQ_CMD_VIID(pi->viid) |
+		V_FW_IQ_CMD_IQANDST(intr_idx < 0) |
+		V_FW_IQ_CMD_IQANUD(X_UPDATEDELIVERY_INTERRUPT) |
+		V_FW_IQ_CMD_IQANDSTINDEX(intr_idx >= 0 ? intr_idx :
+							-intr_idx - 1));
+	c.iqdroprss_to_iqesize = htons(V_FW_IQ_CMD_IQPCIECH(pi->tx_chan) |
+		F_FW_IQ_CMD_IQGTSMODE |
+		V_FW_IQ_CMD_IQINTCNTTHRESH(iq->pktcnt_idx) |
+		V_FW_IQ_CMD_IQESIZE(ilog2(iq->iqe_len) - 4));
+	c.iqsize = htons(iq->size);
+	c.iqaddr = cpu_to_be64(iq->phys_addr);
+	if (cong >= 0)
+		c.iqns_to_fl0congen = htonl(F_FW_IQ_CMD_IQFLINTCONGEN);
+
+	if (fl) {
+		struct sge_eth_rxq *rxq = container_of(fl, struct sge_eth_rxq,
+						       fl);
+
+		/*
+		 * Allocate the ring for the hardware free list (with space
+		 * for its status page) along with the associated software
+		 * descriptor ring.  The free list size needs to be a multiple
+		 * of the Egress Queue Unit and at least 2 Egress Units larger
+		 * than the SGE's Egress Congrestion Threshold
+		 * (fl_starve_thres - 1).
+		 */
+		if (fl->size < s->fl_starve_thres - 1 + 2*8)
+			fl->size = s->fl_starve_thres - 1 + 2*8;
+		fl->size = roundup(fl->size, 8);
+		fl->desc = alloc_ring(adap->pdev_dev, fl->size, sizeof(__be64),
+				      sizeof(struct rx_sw_desc), &fl->addr,
+				      &fl->sdesc, s->stat_len);
+		if (!fl->desc)
+			goto fl_nomem;
+
+		flsz = fl->size / 8 + s->stat_len / sizeof(struct tx_desc);
+		c.iqns_to_fl0congen |=
+			htonl(V_FW_IQ_CMD_FL0HOSTFCMODE(X_HOSTFCMODE_NONE) |
+			      (unlikely(rxq->useskbs)
+			       ? 0
+			       : F_FW_IQ_CMD_FL0PACKEN) |
+			      F_FW_IQ_CMD_FL0FETCHRO | F_FW_IQ_CMD_FL0DATARO |
+			      F_FW_IQ_CMD_FL0PADEN);
+		if (cong >= 0)
+			c.iqns_to_fl0congen |=
+				htonl(V_FW_IQ_CMD_FL0CNGCHMAP(cong) |
+				      F_FW_IQ_CMD_FL0CONGCIF |
+				      F_FW_IQ_CMD_FL0CONGEN);
+		c.fl0dcaen_to_fl0cidxfthresh =
+			htons(V_FW_IQ_CMD_FL0FBMIN(X_FETCHBURSTMIN_64B) |
+			      V_FW_IQ_CMD_FL0FBMAX(X_FETCHBURSTMAX_512B));
+		c.fl0size = htons(flsz);
+		c.fl0addr = cpu_to_be64(fl->addr);
+	}
+
+	ret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);
+	if (ret)
+		goto err;
+
+	netif_napi_add(dev, &iq->napi, napi_rx_handler, 64);
+	iq->cur_desc = iq->desc;
+	iq->cidx = 0;
+	iq->gen = 1;
+	iq->next_intr_params = iq->intr_params;
+	iq->cntxt_id = ntohs(c.iqid);
+	iq->abs_id = ntohs(c.physiqid);
+	iq->size--;                           /* subtract status entry */
+	iq->netdev = dev;   // XXX use napi.dev in newer kernels
+	iq->handler = hnd;
+
+	/* set offset to -1 to distinguish ingress queues without FL */
+	iq->offset = fl ? 0 : -1;
+
+	adap->sge.ingr_map[iq->cntxt_id - adap->sge.ingr_start] = iq;
+
+	if (fl) {
+		fl->cntxt_id = ntohs(c.fl0id);
+		fl->avail = fl->pend_cred = 0;
+		fl->pidx = fl->cidx = 0;
+		fl->alloc_failed = fl->large_alloc_failed = fl->starving = 0;
+		adap->sge.egr_map[fl->cntxt_id - adap->sge.egr_start] = fl;
+		refill_fl(adap, fl, fl_cap(fl), GFP_KERNEL);
+	}
+	return 0;
+
+fl_nomem:
+	ret = -ENOMEM;
+err:
+	if (iq->desc) {
+		dma_free_coherent(adap->pdev_dev, iq->size * iq->iqe_len,
+				  iq->desc, iq->phys_addr);
+		iq->desc = NULL;
+	}
+	if (fl && fl->desc) {
+		kfree(fl->sdesc);
+		fl->sdesc = NULL;
+		dma_free_coherent(adap->pdev_dev, flsz * sizeof(struct tx_desc),
+				  fl->desc, fl->addr);
+		fl->desc = NULL;
+	}
+	return ret;
+}
+
+static void init_txq(struct adapter *adap, struct sge_txq *q, unsigned int id)
+{
+	q->in_use = 0;
+	q->cidx = q->pidx = 0;
+	q->stops = q->restarts = 0;
+	q->coalesce.idx = q->coalesce.flits = 0;
+	q->coalesce.ison = q->coalesce.intr = false;
+	q->stat = (void *)&q->desc[q->size];
+	q->cntxt_id = id;
+	q->txp = 0;
+	spin_lock_init(&q->db_lock);
+	adap->sge.egr_map[id - adap->sge.egr_start] = q;
+}
+
+int t4_sge_alloc_eth_txq(struct adapter *adap, struct sge_eth_txq *txq,
+			 struct net_device *dev, struct netdev_queue *netdevq,
+			 unsigned int iqid)
+{
+	int ret, nentries;
+	struct fw_eq_eth_cmd c;
+	struct sge *s = &adap->sge;
+	struct port_info *pi = netdev_priv(dev);
+
+	/* Add status entries */
+	nentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);
+
+	txq->q.desc = alloc_ring(adap->pdev_dev, txq->q.size,
+			sizeof(struct tx_desc), sizeof(struct tx_sw_desc),
+			&txq->q.phys_addr, &txq->q.sdesc, s->stat_len);
+	if (!txq->q.desc)
+		return -ENOMEM;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_ETH_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | F_FW_CMD_EXEC |
+			    V_FW_EQ_ETH_CMD_PFN(adap->pf) |
+			    V_FW_EQ_ETH_CMD_VFN(0));
+	c.alloc_to_len16 = htonl(F_FW_EQ_ETH_CMD_ALLOC |
+				 F_FW_EQ_ETH_CMD_EQSTART | (sizeof(c) / 16));
+	c.viid_pkd = htonl(V_FW_EQ_ETH_CMD_VIID(pi->viid));
+	c.fetchszm_to_iqid =
+		htonl(V_FW_EQ_ETH_CMD_HOSTFCMODE(X_HOSTFCMODE_STATUS_PAGE) |
+		      V_FW_EQ_ETH_CMD_PCIECHN(pi->tx_chan) |
+		      F_FW_EQ_ETH_CMD_FETCHRO | V_FW_EQ_ETH_CMD_IQID(iqid));
+	c.dcaen_to_eqsize =
+		htonl(V_FW_EQ_ETH_CMD_FBMIN(X_FETCHBURSTMIN_64B) |
+		      V_FW_EQ_ETH_CMD_FBMAX(X_FETCHBURSTMAX_512B) |
+		      V_FW_EQ_ETH_CMD_CIDXFTHRESH(X_CIDXFLUSHTHRESH_32) |
+		      F_FW_EQ_ETH_CMD_CIDXFTHRESHO |
+		      V_FW_EQ_ETH_CMD_EQSIZE(nentries));
+	c.eqaddr = cpu_to_be64(txq->q.phys_addr);
+
+	ret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);
+	if (ret) {
+		kfree(txq->q.sdesc);
+		txq->q.sdesc = NULL;
+		dma_free_coherent(adap->pdev_dev,
+				  nentries * sizeof(struct tx_desc),
+				  txq->q.desc, txq->q.phys_addr);
+		txq->q.desc = NULL;
+		return ret;
+	}
+
+	init_txq(adap, &txq->q, G_FW_EQ_ETH_CMD_EQID(ntohl(c.eqid_pkd)));
+	txq->txq = netdevq;
+	txq->tso = txq->tx_cso = txq->vlan_ins = 0;
+	txq->mapping_err = 0;
+	return 0;
+}
+
+int t4_sge_alloc_ctrl_txq(struct adapter *adap, struct sge_ctrl_txq *txq,
+			  struct net_device *dev, unsigned int iqid,
+			  unsigned int cmplqid)
+{
+	int ret, nentries;
+	struct fw_eq_ctrl_cmd c;
+	struct sge *s = &adap->sge;
+	struct port_info *pi = netdev_priv(dev);
+
+	/* Add status entries */
+	nentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);
+
+	txq->q.desc = alloc_ring(adap->pdev_dev, nentries,
+				 sizeof(struct tx_desc), 0, &txq->q.phys_addr,
+				 NULL, 0);
+	if (!txq->q.desc)
+		return -ENOMEM;
+
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_CTRL_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | F_FW_CMD_EXEC |
+			    V_FW_EQ_CTRL_CMD_PFN(adap->pf) |
+			    V_FW_EQ_CTRL_CMD_VFN(0));
+	c.alloc_to_len16 = htonl(F_FW_EQ_CTRL_CMD_ALLOC |
+				 F_FW_EQ_CTRL_CMD_EQSTART | (sizeof(c) / 16));
+	c.cmpliqid_eqid = htonl(V_FW_EQ_CTRL_CMD_CMPLIQID(cmplqid));
+	c.physeqid_pkd = htonl(0);
+	c.fetchszm_to_iqid =
+		htonl(V_FW_EQ_CTRL_CMD_HOSTFCMODE(X_HOSTFCMODE_STATUS_PAGE) |
+		      V_FW_EQ_CTRL_CMD_PCIECHN(pi->tx_chan) |
+		      F_FW_EQ_CTRL_CMD_FETCHRO | V_FW_EQ_CTRL_CMD_IQID(iqid));
+	c.dcaen_to_eqsize =
+		htonl(V_FW_EQ_CTRL_CMD_FBMIN(X_FETCHBURSTMIN_64B) |
+		      V_FW_EQ_CTRL_CMD_FBMAX(X_FETCHBURSTMAX_512B) |
+		      V_FW_EQ_CTRL_CMD_CIDXFTHRESH(X_CIDXFLUSHTHRESH_32) |
+		      V_FW_EQ_CTRL_CMD_EQSIZE(nentries));
+	c.eqaddr = cpu_to_be64(txq->q.phys_addr);
+
+	ret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);
+	if (ret) {
+		dma_free_coherent(adap->pdev_dev,
+				  nentries * sizeof(struct tx_desc),
+				  txq->q.desc, txq->q.phys_addr);
+		txq->q.desc = NULL;
+		return ret;
+	}
+
+	init_txq(adap, &txq->q, G_FW_EQ_CTRL_CMD_EQID(ntohl(c.cmpliqid_eqid)));
+	txq->adap = adap;
+	skb_queue_head_init(&txq->sendq);
+	tasklet_init(&txq->qresume_tsk, restart_ctrlq, (unsigned long)txq);
+	txq->full = 0;
+	return 0;
+}
+
+int t4_sge_alloc_ofld_txq(struct adapter *adap, struct sge_ofld_txq *txq,
+			  struct net_device *dev, unsigned int iqid)
+{
+	int ret, nentries;
+	struct fw_eq_ofld_cmd c;
+	struct sge *s = &adap->sge;
+	struct port_info *pi = netdev_priv(dev);
+
+	/* Add status entries */
+	nentries = txq->q.size + s->stat_len / sizeof(struct tx_desc);
+
+	txq->q.desc = alloc_ring(adap->pdev_dev, txq->q.size,
+			sizeof(struct tx_desc), sizeof(struct tx_sw_desc),
+			&txq->q.phys_addr, &txq->q.sdesc, s->stat_len);
+	if (!txq->q.desc)
+		return -ENOMEM;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_OFLD_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | F_FW_CMD_EXEC |
+			    V_FW_EQ_OFLD_CMD_PFN(adap->pf) |
+			    V_FW_EQ_OFLD_CMD_VFN(0));
+	c.alloc_to_len16 = htonl(F_FW_EQ_OFLD_CMD_ALLOC |
+				 F_FW_EQ_OFLD_CMD_EQSTART | (sizeof(c) / 16));
+	c.fetchszm_to_iqid =
+		htonl(V_FW_EQ_OFLD_CMD_HOSTFCMODE(X_HOSTFCMODE_STATUS_PAGE) |
+		      V_FW_EQ_OFLD_CMD_PCIECHN(pi->tx_chan) |
+		      F_FW_EQ_OFLD_CMD_FETCHRO | V_FW_EQ_OFLD_CMD_IQID(iqid));
+	c.dcaen_to_eqsize =
+		htonl(V_FW_EQ_OFLD_CMD_FBMIN(X_FETCHBURSTMIN_64B) |
+		      V_FW_EQ_OFLD_CMD_FBMAX(X_FETCHBURSTMAX_512B) |
+		      V_FW_EQ_OFLD_CMD_CIDXFTHRESH(X_CIDXFLUSHTHRESH_32) |
+		      V_FW_EQ_OFLD_CMD_EQSIZE(nentries));
+	c.eqaddr = cpu_to_be64(txq->q.phys_addr);
+
+	ret = t4_wr_mbox(adap, adap->mbox, &c, sizeof(c), &c);
+	if (ret) {
+		kfree(txq->q.sdesc);
+		txq->q.sdesc = NULL;
+		dma_free_coherent(adap->pdev_dev,
+				  nentries * sizeof(struct tx_desc),
+				  txq->q.desc, txq->q.phys_addr);
+		txq->q.desc = NULL;
+		return ret;
+	}
+
+	init_txq(adap, &txq->q, G_FW_EQ_OFLD_CMD_EQID(ntohl(c.eqid_pkd)));
+	txq->adap = adap;
+	skb_queue_head_init(&txq->sendq);
+	tasklet_init(&txq->qresume_tsk, restart_ofldq, (unsigned long)txq);
+	txq->full = 0;
+	txq->mapping_err = 0;
+
+	return 0;
+}
+
+static void free_txq(struct adapter *adap, struct sge_txq *q)
+{
+	struct sge *s = &adap->sge;
+
+	dma_free_coherent(adap->pdev_dev,
+			  q->size * sizeof(struct tx_desc) + s->stat_len,
+			  q->desc, q->phys_addr);
+	q->cntxt_id = 0;
+	q->sdesc = NULL;
+	q->desc = NULL;
+}
+
+static void free_rspq_fl(struct adapter *adap, struct sge_rspq *rq,
+			 struct sge_fl *fl)
+{
+	struct sge *s = &adap->sge;
+	unsigned int fl_id = fl ? fl->cntxt_id : 0xffff;
+
+	adap->sge.ingr_map[rq->cntxt_id - adap->sge.ingr_start] = NULL;
+	t4_iq_free(adap, adap->mbox, adap->pf, 0, FW_IQ_TYPE_FL_INT_CAP,
+		   rq->cntxt_id, fl_id, 0xffff);
+	dma_free_coherent(adap->pdev_dev, (rq->size + 1) * rq->iqe_len,
+			  rq->desc, rq->phys_addr);
+	netif_napi_del(&rq->napi);
+	rq->netdev = NULL;
+	rq->cntxt_id = rq->abs_id = 0;
+	rq->desc = NULL;
+
+	if (fl) {
+		free_rx_bufs(adap, fl, fl->avail);
+		dma_free_coherent(adap->pdev_dev, fl->size * 8 + s->stat_len,
+				  fl->desc, fl->addr);
+		kfree(fl->sdesc);
+		fl->sdesc = NULL;
+		fl->cntxt_id = 0;
+		fl->desc = NULL;
+	}
+}
+
+/**
+ *	t4_free_ofld_rxqs - free a block of consecutive Rx queues
+ *	@adap: the adapter
+ *	@n: number of queues
+ *	@q: pointer to first queue
+ *
+ *	Release the resources of a consecutive block of offload Rx queues.
+ */
+void t4_free_ofld_rxqs(struct adapter *adap, int n, struct sge_ofld_rxq *q)
+{
+	for ( ; n; n--, q++)
+		if (q->rspq.desc)
+			free_rspq_fl(adap, &q->rspq, &q->fl);
+}
+
+/**
+ *	t4_free_sge_resources - free SGE resources
+ *	@adap: the adapter
+ *
+ *	Frees resources used by the SGE queue sets.
+ */
+void t4_free_sge_resources(struct adapter *adap)
+{
+	int i;
+	struct sge_eth_rxq *eq = adap->sge.ethrxq;
+	struct sge_eth_txq *etq = adap->sge.ethtxq;
+
+	/* clean up Ethernet Tx/Rx queues */
+	for (i = 0; i < adap->sge.ethqsets; i++, eq++, etq++) {
+		if (eq->rspq.desc)
+			free_rspq_fl(adap, &eq->rspq, &eq->fl);
+		if (etq->q.desc) {
+			t4_eth_eq_free(adap, adap->mbox, adap->pf, 0,
+				       etq->q.cntxt_id);
+			free_tx_desc(adap, &etq->q, etq->q.in_use, true);
+			kfree(etq->q.sdesc);
+			free_txq(adap, &etq->q);
+		}
+	}
+
+	/* clean up TOE, RDMA and iSCSI Rx queues */
+	t4_free_ofld_rxqs(adap, adap->sge.ofldqsets, adap->sge.ofldrxq);
+	t4_free_ofld_rxqs(adap, adap->sge.rdmaqs, adap->sge.rdmarxq);
+	t4_free_ofld_rxqs(adap, adap->sge.niscsiq, adap->sge.iscsirxq);
+
+	/* clean up offload Tx queues */
+	for (i = 0; i < ARRAY_SIZE(adap->sge.ofldtxq); i++) {
+		struct sge_ofld_txq *q = &adap->sge.ofldtxq[i];
+
+		if (q->q.desc) {
+			tasklet_kill(&q->qresume_tsk);
+			t4_ofld_eq_free(adap, adap->mbox, adap->pf,
+					0, q->q.cntxt_id);
+			free_tx_desc(adap, &q->q, q->q.in_use, false);
+			kfree(q->q.sdesc);
+			__skb_queue_purge(&q->sendq);
+			free_txq(adap, &q->q);
+		}
+	}
+
+	/* clean up control Tx queues */
+	for (i = 0; i < ARRAY_SIZE(adap->sge.ctrlq); i++) {
+		struct sge_ctrl_txq *cq = &adap->sge.ctrlq[i];
+
+		if (cq->q.desc) {
+			tasklet_kill(&cq->qresume_tsk);
+			t4_ctrl_eq_free(adap, adap->mbox, adap->pf, 0,
+					cq->q.cntxt_id);
+			__skb_queue_purge(&cq->sendq);
+			free_txq(adap, &cq->q);
+		}
+	}
+
+	if (adap->sge.fw_evtq.desc)
+		free_rspq_fl(adap, &adap->sge.fw_evtq, NULL);
+
+	if (adap->sge.intrq.desc)
+		free_rspq_fl(adap, &adap->sge.intrq, NULL);
+
+	/* clear the reverse egress queue map */
+	memset(adap->sge.egr_map, 0, sizeof(adap->sge.egr_map));
+}
+
+void t4_sge_start(struct adapter *adap)
+{
+	adap->sge.ethtxq_rover = 0;
+	mod_timer(&adap->sge.rx_timer, jiffies + RX_QCHECK_PERIOD);
+	mod_timer(&adap->sge.tx_timer, jiffies + TX_QCHECK_PERIOD);
+}
+
+/**
+ *	t4_sge_stop - disable SGE operation
+ *	@adap: the adapter
+ *
+ *	Stop tasklets and timers associated with the DMA engine.  Note that
+ *	this is effective only if measures have been taken to disable any HW
+ *	events that may restart them.
+ */
+void t4_sge_stop(struct adapter *adap)
+{
+	int i;
+	struct sge *s = &adap->sge;
+
+	if (in_interrupt())  /* actions below require waiting */
+		return;
+
+	if (s->rx_timer.function)
+		del_timer_sync(&s->rx_timer);
+	if (s->tx_timer.function)
+		del_timer_sync(&s->tx_timer);
+
+	for (i = 0; i < ARRAY_SIZE(s->ofldtxq); i++) {
+		struct sge_ofld_txq *q = &s->ofldtxq[i];
+
+		if (q->q.desc)
+			tasklet_kill(&q->qresume_tsk);
+	}
+	for (i = 0; i < ARRAY_SIZE(s->ctrlq); i++) {
+		struct sge_ctrl_txq *cq = &s->ctrlq[i];
+
+		if (cq->q.desc)
+			tasklet_kill(&cq->qresume_tsk);
+	}
+}
+
+/**
+ *	t4_sge_init - initialize SGE
+ *	@adap: the adapter
+ *
+ *	Performs SGE initialization needed every time after a chip reset.
+ *	We do not initialize any of the queues here, instead the driver
+ *	top-level must request those individually.
+ *
+ *	Called in two different modes:
+ *
+ *	 1. Perform actual hardware initialization and record hard-coded
+ *	    parameters which were used.  This gets used when we're the
+ *	    Master PF and the Firmware Configuration File support didn't
+ *	    work for some reason.
+ *
+ *	 2. We're not the Master PF or initialization was performed with
+ *	    a Firmware Configuration File.  In this case we need to grab
+ *	    any of the SGE operating parameters that we need to have in
+ *	    order to do our job and make sure we can live with them ...
+ */
+
+static int t4_sge_init_soft(struct adapter *adap)
+{
+	struct sge *s = &adap->sge;
+	u32 fl_small_pg, fl_large_pg, fl_small_mtu, fl_large_mtu;
+	u32 timer_value_0_and_1, timer_value_2_and_3, timer_value_4_and_5;
+	u32 ingress_rx_threshold;
+
+	/*
+	 * Verify that CPL messages are going to the Ingress Queue for
+	 * process_responses() and that only packet data is going to the
+	 * Free Lists.
+	 */
+	if ((t4_read_reg(adap, A_SGE_CONTROL) & F_RXPKTCPLMODE) !=
+	    V_RXPKTCPLMODE(X_RXPKTCPLMODE_SPLIT)) {
+		dev_err(adap->pdev_dev, "bad SGE CPL MODE\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Validate the Host Buffer Register Array indices that we want to
+	 * use ...
+	 *
+	 * XXX Note that we should really read through the Host Buffer Size
+	 * XXX register array and find the indices of the Buffer Sizes which
+	 * XXX meet our needs!
+	 */
+	#define READ_FL_BUF(x) \
+		t4_read_reg(adap, A_SGE_FL_BUFFER_SIZE0+(x)*sizeof(u32))
+
+	fl_small_pg = READ_FL_BUF(RX_SMALL_PG_BUF);
+	fl_large_pg = READ_FL_BUF(RX_LARGE_PG_BUF);
+	fl_small_mtu = READ_FL_BUF(RX_SMALL_MTU_BUF);
+	fl_large_mtu = READ_FL_BUF(RX_LARGE_MTU_BUF);
+
+	#undef READ_FL_BUF
+
+	if (fl_small_pg != PAGE_SIZE ||
+	    (fl_large_pg != 0 && (fl_large_pg <= fl_small_pg ||
+				  (fl_large_pg & (fl_large_pg-1)) != 0))) {
+		dev_err(adap->pdev_dev, "bad SGE FL page buffer sizes [%d, %d]\n",
+			fl_small_pg, fl_large_pg);
+		return -EINVAL;
+	}
+	if (fl_large_pg)
+		s->fl_pg_order = ilog2(fl_large_pg) - PAGE_SHIFT;
+
+	if (fl_small_mtu < FL_MTU_SMALL_BUFSIZE(adap) ||
+	    fl_large_mtu < FL_MTU_LARGE_BUFSIZE(adap)) {
+		dev_err(adap->pdev_dev, "bad SGE FL MTU sizes [%d, %d]\n",
+			fl_small_mtu, fl_large_mtu);
+		return -EINVAL;
+	}
+
+	/*
+	 * Retrieve our RX interrupt holdoff timer values and counter
+	 * threshold values from the SGE parameters.
+	 */
+	timer_value_0_and_1 = t4_read_reg(adap, A_SGE_TIMER_VALUE_0_AND_1);
+	timer_value_2_and_3 = t4_read_reg(adap, A_SGE_TIMER_VALUE_2_AND_3);
+	timer_value_4_and_5 = t4_read_reg(adap, A_SGE_TIMER_VALUE_4_AND_5);
+	s->timer_val[0] = core_ticks_to_us(adap,
+		G_TIMERVALUE0(timer_value_0_and_1));
+	s->timer_val[1] = core_ticks_to_us(adap,
+		G_TIMERVALUE1(timer_value_0_and_1));
+	s->timer_val[2] = core_ticks_to_us(adap,
+		G_TIMERVALUE2(timer_value_2_and_3));
+	s->timer_val[3] = core_ticks_to_us(adap,
+		G_TIMERVALUE3(timer_value_2_and_3));
+	s->timer_val[4] = core_ticks_to_us(adap,
+		G_TIMERVALUE4(timer_value_4_and_5));
+	s->timer_val[5] = core_ticks_to_us(adap,
+		G_TIMERVALUE5(timer_value_4_and_5));
+
+	ingress_rx_threshold = t4_read_reg(adap, A_SGE_INGRESS_RX_THRESHOLD);
+	s->counter_val[0] = G_THRESHOLD_0(ingress_rx_threshold);
+	s->counter_val[1] = G_THRESHOLD_1(ingress_rx_threshold);
+	s->counter_val[2] = G_THRESHOLD_2(ingress_rx_threshold);
+	s->counter_val[3] = G_THRESHOLD_3(ingress_rx_threshold);
+
+	return 0;
+}
+
+static int t4_sge_init_hard(struct adapter *adap)
+{
+	struct sge *s = &adap->sge;
+
+	/*
+	 * Set up our basic SGE mode to deliver CPL messages to our Ingress
+	 * Queue and Packet Date to the Free List.
+	 */
+	t4_set_reg_field(adap, A_SGE_CONTROL, F_RXPKTCPLMODE, F_RXPKTCPLMODE);
+
+	/*
+	 * Set up to drop DOORBELL writes when the DOORBELL FIFO overflows
+	 * and generate an interrupt when this occurs so we can recover.
+	 */
+	t4_set_reg_field(adap, A_SGE_DBFIFO_STATUS, 
+			 V_HP_INT_THRESH(M_HP_INT_THRESH) |
+			 V_LP_INT_THRESH(M_LP_INT_THRESH),
+			 V_HP_INT_THRESH(dbfifo_int_thresh) |
+			 V_LP_INT_THRESH(dbfifo_int_thresh));
+	t4_set_reg_field(adap, A_SGE_DOORBELL_CONTROL, F_ENABLE_DROP,
+			 F_ENABLE_DROP);
+
+	/*
+	 * A_SGE_FL_BUFFER_SIZE0 (RX_SMALL_PG_BUF) is set up by
+	 * t4_fixup_host_params().
+	 */
+	s->fl_pg_order = FL_PG_ORDER;
+	if (s->fl_pg_order)
+		t4_write_reg(adap,
+			     A_SGE_FL_BUFFER_SIZE0+RX_LARGE_PG_BUF*sizeof(u32),
+			     PAGE_SIZE << FL_PG_ORDER);
+	t4_write_reg(adap, A_SGE_FL_BUFFER_SIZE0+RX_SMALL_MTU_BUF*sizeof(u32),
+		     FL_MTU_SMALL_BUFSIZE(adap));
+	t4_write_reg(adap, A_SGE_FL_BUFFER_SIZE0+RX_LARGE_MTU_BUF*sizeof(u32),
+		     FL_MTU_LARGE_BUFSIZE(adap));
+
+	/*
+	 * Note that the SGE Ingress Packet Count Interrupt Threshold and
+	 * Timer Holdoff values must be supplied by our caller.
+	 */
+	t4_write_reg(adap, A_SGE_INGRESS_RX_THRESHOLD,
+		     V_THRESHOLD_0(s->counter_val[0]) |
+		     V_THRESHOLD_1(s->counter_val[1]) |
+		     V_THRESHOLD_2(s->counter_val[2]) |
+		     V_THRESHOLD_3(s->counter_val[3]));
+
+	t4_write_reg(adap, A_SGE_TIMER_VALUE_0_AND_1,
+		     V_TIMERVALUE0(us_to_core_ticks(adap, s->timer_val[0])) |
+		     V_TIMERVALUE1(us_to_core_ticks(adap, s->timer_val[1])));
+	t4_write_reg(adap, A_SGE_TIMER_VALUE_2_AND_3,
+		     V_TIMERVALUE2(us_to_core_ticks(adap, s->timer_val[2])) |
+		     V_TIMERVALUE3(us_to_core_ticks(adap, s->timer_val[3])));
+	t4_write_reg(adap, A_SGE_TIMER_VALUE_4_AND_5,
+		     V_TIMERVALUE4(us_to_core_ticks(adap, s->timer_val[4])) |
+		     V_TIMERVALUE5(us_to_core_ticks(adap, s->timer_val[5])));
+
+	return 0;
+}
+
+int t4_sge_init(struct adapter *adap)
+{
+	struct sge *s = &adap->sge;
+	u32 sge_control;
+	int ret;
+
+	/*
+	 * Ingress Padding Boundary and Egress Status Page Size are set up by
+	 * t4_fixup_host_params().
+	 */
+	sge_control = t4_read_reg(adap, A_SGE_CONTROL);
+	s->pktshift = G_PKTSHIFT(sge_control);
+	s->stat_len = (sge_control & F_EGRSTATUSPAGESIZE) ? 128 : 64;
+	s->fl_align = 1 << (G_INGPADBOUNDARY(sge_control) +
+			    X_INGPADBOUNDARY_SHIFT);
+
+	if (adap->flags & USING_SOFT_PARAMS)
+		ret = t4_sge_init_soft(adap);
+	else
+		ret = t4_sge_init_hard(adap);
+	if (ret < 0)
+		return ret;
+
+        /*
+	 * A FL with <= fl_starve_thres buffers is starving and a periodic
+	 * timer will attempt to refill it.  This needs to be larger than the
+	 * SGE's Egress Congestion Threshold.  If it isn't, then we can get
+	 * stuck waiting for new packets while the SGE is waiting for us to
+	 * give it more Free List entries.  (Note that the SGE's Egress
+	 * Congestion Threshold is in units of 2 Free List pointers.)
+	 */
+	s->fl_starve_thres
+		= G_EGRTHRESHOLD(t4_read_reg(adap, A_SGE_CONM_CTRL))*2 + 1;
+
+	setup_timer(&s->rx_timer, sge_rx_timer_cb, (unsigned long)adap);
+	setup_timer(&s->tx_timer, sge_tx_timer_cb, (unsigned long)adap);
+	s->starve_thres = core_ticks_per_usec(adap) * 1000000;  /* 1 s */
+	s->idma_state[0] = s->idma_state[1] = 0;
+	spin_lock_init(&s->intrq_lock);
+
+	return 0;
+}
diff --git a/drivers/net/cxgb4/t4_hw.c b/drivers/net/cxgb4/t4_hw.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_hw.c
@@ -0,0 +1,5724 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2003-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include "common.h"
+#include "t4_regs.h"
+#include "t4_regs_values.h"
+#include "t4fw_interface.h"
+#include "t4vf_defs.h"
+
+/**
+ *	t4_wait_op_done_val - wait until an operation is completed
+ *	@adapter: the adapter performing the operation
+ *	@reg: the register to check for completion
+ *	@mask: a single-bit field within @reg that indicates completion
+ *	@polarity: the value of the field when the operation is completed
+ *	@attempts: number of check iterations
+ *	@delay: delay in usecs between iterations
+ *	@valp: where to store the value of the register at completion time
+ *
+ *	Wait until an operation is completed by checking a bit in a register
+ *	up to @attempts times.  If @valp is not NULL the value of the register
+ *	at the time it indicated completion is stored there.  Returns 0 if the
+ *	operation completes and	-EAGAIN	otherwise.
+ */
+int t4_wait_op_done_val(struct adapter *adapter, int reg, u32 mask,
+		        int polarity, int attempts, int delay, u32 *valp)
+{
+	while (1) {
+		u32 val = t4_read_reg(adapter, reg);
+
+		if (!!(val & mask) == polarity) {
+			if (valp)
+				*valp = val;
+			return 0;
+		}
+		if (--attempts == 0)
+			return -EAGAIN;
+		if (delay)
+			udelay(delay);
+	}
+}
+
+/**
+ *	t4_set_reg_field - set a register field to a value
+ *	@adapter: the adapter to program
+ *	@addr: the register address
+ *	@mask: specifies the portion of the register to modify
+ *	@val: the new value for the register field
+ *
+ *	Sets a register field specified by the supplied mask to the
+ *	given value.
+ */
+void t4_set_reg_field(struct adapter *adapter, unsigned int addr, u32 mask,
+		      u32 val)
+{
+	u32 v = t4_read_reg(adapter, addr) & ~mask;
+
+	t4_write_reg(adapter, addr, v | val);
+	(void) t4_read_reg(adapter, addr);      /* flush */
+}
+
+/**
+ *	t4_read_indirect - read indirectly addressed registers
+ *	@adap: the adapter
+ *	@addr_reg: register holding the indirect address
+ *	@data_reg: register holding the value of the indirect register
+ *	@vals: where the read register values are stored
+ *	@nregs: how many indirect registers to read
+ *	@start_idx: index of first indirect register to read
+ *
+ *	Reads registers that are accessed indirectly through an address/data
+ *	register pair.
+ */
+void t4_read_indirect(struct adapter *adap, unsigned int addr_reg,
+		      unsigned int data_reg, u32 *vals, unsigned int nregs,
+		      unsigned int start_idx)
+{
+	while (nregs--) {
+		t4_write_reg(adap, addr_reg, start_idx);
+		*vals++ = t4_read_reg(adap, data_reg);
+		start_idx++;
+	}
+}
+
+/**
+ *	t4_write_indirect - write indirectly addressed registers
+ *	@adap: the adapter
+ *	@addr_reg: register holding the indirect addresses
+ *	@data_reg: register holding the value for the indirect registers
+ *	@vals: values to write
+ *	@nregs: how many indirect registers to write
+ *	@start_idx: address of first indirect register to write
+ *
+ *	Writes a sequential block of registers that are accessed indirectly
+ *	through an address/data register pair.
+ */
+void t4_write_indirect(struct adapter *adap, unsigned int addr_reg,
+		       unsigned int data_reg, const u32 *vals,
+		       unsigned int nregs, unsigned int start_idx)
+{
+	while (nregs--) {
+		t4_write_reg(adap, addr_reg, start_idx++);
+		t4_write_reg(adap, data_reg, *vals++);
+	}
+}
+
+/*
+ * Read a 32-bit PCI Configuration Space register via the PCI-E backdoor
+ * mechanism.  This guarantees that we get the real value even if we're
+ * operating within a Virtual Machine and the Hypervisor is trapping our
+ * Configuration Space accesses.
+ */
+void t4_hw_pci_read_cfg4(adapter_t *adap, int reg, u32 *val)
+{
+	t4_write_reg(adap, A_PCIE_CFG_SPACE_REQ,
+		     F_ENABLE | F_LOCALCFG | V_FUNCTION(adap->pf) |
+		     V_REGISTER(reg));
+	*val = t4_read_reg(adap, A_PCIE_CFG_SPACE_DATA);
+}
+
+/*
+ * Get the reply to a mailbox command and store it in @rpl in big-endian order.
+ */
+static void get_mbox_rpl(struct adapter *adap, __be64 *rpl, int nflit,
+			 u32 mbox_addr)
+{
+	for ( ; nflit; nflit--, mbox_addr += 8)
+		*rpl++ = cpu_to_be64(t4_read_reg64(adap, mbox_addr));
+}
+
+/*
+ * Handle a FW assertion reported in a mailbox.
+ */
+static void fw_asrt(struct adapter *adap, u32 mbox_addr)
+{
+	struct fw_debug_cmd asrt;
+
+	get_mbox_rpl(adap, (__be64 *)&asrt, sizeof(asrt) / 8, mbox_addr);
+	CH_ALERT(adap, "FW assertion at %.16s:%u, val0 %#x, val1 %#x\n",
+		 asrt.u.assert.filename_0_7, ntohl(asrt.u.assert.line),
+		 ntohl(asrt.u.assert.x), ntohl(asrt.u.assert.y));
+}
+
+#define X_CIM_PF_NOACCESS 0xeeeeeeee
+/**
+ *	t4_wr_mbox_meat - send a command to FW through the given mailbox
+ *	@adap: the adapter
+ *	@mbox: index of the mailbox to use
+ *	@cmd: the command to write
+ *	@size: command length in bytes
+ *	@rpl: where to optionally store the reply
+ *	@sleep_ok: if true we may sleep while awaiting command completion
+ *
+ *	Sends the given command to FW through the selected mailbox and waits
+ *	for the FW to execute the command.  If @rpl is not %NULL it is used to
+ *	store the FW's reply to the command.  The command and its optional
+ *	reply are of the same length.  Some FW commands like RESET and
+ *	INITIALIZE can take a considerable amount of time to execute.
+ *	@sleep_ok determines whether we may sleep while awaiting the response.
+ *	If sleeping is allowed we use progressive backoff otherwise we spin.
+ *
+ *	The return value is 0 on success or a negative errno on failure.  A
+ *	failure can happen either because we are not able to execute the
+ *	command or FW executes it but signals an error.  In the latter case
+ *	the return value is the error code indicated by FW (negated).
+ */
+int t4_wr_mbox_meat(struct adapter *adap, int mbox, const void *cmd, int size,
+		    void *rpl, bool sleep_ok)
+{
+	/*
+	 * We delay in small increments at first in an effort to maintain
+	 * responsiveness for simple, fast executing commands but then back
+	 * off to larger delays to a maximum retry delay.
+	 */
+	static const int delay[] = {
+		1, 1, 3, 5, 10, 10, 20, 50, 100
+	};
+
+	u32 v;
+	u64 res;
+	int i, ms, delay_idx;
+	const __be64 *p = cmd;
+	u32 data_reg = PF_REG(mbox, A_CIM_PF_MAILBOX_DATA);
+	u32 ctl_reg = PF_REG(mbox, A_CIM_PF_MAILBOX_CTRL);
+	u32 ctl;
+	struct t4_os_list entry;
+
+	if ((size & 15) || size > MBOX_LEN)
+		return -EINVAL;
+
+	/*
+	 * Queue ourselves onto the mailbox access list.  When our entry is at
+	 * the front of the list, we have rights to access the mailbox.  So we
+	 * wait [for a while] till we're at the front [or bail out with an
+	 * EBUSY] ...
+	 */
+	t4_os_atomic_add_tail(&entry, &adap->mbox_list, &adap->mbox_lock);
+
+	delay_idx = 0;
+	ms = delay[0];
+
+	for (i = 0; ; i += ms) {
+		/*
+		 * If we've waited too long, return a busy indication.  This
+		 * really ought to be based on our initial position in the
+		 * mailbox access list but this is a start.  We very rearely
+		 * contend on access to the mailbox ...
+		 */
+		if (i > 4*FW_CMD_MAX_TIMEOUT) {
+			t4_os_atomic_list_del(&entry, &adap->mbox_lock);
+			return -EBUSY;
+		}
+
+		/*
+		 * If we're at the head, break out and start the mailbox
+		 * protocol.
+		 */
+		if (t4_os_list_first_entry(&adap->mbox_list) == &entry)
+			break;
+		
+		/*
+		 * Delay for a bit before checking again ...
+		 */
+		if (sleep_ok) {
+			ms = delay[delay_idx];  /* last element may repeat */
+			if (delay_idx < ARRAY_SIZE(delay) - 1)
+				delay_idx++;
+			msleep(ms);
+		} else
+			mdelay(ms);
+	}
+
+	/*
+	 * Attempt to gain access to the mailbox.
+	 */
+	for (i = 0; i < 4; i++) {
+		ctl = t4_read_reg(adap, ctl_reg);
+		v = G_MBOWNER(ctl);
+		if (v != X_MBOWNER_NONE)
+			break;
+	}
+
+	/*
+	 * If we were unable to gain access, dequeue ourselves from the
+	 * mailbox atomic access list and report the error to our caller.
+	 */
+	if (v != X_MBOWNER_PL) {
+		t4_os_atomic_list_del(&entry, &adap->mbox_lock);
+		return (v == X_MBOWNER_FW ? -EBUSY : -ETIMEDOUT);
+	}
+
+	/*
+	 * If we gain ownership of the mailbox and there's a "valid" message
+	 * in it, this is likely an asynchronous error message from the
+	 * firmware.  So we'll report that and then proceed on with attempting
+	 * to issue our own command ... which may well fail if the error
+	 * presaged the firmware crashing ...
+	 */
+	if (ctl & F_MBMSGVALID) {
+		CH_ERR(adap, "found VALID command in mbox %u: "
+		       "%llx %llx %llx %llx %llx %llx %llx %llx\n", mbox,
+		       (unsigned long long)t4_read_reg64(adap, data_reg),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 8),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 16),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 24),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 32),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 40),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 48),
+		       (unsigned long long)t4_read_reg64(adap, data_reg + 56));
+	}
+
+	/*
+	 * Copy in the new mailbox command and send it on its way ...
+	 */
+	for (i = 0; i < size; i += 8, p++)
+		t4_write_reg64(adap, data_reg + i, be64_to_cpu(*p));
+
+	CH_DUMP_MBOX(adap, mbox, data_reg, size / 8);
+
+	t4_write_reg(adap, ctl_reg, F_MBMSGVALID | V_MBOWNER(X_MBOWNER_FW));
+	t4_read_reg(adap, ctl_reg);          /* flush write */
+
+	delay_idx = 0;
+	ms = delay[0];
+
+	for (i = 0; i < FW_CMD_MAX_TIMEOUT; i += ms) {
+		if (sleep_ok) {
+			ms = delay[delay_idx];  /* last element may repeat */
+			if (delay_idx < ARRAY_SIZE(delay) - 1)
+				delay_idx++;
+			msleep(ms);
+		} else
+			mdelay(ms);
+
+		v = t4_read_reg(adap, ctl_reg);
+		if (v == X_CIM_PF_NOACCESS)
+			continue;
+		if (G_MBOWNER(v) == X_MBOWNER_PL) {
+			if (!(v & F_MBMSGVALID)) {
+				t4_write_reg(adap, ctl_reg,
+					     V_MBOWNER(X_MBOWNER_NONE));
+				continue;
+			}
+			CH_DUMP_MBOX(adap, mbox, data_reg, size / 8);
+			CH_MSG(adap, INFO, MBOX,
+			       "command completed in %d ms (%ssleeping)\n",
+			       i + ms, sleep_ok ? "" : "non-");
+
+			res = t4_read_reg64(adap, data_reg);
+			if (G_FW_CMD_OP(res >> 32) == FW_DEBUG_CMD) {
+				fw_asrt(adap, data_reg);
+				res = V_FW_CMD_RETVAL(EIO);
+			} else if (rpl)
+				get_mbox_rpl(adap, rpl, size / 8, data_reg);
+			t4_write_reg(adap, ctl_reg, V_MBOWNER(X_MBOWNER_NONE));
+			t4_os_atomic_list_del(&entry, &adap->mbox_lock);
+			return -G_FW_CMD_RETVAL((int)res);
+		}
+	}
+
+	CH_ERR(adap, "command %#x in mailbox %d timed out\n",
+	       *(const u8 *)cmd, mbox);
+	t4_os_atomic_list_del(&entry, &adap->mbox_lock);
+	return -ETIMEDOUT;
+}
+
+/**
+ *	t4_mc_read - read from MC through backdoor accesses
+ *	@adap: the adapter
+ *	@addr: address of first byte requested
+ *	@data: 64 bytes of data containing the requested address
+ *	@ecc: where to store the corresponding 64-bit ECC word
+ *
+ *	Read 64 bytes of data from MC starting at a 64-byte-aligned address
+ *	that covers the requested address @addr.  If @parity is not %NULL it
+ *	is assigned the 64-bit ECC word for the read data.
+ */
+int t4_mc_read(struct adapter *adap, u32 addr, __be32 *data, u64 *ecc)
+{
+	int i;
+
+	if (t4_read_reg(adap, A_MC_BIST_CMD) & F_START_BIST)
+		return -EBUSY;
+	t4_write_reg(adap, A_MC_BIST_CMD_ADDR, addr & ~0x3fU);
+	t4_write_reg(adap, A_MC_BIST_CMD_LEN, 64);
+	t4_write_reg(adap, A_MC_BIST_DATA_PATTERN, 0xc);
+	t4_write_reg(adap, A_MC_BIST_CMD, V_BIST_OPCODE(1) | F_START_BIST |
+		     V_BIST_CMD_GAP(1));
+	i = t4_wait_op_done(adap, A_MC_BIST_CMD, F_START_BIST, 0, 10, 1);
+	if (i)
+		return i;
+
+#define MC_DATA(i) MC_BIST_STATUS_REG(A_MC_BIST_STATUS_RDATA, i)
+
+	for (i = 15; i >= 0; i--)
+		*data++ = ntohl(t4_read_reg(adap, MC_DATA(i)));
+	if (ecc)
+		*ecc = t4_read_reg64(adap, MC_DATA(16));
+#undef MC_DATA
+	return 0;
+}
+
+/**
+ *	t4_edc_read - read from EDC through backdoor accesses
+ *	@adap: the adapter
+ *	@idx: which EDC to access
+ *	@addr: address of first byte requested
+ *	@data: 64 bytes of data containing the requested address
+ *	@ecc: where to store the corresponding 64-bit ECC word
+ *
+ *	Read 64 bytes of data from EDC starting at a 64-byte-aligned address
+ *	that covers the requested address @addr.  If @parity is not %NULL it
+ *	is assigned the 64-bit ECC word for the read data.
+ */
+int t4_edc_read(struct adapter *adap, int idx, u32 addr, __be32 *data, u64 *ecc)
+{
+	int i;
+
+	idx *= EDC_STRIDE;
+	if (t4_read_reg(adap, A_EDC_BIST_CMD + idx) & F_START_BIST)
+		return -EBUSY;
+	t4_write_reg(adap, A_EDC_BIST_CMD_ADDR + idx, addr & ~0x3fU);
+	t4_write_reg(adap, A_EDC_BIST_CMD_LEN + idx, 64);
+	t4_write_reg(adap, A_EDC_BIST_DATA_PATTERN + idx, 0xc);
+	t4_write_reg(adap, A_EDC_BIST_CMD + idx,
+		     V_BIST_OPCODE(1) | V_BIST_CMD_GAP(1) | F_START_BIST);
+	i = t4_wait_op_done(adap, A_EDC_BIST_CMD + idx, F_START_BIST, 0, 10, 1);
+	if (i)
+		return i;
+
+#define EDC_DATA(i) (EDC_BIST_STATUS_REG(A_EDC_BIST_STATUS_RDATA, i) + idx)
+
+	for (i = 15; i >= 0; i--)
+		*data++ = ntohl(t4_read_reg(adap, EDC_DATA(i)));
+	if (ecc)
+		*ecc = t4_read_reg64(adap, EDC_DATA(16));
+#undef EDC_DATA
+	return 0;
+}
+
+/**
+ *	t4_mem_read - read EDC 0, EDC 1 or MC into buffer
+ *	@adap: the adapter
+ *	@mtype: memory type: MEM_EDC0, MEM_EDC1 or MEM_MC
+ *	@addr: address within indicated memory type
+ *	@len: amount of memory to read
+ *	@buf: host memory buffer
+ *
+ *	Reads an [almost] arbitrary memory region in the firmware: the
+ *	firmware memory address, length and host buffer must be aligned on
+ *	32-bit boudaries.  The memory is returned as a raw byte sequence from
+ *	the firmware's memory.  If this memory contains data structures which
+ *	contain multi-byte integers, it's the callers responsibility to
+ *	perform appropriate byte order conversions.
+ */
+int t4_mem_read(struct adapter *adap, int mtype, u32 addr, u32 len,
+		__be32 *buf)
+{
+	u32 pos, start, end, offset;
+	int ret;
+
+	/*
+	 * Argument sanity checks ...
+	 */
+	if ((addr & 0x3) || (len & 0x3))
+		return -EINVAL;
+
+	/*
+	 * The underlaying EDC/MC read routines read 64 bytes at a time so we
+	 * need to round down the start and round up the end.  We'll start
+	 * copying out of the first line at (addr - start) a word at a time.
+	 */
+	start = addr & ~(64-1);
+	end = (addr + len + 64-1) & ~(64-1);
+	offset = (addr - start)/sizeof(__be32);
+
+	for (pos = start; pos < end; pos += 64, offset = 0) {
+		__be32 data[16];
+
+		/*
+		 * Read the chip's memory block and bail if there's an error.
+		 */
+		if (mtype == MEM_MC)
+			ret = t4_mc_read(adap, pos, data, NULL);
+		else
+			ret = t4_edc_read(adap, mtype, pos, data, NULL);
+		if (ret)
+			return ret;
+
+		/*
+		 * Copy the data into the caller's memory buffer.
+		 */
+		while (offset < 16 && len > 0) {
+			*buf++ = data[offset++];
+			len -= sizeof(__be32);
+		}
+	}
+
+	return 0;
+}
+
+/*
+ *	t4_mem_win_rw - read/write memory through PCIE memory window
+ *	@adap: the adapter
+ *	@addr: address of first byte requested
+ *	@data: MEMWIN0_APERTURE bytes of data containing the requested address
+ *	@dir: direction of transfer 1 => read, 0 => write
+ *
+ *	Read/write MEMWIN0_APERTURE bytes of data from MC starting at a 
+ *	MEMWIN0_APERTURE-byte-aligned address that covers the requested
+ *	address @addr.
+ */
+static int t4_mem_win_rw(struct adapter *adap, u32 addr, __be32 *data, int dir)
+ {
+ 	int i;
+
+	/*
+	 * Setup offset into PCIE memory window.  Address must be a
+	 * MEMWIN0_APERTURE-byte-aligned address.  (Read back MA register to
+	 * ensure that changes propagate before we attempt to use the new
+	 * values.)
+	 */
+	t4_write_reg(adap, A_PCIE_MEM_ACCESS_OFFSET, addr & ~(MEMWIN0_APERTURE - 1));
+	t4_read_reg(adap, A_PCIE_MEM_ACCESS_OFFSET);
+
+	/* Collecting data 4 bytes at a time upto MEMWIN0_APERTURE */
+	for (i = 0; i < MEMWIN0_APERTURE; i=i+0x4 ) {
+		if (dir)
+			*data++ = t4_read_reg(adap, (MEMWIN0_BASE + i));
+		else
+			t4_write_reg(adap, (MEMWIN0_BASE + i), *data++);
+	}	
+
+ 	return 0;
+ }
+
+int t4_mem_win_read(struct adapter *adap, u32 addr, __be32 *data)
+{
+	return t4_mem_win_rw(adap, addr, data, 1);
+}
+
+/*
+ *	t4_mem_win_read_len - read memory through PCIE memory window
+ *	@adap: the adapter
+ *	@addr: address of first byte requested aligned on 32b.
+ *	@data: len bytes to hold the data read
+ *	@len: amount of data to read from window.  Must be <= 
+ *            MEMWIN0_APERATURE after adjusting for 16B alignment
+ *            requirements of the the memory window.
+ *
+ *	Read len bytes of data from MC starting at @addr.
+ */
+int t4_mem_win_read_len(struct adapter *adap, u32 addr, __be32 *data, int len)
+{
+	int i;
+	int off;
+
+	/*
+	 * Align on a 16B boundary.
+	 */
+	off = addr & 15;
+	if ((addr & 3) || (len + off) > MEMWIN0_APERTURE)
+		return -EINVAL;
+
+	/*
+	 * Setup offset into PCIE memory window.  (Read back MA register to
+	 * ensure that changes propagate before we attempt to use the new
+	 * values.)
+	 */
+	t4_write_reg(adap, A_PCIE_MEM_ACCESS_OFFSET, addr & ~15);
+	t4_read_reg(adap, A_PCIE_MEM_ACCESS_OFFSET);
+
+	for (i = 0; i < len; i += 4 ) {
+		*data++ = t4_read_reg(adap, (MEMWIN0_BASE + off + i));
+	}	
+
+	return 0;
+}
+
+int t4_mem_win_write(struct adapter *adap, u32 addr, __be32 *data)
+{
+	return t4_mem_win_rw(adap, addr, data, 0);
+}
+
+/**
+ *	t4_memory_rw - read/write EDC 0, EDC 1 or MC via PCIE memory window
+ *	@adap: the adapter
+ *	@mtype: memory type: MEM_EDC0, MEM_EDC1 or MEM_MC
+ *	@addr: address within indicated memory type
+ *	@len: amount of memory to transfer
+ *	@buf: host memory buffer
+ *	@dir: direction of transfer 1 => read, 0 => write
+ *
+ *	Reads/writes an [almost] arbitrary memory region in the firmware: the
+ *	firmware memory address, length and host buffer must be aligned on
+ *	32-bit boudaries.  The memory is transferred as a raw byte sequence
+ *	from/to the firmware's memory.  If this memory contains data
+ *	structures which contain multi-byte integers, it's the callers
+ *	responsibility to perform appropriate byte order conversions.
+ */
+static int t4_memory_rw(struct adapter *adap, int mtype, u32 addr, u32 len,
+			__be32 *buf, int dir)
+{
+	u32 pos, start, end, offset, memoffset;
+	int ret;
+
+	/*
+	 * Argument sanity checks ...
+	 */
+	if ((addr & 0x3) || (len & 0x3))
+		return -EINVAL;
+
+	/* Offset into the region of memory which is being accessed
+	 * MEM_EDC0 = 0
+	 * MEM_EDC1 = 1
+	 * MEM_MC   = 2
+	 */
+	memoffset = (mtype * ( 5 * 1024 * 1024));
+
+	/* Determine the PCIE_MEM_ACCESS_OFFSET */
+	addr = addr + memoffset;
+
+	/*
+	 * The underlaying EDC/MC read routines read MEMWIN0_APERTURE bytes 
+	 * at a time so we need to round down the start and round up the end.
+	 * We'll start copying out of the first line at (addr - start) a word 
+	 * at a time.
+	 */
+	start = addr & ~(MEMWIN0_APERTURE-1);
+	end = (addr + len + MEMWIN0_APERTURE-1) & ~(MEMWIN0_APERTURE-1);
+	offset = (addr - start)/sizeof(__be32);
+
+	for (pos = start; pos < end; pos += MEMWIN0_APERTURE, offset = 0) {
+		__be32 data[MEMWIN0_APERTURE/sizeof(__be32)];
+
+		/*
+		 * If we're writing, copy the data from the caller's memory
+		 * buffer
+		 */
+		if (!dir) {
+			/*
+			 * If we're doing a partial write, then we need to do
+			 * a read-modify-write ...
+			 */
+			if (offset || len < MEMWIN0_APERTURE) {
+				ret = t4_mem_win_rw(adap, pos, data, 1);
+				if (ret)
+					return ret;
+			}
+			while (offset < (MEMWIN0_APERTURE/sizeof(__be32)) &&
+			       len > 0) {
+				data[offset++] = *buf++;
+				len -= sizeof(__be32);
+			}
+		}
+
+		/*
+		 * Transfer a block of memory and bail if there's an error.
+		 */
+		ret = t4_mem_win_rw(adap, pos, data, dir);
+		if (ret)
+			return ret;
+
+		/*
+		 * If we're reading, copy the data into the caller's memory
+		 * buffer.
+		 */
+		if (dir)
+			while (offset < (MEMWIN0_APERTURE/sizeof(__be32)) &&
+			       len > 0) {
+				*buf++ = data[offset++];
+				len -= sizeof(__be32);
+			}
+	}
+
+	return 0;
+}
+
+int t4_memory_read(struct adapter *adap, int mtype, u32 addr, u32 len,
+		   __be32 *buf)
+{
+	return t4_memory_rw(adap, mtype, addr, len, buf, 1);
+}
+
+int t4_memory_write(struct adapter *adap, int mtype, u32 addr, u32 len,
+		   __be32 *buf)
+{
+	return t4_memory_rw(adap, mtype, addr, len, buf, 0);
+}
+
+/*
+ * Partial EEPROM Vital Product Data structure.  Includes only the ID and
+ * VPD-R header.
+ */
+struct t4_vpd_hdr {
+	u8  id_tag;
+	u8  id_len[2];
+	u8  id_data[ID_LEN];
+	u8  vpdr_tag;
+	u8  vpdr_len[2];
+};
+
+/*
+ * EEPROM reads take a few tens of us while writes can take a bit over 5 ms.
+ */
+#define EEPROM_MAX_RD_POLL 40
+#define EEPROM_MAX_WR_POLL 6
+#define EEPROM_STAT_ADDR   0x7bfc
+#define VPD_BASE           0x400
+#define VPD_BASE_OLD       0
+#define VPD_LEN            512
+#define VPD_INFO_FLD_HDR_SIZE	3
+
+/**
+ *	t4_seeprom_read - read a serial EEPROM location
+ *	@adapter: adapter to read
+ *	@addr: EEPROM virtual address
+ *	@data: where to store the read data
+ *
+ *	Read a 32-bit word from a location in serial EEPROM using the card's PCI
+ *	VPD capability.  Note that this function must be called with a virtual
+ *	address.
+ */
+int t4_seeprom_read(struct adapter *adapter, u32 addr, u32 *data)
+{
+	u16 val;
+	int attempts = EEPROM_MAX_RD_POLL;
+	unsigned int base = adapter->params.pci.vpd_cap_addr;
+
+	if (addr >= EEPROMVSIZE || (addr & 3))
+		return -EINVAL;
+
+	t4_os_pci_write_cfg2(adapter, base + PCI_VPD_ADDR, (u16)addr);
+	do {
+		udelay(10);
+		t4_os_pci_read_cfg2(adapter, base + PCI_VPD_ADDR, &val);
+	} while (!(val & PCI_VPD_ADDR_F) && --attempts);
+
+	if (!(val & PCI_VPD_ADDR_F)) {
+		CH_ERR(adapter, "reading EEPROM address 0x%x failed\n", addr);
+		return -EIO;
+	}
+	t4_os_pci_read_cfg4(adapter, base + PCI_VPD_DATA, data);
+	*data = le32_to_cpu(*data);
+	return 0;
+}
+
+/**
+ *	t4_seeprom_write - write a serial EEPROM location
+ *	@adapter: adapter to write
+ *	@addr: virtual EEPROM address
+ *	@data: value to write
+ *
+ *	Write a 32-bit word to a location in serial EEPROM using the card's PCI
+ *	VPD capability.  Note that this function must be called with a virtual
+ *	address.
+ */
+int t4_seeprom_write(struct adapter *adapter, u32 addr, u32 data)
+{
+	u16 val;
+	int attempts = EEPROM_MAX_WR_POLL;
+	unsigned int base = adapter->params.pci.vpd_cap_addr;
+
+	if (addr >= EEPROMVSIZE || (addr & 3))
+		return -EINVAL;
+
+	t4_os_pci_write_cfg4(adapter, base + PCI_VPD_DATA,
+				 cpu_to_le32(data));
+	t4_os_pci_write_cfg2(adapter, base + PCI_VPD_ADDR,
+				 (u16)addr | PCI_VPD_ADDR_F);
+	do {
+		msleep(1);
+		t4_os_pci_read_cfg2(adapter, base + PCI_VPD_ADDR, &val);
+	} while ((val & PCI_VPD_ADDR_F) && --attempts);
+
+	if (val & PCI_VPD_ADDR_F) {
+		CH_ERR(adapter, "write to EEPROM address 0x%x failed\n", addr);
+		return -EIO;
+	}
+	return 0;
+}
+
+/**
+ *	t4_eeprom_ptov - translate a physical EEPROM address to virtual
+ *	@phys_addr: the physical EEPROM address
+ *	@fn: the PCI function number
+ *	@sz: size of function-specific area
+ *
+ *	Translate a physical EEPROM address to virtual.  The first 1K is
+ *	accessed through virtual addresses starting at 31K, the rest is
+ *	accessed through virtual addresses starting at 0.
+ *
+ *	The mapping is as follows:
+ *	[0..1K) -> [31K..32K)
+ *	[1K..1K+A) -> [ES-A..ES)
+ *	[1K+A..ES) -> [0..ES-A-1K)
+ *
+ *	where A = @fn * @sz, and ES = EEPROM size.
+ */
+int t4_eeprom_ptov(unsigned int phys_addr, unsigned int fn, unsigned int sz)
+{
+	fn *= sz;
+	if (phys_addr < 1024)
+		return phys_addr + (31 << 10);
+	if (phys_addr < 1024 + fn)
+		return EEPROMSIZE - fn + phys_addr - 1024;
+	if (phys_addr < EEPROMSIZE)
+		return phys_addr - 1024 - fn;
+	return -EINVAL;
+}
+
+/**
+ *	t4_seeprom_wp - enable/disable EEPROM write protection
+ *	@adapter: the adapter
+ *	@enable: whether to enable or disable write protection
+ *
+ *	Enables or disables write protection on the serial EEPROM.
+ */
+int t4_seeprom_wp(struct adapter *adapter, int enable)
+{
+	return t4_seeprom_write(adapter, EEPROM_STAT_ADDR, enable ? 0xc : 0);
+}
+
+/**
+ *	get_vpd_keyword_val - Locates an information field keyword in the VPD
+ *	@v: Pointer to buffered vpd data structure
+ *	@kw: The keyword to search for
+ *	
+ *	Returns the value of the information field keyword or
+ *	-ENOENT otherwise.
+ */
+int get_vpd_keyword_val(const struct t4_vpd_hdr *v, const char *kw)
+{
+         int i;
+	 unsigned int offset , len;
+	 const u8 *buf = &v->id_tag;
+	 const u8 *vpdr_len = &v->vpdr_tag; 
+	 offset = sizeof(struct t4_vpd_hdr);
+	 len =  (u16)vpdr_len[1] + ((u16)vpdr_len[2] << 8);
+	 
+	 if (len + sizeof(struct t4_vpd_hdr) > VPD_LEN) {
+		 return -ENOENT;
+	 }
+
+         for (i = offset; i + VPD_INFO_FLD_HDR_SIZE <= offset + len;) {
+		 if(memcmp(buf + i , kw , 2) == 0){
+			 i += VPD_INFO_FLD_HDR_SIZE;
+                         return i;
+		  }
+
+                 i += VPD_INFO_FLD_HDR_SIZE + buf[i+2];
+         }
+
+         return -ENOENT;
+}
+
+/*
+ * str_strip
+ * Removes trailing whitespaces from string "s"
+ * Based on strstrip() implementation in string.c
+ */
+static void str_strip(char *s)
+{
+	size_t size;
+	char *end;
+
+	size = strlen(s);
+	if (!size)
+		return;
+
+	end = s + size - 1;
+	while (end >= s && isspace(*end))
+		end--;
+	*(end + 1) = '\0';
+}
+
+
+/**
+ *	t4_get_raw_vpd_params - read VPD parameters from VPD EEPROM
+ *	@adapter: adapter to read
+ *	@p: where to store the parameters
+ *
+ *	Reads card parameters stored in VPD EEPROM.
+ */
+int t4_get_raw_vpd_params(struct adapter *adapter, struct vpd_params *p)
+{
+	int i, ret, addr;
+	int ec, sn, pn, na;
+	u8 vpd[VPD_LEN], csum;
+	const struct t4_vpd_hdr *v;
+
+	/*
+	 * Card information normally starts at VPD_BASE but early cards had
+	 * it at 0.
+	 */
+	ret = t4_seeprom_read(adapter, VPD_BASE, (u32 *)(vpd));
+	addr = *vpd == 0x82 ? VPD_BASE : VPD_BASE_OLD; 
+
+	for (i = 0; i < sizeof(vpd); i += 4) {
+		ret = t4_seeprom_read(adapter, addr + i, (u32 *)(vpd + i));
+		if (ret)
+			return ret;
+	}
+ 	v = (const struct t4_vpd_hdr *)vpd;
+	
+#define FIND_VPD_KW(var,name) do { \
+	var = get_vpd_keyword_val(v , name); \
+	if (var < 0) { \
+		CH_ERR(adapter, "missing VPD keyword " name "\n"); \
+		return -EINVAL; \
+	} \
+} while (0)	
+
+	FIND_VPD_KW(i, "RV");
+	for (csum = 0; i >= 0; i--)
+		csum += vpd[i];
+
+	if (csum) {
+		CH_ERR(adapter, "corrupted VPD EEPROM, actual csum %u\n", csum);
+		return -EINVAL;
+	}
+	FIND_VPD_KW(ec, "EC");
+	FIND_VPD_KW(sn, "SN");
+	FIND_VPD_KW(pn, "PN");
+	FIND_VPD_KW(na, "NA");
+#undef FIND_VPD_KW
+
+	memcpy(p->id, v->id_data, ID_LEN);
+	str_strip((char *)p->id);
+	memcpy(p->ec, vpd + ec, EC_LEN);
+	str_strip((char *)p->ec);
+	i = vpd[sn - VPD_INFO_FLD_HDR_SIZE + 2];
+	memcpy(p->sn, vpd + sn, min(i, SERNUM_LEN));
+	str_strip((char *)p->sn);
+	memcpy(p->pn, vpd + pn, min(i, PN_LEN));
+	str_strip((char *)p->pn);
+	memcpy(p->na, vpd + na, min(i, MACADDR_LEN));
+	str_strip((char *)p->na);
+
+	return 0;
+}
+
+/**
+ *	t4_get_vpd_params - read VPD parameters & retrieve Core Clock
+ *	@adapter: adapter to read
+ *	@p: where to store the parameters
+ *
+ *	Reads card parameters stored in VPD EEPROM and retrieves the Core
+ *	Clock.  This can only be called after a connection to the firmware
+ *	is established.
+ */
+int t4_get_vpd_params(struct adapter *adapter, struct vpd_params *p)
+{
+	u32 cclk_param, cclk_val;
+	int ret;
+
+	/*
+	 * Grab the raw VPD parameters.
+	 */
+	ret = t4_get_raw_vpd_params(adapter, p);
+	if (ret)
+		return ret;
+
+	/*
+	 * Ask firmware for the Core Clock since it knows how to translate the
+	 * Reference Clock ('V2') VPD field into a Core Clock value ...
+	 */
+	cclk_param = (V_FW_PARAMS_MNEM(FW_PARAMS_MNEM_DEV) |
+		      V_FW_PARAMS_PARAM_X(FW_PARAMS_PARAM_DEV_CCLK));
+	ret = t4_query_params(adapter, adapter->mbox, 0, 0,
+			      1, &cclk_param, &cclk_val);
+	if (ret)
+		return ret;
+	p->cclk = cclk_val;
+
+	return 0;
+}
+
+/* serial flash and firmware constants and flash config file constants */
+enum {
+	SF_ATTEMPTS = 10,             /* max retries for SF operations */
+
+	/* flash command opcodes */
+	SF_PROG_PAGE    = 2,          /* program page */
+	SF_WR_DISABLE   = 4,          /* disable writes */
+	SF_RD_STATUS    = 5,          /* read status register */
+	SF_WR_ENABLE    = 6,          /* enable writes */
+	SF_RD_DATA_FAST = 0xb,        /* read flash */
+	SF_RD_ID        = 0x9f,       /* read ID */
+	SF_ERASE_SECTOR = 0xd8,       /* erase sector */
+};
+
+/**
+ *	sf1_read - read data from the serial flash
+ *	@adapter: the adapter
+ *	@byte_cnt: number of bytes to read
+ *	@cont: whether another operation will be chained
+ *	@lock: whether to lock SF for PL access only
+ *	@valp: where to store the read data
+ *
+ *	Reads up to 4 bytes of data from the serial flash.  The location of
+ *	the read needs to be specified prior to calling this by issuing the
+ *	appropriate commands to the serial flash.
+ */
+static int sf1_read(struct adapter *adapter, unsigned int byte_cnt, int cont,
+		    int lock, u32 *valp)
+{
+	int ret;
+
+	if (!byte_cnt || byte_cnt > 4)
+		return -EINVAL;
+	if (t4_read_reg(adapter, A_SF_OP) & F_BUSY)
+		return -EBUSY;
+	t4_write_reg(adapter, A_SF_OP,
+		     V_SF_LOCK(lock) | V_CONT(cont) | V_BYTECNT(byte_cnt - 1));
+	ret = t4_wait_op_done(adapter, A_SF_OP, F_BUSY, 0, SF_ATTEMPTS, 5);
+	if (!ret)
+		*valp = t4_read_reg(adapter, A_SF_DATA);
+	return ret;
+}
+
+/**
+ *	sf1_write - write data to the serial flash
+ *	@adapter: the adapter
+ *	@byte_cnt: number of bytes to write
+ *	@cont: whether another operation will be chained
+ *	@lock: whether to lock SF for PL access only
+ *	@val: value to write
+ *
+ *	Writes up to 4 bytes of data to the serial flash.  The location of
+ *	the write needs to be specified prior to calling this by issuing the
+ *	appropriate commands to the serial flash.
+ */
+static int sf1_write(struct adapter *adapter, unsigned int byte_cnt, int cont,
+		     int lock, u32 val)
+{
+	if (!byte_cnt || byte_cnt > 4)
+		return -EINVAL;
+	if (t4_read_reg(adapter, A_SF_OP) & F_BUSY)
+		return -EBUSY;
+	t4_write_reg(adapter, A_SF_DATA, val);
+	t4_write_reg(adapter, A_SF_OP, V_SF_LOCK(lock) |
+		     V_CONT(cont) | V_BYTECNT(byte_cnt - 1) | V_OP(1));
+	return t4_wait_op_done(adapter, A_SF_OP, F_BUSY, 0, SF_ATTEMPTS, 5);
+}
+
+/**
+ *	flash_wait_op - wait for a flash operation to complete
+ *	@adapter: the adapter
+ *	@attempts: max number of polls of the status register
+ *	@delay: delay between polls in ms
+ *
+ *	Wait for a flash operation to complete by polling the status register.
+ */
+static int flash_wait_op(struct adapter *adapter, int attempts, int delay)
+{
+	int ret;
+	u32 status;
+
+	while (1) {
+		if ((ret = sf1_write(adapter, 1, 1, 1, SF_RD_STATUS)) != 0 ||
+		    (ret = sf1_read(adapter, 1, 0, 1, &status)) != 0)
+			return ret;
+		if (!(status & 1))
+			return 0;
+		if (--attempts == 0)
+			return -EAGAIN;
+		if (delay)
+			msleep(delay);
+	}
+}
+
+/**
+ *	t4_read_flash - read words from serial flash
+ *	@adapter: the adapter
+ *	@addr: the start address for the read
+ *	@nwords: how many 32-bit words to read
+ *	@data: where to store the read data
+ *	@byte_oriented: whether to store data as bytes or as words
+ *
+ *	Read the specified number of 32-bit words from the serial flash.
+ *	If @byte_oriented is set the read data is stored as a byte array
+ *	(i.e., big-endian), otherwise as 32-bit words in the platform's
+ *	natural endianess.
+ */
+int t4_read_flash(struct adapter *adapter, unsigned int addr,
+		  unsigned int nwords, u32 *data, int byte_oriented)
+{
+	int ret;
+
+	if (addr + nwords * sizeof(u32) > adapter->params.sf_size || (addr & 3))
+		return -EINVAL;
+
+	addr = swab32(addr) | SF_RD_DATA_FAST;
+
+	if ((ret = sf1_write(adapter, 4, 1, 0, addr)) != 0 ||
+	    (ret = sf1_read(adapter, 1, 1, 0, data)) != 0)
+		return ret;
+
+	for ( ; nwords; nwords--, data++) {
+		ret = sf1_read(adapter, 4, nwords > 1, nwords == 1, data);
+		if (nwords == 1)
+			t4_write_reg(adapter, A_SF_OP, 0);    /* unlock SF */
+		if (ret)
+			return ret;
+		if (byte_oriented)
+			*data = htonl(*data);
+	}
+	return 0;
+}
+
+/**
+ *	t4_write_flash - write up to a page of data to the serial flash
+ *	@adapter: the adapter
+ *	@addr: the start address to write
+ *	@n: length of data to write in bytes
+ *	@data: the data to write
+ *	@byte_oriented: whether to store data as bytes or as words
+ *
+ *	Writes up to a page of data (256 bytes) to the serial flash starting
+ *	at the given address.  All the data must be written to the same page.
+ *	If @byte_oriented is set the write data is stored as byte stream 
+ *	(i.e. matches what on disk), otherwise in big-endian.
+ */
+static int t4_write_flash(struct adapter *adapter, unsigned int addr,
+			  unsigned int n, const u8 *data, int byte_oriented)
+{
+	int ret;
+	u32 buf[64];
+	unsigned int i, c, left, val, offset = addr & 0xff;
+
+	if (addr >= adapter->params.sf_size || offset + n > SF_PAGE_SIZE)
+		return -EINVAL;
+
+	val = swab32(addr) | SF_PROG_PAGE;
+
+	if ((ret = sf1_write(adapter, 1, 0, 1, SF_WR_ENABLE)) != 0 ||
+	    (ret = sf1_write(adapter, 4, 1, 1, val)) != 0)
+		goto unlock;
+
+	for (left = n; left; left -= c) {
+		c = min(left, 4U);
+		for (val = 0, i = 0; i < c; ++i)
+			val = (val << 8) + *data++;
+
+		if (!byte_oriented)
+			val = htonl(val);
+
+		ret = sf1_write(adapter, c, c != left, 1, val);
+		if (ret)
+			goto unlock;
+	}
+	ret = flash_wait_op(adapter, 8, 1);
+	if (ret)
+		goto unlock;
+
+	t4_write_reg(adapter, A_SF_OP, 0);    /* unlock SF */
+
+	/* Read the page to verify the write succeeded */
+	ret = t4_read_flash(adapter, addr & ~0xff, ARRAY_SIZE(buf), buf,
+			    byte_oriented);
+	if (ret)
+		return ret;
+
+	if (memcmp(data - n, (u8 *)buf + offset, n)) {
+		CH_ERR(adapter, "failed to correctly write the flash page "
+		       "at %#x\n", addr);
+		return -EIO;
+	}
+	return 0;
+
+unlock:
+	t4_write_reg(adapter, A_SF_OP, 0);    /* unlock SF */
+	return ret;
+}
+
+/**
+ *	t4_get_fw_version - read the firmware version
+ *	@adapter: the adapter
+ *	@vers: where to place the version
+ *
+ *	Reads the FW version from flash.
+ */
+int t4_get_fw_version(struct adapter *adapter, u32 *vers)
+{
+	return t4_read_flash(adapter,
+			     FLASH_FW_START + offsetof(struct fw_hdr, fw_ver), 1,
+			     vers, 0);
+}
+
+/**
+ *	t4_get_tp_version - read the TP microcode version
+ *	@adapter: the adapter
+ *	@vers: where to place the version
+ *
+ *	Reads the TP microcode version from flash.
+ */
+int t4_get_tp_version(struct adapter *adapter, u32 *vers)
+{
+	return t4_read_flash(adapter, FLASH_FW_START + offsetof(struct fw_hdr,
+							      tp_microcode_ver),
+			     1, vers, 0);
+}
+
+/**
+ *	t4_check_fw_version - check if the FW is compatible with this driver
+ *	@adapter: the adapter
+ *
+ *	Checks if an adapter's FW is compatible with the driver.  Returns 0
+ *	if there's exact match, a negative error if the version could not be
+ *	read or there's a major version mismatch, and a positive value if the
+ *	expected major version is found but there's a minor version mismatch.
+ */
+int t4_check_fw_version(struct adapter *adapter)
+{
+	int ret, major, minor, micro;
+
+	ret = t4_get_fw_version(adapter, &adapter->params.fw_vers);
+	if (!ret)
+		ret = t4_get_tp_version(adapter, &adapter->params.tp_vers);
+	if (ret)
+		return ret;
+
+	major = G_FW_HDR_FW_VER_MAJOR(adapter->params.fw_vers);
+	minor = G_FW_HDR_FW_VER_MINOR(adapter->params.fw_vers);
+	micro = G_FW_HDR_FW_VER_MICRO(adapter->params.fw_vers);
+
+	if (major != FW_VERSION_MAJOR) {            /* major mismatch - fail */
+		CH_ERR(adapter, "card FW has major version %u, driver wants "
+		       "%u\n", major, FW_VERSION_MAJOR);
+		return -EINVAL;
+	}
+
+	if (minor == FW_VERSION_MINOR && micro == FW_VERSION_MICRO)
+		return 0;                                   /* perfect match */
+
+	/* Minor/micro version mismatch.  Report it but often it's OK. */
+	return 1;
+}
+
+/**
+ *	t4_flash_erase_sectors - erase a range of flash sectors
+ *	@adapter: the adapter
+ *	@start: the first sector to erase
+ *	@end: the last sector to erase
+ *
+ *	Erases the sectors in the given inclusive range.
+ */
+static int t4_flash_erase_sectors(struct adapter *adapter, int start, int end)
+{
+	int ret = 0;
+
+	while (start <= end) {
+		if ((ret = sf1_write(adapter, 1, 0, 1, SF_WR_ENABLE)) != 0 ||
+		    (ret = sf1_write(adapter, 4, 0, 1,
+				     SF_ERASE_SECTOR | (start << 8))) != 0 ||
+		    (ret = flash_wait_op(adapter, 14, 500)) != 0) {
+			CH_ERR(adapter, "erase of flash sector %d failed, "
+			       "error %d\n", start, ret);
+			break;
+		}
+		start++;
+	}
+	t4_write_reg(adapter, A_SF_OP, 0);    /* unlock SF */
+	return ret;
+}
+
+/**
+ *	t4_flash_cfg_addr - return the address of the flash configuration file
+ *	@adapter: the adapter
+ *
+ *	Return the address within the flash where the Firmware Configuration
+ *	File is stored.
+ */
+unsigned int t4_flash_cfg_addr(struct adapter *adapter)
+{
+	if (adapter->params.sf_size == 0x100000)
+		return FLASH_FPGA_CFG_START;
+	else
+		return FLASH_CFG_START;
+}
+
+/**
+ *	t4_load_cfg - download config file
+ *	@adap: the adapter
+ *	@cfg_data: the cfg text file to write
+ *	@size: text file size
+ *
+ *	Write the supplied config text file to the card's serial flash.
+ */
+int t4_load_cfg(struct adapter *adap, const u8 *cfg_data, unsigned int size)
+{
+	int ret, i, n;
+	unsigned int addr;
+	unsigned int flash_cfg_start_sec;
+	unsigned int sf_sec_size = adap->params.sf_size / adap->params.sf_nsec;
+
+	addr = t4_flash_cfg_addr(adap);
+	flash_cfg_start_sec = addr / SF_SEC_SIZE;
+
+	if (size > FLASH_CFG_MAX_SIZE) {
+		CH_ERR(adap, "cfg file too large, max is %u bytes\n",
+		       FLASH_CFG_MAX_SIZE);
+		return -EFBIG;
+	}
+
+	i = DIV_ROUND_UP(FLASH_CFG_MAX_SIZE,	/* # of sectors spanned */
+			 sf_sec_size);
+	ret = t4_flash_erase_sectors(adap, flash_cfg_start_sec,
+				     flash_cfg_start_sec + i - 1);
+	/*
+	 * If size == 0 then we're simply erasing the FLASH sectors associated
+	 * with the on-adapter Firmware Configuration File.
+	 */
+	if (ret || size == 0)
+		goto out;
+
+        // this will write to the flash up to SF_PAGE_SIZE at a time
+	for (i = 0; i< size; i+= SF_PAGE_SIZE) {
+		if ( (size - i) <  SF_PAGE_SIZE) 
+			n = size - i;
+		else 
+			n = SF_PAGE_SIZE;
+		ret = t4_write_flash(adap, addr, n, cfg_data, 1);
+		if (ret)
+			goto out;
+		
+		addr += SF_PAGE_SIZE;
+		cfg_data += SF_PAGE_SIZE;
+	} 
+                
+out:
+	if (ret)
+		CH_ERR(adap, "config file %s failed %d\n",
+		       (size == 0 ? "clear" : "download"), ret);
+	return ret;
+}
+
+
+/**
+ *	t4_load_fw - download firmware
+ *	@adap: the adapter
+ *	@fw_data: the firmware image to write
+ *	@size: image size
+ *
+ *	Write the supplied firmware image to the card's serial flash.
+ */
+int t4_load_fw(struct adapter *adap, const u8 *fw_data, unsigned int size)
+{
+	u32 csum;
+	int ret, addr;
+	unsigned int i;
+	u8 first_page[SF_PAGE_SIZE];
+	const u32 *p = (const u32 *)fw_data;
+	const struct fw_hdr *hdr = (const struct fw_hdr *)fw_data;
+	unsigned int sf_sec_size = adap->params.sf_size / adap->params.sf_nsec;
+
+	if (!size) {
+		CH_ERR(adap, "FW image has no data\n");
+		return -EINVAL;
+	}
+	if (size & 511) {
+		CH_ERR(adap, "FW image size not multiple of 512 bytes\n");
+		return -EINVAL;
+	}
+	if (ntohs(hdr->len512) * 512 != size) {
+		CH_ERR(adap, "FW image size differs from size in FW header\n");
+		return -EINVAL;
+	}
+	if (size > FLASH_FW_MAX_SIZE) {
+		CH_ERR(adap, "FW image too large, max is %u bytes\n",
+		       FLASH_FW_MAX_SIZE);
+		return -EFBIG;
+	}
+
+	for (csum = 0, i = 0; i < size / sizeof(csum); i++)
+		csum += ntohl(p[i]);
+
+	if (csum != 0xffffffff) {
+		CH_ERR(adap, "corrupted firmware image, checksum %#x\n",
+		       csum);
+		return -EINVAL;
+	}
+
+	i = DIV_ROUND_UP(size, sf_sec_size);        /* # of sectors spanned */
+	ret = t4_flash_erase_sectors(adap, FLASH_FW_START_SEC, FLASH_FW_START_SEC + i - 1);
+	if (ret)
+		goto out;
+
+	/*
+	 * We write the correct version at the end so the driver can see a bad
+	 * version if the FW write fails.  Start by writing a copy of the
+	 * first page with a bad version.
+	 */
+	memcpy(first_page, fw_data, SF_PAGE_SIZE);
+	((struct fw_hdr *)first_page)->fw_ver = htonl(0xffffffff);
+	ret = t4_write_flash(adap, FLASH_FW_START, SF_PAGE_SIZE, first_page, 1);
+	if (ret)
+		goto out;
+
+	addr = FLASH_FW_START;
+	for (size -= SF_PAGE_SIZE; size; size -= SF_PAGE_SIZE) {
+		addr += SF_PAGE_SIZE;
+		fw_data += SF_PAGE_SIZE;
+		ret = t4_write_flash(adap, addr, SF_PAGE_SIZE, fw_data, 1);
+		if (ret)
+			goto out;
+	}
+
+	ret = t4_write_flash(adap,
+			     FLASH_FW_START + offsetof(struct fw_hdr, fw_ver),
+			     sizeof(hdr->fw_ver), (const u8 *)&hdr->fw_ver, 1);
+out:
+	if (ret)
+		CH_ERR(adap, "firmware download failed, error %d\n", ret);
+	return ret;
+}
+
+/* BIOS boot headers */
+typedef struct pci_expansion_rom_header {
+	u8	signature[2]; /* ROM Signature. Should be 0xaa55 */
+	u8	reserved[22]; /* Reserved per processor Architecture data */
+	u8	pcir_offset[2]; /* Offset to PCI Data Structure */
+} pci_exp_rom_header_t; /* PCI_EXPANSION_ROM_HEADER */
+
+/* Legacy PCI Expansion ROM Header */
+typedef struct legacy_pci_expansion_rom_header {
+	u8	signature[2]; /* ROM Signature. Should be 0xaa55 */
+	u8	size512; /* Current Image Size in units of 512 bytes */
+	u8	initentry_point[4];
+	u8	cksum; /* Checksum computed on the entire Image */
+	u8	reserved[16]; /* Reserved */
+	u8	pcir_offset[2]; /* Offset to PCI Data Struture */
+} legacy_pci_exp_rom_header_t; /* LEGACY_PCI_EXPANSION_ROM_HEADER */
+
+/* EFI PCI Expansion ROM Header */
+typedef struct efi_pci_expansion_rom_header {
+	u8	signature[2]; // ROM signature. The value 0xaa55
+	u8	initialization_size[2]; /* Units 512. Includes this header */
+	u8	efi_signature[4]; /* Signature from EFI image header. 0x0EF1 */
+	u8	efi_subsystem[2]; /* Subsystem value for EFI image header */
+	u8	efi_machine_type[2]; /* Machine type from EFI image header */
+	u8	compression_type[2]; /* Compression type. */
+		/* 
+		 * Compression type definition
+		 * 0x0: uncompressed
+		 * 0x1: Compressed
+		 * 0x2-0xFFFF: Reserved
+		 */
+	u8	reserved[8]; /* Reserved */
+	u8	efi_image_header_offset[2]; /* Offset to EFI Image */
+	u8	pcir_offset[2]; /* Offset to PCI Data Structure */
+} efi_pci_exp_rom_header_t; /* EFI PCI Expansion ROM Header */
+
+/* PCI Data Structure Format */
+typedef struct pcir_data_structure { /* PCI Data Structure */
+	u8	signature[4]; /* Signature. The string "PCIR" */
+	u8	vendor_id[2]; /* Vendor Identification */
+	u8	device_id[2]; /* Device Identification */
+	u8	vital_product[2]; /* Pointer to Vital Product Data */
+	u8	length[2]; /* PCIR Data Structure Length */
+	u8	revision; /* PCIR Data Structure Revision */
+	u8	class_code[3]; /* Class Code */
+	u8	image_length[2]; /* Image Length. Multiple of 512B */
+	u8	code_revision[2]; /* Revision Level of Code/Data */
+	u8	code_type; /* Code Type. */
+		/*
+		 * PCI Expansion ROM Code Types
+		 * 0x00: Intel IA-32, PC-AT compatible. Legacy
+		 * 0x01: Open Firmware standard for PCI. FCODE
+		 * 0x02: Hewlett-Packard PA RISC. HP reserved
+		 * 0x03: EFI Image. EFI
+		 * 0x04-0xFF: Reserved.
+		 */
+	u8	indicator; /* Indicator. Identifies the last image in the ROM */
+	u8	reserved[2]; /* Reserved */
+} pcir_data_t; /* PCI__DATA_STRUCTURE */
+
+/* BOOT constants */
+enum {
+	BOOT_FLASH_BOOT_ADDR = 0x0,/* start address of boot image in flash */
+	BOOT_SIGNATURE = 0xaa55,   /* signature of BIOS boot ROM */
+	BOOT_SIZE_INC = 512,       /* image size measured in 512B chunks */
+	BOOT_MIN_SIZE = sizeof(pci_exp_rom_header_t), /* basic header */
+	BOOT_MAX_SIZE = 1024*BOOT_SIZE_INC, /* 1 byte * length increment  */
+	VENDOR_ID = 0x1425, /* Vendor ID */
+	PCIR_SIGNATURE = 0x52494350 /* PCIR signature */
+};
+
+/*
+ *	modify_device_id - Modifies the device ID of the Boot BIOS image 
+ *	@adatper: the device ID to write.
+ *	@boot_data: the boot image to modify.
+ *
+ *	Write the supplied device ID to the boot BIOS image.
+ */
+void modify_device_id(int device_id, u8 *boot_data)
+{
+	legacy_pci_exp_rom_header_t *header;
+	pcir_data_t *pcir_header;
+	u32 cur_header = 0;
+
+	/*
+	 * Loop through all chained images and change the device ID's
+	 */
+	while (1) {
+		header = (legacy_pci_exp_rom_header_t *) &boot_data[cur_header];
+		pcir_header = (pcir_data_t *) &boot_data[cur_header +
+			      le16_to_cpu(*(u16*)header->pcir_offset)];
+
+		/*
+		 * Only modify the Device ID if code type is Legacy or HP.
+		 * 0x00: Okay to modify
+		 * 0x01: FCODE. Do not be modify
+		 * 0x03: Okay to modify
+		 * 0x04-0xFF: Do not modify
+		 */
+		if (pcir_header->code_type == 0x00) {
+			u8 csum = 0;
+			int i;
+
+			/*
+			 * Modify Device ID to match current adatper
+			 */
+			*(u16*) pcir_header->device_id = device_id;
+
+			/*
+			 * Set checksum temporarily to 0.
+			 * We will recalculate it later.
+			 */
+			header->cksum = 0x0;
+
+			/*
+			 * Calculate and update checksum
+			 */
+			for (i = 0; i < (header->size512 * 512); i++)
+				csum += (u8)boot_data[cur_header + i];
+
+			/*
+			 * Invert summed value to create the checksum
+			 * Writing new checksum value directly to the boot data
+			 */
+			boot_data[cur_header + 7] = -csum;
+
+		} else if (pcir_header->code_type == 0x03) {
+
+			/*
+			 * Modify Device ID to match current adatper
+			 */
+			*(u16*) pcir_header->device_id = device_id;
+
+		}
+
+
+		/*
+		 * Check indicator element to identify if this is the last
+		 * image in the ROM.
+		 */
+		if (pcir_header->indicator & 0x80)
+			break;
+
+		/*
+		 * Move header pointer up to the next image in the ROM.
+		 */
+		cur_header += header->size512 * 512;
+	}
+}
+
+/*
+ *	t4_load_boot - download boot flash
+ *	@adapter: the adapter
+ *	@boot_data: the boot image to write
+ *	@boot_addr: offset in flash to write boot_data
+ *	@size: image size
+ *
+ *	Write the supplied boot image to the card's serial flash.
+ *	The boot image has the following sections: a 28-byte header and the
+ *	boot image.
+ */
+int t4_load_boot(struct adapter *adap, u8 *boot_data, 
+		 unsigned int boot_addr, unsigned int size)
+{
+	pci_exp_rom_header_t *header;
+	int pcir_offset ;
+	pcir_data_t *pcir_header;
+	int ret, addr;
+	uint16_t device_id;
+	unsigned int i;
+	unsigned int boot_sector = (boot_addr * 1024 );
+	unsigned int sf_sec_size = adap->params.sf_size / adap->params.sf_nsec;
+
+	/*
+	 * Make sure the boot image does not encroach on the firmware region
+	 */
+	if ((boot_sector + size) >> 16 > FLASH_FW_START_SEC) {
+		CH_ERR(adap, "boot image encroaching on firmware region\n");
+		return -EFBIG;
+	}
+
+	/*
+	 * Number of sectors spanned
+	 */
+	i = DIV_ROUND_UP(size ? size : FLASH_BOOTCFG_MAX_SIZE,
+			sf_sec_size);
+	ret = t4_flash_erase_sectors(adap, boot_sector >> 16,
+				     (boot_sector >> 16) + i - 1);
+
+	/*
+	 * If size == 0 then we're simply erasing the FLASH sectors associated
+	 * with the on-adapter option ROM file
+	 */
+	if (ret || (size == 0))
+		goto out;
+
+	/* Get boot header */
+	header = (pci_exp_rom_header_t *)boot_data;
+	pcir_offset = le16_to_cpu(*(u16 *)header->pcir_offset);
+	/* PCIR Data Structure */
+	pcir_header = (pcir_data_t *) &boot_data[pcir_offset];
+
+	/*
+	 * Perform some primitive sanity testing to avoid accidentally
+	 * writing garbage over the boot sectors.  We ought to check for
+	 * more but it's not worth it for now ...
+	 */
+	if (size < BOOT_MIN_SIZE || size > BOOT_MAX_SIZE) {
+		CH_ERR(adap, "boot image too small/large\n");
+		return -EFBIG;
+	}
+
+	/*
+	 * Check BOOT ROM header signature
+	 */
+	if (le16_to_cpu(*(u16*)header->signature) != BOOT_SIGNATURE ) {
+		CH_ERR(adap, "Boot image missing signature\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Check PCI header signature
+	 */
+	if (le32_to_cpu(*(u32*)pcir_header->signature) != PCIR_SIGNATURE) {
+		CH_ERR(adap, "PCI header missing signature\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Check Vendor ID matches Chelsio ID
+	 */
+	if (le16_to_cpu(*(u16*)pcir_header->vendor_id) != VENDOR_ID) {
+		CH_ERR(adap, "Vendor ID missing signature\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * Retrieve adapter's device ID
+	 */
+	t4_os_pci_read_cfg2(adap, PCI_DEVICE_ID, &device_id);
+	/* Want to deal with PF 0 so I strip off PF 4 indicator */
+	device_id = (device_id & 0xff) | 0x4000;
+
+	/*
+	 * Check PCIE Device ID
+	 */
+	if (le16_to_cpu(*(u16*)pcir_header->device_id) != device_id) {
+		/*
+		 * Change the device ID in the Boot BIOS image to match
+		 * the Device ID of the current adapter.
+		 */
+		modify_device_id(device_id, boot_data);
+	}
+
+	/*
+	 * Skip over the first SF_PAGE_SIZE worth of data and write it after
+	 * we finish copying the rest of the boot image. This will ensure
+	 * that the BIOS boot header will only be written if the boot image
+	 * was written in full.
+	 */
+	addr = boot_sector;
+	for (size -= SF_PAGE_SIZE; size; size -= SF_PAGE_SIZE) {
+		addr += SF_PAGE_SIZE; 
+		boot_data += SF_PAGE_SIZE;
+		ret = t4_write_flash(adap, addr, SF_PAGE_SIZE, boot_data, 0);
+		if (ret)
+			goto out;
+	}
+
+	ret = t4_write_flash(adap, boot_sector, SF_PAGE_SIZE,
+			     (const u8 *)header, 0);
+
+out:
+	if (ret)
+		CH_ERR(adap, "boot image download failed, error %d\n", ret);
+	return ret;
+}
+
+/**
+ *	t4_read_cimq_cfg - read CIM queue configuration
+ *	@adap: the adapter
+ *	@base: holds the queue base addresses in bytes
+ *	@size: holds the queue sizes in bytes
+ *	@thres: holds the queue full thresholds in bytes
+ *
+ *	Returns the current configuration of the CIM queues, starting with
+ *	the IBQs, then the OBQs.
+ */
+void t4_read_cimq_cfg(struct adapter *adap, u16 *base, u16 *size, u16 *thres)
+{
+	unsigned int i, v;
+
+	for (i = 0; i < CIM_NUM_IBQ; i++) {
+		t4_write_reg(adap, A_CIM_QUEUE_CONFIG_REF, F_IBQSELECT |
+			     V_QUENUMSELECT(i));
+		v = t4_read_reg(adap, A_CIM_QUEUE_CONFIG_CTRL);
+		*base++ = G_CIMQBASE(v) * 256; /* value is in 256-byte units */
+		*size++ = G_CIMQSIZE(v) * 256; /* value is in 256-byte units */
+		*thres++ = G_QUEFULLTHRSH(v) * 8;   /* 8-byte unit */
+	}
+	for (i = 0; i < CIM_NUM_OBQ; i++) {
+		t4_write_reg(adap, A_CIM_QUEUE_CONFIG_REF, F_OBQSELECT |
+			     V_QUENUMSELECT(i));
+		v = t4_read_reg(adap, A_CIM_QUEUE_CONFIG_CTRL);
+		*base++ = G_CIMQBASE(v) * 256; /* value is in 256-byte units */
+		*size++ = G_CIMQSIZE(v) * 256; /* value is in 256-byte units */
+	}
+}
+
+/**
+ *	t4_read_cim_ibq - read the contents of a CIM inbound queue
+ *	@adap: the adapter
+ *	@qid: the queue index
+ *	@data: where to store the queue contents
+ *	@n: capacity of @data in 32-bit words
+ *
+ *	Reads the contents of the selected CIM queue starting at address 0 up
+ *	to the capacity of @data.  @n must be a multiple of 4.  Returns < 0 on
+ *	error and the number of 32-bit words actually read on success.
+ */
+int t4_read_cim_ibq(struct adapter *adap, unsigned int qid, u32 *data, size_t n)
+{
+	int i, err;
+	unsigned int addr;
+	const unsigned int nwords = CIM_IBQ_SIZE * 4;
+
+	if (qid > 5 || (n & 3))
+		return -EINVAL;
+
+	addr = qid * nwords;
+	if (n > nwords)
+		n = nwords;
+
+	for (i = 0; i < n; i++, addr++) {
+		t4_write_reg(adap, A_CIM_IBQ_DBG_CFG, V_IBQDBGADDR(addr) |
+			     F_IBQDBGEN);
+		err = t4_wait_op_done(adap, A_CIM_IBQ_DBG_CFG, F_IBQDBGBUSY, 0,
+				      2, 1);
+		if (err)
+			return err;
+		*data++ = t4_read_reg(adap, A_CIM_IBQ_DBG_DATA);
+	}
+	t4_write_reg(adap, A_CIM_IBQ_DBG_CFG, 0);
+	return i;
+}
+
+/**
+ *	t4_read_cim_obq - read the contents of a CIM outbound queue
+ *	@adap: the adapter
+ *	@qid: the queue index
+ *	@data: where to store the queue contents
+ *	@n: capacity of @data in 32-bit words
+ *
+ *	Reads the contents of the selected CIM queue starting at address 0 up
+ *	to the capacity of @data.  @n must be a multiple of 4.  Returns < 0 on
+ *	error and the number of 32-bit words actually read on success.
+ */
+int t4_read_cim_obq(struct adapter *adap, unsigned int qid, u32 *data, size_t n)
+{
+	int i, err;
+	unsigned int addr, v, nwords;
+
+	if (qid > 5 || (n & 3))
+		return -EINVAL;
+
+	t4_write_reg(adap, A_CIM_QUEUE_CONFIG_REF, F_OBQSELECT |
+		     V_QUENUMSELECT(qid));
+	v = t4_read_reg(adap, A_CIM_QUEUE_CONFIG_CTRL);
+
+	addr = G_CIMQBASE(v) * 64;    /* muliple of 256 -> muliple of 4 */
+	nwords = G_CIMQSIZE(v) * 64;  /* same */
+	if (n > nwords)
+		n = nwords;
+
+	for (i = 0; i < n; i++, addr++) {
+		t4_write_reg(adap, A_CIM_OBQ_DBG_CFG, V_OBQDBGADDR(addr) |
+			     F_OBQDBGEN);
+		err = t4_wait_op_done(adap, A_CIM_OBQ_DBG_CFG, F_OBQDBGBUSY, 0,
+				      2, 1);
+		if (err)
+			return err;
+		*data++ = t4_read_reg(adap, A_CIM_OBQ_DBG_DATA);
+	}
+	t4_write_reg(adap, A_CIM_OBQ_DBG_CFG, 0);
+	return i;
+}
+
+enum {
+	CIM_QCTL_BASE     = 0,
+	CIM_CTL_BASE      = 0x2000,
+	CIM_PBT_ADDR_BASE = 0x2800,
+	CIM_PBT_LRF_BASE  = 0x3000,
+	CIM_PBT_DATA_BASE = 0x3800
+};
+
+/**
+ *	t4_cim_read - read a block from CIM internal address space
+ *	@adap: the adapter
+ *	@addr: the start address within the CIM address space
+ *	@n: number of words to read
+ *	@valp: where to store the result
+ *
+ *	Reads a block of 4-byte words from the CIM intenal address space.
+ */
+int t4_cim_read(struct adapter *adap, unsigned int addr, unsigned int n,
+		unsigned int *valp)
+{
+	int ret = 0;
+
+	if (t4_read_reg(adap, A_CIM_HOST_ACC_CTRL) & F_HOSTBUSY)
+		return -EBUSY;
+
+	for ( ; !ret && n--; addr += 4) {
+		t4_write_reg(adap, A_CIM_HOST_ACC_CTRL, addr);
+		ret = t4_wait_op_done(adap, A_CIM_HOST_ACC_CTRL, F_HOSTBUSY,
+				      0, 5, 2);
+		if (!ret)
+			*valp++ = t4_read_reg(adap, A_CIM_HOST_ACC_DATA);
+	}
+	return ret;
+}
+
+/**
+ *	t4_cim_write - write a block into CIM internal address space
+ *	@adap: the adapter
+ *	@addr: the start address within the CIM address space
+ *	@n: number of words to write
+ *	@valp: set of values to write
+ *
+ *	Writes a block of 4-byte words into the CIM intenal address space.
+ */
+int t4_cim_write(struct adapter *adap, unsigned int addr, unsigned int n,
+		 const unsigned int *valp)
+{
+	int ret = 0;
+
+	if (t4_read_reg(adap, A_CIM_HOST_ACC_CTRL) & F_HOSTBUSY)
+		return -EBUSY;
+
+	for ( ; !ret && n--; addr += 4) {
+		t4_write_reg(adap, A_CIM_HOST_ACC_DATA, *valp++);
+		t4_write_reg(adap, A_CIM_HOST_ACC_CTRL, addr | F_HOSTWRITE);
+		ret = t4_wait_op_done(adap, A_CIM_HOST_ACC_CTRL, F_HOSTBUSY,
+				      0, 5, 2);
+	}
+	return ret;
+}
+
+static int t4_cim_write1(struct adapter *adap, unsigned int addr, unsigned int val)
+{
+	return t4_cim_write(adap, addr, 1, &val);
+}
+
+/**
+ *	t4_cim_ctl_read - read a block from CIM control region
+ *	@adap: the adapter
+ *	@addr: the start address within the CIM control region
+ *	@n: number of words to read
+ *	@valp: where to store the result
+ *
+ *	Reads a block of 4-byte words from the CIM control region.
+ */
+int t4_cim_ctl_read(struct adapter *adap, unsigned int addr, unsigned int n,
+		    unsigned int *valp)
+{
+	return t4_cim_read(adap, addr + CIM_CTL_BASE, n, valp);
+}
+
+/**
+ *	t4_cim_read_la - read CIM LA capture buffer
+ *	@adap: the adapter
+ *	@la_buf: where to store the LA data
+ *	@wrptr: the HW write pointer within the capture buffer
+ *
+ *	Reads the contents of the CIM LA buffer with the most recent entry at
+ *	the end	of the returned data and with the entry at @wrptr first.
+ *	We try to leave the LA in the running state we find it in.
+ */
+int t4_cim_read_la(struct adapter *adap, u32 *la_buf, unsigned int *wrptr)
+{
+	int i, ret;
+	unsigned int cfg, val, idx;
+
+	ret = t4_cim_read(adap, A_UP_UP_DBG_LA_CFG, 1, &cfg);
+	if (ret)
+		return ret;
+
+	if (cfg & F_UPDBGLAEN) {                /* LA is running, freeze it */
+		ret = t4_cim_write1(adap, A_UP_UP_DBG_LA_CFG, 0);
+		if (ret)
+			return ret;
+	}
+
+	ret = t4_cim_read(adap, A_UP_UP_DBG_LA_CFG, 1, &val);
+	if (ret)
+		goto restart;
+
+	idx = G_UPDBGLAWRPTR(val);
+	if (wrptr)
+		*wrptr = idx;
+
+	for (i = 0; i < adap->params.cim_la_size; i++) {
+		ret = t4_cim_write1(adap, A_UP_UP_DBG_LA_CFG,
+				    V_UPDBGLARDPTR(idx) | F_UPDBGLARDEN);
+		if (ret)
+			break;
+		ret = t4_cim_read(adap, A_UP_UP_DBG_LA_CFG, 1, &val);
+		if (ret)
+			break;
+		if (val & F_UPDBGLARDEN) {
+			ret = -ETIMEDOUT;
+			break;
+		}
+		ret = t4_cim_read(adap, A_UP_UP_DBG_LA_DATA, 1, &la_buf[i]);
+		if (ret)
+			break;
+		idx = (idx + 1) & M_UPDBGLARDPTR;
+	}
+restart:
+	if (cfg & F_UPDBGLAEN) {
+		int r = t4_cim_write1(adap, A_UP_UP_DBG_LA_CFG,
+				      cfg & ~F_UPDBGLARDEN);
+		if (!ret)
+			ret = r;
+	}
+	return ret;
+}
+
+void t4_cim_read_pif_la(struct adapter *adap, u32 *pif_req, u32 *pif_rsp,
+			unsigned int *pif_req_wrptr,
+			unsigned int *pif_rsp_wrptr)
+{
+	int i, j;
+	u32 cfg, val, req, rsp;
+
+	cfg = t4_read_reg(adap, A_CIM_DEBUGCFG);
+	if (cfg & F_LADBGEN)
+		t4_write_reg(adap, A_CIM_DEBUGCFG, cfg ^ F_LADBGEN);
+
+	val = t4_read_reg(adap, A_CIM_DEBUGSTS);
+	req = G_POLADBGWRPTR(val);
+	rsp = G_PILADBGWRPTR(val);
+	if (pif_req_wrptr)
+		*pif_req_wrptr = req;
+	if (pif_rsp_wrptr)
+		*pif_rsp_wrptr = rsp;
+
+	for (i = 0; i < CIM_PIFLA_SIZE; i++) {
+		for (j = 0; j < 6; j++) {
+			t4_write_reg(adap, A_CIM_DEBUGCFG, V_POLADBGRDPTR(req) |
+				     V_PILADBGRDPTR(rsp));
+			*pif_req++ = t4_read_reg(adap, A_CIM_PO_LA_DEBUGDATA);
+			*pif_rsp++ = t4_read_reg(adap, A_CIM_PI_LA_DEBUGDATA);
+			req++;
+			rsp++;
+		}
+		req = (req + 2) & M_POLADBGRDPTR;
+		rsp = (rsp + 2) & M_PILADBGRDPTR;
+	}
+	t4_write_reg(adap, A_CIM_DEBUGCFG, cfg);
+}
+
+void t4_cim_read_ma_la(struct adapter *adap, u32 *ma_req, u32 *ma_rsp)
+{
+	u32 cfg;
+	int i, j, idx;
+
+	cfg = t4_read_reg(adap, A_CIM_DEBUGCFG);
+	if (cfg & F_LADBGEN)
+		t4_write_reg(adap, A_CIM_DEBUGCFG, cfg ^ F_LADBGEN);
+
+	for (i = 0; i < CIM_MALA_SIZE; i++) {
+		for (j = 0; j < 5; j++) {
+			idx = 8 * i + j;
+			t4_write_reg(adap, A_CIM_DEBUGCFG, V_POLADBGRDPTR(idx) |
+				     V_PILADBGRDPTR(idx));
+			*ma_req++ = t4_read_reg(adap, A_CIM_PO_LA_MADEBUGDATA);
+			*ma_rsp++ = t4_read_reg(adap, A_CIM_PI_LA_MADEBUGDATA);
+		}
+	}
+	t4_write_reg(adap, A_CIM_DEBUGCFG, cfg);
+}
+
+/**
+ *	t4_tp_read_la - read TP LA capture buffer
+ *	@adap: the adapter
+ *	@la_buf: where to store the LA data
+ *	@wrptr: the HW write pointer within the capture buffer
+ *
+ *	Reads the contents of the TP LA buffer with the most recent entry at
+ *	the end	of the returned data and with the entry at @wrptr first.
+ *	We leave the LA in the running state we find it in.
+ */
+void t4_tp_read_la(struct adapter *adap, u64 *la_buf, unsigned int *wrptr)
+{
+	bool last_incomplete;
+	unsigned int i, cfg, val, idx;
+
+	cfg = t4_read_reg(adap, A_TP_DBG_LA_CONFIG) & 0xffff;
+	if (cfg & F_DBGLAENABLE)                    /* freeze LA */
+		t4_write_reg(adap, A_TP_DBG_LA_CONFIG,
+			     adap->params.tp.la_mask | (cfg ^ F_DBGLAENABLE));
+
+	val = t4_read_reg(adap, A_TP_DBG_LA_CONFIG);
+	idx = G_DBGLAWPTR(val);
+	last_incomplete = G_DBGLAMODE(val) >= 2 && (val & F_DBGLAWHLF) == 0;
+	if (last_incomplete)
+		idx = (idx + 1) & M_DBGLARPTR;
+	if (wrptr)
+		*wrptr = idx;
+
+	val &= 0xffff;
+	val &= ~V_DBGLARPTR(M_DBGLARPTR);
+	val |= adap->params.tp.la_mask;
+
+	for (i = 0; i < TPLA_SIZE; i++) {
+		t4_write_reg(adap, A_TP_DBG_LA_CONFIG, V_DBGLARPTR(idx) | val);
+		la_buf[i] = t4_read_reg64(adap, A_TP_DBG_LA_DATAL);
+		idx = (idx + 1) & M_DBGLARPTR;
+	}
+
+	/* Wipe out last entry if it isn't valid */
+	if (last_incomplete)
+		la_buf[TPLA_SIZE - 1] = ~0ULL;
+
+	if (cfg & F_DBGLAENABLE)                    /* restore running state */
+		t4_write_reg(adap, A_TP_DBG_LA_CONFIG,
+			     cfg | adap->params.tp.la_mask);
+}
+
+void t4_ulprx_read_la(struct adapter *adap, u32 *la_buf)
+{
+	unsigned int i, j;
+
+	for (i = 0; i < 8; i++) {
+		u32 *p = la_buf + i;
+
+		t4_write_reg(adap, A_ULP_RX_LA_CTL, i);
+		j = t4_read_reg(adap, A_ULP_RX_LA_WRPTR);
+		t4_write_reg(adap, A_ULP_RX_LA_RDPTR, j);
+		for (j = 0; j < ULPRX_LA_SIZE; j++, p += 8)
+			*p = t4_read_reg(adap, A_ULP_RX_LA_RDDATA);
+	}
+}
+
+#define ADVERT_MASK (FW_PORT_CAP_SPEED_100M | FW_PORT_CAP_SPEED_1G |\
+		     FW_PORT_CAP_SPEED_10G | FW_PORT_CAP_ANEG)
+
+/**
+ *	t4_link_start - apply link configuration to MAC/PHY
+ *	@phy: the PHY to setup
+ *	@mac: the MAC to setup
+ *	@lc: the requested link configuration
+ *
+ *	Set up a port's MAC and PHY according to a desired link configuration.
+ *	- If the PHY can auto-negotiate first decide what to advertise, then
+ *	  enable/disable auto-negotiation as desired, and reset.
+ *	- If the PHY does not auto-negotiate just reset it.
+ *	- If auto-negotiation is off set the MAC to the proper speed/duplex/FC,
+ *	  otherwise do it later based on the outcome of auto-negotiation.
+ */
+int t4_link_start(struct adapter *adap, unsigned int mbox, unsigned int port,
+		  struct link_config *lc)
+{
+	struct fw_port_cmd c;
+	unsigned int fc = 0, mdi = V_FW_PORT_CAP_MDI(FW_PORT_CAP_MDI_AUTO);
+
+	lc->link_ok = 0;
+	if (lc->requested_fc & PAUSE_RX)
+		fc |= FW_PORT_CAP_FC_RX;
+	if (lc->requested_fc & PAUSE_TX)
+		fc |= FW_PORT_CAP_FC_TX;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_portid = htonl(V_FW_CMD_OP(FW_PORT_CMD) | F_FW_CMD_REQUEST |
+			       F_FW_CMD_EXEC | V_FW_PORT_CMD_PORTID(port));
+	c.action_to_len16 = htonl(V_FW_PORT_CMD_ACTION(FW_PORT_ACTION_L1_CFG) |
+				  FW_LEN16(c));
+
+	if (!(lc->supported & FW_PORT_CAP_ANEG)) {
+		c.u.l1cfg.rcap = htonl((lc->supported & ADVERT_MASK) | fc);
+		lc->fc = lc->requested_fc & (PAUSE_RX | PAUSE_TX);
+	} else if (lc->autoneg == AUTONEG_DISABLE) {
+		c.u.l1cfg.rcap = htonl(lc->requested_speed | fc | mdi);
+		lc->fc = lc->requested_fc & (PAUSE_RX | PAUSE_TX);
+	} else
+		c.u.l1cfg.rcap = htonl(lc->advertising | fc | mdi);
+
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ *	t4_restart_aneg - restart autonegotiation
+ *	@adap: the adapter
+ *	@mbox: mbox to use for the FW command
+ *	@port: the port id
+ *
+ *	Restarts autonegotiation for the selected port.
+ */
+int t4_restart_aneg(struct adapter *adap, unsigned int mbox, unsigned int port)
+{
+	struct fw_port_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_portid = htonl(V_FW_CMD_OP(FW_PORT_CMD) | F_FW_CMD_REQUEST |
+			       F_FW_CMD_EXEC | V_FW_PORT_CMD_PORTID(port));
+	c.action_to_len16 = htonl(V_FW_PORT_CMD_ACTION(FW_PORT_ACTION_L1_CFG) |
+				  FW_LEN16(c));
+	c.u.l1cfg.rcap = htonl(FW_PORT_CAP_ANEG);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+typedef void (*int_handler_t)(struct adapter *adap);
+
+struct intr_info {
+	unsigned int mask;       /* bits to check in interrupt status */
+	const char *msg;         /* message to print or NULL */
+	short stat_idx;          /* stat counter to increment or -1 */
+	unsigned short fatal;    /* whether the condition reported is fatal */
+	int_handler_t int_handler; /* platform-specific int handler */
+};
+
+/**
+ *	t4_handle_intr_status - table driven interrupt handler
+ *	@adapter: the adapter that generated the interrupt
+ *	@reg: the interrupt status register to process
+ *	@acts: table of interrupt actions
+ *
+ *	A table driven interrupt handler that applies a set of masks to an
+ *	interrupt status word and performs the corresponding actions if the
+ *	interrupts described by the mask have occured.  The actions include
+ *	optionally emitting a warning or alert message. The table is terminated
+ *	by an entry specifying mask 0.  Returns the number of fatal interrupt
+ *	conditions.
+ */
+static int t4_handle_intr_status(struct adapter *adapter, unsigned int reg,
+				 const struct intr_info *acts)
+{
+	int fatal = 0;
+	unsigned int mask = 0;
+	unsigned int status = t4_read_reg(adapter, reg);
+
+	for ( ; acts->mask; ++acts) {
+		if (!(status & acts->mask))
+			continue;
+		if (acts->fatal) {
+			fatal++;
+			CH_ALERT(adapter, "%s (0x%x)\n",
+				 acts->msg, status & acts->mask);
+		} else if (acts->msg)
+			CH_WARN_RATELIMIT(adapter, "%s (0x%x)\n",
+					  acts->msg, status & acts->mask);
+		if (acts->int_handler)
+			acts->int_handler(adapter);
+		mask |= acts->mask;
+	}
+	status &= mask;
+	if (status)                           /* clear processed interrupts */
+		t4_write_reg(adapter, reg, status);
+	return fatal;
+}
+
+/*
+ * Interrupt handler for the PCIE module.
+ */
+static void pcie_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info sysbus_intr_info[] = {
+		{ F_RNPP, "RXNP array parity error", -1, 1 },
+		{ F_RPCP, "RXPC array parity error", -1, 1 },
+		{ F_RCIP, "RXCIF array parity error", -1, 1 },
+		{ F_RCCP, "Rx completions control array parity error", -1, 1 },
+		{ F_RFTP, "RXFT array parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info pcie_port_intr_info[] = {
+		{ F_TPCP, "TXPC array parity error", -1, 1 },
+		{ F_TNPP, "TXNP array parity error", -1, 1 },
+		{ F_TFTP, "TXFT array parity error", -1, 1 },
+		{ F_TCAP, "TXCA array parity error", -1, 1 },
+		{ F_TCIP, "TXCIF array parity error", -1, 1 },
+		{ F_RCAP, "RXCA array parity error", -1, 1 },
+		{ F_OTDD, "outbound request TLP discarded", -1, 1 },
+		{ F_RDPE, "Rx data parity error", -1, 1 },
+		{ F_TDUE, "Tx uncorrectable data error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info pcie_intr_info[] = {
+		{ F_MSIADDRLPERR, "MSI AddrL parity error", -1, 1 },
+		{ F_MSIADDRHPERR, "MSI AddrH parity error", -1, 1 },
+		{ F_MSIDATAPERR, "MSI data parity error", -1, 1 },
+		{ F_MSIXADDRLPERR, "MSI-X AddrL parity error", -1, 1 },
+		{ F_MSIXADDRHPERR, "MSI-X AddrH parity error", -1, 1 },
+		{ F_MSIXDATAPERR, "MSI-X data parity error", -1, 1 },
+		{ F_MSIXDIPERR, "MSI-X DI parity error", -1, 1 },
+		{ F_PIOCPLPERR, "PCI PIO completion FIFO parity error", -1, 1 },
+		{ F_PIOREQPERR, "PCI PIO request FIFO parity error", -1, 1 },
+		{ F_TARTAGPERR, "PCI PCI target tag FIFO parity error", -1, 1 },
+		{ F_CCNTPERR, "PCI CMD channel count parity error", -1, 1 },
+		{ F_CREQPERR, "PCI CMD channel request parity error", -1, 1 },
+		{ F_CRSPPERR, "PCI CMD channel response parity error", -1, 1 },
+		{ F_DCNTPERR, "PCI DMA channel count parity error", -1, 1 },
+		{ F_DREQPERR, "PCI DMA channel request parity error", -1, 1 },
+		{ F_DRSPPERR, "PCI DMA channel response parity error", -1, 1 },
+		{ F_HCNTPERR, "PCI HMA channel count parity error", -1, 1 },
+		{ F_HREQPERR, "PCI HMA channel request parity error", -1, 1 },
+		{ F_HRSPPERR, "PCI HMA channel response parity error", -1, 1 },
+		{ F_CFGSNPPERR, "PCI config snoop FIFO parity error", -1, 1 },
+		{ F_FIDPERR, "PCI FID parity error", -1, 1 },
+		{ F_INTXCLRPERR, "PCI INTx clear parity error", -1, 1 },
+		{ F_MATAGPERR, "PCI MA tag parity error", -1, 1 },
+		{ F_PIOTAGPERR, "PCI PIO tag parity error", -1, 1 },
+		{ F_RXCPLPERR, "PCI Rx completion parity error", -1, 1 },
+		{ F_RXWRPERR, "PCI Rx write parity error", -1, 1 },
+		{ F_RPLPERR, "PCI replay buffer parity error", -1, 1 },
+		{ F_PCIESINT, "PCI core secondary fault", -1, 1 },
+		{ F_PCIEPINT, "PCI core primary fault", -1, 1 },
+		{ F_UNXSPLCPLERR, "PCI unexpected split completion error", -1,
+		  0 },
+		{ 0 }
+	};
+
+	int fat;
+
+	fat = t4_handle_intr_status(adapter,
+				    A_PCIE_CORE_UTL_SYSTEM_BUS_AGENT_STATUS,
+				    sysbus_intr_info) +
+	      t4_handle_intr_status(adapter,
+				    A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_STATUS,
+				    pcie_port_intr_info) +
+	      t4_handle_intr_status(adapter, A_PCIE_INT_CAUSE, pcie_intr_info);
+	if (fat)
+		t4_fatal_err(adapter);
+}
+
+/*
+ * TP interrupt handler.
+ */
+static void tp_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info tp_intr_info[] = {
+		{ 0x3fffffff, "TP parity error", -1, 1 },
+		{ F_FLMTXFLSTEMPTY, "TP out of Tx pages", -1, 1 },
+		{ 0 }
+	};
+
+	if (t4_handle_intr_status(adapter, A_TP_INT_CAUSE, tp_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * SGE interrupt handler.
+ */
+static void sge_intr_handler(struct adapter *adapter)
+{
+	u64 v;
+	u32 err;
+
+	static struct intr_info sge_intr_info[] = {
+		{ F_ERR_CPL_EXCEED_IQE_SIZE,
+		  "SGE received CPL exceeding IQE size", -1, 1 },
+		{ F_ERR_INVALID_CIDX_INC,
+		  "SGE GTS CIDX increment too large", -1, 0 },
+		{ F_ERR_CPL_OPCODE_0, "SGE received 0-length CPL", -1, 0 },
+		{ F_DBFIFO_LP_INT, NULL, -1, 0, t4_db_full },
+		{ F_DBFIFO_HP_INT, NULL, -1, 0, t4_db_full },
+		{ F_ERR_DROPPED_DB, NULL, -1, 0, t4_db_dropped },
+		{ F_ERR_DATA_CPL_ON_HIGH_QID1 | F_ERR_DATA_CPL_ON_HIGH_QID0,
+		  "SGE IQID > 1023 received CPL for FL", -1, 0 },
+		{ F_ERR_BAD_DB_PIDX3, "SGE DBP 3 pidx increment too large", -1,
+		  0 },
+		{ F_ERR_BAD_DB_PIDX2, "SGE DBP 2 pidx increment too large", -1,
+		  0 },
+		{ F_ERR_BAD_DB_PIDX1, "SGE DBP 1 pidx increment too large", -1,
+		  0 },
+		{ F_ERR_BAD_DB_PIDX0, "SGE DBP 0 pidx increment too large", -1,
+		  0 },
+		{ F_ERR_ING_CTXT_PRIO,
+		  "SGE too many priority ingress contexts", -1, 0 },
+		{ F_ERR_EGR_CTXT_PRIO,
+		  "SGE too many priority egress contexts", -1, 0 },
+		{ F_INGRESS_SIZE_ERR, "SGE illegal ingress QID", -1, 0 },
+		{ F_EGRESS_SIZE_ERR, "SGE illegal egress QID", -1, 0 },
+		{ 0 }
+	};
+
+	v = (u64)t4_read_reg(adapter, A_SGE_INT_CAUSE1) |
+	    ((u64)t4_read_reg(adapter, A_SGE_INT_CAUSE2) << 32);
+	if (v) {
+		CH_ALERT(adapter, "SGE parity error (%#llx)\n",
+			 (unsigned long long)v);
+		t4_write_reg(adapter, A_SGE_INT_CAUSE1, v);
+		t4_write_reg(adapter, A_SGE_INT_CAUSE2, v >> 32);
+	}
+
+	v |= t4_handle_intr_status(adapter, A_SGE_INT_CAUSE3, sge_intr_info);
+
+	err = t4_read_reg(adapter, A_SGE_ERROR_STATS);
+	if (err & F_ERROR_QID_VALID) {
+		CH_ERR(adapter, "SGE error for queue %u\n", G_ERROR_QID(err));
+		if (err & F_UNCAPTURED_ERROR)
+			CH_ERR(adapter, "SGE UNCAPTURED_ERROR set (clearing)\n");
+		t4_write_reg(adapter, A_SGE_ERROR_STATS, F_ERROR_QID_VALID |
+			     F_UNCAPTURED_ERROR);
+	}
+
+	if (v != 0)
+		t4_fatal_err(adapter);
+}
+
+#define CIM_OBQ_INTR (F_OBQULP0PARERR | F_OBQULP1PARERR | F_OBQULP2PARERR |\
+		      F_OBQULP3PARERR | F_OBQSGEPARERR | F_OBQNCSIPARERR)
+#define CIM_IBQ_INTR (F_IBQTP0PARERR | F_IBQTP1PARERR | F_IBQULPPARERR |\
+		      F_IBQSGEHIPARERR | F_IBQSGELOPARERR | F_IBQNCSIPARERR)
+
+/*
+ * CIM interrupt handler.
+ */
+static void cim_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info cim_intr_info[] = {
+		{ F_PREFDROPINT, "CIM control register prefetch drop", -1, 1 },
+		{ CIM_OBQ_INTR, "CIM OBQ parity error", -1, 1 },
+		{ CIM_IBQ_INTR, "CIM IBQ parity error", -1, 1 },
+		{ F_MBUPPARERR, "CIM mailbox uP parity error", -1, 1 },
+		{ F_MBHOSTPARERR, "CIM mailbox host parity error", -1, 1 },
+		{ F_TIEQINPARERRINT, "CIM TIEQ outgoing parity error", -1, 1 },
+		{ F_TIEQOUTPARERRINT, "CIM TIEQ incoming parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info cim_upintr_info[] = {
+		{ F_RSVDSPACEINT, "CIM reserved space access", -1, 1 },
+		{ F_ILLTRANSINT, "CIM illegal transaction", -1, 1 },
+		{ F_ILLWRINT, "CIM illegal write", -1, 1 },
+		{ F_ILLRDINT, "CIM illegal read", -1, 1 },
+		{ F_ILLRDBEINT, "CIM illegal read BE", -1, 1 },
+		{ F_ILLWRBEINT, "CIM illegal write BE", -1, 1 },
+		{ F_SGLRDBOOTINT, "CIM single read from boot space", -1, 1 },
+		{ F_SGLWRBOOTINT, "CIM single write to boot space", -1, 1 },
+		{ F_BLKWRBOOTINT, "CIM block write to boot space", -1, 1 },
+		{ F_SGLRDFLASHINT, "CIM single read from flash space", -1, 1 },
+		{ F_SGLWRFLASHINT, "CIM single write to flash space", -1, 1 },
+		{ F_BLKWRFLASHINT, "CIM block write to flash space", -1, 1 },
+		{ F_SGLRDEEPROMINT, "CIM single EEPROM read", -1, 1 },
+		{ F_SGLWREEPROMINT, "CIM single EEPROM write", -1, 1 },
+		{ F_BLKRDEEPROMINT, "CIM block EEPROM read", -1, 1 },
+		{ F_BLKWREEPROMINT, "CIM block EEPROM write", -1, 1 },
+		{ F_SGLRDCTLINT , "CIM single read from CTL space", -1, 1 },
+		{ F_SGLWRCTLINT , "CIM single write to CTL space", -1, 1 },
+		{ F_BLKRDCTLINT , "CIM block read from CTL space", -1, 1 },
+		{ F_BLKWRCTLINT , "CIM block write to CTL space", -1, 1 },
+		{ F_SGLRDPLINT , "CIM single read from PL space", -1, 1 },
+		{ F_SGLWRPLINT , "CIM single write to PL space", -1, 1 },
+		{ F_BLKRDPLINT , "CIM block read from PL space", -1, 1 },
+		{ F_BLKWRPLINT , "CIM block write to PL space", -1, 1 },
+		{ F_REQOVRLOOKUPINT , "CIM request FIFO overwrite", -1, 1 },
+		{ F_RSPOVRLOOKUPINT , "CIM response FIFO overwrite", -1, 1 },
+		{ F_TIMEOUTINT , "CIM PIF timeout", -1, 1 },
+		{ F_TIMEOUTMAINT , "CIM PIF MA timeout", -1, 1 },
+		{ 0 }
+        };
+
+	int fat;
+
+	fat = t4_handle_intr_status(adapter, A_CIM_HOST_INT_CAUSE,
+				    cim_intr_info) +
+	      t4_handle_intr_status(adapter, A_CIM_HOST_UPACC_INT_CAUSE,
+				    cim_upintr_info);
+	if (fat)
+		t4_fatal_err(adapter);
+}
+
+/*
+ * ULP RX interrupt handler.
+ */
+static void ulprx_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info ulprx_intr_info[] = {
+		{ F_CAUSE_CTX_1, "ULPRX channel 1 context error", -1, 1 },
+		{ F_CAUSE_CTX_0, "ULPRX channel 0 context error", -1, 1 },
+		{ 0x7fffff, "ULPRX parity error", -1, 1 },
+		{ 0 }
+        };
+
+	if (t4_handle_intr_status(adapter, A_ULP_RX_INT_CAUSE, ulprx_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * ULP TX interrupt handler.
+ */
+static void ulptx_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info ulptx_intr_info[] = {
+		{ F_PBL_BOUND_ERR_CH3, "ULPTX channel 3 PBL out of bounds", -1,
+		  0 },
+		{ F_PBL_BOUND_ERR_CH2, "ULPTX channel 2 PBL out of bounds", -1,
+		  0 },
+		{ F_PBL_BOUND_ERR_CH1, "ULPTX channel 1 PBL out of bounds", -1,
+		  0 },
+		{ F_PBL_BOUND_ERR_CH0, "ULPTX channel 0 PBL out of bounds", -1,
+		  0 },
+		{ 0xfffffff, "ULPTX parity error", -1, 1 },
+		{ 0 }
+        };
+
+	if (t4_handle_intr_status(adapter, A_ULP_TX_INT_CAUSE, ulptx_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * PM TX interrupt handler.
+ */
+static void pmtx_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info pmtx_intr_info[] = {
+		{ F_PCMD_LEN_OVFL0, "PMTX channel 0 pcmd too large", -1, 1 },
+		{ F_PCMD_LEN_OVFL1, "PMTX channel 1 pcmd too large", -1, 1 },
+		{ F_PCMD_LEN_OVFL2, "PMTX channel 2 pcmd too large", -1, 1 },
+		{ F_ZERO_C_CMD_ERROR, "PMTX 0-length pcmd", -1, 1 },
+		{ 0xffffff0, "PMTX framing error", -1, 1 },
+		{ F_OESPI_PAR_ERROR, "PMTX oespi parity error", -1, 1 },
+		{ F_DB_OPTIONS_PAR_ERROR, "PMTX db_options parity error", -1,
+		  1 },
+		{ F_ICSPI_PAR_ERROR, "PMTX icspi parity error", -1, 1 },
+		{ F_C_PCMD_PAR_ERROR, "PMTX c_pcmd parity error", -1, 1},
+		{ 0 }
+        };
+
+	if (t4_handle_intr_status(adapter, A_PM_TX_INT_CAUSE, pmtx_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * PM RX interrupt handler.
+ */
+static void pmrx_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info pmrx_intr_info[] = {
+		{ F_ZERO_E_CMD_ERROR, "PMRX 0-length pcmd", -1, 1 },
+		{ 0x3ffff0, "PMRX framing error", -1, 1 },
+		{ F_OCSPI_PAR_ERROR, "PMRX ocspi parity error", -1, 1 },
+		{ F_DB_OPTIONS_PAR_ERROR, "PMRX db_options parity error", -1,
+		  1 },
+		{ F_IESPI_PAR_ERROR, "PMRX iespi parity error", -1, 1 },
+		{ F_E_PCMD_PAR_ERROR, "PMRX e_pcmd parity error", -1, 1},
+		{ 0 }
+        };
+
+	if (t4_handle_intr_status(adapter, A_PM_RX_INT_CAUSE, pmrx_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * CPL switch interrupt handler.
+ */
+static void cplsw_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info cplsw_intr_info[] = {
+		{ F_CIM_OP_MAP_PERR, "CPLSW CIM op_map parity error", -1, 1 },
+		{ F_CIM_OVFL_ERROR, "CPLSW CIM overflow", -1, 1 },
+		{ F_TP_FRAMING_ERROR, "CPLSW TP framing error", -1, 1 },
+		{ F_SGE_FRAMING_ERROR, "CPLSW SGE framing error", -1, 1 },
+		{ F_CIM_FRAMING_ERROR, "CPLSW CIM framing error", -1, 1 },
+		{ F_ZERO_SWITCH_ERROR, "CPLSW no-switch error", -1, 1 },
+		{ 0 }
+        };
+
+	if (t4_handle_intr_status(adapter, A_CPL_INTR_CAUSE, cplsw_intr_info))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * LE interrupt handler.
+ */
+static void le_intr_handler(struct adapter *adap)
+{
+	static struct intr_info le_intr_info[] = {
+		{ F_LIPMISS, "LE LIP miss", -1, 0 },
+		{ F_LIP0, "LE 0 LIP error", -1, 0 },
+		{ F_PARITYERR, "LE parity error", -1, 1 },
+		{ F_UNKNOWNCMD, "LE unknown command", -1, 1 },
+		{ F_REQQPARERR, "LE request queue parity error", -1, 1 },
+		{ 0 }
+	};
+
+	if (t4_handle_intr_status(adap, A_LE_DB_INT_CAUSE, le_intr_info))
+		t4_fatal_err(adap);
+}
+
+/*
+ * MPS interrupt handler.
+ */
+static void mps_intr_handler(struct adapter *adapter)
+{
+	static struct intr_info mps_rx_intr_info[] = {
+		{ 0xffffff, "MPS Rx parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_tx_intr_info[] = {
+		{ V_TPFIFO(M_TPFIFO), "MPS Tx TP FIFO parity error", -1, 1 },
+		{ F_NCSIFIFO, "MPS Tx NC-SI FIFO parity error", -1, 1 },
+		{ V_TXDATAFIFO(M_TXDATAFIFO), "MPS Tx data FIFO parity error",
+		  -1, 1 },
+		{ V_TXDESCFIFO(M_TXDESCFIFO), "MPS Tx desc FIFO parity error",
+		  -1, 1 },
+		{ F_BUBBLE, "MPS Tx underflow", -1, 1 },
+		{ F_SECNTERR, "MPS Tx SOP/EOP error", -1, 1 },
+		{ F_FRMERR, "MPS Tx framing error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_trc_intr_info[] = {
+		{ V_FILTMEM(M_FILTMEM), "MPS TRC filter parity error", -1, 1 },
+		{ V_PKTFIFO(M_PKTFIFO), "MPS TRC packet FIFO parity error", -1,
+		  1 },
+		{ F_MISCPERR, "MPS TRC misc parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_stat_sram_intr_info[] = {
+		{ 0x1fffff, "MPS statistics SRAM parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_stat_tx_intr_info[] = {
+		{ 0xfffff, "MPS statistics Tx FIFO parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_stat_rx_intr_info[] = {
+		{ 0xffffff, "MPS statistics Rx FIFO parity error", -1, 1 },
+		{ 0 }
+	};
+	static struct intr_info mps_cls_intr_info[] = {
+		{ F_MATCHSRAM, "MPS match SRAM parity error", -1, 1 },
+		{ F_MATCHTCAM, "MPS match TCAM parity error", -1, 1 },
+		{ F_HASHSRAM, "MPS hash SRAM parity error", -1, 1 },
+		{ 0 }
+	};
+
+	int fat;
+
+	fat = t4_handle_intr_status(adapter, A_MPS_RX_PERR_INT_CAUSE,
+				    mps_rx_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_TX_INT_CAUSE,
+				    mps_tx_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_TRC_INT_CAUSE,
+				    mps_trc_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_STAT_PERR_INT_CAUSE_SRAM,
+				    mps_stat_sram_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_STAT_PERR_INT_CAUSE_TX_FIFO,
+				    mps_stat_tx_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_STAT_PERR_INT_CAUSE_RX_FIFO,
+				    mps_stat_rx_intr_info) +
+	      t4_handle_intr_status(adapter, A_MPS_CLS_INT_CAUSE,
+				    mps_cls_intr_info);
+
+	t4_write_reg(adapter, A_MPS_INT_CAUSE, 0);
+	t4_read_reg(adapter, A_MPS_INT_CAUSE);                    /* flush */
+	if (fat)
+		t4_fatal_err(adapter);
+}
+
+#define MEM_INT_MASK (F_PERR_INT_CAUSE | F_ECC_CE_INT_CAUSE | F_ECC_UE_INT_CAUSE)
+
+/*
+ * EDC/MC interrupt handler.
+ */
+static void mem_intr_handler(struct adapter *adapter, int idx)
+{
+	static const char name[3][5] = { "EDC0", "EDC1", "MC" };
+
+	unsigned int addr, cnt_addr, v;
+
+	if (idx <= MEM_EDC1) {
+		addr = EDC_REG(A_EDC_INT_CAUSE, idx);
+		cnt_addr = EDC_REG(A_EDC_ECC_STATUS, idx);
+	} else {
+		addr = A_MC_INT_CAUSE;
+		cnt_addr = A_MC_ECC_STATUS;
+	}
+
+	v = t4_read_reg(adapter, addr) & MEM_INT_MASK;
+	if (v & F_PERR_INT_CAUSE)
+		CH_ALERT(adapter, "%s FIFO parity error\n", name[idx]);
+	if (v & F_ECC_CE_INT_CAUSE) {
+		u32 cnt = G_ECC_CECNT(t4_read_reg(adapter, cnt_addr));
+
+		t4_write_reg(adapter, cnt_addr, V_ECC_CECNT(M_ECC_CECNT));
+		CH_WARN_RATELIMIT(adapter,
+				  "%u %s correctable ECC data error%s\n",
+				  cnt, name[idx], cnt > 1 ? "s" : "");
+	}
+	if (v & F_ECC_UE_INT_CAUSE)
+		CH_ALERT(adapter, "%s uncorrectable ECC data error\n",
+			 name[idx]);
+
+	t4_write_reg(adapter, addr, v);
+	if (v & (F_PERR_INT_CAUSE | F_ECC_UE_INT_CAUSE))
+		t4_fatal_err(adapter);
+}
+
+/*
+ * MA interrupt handler.
+ */
+static void ma_intr_handler(struct adapter *adapter)
+{
+	u32 v, status = t4_read_reg(adapter, A_MA_INT_CAUSE);
+
+	if (status & F_MEM_PERR_INT_CAUSE)
+		CH_ALERT(adapter, "MA parity error, parity status %#x\n",
+			 t4_read_reg(adapter, A_MA_PARITY_ERROR_STATUS));
+	if (status & F_MEM_WRAP_INT_CAUSE) {
+		v = t4_read_reg(adapter, A_MA_INT_WRAP_STATUS);
+		CH_ALERT(adapter, "MA address wrap-around error by client %u to"
+			 " address %#x\n", G_MEM_WRAP_CLIENT_NUM(v),
+			 G_MEM_WRAP_ADDRESS(v) << 4);
+	}
+	t4_write_reg(adapter, A_MA_INT_CAUSE, status);
+	t4_fatal_err(adapter);
+}
+
+/*
+ * SMB interrupt handler.
+ */
+static void smb_intr_handler(struct adapter *adap)
+{
+	static struct intr_info smb_intr_info[] = {
+		{ F_MSTTXFIFOPARINT, "SMB master Tx FIFO parity error", -1, 1 },
+		{ F_MSTRXFIFOPARINT, "SMB master Rx FIFO parity error", -1, 1 },
+		{ F_SLVFIFOPARINT, "SMB slave FIFO parity error", -1, 1 },
+		{ 0 }
+	};
+
+	if (t4_handle_intr_status(adap, A_SMB_INT_CAUSE, smb_intr_info))
+		t4_fatal_err(adap);
+}
+
+/*
+ * NC-SI interrupt handler.
+ */
+static void ncsi_intr_handler(struct adapter *adap)
+{
+	static struct intr_info ncsi_intr_info[] = {
+		{ F_CIM_DM_PRTY_ERR, "NC-SI CIM parity error", -1, 1 },
+		{ F_MPS_DM_PRTY_ERR, "NC-SI MPS parity error", -1, 1 },
+		{ F_TXFIFO_PRTY_ERR, "NC-SI Tx FIFO parity error", -1, 1 },
+		{ F_RXFIFO_PRTY_ERR, "NC-SI Rx FIFO parity error", -1, 1 },
+		{ 0 }
+	};
+
+	if (t4_handle_intr_status(adap, A_NCSI_INT_CAUSE, ncsi_intr_info))
+		t4_fatal_err(adap);
+}
+
+/*
+ * XGMAC interrupt handler.
+ */
+static void xgmac_intr_handler(struct adapter *adap, int port)
+{
+	u32 v = t4_read_reg(adap, PORT_REG(port, A_XGMAC_PORT_INT_CAUSE));
+
+	v &= F_TXFIFO_PRTY_ERR | F_RXFIFO_PRTY_ERR;
+	if (!v)
+		return;
+
+	if (v & F_TXFIFO_PRTY_ERR)
+		CH_ALERT(adap, "XGMAC %d Tx FIFO parity error\n", port);
+	if (v & F_RXFIFO_PRTY_ERR)
+		CH_ALERT(adap, "XGMAC %d Rx FIFO parity error\n", port);
+	t4_write_reg(adap, PORT_REG(port, A_XGMAC_PORT_INT_CAUSE), v);
+	t4_fatal_err(adap);
+}
+
+/*
+ * PL interrupt handler.
+ */
+static void pl_intr_handler(struct adapter *adap)
+{
+	static struct intr_info pl_intr_info[] = {
+		{ F_FATALPERR, "T4 fatal parity error", -1, 1 },
+		{ F_PERRVFID, "PL VFID_MAP parity error", -1, 1 },
+		{ 0 }
+	};
+
+	if (t4_handle_intr_status(adap, A_PL_PL_INT_CAUSE, pl_intr_info))
+		t4_fatal_err(adap);
+}
+
+#define PF_INTR_MASK (F_PFSW | F_PFCIM)
+
+/**
+ *	t4_slow_intr_handler - control path interrupt handler
+ *	@adapter: the adapter
+ *
+ *	T4 interrupt handler for non-data global interrupt events, e.g., errors.
+ *	The designation 'slow' is because it involves register reads, while
+ *	data interrupts typically don't involve any MMIOs.
+ */
+int t4_slow_intr_handler(struct adapter *adapter)
+{
+	u32 cause = t4_read_reg(adapter, A_PL_INT_CAUSE);
+
+	if (!(cause & GLBL_INTR_MASK))
+		return 0;
+	if (cause & F_CIM)
+		cim_intr_handler(adapter);
+	if (cause & F_MPS)
+		mps_intr_handler(adapter);
+	if (cause & F_NCSI)
+		ncsi_intr_handler(adapter);
+	if (cause & F_PL)
+		pl_intr_handler(adapter);
+	if (cause & F_SMB)
+		smb_intr_handler(adapter);
+	if (cause & F_XGMAC0)
+		xgmac_intr_handler(adapter, 0);
+	if (cause & F_XGMAC1)
+		xgmac_intr_handler(adapter, 1);
+	if (cause & F_XGMAC_KR0)
+		xgmac_intr_handler(adapter, 2);
+	if (cause & F_XGMAC_KR1)
+		xgmac_intr_handler(adapter, 3);
+	if (cause & F_PCIE)
+		pcie_intr_handler(adapter);
+	if (cause & F_MC)
+		mem_intr_handler(adapter, MEM_MC);
+	if (cause & F_EDC0)
+		mem_intr_handler(adapter, MEM_EDC0);
+	if (cause & F_EDC1)
+		mem_intr_handler(adapter, MEM_EDC1);
+	if (cause & F_LE)
+		le_intr_handler(adapter);
+	if (cause & F_TP)
+		tp_intr_handler(adapter);
+	if (cause & F_MA)
+		ma_intr_handler(adapter);
+	if (cause & F_PM_TX)
+		pmtx_intr_handler(adapter);
+	if (cause & F_PM_RX)
+		pmrx_intr_handler(adapter);
+	if (cause & F_ULP_RX)
+		ulprx_intr_handler(adapter);
+	if (cause & F_CPL_SWITCH)
+		cplsw_intr_handler(adapter);
+	if (cause & F_SGE)
+		sge_intr_handler(adapter);
+	if (cause & F_ULP_TX)
+		ulptx_intr_handler(adapter);
+
+	/* Clear the interrupts just processed for which we are the master. */
+	t4_write_reg(adapter, A_PL_INT_CAUSE, cause & GLBL_INTR_MASK);
+	(void) t4_read_reg(adapter, A_PL_INT_CAUSE); /* flush */
+	return 1;
+}
+
+/**
+ *	t4_intr_enable - enable interrupts
+ *	@adapter: the adapter whose interrupts should be enabled
+ *
+ *	Enable PF-specific interrupts for the calling function and the top-level
+ *	interrupt concentrator for global interrupts.  Interrupts are already
+ *	enabled at each module,	here we just enable the roots of the interrupt
+ *	hierarchies.
+ *
+ *	Note: this function should be called only when the driver manages
+ *	non PF-specific interrupts from the various HW modules.  Only one PCI
+ *	function at a time should be doing this.
+ */
+void t4_intr_enable(struct adapter *adapter)
+{
+	u32 pf = G_SOURCEPF(t4_read_reg(adapter, A_PL_WHOAMI));
+
+	t4_write_reg(adapter, A_SGE_INT_ENABLE3, F_ERR_CPL_EXCEED_IQE_SIZE |
+		     F_ERR_INVALID_CIDX_INC | F_ERR_CPL_OPCODE_0 |
+		     F_ERR_DROPPED_DB | F_ERR_DATA_CPL_ON_HIGH_QID1 |
+		     F_ERR_DATA_CPL_ON_HIGH_QID0 | F_ERR_BAD_DB_PIDX3 |
+		     F_ERR_BAD_DB_PIDX2 | F_ERR_BAD_DB_PIDX1 |
+		     F_ERR_BAD_DB_PIDX0 | F_ERR_ING_CTXT_PRIO |
+		     F_ERR_EGR_CTXT_PRIO | F_INGRESS_SIZE_ERR |
+		     F_DBFIFO_HP_INT | F_DBFIFO_LP_INT |
+		     F_EGRESS_SIZE_ERR);
+	t4_write_reg(adapter, MYPF_REG(A_PL_PF_INT_ENABLE), PF_INTR_MASK);
+	t4_set_reg_field(adapter, A_PL_INT_MAP0, 0, 1 << pf);
+}
+
+/**
+ *	t4_intr_disable - disable interrupts
+ *	@adapter: the adapter whose interrupts should be disabled
+ *
+ *	Disable interrupts.  We only disable the top-level interrupt
+ *	concentrators.  The caller must be a PCI function managing global
+ *	interrupts.
+ */
+void t4_intr_disable(struct adapter *adapter)
+{
+	u32 pf = G_SOURCEPF(t4_read_reg(adapter, A_PL_WHOAMI));
+
+	t4_write_reg(adapter, MYPF_REG(A_PL_PF_INT_ENABLE), 0);
+	t4_set_reg_field(adapter, A_PL_INT_MAP0, 1 << pf, 0);
+}
+
+/**
+ *	t4_intr_clear - clear all interrupts
+ *	@adapter: the adapter whose interrupts should be cleared
+ *
+ *	Clears all interrupts.  The caller must be a PCI function managing
+ *	global interrupts.
+ */
+void t4_intr_clear(struct adapter *adapter)
+{
+	static const unsigned int cause_reg[] = {
+		A_SGE_INT_CAUSE1, A_SGE_INT_CAUSE2, A_SGE_INT_CAUSE3,
+		A_PCIE_CORE_UTL_SYSTEM_BUS_AGENT_STATUS,
+		A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_STATUS,
+		A_PCIE_NONFAT_ERR, A_PCIE_INT_CAUSE,
+		A_MC_INT_CAUSE,
+		A_MA_INT_WRAP_STATUS, A_MA_PARITY_ERROR_STATUS, A_MA_INT_CAUSE,
+		A_EDC_INT_CAUSE, EDC_REG(A_EDC_INT_CAUSE, 1),
+		A_CIM_HOST_INT_CAUSE, A_CIM_HOST_UPACC_INT_CAUSE,
+		MYPF_REG(A_CIM_PF_HOST_INT_CAUSE),
+		A_TP_INT_CAUSE,
+		A_ULP_RX_INT_CAUSE, A_ULP_TX_INT_CAUSE,
+		A_PM_RX_INT_CAUSE, A_PM_TX_INT_CAUSE,
+		A_MPS_RX_PERR_INT_CAUSE,
+		A_CPL_INTR_CAUSE,
+		MYPF_REG(A_PL_PF_INT_CAUSE),
+		A_PL_PL_INT_CAUSE,
+		A_LE_DB_INT_CAUSE,
+	};
+
+	unsigned int i;
+
+	for (i = 0; i < ARRAY_SIZE(cause_reg); ++i)
+		t4_write_reg(adapter, cause_reg[i], 0xffffffff);
+
+	t4_write_reg(adapter, A_PL_INT_CAUSE, GLBL_INTR_MASK);
+	(void) t4_read_reg(adapter, A_PL_INT_CAUSE);          /* flush */
+}
+
+/**
+ *	hash_mac_addr - return the hash value of a MAC address
+ *	@addr: the 48-bit Ethernet MAC address
+ *
+ *	Hashes a MAC address according to the hash function used by HW inexact
+ *	(hash) address matching.
+ */
+static int hash_mac_addr(const u8 *addr)
+{
+	u32 a = ((u32)addr[0] << 16) | ((u32)addr[1] << 8) | addr[2];
+	u32 b = ((u32)addr[3] << 16) | ((u32)addr[4] << 8) | addr[5];
+	a ^= b;
+	a ^= (a >> 12);
+	a ^= (a >> 6);
+	return a & 0x3f;
+}
+
+/**
+ *	t4_config_rss_range - configure a portion of the RSS mapping table
+ *	@adapter: the adapter
+ *	@mbox: mbox to use for the FW command
+ *	@viid: virtual interface whose RSS subtable is to be written
+ *	@start: start entry in the table to write
+ *	@n: how many table entries to write
+ *	@rspq: values for the "response queue" (Ingress Queue) lookup table
+ *	@nrspq: number of values in @rspq
+ *
+ *	Programs the selected part of the VI's RSS mapping table with the
+ *	provided values.  If @nrspq < @n the supplied values are used repeatedly
+ *	until the full table range is populated.
+ *
+ *	The caller must ensure the values in @rspq are in the range allowed for
+ *	@viid.
+ */
+int t4_config_rss_range(struct adapter *adapter, int mbox, unsigned int viid,
+			int start, int n, const u16 *rspq, unsigned int nrspq)
+{
+	int ret;
+	const u16 *rsp = rspq;
+	const u16 *rsp_end = rspq+nrspq;
+	struct fw_rss_ind_tbl_cmd cmd;
+
+	memset(&cmd, 0, sizeof(cmd));
+	cmd.op_to_viid = htonl(V_FW_CMD_OP(FW_RSS_IND_TBL_CMD) |
+			       F_FW_CMD_REQUEST | F_FW_CMD_WRITE |
+			       V_FW_RSS_IND_TBL_CMD_VIID(viid));
+	cmd.retval_len16 = htonl(FW_LEN16(cmd));
+
+
+	/*
+	 * Each firmware RSS command can accommodate up to 32 RSS Ingress
+	 * Queue Identifiers.  These Ingress Queue IDs are packed three to
+	 * a 32-bit word as 10-bit values with the upper remaining 2 bits
+	 * reserved.
+	 */
+	while (n > 0) {
+		int nq = min(n, 32);
+		int nq_packed = 0;
+		__be32 *qp = &cmd.iq0_to_iq2;
+
+		/*
+		 * Set up the firmware RSS command header to send the next
+		 * "nq" Ingress Queue IDs to the firmware.
+		 */
+		cmd.niqid = htons(nq);
+		cmd.startidx = htons(start);
+
+		/*
+		 * "nq" more done for the start of the next loop.
+		 */
+		start += nq;
+		n -= nq;
+
+		/*
+		 * While there are still Ingress Queue IDs to stuff into the
+		 * current firmware RSS command, retrieve them from the
+		 * Ingress Queue ID array and insert them into the command.
+		 */
+		while (nq > 0) {
+			/*
+			 * Grab up to the next 3 Ingress Queue IDs (wrapping
+			 * around the Ingress Queue ID array if necessary) and
+			 * insert them into the firmware RSS command at the
+			 * current 3-tuple position within the commad.
+			 */
+			u16 qbuf[3];
+			u16 *qbp = qbuf;
+			int nqbuf = min(3, nq);
+
+			nq -= nqbuf;
+			qbuf[0] = qbuf[1] = qbuf[2] = 0;
+			while (nqbuf && nq_packed < 32) {
+				nqbuf--;
+				nq_packed++;
+				*qbp++ = *rsp++;
+				if (rsp >= rsp_end)
+					rsp = rspq;
+			}
+			*qp++ = cpu_to_be32(V_FW_RSS_IND_TBL_CMD_IQ0(qbuf[0]) |
+					    V_FW_RSS_IND_TBL_CMD_IQ1(qbuf[1]) |
+					    V_FW_RSS_IND_TBL_CMD_IQ2(qbuf[2]));
+		}
+
+		/*
+		 * Send this portion of the RRS table update to the firmware;
+		 * bail out on any errors.
+		 */
+		ret = t4_wr_mbox(adapter, mbox, &cmd, sizeof(cmd), NULL);
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+/**
+ *	t4_config_glbl_rss - configure the global RSS mode
+ *	@adapter: the adapter
+ *	@mbox: mbox to use for the FW command
+ *	@mode: global RSS mode
+ *	@flags: mode-specific flags
+ *
+ *	Sets the global RSS mode.
+ */
+int t4_config_glbl_rss(struct adapter *adapter, int mbox, unsigned int mode,
+		       unsigned int flags)
+{
+	struct fw_rss_glb_config_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_write = htonl(V_FW_CMD_OP(FW_RSS_GLB_CONFIG_CMD) |
+			      F_FW_CMD_REQUEST | F_FW_CMD_WRITE);
+	c.retval_len16 = htonl(FW_LEN16(c));
+	if (mode == FW_RSS_GLB_CONFIG_CMD_MODE_MANUAL) {
+		c.u.manual.mode_pkd = htonl(V_FW_RSS_GLB_CONFIG_CMD_MODE(mode));
+	} else if (mode == FW_RSS_GLB_CONFIG_CMD_MODE_BASICVIRTUAL) {
+		c.u.basicvirtual.mode_pkd =
+			htonl(V_FW_RSS_GLB_CONFIG_CMD_MODE(mode));
+		c.u.basicvirtual.synmapen_to_hashtoeplitz = htonl(flags);
+	} else
+		return -EINVAL;
+	return t4_wr_mbox(adapter, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ *	t4_config_vi_rss - configure per VI RSS settings
+ *	@adapter: the adapter
+ *	@mbox: mbox to use for the FW command
+ *	@viid: the VI id
+ *	@flags: RSS flags
+ *	@defq: id of the default RSS queue for the VI.
+ *
+ *	Configures VI-specific RSS properties.
+ */
+int t4_config_vi_rss(struct adapter *adapter, int mbox, unsigned int viid,
+		     unsigned int flags, unsigned int defq)
+{
+	struct fw_rss_vi_config_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_RSS_VI_CONFIG_CMD) |
+			     F_FW_CMD_REQUEST | F_FW_CMD_WRITE |
+			     V_FW_RSS_VI_CONFIG_CMD_VIID(viid));
+	c.retval_len16 = htonl(FW_LEN16(c));
+	c.u.basicvirtual.defaultq_to_udpen = htonl(flags |
+					V_FW_RSS_VI_CONFIG_CMD_DEFAULTQ(defq));
+	return t4_wr_mbox(adapter, mbox, &c, sizeof(c), NULL);
+}
+
+/* Read an RSS table row */
+static int rd_rss_row(struct adapter *adap, int row, u32 *val)
+{
+	t4_write_reg(adap, A_TP_RSS_LKP_TABLE, 0xfff00000 | row);
+	return t4_wait_op_done_val(adap, A_TP_RSS_LKP_TABLE, F_LKPTBLROWVLD, 1,
+				   5, 0, val);
+}
+	
+/**
+ *	t4_read_rss - read the contents of the RSS mapping table
+ *	@adapter: the adapter
+ *	@map: holds the contents of the RSS mapping table
+ *
+ *	Reads the contents of the RSS hash->queue mapping table.
+ */
+int t4_read_rss(struct adapter *adapter, u16 *map)
+{
+	u32 val;
+	int i, ret;
+
+	for (i = 0; i < RSS_NENTRIES / 2; ++i) {
+		ret = rd_rss_row(adapter, i, &val);
+		if (ret)
+			return ret;
+		*map++ = G_LKPTBLQUEUE0(val);
+		*map++ = G_LKPTBLQUEUE1(val);
+	}
+	return 0;
+}
+
+/**
+ *	t4_read_rss_key - read the global RSS key
+ *	@adap: the adapter
+ *	@key: 10-entry array holding the 320-bit RSS key
+ *
+ * 	Reads the global 320-bit RSS key.
+ */
+void t4_read_rss_key(struct adapter *adap, u32 *key)
+{
+	t4_read_indirect(adap, A_TP_PIO_ADDR, A_TP_PIO_DATA, key, 10,
+			 A_TP_RSS_SECRET_KEY0);
+}
+
+/**
+ *	t4_write_rss_key - program one of the RSS keys
+ *	@adap: the adapter
+ *	@key: 10-entry array holding the 320-bit RSS key
+ *	@idx: which RSS key to write
+ *
+ *	Writes one of the RSS keys with the given 320-bit value.  If @idx is
+ *	0..15 the corresponding entry in the RSS key table is written,
+ *	otherwise the global RSS key is written.
+ */
+void t4_write_rss_key(struct adapter *adap, const u32 *key, int idx)
+{
+	t4_write_indirect(adap, A_TP_PIO_ADDR, A_TP_PIO_DATA, key, 10,
+			  A_TP_RSS_SECRET_KEY0);
+	if (idx >= 0 && idx < 16)
+		t4_write_reg(adap, A_TP_RSS_CONFIG_VRT,
+			     V_KEYWRADDR(idx) | F_KEYWREN);
+}
+
+/**
+ *	t4_read_rss_pf_config - read PF RSS Configuration Table
+ *	@adapter: the adapter
+ *	@index: the entry in the PF RSS table to read
+ *	@valp: where to store the returned value
+ *
+ *	Reads the PF RSS Configuration Table at the specified index and returns
+ *	the value found there.
+ */
+void t4_read_rss_pf_config(struct adapter *adapter, unsigned int index, u32 *valp)
+{
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 valp, 1, A_TP_RSS_PF0_CONFIG + index);
+}
+
+/**
+ *	t4_write_rss_pf_config - write PF RSS Configuration Table
+ *	@adapter: the adapter
+ *	@index: the entry in the VF RSS table to read
+ *	@val: the value to store
+ *
+ *	Writes the PF RSS Configuration Table at the specified index with the
+ *	specified value.
+ */
+void t4_write_rss_pf_config(struct adapter *adapter, unsigned int index, u32 val)
+{
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &val, 1, A_TP_RSS_PF0_CONFIG + index);
+}
+
+/**
+ *	t4_read_rss_vf_config - read VF RSS Configuration Table
+ *	@adapter: the adapter
+ *	@index: the entry in the VF RSS table to read
+ *	@vfl: where to store the returned VFL
+ *	@vfh: where to store the returned VFH
+ *
+ *	Reads the VF RSS Configuration Table at the specified index and returns
+ *	the (VFL, VFH) values found there.
+ */
+void t4_read_rss_vf_config(struct adapter *adapter, unsigned int index,
+			   u32 *vfl, u32 *vfh)
+{
+	u32 vrt;
+
+	/*
+	 * Request that the index'th VF Table values be read into VFL/VFH.
+	 */
+	vrt = t4_read_reg(adapter, A_TP_RSS_CONFIG_VRT);
+	vrt &= ~(F_VFRDRG | V_VFWRADDR(M_VFWRADDR) | F_VFWREN | F_KEYWREN);
+	vrt |= V_VFWRADDR(index) | F_VFRDEN;
+	t4_write_reg(adapter, A_TP_RSS_CONFIG_VRT, vrt);
+
+	/*
+	 * Grab the VFL/VFH values ...
+	 */
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 vfl, 1, A_TP_RSS_VFL_CONFIG);
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 vfh, 1, A_TP_RSS_VFH_CONFIG);
+}
+
+/**
+ *	t4_write_rss_vf_config - write VF RSS Configuration Table
+ *	
+ *	@adapter: the adapter
+ *	@index: the entry in the VF RSS table to write
+ *	@vfl: the VFL to store
+ *	@vfh: the VFH to store
+ *
+ *	Writes the VF RSS Configuration Table at the specified index with the
+ *	specified (VFL, VFH) values.
+ */
+void t4_write_rss_vf_config(struct adapter *adapter, unsigned int index,
+			    u32 vfl, u32 vfh)
+{
+	u32 vrt;
+
+	/*
+	 * Load up VFL/VFH with the values to be written ...
+	 */
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &vfl, 1, A_TP_RSS_VFL_CONFIG);
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &vfh, 1, A_TP_RSS_VFH_CONFIG);
+
+	/*
+	 * Write the VFL/VFH into the VF Table at index'th location.
+	 */
+	vrt = t4_read_reg(adapter, A_TP_RSS_CONFIG_VRT);
+	vrt &= ~(F_VFRDRG | F_VFRDEN | V_VFWRADDR(M_VFWRADDR) | F_KEYWREN);
+	vrt |= V_VFWRADDR(index) | F_VFWREN;
+	t4_write_reg(adapter, A_TP_RSS_CONFIG_VRT, vrt);
+}
+
+/**
+ *	t4_read_rss_pf_map - read PF RSS Map
+ *	@adapter: the adapter
+ *
+ *	Reads the PF RSS Map register and returns its value.
+ */
+u32 t4_read_rss_pf_map(struct adapter *adapter)
+{
+	u32 pfmap;
+
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 &pfmap, 1, A_TP_RSS_PF_MAP);
+	return pfmap;
+}
+
+/**
+ *	t4_write_rss_pf_map - write PF RSS Map
+ *	@adapter: the adapter
+ *	@pfmap: PF RSS Map value
+ *
+ *	Writes the specified value to the PF RSS Map register.
+ */
+void t4_write_rss_pf_map(struct adapter *adapter, u32 pfmap)
+{
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &pfmap, 1, A_TP_RSS_PF_MAP);
+}
+
+/**
+ *	t4_read_rss_pf_mask - read PF RSS Mask
+ *	@adapter: the adapter
+ *
+ *	Reads the PF RSS Mask register and returns its value.
+ */
+u32 t4_read_rss_pf_mask(struct adapter *adapter)
+{
+	u32 pfmask;
+
+	t4_read_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			 &pfmask, 1, A_TP_RSS_PF_MSK);
+	return pfmask;
+}
+
+/**
+ *	t4_write_rss_pf_mask - write PF RSS Mask
+ *	@adapter: the adapter
+ *	@pfmask: PF RSS Mask value
+ *
+ *	Writes the specified value to the PF RSS Mask register.
+ */
+void t4_write_rss_pf_mask(struct adapter *adapter, u32 pfmask)
+{
+	t4_write_indirect(adapter, A_TP_PIO_ADDR, A_TP_PIO_DATA,
+			  &pfmask, 1, A_TP_RSS_PF_MSK);
+}
+
+/**
+ *	t4_set_filter_mode - configure the optional components of filter tuples
+ *	@adap: the adapter
+ *	@mode_map: a bitmap selcting which optional filter components to enable
+ *
+ *	Sets the filter mode by selecting the optional components to enable
+ *	in filter tuples.  Returns 0 on success and a negative error if the
+ *	requested mode needs more bits than are available for optional
+ *	components.
+ */
+int t4_set_filter_mode(struct adapter *adap, unsigned int mode_map)
+{
+	static u8 width[] = { 1, 3, 17, 17, 8, 8, 16, 9, 3, 1 };
+
+	int i, nbits = 0;
+
+	for (i = S_FCOE; i <= S_FRAGMENTATION; i++)
+		if (mode_map & (1 << i))
+			nbits += width[i];
+	if (nbits > FILTER_OPT_LEN)
+		return -EINVAL;
+	t4_write_indirect(adap, A_TP_PIO_ADDR, A_TP_PIO_DATA, &mode_map, 1,
+			  A_TP_VLAN_PRI_MAP);
+	return 0;
+}
+
+/**
+ *	t4_tp_get_tcp_stats - read TP's TCP MIB counters
+ *	@adap: the adapter
+ *	@v4: holds the TCP/IP counter values
+ *	@v6: holds the TCP/IPv6 counter values
+ *
+ *	Returns the values of TP's TCP/IP and TCP/IPv6 MIB counters.
+ *	Either @v4 or @v6 may be %NULL to skip the corresponding stats.
+ */
+void t4_tp_get_tcp_stats(struct adapter *adap, struct tp_tcp_stats *v4,
+			 struct tp_tcp_stats *v6)
+{
+	u32 val[A_TP_MIB_TCP_RXT_SEG_LO - A_TP_MIB_TCP_OUT_RST + 1];
+
+#define STAT_IDX(x) ((A_TP_MIB_TCP_##x) - A_TP_MIB_TCP_OUT_RST)
+#define STAT(x)     val[STAT_IDX(x)]
+#define STAT64(x)   (((u64)STAT(x##_HI) << 32) | STAT(x##_LO))
+
+	if (v4) {
+		t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, val,
+				 ARRAY_SIZE(val), A_TP_MIB_TCP_OUT_RST);
+		v4->tcpOutRsts = STAT(OUT_RST);
+		v4->tcpInSegs  = STAT64(IN_SEG);
+		v4->tcpOutSegs = STAT64(OUT_SEG);
+		v4->tcpRetransSegs = STAT64(RXT_SEG);
+	}
+	if (v6) {
+		t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, val,
+				 ARRAY_SIZE(val), A_TP_MIB_TCP_V6OUT_RST);
+		v6->tcpOutRsts = STAT(OUT_RST);
+		v6->tcpInSegs  = STAT64(IN_SEG);
+		v6->tcpOutSegs = STAT64(OUT_SEG);
+		v6->tcpRetransSegs = STAT64(RXT_SEG);
+	}
+#undef STAT64
+#undef STAT
+#undef STAT_IDX
+}
+
+/**
+ *	t4_tp_get_err_stats - read TP's error MIB counters
+ *	@adap: the adapter
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's error counters.
+ */
+void t4_tp_get_err_stats(struct adapter *adap, struct tp_err_stats *st)
+{
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->macInErrs,
+			 12, A_TP_MIB_MAC_IN_ERR_0);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->tnlCongDrops,
+			 8, A_TP_MIB_TNL_CNG_DROP_0);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->tnlTxDrops,
+			 4, A_TP_MIB_TNL_DROP_0);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->ofldVlanDrops,
+			 4, A_TP_MIB_OFD_VLN_DROP_0);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->tcp6InErrs,
+			 4, A_TP_MIB_TCP_V6IN_ERR_0);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, &st->ofldNoNeigh,
+			 2, A_TP_MIB_OFD_ARP_DROP);
+}
+
+/**
+ *	t4_tp_get_proxy_stats - read TP's proxy MIB counters
+ *	@adap: the adapter
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's proxy counters.
+ */
+void t4_tp_get_proxy_stats(struct adapter *adap, struct tp_proxy_stats *st)
+{
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->proxy,
+			 4, A_TP_MIB_TNL_LPBK_0);
+}
+
+/**
+ *	t4_tp_get_cpl_stats - read TP's CPL MIB counters
+ *	@adap: the adapter
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's CPL counters.
+ */
+void t4_tp_get_cpl_stats(struct adapter *adap, struct tp_cpl_stats *st)
+{
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, st->req,
+			 8, A_TP_MIB_CPL_IN_REQ_0);
+}
+
+/**
+ *	t4_tp_get_rdma_stats - read TP's RDMA MIB counters
+ *	@adap: the adapter
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's RDMA counters.
+ */
+void t4_tp_get_rdma_stats(struct adapter *adap, struct tp_rdma_stats *st)
+{
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, &st->rqe_dfr_mod,
+			 2, A_TP_MIB_RQE_DFR_MOD);
+}
+
+/**
+ *	t4_get_fcoe_stats - read TP's FCoE MIB counters for a port
+ *	@adap: the adapter
+ *	@idx: the port index
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's FCoE counters for the selected port.
+ */
+void t4_get_fcoe_stats(struct adapter *adap, unsigned int idx,
+		       struct tp_fcoe_stats *st)
+{
+	u32 val[2];
+
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, &st->framesDDP,
+			 1, A_TP_MIB_FCOE_DDP_0 + idx);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, &st->framesDrop,
+			 1, A_TP_MIB_FCOE_DROP_0 + idx);
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, val,
+			 2, A_TP_MIB_FCOE_BYTE_0_HI + 2 * idx);
+	st->octetsDDP = ((u64)val[0] << 32) | val[1];
+}
+
+/**
+ *	t4_get_usm_stats - read TP's non-TCP DDP MIB counters
+ *	@adap: the adapter
+ *	@st: holds the counter values
+ *
+ *	Returns the values of TP's counters for non-TCP directly-placed packets.
+ */
+void t4_get_usm_stats(struct adapter *adap, struct tp_usm_stats *st)
+{
+	u32 val[4];
+
+	t4_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_DATA, val, 4,
+			 A_TP_MIB_USM_PKTS);
+	st->frames = val[0];
+	st->drops = val[1];
+	st->octets = ((u64)val[2] << 32) | val[3];
+}
+
+/**
+ *	t4_read_mtu_tbl - returns the values in the HW path MTU table
+ *	@adap: the adapter
+ *	@mtus: where to store the MTU values
+ *	@mtu_log: where to store the MTU base-2 log (may be %NULL)
+ *
+ *	Reads the HW path MTU table.
+ */
+void t4_read_mtu_tbl(struct adapter *adap, u16 *mtus, u8 *mtu_log)
+{
+	u32 v;
+	int i;
+
+	for (i = 0; i < NMTUS; ++i) {
+		t4_write_reg(adap, A_TP_MTU_TABLE,
+			     V_MTUINDEX(0xff) | V_MTUVALUE(i));
+		v = t4_read_reg(adap, A_TP_MTU_TABLE);
+		mtus[i] = G_MTUVALUE(v);
+		if (mtu_log)
+			mtu_log[i] = G_MTUWIDTH(v);
+	}
+}
+
+/**
+ *	t4_read_cong_tbl - reads the congestion control table
+ *	@adap: the adapter
+ *	@incr: where to store the alpha values
+ *
+ *	Reads the additive increments programmed into the HW congestion
+ *	control table.
+ */
+void t4_read_cong_tbl(struct adapter *adap, u16 incr[NMTUS][NCCTRL_WIN])
+{
+	unsigned int mtu, w;
+
+	for (mtu = 0; mtu < NMTUS; ++mtu)
+		for (w = 0; w < NCCTRL_WIN; ++w) {
+			t4_write_reg(adap, A_TP_CCTRL_TABLE,
+				     V_ROWINDEX(0xffff) | (mtu << 5) | w);
+			incr[mtu][w] = (u16)t4_read_reg(adap,
+						A_TP_CCTRL_TABLE) & 0x1fff;
+		}
+}
+
+/**
+ *	t4_read_pace_tbl - read the pace table
+ *	@adap: the adapter
+ *	@pace_vals: holds the returned values
+ *
+ *	Returns the values of TP's pace table in microseconds.
+ */
+void t4_read_pace_tbl(struct adapter *adap, unsigned int pace_vals[NTX_SCHED])
+{
+	unsigned int i, v;
+
+	for (i = 0; i < NTX_SCHED; i++) {
+		t4_write_reg(adap, A_TP_PACE_TABLE, 0xffff0000 + i);
+		v = t4_read_reg(adap, A_TP_PACE_TABLE);
+		pace_vals[i] = dack_ticks_to_usec(adap, v);
+	}
+}
+
+/**
+ *	t4_tp_wr_bits_indirect - set/clear bits in an indirect TP register
+ *	@adap: the adapter
+ *	@addr: the indirect TP register address
+ *	@mask: specifies the field within the register to modify
+ *	@val: new value for the field
+ *
+ *	Sets a field of an indirect TP register to the given value.
+ */
+void t4_tp_wr_bits_indirect(struct adapter *adap, unsigned int addr,
+			    unsigned int mask, unsigned int val)
+{
+	t4_write_reg(adap, A_TP_PIO_ADDR, addr);
+	val |= t4_read_reg(adap, A_TP_PIO_DATA) & ~mask;
+	t4_write_reg(adap, A_TP_PIO_DATA, val);
+}
+
+/**
+ *	init_cong_ctrl - initialize congestion control parameters
+ *	@a: the alpha values for congestion control
+ *	@b: the beta values for congestion control
+ *
+ *	Initialize the congestion control parameters.
+ */
+static void __devinit init_cong_ctrl(unsigned short *a, unsigned short *b)
+{
+	a[0] = a[1] = a[2] = a[3] = a[4] = a[5] = a[6] = a[7] = a[8] = 1;
+	a[9] = 2;
+	a[10] = 3;
+	a[11] = 4;
+	a[12] = 5;
+	a[13] = 6;
+	a[14] = 7;
+	a[15] = 8;
+	a[16] = 9;
+	a[17] = 10;
+	a[18] = 14;
+	a[19] = 17;
+	a[20] = 21;
+	a[21] = 25;
+	a[22] = 30;
+	a[23] = 35;
+	a[24] = 45;
+	a[25] = 60;
+	a[26] = 80;
+	a[27] = 100;
+	a[28] = 200;
+	a[29] = 300;
+	a[30] = 400;
+	a[31] = 500;
+
+	b[0] = b[1] = b[2] = b[3] = b[4] = b[5] = b[6] = b[7] = b[8] = 0;
+	b[9] = b[10] = 1;
+	b[11] = b[12] = 2;
+	b[13] = b[14] = b[15] = b[16] = 3;
+	b[17] = b[18] = b[19] = b[20] = b[21] = 4;
+	b[22] = b[23] = b[24] = b[25] = b[26] = b[27] = 5;
+	b[28] = b[29] = 6;
+	b[30] = b[31] = 7;
+}
+
+/* The minimum additive increment value for the congestion control table */
+#define CC_MIN_INCR 2U
+
+/**
+ *	t4_load_mtus - write the MTU and congestion control HW tables
+ *	@adap: the adapter
+ *	@mtus: the values for the MTU table
+ *	@alpha: the values for the congestion control alpha parameter
+ *	@beta: the values for the congestion control beta parameter
+ *
+ *	Write the HW MTU table with the supplied MTUs and the high-speed
+ *	congestion control table with the supplied alpha, beta, and MTUs.
+ *	We write the two tables together because the additive increments
+ *	depend on the MTUs.
+ */
+void t4_load_mtus(struct adapter *adap, const unsigned short *mtus,
+		  const unsigned short *alpha, const unsigned short *beta)
+{
+	static const unsigned int avg_pkts[NCCTRL_WIN] = {
+		2, 6, 10, 14, 20, 28, 40, 56, 80, 112, 160, 224, 320, 448, 640,
+		896, 1281, 1792, 2560, 3584, 5120, 7168, 10240, 14336, 20480,
+		28672, 40960, 57344, 81920, 114688, 163840, 229376
+	};
+
+	unsigned int i, w;
+
+	for (i = 0; i < NMTUS; ++i) {
+		unsigned int mtu = mtus[i];
+		unsigned int log2 = fls(mtu);
+
+		if (!(mtu & ((1 << log2) >> 2)))     /* round */
+			log2--;
+		t4_write_reg(adap, A_TP_MTU_TABLE, V_MTUINDEX(i) |
+			     V_MTUWIDTH(log2) | V_MTUVALUE(mtu));
+
+		for (w = 0; w < NCCTRL_WIN; ++w) {
+			unsigned int inc;
+
+			inc = max(((mtu - 40) * alpha[w]) / avg_pkts[w],
+				  CC_MIN_INCR);
+
+			t4_write_reg(adap, A_TP_CCTRL_TABLE, (i << 21) |
+				     (w << 16) | (beta[w] << 13) | inc);
+		}
+	}
+}
+
+/**
+ *	t4_set_pace_tbl - set the pace table
+ *	@adap: the adapter
+ *	@pace_vals: the pace values in microseconds
+ *	@start: index of the first entry in the HW pace table to set
+ *	@n: how many entries to set
+ *
+ *	Sets (a subset of the) HW pace table.
+ */
+int t4_set_pace_tbl(struct adapter *adap, const unsigned int *pace_vals,
+		     unsigned int start, unsigned int n)
+{
+	unsigned int vals[NTX_SCHED], i;
+	unsigned int tick_ns = dack_ticks_to_usec(adap, 1000);
+
+    if (n > NTX_SCHED)
+        return -ERANGE;
+    
+	/* convert values from us to dack ticks, rounding to closest value */
+	for (i = 0; i < n; i++, pace_vals++) {
+		vals[i] = (1000 * *pace_vals + tick_ns / 2) / tick_ns;
+		if (vals[i] > 0x7ff)
+			return -ERANGE;
+		if (*pace_vals && vals[i] == 0)
+			return -ERANGE;
+	}
+	for (i = 0; i < n; i++, start++)
+		t4_write_reg(adap, A_TP_PACE_TABLE, (start << 16) | vals[i]);
+	return 0;
+}
+
+/**
+ *	t4_set_sched_bps - set the bit rate for a HW traffic scheduler
+ *	@adap: the adapter
+ *	@kbps: target rate in Kbps
+ *	@sched: the scheduler index
+ *
+ *	Configure a Tx HW scheduler for the target rate.
+ */
+int t4_set_sched_bps(struct adapter *adap, int sched, unsigned int kbps)
+{
+	unsigned int v, tps, cpt, bpt, delta, mindelta = ~0;
+	unsigned int clk = adap->params.vpd.cclk * 1000;
+	unsigned int selected_cpt = 0, selected_bpt = 0;
+
+	if (kbps > 0) {
+		kbps *= 125;     /* -> bytes */
+		for (cpt = 1; cpt <= 255; cpt++) {
+			tps = clk / cpt;
+			bpt = (kbps + tps / 2) / tps;
+			if (bpt > 0 && bpt <= 255) {
+				v = bpt * tps;
+				delta = v >= kbps ? v - kbps : kbps - v;
+				if (delta < mindelta) {
+					mindelta = delta;
+					selected_cpt = cpt;
+					selected_bpt = bpt;
+				}
+			} else if (selected_cpt)
+				break;
+		}
+		if (!selected_cpt)
+			return -EINVAL;
+	}
+	t4_write_reg(adap, A_TP_TM_PIO_ADDR,
+		     A_TP_TX_MOD_Q1_Q0_RATE_LIMIT - sched / 2);
+	v = t4_read_reg(adap, A_TP_TM_PIO_DATA);
+	if (sched & 1)
+		v = (v & 0xffff) | (selected_cpt << 16) | (selected_bpt << 24);
+	else
+		v = (v & 0xffff0000) | selected_cpt | (selected_bpt << 8);
+	t4_write_reg(adap, A_TP_TM_PIO_DATA, v);
+	return 0;
+}
+
+/**
+ *	t4_set_sched_ipg - set the IPG for a Tx HW packet rate scheduler
+ *	@adap: the adapter
+ *	@sched: the scheduler index
+ *	@ipg: the interpacket delay in tenths of nanoseconds
+ *
+ *	Set the interpacket delay for a HW packet rate scheduler.
+ */
+int t4_set_sched_ipg(struct adapter *adap, int sched, unsigned int ipg)
+{
+	unsigned int v, addr = A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR - sched / 2;
+
+	/* convert ipg to nearest number of core clocks */
+	ipg *= core_ticks_per_usec(adap);
+	ipg = (ipg + 5000) / 10000;
+	if (ipg > M_TXTIMERSEPQ0)
+		return -EINVAL;
+
+	t4_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+	v = t4_read_reg(adap, A_TP_TM_PIO_DATA);
+	if (sched & 1)
+		v = (v & V_TXTIMERSEPQ0(M_TXTIMERSEPQ0)) | V_TXTIMERSEPQ1(ipg);
+	else
+		v = (v & V_TXTIMERSEPQ1(M_TXTIMERSEPQ1)) | V_TXTIMERSEPQ0(ipg);
+	t4_write_reg(adap, A_TP_TM_PIO_DATA, v);
+	t4_read_reg(adap, A_TP_TM_PIO_DATA);
+	return 0;
+}
+
+/**
+ *	t4_get_tx_sched - get the configuration of a Tx HW traffic scheduler
+ *	@adap: the adapter
+ *	@sched: the scheduler index
+ *	@kbps: the byte rate in Kbps
+ *	@ipg: the interpacket delay in tenths of nanoseconds
+ *
+ *	Return the current configuration of a HW Tx scheduler.
+ */
+void t4_get_tx_sched(struct adapter *adap, unsigned int sched, unsigned int *kbps,
+		     unsigned int *ipg)
+{
+	unsigned int v, addr, bpt, cpt;
+
+	if (kbps) {
+		addr = A_TP_TX_MOD_Q1_Q0_RATE_LIMIT - sched / 2;
+		t4_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+		v = t4_read_reg(adap, A_TP_TM_PIO_DATA);
+		if (sched & 1)
+			v >>= 16;
+		bpt = (v >> 8) & 0xff;
+		cpt = v & 0xff;
+		if (!cpt)
+			*kbps = 0;        /* scheduler disabled */
+		else {
+			v = (adap->params.vpd.cclk * 1000) / cpt; /* ticks/s */
+			*kbps = (v * bpt) / 125;
+		}
+	}
+	if (ipg) {
+		addr = A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR - sched / 2;
+		t4_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+		v = t4_read_reg(adap, A_TP_TM_PIO_DATA);
+		if (sched & 1)
+			v >>= 16;
+		v &= 0xffff;
+		*ipg = (10000 * v) / core_ticks_per_usec(adap);
+	}
+}
+
+/*
+ * Calculates a rate in bytes/s given the number of 256-byte units per 4K core
+ * clocks.  The formula is
+ *
+ * bytes/s = bytes256 * 256 * ClkFreq / 4096
+ *
+ * which is equivalent to
+ *
+ * bytes/s = 62.5 * bytes256 * ClkFreq_ms
+ */
+static u64 chan_rate(struct adapter *adap, unsigned int bytes256)
+{
+	u64 v = bytes256 * adap->params.vpd.cclk;
+
+	return v * 62 + v / 2;
+}
+
+/**
+ *	t4_get_chan_txrate - get the current per channel Tx rates
+ *	@adap: the adapter
+ *	@nic_rate: rates for NIC traffic
+ *	@ofld_rate: rates for offloaded traffic
+ *
+ *	Return the current Tx rates in bytes/s for NIC and offloaded traffic
+ *	for each channel.
+ */
+void t4_get_chan_txrate(struct adapter *adap, u64 *nic_rate, u64 *ofld_rate)
+{
+	u32 v;
+
+	v = t4_read_reg(adap, A_TP_TX_TRATE);
+	nic_rate[0] = chan_rate(adap, G_TNLRATE0(v));
+	nic_rate[1] = chan_rate(adap, G_TNLRATE1(v));
+	nic_rate[2] = chan_rate(adap, G_TNLRATE2(v));
+	nic_rate[3] = chan_rate(adap, G_TNLRATE3(v));
+
+	v = t4_read_reg(adap, A_TP_TX_ORATE);
+	ofld_rate[0] = chan_rate(adap, G_OFDRATE0(v));
+	ofld_rate[1] = chan_rate(adap, G_OFDRATE1(v));
+	ofld_rate[2] = chan_rate(adap, G_OFDRATE2(v));
+	ofld_rate[3] = chan_rate(adap, G_OFDRATE3(v));
+}
+
+/**
+ *	t4_set_trace_filter - configure one of the tracing filters
+ *	@adap: the adapter
+ *	@tp: the desired trace filter parameters
+ *	@idx: which filter to configure
+ *	@enable: whether to enable or disable the filter
+ *
+ *	Configures one of the tracing filters available in HW.  If @enable is
+ *	%0 @tp is not examined and may be %NULL. The user is responsible to
+ *	set the single/multiple trace mode by writing to A_MPS_TRC_CFG register
+ *	by using "cxgbtool iface reg reg_addr=val" command. See t4_sniffer/
+ *	docs/readme.txt for a complete description of how to setup traceing on
+ *	T4.
+ */
+int t4_set_trace_filter(struct adapter *adap, const struct trace_params *tp, int idx,
+			int enable)
+{
+	int i, ofst = idx * 4;
+	u32 data_reg, mask_reg, cfg;
+	u32 multitrc = F_TRCMULTIFILTER;
+
+	if (!enable) {
+		t4_write_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_A + ofst, 0);
+		return 0;
+	}
+
+	/*
+	 * TODO - After T4 data book is updated, specify the exact
+	 * section below.
+	 *
+	 * See T4 data book - MPS section for a complete description 
+	 * of the below if..else handling of A_MPS_TRC_CFG register 
+	 * value.
+	 */ 
+	cfg = t4_read_reg(adap, A_MPS_TRC_CFG);
+	if (cfg & F_TRCMULTIFILTER) {
+		/*
+		 * If multiple tracers are enabled, then maximum
+		 * capture size is 2.5KB (FIFO size of a single channel)
+		 * minus 2 flits for CPL_TRACE_PKT header.
+		 */
+		if (tp->snap_len > ((10 * 1024 / 4) - (2 * 8)))
+			return -EINVAL;		
+	}
+	else {
+		/*
+		 * If multiple tracers are disabled, to avoid deadlocks 
+		 * maximum packet capture size of 9600 bytes is recommended.
+		 * Also in this mode, only trace0 can be enabled and running.
+		 */
+		multitrc = 0;
+		if (tp->snap_len > 9600 || idx)
+			return -EINVAL;
+	}
+
+	if (tp->port > 11 || tp->invert > 1 || tp->skip_len > M_TFLENGTH ||
+	    tp->skip_ofst > M_TFOFFSET || tp->min_len > M_TFMINPKTSIZE)
+		return -EINVAL;
+
+	/* stop the tracer we'll be changing */
+	t4_write_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_A + ofst, 0);
+
+	idx *= (A_MPS_TRC_FILTER1_MATCH - A_MPS_TRC_FILTER0_MATCH);
+	data_reg = A_MPS_TRC_FILTER0_MATCH + idx;
+	mask_reg = A_MPS_TRC_FILTER0_DONT_CARE + idx;
+
+	for (i = 0; i < TRACE_LEN / 4; i++, data_reg += 4, mask_reg += 4) {
+		t4_write_reg(adap, data_reg, tp->data[i]);
+		t4_write_reg(adap, mask_reg, ~tp->mask[i]);
+	}
+	t4_write_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_B + ofst,
+		     V_TFCAPTUREMAX(tp->snap_len) |
+		     V_TFMINPKTSIZE(tp->min_len));
+	t4_write_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_A + ofst,
+		     V_TFOFFSET(tp->skip_ofst) | V_TFLENGTH(tp->skip_len) |
+		     V_TFPORT(tp->port) | F_TFEN | V_TFINVERTMATCH(tp->invert));
+
+	return 0;
+}
+
+/**
+ *	t4_get_trace_filter - query one of the tracing filters
+ *	@adap: the adapter
+ *	@tp: the current trace filter parameters
+ *	@idx: which trace filter to query
+ *	@enabled: non-zero if the filter is enabled
+ *
+ *	Returns the current settings of one of the HW tracing filters.
+ */
+void t4_get_trace_filter(struct adapter *adap, struct trace_params *tp, int idx,
+			 int *enabled)
+{
+	u32 ctla, ctlb;
+	int i, ofst = idx * 4;
+	u32 data_reg, mask_reg;
+
+	ctla = t4_read_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_A + ofst);
+	ctlb = t4_read_reg(adap, A_MPS_TRC_FILTER_MATCH_CTL_B + ofst);
+
+	*enabled = !!(ctla & F_TFEN);
+	tp->snap_len = G_TFCAPTUREMAX(ctlb);
+	tp->min_len = G_TFMINPKTSIZE(ctlb);
+	tp->skip_ofst = G_TFOFFSET(ctla);
+	tp->skip_len = G_TFLENGTH(ctla);
+	tp->invert = !!(ctla & F_TFINVERTMATCH);
+	tp->port = G_TFPORT(ctla);
+
+	ofst = (A_MPS_TRC_FILTER1_MATCH - A_MPS_TRC_FILTER0_MATCH) * idx;
+	data_reg = A_MPS_TRC_FILTER0_MATCH + ofst;
+	mask_reg = A_MPS_TRC_FILTER0_DONT_CARE + ofst;
+
+	for (i = 0; i < TRACE_LEN / 4; i++, data_reg += 4, mask_reg += 4) {
+		tp->mask[i] = ~t4_read_reg(adap, mask_reg);
+		tp->data[i] = t4_read_reg(adap, data_reg) & tp->mask[i];
+	}
+}
+
+/**
+ *	t4_pmtx_get_stats - returns the HW stats from PMTX
+ *	@adap: the adapter
+ *	@cnt: where to store the count statistics
+ *	@cycles: where to store the cycle statistics
+ *
+ *	Returns performance statistics from PMTX.
+ */
+void t4_pmtx_get_stats(struct adapter *adap, u32 cnt[], u64 cycles[])
+{
+	int i;
+
+	for (i = 0; i < PM_NSTATS; i++) {
+		t4_write_reg(adap, A_PM_TX_STAT_CONFIG, i + 1);
+		cnt[i] = t4_read_reg(adap, A_PM_TX_STAT_COUNT);
+		cycles[i] = t4_read_reg64(adap, A_PM_TX_STAT_LSB);
+	}
+}
+
+/**
+ *	t4_pmrx_get_stats - returns the HW stats from PMRX
+ *	@adap: the adapter
+ *	@cnt: where to store the count statistics
+ *	@cycles: where to store the cycle statistics
+ *
+ *	Returns performance statistics from PMRX.
+ */
+void t4_pmrx_get_stats(struct adapter *adap, u32 cnt[], u64 cycles[])
+{
+	int i;
+
+	for (i = 0; i < PM_NSTATS; i++) {
+		t4_write_reg(adap, A_PM_RX_STAT_CONFIG, i + 1);
+		cnt[i] = t4_read_reg(adap, A_PM_RX_STAT_COUNT);
+		cycles[i] = t4_read_reg64(adap, A_PM_RX_STAT_LSB);
+	}
+}
+
+/**
+ *	get_mps_bg_map - return the buffer groups associated with a port
+ *	@adap: the adapter
+ *	@idx: the port index
+ *
+ *	Returns a bitmap indicating which MPS buffer groups are associated
+ *	with the given port.  Bit i is set if buffer group i is used by the
+ *	port.
+ */
+static unsigned int get_mps_bg_map(struct adapter *adap, int idx)
+{
+	u32 n = G_NUMPORTS(t4_read_reg(adap, A_MPS_CMN_CTL));
+
+	if (n == 0)
+		return idx == 0 ? 0xf : 0;
+	if (n == 1)
+		return idx < 2 ? (3 << (2 * idx)) : 0;
+	return 1 << idx;
+}
+
+/**
+ *      t4_get_port_stats_offset - collect port stats relative to a previous
+ *                                 snapshot
+ *      @adap: The adapter
+ *      @idx: The port
+ *      @stats: Current stats to fill
+ *      @offset: Previous stats snapshot
+ */
+void t4_get_port_stats_offset(struct adapter *adap, int idx,
+		struct port_stats *stats,
+		struct port_stats *offset)
+{
+	u64 *s, *o;
+	int i;
+
+	t4_get_port_stats(adap, idx, stats);
+	for (i = 0, s = (u64 *)stats, o = (u64 *)offset ;
+			i < (sizeof(struct port_stats)/sizeof(u64)) ;
+			i++, s++, o++)
+		*s -= *o;
+}
+
+/**
+ *	t4_get_port_stats - collect port statistics
+ *	@adap: the adapter
+ *	@idx: the port index
+ *	@p: the stats structure to fill
+ *
+ *	Collect statistics related to the given port from HW.
+ */
+void t4_get_port_stats(struct adapter *adap, int idx, struct port_stats *p)
+{
+	u32 bgmap = get_mps_bg_map(adap, idx);
+
+#define GET_STAT(name) \
+	t4_read_reg64(adap, PORT_REG(idx, A_MPS_PORT_STAT_##name##_L))
+#define GET_STAT_COM(name) t4_read_reg64(adap, A_MPS_STAT_##name##_L)
+
+	p->tx_octets           = GET_STAT(TX_PORT_BYTES);
+	p->tx_frames           = GET_STAT(TX_PORT_FRAMES);
+	p->tx_bcast_frames     = GET_STAT(TX_PORT_BCAST);
+	p->tx_mcast_frames     = GET_STAT(TX_PORT_MCAST);
+	p->tx_ucast_frames     = GET_STAT(TX_PORT_UCAST);							       
+       	p->tx_error_frames     = GET_STAT(TX_PORT_ERROR);
+	p->tx_frames_64        = GET_STAT(TX_PORT_64B);
+	p->tx_frames_65_127    = GET_STAT(TX_PORT_65B_127B);
+	p->tx_frames_128_255   = GET_STAT(TX_PORT_128B_255B);
+	p->tx_frames_256_511   = GET_STAT(TX_PORT_256B_511B);
+	p->tx_frames_512_1023  = GET_STAT(TX_PORT_512B_1023B);
+	p->tx_frames_1024_1518 = GET_STAT(TX_PORT_1024B_1518B);
+	p->tx_frames_1519_max  = GET_STAT(TX_PORT_1519B_MAX);
+	p->tx_drop             = GET_STAT(TX_PORT_DROP);
+	p->tx_pause            = GET_STAT(TX_PORT_PAUSE);
+	p->tx_ppp0             = GET_STAT(TX_PORT_PPP0);
+	p->tx_ppp1             = GET_STAT(TX_PORT_PPP1);
+	p->tx_ppp2             = GET_STAT(TX_PORT_PPP2);
+	p->tx_ppp3             = GET_STAT(TX_PORT_PPP3);
+	p->tx_ppp4             = GET_STAT(TX_PORT_PPP4);
+	p->tx_ppp5             = GET_STAT(TX_PORT_PPP5);
+	p->tx_ppp6             = GET_STAT(TX_PORT_PPP6);
+	p->tx_ppp7             = GET_STAT(TX_PORT_PPP7);
+
+        p->rx_octets           = GET_STAT(RX_PORT_BYTES);
+	p->rx_frames           = GET_STAT(RX_PORT_FRAMES);
+	p->rx_bcast_frames     = GET_STAT(RX_PORT_BCAST);
+	p->rx_mcast_frames     = GET_STAT(RX_PORT_MCAST);
+	p->rx_ucast_frames     = GET_STAT(RX_PORT_UCAST);
+	p->rx_too_long         = GET_STAT(RX_PORT_MTU_ERROR);
+	p->rx_jabber           = GET_STAT(RX_PORT_MTU_CRC_ERROR);
+	p->rx_fcs_err          = GET_STAT(RX_PORT_CRC_ERROR);
+	p->rx_len_err          = GET_STAT(RX_PORT_LEN_ERROR);
+	p->rx_symbol_err       = GET_STAT(RX_PORT_SYM_ERROR);
+	p->rx_runt             = GET_STAT(RX_PORT_LESS_64B);
+	p->rx_frames_64        = GET_STAT(RX_PORT_64B);
+	p->rx_frames_65_127    = GET_STAT(RX_PORT_65B_127B);
+	p->rx_frames_128_255   = GET_STAT(RX_PORT_128B_255B);
+	p->rx_frames_256_511   = GET_STAT(RX_PORT_256B_511B);
+	p->rx_frames_512_1023  = GET_STAT(RX_PORT_512B_1023B);
+	p->rx_frames_1024_1518 = GET_STAT(RX_PORT_1024B_1518B);
+	p->rx_frames_1519_max  = GET_STAT(RX_PORT_1519B_MAX);
+	p->rx_pause            = GET_STAT(RX_PORT_PAUSE);
+	p->rx_ppp0             = GET_STAT(RX_PORT_PPP0);
+	p->rx_ppp1             = GET_STAT(RX_PORT_PPP1);
+	p->rx_ppp2             = GET_STAT(RX_PORT_PPP2);
+	p->rx_ppp3             = GET_STAT(RX_PORT_PPP3);
+	p->rx_ppp4             = GET_STAT(RX_PORT_PPP4);
+	p->rx_ppp5             = GET_STAT(RX_PORT_PPP5);
+	p->rx_ppp6             = GET_STAT(RX_PORT_PPP6);
+	p->rx_ppp7             = GET_STAT(RX_PORT_PPP7);
+	p->rx_ovflow0 = (bgmap & 1) ? GET_STAT_COM(RX_BG_0_MAC_DROP_FRAME) : 0;
+	p->rx_ovflow1 = (bgmap & 2) ? GET_STAT_COM(RX_BG_1_MAC_DROP_FRAME) : 0;
+	p->rx_ovflow2 = (bgmap & 4) ? GET_STAT_COM(RX_BG_2_MAC_DROP_FRAME) : 0;
+	p->rx_ovflow3 = (bgmap & 8) ? GET_STAT_COM(RX_BG_3_MAC_DROP_FRAME) : 0;
+	p->rx_trunc0 = (bgmap & 1) ? GET_STAT_COM(RX_BG_0_MAC_TRUNC_FRAME) : 0;
+	p->rx_trunc1 = (bgmap & 2) ? GET_STAT_COM(RX_BG_1_MAC_TRUNC_FRAME) : 0;
+	p->rx_trunc2 = (bgmap & 4) ? GET_STAT_COM(RX_BG_2_MAC_TRUNC_FRAME) : 0;
+	p->rx_trunc3 = (bgmap & 8) ? GET_STAT_COM(RX_BG_3_MAC_TRUNC_FRAME) : 0;
+
+#undef GET_STAT
+#undef GET_STAT_COM
+}
+
+/**
+ *	t4_clr_port_stats - clear port statistics
+ *	@adap: the adapter
+ *	@idx: the port index
+ *
+ *	Clear HW statistics for the given port.
+ */
+void t4_clr_port_stats(struct adapter *adap, int idx)
+{
+	unsigned int i;
+	u32 bgmap = get_mps_bg_map(adap, idx);
+
+	for (i = A_MPS_PORT_STAT_TX_PORT_BYTES_L;
+			i <= A_MPS_PORT_STAT_TX_PORT_PPP7_H; i += 8)
+		t4_write_reg(adap, PORT_REG(idx, i), 0);
+	for (i = A_MPS_PORT_STAT_RX_PORT_BYTES_L;
+			i <= A_MPS_PORT_STAT_RX_PORT_LESS_64B_H; i += 8)
+		t4_write_reg(adap, PORT_REG(idx, i), 0);
+	for (i = 0; i < 4; i++)
+		if (bgmap & (1 << i)) {
+			t4_write_reg(adap,
+			A_MPS_STAT_RX_BG_0_MAC_DROP_FRAME_L + i * 8, 0);
+			t4_write_reg(adap,
+			A_MPS_STAT_RX_BG_0_MAC_TRUNC_FRAME_L + i * 8, 0);
+		}
+}
+
+/**
+ *	t4_get_lb_stats - collect loopback port statistics
+ *	@adap: the adapter
+ *	@idx: the loopback port index
+ *	@p: the stats structure to fill
+ *
+ *	Return HW statistics for the given loopback port.
+ */
+void t4_get_lb_stats(struct adapter *adap, int idx, struct lb_port_stats *p)
+{
+	u32 bgmap = get_mps_bg_map(adap, idx);
+
+#define GET_STAT(name) \
+	t4_read_reg64(adap, PORT_REG(idx, A_MPS_PORT_STAT_LB_PORT_##name##_L))
+#define GET_STAT_COM(name) t4_read_reg64(adap, A_MPS_STAT_##name##_L)
+
+	p->octets           = GET_STAT(BYTES);
+	p->frames           = GET_STAT(FRAMES);
+	p->bcast_frames     = GET_STAT(BCAST);
+	p->mcast_frames     = GET_STAT(MCAST);
+	p->ucast_frames     = GET_STAT(UCAST);
+	p->error_frames     = GET_STAT(ERROR);
+
+	p->frames_64        = GET_STAT(64B);
+	p->frames_65_127    = GET_STAT(65B_127B);
+	p->frames_128_255   = GET_STAT(128B_255B);
+	p->frames_256_511   = GET_STAT(256B_511B);
+	p->frames_512_1023  = GET_STAT(512B_1023B);
+	p->frames_1024_1518 = GET_STAT(1024B_1518B);
+	p->frames_1519_max  = GET_STAT(1519B_MAX);
+	p->drop             = t4_read_reg(adap, PORT_REG(idx,
+					  A_MPS_PORT_STAT_LB_PORT_DROP_FRAMES));
+
+	p->ovflow0 = (bgmap & 1) ? GET_STAT_COM(RX_BG_0_LB_DROP_FRAME) : 0;
+	p->ovflow1 = (bgmap & 2) ? GET_STAT_COM(RX_BG_1_LB_DROP_FRAME) : 0;
+	p->ovflow2 = (bgmap & 4) ? GET_STAT_COM(RX_BG_2_LB_DROP_FRAME) : 0;
+	p->ovflow3 = (bgmap & 8) ? GET_STAT_COM(RX_BG_3_LB_DROP_FRAME) : 0;
+	p->trunc0 = (bgmap & 1) ? GET_STAT_COM(RX_BG_0_LB_TRUNC_FRAME) : 0;
+	p->trunc1 = (bgmap & 2) ? GET_STAT_COM(RX_BG_1_LB_TRUNC_FRAME) : 0;
+	p->trunc2 = (bgmap & 4) ? GET_STAT_COM(RX_BG_2_LB_TRUNC_FRAME) : 0;
+	p->trunc3 = (bgmap & 8) ? GET_STAT_COM(RX_BG_3_LB_TRUNC_FRAME) : 0;
+
+#undef GET_STAT
+#undef GET_STAT_COM
+}
+
+/**
+ * 	t4_wol_magic_enable - enable/disable magic packet WoL
+ * 	@adap: the adapter
+ * 	@port: the physical port index
+ * 	@addr: MAC address expected in magic packets, %NULL to disable
+ *
+ * 	Enables/disables magic packet wake-on-LAN for the selected port.
+ */
+void t4_wol_magic_enable(struct adapter *adap, unsigned int port,
+			 const u8 *addr)
+{
+	if (addr) {
+		t4_write_reg(adap, PORT_REG(port, A_XGMAC_PORT_MAGIC_MACID_LO),
+			     (addr[2] << 24) | (addr[3] << 16) |
+			     (addr[4] << 8) | addr[5]);
+		t4_write_reg(adap, PORT_REG(port, A_XGMAC_PORT_MAGIC_MACID_HI),
+			     (addr[0] << 8) | addr[1]);
+	}
+	t4_set_reg_field(adap, PORT_REG(port, A_XGMAC_PORT_CFG2), F_MAGICEN,
+			 V_MAGICEN(addr != NULL));
+}
+
+/**
+ *	t4_wol_pat_enable - enable/disable pattern-based WoL
+ *	@adap: the adapter
+ *	@port: the physical port index
+ *	@map: bitmap of which HW pattern filters to set
+ *	@mask0: byte mask for bytes 0-63 of a packet
+ *	@mask1: byte mask for bytes 64-127 of a packet
+ *	@crc: Ethernet CRC for selected bytes
+ *	@enable: enable/disable switch
+ *
+ *	Sets the pattern filters indicated in @map to mask out the bytes
+ *	specified in @mask0/@mask1 in received packets and compare the CRC of
+ *	the resulting packet against @crc.  If @enable is %true pattern-based
+ *	WoL is enabled, otherwise disabled.
+ */
+int t4_wol_pat_enable(struct adapter *adap, unsigned int port, unsigned int map,
+		      u64 mask0, u64 mask1, unsigned int crc, bool enable)
+{
+	int i;
+
+	if (!enable) {
+		t4_set_reg_field(adap, PORT_REG(port, A_XGMAC_PORT_CFG2),
+				 F_PATEN, 0);
+		return 0;
+	}
+	if (map > 0xff)
+		return -EINVAL;
+
+#define EPIO_REG(name) PORT_REG(port, A_XGMAC_PORT_EPIO_##name)
+
+	t4_write_reg(adap, EPIO_REG(DATA1), mask0 >> 32);
+	t4_write_reg(adap, EPIO_REG(DATA2), mask1);
+	t4_write_reg(adap, EPIO_REG(DATA3), mask1 >> 32);
+
+	for (i = 0; i < NWOL_PAT; i++, map >>= 1) {
+		if (!(map & 1))
+			continue;
+
+		/* write byte masks */
+		t4_write_reg(adap, EPIO_REG(DATA0), mask0);
+		t4_write_reg(adap, EPIO_REG(OP), V_ADDRESS(i) | F_EPIOWR);
+		t4_read_reg(adap, EPIO_REG(OP));                /* flush */
+		if (t4_read_reg(adap, EPIO_REG(OP)) & F_BUSY)
+			return -ETIMEDOUT;
+
+		/* write CRC */
+		t4_write_reg(adap, EPIO_REG(DATA0), crc);
+		t4_write_reg(adap, EPIO_REG(OP), V_ADDRESS(i + 32) | F_EPIOWR);
+		t4_read_reg(adap, EPIO_REG(OP));                /* flush */
+		if (t4_read_reg(adap, EPIO_REG(OP)) & F_BUSY)
+			return -ETIMEDOUT;
+	}
+#undef EPIO_REG
+
+	t4_set_reg_field(adap, PORT_REG(port, A_XGMAC_PORT_CFG2), 0, F_PATEN);
+	return 0;
+}
+
+/**
+ *	t4_mk_filtdelwr - create a delete filter WR
+ *	@ftid: the filter ID
+ *	@wr: the filter work request to populate
+ *	@qid: ingress queue to receive the delete notification
+ *
+ *	Creates a filter work request to delete the supplied filter.  If @qid is
+ *	negative the delete notification is suppressed.
+ */
+void t4_mk_filtdelwr(unsigned int ftid, struct fw_filter_wr *wr, int qid)
+{
+	memset(wr, 0, sizeof(*wr));
+	wr->op_pkd = htonl(V_FW_WR_OP(FW_FILTER_WR));
+	wr->len16_pkd = htonl(V_FW_WR_LEN16(sizeof(*wr) / 16));
+	wr->tid_to_iq = htonl(V_FW_FILTER_WR_TID(ftid) |
+			      V_FW_FILTER_WR_NOREPLY(qid < 0));
+	wr->del_filter_to_l2tix = htonl(F_FW_FILTER_WR_DEL_FILTER);
+	if (qid >= 0)
+		wr->rx_chan_rx_rpl_iq = htons(V_FW_FILTER_WR_RX_RPL_IQ(qid));
+}
+
+#define INIT_CMD(var, cmd, rd_wr) do { \
+	(var).op_to_write = htonl(V_FW_CMD_OP(FW_##cmd##_CMD) | \
+				  F_FW_CMD_REQUEST | F_FW_CMD_##rd_wr); \
+	(var).retval_len16 = htonl(FW_LEN16(var)); \
+} while (0)
+
+int t4_fwaddrspace_write(struct adapter *adap, unsigned int mbox, u32 addr, u32 val)
+{
+	struct fw_ldst_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_addrspace = htonl(V_FW_CMD_OP(FW_LDST_CMD) | F_FW_CMD_REQUEST |
+		F_FW_CMD_WRITE | V_FW_LDST_CMD_ADDRSPACE(FW_LDST_ADDRSPC_FIRMWARE));
+	c.cycles_to_len16 = htonl(FW_LEN16(c));
+	c.u.addrval.addr = htonl(addr);
+	c.u.addrval.val = htonl(val);
+
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_mdio_rd - read a PHY register through MDIO
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@phy_addr: the PHY address
+ * 	@mmd: the PHY MMD to access (0 for clause 22 PHYs)
+ * 	@reg: the register to read
+ * 	@valp: where to store the value
+ *
+ * 	Issues a FW command through the given mailbox to read a PHY register.
+ */
+int t4_mdio_rd(struct adapter *adap, unsigned int mbox, unsigned int phy_addr,
+	       unsigned int mmd, unsigned int reg, unsigned int *valp)
+{
+	int ret;
+	struct fw_ldst_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_addrspace = htonl(V_FW_CMD_OP(FW_LDST_CMD) | F_FW_CMD_REQUEST |
+		F_FW_CMD_READ | V_FW_LDST_CMD_ADDRSPACE(FW_LDST_ADDRSPC_MDIO));
+	c.cycles_to_len16 = htonl(FW_LEN16(c));
+	c.u.mdio.paddr_mmd = htons(V_FW_LDST_CMD_PADDR(phy_addr) |
+				   V_FW_LDST_CMD_MMD(mmd));
+	c.u.mdio.raddr = htons(reg);
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret == 0)
+		*valp = ntohs(c.u.mdio.rval);
+	return ret;
+}
+
+/**
+ * 	t4_mdio_wr - write a PHY register through MDIO
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@phy_addr: the PHY address
+ * 	@mmd: the PHY MMD to access (0 for clause 22 PHYs)
+ * 	@reg: the register to write
+ * 	@valp: value to write
+ *
+ * 	Issues a FW command through the given mailbox to write a PHY register.
+ */
+int t4_mdio_wr(struct adapter *adap, unsigned int mbox, unsigned int phy_addr,
+	       unsigned int mmd, unsigned int reg, unsigned int val)
+{
+	struct fw_ldst_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_addrspace = htonl(V_FW_CMD_OP(FW_LDST_CMD) | F_FW_CMD_REQUEST |
+		F_FW_CMD_WRITE | V_FW_LDST_CMD_ADDRSPACE(FW_LDST_ADDRSPC_MDIO));
+	c.cycles_to_len16 = htonl(FW_LEN16(c));
+	c.u.mdio.paddr_mmd = htons(V_FW_LDST_CMD_PADDR(phy_addr) |
+				   V_FW_LDST_CMD_MMD(mmd));
+	c.u.mdio.raddr = htons(reg);
+	c.u.mdio.rval = htons(val);
+
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ *      t4_sge_ctxt_flush - flush the SGE context cache
+ *      @adap: the adapter
+ *      @mbox: mailbox to use for the FW command
+ *
+ *      Issues a FW command through the given mailbox to flush the
+ *      SGE context cache.
+ */
+int t4_sge_ctxt_flush(struct adapter *adap, unsigned int mbox)
+{
+	int ret;
+	struct fw_ldst_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_addrspace = htonl(V_FW_CMD_OP(FW_LDST_CMD) | F_FW_CMD_REQUEST |
+			F_FW_CMD_READ |
+			V_FW_LDST_CMD_ADDRSPACE(FW_LDST_ADDRSPC_SGE_EGRC));
+	c.cycles_to_len16 = htonl(FW_LEN16(c));
+	c.u.idctxt.msg_ctxtflush = htonl(F_FW_LDST_CMD_CTXTFLUSH);
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	return ret;
+}
+
+/**
+ * 	t4_sge_ctxt_rd - read an SGE context through FW
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@cid: the context id
+ * 	@ctype: the context type
+ * 	@data: where to store the context data
+ *
+ * 	Issues a FW command through the given mailbox to read an SGE context.
+ */
+int t4_sge_ctxt_rd(struct adapter *adap, unsigned int mbox, unsigned int cid,
+		   enum ctxt_type ctype, u32 *data)
+{
+	int ret; 
+	struct fw_ldst_cmd c;
+
+	if (ctype == CTXT_EGRESS)
+		ret = FW_LDST_ADDRSPC_SGE_EGRC;
+	else if (ctype == CTXT_INGRESS)
+		ret = FW_LDST_ADDRSPC_SGE_INGC;
+	else if (ctype == CTXT_FLM)
+		ret = FW_LDST_ADDRSPC_SGE_FLMC;
+	else
+		ret = FW_LDST_ADDRSPC_SGE_CONMC;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_addrspace = htonl(V_FW_CMD_OP(FW_LDST_CMD) | F_FW_CMD_REQUEST |
+				  F_FW_CMD_READ | V_FW_LDST_CMD_ADDRSPACE(ret));
+	c.cycles_to_len16 = htonl(FW_LEN16(c));
+	c.u.idctxt.physid = htonl(cid);
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret == 0) {
+		data[0] = ntohl(c.u.idctxt.ctxt_data0);
+		data[1] = ntohl(c.u.idctxt.ctxt_data1);
+		data[2] = ntohl(c.u.idctxt.ctxt_data2);
+		data[3] = ntohl(c.u.idctxt.ctxt_data3);
+		data[4] = ntohl(c.u.idctxt.ctxt_data4);
+		data[5] = ntohl(c.u.idctxt.ctxt_data5);
+	}
+	return ret;
+}
+
+/**
+ * 	t4_sge_ctxt_rd_bd - read an SGE context bypassing FW
+ * 	@adap: the adapter
+ * 	@cid: the context id
+ * 	@ctype: the context type
+ * 	@data: where to store the context data
+ *
+ * 	Reads an SGE context directly, bypassing FW.  This is only for
+ * 	debugging when FW is unavailable.
+ */
+int t4_sge_ctxt_rd_bd(struct adapter *adap, unsigned int cid, enum ctxt_type ctype,
+		      u32 *data)
+{
+	int i, ret; 
+
+	t4_write_reg(adap, A_SGE_CTXT_CMD, V_CTXTQID(cid) | V_CTXTTYPE(ctype));
+	ret = t4_wait_op_done(adap, A_SGE_CTXT_CMD, F_BUSY, 0, 3, 1);
+	if (!ret)
+		for (i = A_SGE_CTXT_DATA0; i <= A_SGE_CTXT_DATA5; i += 4)
+			*data++ = t4_read_reg(adap, i);
+	return ret;
+}
+
+/**
+ * 	t4_fw_hello - establish communication with FW
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@evt_mbox: mailbox to receive async FW events
+ * 	@master: specifies the caller's willingness to be the device master
+ * 	@state: returns the current device state (if non-NULL)
+ *
+ * 	Issues a command to establish communication with FW.  Returns either
+ *	an error (negative integer) or the mailbox of the Master PF.
+ */
+int t4_fw_hello(struct adapter *adap, unsigned int mbox, unsigned int evt_mbox,
+		enum dev_master master, enum dev_state *state)
+{
+	int ret; 
+	struct fw_hello_cmd c;
+	u32 v;
+	unsigned int master_mbox;
+	int retries = FW_CMD_HELLO_RETRIES;
+
+retry:
+	memset(&c, 0, sizeof(c));
+	INIT_CMD(c, HELLO, WRITE);
+	c.err_to_clearinit = htonl(
+		V_FW_HELLO_CMD_MASTERDIS(master == MASTER_CANT) |
+		V_FW_HELLO_CMD_MASTERFORCE(master == MASTER_MUST) |
+		V_FW_HELLO_CMD_MBMASTER(master == MASTER_MUST ? mbox :
+			M_FW_HELLO_CMD_MBMASTER) |
+		V_FW_HELLO_CMD_MBASYNCNOT(evt_mbox) |
+		V_FW_HELLO_CMD_STAGE(FW_HELLO_CMD_STAGE_OS) |
+		F_FW_HELLO_CMD_CLEARINIT);
+
+	/*
+	 * Issue the HELLO command to the firmware.  If it's not successful
+	 * but indicates that we got a "busy" or "timeout" condition, retry
+	 * the HELLO until we exhaust our retry limit.
+	 */
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret != FW_SUCCESS) {
+		if ((ret == -EBUSY || ret == -ETIMEDOUT) && retries-- > 0)
+			goto retry;
+		return ret;
+	}
+
+	v = ntohl(c.err_to_clearinit);
+	master_mbox = G_FW_HELLO_CMD_MBMASTER(v);
+	if (state) {
+		if (v & F_FW_HELLO_CMD_ERR)
+			*state = DEV_STATE_ERR;
+		else if (v & F_FW_HELLO_CMD_INIT)
+			*state = DEV_STATE_INIT;
+		else
+			*state = DEV_STATE_UNINIT;
+	}
+
+	/*
+	 * If we're not the Master PF then we need to wait around for the
+	 * Master PF Driver to finish setting up the adapter.
+	 *
+	 * Note that we also do this wait if we're a non-Master-capable PF and
+	 * there is no current Master PF; a Master PF may show up momentarily
+	 * and we wouldn't want to fail pointlessly.  (This can happen when an
+	 * OS loads lots of different drivers rapidly at the same time).  In
+	 * this case, the Master PF returned by the firmware will be
+	 * M_PCIE_FW_MASTER so the test below will work ...
+	 */
+	if ((v & (F_FW_HELLO_CMD_ERR|F_FW_HELLO_CMD_INIT)) == 0 &&
+	    master_mbox != mbox) {
+		int waiting = FW_CMD_HELLO_TIMEOUT;
+
+		/*
+		 * Wait for the firmware to either indicate an error or
+		 * initialized state.  If we see either of these we bail out
+		 * and report the issue to the caller.  If we exhaust the
+		 * "hello timeout" and we haven't exhausted our retries, try
+		 * again.  Otherwise bail with a timeout error.
+		 */
+		for (;;) {
+			u32 pcie_fw;
+
+			msleep(50);
+			waiting -= 50;
+
+			/*
+			 * If neither Error nor Initialialized are indicated
+			 * by the firmware keep waiting till we exaust our
+			 * timeout ... and then retry if we haven't exhausted
+			 * our retries ...
+			 */
+			pcie_fw = t4_read_reg(adap, A_PCIE_FW);
+			if (!(pcie_fw & (F_PCIE_FW_ERR|F_PCIE_FW_INIT))) {
+				if (waiting <= 0) {
+					if (retries-- > 0)
+						goto retry;
+
+					return -ETIMEDOUT;
+				}
+				continue;
+			}
+
+			/*
+			 * We either have an Error or Initialized condition
+			 * report errors preferentially.
+			 */
+			if (state) {
+				if (pcie_fw & F_PCIE_FW_ERR)
+					*state = DEV_STATE_ERR;
+				else if (pcie_fw & F_PCIE_FW_INIT)
+					*state = DEV_STATE_INIT;
+			}
+
+			/*
+			 * If we arrived before a Master PF was selected and
+			 * there's not a valid Master PF, grab its identity
+			 * for our caller.
+			 */
+			if (master_mbox == M_PCIE_FW_MASTER &&
+			    (pcie_fw & F_PCIE_FW_MASTER_VLD))
+				master_mbox = G_PCIE_FW_MASTER(pcie_fw);
+			break;
+		}
+	}
+
+	return master_mbox;
+}
+
+/**
+ * 	t4_fw_bye - end communication with FW
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ *
+ * 	Issues a command to terminate communication with FW.
+ */
+int t4_fw_bye(struct adapter *adap, unsigned int mbox)
+{
+	struct fw_bye_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	INIT_CMD(c, BYE, WRITE);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_fw_reset - issue a reset to FW
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@reset: specifies the type of reset to perform
+ *
+ * 	Issues a reset command of the specified type to FW.
+ */
+int t4_fw_reset(struct adapter *adap, unsigned int mbox, int reset)
+{
+	struct fw_reset_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	INIT_CMD(c, RESET, WRITE);
+	c.val = htonl(reset);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ *	t4_fw_halt - issue a reset/halt to FW and put uP into RESET
+ *	@adap: the adapter
+ *	@mbox: mailbox to use for the FW RESET command (if desired)
+ *	@force: force uP into RESET even if FW RESET command fails
+ *
+ *	Issues a RESET command to firmware (if desired) with a HALT indication
+ *	and then puts the microprocessor into RESET state.  The RESET command
+ *	will only be issued if a legitimate mailbox is provided (mbox <=
+ *	M_PCIE_FW_MASTER).
+ *
+ *	This is generally used in order for the host to safely manipulate the
+ *	adapter without fear of conflicting with whatever the firmware might
+ *	be doing.  The only way out of this state is to RESTART the firmware
+ *	...
+ */
+int t4_fw_halt(struct adapter *adap, unsigned int mbox, int force)
+{
+	int ret = 0;
+
+	/*
+	 * If a legitimate mailbox is provided, issue a RESET command
+	 * with a HALT indication.
+	 */
+	if (mbox <= M_PCIE_FW_MASTER) {
+		struct fw_reset_cmd c;
+
+		memset(&c, 0, sizeof(c));
+		INIT_CMD(c, RESET, WRITE);
+		c.val = htonl(F_PIORST | F_PIORSTMODE);
+		c.halt_pkd = htonl(F_FW_RESET_CMD_HALT);
+		ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+	}
+
+	/*
+	 * Normally we won't complete the operation if the firmware RESET
+	 * command fails but if our caller insists we'll go ahead and put the
+	 * uP into RESET.  This can be useful if the firmware is hung or even
+	 * missing ...  We'll have to take the risk of putting the uP into
+	 * RESET without the cooperation of firmware in that case.
+	 *
+	 * We also force the firmware's HALT flag to be on in case we bypassed
+	 * the firmware RESET command above or we're dealing with old firmware
+	 * which doesn't have the HALT capability.  This will serve as a flag
+	 * for the incoming firmware to know that it's coming out of a HALT
+	 * rather than a RESET ... if it's new enough to understand that ...
+	 */
+	if (ret == 0 || force) {
+		t4_set_reg_field(adap, A_CIM_BOOT_CFG, F_UPCRST, F_UPCRST);
+		t4_set_reg_field(adap, A_PCIE_FW, F_PCIE_FW_HALT, F_PCIE_FW_HALT);
+	}
+
+	/*
+	 * And we always return the result of the firmware RESET command
+	 * even when we force the uP into RESET ...
+	 */
+	return ret;
+}
+
+/**
+ *	t4_fw_restart - restart the firmware by taking the uP out of RESET
+ *	@adap: the adapter
+ *	@reset: if we want to do a RESET to restart things
+ *
+ *	Restart firmware previously halted by t4_fw_halt().  On successful
+ *	return the previous PF Master remains as the new PF Master and there
+ *	is no need to issue a new HELLO command, etc.
+ *
+ *	We do this in two ways:
+ *
+ *	 1. If we're dealing with newer firmware we'll simply want to take
+ *	    the chip's microprocessor out of RESET.  This will cause the
+ *	    firmware to start up from its start vector.  And then we'll loop
+ *	    until the firmware indicates it's started again (PCIE_FW.HALT
+ *	    reset to 0) or we timeout.
+ *
+ *	 2. If we're dealing with older firmware then we'll need to RESET
+ *	    the chip since older firmware won't recognize the PCIE_FW.HALT
+ *	    flag and automatically RESET itself on startup.
+ */
+int t4_fw_restart(struct adapter *adap, unsigned int mbox, int reset)
+{
+	if (reset) {
+		/*
+		 * Since we're directing the RESET instead of the firmware
+		 * doing it automatically, we need to clear the PCIE_FW.HALT
+		 * bit.
+		 */
+		t4_set_reg_field(adap, A_PCIE_FW, F_PCIE_FW_HALT, 0);
+
+		/*
+		 * If we've been given a valid mailbox, first try to get the
+		 * firmware to do the RESET.  If that works, great and we can
+		 * return success.  Otherwise, if we haven't been given a
+		 * valid mailbox or the RESET command failed, fall back to
+		 * hitting the chip with a hammer.
+		 */
+		if (mbox <= M_PCIE_FW_MASTER) {
+			t4_set_reg_field(adap, A_CIM_BOOT_CFG, F_UPCRST, 0);
+			msleep(100);
+			if (t4_fw_reset(adap, mbox,
+					F_PIORST | F_PIORSTMODE) == 0)
+				return 0;
+		}
+
+		t4_write_reg(adap, A_PL_RST, F_PIORST | F_PIORSTMODE);
+		msleep(2000);
+	} else {
+		int ms;
+
+		t4_set_reg_field(adap, A_CIM_BOOT_CFG, F_UPCRST, 0);
+		for (ms = 0; ms < FW_CMD_MAX_TIMEOUT; ) {
+			if (!(t4_read_reg(adap, A_PCIE_FW) & F_PCIE_FW_HALT))
+				return FW_SUCCESS;
+			msleep(100);
+			ms += 100;
+		}
+		return -ETIMEDOUT;
+	}
+	return 0;
+}
+
+/**
+ *	t4_fw_upgrade - perform all of the steps necessary to upgrade FW
+ *	@adap: the adapter
+ *	@mbox: mailbox to use for the FW RESET command (if desired)
+ *	@fw_data: the firmware image to write
+ *	@size: image size
+ *	@force: force upgrade even if firmware doesn't cooperate
+ *
+ *	Perform all of the steps necessary for upgrading an adapter's
+ *	firmware image.  Normally this requires the cooperation of the
+ *	existing firmware in order to halt all existing activities
+ *	but if an invalid mailbox token is passed in we skip that step
+ *	(though we'll still put the adapter microprocessor into RESET in
+ *	that case).
+ *
+ *	On successful return the new firmware will have been loaded and
+ *	the adapter will have been fully RESET losing all previous setup
+ *	state.  On unsuccessful return the adapter may be completely hosed ...
+ *	positive errno indicates that the adapter is ~probably~ intact, a
+ *	negative errno indicates that things are looking bad ...
+ */
+int t4_fw_upgrade(struct adapter *adap, unsigned int mbox,
+		  const u8 *fw_data, unsigned int size, int force)
+{
+	const struct fw_hdr *fw_hdr = (const struct fw_hdr *)fw_data;
+	int reset, ret;
+
+	ret = t4_fw_halt(adap, mbox, force);
+	if (ret < 0 && !force)
+		return ret;
+
+	ret = t4_load_fw(adap, fw_data, size);
+	if (ret < 0)
+		return ret;
+
+	/*
+	 * Older versions of the firmware don't understand the new
+	 * PCIE_FW.HALT flag and so won't know to perform a RESET when they
+	 * restart.  So for newly loaded older firmware we'll have to do the
+	 * RESET for it so it starts up on a clean slate.  We can tell if
+	 * the newly loaded firmware will handle this right by checking
+	 * its header flags to see if it advertises the capability.
+	 */
+	reset = ((ntohl(fw_hdr->flags) & FW_HDR_FLAGS_RESET_HALT) == 0);
+	return t4_fw_restart(adap, mbox, reset);
+}
+
+/**
+ *	t4_fw_config_file - setup an adapter via a Configuration File
+ *	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ *	@mtype: the memory type where the Configuration File is located
+ *	@maddr: the memory address where the Configuration File is located
+ *	@finiver: return value for CF [fini] version
+ *	@finicsum: return value for CF [fini] checksum
+ *	@cfcsum: return value for CF computed checksum
+ *
+ *	Issue a command to get the firmware to process the Configuration
+ *	File located at the specified mtype/maddress.  If the Configuration
+ *	File is processed successfully and return value pointers are
+ *	provided, the Configuration File "[fini] section version and
+ *	checksum values will be returned along with the computed checksum.
+ *	It's up to the caller to decide how it wants to respond to the
+ *	checksums not matching but it recommended that a prominant warning
+ *	be emitted in order to help people rapidly identify changed or
+ *	corrupted Configuration Files.
+ *
+ *	Also note that it's possible to modify things like "niccaps",
+ *	"toecaps",etc. between processing the Configuration File and telling
+ *	the firmware to use the new configuration.  Callers which want to
+ *	do this will need to "hand-roll" their own CAPS_CONFIGS commands for
+ *	Configuration Files if they want to do this.
+ */
+int t4_fw_config_file(struct adapter *adap, unsigned int mbox,
+		      unsigned int mtype, unsigned int maddr,
+		      u32 *finiver, u32 *finicsum, u32 *cfcsum)
+{
+	struct fw_caps_config_cmd caps_cmd;
+	int ret;
+
+	/*
+	 * Tell the firmware to process the indicated Configuration File.
+	 * If there are no errors and the caller has provided return value
+	 * pointers for the [fini] section version, checksum and computed
+	 * checksum, pass those back to the caller.
+	 */
+	memset(&caps_cmd, 0, sizeof(caps_cmd));
+	caps_cmd.op_to_write =
+		htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+		      F_FW_CMD_REQUEST |
+		      F_FW_CMD_READ);
+	caps_cmd.cfvalid_to_len16 =
+		htonl(F_FW_CAPS_CONFIG_CMD_CFVALID |
+		      V_FW_CAPS_CONFIG_CMD_MEMTYPE_CF(mtype) |
+		      V_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF(maddr >> 16) |
+		      FW_LEN16(caps_cmd));
+	ret = t4_wr_mbox(adap, mbox, &caps_cmd, sizeof(caps_cmd), &caps_cmd);
+	if (ret < 0)
+		return ret;
+
+	if (finiver)
+		*finiver = ntohl(caps_cmd.finiver);
+	if (finicsum)
+		*finicsum = ntohl(caps_cmd.finicsum);
+	if (cfcsum)
+		*cfcsum = ntohl(caps_cmd.cfcsum);
+
+	/*
+	 * And now tell the firmware to use the configuration we just loaded.
+	 */
+	caps_cmd.op_to_write =
+		htonl(V_FW_CMD_OP(FW_CAPS_CONFIG_CMD) |
+		      F_FW_CMD_REQUEST |
+		      F_FW_CMD_WRITE);
+	caps_cmd.cfvalid_to_len16 = htonl(FW_LEN16(caps_cmd));
+	return t4_wr_mbox(adap, mbox, &caps_cmd, sizeof(caps_cmd), NULL);
+}
+
+/**
+ *	t4_fixup_host_params - fix up host-dependent parameters
+ *	@adap: the adapter
+ *	@page_size: the host's Base Page Size
+ *	@cache_line_size: the host's Cache Line Size
+ *
+ *	Various registers in T4 contain values which are dependent on the
+ *	host's Base Page and Cache Line Sizes.  This function will fix all of
+ *	those registers with the appropriate values as passed in ...
+ */
+int t4_fixup_host_params(struct adapter *adap, unsigned int page_size,
+			 unsigned int cache_line_size)
+{
+	unsigned int page_shift = fls(page_size) - 1;
+	unsigned int sge_hps = page_shift - 10;
+	unsigned int stat_len = cache_line_size > 64 ? 128 : 64;
+	unsigned int fl_align = cache_line_size < 32 ? 32 : cache_line_size;
+	unsigned int fl_align_log = fls(fl_align) - 1;
+
+	t4_write_reg(adap, A_SGE_HOST_PAGE_SIZE,
+		     V_HOSTPAGESIZEPF0(sge_hps) |
+		     V_HOSTPAGESIZEPF1(sge_hps) |
+		     V_HOSTPAGESIZEPF2(sge_hps) |
+		     V_HOSTPAGESIZEPF3(sge_hps) |
+		     V_HOSTPAGESIZEPF4(sge_hps) |
+		     V_HOSTPAGESIZEPF5(sge_hps) |
+		     V_HOSTPAGESIZEPF6(sge_hps) |
+		     V_HOSTPAGESIZEPF7(sge_hps));
+
+	t4_set_reg_field(adap, A_SGE_CONTROL,
+			 V_INGPADBOUNDARY(M_INGPADBOUNDARY) |
+			 F_EGRSTATUSPAGESIZE,
+			 V_INGPADBOUNDARY(fl_align_log - 5) |
+			 V_EGRSTATUSPAGESIZE(stat_len != 64));
+
+	/*
+	 * Adjust various SGE Free List Host Buffer Sizes.
+	 *
+	 * This is something of a crock since we're using fixed indices into
+	 * the array which are also known by the sge.c code and the T4
+	 * Firmware Configuration File.  We need to come up with a much better
+	 * approach to managing this array.  For now, the first four entries
+	 * are:
+	 *
+	 *   0: Host Page Size
+	 *   1: 64KB
+	 *   2: Buffer size corresponding to 1500 byte MTU (unpacked mode)
+	 *   3: Buffer size corresponding to 9000 byte MTU (unpacked mode)
+	 *
+	 * For the single-MTU buffers in unpacked mode we need to include
+	 * space for the SGE Control Packet Shift, 14 byte Ethernet header,
+	 * possible 4 byte VLAN tag, all rounded up to the next Ingress Packet
+	 * Padding boundry.  All of these are accommodated in the Factory
+	 * Default Firmware Configuration File but we need to adjust it for
+	 * this host's cache line size.
+	 */
+	t4_write_reg(adap, A_SGE_FL_BUFFER_SIZE0, page_size);
+	t4_write_reg(adap, A_SGE_FL_BUFFER_SIZE2,
+		     (t4_read_reg(adap, A_SGE_FL_BUFFER_SIZE2) + fl_align-1)
+		     & ~(fl_align-1));
+	t4_write_reg(adap, A_SGE_FL_BUFFER_SIZE3,
+		     (t4_read_reg(adap, A_SGE_FL_BUFFER_SIZE3) + fl_align-1)
+		     & ~(fl_align-1));
+
+	t4_write_reg(adap, A_ULP_RX_TDDP_PSZ, V_HPZ0(page_shift - 12));
+
+	return 0;
+}
+
+/**
+ * 	t4_fw_initialize - ask FW to initialize the device
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ *
+ * 	Issues a command to FW to partially initialize the device.  This
+ * 	performs initialization that generally doesn't depend on user input.
+ */
+int t4_fw_initialize(struct adapter *adap, unsigned int mbox)
+{
+	struct fw_initialize_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	INIT_CMD(c, INITIALIZE, WRITE);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_query_params - query FW or device parameters
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ *      @pf: the PF
+ *      @vf: the VF
+ * 	@nparams: the number of parameters
+ * 	@params: the parameter names
+ * 	@val: the parameter values
+ *
+ * 	Reads the value of FW or device parameters.  Up to 7 parameters can be
+ * 	queried at once.
+ */
+int t4_query_params(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int nparams, const u32 *params,
+		    u32 *val)
+{
+	int i, ret;
+	struct fw_params_cmd c;
+	__be32 *p = &c.param[0].mnem;
+
+	if (nparams > 7)
+		return -EINVAL;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_PARAMS_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_READ | V_FW_PARAMS_CMD_PFN(pf) |
+			    V_FW_PARAMS_CMD_VFN(vf));
+	c.retval_len16 = htonl(FW_LEN16(c));
+
+	for (i = 0; i < nparams; i++, p += 2)
+		*p = htonl(*params++);
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret == 0)
+		for (i = 0, p = &c.param[0].val; i < nparams; i++, p += 2)
+			*val++ = ntohl(*p);
+	return ret;
+}
+
+/**
+ * 	t4_set_params - sets FW or device parameters
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ *      @pf: the PF
+ *      @vf: the VF
+ * 	@nparams: the number of parameters
+ * 	@params: the parameter names
+ * 	@val: the parameter values
+ *
+ * 	Sets the value of FW or device parameters.  Up to 7 parameters can be
+ * 	specified at once.
+ */
+int t4_set_params(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		  unsigned int vf, unsigned int nparams, const u32 *params,
+		  const u32 *val)
+{
+	struct fw_params_cmd c;
+	__be32 *p = &c.param[0].mnem;
+
+	if (nparams > 7)
+		return -EINVAL;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_PARAMS_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | V_FW_PARAMS_CMD_PFN(pf) |
+			    V_FW_PARAMS_CMD_VFN(vf));
+        c.retval_len16 = htonl(FW_LEN16(c));
+  
+	while (nparams--) {
+		*p++ = htonl(*params++);
+		*p++ = htonl(*val++);
+	}
+
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_cfg_pfvf - configure PF/VF resource limits
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF being configured
+ * 	@vf: the VF being configured
+ * 	@txq: the max number of egress queues
+ * 	@txq_eth_ctrl: the max number of egress Ethernet or control queues
+ * 	@rxqi: the max number of interrupt-capable ingress queues
+ * 	@rxq: the max number of interruptless ingress queues
+ * 	@tc: the PCI traffic class
+ * 	@vi: the max number of virtual interfaces
+ *	@cmask: the channel access rights mask for the PF/VF
+ *	@pmask: the port access rights mask for the PF/VF
+ *	@nexact: the maximum number of exact MPS filters
+ * 	@rcaps: read capabilities
+ * 	@wxcaps: write/execute capabilities
+ *
+ * 	Configures resource limits and capabilities for a physical or virtual
+ * 	function.
+ */
+int t4_cfg_pfvf(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		unsigned int vf, unsigned int txq, unsigned int txq_eth_ctrl,
+		unsigned int rxqi, unsigned int rxq, unsigned int tc,
+		unsigned int vi, unsigned int cmask, unsigned int pmask,
+		unsigned int nexact, unsigned int rcaps, unsigned int wxcaps)
+{
+	struct fw_pfvf_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_PFVF_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | V_FW_PFVF_CMD_PFN(pf) |
+			    V_FW_PFVF_CMD_VFN(vf));
+	c.retval_len16 = htonl(FW_LEN16(c));
+	c.niqflint_niq = htonl(V_FW_PFVF_CMD_NIQFLINT(rxqi) |
+			       V_FW_PFVF_CMD_NIQ(rxq));
+	c.type_to_neq = htonl(V_FW_PFVF_CMD_CMASK(cmask) |
+			      V_FW_PFVF_CMD_PMASK(pmask) |
+			      V_FW_PFVF_CMD_NEQ(txq));
+	c.tc_to_nexactf = htonl(V_FW_PFVF_CMD_TC(tc) | V_FW_PFVF_CMD_NVI(vi) |
+				V_FW_PFVF_CMD_NEXACTF(nexact));
+	c.r_caps_to_nethctrl = htonl(V_FW_PFVF_CMD_R_CAPS(rcaps) |
+				     V_FW_PFVF_CMD_WX_CAPS(wxcaps) |
+				     V_FW_PFVF_CMD_NETHCTRL(txq_eth_ctrl));
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_alloc_vi_func - allocate a virtual interface
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@port: physical port associated with the VI
+ * 	@pf: the PF owning the VI
+ * 	@vf: the VF owning the VI
+ * 	@nmac: number of MAC addresses needed (1 to 5)
+ * 	@mac: the MAC addresses of the VI
+ *	@rss_size: size of RSS table slice associated with this VI
+ *	@portfunc: which Port Application Function MAC Address is desired
+ *	@idstype: Intrusion Detection Type
+ *
+ * 	Allocates a virtual interface for the given physical port.  If @mac is
+ * 	not %NULL it contains the MAC addresses of the VI as assigned by FW.
+ * 	@mac should be large enough to hold @nmac Ethernet addresses, they are
+ * 	stored consecutively so the space needed is @nmac * 6 bytes.
+ * 	Returns a negative error number or the non-negative VI id.
+ */
+int t4_alloc_vi_func(struct adapter *adap, unsigned int mbox,
+		     unsigned int port, unsigned int pf, unsigned int vf,
+		     unsigned int nmac, u8 *mac, unsigned int *rss_size,
+		     unsigned int portfunc, unsigned int idstype)
+{
+	int ret;
+	struct fw_vi_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_VI_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_WRITE | F_FW_CMD_EXEC |
+			    V_FW_VI_CMD_PFN(pf) | V_FW_VI_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_VI_CMD_ALLOC | FW_LEN16(c));
+	c.type_to_viid = htons(V_FW_VI_CMD_TYPE(idstype) |
+			       V_FW_VI_CMD_FUNC(portfunc));
+	c.portid_pkd = V_FW_VI_CMD_PORTID(port);
+	c.nmac = nmac - 1;
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret)
+		return ret;
+
+       	if (mac) {
+		memcpy(mac, c.mac, sizeof(c.mac));
+		switch (nmac) {
+			case 5:
+				memcpy(mac + 24, c.nmac3, sizeof(c.nmac3));
+			case 4:
+				memcpy(mac + 18, c.nmac2, sizeof(c.nmac2));
+			case 3:
+				memcpy(mac + 12, c.nmac1, sizeof(c.nmac1));
+			case 2:
+				memcpy(mac + 6,  c.nmac0, sizeof(c.nmac0));
+		}
+	}
+	if (rss_size)
+		*rss_size = G_FW_VI_CMD_RSSSIZE(ntohs(c.rsssize_pkd));
+	return G_FW_VI_CMD_VIID(htons(c.type_to_viid));
+}
+
+/**
+ *      t4_alloc_vi_func - allocate an [Ethernet Function] virtual interface
+ *      @adap: the adapter
+ *      @mbox: mailbox to use for the FW command
+ *      @port: physical port associated with the VI
+ *      @pf: the PF owning the VI
+ *      @vf: the VF owning the VI
+ *      @nmac: number of MAC addresses needed (1 to 5)
+ *      @mac: the MAC addresses of the VI
+ *      @rss_size: size of RSS table slice associated with this VI
+ *
+ *	backwards compatible and convieniance routine to allocate a Virtual
+ *	Interface with a Ethernet Port Application Function and Intrustion
+ *	Detection System disabled.
+ */
+int t4_alloc_vi(struct adapter *adap, unsigned int mbox, unsigned int port,
+		unsigned int pf, unsigned int vf, unsigned int nmac, u8 *mac,
+		unsigned int *rss_size)
+{
+	return t4_alloc_vi_func(adap, mbox, port, pf, vf, nmac, mac, rss_size,
+				FW_VI_FUNC_ETH, 0);
+}
+
+
+/**
+ * 	t4_free_vi - free a virtual interface
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF owning the VI
+ * 	@vf: the VF owning the VI
+ * 	@viid: virtual interface identifiler
+ *
+ * 	Free a previously allocated virtual interface.
+ */
+int t4_free_vi(struct adapter *adap, unsigned int mbox, unsigned int pf,
+	       unsigned int vf, unsigned int viid)
+{
+	struct fw_vi_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_VI_CMD) |
+			    F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC |
+			    V_FW_VI_CMD_PFN(pf) |
+			    V_FW_VI_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_VI_CMD_FREE | FW_LEN16(c));
+	c.type_to_viid = htons(V_FW_VI_CMD_VIID(viid));
+
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+}
+
+/**
+ * 	t4_set_rxmode - set Rx properties of a virtual interface
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@mtu: the new MTU or -1
+ * 	@promisc: 1 to enable promiscuous mode, 0 to disable it, -1 no change
+ * 	@all_multi: 1 to enable all-multi mode, 0 to disable it, -1 no change
+ * 	@bcast: 1 to enable broadcast Rx, 0 to disable it, -1 no change
+ *	@vlanex: 1 to enable hardware VLAN Tag extraction, 0 to disable it,
+ *		-1 no change
+ *	@sleep_ok: if true we may sleep while awaiting command completion
+ *
+ * 	Sets Rx properties of a virtual interface.
+ */
+int t4_set_rxmode(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		  int mtu, int promisc, int all_multi, int bcast, int vlanex,
+		  bool sleep_ok)
+{
+	struct fw_vi_rxmode_cmd c;
+
+	/* convert to FW values */
+	if (mtu < 0)
+		mtu = M_FW_VI_RXMODE_CMD_MTU;
+	if (promisc < 0)
+		promisc = M_FW_VI_RXMODE_CMD_PROMISCEN;
+	if (all_multi < 0)
+		all_multi = M_FW_VI_RXMODE_CMD_ALLMULTIEN;
+	if (bcast < 0)
+		bcast = M_FW_VI_RXMODE_CMD_BROADCASTEN;
+	if (vlanex < 0)
+		vlanex = M_FW_VI_RXMODE_CMD_VLANEXEN;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_RXMODE_CMD) | F_FW_CMD_REQUEST |
+			     F_FW_CMD_WRITE | V_FW_VI_RXMODE_CMD_VIID(viid));
+	c.retval_len16 = htonl(FW_LEN16(c));
+	c.mtu_to_vlanexen = htonl(V_FW_VI_RXMODE_CMD_MTU(mtu) |
+				  V_FW_VI_RXMODE_CMD_PROMISCEN(promisc) |
+				  V_FW_VI_RXMODE_CMD_ALLMULTIEN(all_multi) |
+				  V_FW_VI_RXMODE_CMD_BROADCASTEN(bcast) |
+				  V_FW_VI_RXMODE_CMD_VLANEXEN(vlanex));
+	return t4_wr_mbox_meat(adap, mbox, &c, sizeof(c), NULL, sleep_ok);
+}
+
+/**
+ * 	t4_alloc_mac_filt - allocates exact-match filters for MAC addresses
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@free: if true any existing filters for this VI id are first removed
+ * 	@naddr: the number of MAC addresses to allocate filters
+ * 	@addr: the MAC address(es)
+ * 	@idx: where to store the index of each allocated filter
+ * 	@hash: pointer to hash address filter bitmap
+ * 	@sleep_ok: call is allowed to sleep
+ *
+ * 	Allocates an exact-match filter for each of the supplied addresses and
+ * 	sets it to the corresponding address.  If @idx is not %NULL it should
+ * 	have at least @naddr entries, each of which will be set to the index of
+ * 	the filter allocated for the corresponding MAC address.  If a filter
+ * 	could not be allocated for an address its index is set to 0xffff.
+ * 	If @hash is not %NULL addresses that fail to allocate an exact filter
+ * 	are hashed and update the hash filter bitmap pointed at by @hash.
+ *
+ * 	Returns a negative error number or the number of filters allocated.
+ */
+int t4_alloc_mac_filt(struct adapter *adap, unsigned int mbox,
+		      unsigned int viid, bool free, unsigned int naddr,
+		      const u8 **addr, u16 *idx, u64 *hash, bool sleep_ok)
+{
+	int offset, ret = 0;
+	struct fw_vi_mac_cmd c;
+	unsigned int nfilters = 0;
+	unsigned int rem = naddr;
+
+	if (naddr > NUM_MPS_CLS_SRAM_L_INSTANCES)
+		return -EINVAL;
+
+	for (offset = 0; offset < naddr ; /**/) {
+		unsigned int fw_naddr = (rem < ARRAY_SIZE(c.u.exact)
+					 ? rem
+					 : ARRAY_SIZE(c.u.exact));
+		size_t len16 = DIV_ROUND_UP(offsetof(struct fw_vi_mac_cmd,
+						     u.exact[fw_naddr]), 16);
+		struct fw_vi_mac_exact *p;
+		int i;
+
+		memset(&c, 0, sizeof(c));
+		c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_MAC_CMD) |
+				     F_FW_CMD_REQUEST |
+				     F_FW_CMD_WRITE |
+				     V_FW_CMD_EXEC(free) |
+				     V_FW_VI_MAC_CMD_VIID(viid));
+		c.freemacs_to_len16 = htonl(V_FW_VI_MAC_CMD_FREEMACS(free) |
+					    V_FW_CMD_LEN16(len16));
+
+		for (i = 0, p = c.u.exact; i < fw_naddr; i++, p++) {
+			p->valid_to_idx = htons(
+				F_FW_VI_MAC_CMD_VALID |
+				V_FW_VI_MAC_CMD_IDX(FW_VI_MAC_ADD_MAC));
+			memcpy(p->macaddr, addr[offset+i], sizeof(p->macaddr));
+		}
+
+		/*
+		 * It's okay if we run out of space in our MAC address arena.
+		 * Some of the addresses we submit may get stored so we need
+		 * to run through the reply to see what the results were ...
+		 */
+		ret = t4_wr_mbox_meat(adap, mbox, &c, sizeof(c), &c, sleep_ok);
+		if (ret && ret != -FW_ENOMEM)
+			break;
+
+		for (i = 0, p = c.u.exact; i < fw_naddr; i++, p++) {
+			u16 index = G_FW_VI_MAC_CMD_IDX(ntohs(p->valid_to_idx));
+
+			if (idx)
+				idx[offset+i] = (index >= NUM_MPS_CLS_SRAM_L_INSTANCES
+						 ? 0xffff
+						 : index);
+			if (index < NUM_MPS_CLS_SRAM_L_INSTANCES)
+				nfilters++;
+			else if (hash)
+				*hash |= (1ULL << hash_mac_addr(addr[offset+i]));
+		}
+
+		free = false;
+		offset += fw_naddr;
+		rem -= fw_naddr;
+	}
+
+	if (ret == 0 || ret == -FW_ENOMEM)
+		ret = nfilters; 
+	return ret;
+}
+
+/**
+ * 	t4_change_mac - modifies the exact-match filter for a MAC address
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@idx: index of existing filter for old value of MAC address, or -1
+ * 	@addr: the new MAC address value
+ *	@persist: whether a new MAC allocation should be persistent
+ *	@add_smt: if true also add the address to the HW SMT
+ *
+ *	Modifies an exact-match filter and sets it to the new MAC address if
+ *	@idx >= 0, or adds the MAC address to a new filter if @idx < 0.  In the
+ *	latter case the address is added persistently if @persist is %true.
+ *
+ *	Note that in general it is not possible to modify the value of a given
+ *	filter so the generic way to modify an address filter is to free the one
+ *	being used by the old address value and allocate a new filter for the
+ *	new address value.
+ *
+ * 	Returns a negative error number or the index of the filter with the new
+ * 	MAC value.  Note that this index may differ from @idx.
+ */
+int t4_change_mac(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		  int idx, const u8 *addr, bool persist, bool add_smt)
+{
+	int ret, mode;
+	struct fw_vi_mac_cmd c;
+	struct fw_vi_mac_exact *p = c.u.exact;
+
+	if (idx < 0)                             /* new allocation */
+		idx = persist ? FW_VI_MAC_ADD_PERSIST_MAC : FW_VI_MAC_ADD_MAC;
+	mode = add_smt ? FW_VI_MAC_SMT_AND_MPSTCAM : FW_VI_MAC_MPS_TCAM_ENTRY;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_MAC_CMD) | F_FW_CMD_REQUEST |
+			     F_FW_CMD_WRITE | V_FW_VI_MAC_CMD_VIID(viid));
+	c.freemacs_to_len16 = htonl(V_FW_CMD_LEN16(1));
+	p->valid_to_idx = htons(F_FW_VI_MAC_CMD_VALID |
+				V_FW_VI_MAC_CMD_SMAC_RESULT(mode) |
+				V_FW_VI_MAC_CMD_IDX(idx));
+	memcpy(p->macaddr, addr, sizeof(p->macaddr));
+
+	ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+	if (ret == 0) {
+		ret = G_FW_VI_MAC_CMD_IDX(ntohs(p->valid_to_idx));
+		if (ret >= NUM_MPS_CLS_SRAM_L_INSTANCES)
+			ret = -ENOMEM;
+	}
+	return ret;
+}
+
+/**
+ * 	t4_set_addr_hash - program the MAC inexact-match hash filter
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@ucast: whether the hash filter should also match unicast addresses
+ * 	@vec: the value to be written to the hash filter
+ * 	@sleep_ok: call is allowed to sleep
+ *
+ * 	Sets the 64-bit inexact-match hash filter for a virtual interface.
+ */
+int t4_set_addr_hash(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		     bool ucast, u64 vec, bool sleep_ok)
+{
+	struct fw_vi_mac_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_MAC_CMD) | F_FW_CMD_REQUEST |
+			     F_FW_CMD_WRITE | V_FW_VI_ENABLE_CMD_VIID(viid));
+	c.freemacs_to_len16 = htonl(F_FW_VI_MAC_CMD_HASHVECEN |
+				    V_FW_VI_MAC_CMD_HASHUNIEN(ucast) |
+				    V_FW_CMD_LEN16(1));
+	c.u.hash.hashvec = cpu_to_be64(vec);
+	return t4_wr_mbox_meat(adap, mbox, &c, sizeof(c), NULL, sleep_ok);
+}
+
+/**
+ * 	t4_enable_vi - enable/disable a virtual interface
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@rx_en: 1=enable Rx, 0=disable Rx
+ * 	@tx_en: 1=enable Tx, 0=disable Tx
+ *
+ * 	Enables/disables a virtual interface.
+ */
+int t4_enable_vi(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		 bool rx_en, bool tx_en)
+{
+	struct fw_vi_enable_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_ENABLE_CMD) | F_FW_CMD_REQUEST |
+			     F_FW_CMD_EXEC | V_FW_VI_ENABLE_CMD_VIID(viid));
+	c.ien_to_len16 = htonl(V_FW_VI_ENABLE_CMD_IEN(rx_en) |
+			       V_FW_VI_ENABLE_CMD_EEN(tx_en) | FW_LEN16(c));
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_identify_port - identify a VI's port by blinking its LED
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@viid: the VI id
+ * 	@nblinks: how many times to blink LED at 2.5 Hz
+ *
+ * 	Identifies a VI's port by blinking its LED.
+ */
+int t4_identify_port(struct adapter *adap, unsigned int mbox, unsigned int viid,
+		     unsigned int nblinks)
+{
+	struct fw_vi_enable_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_viid = htonl(V_FW_CMD_OP(FW_VI_ENABLE_CMD) | F_FW_CMD_REQUEST |
+			     F_FW_CMD_EXEC | V_FW_VI_ENABLE_CMD_VIID(viid));
+	c.ien_to_len16 = htonl(F_FW_VI_ENABLE_CMD_LED | FW_LEN16(c));
+	c.blinkdur = htons(nblinks);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_iq_start_stop - enable/disable an ingress queue and its FLs
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@start: %true to enable the queues, %false to disable them
+ * 	@pf: the PF owning the queues
+ * 	@vf: the VF owning the queues
+ * 	@iqid: ingress queue id
+ * 	@fl0id: FL0 queue id or 0xffff if no attached FL0
+ * 	@fl1id: FL1 queue id or 0xffff if no attached FL1
+ *
+ * 	Starts or stops an ingress queue and its associated FLs, if any.
+ */
+int t4_iq_start_stop(struct adapter *adap, unsigned int mbox, bool start,
+		     unsigned int pf, unsigned int vf, unsigned int iqid,
+		     unsigned int fl0id, unsigned int fl1id)
+{
+	struct fw_iq_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_IQ_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC | V_FW_IQ_CMD_PFN(pf) |
+			    V_FW_IQ_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(V_FW_IQ_CMD_IQSTART(start) |
+				 V_FW_IQ_CMD_IQSTOP(!start) | FW_LEN16(c));
+	c.iqid = htons(iqid);
+	c.fl0id = htons(fl0id);
+	c.fl1id = htons(fl1id);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_iq_free - free an ingress queue and its FLs
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF owning the queues
+ * 	@vf: the VF owning the queues
+ *	@iqtype: the ingress queue type (FW_IQ_TYPE_FL_INT_CAP, etc.)
+ * 	@iqid: ingress queue id
+ * 	@fl0id: FL0 queue id or 0xffff if no attached FL0
+ * 	@fl1id: FL1 queue id or 0xffff if no attached FL1
+ *
+ * 	Frees an ingress queue and its associated FLs, if any.
+ */
+int t4_iq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+	       unsigned int vf, unsigned int iqtype, unsigned int iqid,
+	       unsigned int fl0id, unsigned int fl1id)
+{
+	struct fw_iq_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_IQ_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC | V_FW_IQ_CMD_PFN(pf) |
+			    V_FW_IQ_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_IQ_CMD_FREE | FW_LEN16(c));
+	c.type_to_iqandstindex = htonl(V_FW_IQ_CMD_TYPE(iqtype));
+	c.iqid = htons(iqid);
+	c.fl0id = htons(fl0id);
+	c.fl1id = htons(fl1id);
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_eth_eq_free - free an Ethernet egress queue
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF owning the queue
+ * 	@vf: the VF owning the queue
+ * 	@eqid: egress queue id
+ *
+ * 	Frees an Ethernet egress queue.
+ */
+int t4_eth_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		   unsigned int vf, unsigned int eqid)
+{
+	struct fw_eq_eth_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_ETH_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC | V_FW_EQ_ETH_CMD_PFN(pf) |
+			    V_FW_EQ_ETH_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_EQ_ETH_CMD_FREE | FW_LEN16(c));
+	c.eqid_pkd = htonl(V_FW_EQ_ETH_CMD_EQID(eqid));
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_ctrl_eq_free - free a control egress queue
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF owning the queue
+ * 	@vf: the VF owning the queue
+ * 	@eqid: egress queue id
+ *
+ * 	Frees a control egress queue.
+ */
+int t4_ctrl_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int eqid)
+{
+	struct fw_eq_ctrl_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_CTRL_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC | V_FW_EQ_CTRL_CMD_PFN(pf) |
+			    V_FW_EQ_CTRL_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_EQ_CTRL_CMD_FREE | FW_LEN16(c));
+	c.cmpliqid_eqid = htonl(V_FW_EQ_CTRL_CMD_EQID(eqid));
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_ofld_eq_free - free an offload egress queue
+ * 	@adap: the adapter
+ * 	@mbox: mailbox to use for the FW command
+ * 	@pf: the PF owning the queue
+ * 	@vf: the VF owning the queue
+ * 	@eqid: egress queue id
+ *
+ * 	Frees a control egress queue.
+ */
+int t4_ofld_eq_free(struct adapter *adap, unsigned int mbox, unsigned int pf,
+		    unsigned int vf, unsigned int eqid)
+{
+	struct fw_eq_ofld_cmd c;
+
+	memset(&c, 0, sizeof(c));
+	c.op_to_vfn = htonl(V_FW_CMD_OP(FW_EQ_OFLD_CMD) | F_FW_CMD_REQUEST |
+			    F_FW_CMD_EXEC | V_FW_EQ_OFLD_CMD_PFN(pf) |
+			    V_FW_EQ_OFLD_CMD_VFN(vf));
+	c.alloc_to_len16 = htonl(F_FW_EQ_OFLD_CMD_FREE | FW_LEN16(c));
+	c.eqid_pkd = htonl(V_FW_EQ_OFLD_CMD_EQID(eqid));
+	return t4_wr_mbox(adap, mbox, &c, sizeof(c), NULL);
+}
+
+/**
+ * 	t4_handle_fw_rpl - process a FW reply message
+ * 	@adap: the adapter
+ * 	@rpl: start of the FW message
+ *
+ * 	Processes a FW message, such as link state change messages.
+ */
+int t4_handle_fw_rpl(struct adapter *adap, const __be64 *rpl)
+{
+	u8 opcode = *(const u8 *)rpl;
+
+	/*
+	 * This might be a port command ... this simplifies the following
+	 * conditionals ...  We can get away with pre-dereferencing
+	 * action_to_len16 because it's in the first 16 bytes and all messages
+	 * will be at least that long.
+	 */
+	const struct fw_port_cmd *p = (const void *)rpl;
+	unsigned int action = G_FW_PORT_CMD_ACTION(ntohl(p->action_to_len16));
+
+	if (opcode == FW_PORT_CMD && action == FW_PORT_ACTION_GET_PORT_INFO) {
+		/* link/module state change message */
+		int speed = 0, fc = 0, i;
+		int chan = G_FW_PORT_CMD_PORTID(ntohl(p->op_to_portid));
+		struct port_info *pi = NULL;
+		struct link_config *lc;
+		u32 stat = ntohl(p->u.info.lstatus_to_modtype);
+		int link_ok = (stat & F_FW_PORT_CMD_LSTATUS) != 0;
+		u32 mod = G_FW_PORT_CMD_MODTYPE(stat);
+
+		if (stat & F_FW_PORT_CMD_RXPAUSE)
+			fc |= PAUSE_RX;
+		if (stat & F_FW_PORT_CMD_TXPAUSE)
+			fc |= PAUSE_TX;
+		if (stat & V_FW_PORT_CMD_LSPEED(FW_PORT_CAP_SPEED_100M))
+			speed = SPEED_100;
+		else if (stat & V_FW_PORT_CMD_LSPEED(FW_PORT_CAP_SPEED_1G))
+			speed = SPEED_1000;
+		else if (stat & V_FW_PORT_CMD_LSPEED(FW_PORT_CAP_SPEED_10G))
+			speed = SPEED_10000;
+
+		for_each_port(adap, i) {
+			pi = adap2pinfo(adap, i);
+			if (pi->tx_chan == chan)
+				break;
+		}
+		lc = &pi->link_cfg;
+
+		if (link_ok != lc->link_ok || speed != lc->speed ||
+		    fc != lc->fc) {                    /* something changed */
+			lc->link_ok = link_ok;
+			lc->speed = speed;
+			lc->fc = fc;
+			t4_os_link_changed(adap, i, link_ok);
+		}
+		if (mod != pi->mod_type) {
+			pi->mod_type = mod;
+			t4_os_portmod_changed(adap, i);
+		}
+	} else {
+		CH_WARN_RATELIMIT(adap, "Unknown firmware reply %d\n", opcode);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+/**
+ *	get_pci_mode - determine a card's PCI mode
+ *	@adapter: the adapter
+ *	@p: where to store the PCI settings
+ *
+ *	Determines a card's PCI mode and associated parameters, such as speed
+ *	and width.
+ */
+static void __devinit get_pci_mode(struct adapter *adapter,
+				   struct pci_params *p)
+{
+	u16 val;
+	u32 pcie_cap;
+
+	pcie_cap = t4_os_find_pci_capability(adapter, PCI_CAP_ID_EXP);
+	if (pcie_cap) {
+		t4_os_pci_read_cfg2(adapter, pcie_cap + PCI_EXP_LNKSTA, &val);
+		p->speed = val & PCI_EXP_LNKSTA_CLS;
+		p->width = (val & PCI_EXP_LNKSTA_NLW) >> 4;
+	}
+}
+
+/**
+ *	init_link_config - initialize a link's SW state
+ *	@lc: structure holding the link state
+ *	@caps: link capabilities
+ *
+ *	Initializes the SW state maintained for each link, including the link's
+ *	capabilities and default speed/flow-control/autonegotiation settings.
+ */
+void __devinit init_link_config(struct link_config *lc,
+				       unsigned int caps)
+{
+	lc->supported = caps;
+	lc->requested_speed = 0;
+	lc->speed = 0;
+	lc->requested_fc = lc->fc = PAUSE_RX | PAUSE_TX;
+	if (lc->supported & FW_PORT_CAP_ANEG) {
+		lc->advertising = lc->supported & ADVERT_MASK;
+		lc->autoneg = AUTONEG_ENABLE;
+		lc->requested_fc |= PAUSE_AUTONEG;
+	} else {
+		lc->advertising = 0;
+		lc->autoneg = AUTONEG_DISABLE;
+	}
+}
+
+static int __devinit wait_dev_ready(struct adapter *adap)
+{
+	u32 whoami;
+
+	whoami = t4_read_reg(adap, A_PL_WHOAMI);
+
+	if (whoami != 0xffffffff && whoami != X_CIM_PF_NOACCESS)
+		return 0;
+
+	msleep(500);
+	whoami = t4_read_reg(adap, A_PL_WHOAMI);
+	return (whoami != 0xffffffff && whoami != X_CIM_PF_NOACCESS
+		? 0 : -EIO);
+}
+
+static int __devinit get_flash_params(struct adapter *adapter)
+{
+	int ret;
+	u32 info = 0;
+
+	ret = sf1_write(adapter, 1, 1, 0, SF_RD_ID);
+	if (!ret)
+		ret = sf1_read(adapter, 3, 0, 1, &info);
+	t4_write_reg(adapter, A_SF_OP, 0);               /* unlock SF */
+	if (ret < 0)
+		return ret;
+
+	if ((info & 0xff) != 0x20)             /* not a Numonix flash */
+		return -EINVAL;
+	info >>= 16;                           /* log2 of size */
+	if (info >= 0x14 && info < 0x18)
+		adapter->params.sf_nsec = 1 << (info - 16);
+	else if (info == 0x18)
+		adapter->params.sf_nsec = 64;
+	else
+		return -EINVAL;
+	adapter->params.sf_size = 1 << info;
+	return 0;
+}
+
+static void __devinit set_pcie_completion_timeout(struct adapter *adapter,
+						  u8 range)
+{
+	u16 val;
+	u32 pcie_cap;
+
+	pcie_cap = t4_os_find_pci_capability(adapter, PCI_CAP_ID_EXP);
+	if (pcie_cap) {
+		t4_os_pci_read_cfg2(adapter, pcie_cap + PCI_EXP_DEVCTL2, &val);
+		val &= 0xfff0;
+		val |= range ;
+		t4_os_pci_write_cfg2(adapter, pcie_cap + PCI_EXP_DEVCTL2, val);
+	}
+}
+
+/**
+ *	t4_prep_adapter - prepare SW and HW for operation
+ *	@adapter: the adapter
+ *	@reset: if true perform a HW reset
+ *
+ *	Initialize adapter SW state for the various HW modules, set initial
+ *	values for some adapter tunables, take PHYs out of reset, and
+ *	initialize the MDIO interface.
+ */
+int __devinit t4_prep_adapter(struct adapter *adapter, bool reset)
+{
+	int ret;
+
+	ret = wait_dev_ready(adapter);
+	if (ret < 0)
+		return ret;
+
+	get_pci_mode(adapter, &adapter->params.pci);
+
+	adapter->params.rev = t4_read_reg(adapter, A_PL_REV);
+	adapter->params.pci.vpd_cap_addr =
+		t4_os_find_pci_capability(adapter, PCI_CAP_ID_VPD);
+
+	ret = get_flash_params(adapter);
+	if (ret < 0)
+		return ret;
+
+	if (t4_read_reg(adapter, A_PCIE_REVISION) != 0) {
+		/* FPGA */
+		adapter->params.cim_la_size = 2 * CIMLA_SIZE;
+	} else {
+		/* ASIC */
+		adapter->params.cim_la_size = CIMLA_SIZE;
+	}
+
+	init_cong_ctrl(adapter->params.a_wnd, adapter->params.b_wnd);
+
+	/*
+	 * Default port and clock for debugging in case we can't reach FW.
+	 */
+	adapter->params.nports = 1;
+	adapter->params.portvec = 1;
+	adapter->params.vpd.cclk = 50000;
+
+	/* Set pci completion timeout value to 4 seconds. */
+	set_pcie_completion_timeout(adapter, 0xd);
+	return 0;
+}
+
+int __devinit t4_port_init(struct adapter *adap, int mbox, int pf, int vf)
+{
+	u8 addr[6];
+	int ret, i, j = 0;
+	struct fw_port_cmd c;
+
+	memset(&c, 0, sizeof(c));
+
+	for_each_port(adap, i) {
+		unsigned int rss_size;
+		struct port_info *p = adap2pinfo(adap, i);
+
+		while ((adap->params.portvec & (1 << j)) == 0)
+			j++;
+
+		c.op_to_portid = htonl(V_FW_CMD_OP(FW_PORT_CMD) |
+				       F_FW_CMD_REQUEST | F_FW_CMD_READ |
+				       V_FW_PORT_CMD_PORTID(j));
+		c.action_to_len16 = htonl(
+			V_FW_PORT_CMD_ACTION(FW_PORT_ACTION_GET_PORT_INFO) |
+			FW_LEN16(c));
+		ret = t4_wr_mbox(adap, mbox, &c, sizeof(c), &c);
+		if (ret)
+			return ret;
+
+		ret = t4_alloc_vi(adap, mbox, j, pf, vf, 1, addr, &rss_size);
+		if (ret < 0)
+			return ret;
+
+		p->viid = ret;
+		p->tx_chan = j;
+		p->lport = j;
+		p->rss_size = rss_size;
+		t4_os_set_hw_addr(adap, i, addr);
+
+		ret = ntohl(c.u.info.lstatus_to_modtype);
+		p->mdio_addr = (ret & F_FW_PORT_CMD_MDIOCAP) ?
+			G_FW_PORT_CMD_MDIOADDR(ret) : -1;
+		p->port_type = G_FW_PORT_CMD_PTYPE(ret);
+		p->mod_type = FW_PORT_MOD_TYPE_NA;
+
+		init_link_config(&p->link_cfg, ntohs(c.u.info.pcap));
+		j++;
+	}
+	return 0;
+}
diff --git a/drivers/net/cxgb4/t4_hw.h b/drivers/net/cxgb4/t4_hw.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_hw.h
@@ -0,0 +1,245 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver.
+ *
+ * Copyright (C) 2009-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __T4_HW_H
+#define __T4_HW_H
+
+#include "osdep.h"
+
+enum {
+	NCHAN          = 4,     /* # of HW channels */
+	MAX_MTU        = 9600,  /* max MAC MTU, excluding header + FCS */
+	EEPROMSIZE     = 17408, /* Serial EEPROM physical size */
+	EEPROMVSIZE    = 32768, /* Serial EEPROM virtual address space size */
+	EEPROMPFSIZE   = 1024,  /* EEPROM writable area size for PFn, n>0 */
+	RSS_NENTRIES   = 2048,  /* # of entries in RSS mapping table */
+	TCB_SIZE       = 128,   /* TCB size */
+	NMTUS          = 16,    /* size of MTU table */
+	NCCTRL_WIN     = 32,    /* # of congestion control windows */
+	NTX_SCHED      = 8,     /* # of HW Tx scheduling queues */
+	PM_NSTATS      = 5,     /* # of PM stats */
+	MBOX_LEN       = 64,    /* mailbox size in bytes */
+	TRACE_LEN      = 112,   /* length of trace data and mask */
+	FILTER_OPT_LEN = 36,    /* filter tuple width for optional components */
+	NWOL_PAT       = 8,     /* # of WoL patterns */
+	WOL_PAT_LEN    = 128,   /* length of WoL patterns */
+};
+
+enum {
+	CIM_NUM_IBQ    = 6,     /* # of CIM IBQs */
+	CIM_NUM_OBQ    = 6,     /* # of CIM OBQs */
+	CIMLA_SIZE     = 2048,  /* # of 32-bit words in CIM LA */
+	CIM_PIFLA_SIZE = 64,    /* # of 192-bit words in CIM PIF LA */
+	CIM_MALA_SIZE  = 64,    /* # of 160-bit words in CIM MA LA */
+	CIM_IBQ_SIZE   = 128,   /* # of 128-bit words in a CIM IBQ */
+	TPLA_SIZE      = 128,   /* # of 64-bit words in TP LA */
+	ULPRX_LA_SIZE  = 512,   /* # of 256-bit words in ULP_RX LA */
+};
+
+enum {
+	SF_PAGE_SIZE = 256,           /* serial flash page size */
+	SF_SEC_SIZE = 64 * 1024,      /* serial flash sector size */
+};
+
+/* SGE context types */
+enum ctxt_type { CTXT_EGRESS, CTXT_INGRESS, CTXT_FLM, CTXT_CNM };
+
+enum { RSP_TYPE_FLBUF, RSP_TYPE_CPL, RSP_TYPE_INTR }; /* response entry types */
+
+enum { MBOX_OWNER_NONE, MBOX_OWNER_FW, MBOX_OWNER_DRV };    /* mailbox owners */
+
+enum {
+	SGE_MAX_WR_LEN = 512,     /* max WR size in bytes */
+	SGE_CTXT_SIZE = 24,       /* size of SGE context */
+	SGE_NTIMERS = 6,          /* # of interrupt holdoff timer values */
+	SGE_NCOUNTERS = 4,        /* # of interrupt packet counter values */
+};
+
+struct sge_qstat {                /* data written to SGE queue status entries */
+	__be32 qid;
+	__be16 cidx;
+	__be16 pidx;
+};
+
+#define S_QSTAT_PIDX    0
+#define M_QSTAT_PIDX    0xffff
+#define G_QSTAT_PIDX(x) (((x) >> S_QSTAT_PIDX) & M_QSTAT_PIDX)
+
+#define S_QSTAT_CIDX    16
+#define M_QSTAT_CIDX    0xffff
+#define G_QSTAT_CIDX(x) (((x) >> S_QSTAT_CIDX) & M_QSTAT_CIDX)
+
+/*
+ * Structure for last 128 bits of response descriptors
+ */
+struct rsp_ctrl {
+	__be32 hdrbuflen_pidx;
+	__be32 pldbuflen_qid;
+	union {
+		u8 type_gen;
+		__be64 last_flit;
+	} u;
+};
+
+#define S_RSPD_NEWBUF    31
+#define V_RSPD_NEWBUF(x) ((x) << S_RSPD_NEWBUF)
+#define F_RSPD_NEWBUF    V_RSPD_NEWBUF(1U)
+
+#define S_RSPD_LEN    0
+#define M_RSPD_LEN    0x7fffffff
+#define V_RSPD_LEN(x) ((x) << S_RSPD_LEN)
+#define G_RSPD_LEN(x) (((x) >> S_RSPD_LEN) & M_RSPD_LEN)
+
+#define S_RSPD_QID    S_RSPD_LEN
+#define M_RSPD_QID    M_RSPD_LEN
+#define V_RSPD_QID(x) V_RSPD_LEN(x)
+#define G_RSPD_QID(x) G_RSPD_LEN(x)
+
+#define S_RSPD_GEN    7
+#define V_RSPD_GEN(x) ((x) << S_RSPD_GEN)
+#define F_RSPD_GEN    V_RSPD_GEN(1U)
+
+#define S_RSPD_QOVFL    6
+#define V_RSPD_QOVFL(x) ((x) << S_RSPD_QOVFL)
+#define F_RSPD_QOVFL    V_RSPD_QOVFL(1U)
+
+#define S_RSPD_TYPE    4
+#define M_RSPD_TYPE    0x3
+#define V_RSPD_TYPE(x) ((x) << S_RSPD_TYPE)
+#define G_RSPD_TYPE(x) (((x) >> S_RSPD_TYPE) & M_RSPD_TYPE)
+
+/* Rx queue interrupt deferral fields: counter enable and timer index */
+#define S_QINTR_CNT_EN    0
+#define V_QINTR_CNT_EN(x) ((x) << S_QINTR_CNT_EN)
+#define F_QINTR_CNT_EN    V_QINTR_CNT_EN(1U)
+
+#define S_QINTR_TIMER_IDX    1
+#define M_QINTR_TIMER_IDX    0x7
+#define V_QINTR_TIMER_IDX(x) ((x) << S_QINTR_TIMER_IDX)
+#define G_QINTR_TIMER_IDX(x) (((x) >> S_QINTR_TIMER_IDX) & M_QINTR_TIMER_IDX)
+
+/* # of pages a pagepod can hold without needing another pagepod */
+#define PPOD_PAGES 4U
+
+struct pagepod {
+	__be64 vld_tid_pgsz_tag_color;
+	__be64 len_offset;
+	__be64 rsvd;
+	__be64 addr[PPOD_PAGES + 1];
+};
+
+#define S_PPOD_COLOR    0
+#define M_PPOD_COLOR    0x3F
+#define V_PPOD_COLOR(x) ((x) << S_PPOD_COLOR)
+
+#define S_PPOD_TAG    6
+#define M_PPOD_TAG    0xFFFFFF
+#define V_PPOD_TAG(x) ((x) << S_PPOD_TAG)
+
+#define S_PPOD_PGSZ    30
+#define M_PPOD_PGSZ    0x3
+#define V_PPOD_PGSZ(x) ((x) << S_PPOD_PGSZ)
+
+#define S_PPOD_TID    32
+#define M_PPOD_TID    0xFFFFFF
+#define V_PPOD_TID(x) ((__u64)(x) << S_PPOD_TID)
+
+#define S_PPOD_VALID    56
+#define V_PPOD_VALID(x) ((__u64)(x) << S_PPOD_VALID)
+#define F_PPOD_VALID    V_PPOD_VALID(1ULL)
+
+#define S_PPOD_LEN    32
+#define M_PPOD_LEN    0xFFFFFFFF
+#define V_PPOD_LEN(x) ((__u64)(x) << S_PPOD_LEN)
+
+#define S_PPOD_OFST    0
+#define M_PPOD_OFST    0xFFFFFFFF
+#define V_PPOD_OFST(x) ((x) << S_PPOD_OFST)
+
+/*
+ * Flash layout.
+ */
+#define FLASH_START(start)	((start) * SF_SEC_SIZE)
+#define FLASH_MAX_SIZE(nsecs)	((nsecs) * SF_SEC_SIZE)
+
+enum {
+	/*
+	 * Various Expansion-ROM boot images, etc.
+	 */
+	FLASH_EXP_ROM_START_SEC = 0,
+	FLASH_EXP_ROM_NSECS = 6,
+	FLASH_EXP_ROM_START = FLASH_START(FLASH_EXP_ROM_START_SEC),
+	FLASH_EXP_ROM_MAX_SIZE = FLASH_MAX_SIZE(FLASH_EXP_ROM_NSECS),
+
+	/*
+	 * iSCSI Boot Firmware Table (iBFT) and other driver-related
+	 * parameters ...
+	 */
+	FLASH_IBFT_START_SEC = 6,
+	FLASH_IBFT_NSECS = 1,
+	FLASH_IBFT_START = FLASH_START(FLASH_IBFT_START_SEC),
+	FLASH_IBFT_MAX_SIZE = FLASH_MAX_SIZE(FLASH_IBFT_NSECS),
+
+	/*
+	 * Boot configuration data.
+	 */
+	FLASH_BOOTCFG_START_SEC = 7,
+	FLASH_BOOTCFG_NSECS = 1,
+	FLASH_BOOTCFG_START = FLASH_START(FLASH_BOOTCFG_START_SEC),
+	FLASH_BOOTCFG_MAX_SIZE = FLASH_MAX_SIZE(FLASH_BOOTCFG_NSECS),
+
+	/*
+	 * Location of firmware image in FLASH.
+	 */
+	FLASH_FW_START_SEC = 8,
+	FLASH_FW_NSECS = 8,
+	FLASH_FW_START = FLASH_START(FLASH_FW_START_SEC),
+	FLASH_FW_MAX_SIZE = FLASH_MAX_SIZE(FLASH_FW_NSECS),
+        
+	/*
+	 * iSCSI persistent/crash information.
+	 */
+	FLASH_ISCSI_CRASH_START_SEC = 29,
+	FLASH_ISCSI_CRASH_NSECS = 1,
+	FLASH_ISCSI_CRASH_START = FLASH_START(FLASH_ISCSI_CRASH_START_SEC),
+	FLASH_ISCSI_CRASH_MAX_SIZE = FLASH_MAX_SIZE(FLASH_ISCSI_CRASH_NSECS),
+
+	/*
+	 * FCoE persistent/crash information.
+	 */
+	FLASH_FCOE_CRASH_START_SEC = 30,
+	FLASH_FCOE_CRASH_NSECS = 1,
+	FLASH_FCOE_CRASH_START = FLASH_START(FLASH_FCOE_CRASH_START_SEC),
+	FLASH_FCOE_CRASH_MAX_SIZE = FLASH_MAX_SIZE(FLASH_FCOE_CRASH_NSECS),
+
+	/*
+	 * Location of Firmware Configuration File in FLASH.  Since the FPGA
+	 * "FLASH" is smaller we need to store the Configuration File in a
+	 * different location -- which will overlap the end of the firmware
+	 * image if firmware ever gets that large ...
+	 */
+	FLASH_CFG_START_SEC = 31,
+	FLASH_CFG_NSECS = 1,
+	FLASH_CFG_START = FLASH_START(FLASH_CFG_START_SEC),
+	FLASH_CFG_MAX_SIZE = FLASH_MAX_SIZE(FLASH_CFG_NSECS),
+
+	FLASH_FPGA_CFG_START_SEC = 15,
+	FLASH_FPGA_CFG_START = FLASH_START(FLASH_FPGA_CFG_START_SEC),
+
+	/*
+	 * Sectors 32-63 are reserved for FLASH failover.
+	 */
+};
+
+#undef FLASH_START
+#undef FLASH_MAX_SIZE
+
+#endif /* __T4_HW_H */
diff --git a/drivers/net/cxgb4/t4_msg.h b/drivers/net/cxgb4/t4_msg.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_msg.h
@@ -0,0 +1,2398 @@
+/*
+ * Definitions of T4 work request and CPL5 commands and status codes.
+ *
+ * Copyright (C) 2008-2011 Chelsio Communications.  All rights reserved.
+ *
+ * Written by Dimitris Michailidis (dm@chelsio.com)
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef T4_MSG_H
+#define T4_MSG_H
+
+#if !defined(__LITTLE_ENDIAN_BITFIELD) && !defined(__BIG_ENDIAN_BITFIELD)
+# include <asm/byteorder.h>
+#endif
+
+enum {
+	CPL_PASS_OPEN_REQ     = 0x1,
+	CPL_PASS_ACCEPT_RPL   = 0x2,
+	CPL_ACT_OPEN_REQ      = 0x3,
+	CPL_SET_TCB           = 0x4,
+	CPL_SET_TCB_FIELD     = 0x5,
+	CPL_GET_TCB           = 0x6,
+	CPL_CLOSE_CON_REQ     = 0x8,
+	CPL_CLOSE_LISTSRV_REQ = 0x9,
+	CPL_ABORT_REQ         = 0xA,
+	CPL_ABORT_RPL         = 0xB,
+	CPL_TX_DATA           = 0xC,
+	CPL_RX_DATA_ACK       = 0xD,
+	CPL_TX_PKT            = 0xE,
+	CPL_RTE_DELETE_REQ    = 0xF,
+	CPL_RTE_WRITE_REQ     = 0x10,
+	CPL_RTE_READ_REQ      = 0x11,
+	CPL_L2T_WRITE_REQ     = 0x12,
+	CPL_L2T_READ_REQ      = 0x13,
+	CPL_SMT_WRITE_REQ     = 0x14,
+	CPL_SMT_READ_REQ      = 0x15,
+	CPL_TAG_WRITE_REQ     = 0x16,
+	CPL_BARRIER           = 0x18,
+	CPL_TID_RELEASE       = 0x1A,
+	CPL_TAG_READ_REQ      = 0x1B,
+	CPL_TX_PKT_FSO        = 0x1E,
+	CPL_TX_PKT_ISO        = 0x1F,
+
+	CPL_CLOSE_LISTSRV_RPL = 0x20,
+	CPL_ERROR             = 0x21,
+	CPL_GET_TCB_RPL       = 0x22,
+	CPL_L2T_WRITE_RPL     = 0x23,
+	CPL_PASS_OPEN_RPL     = 0x24,
+	CPL_ACT_OPEN_RPL      = 0x25,
+	CPL_PEER_CLOSE        = 0x26,
+	CPL_RTE_DELETE_RPL    = 0x27,
+	CPL_RTE_WRITE_RPL     = 0x28,
+	CPL_RX_URG_PKT        = 0x29,
+	CPL_TAG_WRITE_RPL     = 0x2A,
+	CPL_ABORT_REQ_RSS     = 0x2B,
+	CPL_RX_URG_NOTIFY     = 0x2C,
+	CPL_ABORT_RPL_RSS     = 0x2D,
+	CPL_SMT_WRITE_RPL     = 0x2E,
+	CPL_TX_DATA_ACK       = 0x2F,
+
+	CPL_RX_PHYS_ADDR      = 0x30,
+	CPL_PCMD_READ_RPL     = 0x31,
+	CPL_CLOSE_CON_RPL     = 0x32,
+	CPL_ISCSI_HDR         = 0x33,
+	CPL_L2T_READ_RPL      = 0x34,
+	CPL_RDMA_CQE          = 0x35,
+	CPL_RDMA_CQE_READ_RSP = 0x36,
+	CPL_RDMA_CQE_ERR      = 0x37,
+	CPL_RTE_READ_RPL      = 0x38,
+	CPL_RX_DATA           = 0x39,
+	CPL_SET_TCB_RPL       = 0x3A,
+	CPL_RX_PKT            = 0x3B,
+	CPL_TAG_READ_RPL      = 0x3C,
+	CPL_HIT_NOTIFY        = 0x3D,
+	CPL_PKT_NOTIFY        = 0x3E,
+	CPL_RX_DDP_COMPLETE   = 0x3F,
+
+	CPL_ACT_ESTABLISH     = 0x40,
+	CPL_PASS_ESTABLISH    = 0x41,
+	CPL_RX_DATA_DDP       = 0x42,
+	CPL_SMT_READ_RPL      = 0x43,
+	CPL_PASS_ACCEPT_REQ   = 0x44,
+	CPL_RX2TX_PKT         = 0x45,
+	CPL_RX_FCOE_DDP       = 0x46,
+	CPL_FCOE_HDR          = 0x47,
+	CPL_T5_TRACE_PKT      = 0x48,
+	CPL_RX_ISCSI_DDP      = 0x49,
+	CPL_RX_FCOE_DIF       = 0x4A,
+	CPL_RX_DATA_DIF       = 0x4B,
+
+	CPL_RDMA_READ_REQ     = 0x60,
+	CPL_RX_ISCSI_DIF      = 0x60,
+
+	CPL_SET_LE_REQ        = 0x80,
+	CPL_PASS_OPEN_REQ6    = 0x81,
+	CPL_ACT_OPEN_REQ6     = 0x83,
+
+	CPL_RDMA_TERMINATE    = 0xA2,
+	CPL_RDMA_WRITE        = 0xA4,
+	CPL_SGE_EGR_UPDATE    = 0xA5,
+	CPL_SET_LE_RPL        = 0xA6,
+	CPL_FW2_MSG           = 0xA7,
+	CPL_FW2_PLD           = 0xA8,
+	CPL_T5_RDMA_READ_REQ  = 0xA9,
+	CPL_RDMA_ATOMIC_REQ   = 0xAA,
+	CPL_RDMA_ATOMIC_RPL   = 0xAB,
+	CPL_RDMA_IMM_DATA     = 0xAC,
+	CPL_RDMA_IMM_DATA_SE  = 0xAD,
+
+	CPL_TRACE_PKT         = 0xB0,
+	CPL_RX2TX_DATA        = 0xB1,
+	CPL_ISCSI_DATA        = 0xB2,
+	CPL_FCOE_DATA         = 0xB3,
+
+	CPL_FW4_MSG           = 0xC0,
+	CPL_FW4_PLD           = 0xC1,
+	CPL_FW4_ACK           = 0xC3,
+
+	CPL_FW6_MSG           = 0xE0,
+	CPL_FW6_PLD           = 0xE1,
+	CPL_TX_PKT_LSO        = 0xED,
+	CPL_TX_PKT_XT         = 0xEE,
+
+	NUM_CPL_CMDS    /* must be last and previous entries must be sorted */
+};
+
+enum CPL_error {
+	CPL_ERR_NONE               = 0,
+	CPL_ERR_TCAM_PARITY        = 1,
+	CPL_ERR_TCAM_FULL          = 3,
+	CPL_ERR_BAD_LENGTH         = 15,
+	CPL_ERR_BAD_ROUTE          = 18,
+	CPL_ERR_CONN_RESET         = 20,
+	CPL_ERR_CONN_EXIST_SYNRECV = 21,
+	CPL_ERR_CONN_EXIST         = 22,
+	CPL_ERR_ARP_MISS           = 23,
+	CPL_ERR_BAD_SYN            = 24,
+	CPL_ERR_CONN_TIMEDOUT      = 30,
+	CPL_ERR_XMIT_TIMEDOUT      = 31,
+	CPL_ERR_PERSIST_TIMEDOUT   = 32,
+	CPL_ERR_FINWAIT2_TIMEDOUT  = 33,
+	CPL_ERR_KEEPALIVE_TIMEDOUT = 34,
+	CPL_ERR_RTX_NEG_ADVICE     = 35,
+	CPL_ERR_PERSIST_NEG_ADVICE = 36,
+	CPL_ERR_ABORT_FAILED       = 42,
+	CPL_ERR_IWARP_FLM          = 50,
+};
+
+enum {
+	CPL_CONN_POLICY_AUTO = 0,
+	CPL_CONN_POLICY_ASK  = 1,
+	CPL_CONN_POLICY_FILTER = 2,
+	CPL_CONN_POLICY_DENY = 3
+};
+
+enum {
+	ULP_MODE_NONE          = 0,
+	ULP_MODE_ISCSI         = 2,
+	ULP_MODE_RDMA          = 4,
+	ULP_MODE_TCPDDP        = 5,
+	ULP_MODE_FCOE          = 6,
+};
+
+enum {
+	ULP_CRC_HEADER = 1 << 0,
+	ULP_CRC_DATA   = 1 << 1
+};
+
+enum {
+	CPL_PASS_OPEN_ACCEPT,
+	CPL_PASS_OPEN_REJECT,
+	CPL_PASS_OPEN_ACCEPT_TNL
+};
+
+enum {
+	CPL_ABORT_SEND_RST = 0,
+	CPL_ABORT_NO_RST,
+};
+
+enum {                     /* TX_PKT_XT checksum types */
+	TX_CSUM_TCP    = 0,
+	TX_CSUM_UDP    = 1,
+	TX_CSUM_CRC16  = 4,
+	TX_CSUM_CRC32  = 5,
+	TX_CSUM_CRC32C = 6,
+	TX_CSUM_FCOE   = 7,
+	TX_CSUM_TCPIP  = 8,
+	TX_CSUM_UDPIP  = 9,
+	TX_CSUM_TCPIP6 = 10,
+	TX_CSUM_UDPIP6 = 11,
+	TX_CSUM_IP     = 12,
+};
+
+enum {                     /* packet type in CPL_RX_PKT */
+	PKTYPE_XACT_UCAST = 0,
+	PKTYPE_HASH_UCAST = 1,
+	PKTYPE_XACT_MCAST = 2,
+	PKTYPE_HASH_MCAST = 3,
+	PKTYPE_PROMISC    = 4,
+	PKTYPE_HPROMISC   = 5,
+	PKTYPE_BCAST      = 6
+};
+
+enum {                     /* DMAC type in CPL_RX_PKT */
+	DATYPE_UCAST,
+	DATYPE_MCAST,
+	DATYPE_BCAST
+};
+
+enum {                     /* TCP congestion control algorithms */
+	CONG_ALG_RENO,
+	CONG_ALG_TAHOE,
+	CONG_ALG_NEWRENO,
+	CONG_ALG_HIGHSPEED
+};
+
+enum {                     /* RSS hash type */
+	RSS_HASH_NONE = 0, /* no hash computed */
+	RSS_HASH_IP   = 1, /* IP or IPv6 2-tuple hash */
+	RSS_HASH_TCP  = 2, /* TCP 4-tuple hash */
+	RSS_HASH_UDP  = 3  /* UDP 4-tuple hash */
+};
+
+enum {                     /* LE commands */
+	LE_CMD_READ  = 0x4,
+	LE_CMD_WRITE = 0xb
+};
+
+enum {                     /* LE request size */
+	LE_SZ_NONE = 0,
+	LE_SZ_33   = 1,
+	LE_SZ_66   = 2,
+	LE_SZ_132  = 3,
+	LE_SZ_264  = 4,
+	LE_SZ_528  = 5
+};
+
+union opcode_tid {
+	__be32 opcode_tid;
+	__u8 opcode;
+};
+
+#define S_CPL_OPCODE    24
+#define V_CPL_OPCODE(x) ((x) << S_CPL_OPCODE)
+#define G_CPL_OPCODE(x) (((x) >> S_CPL_OPCODE) & 0xFF)
+#define G_TID(x)    ((x) & 0xFFFFFF)
+
+/* tid is assumed to be 24-bits */
+#define MK_OPCODE_TID(opcode, tid) (V_CPL_OPCODE(opcode) | (tid))
+
+#define OPCODE_TID(cmd) ((cmd)->ot.opcode_tid)
+
+/* extract the TID from a CPL command */
+#define GET_TID(cmd) (G_TID(ntohl(OPCODE_TID(cmd))))
+
+/* partitioning of TID fields that also carry a queue id */
+#define S_TID_TID    0
+#define M_TID_TID    0x3fff
+#define V_TID_TID(x) ((x) << S_TID_TID)
+#define G_TID_TID(x) (((x) >> S_TID_TID) & M_TID_TID)
+
+#define S_TID_QID    14
+#define M_TID_QID    0x3ff
+#define V_TID_QID(x) ((x) << S_TID_QID)
+#define G_TID_QID(x) (((x) >> S_TID_QID) & M_TID_QID)
+
+union opcode_info {
+	__be64 opcode_info;
+	__u8 opcode;
+};
+
+struct tcp_options {
+	__be16 mss;
+	__u8 wsf;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 :4;
+	__u8 unknown:1;
+	__u8 ecn:1;
+	__u8 sack:1;
+	__u8 tstamp:1;
+#else
+	__u8 tstamp:1;
+	__u8 sack:1;
+	__u8 ecn:1;
+	__u8 unknown:1;
+	__u8 :4;
+#endif
+};
+
+struct rss_header {
+	__u8 opcode;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 channel:2;
+	__u8 filter_hit:1;
+	__u8 filter_tid:1;
+	__u8 hash_type:2;
+	__u8 ipv6:1;
+	__u8 send2fw:1;
+#else
+	__u8 send2fw:1;
+	__u8 ipv6:1;
+	__u8 hash_type:2;
+	__u8 filter_tid:1;
+	__u8 filter_hit:1;
+	__u8 channel:2;
+#endif
+	__be16 qid;
+	__be32 hash_val;
+};
+
+#define S_HASHTYPE 20
+#define M_HASHTYPE 0x3
+#define G_HASHTYPE(x) (((x) >> S_HASHTYPE) & M_HASHTYPE)
+
+#define S_QNUM 0
+#define M_QNUM 0xFFFF
+#define G_QNUM(x) (((x) >> S_QNUM) & M_QNUM)
+
+#if defined(RSS_HDR_VLD) || defined(CHELSIO_FW)
+# define RSS_HDR struct rss_header rss_hdr;
+#else
+# define RSS_HDR
+#endif
+
+#ifndef CHELSIO_FW
+struct work_request_hdr {
+	__be32 wr_hi;
+	__be32 wr_mid;
+	__be64 wr_lo;
+};
+
+/* wr_mid fields */
+#define S_WR_LEN16    0
+#define M_WR_LEN16    0xFF
+#define V_WR_LEN16(x) ((x) << S_WR_LEN16)
+#define G_WR_LEN16(x) (((x) >> S_WR_LEN16) & M_WR_LEN16)
+
+/* wr_hi fields */
+#define S_WR_OP    24
+#define M_WR_OP    0xFF
+#define V_WR_OP(x) ((__u64)(x) << S_WR_OP)
+#define G_WR_OP(x) (((x) >> S_WR_OP) & M_WR_OP)
+
+# define WR_HDR struct work_request_hdr wr
+# define WR_HDR_SIZE sizeof(struct work_request_hdr)
+#else
+# define WR_HDR
+# define WR_HDR_SIZE 0
+#endif
+
+/* option 0 fields */
+#define S_ACCEPT_MODE    0
+#define M_ACCEPT_MODE    0x3
+#define V_ACCEPT_MODE(x) ((x) << S_ACCEPT_MODE)
+#define G_ACCEPT_MODE(x) (((x) >> S_ACCEPT_MODE) & M_ACCEPT_MODE)
+
+#define S_TX_CHAN    2
+#define M_TX_CHAN    0x3
+#define V_TX_CHAN(x) ((x) << S_TX_CHAN)
+#define G_TX_CHAN(x) (((x) >> S_TX_CHAN) & M_TX_CHAN)
+
+#define S_NO_CONG    4
+#define V_NO_CONG(x) ((x) << S_NO_CONG)
+#define F_NO_CONG    V_NO_CONG(1U)
+
+#define S_DELACK    5
+#define V_DELACK(x) ((x) << S_DELACK)
+#define F_DELACK    V_DELACK(1U)
+
+#define S_INJECT_TIMER    6
+#define V_INJECT_TIMER(x) ((x) << S_INJECT_TIMER)
+#define F_INJECT_TIMER    V_INJECT_TIMER(1U)
+
+#define S_NON_OFFLOAD    7
+#define V_NON_OFFLOAD(x) ((x) << S_NON_OFFLOAD)
+#define F_NON_OFFLOAD    V_NON_OFFLOAD(1U)
+
+#define S_ULP_MODE    8
+#define M_ULP_MODE    0xF
+#define V_ULP_MODE(x) ((x) << S_ULP_MODE)
+#define G_ULP_MODE(x) (((x) >> S_ULP_MODE) & M_ULP_MODE)
+
+#define S_RCV_BUFSIZ    12
+#define M_RCV_BUFSIZ    0x3FFU
+#define V_RCV_BUFSIZ(x) ((x) << S_RCV_BUFSIZ)
+#define G_RCV_BUFSIZ(x) (((x) >> S_RCV_BUFSIZ) & M_RCV_BUFSIZ)
+
+#define S_DSCP    22
+#define M_DSCP    0x3F
+#define V_DSCP(x) ((x) << S_DSCP)
+#define G_DSCP(x) (((x) >> S_DSCP) & M_DSCP)
+
+#define S_SMAC_SEL    28
+#define M_SMAC_SEL    0xFF
+#define V_SMAC_SEL(x) ((__u64)(x) << S_SMAC_SEL)
+#define G_SMAC_SEL(x) (((x) >> S_SMAC_SEL) & M_SMAC_SEL)
+
+#define S_L2T_IDX    36
+#define M_L2T_IDX    0xFFF
+#define V_L2T_IDX(x) ((__u64)(x) << S_L2T_IDX)
+#define G_L2T_IDX(x) (((x) >> S_L2T_IDX) & M_L2T_IDX)
+
+#define S_TCAM_BYPASS    48
+#define V_TCAM_BYPASS(x) ((__u64)(x) << S_TCAM_BYPASS)
+#define F_TCAM_BYPASS    V_TCAM_BYPASS(1ULL)
+
+#define S_NAGLE    49
+#define V_NAGLE(x) ((__u64)(x) << S_NAGLE)
+#define F_NAGLE    V_NAGLE(1ULL)
+
+#define S_WND_SCALE    50
+#define M_WND_SCALE    0xF
+#define V_WND_SCALE(x) ((__u64)(x) << S_WND_SCALE)
+#define G_WND_SCALE(x) (((x) >> S_WND_SCALE) & M_WND_SCALE)
+
+#define S_KEEP_ALIVE    54
+#define V_KEEP_ALIVE(x) ((__u64)(x) << S_KEEP_ALIVE)
+#define F_KEEP_ALIVE    V_KEEP_ALIVE(1ULL)
+
+#define S_MAX_RT    55
+#define M_MAX_RT    0xF
+#define V_MAX_RT(x) ((__u64)(x) << S_MAX_RT)
+#define G_MAX_RT(x) (((x) >> S_MAX_RT) & M_MAX_RT)
+
+#define S_MAX_RT_OVERRIDE    59
+#define V_MAX_RT_OVERRIDE(x) ((__u64)(x) << S_MAX_RT_OVERRIDE)
+#define F_MAX_RT_OVERRIDE    V_MAX_RT_OVERRIDE(1ULL)
+
+#define S_MSS_IDX    60
+#define M_MSS_IDX    0xF
+#define V_MSS_IDX(x) ((__u64)(x) << S_MSS_IDX)
+#define G_MSS_IDX(x) (((x) >> S_MSS_IDX) & M_MSS_IDX)
+
+/* option 1 fields */
+#define S_SYN_RSS_ENABLE    0
+#define V_SYN_RSS_ENABLE(x) ((x) << S_SYN_RSS_ENABLE)
+#define F_SYN_RSS_ENABLE    V_SYN_RSS_ENABLE(1U)
+
+#define S_SYN_RSS_USE_HASH    1
+#define V_SYN_RSS_USE_HASH(x) ((x) << S_SYN_RSS_USE_HASH)
+#define F_SYN_RSS_USE_HASH    V_SYN_RSS_USE_HASH(1U)
+
+#define S_SYN_RSS_QUEUE    2
+#define M_SYN_RSS_QUEUE    0x3FF
+#define V_SYN_RSS_QUEUE(x) ((x) << S_SYN_RSS_QUEUE)
+#define G_SYN_RSS_QUEUE(x) (((x) >> S_SYN_RSS_QUEUE) & M_SYN_RSS_QUEUE)
+
+#define S_LISTEN_INTF    12
+#define M_LISTEN_INTF    0xFF
+#define V_LISTEN_INTF(x) ((x) << S_LISTEN_INTF)
+#define G_LISTEN_INTF(x) (((x) >> S_LISTEN_INTF) & M_LISTEN_INTF)
+
+#define S_LISTEN_FILTER    20
+#define V_LISTEN_FILTER(x) ((x) << S_LISTEN_FILTER)
+#define F_LISTEN_FILTER    V_LISTEN_FILTER(1U)
+
+#define S_SYN_DEFENSE    21
+#define V_SYN_DEFENSE(x) ((x) << S_SYN_DEFENSE)
+#define F_SYN_DEFENSE    V_SYN_DEFENSE(1U)
+
+#define S_CONN_POLICY    22
+#define M_CONN_POLICY    0x3
+#define V_CONN_POLICY(x) ((x) << S_CONN_POLICY)
+#define G_CONN_POLICY(x) (((x) >> S_CONN_POLICY) & M_CONN_POLICY)
+
+/* option 2 fields */
+#define S_RSS_QUEUE    0
+#define M_RSS_QUEUE    0x3FF
+#define V_RSS_QUEUE(x) ((x) << S_RSS_QUEUE)
+#define G_RSS_QUEUE(x) (((x) >> S_RSS_QUEUE) & M_RSS_QUEUE)
+
+#define S_RSS_QUEUE_VALID    10
+#define V_RSS_QUEUE_VALID(x) ((x) << S_RSS_QUEUE_VALID)
+#define F_RSS_QUEUE_VALID    V_RSS_QUEUE_VALID(1U)
+
+#define S_RX_COALESCE_VALID    11
+#define V_RX_COALESCE_VALID(x) ((x) << S_RX_COALESCE_VALID)
+#define F_RX_COALESCE_VALID    V_RX_COALESCE_VALID(1U)
+
+#define S_RX_COALESCE    12
+#define M_RX_COALESCE    0x3
+#define V_RX_COALESCE(x) ((x) << S_RX_COALESCE)
+#define G_RX_COALESCE(x) (((x) >> S_RX_COALESCE) & M_RX_COALESCE)
+
+#define S_CONG_CNTRL    14
+#define M_CONG_CNTRL    0x3
+#define V_CONG_CNTRL(x) ((x) << S_CONG_CNTRL)
+#define G_CONG_CNTRL(x) (((x) >> S_CONG_CNTRL) & M_CONG_CNTRL)
+
+#define S_PACE    16
+#define M_PACE    0x3
+#define V_PACE(x) ((x) << S_PACE)
+#define G_PACE(x) (((x) >> S_PACE) & M_PACE)
+
+#define S_CONG_CNTRL_VALID    18
+#define V_CONG_CNTRL_VALID(x) ((x) << S_CONG_CNTRL_VALID)
+#define F_CONG_CNTRL_VALID    V_CONG_CNTRL_VALID(1U)
+
+#define S_PACE_VALID    19
+#define V_PACE_VALID(x) ((x) << S_PACE_VALID)
+#define F_PACE_VALID    V_PACE_VALID(1U)
+
+#define S_RX_FC_DISABLE    20
+#define V_RX_FC_DISABLE(x) ((x) << S_RX_FC_DISABLE)
+#define F_RX_FC_DISABLE    V_RX_FC_DISABLE(1U)
+
+#define S_RX_FC_DDP    21
+#define V_RX_FC_DDP(x) ((x) << S_RX_FC_DDP)
+#define F_RX_FC_DDP    V_RX_FC_DDP(1U)
+
+#define S_RX_FC_VALID    22
+#define V_RX_FC_VALID(x) ((x) << S_RX_FC_VALID)
+#define F_RX_FC_VALID    V_RX_FC_VALID(1U)
+
+#define S_TX_QUEUE    23
+#define M_TX_QUEUE    0x7
+#define V_TX_QUEUE(x) ((x) << S_TX_QUEUE)
+#define G_TX_QUEUE(x) (((x) >> S_TX_QUEUE) & M_TX_QUEUE)
+
+#define S_RX_CHANNEL    26
+#define V_RX_CHANNEL(x) ((x) << S_RX_CHANNEL)
+#define F_RX_CHANNEL    V_RX_CHANNEL(1U)
+
+#define S_CCTRL_ECN    27
+#define V_CCTRL_ECN(x) ((x) << S_CCTRL_ECN)
+#define F_CCTRL_ECN    V_CCTRL_ECN(1U)
+
+#define S_WND_SCALE_EN    28
+#define V_WND_SCALE_EN(x) ((x) << S_WND_SCALE_EN)
+#define F_WND_SCALE_EN    V_WND_SCALE_EN(1U)
+
+#define S_TSTAMPS_EN    29
+#define V_TSTAMPS_EN(x) ((x) << S_TSTAMPS_EN)
+#define F_TSTAMPS_EN    V_TSTAMPS_EN(1U)
+
+#define S_SACK_EN    30
+#define V_SACK_EN(x) ((x) << S_SACK_EN)
+#define F_SACK_EN    V_SACK_EN(1U)
+
+struct cpl_pass_open_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be32 local_ip;
+	__be32 peer_ip;
+	__be64 opt0;
+	__be64 opt1;
+};
+
+struct cpl_pass_open_req6 {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be64 local_ip_hi;
+	__be64 local_ip_lo;
+	__be64 peer_ip_hi;
+	__be64 peer_ip_lo;
+	__be64 opt0;
+	__be64 opt1;
+};
+
+struct cpl_pass_open_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 rsvd[3];
+	__u8 status;
+};
+
+struct cpl_pass_establish {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 rsvd;
+	__be32 tos_stid;
+	__be16 mac_idx;
+	__be16 tcp_opt;
+	__be32 snd_isn;
+	__be32 rcv_isn;
+};
+
+/* cpl_pass_establish.tos_stid fields */
+#define S_PASS_OPEN_TID    0
+#define M_PASS_OPEN_TID    0xFFFFFF
+#define V_PASS_OPEN_TID(x) ((x) << S_PASS_OPEN_TID)
+#define G_PASS_OPEN_TID(x) (((x) >> S_PASS_OPEN_TID) & M_PASS_OPEN_TID)
+
+#define S_PASS_OPEN_TOS    24
+#define M_PASS_OPEN_TOS    0xFF
+#define V_PASS_OPEN_TOS(x) ((x) << S_PASS_OPEN_TOS)
+#define G_PASS_OPEN_TOS(x) (((x) >> S_PASS_OPEN_TOS) & M_PASS_OPEN_TOS)
+
+/* cpl_pass_establish.tcp_opt fields (also applies to act_open_establish) */
+#define G_TCPOPT_WSCALE_OK(x)  (((x) >> 5) & 1)
+#define G_TCPOPT_SACK(x)       (((x) >> 6) & 1)
+#define G_TCPOPT_TSTAMP(x)     (((x) >> 7) & 1)
+#define G_TCPOPT_SND_WSCALE(x) (((x) >> 8) & 0xf)
+#define G_TCPOPT_MSS(x)        (((x) >> 12) & 0xf)
+
+struct cpl_pass_accept_req {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+	__be32 hdr_len;
+	__be16 vlan;
+	__be16 l2info;
+	__be32 tos_stid;
+	struct tcp_options tcpopt;
+};
+
+/* cpl_pass_accept_req.hdr_len fields */
+#define S_SYN_RX_CHAN    0
+#define M_SYN_RX_CHAN    0xF
+#define V_SYN_RX_CHAN(x) ((x) << S_SYN_RX_CHAN)
+#define G_SYN_RX_CHAN(x) (((x) >> S_SYN_RX_CHAN) & M_SYN_RX_CHAN)
+
+#define S_TCP_HDR_LEN    10
+#define M_TCP_HDR_LEN    0x3F
+#define V_TCP_HDR_LEN(x) ((x) << S_TCP_HDR_LEN)
+#define G_TCP_HDR_LEN(x) (((x) >> S_TCP_HDR_LEN) & M_TCP_HDR_LEN)
+
+#define S_IP_HDR_LEN    16
+#define M_IP_HDR_LEN    0x3FF
+#define V_IP_HDR_LEN(x) ((x) << S_IP_HDR_LEN)
+#define G_IP_HDR_LEN(x) (((x) >> S_IP_HDR_LEN) & M_IP_HDR_LEN)
+
+#define S_ETH_HDR_LEN    26
+#define M_ETH_HDR_LEN    0x3F
+#define V_ETH_HDR_LEN(x) ((x) << S_ETH_HDR_LEN)
+#define G_ETH_HDR_LEN(x) (((x) >> S_ETH_HDR_LEN) & M_ETH_HDR_LEN)
+
+/* cpl_pass_accept_req.l2info fields */
+#define S_SYN_MAC_IDX    0
+#define M_SYN_MAC_IDX    0x1FF
+#define V_SYN_MAC_IDX(x) ((x) << S_SYN_MAC_IDX)
+#define G_SYN_MAC_IDX(x) (((x) >> S_SYN_MAC_IDX) & M_SYN_MAC_IDX)
+
+#define S_SYN_XACT_MATCH    9
+#define V_SYN_XACT_MATCH(x) ((x) << S_SYN_XACT_MATCH)
+#define F_SYN_XACT_MATCH    V_SYN_XACT_MATCH(1U)
+
+#define S_SYN_INTF    12
+#define M_SYN_INTF    0xF
+#define V_SYN_INTF(x) ((x) << S_SYN_INTF)
+#define G_SYN_INTF(x) (((x) >> S_SYN_INTF) & M_SYN_INTF)
+
+struct cpl_pass_accept_rpl {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 opt2;
+	__be64 opt0;
+};
+
+struct cpl_act_open_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be32 local_ip;
+	__be32 peer_ip;
+	__be64 opt0;
+	__be32 params;
+	__be32 opt2;
+};
+
+struct cpl_t5_act_open_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be32 local_ip;
+	__be32 peer_ip;
+	__be64 opt0;
+	__be32 rsvd;
+	__be32 opt2;
+	__be64 params;
+};
+
+struct cpl_act_open_req6 {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be64 local_ip_hi;
+	__be64 local_ip_lo;
+	__be64 peer_ip_hi;
+	__be64 peer_ip_lo;
+	__be64 opt0;
+	__be32 params;
+	__be32 opt2;
+};
+
+struct cpl_t5_act_open_req6 {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 local_port;
+	__be16 peer_port;
+	__be64 local_ip_hi;
+	__be64 local_ip_lo;
+	__be64 peer_ip_hi;
+	__be64 peer_ip_lo;
+	__be64 opt0;
+	__be32 rsvd;
+	__be32 opt2;
+	__be64 params;
+};
+
+struct cpl_act_open_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 atid_status;
+};
+
+/* cpl_act_open_rpl.atid_status fields */
+#define S_AOPEN_STATUS    0
+#define M_AOPEN_STATUS    0xFF
+#define V_AOPEN_STATUS(x) ((x) << S_AOPEN_STATUS)
+#define G_AOPEN_STATUS(x) (((x) >> S_AOPEN_STATUS) & M_AOPEN_STATUS)
+
+#define S_AOPEN_ATID    8
+#define M_AOPEN_ATID    0xFFFFFF
+#define V_AOPEN_ATID(x) ((x) << S_AOPEN_ATID)
+#define G_AOPEN_ATID(x) (((x) >> S_AOPEN_ATID) & M_AOPEN_ATID)
+
+struct cpl_act_establish {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 rsvd;
+	__be32 tos_atid;
+	__be16 mac_idx;
+	__be16 tcp_opt;
+	__be32 snd_isn;
+	__be32 rcv_isn;
+};
+
+struct cpl_get_tcb {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 reply_ctrl;
+	__be16 cookie;
+};
+
+/* cpl_get_tcb.reply_ctrl fields */
+#define S_QUEUENO    0
+#define M_QUEUENO    0x3FF
+#define V_QUEUENO(x) ((x) << S_QUEUENO)
+#define G_QUEUENO(x) (((x) >> S_QUEUENO) & M_QUEUENO)
+
+#define S_REPLY_CHAN    14
+#define V_REPLY_CHAN(x) ((x) << S_REPLY_CHAN)
+#define F_REPLY_CHAN    V_REPLY_CHAN(1U)
+
+#define S_NO_REPLY    15
+#define V_NO_REPLY(x) ((x) << S_NO_REPLY)
+#define F_NO_REPLY    V_NO_REPLY(1U)
+
+struct cpl_get_tcb_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 cookie;
+	__u8 status;
+	__be16 len;
+};
+
+struct cpl_set_tcb {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 reply_ctrl;
+	__be16 cookie;
+};
+
+struct cpl_set_tcb_field {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 reply_ctrl;
+	__be16 word_cookie;
+	__be64 mask;
+	__be64 val;
+};
+
+/* cpl_set_tcb_field.word_cookie fields */
+#define S_WORD    0
+#define M_WORD    0x1F
+#define V_WORD(x) ((x) << S_WORD)
+#define G_WORD(x) (((x) >> S_WORD) & M_WORD)
+
+#define S_COOKIE    5
+#define M_COOKIE    0x7
+#define V_COOKIE(x) ((x) << S_COOKIE)
+#define G_COOKIE(x) (((x) >> S_COOKIE) & M_COOKIE)
+
+struct cpl_set_tcb_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__u8   cookie;
+	__u8   status;
+	__be64 oldval;
+};
+
+struct cpl_close_con_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 rsvd;
+};
+
+struct cpl_close_con_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  rsvd[3];
+	__u8  status;
+	__be32 snd_nxt;
+	__be32 rcv_nxt;
+};
+
+struct cpl_close_listsvr_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 reply_ctrl;
+	__be16 rsvd;
+};
+
+/* additional cpl_close_listsvr_req.reply_ctrl field */
+#define S_LISTSVR_IPV6    14
+#define V_LISTSVR_IPV6(x) ((x) << S_LISTSVR_IPV6)
+#define F_LISTSVR_IPV6    V_LISTSVR_IPV6(1U)
+
+struct cpl_close_listsvr_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 rsvd[3];
+	__u8 status;
+};
+
+struct cpl_abort_req_rss {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  rsvd[3];
+	__u8  status;
+};
+
+struct cpl_abort_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 rsvd0;
+	__u8  rsvd1;
+	__u8  cmd;
+	__u8  rsvd2[6];
+};
+
+struct cpl_abort_rpl_rss {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  rsvd[3];
+	__u8  status;
+};
+
+struct cpl_abort_rpl {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 rsvd0;
+	__u8  rsvd1;
+	__u8  cmd;
+	__u8  rsvd2[6];
+};
+
+struct cpl_peer_close {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 rcv_nxt;
+};
+
+struct cpl_tid_release {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 rsvd;
+};
+
+struct tx_data_wr {
+	__be32 wr_hi;
+	__be32 wr_lo;
+	__be32 len;
+	__be32 flags;
+	__be32 sndseq;
+	__be32 param;
+};
+
+/* tx_data_wr.flags fields */
+#define S_TX_ACK_PAGES    21
+#define M_TX_ACK_PAGES    0x7
+#define V_TX_ACK_PAGES(x) ((x) << S_TX_ACK_PAGES)
+#define G_TX_ACK_PAGES(x) (((x) >> S_TX_ACK_PAGES) & M_TX_ACK_PAGES)
+
+/* tx_data_wr.param fields */
+#define S_TX_PORT    0
+#define M_TX_PORT    0x7
+#define V_TX_PORT(x) ((x) << S_TX_PORT)
+#define G_TX_PORT(x) (((x) >> S_TX_PORT) & M_TX_PORT)
+
+#define S_TX_MSS    4
+#define M_TX_MSS    0xF
+#define V_TX_MSS(x) ((x) << S_TX_MSS)
+#define G_TX_MSS(x) (((x) >> S_TX_MSS) & M_TX_MSS)
+
+#define S_TX_QOS    8
+#define M_TX_QOS    0xFF
+#define V_TX_QOS(x) ((x) << S_TX_QOS)
+#define G_TX_QOS(x) (((x) >> S_TX_QOS) & M_TX_QOS)
+
+#define S_TX_SNDBUF 16
+#define M_TX_SNDBUF 0xFFFF
+#define V_TX_SNDBUF(x) ((x) << S_TX_SNDBUF)
+#define G_TX_SNDBUF(x) (((x) >> S_TX_SNDBUF) & M_TX_SNDBUF)
+
+struct cpl_tx_data {
+	union opcode_tid ot;
+	__be32 len;
+	__be32 rsvd;
+	__be32 flags;
+};
+
+/* cpl_tx_data.flags fields */
+#define S_TX_PROXY    5
+#define V_TX_PROXY(x) ((x) << S_TX_PROXY)
+#define F_TX_PROXY    V_TX_PROXY(1U)
+
+#define S_TX_ULP_SUBMODE    6
+#define M_TX_ULP_SUBMODE    0xF
+#define V_TX_ULP_SUBMODE(x) ((x) << S_TX_ULP_SUBMODE)
+#define G_TX_ULP_SUBMODE(x) (((x) >> S_TX_ULP_SUBMODE) & M_TX_ULP_SUBMODE)
+
+#define S_TX_ULP_MODE    10
+#define M_TX_ULP_MODE    0xF
+#define V_TX_ULP_MODE(x) ((x) << S_TX_ULP_MODE)
+#define G_TX_ULP_MODE(x) (((x) >> S_TX_ULP_MODE) & M_TX_ULP_MODE)
+
+#define S_TX_SHOVE    14
+#define V_TX_SHOVE(x) ((x) << S_TX_SHOVE)
+#define F_TX_SHOVE    V_TX_SHOVE(1U)
+
+#define S_TX_MORE    15
+#define V_TX_MORE(x) ((x) << S_TX_MORE)
+#define F_TX_MORE    V_TX_MORE(1U)
+
+#define S_TX_URG    16
+#define V_TX_URG(x) ((x) << S_TX_URG)
+#define F_TX_URG    V_TX_URG(1U)
+
+#define S_TX_FLUSH    17
+#define V_TX_FLUSH(x) ((x) << S_TX_FLUSH)
+#define F_TX_FLUSH    V_TX_FLUSH(1U)
+
+#define S_TX_SAVE    18
+#define V_TX_SAVE(x) ((x) << S_TX_SAVE)
+#define F_TX_SAVE    V_TX_SAVE(1U)
+
+#define S_TX_TNL    19
+#define V_TX_TNL(x) ((x) << S_TX_TNL)
+#define F_TX_TNL    V_TX_TNL(1U)
+
+/* additional tx_data_wr.flags fields */
+#define S_TX_CPU_IDX    0
+#define M_TX_CPU_IDX    0x3F
+#define V_TX_CPU_IDX(x) ((x) << S_TX_CPU_IDX)
+#define G_TX_CPU_IDX(x) (((x) >> S_TX_CPU_IDX) & M_TX_CPU_IDX)
+
+#define S_TX_CLOSE    17
+#define V_TX_CLOSE(x) ((x) << S_TX_CLOSE)
+#define F_TX_CLOSE    V_TX_CLOSE(1U)
+
+#define S_TX_INIT    18
+#define V_TX_INIT(x) ((x) << S_TX_INIT)
+#define F_TX_INIT    V_TX_INIT(1U)
+
+#define S_TX_IMM_ACK    19
+#define V_TX_IMM_ACK(x) ((x) << S_TX_IMM_ACK)
+#define F_TX_IMM_ACK    V_TX_IMM_ACK(1U)
+
+#define S_TX_IMM_DMA    20
+#define V_TX_IMM_DMA(x) ((x) << S_TX_IMM_DMA)
+#define F_TX_IMM_DMA    V_TX_IMM_DMA(1U)
+
+struct cpl_tx_data_ack {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 snd_una;
+};
+
+struct cpl_wr_ack {  /* XXX */
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 credits;
+	__be16 rsvd;
+	__be32 snd_nxt;
+	__be32 snd_una;
+};
+
+struct cpl_tx_pkt_core {
+	__be32 ctrl0;
+	__be16 pack;
+	__be16 len;
+	__be64 ctrl1;
+};
+
+struct cpl_tx_pkt {
+	WR_HDR;
+	struct cpl_tx_pkt_core c;
+};
+
+#define cpl_tx_pkt_xt cpl_tx_pkt
+
+/* cpl_tx_pkt_core.ctrl0 fields */
+#define S_TXPKT_VF    0
+#define M_TXPKT_VF    0xFF
+#define V_TXPKT_VF(x) ((x) << S_TXPKT_VF)
+#define G_TXPKT_VF(x) (((x) >> S_TXPKT_VF) & M_TXPKT_VF)
+
+#define S_TXPKT_PF    8
+#define M_TXPKT_PF    0x7
+#define V_TXPKT_PF(x) ((x) << S_TXPKT_PF)
+#define G_TXPKT_PF(x) (((x) >> S_TXPKT_PF) & M_TXPKT_PF)
+
+#define S_TXPKT_VF_VLD    11
+#define V_TXPKT_VF_VLD(x) ((x) << S_TXPKT_VF_VLD)
+#define F_TXPKT_VF_VLD    V_TXPKT_VF_VLD(1U)
+
+#define S_TXPKT_OVLAN_IDX    12
+#define M_TXPKT_OVLAN_IDX    0xF
+#define V_TXPKT_OVLAN_IDX(x) ((x) << S_TXPKT_OVLAN_IDX)
+#define G_TXPKT_OVLAN_IDX(x) (((x) >> S_TXPKT_OVLAN_IDX) & M_TXPKT_OVLAN_IDX)
+
+#define S_TXPKT_INTF    16
+#define M_TXPKT_INTF    0xF
+#define V_TXPKT_INTF(x) ((x) << S_TXPKT_INTF)
+#define G_TXPKT_INTF(x) (((x) >> S_TXPKT_INTF) & M_TXPKT_INTF)
+
+#define S_TXPKT_SPECIAL_STAT    20
+#define V_TXPKT_SPECIAL_STAT(x) ((x) << S_TXPKT_SPECIAL_STAT)
+#define F_TXPKT_SPECIAL_STAT    V_TXPKT_SPECIAL_STAT(1U)
+
+#define S_TXPKT_INS_OVLAN    21
+#define V_TXPKT_INS_OVLAN(x) ((x) << S_TXPKT_INS_OVLAN)
+#define F_TXPKT_INS_OVLAN    V_TXPKT_INS_OVLAN(1U)
+
+#define S_TXPKT_STAT_DIS    22
+#define V_TXPKT_STAT_DIS(x) ((x) << S_TXPKT_STAT_DIS)
+#define F_TXPKT_STAT_DIS    V_TXPKT_STAT_DIS(1U)
+
+#define S_TXPKT_LOOPBACK    23
+#define V_TXPKT_LOOPBACK(x) ((x) << S_TXPKT_LOOPBACK)
+#define F_TXPKT_LOOPBACK    V_TXPKT_LOOPBACK(1U)
+
+#define S_TXPKT_TSTAMP    23
+#define V_TXPKT_TSTAMP(x) ((x) << S_TXPKT_TSTAMP)
+#define F_TXPKT_TSTAMP    V_TXPKT_TSTAMP(1U)
+
+#define S_TXPKT_OPCODE    24
+#define M_TXPKT_OPCODE    0xFF
+#define V_TXPKT_OPCODE(x) ((x) << S_TXPKT_OPCODE)
+#define G_TXPKT_OPCODE(x) (((x) >> S_TXPKT_OPCODE) & M_TXPKT_OPCODE)
+
+/* cpl_tx_pkt_core.ctrl1 fields */
+#define S_TXPKT_SA_IDX    0
+#define M_TXPKT_SA_IDX    0xFFF
+#define V_TXPKT_SA_IDX(x) ((x) << S_TXPKT_SA_IDX)
+#define G_TXPKT_SA_IDX(x) (((x) >> S_TXPKT_SA_IDX) & M_TXPKT_SA_IDX)
+
+#define S_TXPKT_CSUM_END    12
+#define M_TXPKT_CSUM_END    0xFF
+#define V_TXPKT_CSUM_END(x) ((x) << S_TXPKT_CSUM_END)
+#define G_TXPKT_CSUM_END(x) (((x) >> S_TXPKT_CSUM_END) & M_TXPKT_CSUM_END)
+
+#define S_TXPKT_CSUM_START    20
+#define M_TXPKT_CSUM_START    0x3FF
+#define V_TXPKT_CSUM_START(x) ((x) << S_TXPKT_CSUM_START)
+#define G_TXPKT_CSUM_START(x) (((x) >> S_TXPKT_CSUM_START) & M_TXPKT_CSUM_START)
+
+#define S_TXPKT_IPHDR_LEN    20
+#define M_TXPKT_IPHDR_LEN    0x3FFF
+#define V_TXPKT_IPHDR_LEN(x) ((__u64)(x) << S_TXPKT_IPHDR_LEN)
+#define G_TXPKT_IPHDR_LEN(x) (((x) >> S_TXPKT_IPHDR_LEN) & M_TXPKT_IPHDR_LEN)
+
+#define S_TXPKT_CSUM_LOC    30
+#define M_TXPKT_CSUM_LOC    0x3FF
+#define V_TXPKT_CSUM_LOC(x) ((__u64)(x) << S_TXPKT_CSUM_LOC)
+#define G_TXPKT_CSUM_LOC(x) (((x) >> S_TXPKT_CSUM_LOC) & M_TXPKT_CSUM_LOC)
+
+#define S_TXPKT_ETHHDR_LEN    34
+#define M_TXPKT_ETHHDR_LEN    0x3F
+#define V_TXPKT_ETHHDR_LEN(x) ((__u64)(x) << S_TXPKT_ETHHDR_LEN)
+#define G_TXPKT_ETHHDR_LEN(x) (((x) >> S_TXPKT_ETHHDR_LEN) & M_TXPKT_ETHHDR_LEN)
+
+#define S_TXPKT_CSUM_TYPE    40
+#define M_TXPKT_CSUM_TYPE    0xF
+#define V_TXPKT_CSUM_TYPE(x) ((__u64)(x) << S_TXPKT_CSUM_TYPE)
+#define G_TXPKT_CSUM_TYPE(x) (((x) >> S_TXPKT_CSUM_TYPE) & M_TXPKT_CSUM_TYPE)
+
+#define S_TXPKT_VLAN    44
+#define M_TXPKT_VLAN    0xFFFF
+#define V_TXPKT_VLAN(x) ((__u64)(x) << S_TXPKT_VLAN)
+#define G_TXPKT_VLAN(x) (((x) >> S_TXPKT_VLAN) & M_TXPKT_VLAN)
+
+#define S_TXPKT_VLAN_VLD    60
+#define V_TXPKT_VLAN_VLD(x) ((__u64)(x) << S_TXPKT_VLAN_VLD)
+#define F_TXPKT_VLAN_VLD    V_TXPKT_VLAN_VLD(1ULL)
+
+#define S_TXPKT_IPSEC    61
+#define V_TXPKT_IPSEC(x) ((__u64)(x) << S_TXPKT_IPSEC)
+#define F_TXPKT_IPSEC    V_TXPKT_IPSEC(1ULL)
+
+#define S_TXPKT_IPCSUM_DIS    62
+#define V_TXPKT_IPCSUM_DIS(x) ((__u64)(x) << S_TXPKT_IPCSUM_DIS)
+#define F_TXPKT_IPCSUM_DIS    V_TXPKT_IPCSUM_DIS(1ULL)
+
+#define S_TXPKT_L4CSUM_DIS    63
+#define V_TXPKT_L4CSUM_DIS(x) ((__u64)(x) << S_TXPKT_L4CSUM_DIS)
+#define F_TXPKT_L4CSUM_DIS    V_TXPKT_L4CSUM_DIS(1ULL)
+
+struct cpl_tx_pkt_lso_core {
+	__be32 lso_ctrl;
+	__be16 ipid_ofst;
+	__be16 mss;
+	__be32 seqno_offset;
+	__be32 len;
+	/* encapsulated CPL (TX_PKT, TX_PKT_XT or TX_DATA) follows here */
+};
+
+struct cpl_tx_pkt_lso {
+	WR_HDR;
+	struct cpl_tx_pkt_lso_core c;
+	/* encapsulated CPL (TX_PKT, TX_PKT_XT or TX_DATA) follows here */
+};
+
+struct cpl_tx_pkt_ufo_core {
+	__be16 ethlen;
+	__be16 iplen;
+	__be16 udplen;
+	__be16 mss;
+	__be32 len;
+	__be32 r1;
+	/* encapsulated CPL (TX_PKT, TX_PKT_XT or TX_DATA) follows here */
+};
+
+struct cpl_tx_pkt_ufo {
+	WR_HDR;
+	struct cpl_tx_pkt_ufo_core c;
+	/* encapsulated CPL (TX_PKT, TX_PKT_XT or TX_DATA) follows here */
+};
+
+/* cpl_tx_pkt_lso_core.lso_ctrl fields */
+#define S_LSO_TCPHDR_LEN    0
+#define M_LSO_TCPHDR_LEN    0xF
+#define V_LSO_TCPHDR_LEN(x) ((x) << S_LSO_TCPHDR_LEN)
+#define G_LSO_TCPHDR_LEN(x) (((x) >> S_LSO_TCPHDR_LEN) & M_LSO_TCPHDR_LEN)
+
+#define S_LSO_IPHDR_LEN    4
+#define M_LSO_IPHDR_LEN    0xFFF
+#define V_LSO_IPHDR_LEN(x) ((x) << S_LSO_IPHDR_LEN)
+#define G_LSO_IPHDR_LEN(x) (((x) >> S_LSO_IPHDR_LEN) & M_LSO_IPHDR_LEN)
+
+#define S_LSO_ETHHDR_LEN    16
+#define M_LSO_ETHHDR_LEN    0xF
+#define V_LSO_ETHHDR_LEN(x) ((x) << S_LSO_ETHHDR_LEN)
+#define G_LSO_ETHHDR_LEN(x) (((x) >> S_LSO_ETHHDR_LEN) & M_LSO_ETHHDR_LEN)
+
+#define S_LSO_IPV6    20
+#define V_LSO_IPV6(x) ((x) << S_LSO_IPV6)
+#define F_LSO_IPV6    V_LSO_IPV6(1U)
+
+#define S_LSO_OFLD_ENCAP    21
+#define V_LSO_OFLD_ENCAP(x) ((x) << S_LSO_OFLD_ENCAP)
+#define F_LSO_OFLD_ENCAP    V_LSO_OFLD_ENCAP(1U)
+
+#define S_LSO_LAST_SLICE    22
+#define V_LSO_LAST_SLICE(x) ((x) << S_LSO_LAST_SLICE)
+#define F_LSO_LAST_SLICE    V_LSO_LAST_SLICE(1U)
+
+#define S_LSO_FIRST_SLICE    23
+#define V_LSO_FIRST_SLICE(x) ((x) << S_LSO_FIRST_SLICE)
+#define F_LSO_FIRST_SLICE    V_LSO_FIRST_SLICE(1U)
+
+#define S_LSO_OPCODE    24
+#define M_LSO_OPCODE    0xFF
+#define V_LSO_OPCODE(x) ((x) << S_LSO_OPCODE)
+#define G_LSO_OPCODE(x) (((x) >> S_LSO_OPCODE) & M_LSO_OPCODE)
+
+/* cpl_tx_pkt_lso_core.mss fields */
+#define S_LSO_MSS    0
+#define M_LSO_MSS    0x3FFF
+#define V_LSO_MSS(x) ((x) << S_LSO_MSS)
+#define G_LSO_MSS(x) (((x) >> S_LSO_MSS) & M_LSO_MSS)
+
+#define S_LSO_IPID_SPLIT    15
+#define V_LSO_IPID_SPLIT(x) ((x) << S_LSO_IPID_SPLIT)
+#define F_LSO_IPID_SPLIT    V_LSO_IPID_SPLIT(1U)
+
+struct cpl_tx_pkt_fso {
+	WR_HDR;
+	__be32 fso_ctrl;
+	__be16 seqcnt_ofst;
+	__be16 mtu;
+	__be32 param_offset;
+	__be32 len;
+	/* encapsulated CPL (TX_PKT or TX_PKT_XT) follows here */
+};
+
+/* cpl_tx_pkt_fso.fso_ctrl fields different from cpl_tx_pkt_lso.lso_ctrl */
+#define S_FSO_XCHG_CLASS    21
+#define V_FSO_XCHG_CLASS(x) ((x) << S_FSO_XCHG_CLASS)
+#define F_FSO_XCHG_CLASS    V_FSO_XCHG_CLASS(1U)
+
+#define S_FSO_INITIATOR    20
+#define V_FSO_INITIATOR(x) ((x) << S_FSO_INITIATOR)
+#define F_FSO_INITIATOR    V_FSO_INITIATOR(1U)
+
+#define S_FSO_FCHDR_LEN    12
+#define M_FSO_FCHDR_LEN    0xF
+#define V_FSO_FCHDR_LEN(x) ((x) << S_FSO_FCHDR_LEN)
+#define G_FSO_FCHDR_LEN(x) (((x) >> S_FSO_FCHDR_LEN) & M_FSO_FCHDR_LEN)
+
+struct cpl_iscsi_hdr_no_rss {
+	union opcode_tid ot;
+	__be16 pdu_len_ddp;
+	__be16 len;
+	__be32 seq;
+	__be16 urg;
+	__u8 rsvd;
+	__u8 status;
+};
+
+struct cpl_tx_data_iso {
+	WR_HDR;
+	__be32 iso_ctrl;
+	__u8   rsvd;
+	__u8   ahs_len;
+	__be16 mss;
+	__be32 burst_size;
+	__be32 len;
+	/* encapsulated CPL_TX_DATA follows here */
+};
+
+/* cpl_tx_data_iso.iso_ctrl fields different from cpl_tx_pkt_lso.lso_ctrl */
+#define S_ISO_CPLHDR_LEN    18
+#define M_ISO_CPLHDR_LEN    0xF
+#define V_ISO_CPLHDR_LEN(x) ((x) << S_ISO_CPLHDR_LEN)
+#define G_ISO_CPLHDR_LEN(x) (((x) >> S_ISO_CPLHDR_LEN) & M_ISO_CPLHDR_LEN)
+
+#define S_ISO_HDR_CRC    17
+#define V_ISO_HDR_CRC(x) ((x) << S_ISO_HDR_CRC)
+#define F_ISO_HDR_CRC    V_ISO_HDR_CRC(1U)
+
+#define S_ISO_DATA_CRC    16
+#define V_ISO_DATA_CRC(x) ((x) << S_ISO_DATA_CRC)
+#define F_ISO_DATA_CRC    V_ISO_DATA_CRC(1U)
+
+#define S_ISO_IMD_DATA_EN    15
+#define V_ISO_IMD_DATA_EN(x) ((x) << S_ISO_IMD_DATA_EN)
+#define F_ISO_IMD_DATA_EN    V_ISO_IMD_DATA_EN(1U)
+
+#define S_ISO_PDU_TYPE    13
+#define M_ISO_PDU_TYPE    0x3
+#define V_ISO_PDU_TYPE(x) ((x) << S_ISO_PDU_TYPE)
+#define G_ISO_PDU_TYPE(x) (((x) >> S_ISO_PDU_TYPE) & M_ISO_PDU_TYPE)
+
+struct cpl_iscsi_hdr {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 pdu_len_ddp;
+	__be16 len;
+	__be32 seq;
+	__be16 urg;
+	__u8 rsvd;
+	__u8 status;
+};
+
+/* cpl_iscsi_hdr.pdu_len_ddp fields */
+#define S_ISCSI_PDU_LEN    0
+#define M_ISCSI_PDU_LEN    0x7FFF
+#define V_ISCSI_PDU_LEN(x) ((x) << S_ISCSI_PDU_LEN)
+#define G_ISCSI_PDU_LEN(x) (((x) >> S_ISCSI_PDU_LEN) & M_ISCSI_PDU_LEN)
+
+#define S_ISCSI_DDP    15
+#define V_ISCSI_DDP(x) ((x) << S_ISCSI_DDP)
+#define F_ISCSI_DDP    V_ISCSI_DDP(1U)
+
+struct cpl_iscsi_data {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 rsvd0[2];
+	__be16 len;
+	__be32 seq;
+	__be16 urg;
+	__u8 rsvd1;
+	__u8 status;
+};
+
+struct cpl_rx_data {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+	__be32 seq;
+	__be16 urg;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 dack_mode:2;
+	__u8 psh:1;
+	__u8 heartbeat:1;
+	__u8 ddp_off:1;
+	__u8 :3;
+#else
+	__u8 :3;
+	__u8 ddp_off:1;
+	__u8 heartbeat:1;
+	__u8 psh:1;
+	__u8 dack_mode:2;
+#endif
+	__u8 status;
+};
+
+struct cpl_fcoe_hdr {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 oxid;
+	__be16 len;
+	__be32 rctl_fctl;
+	__u8 cs_ctl;
+	__u8 df_ctl;
+	__u8 sof;
+	__u8 eof;
+	__be16 seq_cnt;
+	__u8 seq_id;
+	__u8 type;
+	__be32 param;
+};
+
+struct cpl_fcoe_data {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 rsvd0[2];
+	__be16 len;
+	__be32 seq;
+	__u8 rsvd1[3];
+	__u8 status;
+};
+
+struct cpl_rx_urg_notify {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 seq;
+};
+
+struct cpl_rx_urg_pkt {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+};
+
+struct cpl_rx_data_ack {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 credit_dack;
+};
+
+/* cpl_rx_data_ack.ack_seq fields */
+#define S_RX_CREDITS    0
+#define M_RX_CREDITS    0x3FFFFFF
+#define V_RX_CREDITS(x) ((x) << S_RX_CREDITS)
+#define G_RX_CREDITS(x) (((x) >> S_RX_CREDITS) & M_RX_CREDITS)
+
+#define S_RX_MODULATE_TX    26
+#define V_RX_MODULATE_TX(x) ((x) << S_RX_MODULATE_TX)
+#define F_RX_MODULATE_TX    V_RX_MODULATE_TX(1U)
+
+#define S_RX_MODULATE_RX    27
+#define V_RX_MODULATE_RX(x) ((x) << S_RX_MODULATE_RX)
+#define F_RX_MODULATE_RX    V_RX_MODULATE_RX(1U)
+
+#define S_RX_FORCE_ACK    28
+#define V_RX_FORCE_ACK(x) ((x) << S_RX_FORCE_ACK)
+#define F_RX_FORCE_ACK    V_RX_FORCE_ACK(1U)
+
+#define S_RX_DACK_MODE    29
+#define M_RX_DACK_MODE    0x3
+#define V_RX_DACK_MODE(x) ((x) << S_RX_DACK_MODE)
+#define G_RX_DACK_MODE(x) (((x) >> S_RX_DACK_MODE) & M_RX_DACK_MODE)
+
+#define S_RX_DACK_CHANGE    31
+#define V_RX_DACK_CHANGE(x) ((x) << S_RX_DACK_CHANGE)
+#define F_RX_DACK_CHANGE    V_RX_DACK_CHANGE(1U)
+
+struct cpl_rx_ddp_complete {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 ddp_report;
+	__be32 rcv_nxt;
+	__be32 rsvd;
+};
+
+struct cpl_rx_data_ddp {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 urg;
+	__be16 len;
+	__be32 seq;
+	union {
+		__be32 nxt_seq;
+		__be32 ddp_report;
+	};
+	__be32 ulp_crc;
+	__be32 ddpvld;
+};
+
+#define cpl_rx_iscsi_ddp cpl_rx_data_ddp
+
+struct cpl_rx_fcoe_ddp {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+	__be32 seq;
+	__be32 ddp_report;
+	__be32 ulp_crc;
+	__be32 ddpvld;
+};
+
+struct cpl_rx_data_dif {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 ddp_len;
+	__be16 msg_len;
+	__be32 seq;
+	union {
+		__be32 nxt_seq;
+		__be32 ddp_report;
+	};
+	__be32 err_vec;
+	__be32 ddpvld;
+};
+
+struct cpl_rx_iscsi_dif {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 ddp_len;
+	__be16 msg_len;
+	__be32 seq;
+	union {
+		__be32 nxt_seq;
+		__be32 ddp_report;
+	};
+	__be32 ulp_crc;
+	__be32 ddpvld;
+	__u8 rsvd0[8];
+	__be32 err_vec;
+	__u8 rsvd1[4];
+};
+
+struct cpl_rx_fcoe_dif {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 ddp_len;
+	__be16 msg_len;
+	__be32 seq;
+	__be32 ddp_report;
+	__be32 err_vec;
+	__be32 ddpvld;
+};
+
+/* cpl_rx_{data,iscsi,fcoe}_{ddp,dif}.ddpvld fields */
+#define S_DDP_VALID    15
+#define M_DDP_VALID    0x1FFFF
+#define V_DDP_VALID(x) ((x) << S_DDP_VALID)
+#define G_DDP_VALID(x) (((x) >> S_DDP_VALID) & M_DDP_VALID)
+
+#define S_DDP_PPOD_MISMATCH    15
+#define V_DDP_PPOD_MISMATCH(x) ((x) << S_DDP_PPOD_MISMATCH)
+#define F_DDP_PPOD_MISMATCH    V_DDP_PPOD_MISMATCH(1U)
+
+#define S_DDP_PDU    16
+#define V_DDP_PDU(x) ((x) << S_DDP_PDU)
+#define F_DDP_PDU    V_DDP_PDU(1U)
+
+#define S_DDP_LLIMIT_ERR    17
+#define V_DDP_LLIMIT_ERR(x) ((x) << S_DDP_LLIMIT_ERR)
+#define F_DDP_LLIMIT_ERR    V_DDP_LLIMIT_ERR(1U)
+
+#define S_DDP_PPOD_PARITY_ERR    18
+#define V_DDP_PPOD_PARITY_ERR(x) ((x) << S_DDP_PPOD_PARITY_ERR)
+#define F_DDP_PPOD_PARITY_ERR    V_DDP_PPOD_PARITY_ERR(1U)
+
+#define S_DDP_PADDING_ERR    19
+#define V_DDP_PADDING_ERR(x) ((x) << S_DDP_PADDING_ERR)
+#define F_DDP_PADDING_ERR    V_DDP_PADDING_ERR(1U)
+
+#define S_DDP_HDRCRC_ERR    20
+#define V_DDP_HDRCRC_ERR(x) ((x) << S_DDP_HDRCRC_ERR)
+#define F_DDP_HDRCRC_ERR    V_DDP_HDRCRC_ERR(1U)
+
+#define S_DDP_DATACRC_ERR    21
+#define V_DDP_DATACRC_ERR(x) ((x) << S_DDP_DATACRC_ERR)
+#define F_DDP_DATACRC_ERR    V_DDP_DATACRC_ERR(1U)
+
+#define S_DDP_INVALID_TAG    22
+#define V_DDP_INVALID_TAG(x) ((x) << S_DDP_INVALID_TAG)
+#define F_DDP_INVALID_TAG    V_DDP_INVALID_TAG(1U)
+
+#define S_DDP_ULIMIT_ERR    23
+#define V_DDP_ULIMIT_ERR(x) ((x) << S_DDP_ULIMIT_ERR)
+#define F_DDP_ULIMIT_ERR    V_DDP_ULIMIT_ERR(1U)
+
+#define S_DDP_OFFSET_ERR    24
+#define V_DDP_OFFSET_ERR(x) ((x) << S_DDP_OFFSET_ERR)
+#define F_DDP_OFFSET_ERR    V_DDP_OFFSET_ERR(1U)
+
+#define S_DDP_COLOR_ERR    25
+#define V_DDP_COLOR_ERR(x) ((x) << S_DDP_COLOR_ERR)
+#define F_DDP_COLOR_ERR    V_DDP_COLOR_ERR(1U)
+
+#define S_DDP_TID_MISMATCH    26
+#define V_DDP_TID_MISMATCH(x) ((x) << S_DDP_TID_MISMATCH)
+#define F_DDP_TID_MISMATCH    V_DDP_TID_MISMATCH(1U)
+
+#define S_DDP_INVALID_PPOD    27
+#define V_DDP_INVALID_PPOD(x) ((x) << S_DDP_INVALID_PPOD)
+#define F_DDP_INVALID_PPOD    V_DDP_INVALID_PPOD(1U)
+
+#define S_DDP_ULP_MODE    28
+#define M_DDP_ULP_MODE    0xF
+#define V_DDP_ULP_MODE(x) ((x) << S_DDP_ULP_MODE)
+#define G_DDP_ULP_MODE(x) (((x) >> S_DDP_ULP_MODE) & M_DDP_ULP_MODE)
+
+/* cpl_rx_{data,iscsi,fcoe}_{ddp,dif}.ddp_report fields */
+#define S_DDP_OFFSET    0
+#define M_DDP_OFFSET    0xFFFFFF
+#define V_DDP_OFFSET(x) ((x) << S_DDP_OFFSET)
+#define G_DDP_OFFSET(x) (((x) >> S_DDP_OFFSET) & M_DDP_OFFSET)
+
+#define S_DDP_DACK_MODE    24
+#define M_DDP_DACK_MODE    0x3
+#define V_DDP_DACK_MODE(x) ((x) << S_DDP_DACK_MODE)
+#define G_DDP_DACK_MODE(x) (((x) >> S_DDP_DACK_MODE) & M_DDP_DACK_MODE)
+
+#define S_DDP_BUF_IDX    26
+#define V_DDP_BUF_IDX(x) ((x) << S_DDP_BUF_IDX)
+#define F_DDP_BUF_IDX    V_DDP_BUF_IDX(1U)
+
+#define S_DDP_URG    27
+#define V_DDP_URG(x) ((x) << S_DDP_URG)
+#define F_DDP_URG    V_DDP_URG(1U)
+
+#define S_DDP_PSH    28
+#define V_DDP_PSH(x) ((x) << S_DDP_PSH)
+#define F_DDP_PSH    V_DDP_PSH(1U)
+
+#define S_DDP_BUF_COMPLETE    29
+#define V_DDP_BUF_COMPLETE(x) ((x) << S_DDP_BUF_COMPLETE)
+#define F_DDP_BUF_COMPLETE    V_DDP_BUF_COMPLETE(1U)
+
+#define S_DDP_BUF_TIMED_OUT    30
+#define V_DDP_BUF_TIMED_OUT(x) ((x) << S_DDP_BUF_TIMED_OUT)
+#define F_DDP_BUF_TIMED_OUT    V_DDP_BUF_TIMED_OUT(1U)
+
+#define S_DDP_INV    31
+#define V_DDP_INV(x) ((x) << S_DDP_INV)
+#define F_DDP_INV    V_DDP_INV(1U)
+
+struct cpl_rx_pkt {
+	RSS_HDR
+	__u8 opcode;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 iff:4;
+	__u8 csum_calc:1;
+	__u8 ipmi_pkt:1;
+	__u8 vlan_ex:1;
+	__u8 ip_frag:1;
+#else
+	__u8 ip_frag:1;
+	__u8 vlan_ex:1;
+	__u8 ipmi_pkt:1;
+	__u8 csum_calc:1;
+	__u8 iff:4;
+#endif
+	__be16 csum;
+	__be16 vlan;
+	__be16 len;
+	__be32 l2info;
+	__be16 hdr_len;
+	__be16 err_vec;
+};
+
+/* rx_pkt.l2info fields */
+#define S_RX_ETHHDR_LEN    0
+#define M_RX_ETHHDR_LEN    0x1F
+#define V_RX_ETHHDR_LEN(x) ((x) << S_RX_ETHHDR_LEN)
+#define G_RX_ETHHDR_LEN(x) (((x) >> S_RX_ETHHDR_LEN) & M_RX_ETHHDR_LEN)
+
+#define S_RX_T5_ETHHDR_LEN    0
+#define M_RX_T5_ETHHDR_LEN    0x3F
+#define V_RX_T5_ETHHDR_LEN(x) ((x) << S_RX_T5_ETHHDR_LEN)
+#define G_RX_T5_ETHHDR_LEN(x) (((x) >> S_RX_T5_ETHHDR_LEN) & M_RX_T5_ETHHDR_LEN)
+
+#define S_RX_PKTYPE    5
+#define M_RX_PKTYPE    0x7
+#define V_RX_PKTYPE(x) ((x) << S_RX_PKTYPE)
+#define G_RX_PKTYPE(x) (((x) >> S_RX_PKTYPE) & M_RX_PKTYPE)
+
+#define S_RX_T5_DATYPE    6
+#define M_RX_T5_DATYPE    0x3
+#define V_RX_T5_DATYPE(x) ((x) << S_RX_T5_DATYPE)
+#define G_RX_T5_DATYPE(x) (((x) >> S_RX_T5_DATYPE) & M_RX_T5_DATYPE)
+
+#define S_RX_MACIDX    8
+#define M_RX_MACIDX    0x1FF
+#define V_RX_MACIDX(x) ((x) << S_RX_MACIDX)
+#define G_RX_MACIDX(x) (((x) >> S_RX_MACIDX) & M_RX_MACIDX)
+
+#define S_RX_T5_PKTYPE    17
+#define M_RX_T5_PKTYPE    0x7
+#define V_RX_T5_PKTYPE(x) ((x) << S_RX_T5_PKTYPE)
+#define G_RX_T5_PKTYPE(x) (((x) >> S_RX_T5_PKTYPE) & M_RX_T5_PKTYPE)
+
+#define S_RX_DATYPE    18
+#define M_RX_DATYPE    0x3
+#define V_RX_DATYPE(x) ((x) << S_RX_DATYPE)
+#define G_RX_DATYPE(x) (((x) >> S_RX_DATYPE) & M_RX_DATYPE)
+
+#define S_RXF_PSH    20
+#define V_RXF_PSH(x) ((x) << S_RXF_PSH)
+#define F_RXF_PSH    V_RXF_PSH(1U)
+
+#define S_RXF_SYN    21
+#define V_RXF_SYN(x) ((x) << S_RXF_SYN)
+#define F_RXF_SYN    V_RXF_SYN(1U)
+
+#define S_RXF_UDP    22
+#define V_RXF_UDP(x) ((x) << S_RXF_UDP)
+#define F_RXF_UDP    V_RXF_UDP(1U)
+
+#define S_RXF_TCP    23
+#define V_RXF_TCP(x) ((x) << S_RXF_TCP)
+#define F_RXF_TCP    V_RXF_TCP(1U)
+
+#define S_RXF_IP    24
+#define V_RXF_IP(x) ((x) << S_RXF_IP)
+#define F_RXF_IP    V_RXF_IP(1U)
+
+#define S_RXF_IP6    25
+#define V_RXF_IP6(x) ((x) << S_RXF_IP6)
+#define F_RXF_IP6    V_RXF_IP6(1U)
+
+#define S_RXF_SYN_COOKIE    26
+#define V_RXF_SYN_COOKIE(x) ((x) << S_RXF_SYN_COOKIE)
+#define F_RXF_SYN_COOKIE    V_RXF_SYN_COOKIE(1U)
+
+#define S_RXF_FCOE    26
+#define V_RXF_FCOE(x) ((x) << S_RXF_FCOE)
+#define F_RXF_FCOE    V_RXF_FCOE(1U)
+
+#define S_RXF_LRO    27
+#define V_RXF_LRO(x) ((x) << S_RXF_LRO)
+#define F_RXF_LRO    V_RXF_LRO(1U)
+
+#define S_RX_CHAN    28
+#define M_RX_CHAN    0xF
+#define V_RX_CHAN(x) ((x) << S_RX_CHAN)
+#define G_RX_CHAN(x) (((x) >> S_RX_CHAN) & M_RX_CHAN)
+
+/* rx_pkt.hdr_len fields */
+#define S_RX_TCPHDR_LEN    0
+#define M_RX_TCPHDR_LEN    0x3F
+#define V_RX_TCPHDR_LEN(x) ((x) << S_RX_TCPHDR_LEN)
+#define G_RX_TCPHDR_LEN(x) (((x) >> S_RX_TCPHDR_LEN) & M_RX_TCPHDR_LEN)
+
+#define S_RX_IPHDR_LEN    6
+#define M_RX_IPHDR_LEN    0x3FF
+#define V_RX_IPHDR_LEN(x) ((x) << S_RX_IPHDR_LEN)
+#define G_RX_IPHDR_LEN(x) (((x) >> S_RX_IPHDR_LEN) & M_RX_IPHDR_LEN)
+
+/* rx_pkt.err_vec fields */
+#define S_RXERR_OR    0
+#define V_RXERR_OR(x) ((x) << S_RXERR_OR)
+#define F_RXERR_OR    V_RXERR_OR(1U)
+
+#define S_RXERR_MAC    1
+#define V_RXERR_MAC(x) ((x) << S_RXERR_MAC)
+#define F_RXERR_MAC    V_RXERR_MAC(1U)
+
+#define S_RXERR_IPVERS    2
+#define V_RXERR_IPVERS(x) ((x) << S_RXERR_IPVERS)
+#define F_RXERR_IPVERS    V_RXERR_IPVERS(1U)
+
+#define S_RXERR_FRAG    3
+#define V_RXERR_FRAG(x) ((x) << S_RXERR_FRAG)
+#define F_RXERR_FRAG    V_RXERR_FRAG(1U)
+
+#define S_RXERR_ATTACK    4
+#define V_RXERR_ATTACK(x) ((x) << S_RXERR_ATTACK)
+#define F_RXERR_ATTACK    V_RXERR_ATTACK(1U)
+
+#define S_RXERR_ETHHDR_LEN    5
+#define V_RXERR_ETHHDR_LEN(x) ((x) << S_RXERR_ETHHDR_LEN)
+#define F_RXERR_ETHHDR_LEN    V_RXERR_ETHHDR_LEN(1U)
+
+#define S_RXERR_IPHDR_LEN    6
+#define V_RXERR_IPHDR_LEN(x) ((x) << S_RXERR_IPHDR_LEN)
+#define F_RXERR_IPHDR_LEN    V_RXERR_IPHDR_LEN(1U)
+
+#define S_RXERR_TCPHDR_LEN    7
+#define V_RXERR_TCPHDR_LEN(x) ((x) << S_RXERR_TCPHDR_LEN)
+#define F_RXERR_TCPHDR_LEN    V_RXERR_TCPHDR_LEN(1U)
+
+#define S_RXERR_PKT_LEN    8
+#define V_RXERR_PKT_LEN(x) ((x) << S_RXERR_PKT_LEN)
+#define F_RXERR_PKT_LEN    V_RXERR_PKT_LEN(1U)
+
+#define S_RXERR_TCP_OPT    9
+#define V_RXERR_TCP_OPT(x) ((x) << S_RXERR_TCP_OPT)
+#define F_RXERR_TCP_OPT    V_RXERR_TCP_OPT(1U)
+
+#define S_RXERR_IPCSUM    12
+#define V_RXERR_IPCSUM(x) ((x) << S_RXERR_IPCSUM)
+#define F_RXERR_IPCSUM    V_RXERR_IPCSUM(1U)
+
+#define S_RXERR_CSUM    13
+#define V_RXERR_CSUM(x) ((x) << S_RXERR_CSUM)
+#define F_RXERR_CSUM    V_RXERR_CSUM(1U)
+
+#define S_RXERR_PING    14
+#define V_RXERR_PING(x) ((x) << S_RXERR_PING)
+#define F_RXERR_PING    V_RXERR_PING(1U)
+
+struct cpl_trace_pkt {
+	RSS_HDR
+	__u8 opcode;
+	__u8 intf;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 runt:4;
+	__u8 filter_hit:4;
+	__u8 :6;
+	__u8 err:1;
+	__u8 trunc:1;
+#else
+	__u8 filter_hit:4;
+	__u8 runt:4;
+	__u8 trunc:1;
+	__u8 err:1;
+	__u8 :6;
+#endif
+	__be16 rsvd;
+	__be16 len;
+	__be64 tstamp;
+};
+
+struct cpl_t5_trace_pkt {
+	RSS_HDR
+	__u8 opcode;
+	__u8 intf;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 runt:4;
+	__u8 filter_hit:4;
+	__u8 :6;
+	__u8 err:1;
+	__u8 trunc:1;
+#else
+	__u8 filter_hit:4;
+	__u8 runt:4;
+	__u8 trunc:1;
+	__u8 err:1;
+	__u8 :6;
+#endif
+	__be16 rsvd;
+	__be16 len;
+	__be64 tstamp;
+	__be64 rsvd1;
+};
+
+struct cpl_rte_delete_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+};
+
+/* {cpl_rte_delete_req, cpl_rte_read_req}.params fields */
+#define S_RTE_REQ_LUT_IX    8
+#define M_RTE_REQ_LUT_IX    0x7FF
+#define V_RTE_REQ_LUT_IX(x) ((x) << S_RTE_REQ_LUT_IX)
+#define G_RTE_REQ_LUT_IX(x) (((x) >> S_RTE_REQ_LUT_IX) & M_RTE_REQ_LUT_IX)
+
+#define S_RTE_REQ_LUT_BASE    19
+#define M_RTE_REQ_LUT_BASE    0x7FF
+#define V_RTE_REQ_LUT_BASE(x) ((x) << S_RTE_REQ_LUT_BASE)
+#define G_RTE_REQ_LUT_BASE(x) (((x) >> S_RTE_REQ_LUT_BASE) & M_RTE_REQ_LUT_BASE)
+
+#define S_RTE_READ_REQ_SELECT    31
+#define V_RTE_READ_REQ_SELECT(x) ((x) << S_RTE_READ_REQ_SELECT)
+#define F_RTE_READ_REQ_SELECT    V_RTE_READ_REQ_SELECT(1U)
+
+struct cpl_rte_delete_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd[3];
+};
+
+struct cpl_rte_write_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__u32 write_sel;
+	__be32 lut_params;
+	__be32 l2t_idx;
+	__be32 netmask;
+	__be32 faddr;
+};
+
+/* cpl_rte_write_req.write_sel fields */
+#define S_RTE_WR_L2TIDX    31
+#define V_RTE_WR_L2TIDX(x) ((x) << S_RTE_WR_L2TIDX)
+#define F_RTE_WR_L2TIDX    V_RTE_WR_L2TIDX(1U)
+
+#define S_RTE_WR_FADDR    30
+#define V_RTE_WR_FADDR(x) ((x) << S_RTE_WR_FADDR)
+#define F_RTE_WR_FADDR    V_RTE_WR_FADDR(1U)
+
+/* cpl_rte_write_req.lut_params fields */
+#define S_RTE_WR_LUT_IX    10
+#define M_RTE_WR_LUT_IX    0x7FF
+#define V_RTE_WR_LUT_IX(x) ((x) << S_RTE_WR_LUT_IX)
+#define G_RTE_WR_LUT_IX(x) (((x) >> S_RTE_WR_LUT_IX) & M_RTE_WR_LUT_IX)
+
+#define S_RTE_WR_LUT_BASE    21
+#define M_RTE_WR_LUT_BASE    0x7FF
+#define V_RTE_WR_LUT_BASE(x) ((x) << S_RTE_WR_LUT_BASE)
+#define G_RTE_WR_LUT_BASE(x) (((x) >> S_RTE_WR_LUT_BASE) & M_RTE_WR_LUT_BASE)
+
+struct cpl_rte_write_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd[3];
+};
+
+struct cpl_rte_read_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+};
+
+struct cpl_rte_read_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd;
+	__be16 l2t_idx;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u32 :30;
+	__u32 select:1;
+#else
+	__u32 select:1;
+	__u32 :30;
+#endif
+	__be32 addr;
+};
+
+struct cpl_l2t_write_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 params;
+	__be16 l2t_idx;
+	__be16 vlan;
+	__u8   dst_mac[6];
+};
+
+/* cpl_l2t_write_req.params fields */
+#define S_L2T_W_INFO    2
+#define M_L2T_W_INFO    0x3F
+#define V_L2T_W_INFO(x) ((x) << S_L2T_W_INFO)
+#define G_L2T_W_INFO(x) (((x) >> S_L2T_W_INFO) & M_L2T_W_INFO)
+
+#define S_L2T_W_PORT    8
+#define M_L2T_W_PORT    0xF
+#define V_L2T_W_PORT(x) ((x) << S_L2T_W_PORT)
+#define G_L2T_W_PORT(x) (((x) >> S_L2T_W_PORT) & M_L2T_W_PORT)
+
+#define S_L2T_W_NOREPLY    15
+#define V_L2T_W_NOREPLY(x) ((x) << S_L2T_W_NOREPLY)
+#define F_L2T_W_NOREPLY    V_L2T_W_NOREPLY(1U)
+
+struct cpl_l2t_write_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd[3];
+};
+
+struct cpl_l2t_read_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 l2t_idx;
+};
+
+struct cpl_l2t_read_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 :4;
+	__u8 iff:4;
+#else
+	__u8 iff:4;
+	__u8 :4;
+#endif
+	__be16 vlan;
+	__be16 info;
+	__u8 dst_mac[6];
+};
+
+struct cpl_smt_write_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+	__be16 pfvf1;
+	__u8   src_mac1[6];
+	__be16 pfvf0;
+	__u8   src_mac0[6];
+};
+
+struct cpl_smt_write_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd[3];
+};
+
+struct cpl_smt_read_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+};
+
+struct cpl_smt_read_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8   status;
+	__u8   ovlan_idx;
+	__be16 rsvd;
+	__be16 pfvf1;
+	__u8   src_mac1[6];
+	__be16 pfvf0;
+	__u8   src_mac0[6];
+};
+
+/* cpl_smt_{read,write}_req.params fields */
+#define S_SMTW_OVLAN_IDX    16
+#define M_SMTW_OVLAN_IDX    0xF
+#define V_SMTW_OVLAN_IDX(x) ((x) << S_SMTW_OVLAN_IDX)
+#define G_SMTW_OVLAN_IDX(x) (((x) >> S_SMTW_OVLAN_IDX) & M_SMTW_OVLAN_IDX)
+
+#define S_SMTW_IDX    20
+#define M_SMTW_IDX    0x7F
+#define V_SMTW_IDX(x) ((x) << S_SMTW_IDX)
+#define G_SMTW_IDX(x) (((x) >> S_SMTW_IDX) & M_SMTW_IDX)
+
+#define S_SMTW_NORPL    31
+#define V_SMTW_NORPL(x) ((x) << S_SMTW_NORPL)
+#define F_SMTW_NORPL    V_SMTW_NORPL(1U)
+
+/* cpl_smt_{read,write}_req.pfvf? fields */
+#define S_SMTW_VF    0
+#define M_SMTW_VF    0xFF
+#define V_SMTW_VF(x) ((x) << S_SMTW_VF)
+#define G_SMTW_VF(x) (((x) >> S_SMTW_VF) & M_SMTW_VF)
+
+#define S_SMTW_PF    8
+#define M_SMTW_PF    0x7
+#define V_SMTW_PF(x) ((x) << S_SMTW_PF)
+#define G_SMTW_PF(x) (((x) >> S_SMTW_PF) & M_SMTW_PF)
+
+#define S_SMTW_VF_VLD    11
+#define V_SMTW_VF_VLD(x) ((x) << S_SMTW_VF_VLD)
+#define F_SMTW_VF_VLD    V_SMTW_VF_VLD(1U)
+
+struct cpl_tag_write_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+	__be64 tag_val;
+};
+
+struct cpl_tag_write_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 status;
+	__u8 rsvd[2];
+	__u8 idx;
+};
+
+struct cpl_tag_read_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be32 params;
+};
+
+struct cpl_tag_read_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8   status;
+#if defined(__LITTLE_ENDIAN_BITFIELD)
+	__u8 :4;
+	__u8 tag_len:1;
+	__u8 :2;
+	__u8 ins_enable:1;
+#else
+	__u8 ins_enable:1;
+	__u8 :2;
+	__u8 tag_len:1;
+	__u8 :4;
+#endif
+	__u8   rsvd;
+	__u8   tag_idx;
+	__be64 tag_val;
+};
+
+/* cpl_tag{read,write}_req.params fields */
+#define S_TAGW_IDX    0
+#define M_TAGW_IDX    0x7F
+#define V_TAGW_IDX(x) ((x) << S_TAGW_IDX)
+#define G_TAGW_IDX(x) (((x) >> S_TAGW_IDX) & M_TAGW_IDX)
+
+#define S_TAGW_LEN    20
+#define V_TAGW_LEN(x) ((x) << S_TAGW_LEN)
+#define F_TAGW_LEN    V_TAGW_LEN(1U)
+
+#define S_TAGW_INS_ENABLE    23
+#define V_TAGW_INS_ENABLE(x) ((x) << S_TAGW_INS_ENABLE)
+#define F_TAGW_INS_ENABLE    V_TAGW_INS_ENABLE(1U)
+
+#define S_TAGW_NORPL    31
+#define V_TAGW_NORPL(x) ((x) << S_TAGW_NORPL)
+#define F_TAGW_NORPL    V_TAGW_NORPL(1U)
+
+struct cpl_barrier {
+	WR_HDR;
+	__u8 opcode;
+	__u8 chan_map;
+	__be16 rsvd0;
+	__be32 rsvd1;
+};
+
+/* cpl_barrier.chan_map fields */
+#define S_CHAN_MAP    4
+#define M_CHAN_MAP    0xF
+#define V_CHAN_MAP(x) ((x) << S_CHAN_MAP)
+#define G_CHAN_MAP(x) (((x) >> S_CHAN_MAP) & M_CHAN_MAP)
+
+struct cpl_error {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 error;
+};
+
+struct cpl_hit_notify {
+	RSS_HDR
+	union opcode_tid ot;
+	__be32 rsvd;
+	__be32 info;
+	__be32 reason;
+};
+
+struct cpl_pkt_notify {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+	__be32 info;
+	__be32 reason;
+};
+
+/* cpl_{hit,pkt}_notify.info fields */
+#define S_NTFY_MAC_IDX    0
+#define M_NTFY_MAC_IDX    0x1FF
+#define V_NTFY_MAC_IDX(x) ((x) << S_NTFY_MAC_IDX)
+#define G_NTFY_MAC_IDX(x) (((x) >> S_NTFY_MAC_IDX) & M_NTFY_MAC_IDX)
+
+#define S_NTFY_INTF    10
+#define M_NTFY_INTF    0xF
+#define V_NTFY_INTF(x) ((x) << S_NTFY_INTF)
+#define G_NTFY_INTF(x) (((x) >> S_NTFY_INTF) & M_NTFY_INTF)
+
+#define S_NTFY_TCPHDR_LEN    14
+#define M_NTFY_TCPHDR_LEN    0xF
+#define V_NTFY_TCPHDR_LEN(x) ((x) << S_NTFY_TCPHDR_LEN)
+#define G_NTFY_TCPHDR_LEN(x) (((x) >> S_NTFY_TCPHDR_LEN) & M_NTFY_TCPHDR_LEN)
+
+#define S_NTFY_IPHDR_LEN    18
+#define M_NTFY_IPHDR_LEN    0x1FF
+#define V_NTFY_IPHDR_LEN(x) ((x) << S_NTFY_IPHDR_LEN)
+#define G_NTFY_IPHDR_LEN(x) (((x) >> S_NTFY_IPHDR_LEN) & M_NTFY_IPHDR_LEN)
+
+#define S_NTFY_ETHHDR_LEN    27
+#define M_NTFY_ETHHDR_LEN    0x1F
+#define V_NTFY_ETHHDR_LEN(x) ((x) << S_NTFY_ETHHDR_LEN)
+#define G_NTFY_ETHHDR_LEN(x) (((x) >> S_NTFY_ETHHDR_LEN) & M_NTFY_ETHHDR_LEN)
+
+#define S_NTFY_T5_IPHDR_LEN    18
+#define M_NTFY_T5_IPHDR_LEN    0xFF
+#define V_NTFY_T5_IPHDR_LEN(x) ((x) << S_NTFY_T5_IPHDR_LEN)
+#define G_NTFY_T5_IPHDR_LEN(x) (((x) >> S_NTFY_T5_IPHDR_LEN) & M_NTFY_T5_IPHDR_LEN)
+
+#define S_NTFY_T5_ETHHDR_LEN    26
+#define M_NTFY_T5_ETHHDR_LEN    0x3F
+#define V_NTFY_T5_ETHHDR_LEN(x) ((x) << S_NTFY_T5_ETHHDR_LEN)
+#define G_NTFY_T5_ETHHDR_LEN(x) (((x) >> S_NTFY_T5_ETHHDR_LEN) & M_NTFY_T5_ETHHDR_LEN)
+
+struct cpl_rdma_terminate {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 rsvd;
+	__be16 len;
+};
+
+struct cpl_set_le_req {
+	WR_HDR;
+	union opcode_tid ot;
+	__be16 reply_ctrl;
+	__be16 params;
+	__be64 mask_hi;
+	__be64 mask_lo;
+	__be64 val_hi;
+	__be64 val_lo;
+};
+
+/* cpl_set_le_req.reply_ctrl additional fields */
+#define S_LE_REQ_IP6    13
+#define V_LE_REQ_IP6(x) ((x) << S_LE_REQ_IP6)
+#define F_LE_REQ_IP6    V_LE_REQ_IP6(1U)
+
+/* cpl_set_le_req.params fields */
+#define S_LE_CHAN    0
+#define M_LE_CHAN    0x3
+#define V_LE_CHAN(x) ((x) << S_LE_CHAN)
+#define G_LE_CHAN(x) (((x) >> S_LE_CHAN) & M_LE_CHAN)
+
+#define S_LE_OFFSET    5
+#define M_LE_OFFSET    0x7
+#define V_LE_OFFSET(x) ((x) << S_LE_OFFSET)
+#define G_LE_OFFSET(x) (((x) >> S_LE_OFFSET) & M_LE_OFFSET)
+
+#define S_LE_MORE    8
+#define V_LE_MORE(x) ((x) << S_LE_MORE)
+#define F_LE_MORE    V_LE_MORE(1U)
+
+#define S_LE_REQSIZE    9
+#define M_LE_REQSIZE    0x7
+#define V_LE_REQSIZE(x) ((x) << S_LE_REQSIZE)
+#define G_LE_REQSIZE(x) (((x) >> S_LE_REQSIZE) & M_LE_REQSIZE)
+
+#define S_LE_REQCMD    12
+#define M_LE_REQCMD    0xF
+#define V_LE_REQCMD(x) ((x) << S_LE_REQCMD)
+#define G_LE_REQCMD(x) (((x) >> S_LE_REQCMD) & M_LE_REQCMD)
+
+struct cpl_set_le_rpl {
+	RSS_HDR
+	union opcode_tid ot;
+	__u8 chan;
+	__u8 info;
+	__be16 len;
+};
+
+/* cpl_set_le_rpl.info fields */
+#define S_LE_RSPCMD    0
+#define M_LE_RSPCMD    0xF
+#define V_LE_RSPCMD(x) ((x) << S_LE_RSPCMD)
+#define G_LE_RSPCMD(x) (((x) >> S_LE_RSPCMD) & M_LE_RSPCMD)
+
+#define S_LE_RSPSIZE    4
+#define M_LE_RSPSIZE    0x7
+#define V_LE_RSPSIZE(x) ((x) << S_LE_RSPSIZE)
+#define G_LE_RSPSIZE(x) (((x) >> S_LE_RSPSIZE) & M_LE_RSPSIZE)
+
+#define S_LE_RSPTYPE    7
+#define V_LE_RSPTYPE(x) ((x) << S_LE_RSPTYPE)
+#define F_LE_RSPTYPE    V_LE_RSPTYPE(1U)
+
+struct cpl_sge_egr_update {
+	RSS_HDR
+	__be32 opcode_qid;
+	__be16 cidx;
+	__be16 pidx;
+};
+
+/* cpl_sge_egr_update.ot fields */
+#define S_EGR_QID    0
+#define M_EGR_QID    0x1FFFF
+#define V_EGR_QID(x) ((x) << S_EGR_QID)
+#define G_EGR_QID(x) (((x) >> S_EGR_QID) & M_EGR_QID)
+
+struct cpl_fw2_pld {
+	RSS_HDR
+	u8 opcode;
+	u8 rsvd[5];
+	__be16 len;
+};
+
+struct cpl_fw4_pld {
+	RSS_HDR
+	u8 opcode;
+	u8 rsvd0[3];
+	u8 type;
+	u8 rsvd1;
+	__be16 len;
+	__be64 data;
+	__be64 rsvd2;
+};
+
+struct cpl_fw6_pld {
+	RSS_HDR
+	u8 opcode;
+	u8 rsvd[5];
+	__be16 len;
+	__be64 data[4];
+};
+
+struct cpl_fw2_msg {
+	RSS_HDR
+	union opcode_info oi;
+};
+
+struct cpl_fw4_msg {
+	RSS_HDR
+	u8 opcode;
+	u8 type;
+	__be16 rsvd0;
+	__be32 rsvd1;
+	__be64 data[2];
+};
+
+struct cpl_fw4_ack {
+	RSS_HDR
+	union opcode_tid ot;
+	u8 credits;
+	u8 rsvd0[2];
+	u8 flags;
+	__be32 snd_nxt;
+	__be32 snd_una;
+	__be64 rsvd1;
+};
+
+enum {
+	CPL_FW4_ACK_FLAGS_SEQVAL	= 0x1,	/* seqn valid */
+	CPL_FW4_ACK_FLAGS_CH		= 0x2,	/* channel change complete */
+	CPL_FW4_ACK_FLAGS_FLOWC		= 0x4,	/* fw_flowc_wr complete */
+};
+
+struct cpl_fw6_msg {
+	RSS_HDR
+	u8 opcode;
+	u8 type;
+	__be16 rsvd0;
+	__be32 rsvd1;
+	__be64 data[4];
+};
+
+/* cpl_fw6_msg.type values */
+enum {
+	FW6_TYPE_CMD_RPL = 0,
+	FW6_TYPE_WR_RPL = 1,
+	FW6_TYPE_CQE = 2,
+	FW6_TYPE_OFLD_CONNECTION_WR_RPL = 3,
+};
+
+struct cpl_fw6_msg_ofld_connection_wr_rpl {
+	__u64	cookie;
+	__be32	tid;	/* or atid in case of active failure */
+	__u8	t_state;
+	__u8	retval;
+	__u8	rsvd[2];
+};
+
+/* ULP_TX opcodes */
+enum {
+	ULP_TX_MEM_READ = 2,
+	ULP_TX_MEM_WRITE = 3,
+	ULP_TX_PKT = 4
+};
+
+enum {
+	ULP_TX_SC_NOOP = 0x80,
+	ULP_TX_SC_IMM  = 0x81,
+	ULP_TX_SC_DSGL = 0x82,
+	ULP_TX_SC_ISGL = 0x83
+};
+
+#define S_ULPTX_CMD    24
+#define M_ULPTX_CMD    0xFF
+#define V_ULPTX_CMD(x) ((x) << S_ULPTX_CMD)
+
+#define S_ULPTX_LEN16    0
+#define M_ULPTX_LEN16    0xFF
+#define V_ULPTX_LEN16(x) ((x) << S_ULPTX_LEN16)
+
+#define S_ULP_TX_SC_MORE 23
+#define V_ULP_TX_SC_MORE(x) ((x) << S_ULP_TX_SC_MORE)
+#define F_ULP_TX_SC_MORE  V_ULP_TX_SC_MORE(1U)
+
+struct ulptx_sge_pair {
+	__be32 len[2];
+	__be64 addr[2];
+};
+
+struct ulptx_sgl {
+	__be32 cmd_nsge;
+	__be32 len0;
+	__be64 addr0;
+
+#if !(defined C99_NOT_SUPPORTED)
+	struct ulptx_sge_pair sge[0];
+#endif
+
+};
+
+struct ulptx_isge {
+	__be32 stag;
+	__be32 len;
+	__be64 target_ofst;
+};
+
+struct ulptx_isgl {
+	__be32 cmd_nisge;
+	__be32 rsvd;
+
+#if !(defined C99_NOT_SUPPORTED)
+	struct ulptx_isge sge[0];
+#endif
+
+};
+
+struct ulptx_idata {
+	__be32 cmd_more;
+	__be32 len;
+};
+
+#define S_ULPTX_NSGE    0
+#define M_ULPTX_NSGE    0xFFFF
+#define V_ULPTX_NSGE(x) ((x) << S_ULPTX_NSGE)
+
+struct ulp_mem_io {
+	WR_HDR;
+	__be32 cmd;
+	__be32 len16;             /* command length */
+	__be32 dlen;              /* data length in 32-byte units */
+	__be32 lock_addr;
+};
+
+/* additional ulp_mem_io.cmd fields */
+#define S_ULP_MEMIO_ORDER    23
+#define V_ULP_MEMIO_ORDER(x) ((x) << S_ULP_MEMIO_ORDER)
+#define F_ULP_MEMIO_ORDER    V_ULP_MEMIO_ORDER(1U)
+
+/* ulp_mem_io.lock_addr fields */
+#define S_ULP_MEMIO_ADDR    0
+#define M_ULP_MEMIO_ADDR    0x7FFFFFF
+#define V_ULP_MEMIO_ADDR(x) ((x) << S_ULP_MEMIO_ADDR)
+
+#define S_ULP_MEMIO_LOCK    31
+#define V_ULP_MEMIO_LOCK(x) ((x) << S_ULP_MEMIO_LOCK)
+#define F_ULP_MEMIO_LOCK    V_ULP_MEMIO_LOCK(1U)
+
+/* ulp_mem_io.dlen fields */
+#define S_ULP_MEMIO_DATA_LEN    0
+#define M_ULP_MEMIO_DATA_LEN    0x1F
+#define V_ULP_MEMIO_DATA_LEN(x) ((x) << S_ULP_MEMIO_DATA_LEN)
+
+struct ulp_txpkt {
+	__be32 cmd_dest;
+	__be32 len;
+};
+
+/* ulp_txpkt.cmd_dest fields */
+#define S_ULP_TXPKT_DEST    16
+#define M_ULP_TXPKT_DEST    0x3
+#define V_ULP_TXPKT_DEST(x) ((x) << S_ULP_TXPKT_DEST)
+
+#define S_ULP_TXPKT_FID	    4
+#define M_ULP_TXPKT_FID     0x7ff
+#define V_ULP_TXPKT_FID(x)  ((x) << S_ULP_TXPKT_FID)
+
+#define S_ULP_TXPKT_RO      3
+#define V_ULP_TXPKT_RO(x) ((x) << S_ULP_TXPKT_RO)
+#define F_ULP_TXPKT_RO V_ULP_TXPKT_RO(1U)
+
+#endif  /* T4_MSG_H */
diff --git a/drivers/net/cxgb4/t4_regs.h b/drivers/net/cxgb4/t4_regs.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_regs.h
@@ -0,0 +1,23943 @@
+/* This file is automatically generated --- changes will be lost */
+
+#define MYPF_BASE 0x1b000
+#define MYPF_REG(reg_addr) (MYPF_BASE + (reg_addr))
+
+#define PF0_BASE 0x1e000
+#define PF0_REG(reg_addr) (PF0_BASE + (reg_addr))
+
+#define PF1_BASE 0x1e400
+#define PF1_REG(reg_addr) (PF1_BASE + (reg_addr))
+
+#define PF2_BASE 0x1e800
+#define PF2_REG(reg_addr) (PF2_BASE + (reg_addr))
+
+#define PF3_BASE 0x1ec00
+#define PF3_REG(reg_addr) (PF3_BASE + (reg_addr))
+
+#define PF4_BASE 0x1f000
+#define PF4_REG(reg_addr) (PF4_BASE + (reg_addr))
+
+#define PF5_BASE 0x1f400
+#define PF5_REG(reg_addr) (PF5_BASE + (reg_addr))
+
+#define PF6_BASE 0x1f800
+#define PF6_REG(reg_addr) (PF6_BASE + (reg_addr))
+
+#define PF7_BASE 0x1fc00
+#define PF7_REG(reg_addr) (PF7_BASE + (reg_addr))
+
+#define PF_STRIDE 0x400
+#define PF_BASE(idx) (PF0_BASE + (idx) * PF_STRIDE)
+#define PF_REG(idx, reg) (PF_BASE(idx) + (reg))
+
+#define MYPORT_BASE 0x1c000
+#define MYPORT_REG(reg_addr) (MYPORT_BASE + (reg_addr))
+
+#define PORT0_BASE 0x20000
+#define PORT0_REG(reg_addr) (PORT0_BASE + (reg_addr))
+
+#define PORT1_BASE 0x22000
+#define PORT1_REG(reg_addr) (PORT1_BASE + (reg_addr))
+
+#define PORT2_BASE 0x24000
+#define PORT2_REG(reg_addr) (PORT2_BASE + (reg_addr))
+
+#define PORT3_BASE 0x26000
+#define PORT3_REG(reg_addr) (PORT3_BASE + (reg_addr))
+
+#define PORT_STRIDE 0x2000
+#define PORT_BASE(idx) (PORT0_BASE + (idx) * PORT_STRIDE)
+#define PORT_REG(idx, reg) (PORT_BASE(idx) + (reg))
+
+#define VF_SGE_BASE 0x0
+#define VF_SGE_REG(reg_addr) (VF_SGE_BASE + (reg_addr))
+
+#define VF_MPS_BASE 0x100
+#define VF_MPS_REG(reg_addr) (VF_MPS_BASE + (reg_addr))
+
+#define VF_PL_BASE 0x200
+#define VF_PL_REG(reg_addr) (VF_PL_BASE + (reg_addr))
+
+#define VF_MBDATA_BASE 0x240
+#define VF_MBDATA_REG(reg_addr) (VF_MBDATA_BASE + (reg_addr))
+
+#define VF_CIM_BASE 0x300
+#define VF_CIM_REG(reg_addr) (VF_CIM_BASE + (reg_addr))
+
+#define EDC_STRIDE (EDC_1_BASE_ADDR - EDC_0_BASE_ADDR)
+#define EDC_REG(reg, idx) (reg + EDC_STRIDE * idx)
+
+#define SGE_QUEUE_BASE_MAP_HIGH(idx) (A_SGE_QUEUE_BASE_MAP_HIGH + (idx) * 8)
+#define NUM_SGE_QUEUE_BASE_MAP_HIGH_INSTANCES 136
+
+#define SGE_QUEUE_BASE_MAP_LOW(idx) (A_SGE_QUEUE_BASE_MAP_LOW + (idx) * 8)
+#define NUM_SGE_QUEUE_BASE_MAP_LOW_INSTANCES 136
+
+#define PCIE_DMA_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_DMA_INSTANCES 4
+
+#define PCIE_CMD_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_CMD_INSTANCES 2
+
+#define PCIE_HMA_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_HMA_INSTANCES 1
+
+#define PCIE_MEM_ACCESS_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_MEM_ACCESS_INSTANCES 8
+
+#define PCIE_MAILBOX_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_MAILBOX_INSTANCES 1
+
+#define PCIE_FW_REG(reg_addr, idx) ((reg_addr) + (idx) * 4)
+#define NUM_PCIE_FW_INSTANCES 8
+
+#define PCIE_FUNC_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_FUNC_INSTANCES 256
+
+#define PCIE_FID(idx) (A_PCIE_FID + (idx) * 4)
+#define NUM_PCIE_FID_INSTANCES 2048
+
+#define PCIE_DMA_BUF_REG(reg_addr, idx) ((reg_addr) + (idx) * 8)
+#define NUM_PCIE_DMA_BUF_INSTANCES 4
+
+#define MC_DDR3PHYDATX8_REG(reg_addr, idx) ((reg_addr) + (idx) * 256)
+#define NUM_MC_DDR3PHYDATX8_INSTANCES 9
+
+#define MC_BIST_STATUS_REG(reg_addr, idx) ((reg_addr) + (idx) * 4)
+#define NUM_MC_BIST_STATUS_INSTANCES 18
+
+#define EDC_BIST_STATUS_REG(reg_addr, idx) ((reg_addr) + (idx) * 4)
+#define NUM_EDC_BIST_STATUS_INSTANCES 18
+
+#define CIM_PF_MAILBOX_DATA(idx) (A_CIM_PF_MAILBOX_DATA + (idx) * 4)
+#define NUM_CIM_PF_MAILBOX_DATA_INSTANCES 16
+
+#define MPS_TRC_FILTER_MATCH_CTL_A(idx) (A_MPS_TRC_FILTER_MATCH_CTL_A + (idx) * 4)
+#define NUM_MPS_TRC_FILTER_MATCH_CTL_A_INSTANCES 4
+
+#define MPS_TRC_FILTER_MATCH_CTL_B(idx) (A_MPS_TRC_FILTER_MATCH_CTL_B + (idx) * 4)
+#define NUM_MPS_TRC_FILTER_MATCH_CTL_B_INSTANCES 4
+
+#define MPS_TRC_FILTER_RUNT_CTL(idx) (A_MPS_TRC_FILTER_RUNT_CTL + (idx) * 4)
+#define NUM_MPS_TRC_FILTER_RUNT_CTL_INSTANCES 4
+
+#define MPS_TRC_FILTER_DROP(idx) (A_MPS_TRC_FILTER_DROP + (idx) * 4)
+#define NUM_MPS_TRC_FILTER_DROP_INSTANCES 4
+
+#define MPS_TRC_FILTER0_MATCH(idx) (A_MPS_TRC_FILTER0_MATCH + (idx) * 4)
+#define NUM_MPS_TRC_FILTER0_MATCH_INSTANCES 28
+
+#define MPS_TRC_FILTER0_DONT_CARE(idx) (A_MPS_TRC_FILTER0_DONT_CARE + (idx) * 4)
+#define NUM_MPS_TRC_FILTER0_DONT_CARE_INSTANCES 28
+
+#define MPS_TRC_FILTER1_MATCH(idx) (A_MPS_TRC_FILTER1_MATCH + (idx) * 4)
+#define NUM_MPS_TRC_FILTER1_MATCH_INSTANCES 28
+
+#define MPS_TRC_FILTER1_DONT_CARE(idx) (A_MPS_TRC_FILTER1_DONT_CARE + (idx) * 4)
+#define NUM_MPS_TRC_FILTER1_DONT_CARE_INSTANCES 28
+
+#define MPS_TRC_FILTER2_MATCH(idx) (A_MPS_TRC_FILTER2_MATCH + (idx) * 4)
+#define NUM_MPS_TRC_FILTER2_MATCH_INSTANCES 28
+
+#define MPS_TRC_FILTER2_DONT_CARE(idx) (A_MPS_TRC_FILTER2_DONT_CARE + (idx) * 4)
+#define NUM_MPS_TRC_FILTER2_DONT_CARE_INSTANCES 28
+
+#define MPS_TRC_FILTER3_MATCH(idx) (A_MPS_TRC_FILTER3_MATCH + (idx) * 4)
+#define NUM_MPS_TRC_FILTER3_MATCH_INSTANCES 28
+
+#define MPS_TRC_FILTER3_DONT_CARE(idx) (A_MPS_TRC_FILTER3_DONT_CARE + (idx) * 4)
+#define NUM_MPS_TRC_FILTER3_DONT_CARE_INSTANCES 28
+
+#define MPS_PORT_CLS_HASH_SRAM(idx) (A_MPS_PORT_CLS_HASH_SRAM + (idx) * 4)
+#define NUM_MPS_PORT_CLS_HASH_SRAM_INSTANCES 65
+
+#define MPS_CLS_VLAN_TABLE(idx) (A_MPS_CLS_VLAN_TABLE + (idx) * 4)
+#define NUM_MPS_CLS_VLAN_TABLE_INSTANCES 9
+
+#define MPS_CLS_SRAM_L(idx) (A_MPS_CLS_SRAM_L + (idx) * 8)
+#define NUM_MPS_CLS_SRAM_L_INSTANCES 336
+
+#define MPS_CLS_SRAM_H(idx) (A_MPS_CLS_SRAM_H + (idx) * 8)
+#define NUM_MPS_CLS_SRAM_H_INSTANCES 336
+
+#define MPS_CLS_TCAM_Y_L(idx) (A_MPS_CLS_TCAM_Y_L + (idx) * 16)
+#define NUM_MPS_CLS_TCAM_Y_L_INSTANCES 512
+
+#define MPS_CLS_TCAM_Y_H(idx) (A_MPS_CLS_TCAM_Y_H + (idx) * 16)
+#define NUM_MPS_CLS_TCAM_Y_H_INSTANCES 512
+
+#define MPS_CLS_TCAM_X_L(idx) (A_MPS_CLS_TCAM_X_L + (idx) * 16)
+#define NUM_MPS_CLS_TCAM_X_L_INSTANCES 512
+
+#define MPS_CLS_TCAM_X_H(idx) (A_MPS_CLS_TCAM_X_H + (idx) * 16)
+#define NUM_MPS_CLS_TCAM_X_H_INSTANCES 512
+
+#define PL_SEMAPHORE_LOCK(idx) (A_PL_SEMAPHORE_LOCK + (idx) * 4)
+#define NUM_PL_SEMAPHORE_LOCK_INSTANCES 8
+
+#define PL_VF_SLICE_L(idx) (A_PL_VF_SLICE_L + (idx) * 8)
+#define NUM_PL_VF_SLICE_L_INSTANCES 8
+
+#define PL_VF_SLICE_H(idx) (A_PL_VF_SLICE_H + (idx) * 8)
+#define NUM_PL_VF_SLICE_H_INSTANCES 8
+
+#define PL_FLR_VF_STATUS(idx) (A_PL_FLR_VF_STATUS + (idx) * 4)
+#define NUM_PL_FLR_VF_STATUS_INSTANCES 4
+
+#define PL_VFID_MAP(idx) (A_PL_VFID_MAP + (idx) * 4)
+#define NUM_PL_VFID_MAP_INSTANCES 256
+
+#define LE_DB_MASK_IPV4(idx) (A_LE_DB_MASK_IPV4 + (idx) * 4)
+#define NUM_LE_DB_MASK_IPV4_INSTANCES 17
+
+#define LE_DB_MASK_IPV6(idx) (A_LE_DB_MASK_IPV6 + (idx) * 4)
+#define NUM_LE_DB_MASK_IPV6_INSTANCES 17
+
+#define LE_DB_DBGI_REQ_DATA(idx) (A_LE_DB_DBGI_REQ_DATA + (idx) * 4)
+#define NUM_LE_DB_DBGI_REQ_DATA_INSTANCES 17
+
+#define LE_DB_DBGI_REQ_MASK(idx) (A_LE_DB_DBGI_REQ_MASK + (idx) * 4)
+#define NUM_LE_DB_DBGI_REQ_MASK_INSTANCES 17
+
+#define LE_DB_DBGI_RSP_DATA(idx) (A_LE_DB_DBGI_RSP_DATA + (idx) * 4)
+#define NUM_LE_DB_DBGI_RSP_DATA_INSTANCES 17
+
+#define LE_DB_ACTIVE_MASK_IPV4(idx) (A_LE_DB_ACTIVE_MASK_IPV4 + (idx) * 4)
+#define NUM_LE_DB_ACTIVE_MASK_IPV4_INSTANCES 17
+
+#define LE_DB_ACTIVE_MASK_IPV6(idx) (A_LE_DB_ACTIVE_MASK_IPV6 + (idx) * 4)
+#define NUM_LE_DB_ACTIVE_MASK_IPV6_INSTANCES 17
+
+#define LE_HASH_MASK_GEN_IPV4(idx) (A_LE_HASH_MASK_GEN_IPV4 + (idx) * 4)
+#define NUM_LE_HASH_MASK_GEN_IPV4_INSTANCES 4
+
+#define LE_HASH_MASK_GEN_IPV6(idx) (A_LE_HASH_MASK_GEN_IPV6 + (idx) * 4)
+#define NUM_LE_HASH_MASK_GEN_IPV6_INSTANCES 12
+
+#define LE_HASH_MASK_CMP_IPV4(idx) (A_LE_HASH_MASK_CMP_IPV4 + (idx) * 4)
+#define NUM_LE_HASH_MASK_CMP_IPV4_INSTANCES 4
+
+#define LE_HASH_MASK_CMP_IPV6(idx) (A_LE_HASH_MASK_CMP_IPV6 + (idx) * 4)
+#define NUM_LE_HASH_MASK_CMP_IPV6_INSTANCES 12
+
+#define UP_TSCH_CHANNEL_REG(reg_addr, idx) ((reg_addr) + (idx) * 16)
+#define NUM_UP_TSCH_CHANNEL_INSTANCES 4
+
+#define CIM_CTL_MAILBOX_VF_STATUS(idx) (A_CIM_CTL_MAILBOX_VF_STATUS + (idx) * 4)
+#define NUM_CIM_CTL_MAILBOX_VF_STATUS_INSTANCES 4
+
+#define CIM_CTL_MAILBOX_VFN_CTL(idx) (A_CIM_CTL_MAILBOX_VFN_CTL + (idx) * 16)
+#define NUM_CIM_CTL_MAILBOX_VFN_CTL_INSTANCES 128
+
+#define CIM_CTL_TSCH_CHANNEL_REG(reg_addr, idx) ((reg_addr) + (idx) * 288)
+#define NUM_CIM_CTL_TSCH_CHANNEL_INSTANCES 4
+
+#define CIM_CTL_TSCH_CHANNEL_TSCH_CLASS_REG(reg_addr, idx) ((reg_addr) + (idx) * 16)
+#define NUM_CIM_CTL_TSCH_CHANNEL_TSCH_CLASS_INSTANCES 16
+
+/* registers for module SGE */
+#define SGE_BASE_ADDR 0x1000
+
+#define A_SGE_PF_KDOORBELL 0x0
+
+#define S_QID    15
+#define M_QID    0x1ffffU
+#define V_QID(x) ((x) << S_QID)
+#define G_QID(x) (((x) >> S_QID) & M_QID)
+
+#define S_DBPRIO    14
+#define V_DBPRIO(x) ((x) << S_DBPRIO)
+#define F_DBPRIO    V_DBPRIO(1U)
+
+#define S_PIDX    0
+#define M_PIDX    0x3fffU
+#define V_PIDX(x) ((x) << S_PIDX)
+#define G_PIDX(x) (((x) >> S_PIDX) & M_PIDX)
+
+#define A_SGE_VF_KDOORBELL 0x0
+#define A_SGE_PF_GTS 0x4
+
+#define S_INGRESSQID    16
+#define M_INGRESSQID    0xffffU
+#define V_INGRESSQID(x) ((x) << S_INGRESSQID)
+#define G_INGRESSQID(x) (((x) >> S_INGRESSQID) & M_INGRESSQID)
+
+#define S_TIMERREG    13
+#define M_TIMERREG    0x7U
+#define V_TIMERREG(x) ((x) << S_TIMERREG)
+#define G_TIMERREG(x) (((x) >> S_TIMERREG) & M_TIMERREG)
+
+#define S_SEINTARM    12
+#define V_SEINTARM(x) ((x) << S_SEINTARM)
+#define F_SEINTARM    V_SEINTARM(1U)
+
+#define S_CIDXINC    0
+#define M_CIDXINC    0xfffU
+#define V_CIDXINC(x) ((x) << S_CIDXINC)
+#define G_CIDXINC(x) (((x) >> S_CIDXINC) & M_CIDXINC)
+
+#define A_SGE_VF_GTS 0x4
+#define A_SGE_CONTROL 0x1008
+
+#define S_IGRALLCPLTOFL    31
+#define V_IGRALLCPLTOFL(x) ((x) << S_IGRALLCPLTOFL)
+#define F_IGRALLCPLTOFL    V_IGRALLCPLTOFL(1U)
+
+#define S_FLSPLITMIN    22
+#define M_FLSPLITMIN    0x1ffU
+#define V_FLSPLITMIN(x) ((x) << S_FLSPLITMIN)
+#define G_FLSPLITMIN(x) (((x) >> S_FLSPLITMIN) & M_FLSPLITMIN)
+
+#define S_FLSPLITMODE    20
+#define M_FLSPLITMODE    0x3U
+#define V_FLSPLITMODE(x) ((x) << S_FLSPLITMODE)
+#define G_FLSPLITMODE(x) (((x) >> S_FLSPLITMODE) & M_FLSPLITMODE)
+
+#define S_DCASYSTYPE    19
+#define V_DCASYSTYPE(x) ((x) << S_DCASYSTYPE)
+#define F_DCASYSTYPE    V_DCASYSTYPE(1U)
+
+#define S_RXPKTCPLMODE    18
+#define V_RXPKTCPLMODE(x) ((x) << S_RXPKTCPLMODE)
+#define F_RXPKTCPLMODE    V_RXPKTCPLMODE(1U)
+
+#define S_EGRSTATUSPAGESIZE    17
+#define V_EGRSTATUSPAGESIZE(x) ((x) << S_EGRSTATUSPAGESIZE)
+#define F_EGRSTATUSPAGESIZE    V_EGRSTATUSPAGESIZE(1U)
+
+#define S_INGHINTENABLE1    15
+#define V_INGHINTENABLE1(x) ((x) << S_INGHINTENABLE1)
+#define F_INGHINTENABLE1    V_INGHINTENABLE1(1U)
+
+#define S_INGHINTENABLE0    14
+#define V_INGHINTENABLE0(x) ((x) << S_INGHINTENABLE0)
+#define F_INGHINTENABLE0    V_INGHINTENABLE0(1U)
+
+#define S_INGINTCOMPAREIDX    13
+#define V_INGINTCOMPAREIDX(x) ((x) << S_INGINTCOMPAREIDX)
+#define F_INGINTCOMPAREIDX    V_INGINTCOMPAREIDX(1U)
+
+#define S_PKTSHIFT    10
+#define M_PKTSHIFT    0x7U
+#define V_PKTSHIFT(x) ((x) << S_PKTSHIFT)
+#define G_PKTSHIFT(x) (((x) >> S_PKTSHIFT) & M_PKTSHIFT)
+
+#define S_INGPCIEBOUNDARY    7
+#define M_INGPCIEBOUNDARY    0x7U
+#define V_INGPCIEBOUNDARY(x) ((x) << S_INGPCIEBOUNDARY)
+#define G_INGPCIEBOUNDARY(x) (((x) >> S_INGPCIEBOUNDARY) & M_INGPCIEBOUNDARY)
+
+#define S_INGPADBOUNDARY    4
+#define M_INGPADBOUNDARY    0x7U
+#define V_INGPADBOUNDARY(x) ((x) << S_INGPADBOUNDARY)
+#define G_INGPADBOUNDARY(x) (((x) >> S_INGPADBOUNDARY) & M_INGPADBOUNDARY)
+
+#define S_EGRPCIEBOUNDARY    1
+#define M_EGRPCIEBOUNDARY    0x7U
+#define V_EGRPCIEBOUNDARY(x) ((x) << S_EGRPCIEBOUNDARY)
+#define G_EGRPCIEBOUNDARY(x) (((x) >> S_EGRPCIEBOUNDARY) & M_EGRPCIEBOUNDARY)
+
+#define S_GLOBALENABLE    0
+#define V_GLOBALENABLE(x) ((x) << S_GLOBALENABLE)
+#define F_GLOBALENABLE    V_GLOBALENABLE(1U)
+
+#define A_SGE_HOST_PAGE_SIZE 0x100c
+
+#define S_HOSTPAGESIZEPF7    28
+#define M_HOSTPAGESIZEPF7    0xfU
+#define V_HOSTPAGESIZEPF7(x) ((x) << S_HOSTPAGESIZEPF7)
+#define G_HOSTPAGESIZEPF7(x) (((x) >> S_HOSTPAGESIZEPF7) & M_HOSTPAGESIZEPF7)
+
+#define S_HOSTPAGESIZEPF6    24
+#define M_HOSTPAGESIZEPF6    0xfU
+#define V_HOSTPAGESIZEPF6(x) ((x) << S_HOSTPAGESIZEPF6)
+#define G_HOSTPAGESIZEPF6(x) (((x) >> S_HOSTPAGESIZEPF6) & M_HOSTPAGESIZEPF6)
+
+#define S_HOSTPAGESIZEPF5    20
+#define M_HOSTPAGESIZEPF5    0xfU
+#define V_HOSTPAGESIZEPF5(x) ((x) << S_HOSTPAGESIZEPF5)
+#define G_HOSTPAGESIZEPF5(x) (((x) >> S_HOSTPAGESIZEPF5) & M_HOSTPAGESIZEPF5)
+
+#define S_HOSTPAGESIZEPF4    16
+#define M_HOSTPAGESIZEPF4    0xfU
+#define V_HOSTPAGESIZEPF4(x) ((x) << S_HOSTPAGESIZEPF4)
+#define G_HOSTPAGESIZEPF4(x) (((x) >> S_HOSTPAGESIZEPF4) & M_HOSTPAGESIZEPF4)
+
+#define S_HOSTPAGESIZEPF3    12
+#define M_HOSTPAGESIZEPF3    0xfU
+#define V_HOSTPAGESIZEPF3(x) ((x) << S_HOSTPAGESIZEPF3)
+#define G_HOSTPAGESIZEPF3(x) (((x) >> S_HOSTPAGESIZEPF3) & M_HOSTPAGESIZEPF3)
+
+#define S_HOSTPAGESIZEPF2    8
+#define M_HOSTPAGESIZEPF2    0xfU
+#define V_HOSTPAGESIZEPF2(x) ((x) << S_HOSTPAGESIZEPF2)
+#define G_HOSTPAGESIZEPF2(x) (((x) >> S_HOSTPAGESIZEPF2) & M_HOSTPAGESIZEPF2)
+
+#define S_HOSTPAGESIZEPF1    4
+#define M_HOSTPAGESIZEPF1    0xfU
+#define V_HOSTPAGESIZEPF1(x) ((x) << S_HOSTPAGESIZEPF1)
+#define G_HOSTPAGESIZEPF1(x) (((x) >> S_HOSTPAGESIZEPF1) & M_HOSTPAGESIZEPF1)
+
+#define S_HOSTPAGESIZEPF0    0
+#define M_HOSTPAGESIZEPF0    0xfU
+#define V_HOSTPAGESIZEPF0(x) ((x) << S_HOSTPAGESIZEPF0)
+#define G_HOSTPAGESIZEPF0(x) (((x) >> S_HOSTPAGESIZEPF0) & M_HOSTPAGESIZEPF0)
+
+#define A_SGE_EGRESS_QUEUES_PER_PAGE_PF 0x1010
+
+#define S_QUEUESPERPAGEPF7    28
+#define M_QUEUESPERPAGEPF7    0xfU
+#define V_QUEUESPERPAGEPF7(x) ((x) << S_QUEUESPERPAGEPF7)
+#define G_QUEUESPERPAGEPF7(x) (((x) >> S_QUEUESPERPAGEPF7) & M_QUEUESPERPAGEPF7)
+
+#define S_QUEUESPERPAGEPF6    24
+#define M_QUEUESPERPAGEPF6    0xfU
+#define V_QUEUESPERPAGEPF6(x) ((x) << S_QUEUESPERPAGEPF6)
+#define G_QUEUESPERPAGEPF6(x) (((x) >> S_QUEUESPERPAGEPF6) & M_QUEUESPERPAGEPF6)
+
+#define S_QUEUESPERPAGEPF5    20
+#define M_QUEUESPERPAGEPF5    0xfU
+#define V_QUEUESPERPAGEPF5(x) ((x) << S_QUEUESPERPAGEPF5)
+#define G_QUEUESPERPAGEPF5(x) (((x) >> S_QUEUESPERPAGEPF5) & M_QUEUESPERPAGEPF5)
+
+#define S_QUEUESPERPAGEPF4    16
+#define M_QUEUESPERPAGEPF4    0xfU
+#define V_QUEUESPERPAGEPF4(x) ((x) << S_QUEUESPERPAGEPF4)
+#define G_QUEUESPERPAGEPF4(x) (((x) >> S_QUEUESPERPAGEPF4) & M_QUEUESPERPAGEPF4)
+
+#define S_QUEUESPERPAGEPF3    12
+#define M_QUEUESPERPAGEPF3    0xfU
+#define V_QUEUESPERPAGEPF3(x) ((x) << S_QUEUESPERPAGEPF3)
+#define G_QUEUESPERPAGEPF3(x) (((x) >> S_QUEUESPERPAGEPF3) & M_QUEUESPERPAGEPF3)
+
+#define S_QUEUESPERPAGEPF2    8
+#define M_QUEUESPERPAGEPF2    0xfU
+#define V_QUEUESPERPAGEPF2(x) ((x) << S_QUEUESPERPAGEPF2)
+#define G_QUEUESPERPAGEPF2(x) (((x) >> S_QUEUESPERPAGEPF2) & M_QUEUESPERPAGEPF2)
+
+#define S_QUEUESPERPAGEPF1    4
+#define M_QUEUESPERPAGEPF1    0xfU
+#define V_QUEUESPERPAGEPF1(x) ((x) << S_QUEUESPERPAGEPF1)
+#define G_QUEUESPERPAGEPF1(x) (((x) >> S_QUEUESPERPAGEPF1) & M_QUEUESPERPAGEPF1)
+
+#define S_QUEUESPERPAGEPF0    0
+#define M_QUEUESPERPAGEPF0    0xfU
+#define V_QUEUESPERPAGEPF0(x) ((x) << S_QUEUESPERPAGEPF0)
+#define G_QUEUESPERPAGEPF0(x) (((x) >> S_QUEUESPERPAGEPF0) & M_QUEUESPERPAGEPF0)
+
+#define A_SGE_EGRESS_QUEUES_PER_PAGE_VF 0x1014
+
+#define S_QUEUESPERPAGEVFPF7    28
+#define M_QUEUESPERPAGEVFPF7    0xfU
+#define V_QUEUESPERPAGEVFPF7(x) ((x) << S_QUEUESPERPAGEVFPF7)
+#define G_QUEUESPERPAGEVFPF7(x) (((x) >> S_QUEUESPERPAGEVFPF7) & M_QUEUESPERPAGEVFPF7)
+
+#define S_QUEUESPERPAGEVFPF6    24
+#define M_QUEUESPERPAGEVFPF6    0xfU
+#define V_QUEUESPERPAGEVFPF6(x) ((x) << S_QUEUESPERPAGEVFPF6)
+#define G_QUEUESPERPAGEVFPF6(x) (((x) >> S_QUEUESPERPAGEVFPF6) & M_QUEUESPERPAGEVFPF6)
+
+#define S_QUEUESPERPAGEVFPF5    20
+#define M_QUEUESPERPAGEVFPF5    0xfU
+#define V_QUEUESPERPAGEVFPF5(x) ((x) << S_QUEUESPERPAGEVFPF5)
+#define G_QUEUESPERPAGEVFPF5(x) (((x) >> S_QUEUESPERPAGEVFPF5) & M_QUEUESPERPAGEVFPF5)
+
+#define S_QUEUESPERPAGEVFPF4    16
+#define M_QUEUESPERPAGEVFPF4    0xfU
+#define V_QUEUESPERPAGEVFPF4(x) ((x) << S_QUEUESPERPAGEVFPF4)
+#define G_QUEUESPERPAGEVFPF4(x) (((x) >> S_QUEUESPERPAGEVFPF4) & M_QUEUESPERPAGEVFPF4)
+
+#define S_QUEUESPERPAGEVFPF3    12
+#define M_QUEUESPERPAGEVFPF3    0xfU
+#define V_QUEUESPERPAGEVFPF3(x) ((x) << S_QUEUESPERPAGEVFPF3)
+#define G_QUEUESPERPAGEVFPF3(x) (((x) >> S_QUEUESPERPAGEVFPF3) & M_QUEUESPERPAGEVFPF3)
+
+#define S_QUEUESPERPAGEVFPF2    8
+#define M_QUEUESPERPAGEVFPF2    0xfU
+#define V_QUEUESPERPAGEVFPF2(x) ((x) << S_QUEUESPERPAGEVFPF2)
+#define G_QUEUESPERPAGEVFPF2(x) (((x) >> S_QUEUESPERPAGEVFPF2) & M_QUEUESPERPAGEVFPF2)
+
+#define S_QUEUESPERPAGEVFPF1    4
+#define M_QUEUESPERPAGEVFPF1    0xfU
+#define V_QUEUESPERPAGEVFPF1(x) ((x) << S_QUEUESPERPAGEVFPF1)
+#define G_QUEUESPERPAGEVFPF1(x) (((x) >> S_QUEUESPERPAGEVFPF1) & M_QUEUESPERPAGEVFPF1)
+
+#define S_QUEUESPERPAGEVFPF0    0
+#define M_QUEUESPERPAGEVFPF0    0xfU
+#define V_QUEUESPERPAGEVFPF0(x) ((x) << S_QUEUESPERPAGEVFPF0)
+#define G_QUEUESPERPAGEVFPF0(x) (((x) >> S_QUEUESPERPAGEVFPF0) & M_QUEUESPERPAGEVFPF0)
+
+#define A_SGE_USER_MODE_LIMITS 0x1018
+
+#define S_OPCODE_MIN    24
+#define M_OPCODE_MIN    0xffU
+#define V_OPCODE_MIN(x) ((x) << S_OPCODE_MIN)
+#define G_OPCODE_MIN(x) (((x) >> S_OPCODE_MIN) & M_OPCODE_MIN)
+
+#define S_OPCODE_MAX    16
+#define M_OPCODE_MAX    0xffU
+#define V_OPCODE_MAX(x) ((x) << S_OPCODE_MAX)
+#define G_OPCODE_MAX(x) (((x) >> S_OPCODE_MAX) & M_OPCODE_MAX)
+
+#define S_LENGTH_MIN    8
+#define M_LENGTH_MIN    0xffU
+#define V_LENGTH_MIN(x) ((x) << S_LENGTH_MIN)
+#define G_LENGTH_MIN(x) (((x) >> S_LENGTH_MIN) & M_LENGTH_MIN)
+
+#define S_LENGTH_MAX    0
+#define M_LENGTH_MAX    0xffU
+#define V_LENGTH_MAX(x) ((x) << S_LENGTH_MAX)
+#define G_LENGTH_MAX(x) (((x) >> S_LENGTH_MAX) & M_LENGTH_MAX)
+
+#define A_SGE_WR_ERROR 0x101c
+
+#define S_WR_ERROR_OPCODE    0
+#define M_WR_ERROR_OPCODE    0xffU
+#define V_WR_ERROR_OPCODE(x) ((x) << S_WR_ERROR_OPCODE)
+#define G_WR_ERROR_OPCODE(x) (((x) >> S_WR_ERROR_OPCODE) & M_WR_ERROR_OPCODE)
+
+#define A_SGE_PERR_INJECT 0x1020
+
+#define S_MEMSEL    1
+#define M_MEMSEL    0x1fU
+#define V_MEMSEL(x) ((x) << S_MEMSEL)
+#define G_MEMSEL(x) (((x) >> S_MEMSEL) & M_MEMSEL)
+
+#define S_INJECTDATAERR    0
+#define V_INJECTDATAERR(x) ((x) << S_INJECTDATAERR)
+#define F_INJECTDATAERR    V_INJECTDATAERR(1U)
+
+#define A_SGE_INT_CAUSE1 0x1024
+
+#define S_PERR_FLM_CREDITFIFO    30
+#define V_PERR_FLM_CREDITFIFO(x) ((x) << S_PERR_FLM_CREDITFIFO)
+#define F_PERR_FLM_CREDITFIFO    V_PERR_FLM_CREDITFIFO(1U)
+
+#define S_PERR_IMSG_HINT_FIFO    29
+#define V_PERR_IMSG_HINT_FIFO(x) ((x) << S_PERR_IMSG_HINT_FIFO)
+#define F_PERR_IMSG_HINT_FIFO    V_PERR_IMSG_HINT_FIFO(1U)
+
+#define S_PERR_MC_PC    28
+#define V_PERR_MC_PC(x) ((x) << S_PERR_MC_PC)
+#define F_PERR_MC_PC    V_PERR_MC_PC(1U)
+
+#define S_PERR_MC_IGR_CTXT    27
+#define V_PERR_MC_IGR_CTXT(x) ((x) << S_PERR_MC_IGR_CTXT)
+#define F_PERR_MC_IGR_CTXT    V_PERR_MC_IGR_CTXT(1U)
+
+#define S_PERR_MC_EGR_CTXT    26
+#define V_PERR_MC_EGR_CTXT(x) ((x) << S_PERR_MC_EGR_CTXT)
+#define F_PERR_MC_EGR_CTXT    V_PERR_MC_EGR_CTXT(1U)
+
+#define S_PERR_MC_FLM    25
+#define V_PERR_MC_FLM(x) ((x) << S_PERR_MC_FLM)
+#define F_PERR_MC_FLM    V_PERR_MC_FLM(1U)
+
+#define S_PERR_PC_MCTAG    24
+#define V_PERR_PC_MCTAG(x) ((x) << S_PERR_PC_MCTAG)
+#define F_PERR_PC_MCTAG    V_PERR_PC_MCTAG(1U)
+
+#define S_PERR_PC_CHPI_RSP1    23
+#define V_PERR_PC_CHPI_RSP1(x) ((x) << S_PERR_PC_CHPI_RSP1)
+#define F_PERR_PC_CHPI_RSP1    V_PERR_PC_CHPI_RSP1(1U)
+
+#define S_PERR_PC_CHPI_RSP0    22
+#define V_PERR_PC_CHPI_RSP0(x) ((x) << S_PERR_PC_CHPI_RSP0)
+#define F_PERR_PC_CHPI_RSP0    V_PERR_PC_CHPI_RSP0(1U)
+
+#define S_PERR_DBP_PC_RSP_FIFO3    21
+#define V_PERR_DBP_PC_RSP_FIFO3(x) ((x) << S_PERR_DBP_PC_RSP_FIFO3)
+#define F_PERR_DBP_PC_RSP_FIFO3    V_PERR_DBP_PC_RSP_FIFO3(1U)
+
+#define S_PERR_DBP_PC_RSP_FIFO2    20
+#define V_PERR_DBP_PC_RSP_FIFO2(x) ((x) << S_PERR_DBP_PC_RSP_FIFO2)
+#define F_PERR_DBP_PC_RSP_FIFO2    V_PERR_DBP_PC_RSP_FIFO2(1U)
+
+#define S_PERR_DBP_PC_RSP_FIFO1    19
+#define V_PERR_DBP_PC_RSP_FIFO1(x) ((x) << S_PERR_DBP_PC_RSP_FIFO1)
+#define F_PERR_DBP_PC_RSP_FIFO1    V_PERR_DBP_PC_RSP_FIFO1(1U)
+
+#define S_PERR_DBP_PC_RSP_FIFO0    18
+#define V_PERR_DBP_PC_RSP_FIFO0(x) ((x) << S_PERR_DBP_PC_RSP_FIFO0)
+#define F_PERR_DBP_PC_RSP_FIFO0    V_PERR_DBP_PC_RSP_FIFO0(1U)
+
+#define S_PERR_DMARBT    17
+#define V_PERR_DMARBT(x) ((x) << S_PERR_DMARBT)
+#define F_PERR_DMARBT    V_PERR_DMARBT(1U)
+
+#define S_PERR_FLM_DBPFIFO    16
+#define V_PERR_FLM_DBPFIFO(x) ((x) << S_PERR_FLM_DBPFIFO)
+#define F_PERR_FLM_DBPFIFO    V_PERR_FLM_DBPFIFO(1U)
+
+#define S_PERR_FLM_MCREQ_FIFO    15
+#define V_PERR_FLM_MCREQ_FIFO(x) ((x) << S_PERR_FLM_MCREQ_FIFO)
+#define F_PERR_FLM_MCREQ_FIFO    V_PERR_FLM_MCREQ_FIFO(1U)
+
+#define S_PERR_FLM_HINTFIFO    14
+#define V_PERR_FLM_HINTFIFO(x) ((x) << S_PERR_FLM_HINTFIFO)
+#define F_PERR_FLM_HINTFIFO    V_PERR_FLM_HINTFIFO(1U)
+
+#define S_PERR_ALIGN_CTL_FIFO3    13
+#define V_PERR_ALIGN_CTL_FIFO3(x) ((x) << S_PERR_ALIGN_CTL_FIFO3)
+#define F_PERR_ALIGN_CTL_FIFO3    V_PERR_ALIGN_CTL_FIFO3(1U)
+
+#define S_PERR_ALIGN_CTL_FIFO2    12
+#define V_PERR_ALIGN_CTL_FIFO2(x) ((x) << S_PERR_ALIGN_CTL_FIFO2)
+#define F_PERR_ALIGN_CTL_FIFO2    V_PERR_ALIGN_CTL_FIFO2(1U)
+
+#define S_PERR_ALIGN_CTL_FIFO1    11
+#define V_PERR_ALIGN_CTL_FIFO1(x) ((x) << S_PERR_ALIGN_CTL_FIFO1)
+#define F_PERR_ALIGN_CTL_FIFO1    V_PERR_ALIGN_CTL_FIFO1(1U)
+
+#define S_PERR_ALIGN_CTL_FIFO0    10
+#define V_PERR_ALIGN_CTL_FIFO0(x) ((x) << S_PERR_ALIGN_CTL_FIFO0)
+#define F_PERR_ALIGN_CTL_FIFO0    V_PERR_ALIGN_CTL_FIFO0(1U)
+
+#define S_PERR_EDMA_FIFO3    9
+#define V_PERR_EDMA_FIFO3(x) ((x) << S_PERR_EDMA_FIFO3)
+#define F_PERR_EDMA_FIFO3    V_PERR_EDMA_FIFO3(1U)
+
+#define S_PERR_EDMA_FIFO2    8
+#define V_PERR_EDMA_FIFO2(x) ((x) << S_PERR_EDMA_FIFO2)
+#define F_PERR_EDMA_FIFO2    V_PERR_EDMA_FIFO2(1U)
+
+#define S_PERR_EDMA_FIFO1    7
+#define V_PERR_EDMA_FIFO1(x) ((x) << S_PERR_EDMA_FIFO1)
+#define F_PERR_EDMA_FIFO1    V_PERR_EDMA_FIFO1(1U)
+
+#define S_PERR_EDMA_FIFO0    6
+#define V_PERR_EDMA_FIFO0(x) ((x) << S_PERR_EDMA_FIFO0)
+#define F_PERR_EDMA_FIFO0    V_PERR_EDMA_FIFO0(1U)
+
+#define S_PERR_PD_FIFO3    5
+#define V_PERR_PD_FIFO3(x) ((x) << S_PERR_PD_FIFO3)
+#define F_PERR_PD_FIFO3    V_PERR_PD_FIFO3(1U)
+
+#define S_PERR_PD_FIFO2    4
+#define V_PERR_PD_FIFO2(x) ((x) << S_PERR_PD_FIFO2)
+#define F_PERR_PD_FIFO2    V_PERR_PD_FIFO2(1U)
+
+#define S_PERR_PD_FIFO1    3
+#define V_PERR_PD_FIFO1(x) ((x) << S_PERR_PD_FIFO1)
+#define F_PERR_PD_FIFO1    V_PERR_PD_FIFO1(1U)
+
+#define S_PERR_PD_FIFO0    2
+#define V_PERR_PD_FIFO0(x) ((x) << S_PERR_PD_FIFO0)
+#define F_PERR_PD_FIFO0    V_PERR_PD_FIFO0(1U)
+
+#define S_PERR_ING_CTXT_MIFRSP    1
+#define V_PERR_ING_CTXT_MIFRSP(x) ((x) << S_PERR_ING_CTXT_MIFRSP)
+#define F_PERR_ING_CTXT_MIFRSP    V_PERR_ING_CTXT_MIFRSP(1U)
+
+#define S_PERR_EGR_CTXT_MIFRSP    0
+#define V_PERR_EGR_CTXT_MIFRSP(x) ((x) << S_PERR_EGR_CTXT_MIFRSP)
+#define F_PERR_EGR_CTXT_MIFRSP    V_PERR_EGR_CTXT_MIFRSP(1U)
+
+#define A_SGE_INT_ENABLE1 0x1028
+#define A_SGE_PERR_ENABLE1 0x102c
+#define A_SGE_INT_CAUSE2 0x1030
+
+#define S_PERR_HINT_DELAY_FIFO1    30
+#define V_PERR_HINT_DELAY_FIFO1(x) ((x) << S_PERR_HINT_DELAY_FIFO1)
+#define F_PERR_HINT_DELAY_FIFO1    V_PERR_HINT_DELAY_FIFO1(1U)
+
+#define S_PERR_HINT_DELAY_FIFO0    29
+#define V_PERR_HINT_DELAY_FIFO0(x) ((x) << S_PERR_HINT_DELAY_FIFO0)
+#define F_PERR_HINT_DELAY_FIFO0    V_PERR_HINT_DELAY_FIFO0(1U)
+
+#define S_PERR_IMSG_PD_FIFO    28
+#define V_PERR_IMSG_PD_FIFO(x) ((x) << S_PERR_IMSG_PD_FIFO)
+#define F_PERR_IMSG_PD_FIFO    V_PERR_IMSG_PD_FIFO(1U)
+
+#define S_PERR_ULPTX_FIFO1    27
+#define V_PERR_ULPTX_FIFO1(x) ((x) << S_PERR_ULPTX_FIFO1)
+#define F_PERR_ULPTX_FIFO1    V_PERR_ULPTX_FIFO1(1U)
+
+#define S_PERR_ULPTX_FIFO0    26
+#define V_PERR_ULPTX_FIFO0(x) ((x) << S_PERR_ULPTX_FIFO0)
+#define F_PERR_ULPTX_FIFO0    V_PERR_ULPTX_FIFO0(1U)
+
+#define S_PERR_IDMA2IMSG_FIFO1    25
+#define V_PERR_IDMA2IMSG_FIFO1(x) ((x) << S_PERR_IDMA2IMSG_FIFO1)
+#define F_PERR_IDMA2IMSG_FIFO1    V_PERR_IDMA2IMSG_FIFO1(1U)
+
+#define S_PERR_IDMA2IMSG_FIFO0    24
+#define V_PERR_IDMA2IMSG_FIFO0(x) ((x) << S_PERR_IDMA2IMSG_FIFO0)
+#define F_PERR_IDMA2IMSG_FIFO0    V_PERR_IDMA2IMSG_FIFO0(1U)
+
+#define S_PERR_HEADERSPLIT_FIFO1    23
+#define V_PERR_HEADERSPLIT_FIFO1(x) ((x) << S_PERR_HEADERSPLIT_FIFO1)
+#define F_PERR_HEADERSPLIT_FIFO1    V_PERR_HEADERSPLIT_FIFO1(1U)
+
+#define S_PERR_HEADERSPLIT_FIFO0    22
+#define V_PERR_HEADERSPLIT_FIFO0(x) ((x) << S_PERR_HEADERSPLIT_FIFO0)
+#define F_PERR_HEADERSPLIT_FIFO0    V_PERR_HEADERSPLIT_FIFO0(1U)
+
+#define S_PERR_ESWITCH_FIFO3    21
+#define V_PERR_ESWITCH_FIFO3(x) ((x) << S_PERR_ESWITCH_FIFO3)
+#define F_PERR_ESWITCH_FIFO3    V_PERR_ESWITCH_FIFO3(1U)
+
+#define S_PERR_ESWITCH_FIFO2    20
+#define V_PERR_ESWITCH_FIFO2(x) ((x) << S_PERR_ESWITCH_FIFO2)
+#define F_PERR_ESWITCH_FIFO2    V_PERR_ESWITCH_FIFO2(1U)
+
+#define S_PERR_ESWITCH_FIFO1    19
+#define V_PERR_ESWITCH_FIFO1(x) ((x) << S_PERR_ESWITCH_FIFO1)
+#define F_PERR_ESWITCH_FIFO1    V_PERR_ESWITCH_FIFO1(1U)
+
+#define S_PERR_ESWITCH_FIFO0    18
+#define V_PERR_ESWITCH_FIFO0(x) ((x) << S_PERR_ESWITCH_FIFO0)
+#define F_PERR_ESWITCH_FIFO0    V_PERR_ESWITCH_FIFO0(1U)
+
+#define S_PERR_PC_DBP1    17
+#define V_PERR_PC_DBP1(x) ((x) << S_PERR_PC_DBP1)
+#define F_PERR_PC_DBP1    V_PERR_PC_DBP1(1U)
+
+#define S_PERR_PC_DBP0    16
+#define V_PERR_PC_DBP0(x) ((x) << S_PERR_PC_DBP0)
+#define F_PERR_PC_DBP0    V_PERR_PC_DBP0(1U)
+
+#define S_PERR_IMSG_OB_FIFO    15
+#define V_PERR_IMSG_OB_FIFO(x) ((x) << S_PERR_IMSG_OB_FIFO)
+#define F_PERR_IMSG_OB_FIFO    V_PERR_IMSG_OB_FIFO(1U)
+
+#define S_PERR_CONM_SRAM    14
+#define V_PERR_CONM_SRAM(x) ((x) << S_PERR_CONM_SRAM)
+#define F_PERR_CONM_SRAM    V_PERR_CONM_SRAM(1U)
+
+#define S_PERR_PC_MC_RSP    13
+#define V_PERR_PC_MC_RSP(x) ((x) << S_PERR_PC_MC_RSP)
+#define F_PERR_PC_MC_RSP    V_PERR_PC_MC_RSP(1U)
+
+#define S_PERR_ISW_IDMA0_FIFO    12
+#define V_PERR_ISW_IDMA0_FIFO(x) ((x) << S_PERR_ISW_IDMA0_FIFO)
+#define F_PERR_ISW_IDMA0_FIFO    V_PERR_ISW_IDMA0_FIFO(1U)
+
+#define S_PERR_ISW_IDMA1_FIFO    11
+#define V_PERR_ISW_IDMA1_FIFO(x) ((x) << S_PERR_ISW_IDMA1_FIFO)
+#define F_PERR_ISW_IDMA1_FIFO    V_PERR_ISW_IDMA1_FIFO(1U)
+
+#define S_PERR_ISW_DBP_FIFO    10
+#define V_PERR_ISW_DBP_FIFO(x) ((x) << S_PERR_ISW_DBP_FIFO)
+#define F_PERR_ISW_DBP_FIFO    V_PERR_ISW_DBP_FIFO(1U)
+
+#define S_PERR_ISW_GTS_FIFO    9
+#define V_PERR_ISW_GTS_FIFO(x) ((x) << S_PERR_ISW_GTS_FIFO)
+#define F_PERR_ISW_GTS_FIFO    V_PERR_ISW_GTS_FIFO(1U)
+
+#define S_PERR_ITP_EVR    8
+#define V_PERR_ITP_EVR(x) ((x) << S_PERR_ITP_EVR)
+#define F_PERR_ITP_EVR    V_PERR_ITP_EVR(1U)
+
+#define S_PERR_FLM_CNTXMEM    7
+#define V_PERR_FLM_CNTXMEM(x) ((x) << S_PERR_FLM_CNTXMEM)
+#define F_PERR_FLM_CNTXMEM    V_PERR_FLM_CNTXMEM(1U)
+
+#define S_PERR_FLM_L1CACHE    6
+#define V_PERR_FLM_L1CACHE(x) ((x) << S_PERR_FLM_L1CACHE)
+#define F_PERR_FLM_L1CACHE    V_PERR_FLM_L1CACHE(1U)
+
+#define S_PERR_DBP_HINT_FIFO    5
+#define V_PERR_DBP_HINT_FIFO(x) ((x) << S_PERR_DBP_HINT_FIFO)
+#define F_PERR_DBP_HINT_FIFO    V_PERR_DBP_HINT_FIFO(1U)
+
+#define S_PERR_DBP_HP_FIFO    4
+#define V_PERR_DBP_HP_FIFO(x) ((x) << S_PERR_DBP_HP_FIFO)
+#define F_PERR_DBP_HP_FIFO    V_PERR_DBP_HP_FIFO(1U)
+
+#define S_PERR_DBP_LP_FIFO    3
+#define V_PERR_DBP_LP_FIFO(x) ((x) << S_PERR_DBP_LP_FIFO)
+#define F_PERR_DBP_LP_FIFO    V_PERR_DBP_LP_FIFO(1U)
+
+#define S_PERR_ING_CTXT_CACHE    2
+#define V_PERR_ING_CTXT_CACHE(x) ((x) << S_PERR_ING_CTXT_CACHE)
+#define F_PERR_ING_CTXT_CACHE    V_PERR_ING_CTXT_CACHE(1U)
+
+#define S_PERR_EGR_CTXT_CACHE    1
+#define V_PERR_EGR_CTXT_CACHE(x) ((x) << S_PERR_EGR_CTXT_CACHE)
+#define F_PERR_EGR_CTXT_CACHE    V_PERR_EGR_CTXT_CACHE(1U)
+
+#define S_PERR_BASE_SIZE    0
+#define V_PERR_BASE_SIZE(x) ((x) << S_PERR_BASE_SIZE)
+#define F_PERR_BASE_SIZE    V_PERR_BASE_SIZE(1U)
+
+#define A_SGE_INT_ENABLE2 0x1034
+#define A_SGE_PERR_ENABLE2 0x1038
+#define A_SGE_INT_CAUSE3 0x103c
+
+#define S_ERR_FLM_DBP    31
+#define V_ERR_FLM_DBP(x) ((x) << S_ERR_FLM_DBP)
+#define F_ERR_FLM_DBP    V_ERR_FLM_DBP(1U)
+
+#define S_ERR_FLM_IDMA1    30
+#define V_ERR_FLM_IDMA1(x) ((x) << S_ERR_FLM_IDMA1)
+#define F_ERR_FLM_IDMA1    V_ERR_FLM_IDMA1(1U)
+
+#define S_ERR_FLM_IDMA0    29
+#define V_ERR_FLM_IDMA0(x) ((x) << S_ERR_FLM_IDMA0)
+#define F_ERR_FLM_IDMA0    V_ERR_FLM_IDMA0(1U)
+
+#define S_ERR_FLM_HINT    28
+#define V_ERR_FLM_HINT(x) ((x) << S_ERR_FLM_HINT)
+#define F_ERR_FLM_HINT    V_ERR_FLM_HINT(1U)
+
+#define S_ERR_PCIE_ERROR3    27
+#define V_ERR_PCIE_ERROR3(x) ((x) << S_ERR_PCIE_ERROR3)
+#define F_ERR_PCIE_ERROR3    V_ERR_PCIE_ERROR3(1U)
+
+#define S_ERR_PCIE_ERROR2    26
+#define V_ERR_PCIE_ERROR2(x) ((x) << S_ERR_PCIE_ERROR2)
+#define F_ERR_PCIE_ERROR2    V_ERR_PCIE_ERROR2(1U)
+
+#define S_ERR_PCIE_ERROR1    25
+#define V_ERR_PCIE_ERROR1(x) ((x) << S_ERR_PCIE_ERROR1)
+#define F_ERR_PCIE_ERROR1    V_ERR_PCIE_ERROR1(1U)
+
+#define S_ERR_PCIE_ERROR0    24
+#define V_ERR_PCIE_ERROR0(x) ((x) << S_ERR_PCIE_ERROR0)
+#define F_ERR_PCIE_ERROR0    V_ERR_PCIE_ERROR0(1U)
+
+#define S_ERR_TIMER_ABOVE_MAX_QID    23
+#define V_ERR_TIMER_ABOVE_MAX_QID(x) ((x) << S_ERR_TIMER_ABOVE_MAX_QID)
+#define F_ERR_TIMER_ABOVE_MAX_QID    V_ERR_TIMER_ABOVE_MAX_QID(1U)
+
+#define S_ERR_CPL_EXCEED_IQE_SIZE    22
+#define V_ERR_CPL_EXCEED_IQE_SIZE(x) ((x) << S_ERR_CPL_EXCEED_IQE_SIZE)
+#define F_ERR_CPL_EXCEED_IQE_SIZE    V_ERR_CPL_EXCEED_IQE_SIZE(1U)
+
+#define S_ERR_INVALID_CIDX_INC    21
+#define V_ERR_INVALID_CIDX_INC(x) ((x) << S_ERR_INVALID_CIDX_INC)
+#define F_ERR_INVALID_CIDX_INC    V_ERR_INVALID_CIDX_INC(1U)
+
+#define S_ERR_ITP_TIME_PAUSED    20
+#define V_ERR_ITP_TIME_PAUSED(x) ((x) << S_ERR_ITP_TIME_PAUSED)
+#define F_ERR_ITP_TIME_PAUSED    V_ERR_ITP_TIME_PAUSED(1U)
+
+#define S_ERR_CPL_OPCODE_0    19
+#define V_ERR_CPL_OPCODE_0(x) ((x) << S_ERR_CPL_OPCODE_0)
+#define F_ERR_CPL_OPCODE_0    V_ERR_CPL_OPCODE_0(1U)
+
+#define S_ERR_DROPPED_DB    18
+#define V_ERR_DROPPED_DB(x) ((x) << S_ERR_DROPPED_DB)
+#define F_ERR_DROPPED_DB    V_ERR_DROPPED_DB(1U)
+
+#define S_ERR_DATA_CPL_ON_HIGH_QID1    17
+#define V_ERR_DATA_CPL_ON_HIGH_QID1(x) ((x) << S_ERR_DATA_CPL_ON_HIGH_QID1)
+#define F_ERR_DATA_CPL_ON_HIGH_QID1    V_ERR_DATA_CPL_ON_HIGH_QID1(1U)
+
+#define S_ERR_DATA_CPL_ON_HIGH_QID0    16
+#define V_ERR_DATA_CPL_ON_HIGH_QID0(x) ((x) << S_ERR_DATA_CPL_ON_HIGH_QID0)
+#define F_ERR_DATA_CPL_ON_HIGH_QID0    V_ERR_DATA_CPL_ON_HIGH_QID0(1U)
+
+#define S_ERR_BAD_DB_PIDX3    15
+#define V_ERR_BAD_DB_PIDX3(x) ((x) << S_ERR_BAD_DB_PIDX3)
+#define F_ERR_BAD_DB_PIDX3    V_ERR_BAD_DB_PIDX3(1U)
+
+#define S_ERR_BAD_DB_PIDX2    14
+#define V_ERR_BAD_DB_PIDX2(x) ((x) << S_ERR_BAD_DB_PIDX2)
+#define F_ERR_BAD_DB_PIDX2    V_ERR_BAD_DB_PIDX2(1U)
+
+#define S_ERR_BAD_DB_PIDX1    13
+#define V_ERR_BAD_DB_PIDX1(x) ((x) << S_ERR_BAD_DB_PIDX1)
+#define F_ERR_BAD_DB_PIDX1    V_ERR_BAD_DB_PIDX1(1U)
+
+#define S_ERR_BAD_DB_PIDX0    12
+#define V_ERR_BAD_DB_PIDX0(x) ((x) << S_ERR_BAD_DB_PIDX0)
+#define F_ERR_BAD_DB_PIDX0    V_ERR_BAD_DB_PIDX0(1U)
+
+#define S_ERR_ING_PCIE_CHAN    11
+#define V_ERR_ING_PCIE_CHAN(x) ((x) << S_ERR_ING_PCIE_CHAN)
+#define F_ERR_ING_PCIE_CHAN    V_ERR_ING_PCIE_CHAN(1U)
+
+#define S_ERR_ING_CTXT_PRIO    10
+#define V_ERR_ING_CTXT_PRIO(x) ((x) << S_ERR_ING_CTXT_PRIO)
+#define F_ERR_ING_CTXT_PRIO    V_ERR_ING_CTXT_PRIO(1U)
+
+#define S_ERR_EGR_CTXT_PRIO    9
+#define V_ERR_EGR_CTXT_PRIO(x) ((x) << S_ERR_EGR_CTXT_PRIO)
+#define F_ERR_EGR_CTXT_PRIO    V_ERR_EGR_CTXT_PRIO(1U)
+
+#define S_DBFIFO_HP_INT    8
+#define V_DBFIFO_HP_INT(x) ((x) << S_DBFIFO_HP_INT)
+#define F_DBFIFO_HP_INT    V_DBFIFO_HP_INT(1U)
+
+#define S_DBFIFO_LP_INT    7
+#define V_DBFIFO_LP_INT(x) ((x) << S_DBFIFO_LP_INT)
+#define F_DBFIFO_LP_INT    V_DBFIFO_LP_INT(1U)
+
+#define S_REG_ADDRESS_ERR    6
+#define V_REG_ADDRESS_ERR(x) ((x) << S_REG_ADDRESS_ERR)
+#define F_REG_ADDRESS_ERR    V_REG_ADDRESS_ERR(1U)
+
+#define S_INGRESS_SIZE_ERR    5
+#define V_INGRESS_SIZE_ERR(x) ((x) << S_INGRESS_SIZE_ERR)
+#define F_INGRESS_SIZE_ERR    V_INGRESS_SIZE_ERR(1U)
+
+#define S_EGRESS_SIZE_ERR    4
+#define V_EGRESS_SIZE_ERR(x) ((x) << S_EGRESS_SIZE_ERR)
+#define F_EGRESS_SIZE_ERR    V_EGRESS_SIZE_ERR(1U)
+
+#define S_ERR_INV_CTXT3    3
+#define V_ERR_INV_CTXT3(x) ((x) << S_ERR_INV_CTXT3)
+#define F_ERR_INV_CTXT3    V_ERR_INV_CTXT3(1U)
+
+#define S_ERR_INV_CTXT2    2
+#define V_ERR_INV_CTXT2(x) ((x) << S_ERR_INV_CTXT2)
+#define F_ERR_INV_CTXT2    V_ERR_INV_CTXT2(1U)
+
+#define S_ERR_INV_CTXT1    1
+#define V_ERR_INV_CTXT1(x) ((x) << S_ERR_INV_CTXT1)
+#define F_ERR_INV_CTXT1    V_ERR_INV_CTXT1(1U)
+
+#define S_ERR_INV_CTXT0    0
+#define V_ERR_INV_CTXT0(x) ((x) << S_ERR_INV_CTXT0)
+#define F_ERR_INV_CTXT0    V_ERR_INV_CTXT0(1U)
+
+#define A_SGE_INT_ENABLE3 0x1040
+#define A_SGE_FL_BUFFER_SIZE0 0x1044
+
+#define S_SIZE    4
+#define M_SIZE    0xfffffffU
+#define V_SIZE(x) ((x) << S_SIZE)
+#define G_SIZE(x) (((x) >> S_SIZE) & M_SIZE)
+
+#define A_SGE_FL_BUFFER_SIZE1 0x1048
+#define A_SGE_FL_BUFFER_SIZE2 0x104c
+#define A_SGE_FL_BUFFER_SIZE3 0x1050
+#define A_SGE_FL_BUFFER_SIZE4 0x1054
+#define A_SGE_FL_BUFFER_SIZE5 0x1058
+#define A_SGE_FL_BUFFER_SIZE6 0x105c
+#define A_SGE_FL_BUFFER_SIZE7 0x1060
+#define A_SGE_FL_BUFFER_SIZE8 0x1064
+#define A_SGE_FL_BUFFER_SIZE9 0x1068
+#define A_SGE_FL_BUFFER_SIZE10 0x106c
+#define A_SGE_FL_BUFFER_SIZE11 0x1070
+#define A_SGE_FL_BUFFER_SIZE12 0x1074
+#define A_SGE_FL_BUFFER_SIZE13 0x1078
+#define A_SGE_FL_BUFFER_SIZE14 0x107c
+#define A_SGE_FL_BUFFER_SIZE15 0x1080
+#define A_SGE_DBQ_CTXT_BADDR 0x1084
+
+#define S_BASEADDR    3
+#define M_BASEADDR    0x1fffffffU
+#define V_BASEADDR(x) ((x) << S_BASEADDR)
+#define G_BASEADDR(x) (((x) >> S_BASEADDR) & M_BASEADDR)
+
+#define A_SGE_IMSG_CTXT_BADDR 0x1088
+#define A_SGE_FLM_CACHE_BADDR 0x108c
+#define A_SGE_FLM_CFG 0x1090
+
+#define S_OPMODE    26
+#define M_OPMODE    0x3fU
+#define V_OPMODE(x) ((x) << S_OPMODE)
+#define G_OPMODE(x) (((x) >> S_OPMODE) & M_OPMODE)
+
+#define S_NOHDR    18
+#define V_NOHDR(x) ((x) << S_NOHDR)
+#define F_NOHDR    V_NOHDR(1U)
+
+#define S_CACHEPTRCNT    16
+#define M_CACHEPTRCNT    0x3U
+#define V_CACHEPTRCNT(x) ((x) << S_CACHEPTRCNT)
+#define G_CACHEPTRCNT(x) (((x) >> S_CACHEPTRCNT) & M_CACHEPTRCNT)
+
+#define S_EDRAMPTRCNT    14
+#define M_EDRAMPTRCNT    0x3U
+#define V_EDRAMPTRCNT(x) ((x) << S_EDRAMPTRCNT)
+#define G_EDRAMPTRCNT(x) (((x) >> S_EDRAMPTRCNT) & M_EDRAMPTRCNT)
+
+#define S_HDRSTARTFLQ    11
+#define M_HDRSTARTFLQ    0x7U
+#define V_HDRSTARTFLQ(x) ((x) << S_HDRSTARTFLQ)
+#define G_HDRSTARTFLQ(x) (((x) >> S_HDRSTARTFLQ) & M_HDRSTARTFLQ)
+
+#define S_FETCHTHRESH    6
+#define M_FETCHTHRESH    0x1fU
+#define V_FETCHTHRESH(x) ((x) << S_FETCHTHRESH)
+#define G_FETCHTHRESH(x) (((x) >> S_FETCHTHRESH) & M_FETCHTHRESH)
+
+#define S_CREDITCNT    4
+#define M_CREDITCNT    0x3U
+#define V_CREDITCNT(x) ((x) << S_CREDITCNT)
+#define G_CREDITCNT(x) (((x) >> S_CREDITCNT) & M_CREDITCNT)
+
+#define S_NOEDRAM    0
+#define V_NOEDRAM(x) ((x) << S_NOEDRAM)
+#define F_NOEDRAM    V_NOEDRAM(1U)
+
+#define A_SGE_CONM_CTRL 0x1094
+
+#define S_EGRTHRESHOLD    8
+#define M_EGRTHRESHOLD    0x3fU
+#define V_EGRTHRESHOLD(x) ((x) << S_EGRTHRESHOLD)
+#define G_EGRTHRESHOLD(x) (((x) >> S_EGRTHRESHOLD) & M_EGRTHRESHOLD)
+
+#define S_INGTHRESHOLD    2
+#define M_INGTHRESHOLD    0x3fU
+#define V_INGTHRESHOLD(x) ((x) << S_INGTHRESHOLD)
+#define G_INGTHRESHOLD(x) (((x) >> S_INGTHRESHOLD) & M_INGTHRESHOLD)
+
+#define S_MPS_ENABLE    1
+#define V_MPS_ENABLE(x) ((x) << S_MPS_ENABLE)
+#define F_MPS_ENABLE    V_MPS_ENABLE(1U)
+
+#define S_TP_ENABLE    0
+#define V_TP_ENABLE(x) ((x) << S_TP_ENABLE)
+#define F_TP_ENABLE    V_TP_ENABLE(1U)
+
+#define A_SGE_TIMESTAMP_LO 0x1098
+#define A_SGE_TIMESTAMP_HI 0x109c
+
+#define S_TSOP    28
+#define M_TSOP    0x3U
+#define V_TSOP(x) ((x) << S_TSOP)
+#define G_TSOP(x) (((x) >> S_TSOP) & M_TSOP)
+
+#define S_TSVAL    0
+#define M_TSVAL    0xfffffffU
+#define V_TSVAL(x) ((x) << S_TSVAL)
+#define G_TSVAL(x) (((x) >> S_TSVAL) & M_TSVAL)
+
+#define A_SGE_INGRESS_RX_THRESHOLD 0x10a0
+
+#define S_THRESHOLD_0    24
+#define M_THRESHOLD_0    0x3fU
+#define V_THRESHOLD_0(x) ((x) << S_THRESHOLD_0)
+#define G_THRESHOLD_0(x) (((x) >> S_THRESHOLD_0) & M_THRESHOLD_0)
+
+#define S_THRESHOLD_1    16
+#define M_THRESHOLD_1    0x3fU
+#define V_THRESHOLD_1(x) ((x) << S_THRESHOLD_1)
+#define G_THRESHOLD_1(x) (((x) >> S_THRESHOLD_1) & M_THRESHOLD_1)
+
+#define S_THRESHOLD_2    8
+#define M_THRESHOLD_2    0x3fU
+#define V_THRESHOLD_2(x) ((x) << S_THRESHOLD_2)
+#define G_THRESHOLD_2(x) (((x) >> S_THRESHOLD_2) & M_THRESHOLD_2)
+
+#define S_THRESHOLD_3    0
+#define M_THRESHOLD_3    0x3fU
+#define V_THRESHOLD_3(x) ((x) << S_THRESHOLD_3)
+#define G_THRESHOLD_3(x) (((x) >> S_THRESHOLD_3) & M_THRESHOLD_3)
+
+#define A_SGE_DBFIFO_STATUS 0x10a4
+
+#define S_HP_INT_THRESH    28
+#define M_HP_INT_THRESH    0xfU
+#define V_HP_INT_THRESH(x) ((x) << S_HP_INT_THRESH)
+#define G_HP_INT_THRESH(x) (((x) >> S_HP_INT_THRESH) & M_HP_INT_THRESH)
+
+#define S_HP_COUNT    16
+#define M_HP_COUNT    0x7ffU
+#define V_HP_COUNT(x) ((x) << S_HP_COUNT)
+#define G_HP_COUNT(x) (((x) >> S_HP_COUNT) & M_HP_COUNT)
+
+#define S_LP_INT_THRESH    12
+#define M_LP_INT_THRESH    0xfU
+#define V_LP_INT_THRESH(x) ((x) << S_LP_INT_THRESH)
+#define G_LP_INT_THRESH(x) (((x) >> S_LP_INT_THRESH) & M_LP_INT_THRESH)
+
+#define S_LP_COUNT    0
+#define M_LP_COUNT    0x7ffU
+#define V_LP_COUNT(x) ((x) << S_LP_COUNT)
+#define G_LP_COUNT(x) (((x) >> S_LP_COUNT) & M_LP_COUNT)
+
+#define A_SGE_DOORBELL_CONTROL 0x10a8
+
+#define S_HINTDEPTHCTL    27
+#define M_HINTDEPTHCTL    0x1fU
+#define V_HINTDEPTHCTL(x) ((x) << S_HINTDEPTHCTL)
+#define G_HINTDEPTHCTL(x) (((x) >> S_HINTDEPTHCTL) & M_HINTDEPTHCTL)
+
+#define S_NOCOALESCE    26
+#define V_NOCOALESCE(x) ((x) << S_NOCOALESCE)
+#define F_NOCOALESCE    V_NOCOALESCE(1U)
+
+#define S_HP_WEIGHT    24
+#define M_HP_WEIGHT    0x3U
+#define V_HP_WEIGHT(x) ((x) << S_HP_WEIGHT)
+#define G_HP_WEIGHT(x) (((x) >> S_HP_WEIGHT) & M_HP_WEIGHT)
+
+#define S_HP_DISABLE    23
+#define V_HP_DISABLE(x) ((x) << S_HP_DISABLE)
+#define F_HP_DISABLE    V_HP_DISABLE(1U)
+
+#define S_FORCEUSERDBTOLP    22
+#define V_FORCEUSERDBTOLP(x) ((x) << S_FORCEUSERDBTOLP)
+#define F_FORCEUSERDBTOLP    V_FORCEUSERDBTOLP(1U)
+
+#define S_FORCEVFPF0DBTOLP    21
+#define V_FORCEVFPF0DBTOLP(x) ((x) << S_FORCEVFPF0DBTOLP)
+#define F_FORCEVFPF0DBTOLP    V_FORCEVFPF0DBTOLP(1U)
+
+#define S_FORCEVFPF1DBTOLP    20
+#define V_FORCEVFPF1DBTOLP(x) ((x) << S_FORCEVFPF1DBTOLP)
+#define F_FORCEVFPF1DBTOLP    V_FORCEVFPF1DBTOLP(1U)
+
+#define S_FORCEVFPF2DBTOLP    19
+#define V_FORCEVFPF2DBTOLP(x) ((x) << S_FORCEVFPF2DBTOLP)
+#define F_FORCEVFPF2DBTOLP    V_FORCEVFPF2DBTOLP(1U)
+
+#define S_FORCEVFPF3DBTOLP    18
+#define V_FORCEVFPF3DBTOLP(x) ((x) << S_FORCEVFPF3DBTOLP)
+#define F_FORCEVFPF3DBTOLP    V_FORCEVFPF3DBTOLP(1U)
+
+#define S_FORCEVFPF4DBTOLP    17
+#define V_FORCEVFPF4DBTOLP(x) ((x) << S_FORCEVFPF4DBTOLP)
+#define F_FORCEVFPF4DBTOLP    V_FORCEVFPF4DBTOLP(1U)
+
+#define S_FORCEVFPF5DBTOLP    16
+#define V_FORCEVFPF5DBTOLP(x) ((x) << S_FORCEVFPF5DBTOLP)
+#define F_FORCEVFPF5DBTOLP    V_FORCEVFPF5DBTOLP(1U)
+
+#define S_FORCEVFPF6DBTOLP    15
+#define V_FORCEVFPF6DBTOLP(x) ((x) << S_FORCEVFPF6DBTOLP)
+#define F_FORCEVFPF6DBTOLP    V_FORCEVFPF6DBTOLP(1U)
+
+#define S_FORCEVFPF7DBTOLP    14
+#define V_FORCEVFPF7DBTOLP(x) ((x) << S_FORCEVFPF7DBTOLP)
+#define F_FORCEVFPF7DBTOLP    V_FORCEVFPF7DBTOLP(1U)
+
+#define S_ENABLE_DROP    13
+#define V_ENABLE_DROP(x) ((x) << S_ENABLE_DROP)
+#define F_ENABLE_DROP    V_ENABLE_DROP(1U)
+
+#define S_DROP_TIMEOUT    1
+#define M_DROP_TIMEOUT    0xfffU
+#define V_DROP_TIMEOUT(x) ((x) << S_DROP_TIMEOUT)
+#define G_DROP_TIMEOUT(x) (((x) >> S_DROP_TIMEOUT) & M_DROP_TIMEOUT)
+
+#define S_DROPPED_DB    0
+#define V_DROPPED_DB(x) ((x) << S_DROPPED_DB)
+#define F_DROPPED_DB    V_DROPPED_DB(1U)
+
+#define A_SGE_DROPPED_DOORBELL 0x10ac
+#define A_SGE_DOORBELL_THROTTLE_CONTROL 0x10b0
+
+#define S_THROTTLE_COUNT    1
+#define M_THROTTLE_COUNT    0xfffU
+#define V_THROTTLE_COUNT(x) ((x) << S_THROTTLE_COUNT)
+#define G_THROTTLE_COUNT(x) (((x) >> S_THROTTLE_COUNT) & M_THROTTLE_COUNT)
+
+#define S_THROTTLE_ENABLE    0
+#define V_THROTTLE_ENABLE(x) ((x) << S_THROTTLE_ENABLE)
+#define F_THROTTLE_ENABLE    V_THROTTLE_ENABLE(1U)
+
+#define A_SGE_ITP_CONTROL 0x10b4
+
+#define S_CRITICAL_TIME    10
+#define M_CRITICAL_TIME    0x7fffU
+#define V_CRITICAL_TIME(x) ((x) << S_CRITICAL_TIME)
+#define G_CRITICAL_TIME(x) (((x) >> S_CRITICAL_TIME) & M_CRITICAL_TIME)
+
+#define S_LL_EMPTY    4
+#define M_LL_EMPTY    0x3fU
+#define V_LL_EMPTY(x) ((x) << S_LL_EMPTY)
+#define G_LL_EMPTY(x) (((x) >> S_LL_EMPTY) & M_LL_EMPTY)
+
+#define S_LL_READ_WAIT_DISABLE    0
+#define V_LL_READ_WAIT_DISABLE(x) ((x) << S_LL_READ_WAIT_DISABLE)
+#define F_LL_READ_WAIT_DISABLE    V_LL_READ_WAIT_DISABLE(1U)
+
+#define A_SGE_TIMER_VALUE_0_AND_1 0x10b8
+
+#define S_TIMERVALUE0    16
+#define M_TIMERVALUE0    0xffffU
+#define V_TIMERVALUE0(x) ((x) << S_TIMERVALUE0)
+#define G_TIMERVALUE0(x) (((x) >> S_TIMERVALUE0) & M_TIMERVALUE0)
+
+#define S_TIMERVALUE1    0
+#define M_TIMERVALUE1    0xffffU
+#define V_TIMERVALUE1(x) ((x) << S_TIMERVALUE1)
+#define G_TIMERVALUE1(x) (((x) >> S_TIMERVALUE1) & M_TIMERVALUE1)
+
+#define A_SGE_TIMER_VALUE_2_AND_3 0x10bc
+
+#define S_TIMERVALUE2    16
+#define M_TIMERVALUE2    0xffffU
+#define V_TIMERVALUE2(x) ((x) << S_TIMERVALUE2)
+#define G_TIMERVALUE2(x) (((x) >> S_TIMERVALUE2) & M_TIMERVALUE2)
+
+#define S_TIMERVALUE3    0
+#define M_TIMERVALUE3    0xffffU
+#define V_TIMERVALUE3(x) ((x) << S_TIMERVALUE3)
+#define G_TIMERVALUE3(x) (((x) >> S_TIMERVALUE3) & M_TIMERVALUE3)
+
+#define A_SGE_TIMER_VALUE_4_AND_5 0x10c0
+
+#define S_TIMERVALUE4    16
+#define M_TIMERVALUE4    0xffffU
+#define V_TIMERVALUE4(x) ((x) << S_TIMERVALUE4)
+#define G_TIMERVALUE4(x) (((x) >> S_TIMERVALUE4) & M_TIMERVALUE4)
+
+#define S_TIMERVALUE5    0
+#define M_TIMERVALUE5    0xffffU
+#define V_TIMERVALUE5(x) ((x) << S_TIMERVALUE5)
+#define G_TIMERVALUE5(x) (((x) >> S_TIMERVALUE5) & M_TIMERVALUE5)
+
+#define A_SGE_PD_RSP_CREDIT01 0x10c4
+
+#define S_RSPCREDITEN0    31
+#define V_RSPCREDITEN0(x) ((x) << S_RSPCREDITEN0)
+#define F_RSPCREDITEN0    V_RSPCREDITEN0(1U)
+
+#define S_MAXTAG0    24
+#define M_MAXTAG0    0x7fU
+#define V_MAXTAG0(x) ((x) << S_MAXTAG0)
+#define G_MAXTAG0(x) (((x) >> S_MAXTAG0) & M_MAXTAG0)
+
+#define S_MAXRSPCNT0    16
+#define M_MAXRSPCNT0    0xffU
+#define V_MAXRSPCNT0(x) ((x) << S_MAXRSPCNT0)
+#define G_MAXRSPCNT0(x) (((x) >> S_MAXRSPCNT0) & M_MAXRSPCNT0)
+
+#define S_RSPCREDITEN1    15
+#define V_RSPCREDITEN1(x) ((x) << S_RSPCREDITEN1)
+#define F_RSPCREDITEN1    V_RSPCREDITEN1(1U)
+
+#define S_MAXTAG1    8
+#define M_MAXTAG1    0x7fU
+#define V_MAXTAG1(x) ((x) << S_MAXTAG1)
+#define G_MAXTAG1(x) (((x) >> S_MAXTAG1) & M_MAXTAG1)
+
+#define S_MAXRSPCNT1    0
+#define M_MAXRSPCNT1    0xffU
+#define V_MAXRSPCNT1(x) ((x) << S_MAXRSPCNT1)
+#define G_MAXRSPCNT1(x) (((x) >> S_MAXRSPCNT1) & M_MAXRSPCNT1)
+
+#define A_SGE_PD_RSP_CREDIT23 0x10c8
+
+#define S_RSPCREDITEN2    31
+#define V_RSPCREDITEN2(x) ((x) << S_RSPCREDITEN2)
+#define F_RSPCREDITEN2    V_RSPCREDITEN2(1U)
+
+#define S_MAXTAG2    24
+#define M_MAXTAG2    0x7fU
+#define V_MAXTAG2(x) ((x) << S_MAXTAG2)
+#define G_MAXTAG2(x) (((x) >> S_MAXTAG2) & M_MAXTAG2)
+
+#define S_MAXRSPCNT2    16
+#define M_MAXRSPCNT2    0xffU
+#define V_MAXRSPCNT2(x) ((x) << S_MAXRSPCNT2)
+#define G_MAXRSPCNT2(x) (((x) >> S_MAXRSPCNT2) & M_MAXRSPCNT2)
+
+#define S_RSPCREDITEN3    15
+#define V_RSPCREDITEN3(x) ((x) << S_RSPCREDITEN3)
+#define F_RSPCREDITEN3    V_RSPCREDITEN3(1U)
+
+#define S_MAXTAG3    8
+#define M_MAXTAG3    0x7fU
+#define V_MAXTAG3(x) ((x) << S_MAXTAG3)
+#define G_MAXTAG3(x) (((x) >> S_MAXTAG3) & M_MAXTAG3)
+
+#define S_MAXRSPCNT3    0
+#define M_MAXRSPCNT3    0xffU
+#define V_MAXRSPCNT3(x) ((x) << S_MAXRSPCNT3)
+#define G_MAXRSPCNT3(x) (((x) >> S_MAXRSPCNT3) & M_MAXRSPCNT3)
+
+#define A_SGE_DEBUG_INDEX 0x10cc
+#define A_SGE_DEBUG_DATA_HIGH 0x10d0
+#define A_SGE_DEBUG_DATA_LOW 0x10d4
+#define A_SGE_REVISION 0x10d8
+#define A_SGE_INT_CAUSE4 0x10dc
+
+#define S_ERR_BAD_UPFL_INC_CREDIT3    8
+#define V_ERR_BAD_UPFL_INC_CREDIT3(x) ((x) << S_ERR_BAD_UPFL_INC_CREDIT3)
+#define F_ERR_BAD_UPFL_INC_CREDIT3    V_ERR_BAD_UPFL_INC_CREDIT3(1U)
+
+#define S_ERR_BAD_UPFL_INC_CREDIT2    7
+#define V_ERR_BAD_UPFL_INC_CREDIT2(x) ((x) << S_ERR_BAD_UPFL_INC_CREDIT2)
+#define F_ERR_BAD_UPFL_INC_CREDIT2    V_ERR_BAD_UPFL_INC_CREDIT2(1U)
+
+#define S_ERR_BAD_UPFL_INC_CREDIT1    6
+#define V_ERR_BAD_UPFL_INC_CREDIT1(x) ((x) << S_ERR_BAD_UPFL_INC_CREDIT1)
+#define F_ERR_BAD_UPFL_INC_CREDIT1    V_ERR_BAD_UPFL_INC_CREDIT1(1U)
+
+#define S_ERR_BAD_UPFL_INC_CREDIT0    5
+#define V_ERR_BAD_UPFL_INC_CREDIT0(x) ((x) << S_ERR_BAD_UPFL_INC_CREDIT0)
+#define F_ERR_BAD_UPFL_INC_CREDIT0    V_ERR_BAD_UPFL_INC_CREDIT0(1U)
+
+#define S_ERR_PHYSADDR_LEN0_IDMA1    4
+#define V_ERR_PHYSADDR_LEN0_IDMA1(x) ((x) << S_ERR_PHYSADDR_LEN0_IDMA1)
+#define F_ERR_PHYSADDR_LEN0_IDMA1    V_ERR_PHYSADDR_LEN0_IDMA1(1U)
+
+#define S_ERR_PHYSADDR_LEN0_IDMA0    3
+#define V_ERR_PHYSADDR_LEN0_IDMA0(x) ((x) << S_ERR_PHYSADDR_LEN0_IDMA0)
+#define F_ERR_PHYSADDR_LEN0_IDMA0    V_ERR_PHYSADDR_LEN0_IDMA0(1U)
+
+#define S_ERR_FLM_INVALID_PKT_DROP1    2
+#define V_ERR_FLM_INVALID_PKT_DROP1(x) ((x) << S_ERR_FLM_INVALID_PKT_DROP1)
+#define F_ERR_FLM_INVALID_PKT_DROP1    V_ERR_FLM_INVALID_PKT_DROP1(1U)
+
+#define S_ERR_FLM_INVALID_PKT_DROP0    1
+#define V_ERR_FLM_INVALID_PKT_DROP0(x) ((x) << S_ERR_FLM_INVALID_PKT_DROP0)
+#define F_ERR_FLM_INVALID_PKT_DROP0    V_ERR_FLM_INVALID_PKT_DROP0(1U)
+
+#define S_ERR_UNEXPECTED_TIMER    0
+#define V_ERR_UNEXPECTED_TIMER(x) ((x) << S_ERR_UNEXPECTED_TIMER)
+#define F_ERR_UNEXPECTED_TIMER    V_ERR_UNEXPECTED_TIMER(1U)
+
+#define A_SGE_INT_ENABLE4 0x10e0
+#define A_SGE_STAT_TOTAL 0x10e4
+#define A_SGE_STAT_MATCH 0x10e8
+#define A_SGE_STAT_CFG 0x10ec
+
+#define S_ITPOPMODE    8
+#define V_ITPOPMODE(x) ((x) << S_ITPOPMODE)
+#define F_ITPOPMODE    V_ITPOPMODE(1U)
+
+#define S_EGRCTXTOPMODE    6
+#define M_EGRCTXTOPMODE    0x3U
+#define V_EGRCTXTOPMODE(x) ((x) << S_EGRCTXTOPMODE)
+#define G_EGRCTXTOPMODE(x) (((x) >> S_EGRCTXTOPMODE) & M_EGRCTXTOPMODE)
+
+#define S_INGCTXTOPMODE    4
+#define M_INGCTXTOPMODE    0x3U
+#define V_INGCTXTOPMODE(x) ((x) << S_INGCTXTOPMODE)
+#define G_INGCTXTOPMODE(x) (((x) >> S_INGCTXTOPMODE) & M_INGCTXTOPMODE)
+
+#define S_STATMODE    2
+#define M_STATMODE    0x3U
+#define V_STATMODE(x) ((x) << S_STATMODE)
+#define G_STATMODE(x) (((x) >> S_STATMODE) & M_STATMODE)
+
+#define S_STATSOURCE    0
+#define M_STATSOURCE    0x3U
+#define V_STATSOURCE(x) ((x) << S_STATSOURCE)
+#define G_STATSOURCE(x) (((x) >> S_STATSOURCE) & M_STATSOURCE)
+
+#define A_SGE_HINT_CFG 0x10f0
+
+#define S_HINTSALLOWEDNOHDR    6
+#define M_HINTSALLOWEDNOHDR    0x3fU
+#define V_HINTSALLOWEDNOHDR(x) ((x) << S_HINTSALLOWEDNOHDR)
+#define G_HINTSALLOWEDNOHDR(x) (((x) >> S_HINTSALLOWEDNOHDR) & M_HINTSALLOWEDNOHDR)
+
+#define S_HINTSALLOWEDHDR    0
+#define M_HINTSALLOWEDHDR    0x3fU
+#define V_HINTSALLOWEDHDR(x) ((x) << S_HINTSALLOWEDHDR)
+#define G_HINTSALLOWEDHDR(x) (((x) >> S_HINTSALLOWEDHDR) & M_HINTSALLOWEDHDR)
+
+#define A_SGE_INGRESS_QUEUES_PER_PAGE_PF 0x10f4
+#define A_SGE_INGRESS_QUEUES_PER_PAGE_VF 0x10f8
+#define A_SGE_PD_WRR_CONFIG 0x10fc
+
+#define S_EDMA_WEIGHT    0
+#define M_EDMA_WEIGHT    0x3fU
+#define V_EDMA_WEIGHT(x) ((x) << S_EDMA_WEIGHT)
+#define G_EDMA_WEIGHT(x) (((x) >> S_EDMA_WEIGHT) & M_EDMA_WEIGHT)
+
+#define A_SGE_ERROR_STATS 0x1100
+
+#define S_UNCAPTURED_ERROR    18
+#define V_UNCAPTURED_ERROR(x) ((x) << S_UNCAPTURED_ERROR)
+#define F_UNCAPTURED_ERROR    V_UNCAPTURED_ERROR(1U)
+
+#define S_ERROR_QID_VALID    17
+#define V_ERROR_QID_VALID(x) ((x) << S_ERROR_QID_VALID)
+#define F_ERROR_QID_VALID    V_ERROR_QID_VALID(1U)
+
+#define S_ERROR_QID    0
+#define M_ERROR_QID    0x1ffffU
+#define V_ERROR_QID(x) ((x) << S_ERROR_QID)
+#define G_ERROR_QID(x) (((x) >> S_ERROR_QID) & M_ERROR_QID)
+
+#define A_SGE_SHARED_TAG_CHAN_CFG 0x1104
+
+#define S_MINTAG3    24
+#define M_MINTAG3    0xffU
+#define V_MINTAG3(x) ((x) << S_MINTAG3)
+#define G_MINTAG3(x) (((x) >> S_MINTAG3) & M_MINTAG3)
+
+#define S_MINTAG2    16
+#define M_MINTAG2    0xffU
+#define V_MINTAG2(x) ((x) << S_MINTAG2)
+#define G_MINTAG2(x) (((x) >> S_MINTAG2) & M_MINTAG2)
+
+#define S_MINTAG1    8
+#define M_MINTAG1    0xffU
+#define V_MINTAG1(x) ((x) << S_MINTAG1)
+#define G_MINTAG1(x) (((x) >> S_MINTAG1) & M_MINTAG1)
+
+#define S_MINTAG0    0
+#define M_MINTAG0    0xffU
+#define V_MINTAG0(x) ((x) << S_MINTAG0)
+#define G_MINTAG0(x) (((x) >> S_MINTAG0) & M_MINTAG0)
+
+#define A_SGE_SHARED_TAG_POOL_CFG 0x1108
+
+#define S_TAGPOOLTOTAL    0
+#define M_TAGPOOLTOTAL    0xffU
+#define V_TAGPOOLTOTAL(x) ((x) << S_TAGPOOLTOTAL)
+#define G_TAGPOOLTOTAL(x) (((x) >> S_TAGPOOLTOTAL) & M_TAGPOOLTOTAL)
+
+#define A_SGE_PC0_REQ_BIST_CMD 0x1180
+#define A_SGE_PC0_REQ_BIST_ERROR_CNT 0x1184
+#define A_SGE_PC1_REQ_BIST_CMD 0x1190
+#define A_SGE_PC1_REQ_BIST_ERROR_CNT 0x1194
+#define A_SGE_PC0_RSP_BIST_CMD 0x11a0
+#define A_SGE_PC0_RSP_BIST_ERROR_CNT 0x11a4
+#define A_SGE_PC1_RSP_BIST_CMD 0x11b0
+#define A_SGE_PC1_RSP_BIST_ERROR_CNT 0x11b4
+#define A_SGE_CTXT_CMD 0x11fc
+
+#define S_BUSY    31
+#define V_BUSY(x) ((x) << S_BUSY)
+#define F_BUSY    V_BUSY(1U)
+
+#define S_CTXTOP    28
+#define M_CTXTOP    0x3U
+#define V_CTXTOP(x) ((x) << S_CTXTOP)
+#define G_CTXTOP(x) (((x) >> S_CTXTOP) & M_CTXTOP)
+
+#define S_CTXTTYPE    24
+#define M_CTXTTYPE    0x3U
+#define V_CTXTTYPE(x) ((x) << S_CTXTTYPE)
+#define G_CTXTTYPE(x) (((x) >> S_CTXTTYPE) & M_CTXTTYPE)
+
+#define S_CTXTQID    0
+#define M_CTXTQID    0x1ffffU
+#define V_CTXTQID(x) ((x) << S_CTXTQID)
+#define G_CTXTQID(x) (((x) >> S_CTXTQID) & M_CTXTQID)
+
+#define A_SGE_CTXT_DATA0 0x1200
+#define A_SGE_CTXT_DATA1 0x1204
+#define A_SGE_CTXT_DATA2 0x1208
+#define A_SGE_CTXT_DATA3 0x120c
+#define A_SGE_CTXT_DATA4 0x1210
+#define A_SGE_CTXT_DATA5 0x1214
+#define A_SGE_CTXT_DATA6 0x1218
+#define A_SGE_CTXT_DATA7 0x121c
+#define A_SGE_CTXT_MASK0 0x1220
+#define A_SGE_CTXT_MASK1 0x1224
+#define A_SGE_CTXT_MASK2 0x1228
+#define A_SGE_CTXT_MASK3 0x122c
+#define A_SGE_CTXT_MASK4 0x1230
+#define A_SGE_CTXT_MASK5 0x1234
+#define A_SGE_CTXT_MASK6 0x1238
+#define A_SGE_CTXT_MASK7 0x123c
+#define A_SGE_QUEUE_BASE_MAP_HIGH 0x1300
+
+#define S_EGRESS_LOG2SIZE    27
+#define M_EGRESS_LOG2SIZE    0x1fU
+#define V_EGRESS_LOG2SIZE(x) ((x) << S_EGRESS_LOG2SIZE)
+#define G_EGRESS_LOG2SIZE(x) (((x) >> S_EGRESS_LOG2SIZE) & M_EGRESS_LOG2SIZE)
+
+#define S_EGRESS_BASE    10
+#define M_EGRESS_BASE    0x1ffffU
+#define V_EGRESS_BASE(x) ((x) << S_EGRESS_BASE)
+#define G_EGRESS_BASE(x) (((x) >> S_EGRESS_BASE) & M_EGRESS_BASE)
+
+#define S_INGRESS2_LOG2SIZE    5
+#define M_INGRESS2_LOG2SIZE    0x1fU
+#define V_INGRESS2_LOG2SIZE(x) ((x) << S_INGRESS2_LOG2SIZE)
+#define G_INGRESS2_LOG2SIZE(x) (((x) >> S_INGRESS2_LOG2SIZE) & M_INGRESS2_LOG2SIZE)
+
+#define S_INGRESS1_LOG2SIZE    0
+#define M_INGRESS1_LOG2SIZE    0x1fU
+#define V_INGRESS1_LOG2SIZE(x) ((x) << S_INGRESS1_LOG2SIZE)
+#define G_INGRESS1_LOG2SIZE(x) (((x) >> S_INGRESS1_LOG2SIZE) & M_INGRESS1_LOG2SIZE)
+
+#define A_SGE_QUEUE_BASE_MAP_LOW 0x1304
+
+#define S_INGRESS2_BASE    16
+#define M_INGRESS2_BASE    0xffffU
+#define V_INGRESS2_BASE(x) ((x) << S_INGRESS2_BASE)
+#define G_INGRESS2_BASE(x) (((x) >> S_INGRESS2_BASE) & M_INGRESS2_BASE)
+
+#define S_INGRESS1_BASE    0
+#define M_INGRESS1_BASE    0xffffU
+#define V_INGRESS1_BASE(x) ((x) << S_INGRESS1_BASE)
+#define G_INGRESS1_BASE(x) (((x) >> S_INGRESS1_BASE) & M_INGRESS1_BASE)
+
+#define A_SGE_LA_RDPTR_0 0x1800
+#define A_SGE_LA_RDDATA_0 0x1804
+#define A_SGE_LA_WRPTR_0 0x1808
+#define A_SGE_LA_RESERVED_0 0x180c
+#define A_SGE_LA_RDPTR_1 0x1810
+#define A_SGE_LA_RDDATA_1 0x1814
+#define A_SGE_LA_WRPTR_1 0x1818
+#define A_SGE_LA_RESERVED_1 0x181c
+#define A_SGE_LA_RDPTR_2 0x1820
+#define A_SGE_LA_RDDATA_2 0x1824
+#define A_SGE_LA_WRPTR_2 0x1828
+#define A_SGE_LA_RESERVED_2 0x182c
+#define A_SGE_LA_RDPTR_3 0x1830
+#define A_SGE_LA_RDDATA_3 0x1834
+#define A_SGE_LA_WRPTR_3 0x1838
+#define A_SGE_LA_RESERVED_3 0x183c
+#define A_SGE_LA_RDPTR_4 0x1840
+#define A_SGE_LA_RDDATA_4 0x1844
+#define A_SGE_LA_WRPTR_4 0x1848
+#define A_SGE_LA_RESERVED_4 0x184c
+#define A_SGE_LA_RDPTR_5 0x1850
+#define A_SGE_LA_RDDATA_5 0x1854
+#define A_SGE_LA_WRPTR_5 0x1858
+#define A_SGE_LA_RESERVED_5 0x185c
+#define A_SGE_LA_RDPTR_6 0x1860
+#define A_SGE_LA_RDDATA_6 0x1864
+#define A_SGE_LA_WRPTR_6 0x1868
+#define A_SGE_LA_RESERVED_6 0x186c
+#define A_SGE_LA_RDPTR_7 0x1870
+#define A_SGE_LA_RDDATA_7 0x1874
+#define A_SGE_LA_WRPTR_7 0x1878
+#define A_SGE_LA_RESERVED_7 0x187c
+#define A_SGE_LA_RDPTR_8 0x1880
+#define A_SGE_LA_RDDATA_8 0x1884
+#define A_SGE_LA_WRPTR_8 0x1888
+#define A_SGE_LA_RESERVED_8 0x188c
+#define A_SGE_LA_RDPTR_9 0x1890
+#define A_SGE_LA_RDDATA_9 0x1894
+#define A_SGE_LA_WRPTR_9 0x1898
+#define A_SGE_LA_RESERVED_9 0x189c
+#define A_SGE_LA_RDPTR_10 0x18a0
+#define A_SGE_LA_RDDATA_10 0x18a4
+#define A_SGE_LA_WRPTR_10 0x18a8
+#define A_SGE_LA_RESERVED_10 0x18ac
+#define A_SGE_LA_RDPTR_11 0x18b0
+#define A_SGE_LA_RDDATA_11 0x18b4
+#define A_SGE_LA_WRPTR_11 0x18b8
+#define A_SGE_LA_RESERVED_11 0x18bc
+#define A_SGE_LA_RDPTR_12 0x18c0
+#define A_SGE_LA_RDDATA_12 0x18c4
+#define A_SGE_LA_WRPTR_12 0x18c8
+#define A_SGE_LA_RESERVED_12 0x18cc
+#define A_SGE_LA_RDPTR_13 0x18d0
+#define A_SGE_LA_RDDATA_13 0x18d4
+#define A_SGE_LA_WRPTR_13 0x18d8
+#define A_SGE_LA_RESERVED_13 0x18dc
+#define A_SGE_LA_RDPTR_14 0x18e0
+#define A_SGE_LA_RDDATA_14 0x18e4
+#define A_SGE_LA_WRPTR_14 0x18e8
+#define A_SGE_LA_RESERVED_14 0x18ec
+#define A_SGE_LA_RDPTR_15 0x18f0
+#define A_SGE_LA_RDDATA_15 0x18f4
+#define A_SGE_LA_WRPTR_15 0x18f8
+#define A_SGE_LA_RESERVED_15 0x18fc
+
+/* registers for module PCIE */
+#define PCIE_BASE_ADDR 0x3000
+
+#define A_PCIE_PF_CFG 0x40
+
+#define S_INTXSTAT    16
+#define V_INTXSTAT(x) ((x) << S_INTXSTAT)
+#define F_INTXSTAT    V_INTXSTAT(1U)
+
+#define S_AUXPWRPMEN    15
+#define V_AUXPWRPMEN(x) ((x) << S_AUXPWRPMEN)
+#define F_AUXPWRPMEN    V_AUXPWRPMEN(1U)
+
+#define S_NOSOFTRESET    14
+#define V_NOSOFTRESET(x) ((x) << S_NOSOFTRESET)
+#define F_NOSOFTRESET    V_NOSOFTRESET(1U)
+
+#define S_AIVEC    4
+#define M_AIVEC    0x3ffU
+#define V_AIVEC(x) ((x) << S_AIVEC)
+#define G_AIVEC(x) (((x) >> S_AIVEC) & M_AIVEC)
+
+#define S_INTXTYPE    2
+#define M_INTXTYPE    0x3U
+#define V_INTXTYPE(x) ((x) << S_INTXTYPE)
+#define G_INTXTYPE(x) (((x) >> S_INTXTYPE) & M_INTXTYPE)
+
+#define S_D3HOTEN    1
+#define V_D3HOTEN(x) ((x) << S_D3HOTEN)
+#define F_D3HOTEN    V_D3HOTEN(1U)
+
+#define S_CLIDECEN    0
+#define V_CLIDECEN(x) ((x) << S_CLIDECEN)
+#define F_CLIDECEN    V_CLIDECEN(1U)
+
+#define A_PCIE_PF_CLI 0x44
+#define A_PCIE_PF_GEN_MSG 0x48
+
+#define S_MSGTYPE    0
+#define M_MSGTYPE    0xffU
+#define V_MSGTYPE(x) ((x) << S_MSGTYPE)
+#define G_MSGTYPE(x) (((x) >> S_MSGTYPE) & M_MSGTYPE)
+
+#define A_PCIE_PF_EXPROM_OFST 0x4c
+
+#define S_OFFSET    10
+#define M_OFFSET    0x3fffU
+#define V_OFFSET(x) ((x) << S_OFFSET)
+#define G_OFFSET(x) (((x) >> S_OFFSET) & M_OFFSET)
+
+#define A_PCIE_INT_ENABLE 0x3000
+
+#define S_NONFATALERR    30
+#define V_NONFATALERR(x) ((x) << S_NONFATALERR)
+#define F_NONFATALERR    V_NONFATALERR(1U)
+
+#define S_UNXSPLCPLERR    29
+#define V_UNXSPLCPLERR(x) ((x) << S_UNXSPLCPLERR)
+#define F_UNXSPLCPLERR    V_UNXSPLCPLERR(1U)
+
+#define S_PCIEPINT    28
+#define V_PCIEPINT(x) ((x) << S_PCIEPINT)
+#define F_PCIEPINT    V_PCIEPINT(1U)
+
+#define S_PCIESINT    27
+#define V_PCIESINT(x) ((x) << S_PCIESINT)
+#define F_PCIESINT    V_PCIESINT(1U)
+
+#define S_RPLPERR    26
+#define V_RPLPERR(x) ((x) << S_RPLPERR)
+#define F_RPLPERR    V_RPLPERR(1U)
+
+#define S_RXWRPERR    25
+#define V_RXWRPERR(x) ((x) << S_RXWRPERR)
+#define F_RXWRPERR    V_RXWRPERR(1U)
+
+#define S_RXCPLPERR    24
+#define V_RXCPLPERR(x) ((x) << S_RXCPLPERR)
+#define F_RXCPLPERR    V_RXCPLPERR(1U)
+
+#define S_PIOTAGPERR    23
+#define V_PIOTAGPERR(x) ((x) << S_PIOTAGPERR)
+#define F_PIOTAGPERR    V_PIOTAGPERR(1U)
+
+#define S_MATAGPERR    22
+#define V_MATAGPERR(x) ((x) << S_MATAGPERR)
+#define F_MATAGPERR    V_MATAGPERR(1U)
+
+#define S_INTXCLRPERR    21
+#define V_INTXCLRPERR(x) ((x) << S_INTXCLRPERR)
+#define F_INTXCLRPERR    V_INTXCLRPERR(1U)
+
+#define S_FIDPERR    20
+#define V_FIDPERR(x) ((x) << S_FIDPERR)
+#define F_FIDPERR    V_FIDPERR(1U)
+
+#define S_CFGSNPPERR    19
+#define V_CFGSNPPERR(x) ((x) << S_CFGSNPPERR)
+#define F_CFGSNPPERR    V_CFGSNPPERR(1U)
+
+#define S_HRSPPERR    18
+#define V_HRSPPERR(x) ((x) << S_HRSPPERR)
+#define F_HRSPPERR    V_HRSPPERR(1U)
+
+#define S_HREQPERR    17
+#define V_HREQPERR(x) ((x) << S_HREQPERR)
+#define F_HREQPERR    V_HREQPERR(1U)
+
+#define S_HCNTPERR    16
+#define V_HCNTPERR(x) ((x) << S_HCNTPERR)
+#define F_HCNTPERR    V_HCNTPERR(1U)
+
+#define S_DRSPPERR    15
+#define V_DRSPPERR(x) ((x) << S_DRSPPERR)
+#define F_DRSPPERR    V_DRSPPERR(1U)
+
+#define S_DREQPERR    14
+#define V_DREQPERR(x) ((x) << S_DREQPERR)
+#define F_DREQPERR    V_DREQPERR(1U)
+
+#define S_DCNTPERR    13
+#define V_DCNTPERR(x) ((x) << S_DCNTPERR)
+#define F_DCNTPERR    V_DCNTPERR(1U)
+
+#define S_CRSPPERR    12
+#define V_CRSPPERR(x) ((x) << S_CRSPPERR)
+#define F_CRSPPERR    V_CRSPPERR(1U)
+
+#define S_CREQPERR    11
+#define V_CREQPERR(x) ((x) << S_CREQPERR)
+#define F_CREQPERR    V_CREQPERR(1U)
+
+#define S_CCNTPERR    10
+#define V_CCNTPERR(x) ((x) << S_CCNTPERR)
+#define F_CCNTPERR    V_CCNTPERR(1U)
+
+#define S_TARTAGPERR    9
+#define V_TARTAGPERR(x) ((x) << S_TARTAGPERR)
+#define F_TARTAGPERR    V_TARTAGPERR(1U)
+
+#define S_PIOREQPERR    8
+#define V_PIOREQPERR(x) ((x) << S_PIOREQPERR)
+#define F_PIOREQPERR    V_PIOREQPERR(1U)
+
+#define S_PIOCPLPERR    7
+#define V_PIOCPLPERR(x) ((x) << S_PIOCPLPERR)
+#define F_PIOCPLPERR    V_PIOCPLPERR(1U)
+
+#define S_MSIXDIPERR    6
+#define V_MSIXDIPERR(x) ((x) << S_MSIXDIPERR)
+#define F_MSIXDIPERR    V_MSIXDIPERR(1U)
+
+#define S_MSIXDATAPERR    5
+#define V_MSIXDATAPERR(x) ((x) << S_MSIXDATAPERR)
+#define F_MSIXDATAPERR    V_MSIXDATAPERR(1U)
+
+#define S_MSIXADDRHPERR    4
+#define V_MSIXADDRHPERR(x) ((x) << S_MSIXADDRHPERR)
+#define F_MSIXADDRHPERR    V_MSIXADDRHPERR(1U)
+
+#define S_MSIXADDRLPERR    3
+#define V_MSIXADDRLPERR(x) ((x) << S_MSIXADDRLPERR)
+#define F_MSIXADDRLPERR    V_MSIXADDRLPERR(1U)
+
+#define S_MSIDATAPERR    2
+#define V_MSIDATAPERR(x) ((x) << S_MSIDATAPERR)
+#define F_MSIDATAPERR    V_MSIDATAPERR(1U)
+
+#define S_MSIADDRHPERR    1
+#define V_MSIADDRHPERR(x) ((x) << S_MSIADDRHPERR)
+#define F_MSIADDRHPERR    V_MSIADDRHPERR(1U)
+
+#define S_MSIADDRLPERR    0
+#define V_MSIADDRLPERR(x) ((x) << S_MSIADDRLPERR)
+#define F_MSIADDRLPERR    V_MSIADDRLPERR(1U)
+
+#define A_PCIE_INT_CAUSE 0x3004
+#define A_PCIE_PERR_ENABLE 0x3008
+#define A_PCIE_PERR_INJECT 0x300c
+
+#define S_IDE    0
+#define V_IDE(x) ((x) << S_IDE)
+#define F_IDE    V_IDE(1U)
+
+#define A_PCIE_NONFAT_ERR 0x3010
+
+#define S_RDRSPERR    9
+#define V_RDRSPERR(x) ((x) << S_RDRSPERR)
+#define F_RDRSPERR    V_RDRSPERR(1U)
+
+#define S_VPDRSPERR    8
+#define V_VPDRSPERR(x) ((x) << S_VPDRSPERR)
+#define F_VPDRSPERR    V_VPDRSPERR(1U)
+
+#define S_POPD    7
+#define V_POPD(x) ((x) << S_POPD)
+#define F_POPD    V_POPD(1U)
+
+#define S_POPH    6
+#define V_POPH(x) ((x) << S_POPH)
+#define F_POPH    V_POPH(1U)
+
+#define S_POPC    5
+#define V_POPC(x) ((x) << S_POPC)
+#define F_POPC    V_POPC(1U)
+
+#define S_MEMREQ    4
+#define V_MEMREQ(x) ((x) << S_MEMREQ)
+#define F_MEMREQ    V_MEMREQ(1U)
+
+#define S_PIOREQ    3
+#define V_PIOREQ(x) ((x) << S_PIOREQ)
+#define F_PIOREQ    V_PIOREQ(1U)
+
+#define S_TAGDROP    2
+#define V_TAGDROP(x) ((x) << S_TAGDROP)
+#define F_TAGDROP    V_TAGDROP(1U)
+
+#define S_TAGCPL    1
+#define V_TAGCPL(x) ((x) << S_TAGCPL)
+#define F_TAGCPL    V_TAGCPL(1U)
+
+#define S_CFGSNP    0
+#define V_CFGSNP(x) ((x) << S_CFGSNP)
+#define F_CFGSNP    V_CFGSNP(1U)
+
+#define A_PCIE_CFG 0x3014
+
+#define S_CFGDMAXPYLDSZRX    26
+#define M_CFGDMAXPYLDSZRX    0x7U
+#define V_CFGDMAXPYLDSZRX(x) ((x) << S_CFGDMAXPYLDSZRX)
+#define G_CFGDMAXPYLDSZRX(x) (((x) >> S_CFGDMAXPYLDSZRX) & M_CFGDMAXPYLDSZRX)
+
+#define S_CFGDMAXPYLDSZTX    23
+#define M_CFGDMAXPYLDSZTX    0x7U
+#define V_CFGDMAXPYLDSZTX(x) ((x) << S_CFGDMAXPYLDSZTX)
+#define G_CFGDMAXPYLDSZTX(x) (((x) >> S_CFGDMAXPYLDSZTX) & M_CFGDMAXPYLDSZTX)
+
+#define S_CFGDMAXRDREQSZ    20
+#define M_CFGDMAXRDREQSZ    0x7U
+#define V_CFGDMAXRDREQSZ(x) ((x) << S_CFGDMAXRDREQSZ)
+#define G_CFGDMAXRDREQSZ(x) (((x) >> S_CFGDMAXRDREQSZ) & M_CFGDMAXRDREQSZ)
+
+#define S_MASYNCEN    19
+#define V_MASYNCEN(x) ((x) << S_MASYNCEN)
+#define F_MASYNCEN    V_MASYNCEN(1U)
+
+#define S_DCAENDMA    18
+#define V_DCAENDMA(x) ((x) << S_DCAENDMA)
+#define F_DCAENDMA    V_DCAENDMA(1U)
+
+#define S_DCAENCMD    17
+#define V_DCAENCMD(x) ((x) << S_DCAENCMD)
+#define F_DCAENCMD    V_DCAENCMD(1U)
+
+#define S_VFMSIPNDEN    16
+#define V_VFMSIPNDEN(x) ((x) << S_VFMSIPNDEN)
+#define F_VFMSIPNDEN    V_VFMSIPNDEN(1U)
+
+#define S_FORCETXERROR    15
+#define V_FORCETXERROR(x) ((x) << S_FORCETXERROR)
+#define F_FORCETXERROR    V_FORCETXERROR(1U)
+
+#define S_VPDREQPROTECT    14
+#define V_VPDREQPROTECT(x) ((x) << S_VPDREQPROTECT)
+#define F_VPDREQPROTECT    V_VPDREQPROTECT(1U)
+
+#define S_FIDTABLEINVALID    13
+#define V_FIDTABLEINVALID(x) ((x) << S_FIDTABLEINVALID)
+#define F_FIDTABLEINVALID    V_FIDTABLEINVALID(1U)
+
+#define S_BYPASSMSIXCACHE    12
+#define V_BYPASSMSIXCACHE(x) ((x) << S_BYPASSMSIXCACHE)
+#define F_BYPASSMSIXCACHE    V_BYPASSMSIXCACHE(1U)
+
+#define S_BYPASSMSICACHE    11
+#define V_BYPASSMSICACHE(x) ((x) << S_BYPASSMSICACHE)
+#define F_BYPASSMSICACHE    V_BYPASSMSICACHE(1U)
+
+#define S_SIMSPEED    10
+#define V_SIMSPEED(x) ((x) << S_SIMSPEED)
+#define F_SIMSPEED    V_SIMSPEED(1U)
+
+#define S_TC0_STAMP    9
+#define V_TC0_STAMP(x) ((x) << S_TC0_STAMP)
+#define F_TC0_STAMP    V_TC0_STAMP(1U)
+
+#define S_AI_TCVAL    6
+#define M_AI_TCVAL    0x7U
+#define V_AI_TCVAL(x) ((x) << S_AI_TCVAL)
+#define G_AI_TCVAL(x) (((x) >> S_AI_TCVAL) & M_AI_TCVAL)
+
+#define S_DMASTOPEN    5
+#define V_DMASTOPEN(x) ((x) << S_DMASTOPEN)
+#define F_DMASTOPEN    V_DMASTOPEN(1U)
+
+#define S_DEVSTATERSTMODE    4
+#define V_DEVSTATERSTMODE(x) ((x) << S_DEVSTATERSTMODE)
+#define F_DEVSTATERSTMODE    V_DEVSTATERSTMODE(1U)
+
+#define S_HOTRSTPCIECRSTMODE    3
+#define V_HOTRSTPCIECRSTMODE(x) ((x) << S_HOTRSTPCIECRSTMODE)
+#define F_HOTRSTPCIECRSTMODE    V_HOTRSTPCIECRSTMODE(1U)
+
+#define S_DLDNPCIECRSTMODE    2
+#define V_DLDNPCIECRSTMODE(x) ((x) << S_DLDNPCIECRSTMODE)
+#define F_DLDNPCIECRSTMODE    V_DLDNPCIECRSTMODE(1U)
+
+#define S_DLDNPCIEPRECRSTMODE    1
+#define V_DLDNPCIEPRECRSTMODE(x) ((x) << S_DLDNPCIEPRECRSTMODE)
+#define F_DLDNPCIEPRECRSTMODE    V_DLDNPCIEPRECRSTMODE(1U)
+
+#define S_LINKDNRSTEN    0
+#define V_LINKDNRSTEN(x) ((x) << S_LINKDNRSTEN)
+#define F_LINKDNRSTEN    V_LINKDNRSTEN(1U)
+
+#define A_PCIE_DMA_CTRL 0x3018
+
+#define S_LITTLEENDIAN    7
+#define V_LITTLEENDIAN(x) ((x) << S_LITTLEENDIAN)
+#define F_LITTLEENDIAN    V_LITTLEENDIAN(1U)
+
+#define A_PCIE_DMA_CFG 0x301c
+
+#define S_MAXPYLDSIZE    28
+#define M_MAXPYLDSIZE    0x7U
+#define V_MAXPYLDSIZE(x) ((x) << S_MAXPYLDSIZE)
+#define G_MAXPYLDSIZE(x) (((x) >> S_MAXPYLDSIZE) & M_MAXPYLDSIZE)
+
+#define S_MAXRDREQSIZE    25
+#define M_MAXRDREQSIZE    0x7U
+#define V_MAXRDREQSIZE(x) ((x) << S_MAXRDREQSIZE)
+#define G_MAXRDREQSIZE(x) (((x) >> S_MAXRDREQSIZE) & M_MAXRDREQSIZE)
+
+#define S_DMA_MAXRSPCNT    16
+#define M_DMA_MAXRSPCNT    0x1ffU
+#define V_DMA_MAXRSPCNT(x) ((x) << S_DMA_MAXRSPCNT)
+#define G_DMA_MAXRSPCNT(x) (((x) >> S_DMA_MAXRSPCNT) & M_DMA_MAXRSPCNT)
+
+#define S_DMA_MAXREQCNT    8
+#define M_DMA_MAXREQCNT    0xffU
+#define V_DMA_MAXREQCNT(x) ((x) << S_DMA_MAXREQCNT)
+#define G_DMA_MAXREQCNT(x) (((x) >> S_DMA_MAXREQCNT) & M_DMA_MAXREQCNT)
+
+#define S_MAXTAG    0
+#define M_MAXTAG    0x7fU
+#define V_MAXTAG(x) ((x) << S_MAXTAG)
+#define G_MAXTAG(x) (((x) >> S_MAXTAG) & M_MAXTAG)
+
+#define A_PCIE_DMA_STAT 0x3020
+
+#define S_STATEREQ    28
+#define M_STATEREQ    0xfU
+#define V_STATEREQ(x) ((x) << S_STATEREQ)
+#define G_STATEREQ(x) (((x) >> S_STATEREQ) & M_STATEREQ)
+
+#define S_DMA_RSPCNT    16
+#define M_DMA_RSPCNT    0xfffU
+#define V_DMA_RSPCNT(x) ((x) << S_DMA_RSPCNT)
+#define G_DMA_RSPCNT(x) (((x) >> S_DMA_RSPCNT) & M_DMA_RSPCNT)
+
+#define S_STATEAREQ    13
+#define M_STATEAREQ    0x7U
+#define V_STATEAREQ(x) ((x) << S_STATEAREQ)
+#define G_STATEAREQ(x) (((x) >> S_STATEAREQ) & M_STATEAREQ)
+
+#define S_TAGFREE    12
+#define V_TAGFREE(x) ((x) << S_TAGFREE)
+#define F_TAGFREE    V_TAGFREE(1U)
+
+#define S_DMA_REQCNT    0
+#define M_DMA_REQCNT    0x7ffU
+#define V_DMA_REQCNT(x) ((x) << S_DMA_REQCNT)
+#define G_DMA_REQCNT(x) (((x) >> S_DMA_REQCNT) & M_DMA_REQCNT)
+
+#define A_PCIE_CMD_CTRL 0x303c
+#define A_PCIE_CMD_CFG 0x3040
+
+#define S_MAXRSPCNT    16
+#define M_MAXRSPCNT    0xfU
+#define V_MAXRSPCNT(x) ((x) << S_MAXRSPCNT)
+#define G_MAXRSPCNT(x) (((x) >> S_MAXRSPCNT) & M_MAXRSPCNT)
+
+#define S_MAXREQCNT    8
+#define M_MAXREQCNT    0x1fU
+#define V_MAXREQCNT(x) ((x) << S_MAXREQCNT)
+#define G_MAXREQCNT(x) (((x) >> S_MAXREQCNT) & M_MAXREQCNT)
+
+#define A_PCIE_CMD_STAT 0x3044
+
+#define S_RSPCNT    16
+#define M_RSPCNT    0x7fU
+#define V_RSPCNT(x) ((x) << S_RSPCNT)
+#define G_RSPCNT(x) (((x) >> S_RSPCNT) & M_RSPCNT)
+
+#define S_REQCNT    0
+#define M_REQCNT    0xffU
+#define V_REQCNT(x) ((x) << S_REQCNT)
+#define G_REQCNT(x) (((x) >> S_REQCNT) & M_REQCNT)
+
+#define A_PCIE_HMA_CTRL 0x3050
+
+#define S_IPLTSSM    12
+#define M_IPLTSSM    0xfU
+#define V_IPLTSSM(x) ((x) << S_IPLTSSM)
+#define G_IPLTSSM(x) (((x) >> S_IPLTSSM) & M_IPLTSSM)
+
+#define S_IPCONFIGDOWN    8
+#define M_IPCONFIGDOWN    0x7U
+#define V_IPCONFIGDOWN(x) ((x) << S_IPCONFIGDOWN)
+#define G_IPCONFIGDOWN(x) (((x) >> S_IPCONFIGDOWN) & M_IPCONFIGDOWN)
+
+#define A_PCIE_HMA_CFG 0x3054
+
+#define S_HMA_MAXRSPCNT    16
+#define M_HMA_MAXRSPCNT    0x1fU
+#define V_HMA_MAXRSPCNT(x) ((x) << S_HMA_MAXRSPCNT)
+#define G_HMA_MAXRSPCNT(x) (((x) >> S_HMA_MAXRSPCNT) & M_HMA_MAXRSPCNT)
+
+#define A_PCIE_HMA_STAT 0x3058
+
+#define S_HMA_RSPCNT    16
+#define M_HMA_RSPCNT    0xffU
+#define V_HMA_RSPCNT(x) ((x) << S_HMA_RSPCNT)
+#define G_HMA_RSPCNT(x) (((x) >> S_HMA_RSPCNT) & M_HMA_RSPCNT)
+
+#define A_PCIE_PIO_FIFO_CFG 0x305c
+
+#define S_CPLCONFIG    16
+#define M_CPLCONFIG    0xffffU
+#define V_CPLCONFIG(x) ((x) << S_CPLCONFIG)
+#define G_CPLCONFIG(x) (((x) >> S_CPLCONFIG) & M_CPLCONFIG)
+
+#define S_PIOSTOPEN    12
+#define V_PIOSTOPEN(x) ((x) << S_PIOSTOPEN)
+#define F_PIOSTOPEN    V_PIOSTOPEN(1U)
+
+#define S_IPLANESWAP    11
+#define V_IPLANESWAP(x) ((x) << S_IPLANESWAP)
+#define F_IPLANESWAP    V_IPLANESWAP(1U)
+
+#define S_FORCESTRICTTS1    10
+#define V_FORCESTRICTTS1(x) ((x) << S_FORCESTRICTTS1)
+#define F_FORCESTRICTTS1    V_FORCESTRICTTS1(1U)
+
+#define S_FORCEPROGRESSCNT    0
+#define M_FORCEPROGRESSCNT    0x3ffU
+#define V_FORCEPROGRESSCNT(x) ((x) << S_FORCEPROGRESSCNT)
+#define G_FORCEPROGRESSCNT(x) (((x) >> S_FORCEPROGRESSCNT) & M_FORCEPROGRESSCNT)
+
+#define A_PCIE_CFG_SPACE_REQ 0x3060
+
+#define S_ENABLE    30
+#define V_ENABLE(x) ((x) << S_ENABLE)
+#define F_ENABLE    V_ENABLE(1U)
+
+#define S_AI    29
+#define V_AI(x) ((x) << S_AI)
+#define F_AI    V_AI(1U)
+
+#define S_LOCALCFG    28
+#define V_LOCALCFG(x) ((x) << S_LOCALCFG)
+#define F_LOCALCFG    V_LOCALCFG(1U)
+
+#define S_BUS    20
+#define M_BUS    0xffU
+#define V_BUS(x) ((x) << S_BUS)
+#define G_BUS(x) (((x) >> S_BUS) & M_BUS)
+
+#define S_DEVICE    15
+#define M_DEVICE    0x1fU
+#define V_DEVICE(x) ((x) << S_DEVICE)
+#define G_DEVICE(x) (((x) >> S_DEVICE) & M_DEVICE)
+
+#define S_FUNCTION    12
+#define M_FUNCTION    0x7U
+#define V_FUNCTION(x) ((x) << S_FUNCTION)
+#define G_FUNCTION(x) (((x) >> S_FUNCTION) & M_FUNCTION)
+
+#define S_EXTREGISTER    8
+#define M_EXTREGISTER    0xfU
+#define V_EXTREGISTER(x) ((x) << S_EXTREGISTER)
+#define G_EXTREGISTER(x) (((x) >> S_EXTREGISTER) & M_EXTREGISTER)
+
+#define S_REGISTER    0
+#define M_REGISTER    0xffU
+#define V_REGISTER(x) ((x) << S_REGISTER)
+#define G_REGISTER(x) (((x) >> S_REGISTER) & M_REGISTER)
+
+#define A_PCIE_CFG_SPACE_DATA 0x3064
+#define A_PCIE_MEM_ACCESS_BASE_WIN 0x3068
+
+#define S_PCIEOFST    10
+#define M_PCIEOFST    0x3fffffU
+#define V_PCIEOFST(x) ((x) << S_PCIEOFST)
+#define G_PCIEOFST(x) (((x) >> S_PCIEOFST) & M_PCIEOFST)
+
+#define S_BIR    8
+#define M_BIR    0x3U
+#define V_BIR(x) ((x) << S_BIR)
+#define G_BIR(x) (((x) >> S_BIR) & M_BIR)
+
+#define S_WINDOW    0
+#define M_WINDOW    0xffU
+#define V_WINDOW(x) ((x) << S_WINDOW)
+#define G_WINDOW(x) (((x) >> S_WINDOW) & M_WINDOW)
+
+#define A_PCIE_MEM_ACCESS_OFFSET 0x306c
+#define A_PCIE_MAILBOX_BASE_WIN 0x30a8
+
+#define S_MBOXPCIEOFST    6
+#define M_MBOXPCIEOFST    0x3ffffffU
+#define V_MBOXPCIEOFST(x) ((x) << S_MBOXPCIEOFST)
+#define G_MBOXPCIEOFST(x) (((x) >> S_MBOXPCIEOFST) & M_MBOXPCIEOFST)
+
+#define S_MBOXBIR    4
+#define M_MBOXBIR    0x3U
+#define V_MBOXBIR(x) ((x) << S_MBOXBIR)
+#define G_MBOXBIR(x) (((x) >> S_MBOXBIR) & M_MBOXBIR)
+
+#define S_MBOXWIN    0
+#define M_MBOXWIN    0x3U
+#define V_MBOXWIN(x) ((x) << S_MBOXWIN)
+#define G_MBOXWIN(x) (((x) >> S_MBOXWIN) & M_MBOXWIN)
+
+#define A_PCIE_MAILBOX_OFFSET 0x30ac
+#define A_PCIE_MA_CTRL 0x30b0
+
+#define S_MA_TAGFREE    29
+#define V_MA_TAGFREE(x) ((x) << S_MA_TAGFREE)
+#define F_MA_TAGFREE    V_MA_TAGFREE(1U)
+
+#define S_MA_MAXRSPCNT    24
+#define M_MA_MAXRSPCNT    0x1fU
+#define V_MA_MAXRSPCNT(x) ((x) << S_MA_MAXRSPCNT)
+#define G_MA_MAXRSPCNT(x) (((x) >> S_MA_MAXRSPCNT) & M_MA_MAXRSPCNT)
+
+#define S_MA_MAXREQCNT    16
+#define M_MA_MAXREQCNT    0x1fU
+#define V_MA_MAXREQCNT(x) ((x) << S_MA_MAXREQCNT)
+#define G_MA_MAXREQCNT(x) (((x) >> S_MA_MAXREQCNT) & M_MA_MAXREQCNT)
+
+#define S_MA_LE    15
+#define V_MA_LE(x) ((x) << S_MA_LE)
+#define F_MA_LE    V_MA_LE(1U)
+
+#define S_MA_MAXPYLDSIZE    12
+#define M_MA_MAXPYLDSIZE    0x7U
+#define V_MA_MAXPYLDSIZE(x) ((x) << S_MA_MAXPYLDSIZE)
+#define G_MA_MAXPYLDSIZE(x) (((x) >> S_MA_MAXPYLDSIZE) & M_MA_MAXPYLDSIZE)
+
+#define S_MA_MAXRDREQSIZE    8
+#define M_MA_MAXRDREQSIZE    0x7U
+#define V_MA_MAXRDREQSIZE(x) ((x) << S_MA_MAXRDREQSIZE)
+#define G_MA_MAXRDREQSIZE(x) (((x) >> S_MA_MAXRDREQSIZE) & M_MA_MAXRDREQSIZE)
+
+#define S_MA_MAXTAG    0
+#define M_MA_MAXTAG    0x1fU
+#define V_MA_MAXTAG(x) ((x) << S_MA_MAXTAG)
+#define G_MA_MAXTAG(x) (((x) >> S_MA_MAXTAG) & M_MA_MAXTAG)
+
+#define A_PCIE_MA_SYNC 0x30b4
+#define A_PCIE_FW 0x30b8
+#define A_PCIE_FW_PF 0x30bc
+#define A_PCIE_PIO_PAUSE 0x30dc
+
+#define S_PIOPAUSEDONE    31
+#define V_PIOPAUSEDONE(x) ((x) << S_PIOPAUSEDONE)
+#define F_PIOPAUSEDONE    V_PIOPAUSEDONE(1U)
+
+#define S_PIOPAUSETIME    4
+#define M_PIOPAUSETIME    0xffffffU
+#define V_PIOPAUSETIME(x) ((x) << S_PIOPAUSETIME)
+#define G_PIOPAUSETIME(x) (((x) >> S_PIOPAUSETIME) & M_PIOPAUSETIME)
+
+#define S_PIOPAUSE    0
+#define V_PIOPAUSE(x) ((x) << S_PIOPAUSE)
+#define F_PIOPAUSE    V_PIOPAUSE(1U)
+
+#define A_PCIE_SYS_CFG_READY 0x30e0
+#define A_PCIE_STATIC_CFG1 0x30e4
+
+#define S_LINKDOWN_RESET_EN    26
+#define V_LINKDOWN_RESET_EN(x) ((x) << S_LINKDOWN_RESET_EN)
+#define F_LINKDOWN_RESET_EN    V_LINKDOWN_RESET_EN(1U)
+
+#define S_IN_WR_DISCONTIG    25
+#define V_IN_WR_DISCONTIG(x) ((x) << S_IN_WR_DISCONTIG)
+#define F_IN_WR_DISCONTIG    V_IN_WR_DISCONTIG(1U)
+
+#define S_IN_RD_CPLSIZE    22
+#define M_IN_RD_CPLSIZE    0x7U
+#define V_IN_RD_CPLSIZE(x) ((x) << S_IN_RD_CPLSIZE)
+#define G_IN_RD_CPLSIZE(x) (((x) >> S_IN_RD_CPLSIZE) & M_IN_RD_CPLSIZE)
+
+#define S_IN_RD_BUFMODE    20
+#define M_IN_RD_BUFMODE    0x3U
+#define V_IN_RD_BUFMODE(x) ((x) << S_IN_RD_BUFMODE)
+#define G_IN_RD_BUFMODE(x) (((x) >> S_IN_RD_BUFMODE) & M_IN_RD_BUFMODE)
+
+#define S_GBIF_NPTRANS_TOT    18
+#define M_GBIF_NPTRANS_TOT    0x3U
+#define V_GBIF_NPTRANS_TOT(x) ((x) << S_GBIF_NPTRANS_TOT)
+#define G_GBIF_NPTRANS_TOT(x) (((x) >> S_GBIF_NPTRANS_TOT) & M_GBIF_NPTRANS_TOT)
+
+#define S_IN_PDAT_TOT    15
+#define M_IN_PDAT_TOT    0x7U
+#define V_IN_PDAT_TOT(x) ((x) << S_IN_PDAT_TOT)
+#define G_IN_PDAT_TOT(x) (((x) >> S_IN_PDAT_TOT) & M_IN_PDAT_TOT)
+
+#define S_PCIE_NPTRANS_TOT    12
+#define M_PCIE_NPTRANS_TOT    0x7U
+#define V_PCIE_NPTRANS_TOT(x) ((x) << S_PCIE_NPTRANS_TOT)
+#define G_PCIE_NPTRANS_TOT(x) (((x) >> S_PCIE_NPTRANS_TOT) & M_PCIE_NPTRANS_TOT)
+
+#define S_OUT_PDAT_TOT    9
+#define M_OUT_PDAT_TOT    0x7U
+#define V_OUT_PDAT_TOT(x) ((x) << S_OUT_PDAT_TOT)
+#define G_OUT_PDAT_TOT(x) (((x) >> S_OUT_PDAT_TOT) & M_OUT_PDAT_TOT)
+
+#define S_GBIF_MAX_WRSIZE    6
+#define M_GBIF_MAX_WRSIZE    0x7U
+#define V_GBIF_MAX_WRSIZE(x) ((x) << S_GBIF_MAX_WRSIZE)
+#define G_GBIF_MAX_WRSIZE(x) (((x) >> S_GBIF_MAX_WRSIZE) & M_GBIF_MAX_WRSIZE)
+
+#define S_GBIF_MAX_RDSIZE    3
+#define M_GBIF_MAX_RDSIZE    0x7U
+#define V_GBIF_MAX_RDSIZE(x) ((x) << S_GBIF_MAX_RDSIZE)
+#define G_GBIF_MAX_RDSIZE(x) (((x) >> S_GBIF_MAX_RDSIZE) & M_GBIF_MAX_RDSIZE)
+
+#define S_PCIE_MAX_RDSIZE    0
+#define M_PCIE_MAX_RDSIZE    0x7U
+#define V_PCIE_MAX_RDSIZE(x) ((x) << S_PCIE_MAX_RDSIZE)
+#define G_PCIE_MAX_RDSIZE(x) (((x) >> S_PCIE_MAX_RDSIZE) & M_PCIE_MAX_RDSIZE)
+
+#define A_PCIE_DBG_INDIR_REQ 0x30ec
+
+#define S_DBGENABLE    31
+#define V_DBGENABLE(x) ((x) << S_DBGENABLE)
+#define F_DBGENABLE    V_DBGENABLE(1U)
+
+#define S_DBGAUTOINC    30
+#define V_DBGAUTOINC(x) ((x) << S_DBGAUTOINC)
+#define F_DBGAUTOINC    V_DBGAUTOINC(1U)
+
+#define S_POINTER    8
+#define M_POINTER    0xffffU
+#define V_POINTER(x) ((x) << S_POINTER)
+#define G_POINTER(x) (((x) >> S_POINTER) & M_POINTER)
+
+#define S_SELECT    0
+#define M_SELECT    0xfU
+#define V_SELECT(x) ((x) << S_SELECT)
+#define G_SELECT(x) (((x) >> S_SELECT) & M_SELECT)
+
+#define A_PCIE_DBG_INDIR_DATA_0 0x30f0
+#define A_PCIE_DBG_INDIR_DATA_1 0x30f4
+#define A_PCIE_DBG_INDIR_DATA_2 0x30f8
+#define A_PCIE_DBG_INDIR_DATA_3 0x30fc
+#define A_PCIE_FUNC_INT_CFG 0x3100
+
+#define S_PBAOFST    28
+#define M_PBAOFST    0xfU
+#define V_PBAOFST(x) ((x) << S_PBAOFST)
+#define G_PBAOFST(x) (((x) >> S_PBAOFST) & M_PBAOFST)
+
+#define S_TABOFST    24
+#define M_TABOFST    0xfU
+#define V_TABOFST(x) ((x) << S_TABOFST)
+#define G_TABOFST(x) (((x) >> S_TABOFST) & M_TABOFST)
+
+#define S_VECNUM    12
+#define M_VECNUM    0x3ffU
+#define V_VECNUM(x) ((x) << S_VECNUM)
+#define G_VECNUM(x) (((x) >> S_VECNUM) & M_VECNUM)
+
+#define S_VECBASE    0
+#define M_VECBASE    0x7ffU
+#define V_VECBASE(x) ((x) << S_VECBASE)
+#define G_VECBASE(x) (((x) >> S_VECBASE) & M_VECBASE)
+
+#define A_PCIE_FUNC_CTL_STAT 0x3104
+
+#define S_SENDFLRRSP    31
+#define V_SENDFLRRSP(x) ((x) << S_SENDFLRRSP)
+#define F_SENDFLRRSP    V_SENDFLRRSP(1U)
+
+#define S_IMMFLRRSP    24
+#define V_IMMFLRRSP(x) ((x) << S_IMMFLRRSP)
+#define F_IMMFLRRSP    V_IMMFLRRSP(1U)
+
+#define S_TXNDISABLE    20
+#define V_TXNDISABLE(x) ((x) << S_TXNDISABLE)
+#define F_TXNDISABLE    V_TXNDISABLE(1U)
+
+#define S_PNDTXNS    8
+#define M_PNDTXNS    0x3ffU
+#define V_PNDTXNS(x) ((x) << S_PNDTXNS)
+#define G_PNDTXNS(x) (((x) >> S_PNDTXNS) & M_PNDTXNS)
+
+#define S_VFVLD    3
+#define V_VFVLD(x) ((x) << S_VFVLD)
+#define F_VFVLD    V_VFVLD(1U)
+
+#define S_PFNUM    0
+#define M_PFNUM    0x7U
+#define V_PFNUM(x) ((x) << S_PFNUM)
+#define G_PFNUM(x) (((x) >> S_PFNUM) & M_PFNUM)
+
+#define A_PCIE_FID 0x3900
+
+#define S_PAD    11
+#define V_PAD(x) ((x) << S_PAD)
+#define F_PAD    V_PAD(1U)
+
+#define S_TC    8
+#define M_TC    0x7U
+#define V_TC(x) ((x) << S_TC)
+#define G_TC(x) (((x) >> S_TC) & M_TC)
+
+#define S_FUNC    0
+#define M_FUNC    0xffU
+#define V_FUNC(x) ((x) << S_FUNC)
+#define G_FUNC(x) (((x) >> S_FUNC) & M_FUNC)
+
+#define A_PCIE_CORE_UTL_SYSTEM_BUS_CONTROL 0x5900
+
+#define S_SMTD    27
+#define V_SMTD(x) ((x) << S_SMTD)
+#define F_SMTD    V_SMTD(1U)
+
+#define S_SSTD    26
+#define V_SSTD(x) ((x) << S_SSTD)
+#define F_SSTD    V_SSTD(1U)
+
+#define S_SWD0    23
+#define V_SWD0(x) ((x) << S_SWD0)
+#define F_SWD0    V_SWD0(1U)
+
+#define S_SWD1    22
+#define V_SWD1(x) ((x) << S_SWD1)
+#define F_SWD1    V_SWD1(1U)
+
+#define S_SWD2    21
+#define V_SWD2(x) ((x) << S_SWD2)
+#define F_SWD2    V_SWD2(1U)
+
+#define S_SWD3    20
+#define V_SWD3(x) ((x) << S_SWD3)
+#define F_SWD3    V_SWD3(1U)
+
+#define S_SWD4    19
+#define V_SWD4(x) ((x) << S_SWD4)
+#define F_SWD4    V_SWD4(1U)
+
+#define S_SWD5    18
+#define V_SWD5(x) ((x) << S_SWD5)
+#define F_SWD5    V_SWD5(1U)
+
+#define S_SWD6    17
+#define V_SWD6(x) ((x) << S_SWD6)
+#define F_SWD6    V_SWD6(1U)
+
+#define S_SWD7    16
+#define V_SWD7(x) ((x) << S_SWD7)
+#define F_SWD7    V_SWD7(1U)
+
+#define S_SWD8    15
+#define V_SWD8(x) ((x) << S_SWD8)
+#define F_SWD8    V_SWD8(1U)
+
+#define S_SRD0    13
+#define V_SRD0(x) ((x) << S_SRD0)
+#define F_SRD0    V_SRD0(1U)
+
+#define S_SRD1    12
+#define V_SRD1(x) ((x) << S_SRD1)
+#define F_SRD1    V_SRD1(1U)
+
+#define S_SRD2    11
+#define V_SRD2(x) ((x) << S_SRD2)
+#define F_SRD2    V_SRD2(1U)
+
+#define S_SRD3    10
+#define V_SRD3(x) ((x) << S_SRD3)
+#define F_SRD3    V_SRD3(1U)
+
+#define S_SRD4    9
+#define V_SRD4(x) ((x) << S_SRD4)
+#define F_SRD4    V_SRD4(1U)
+
+#define S_SRD5    8
+#define V_SRD5(x) ((x) << S_SRD5)
+#define F_SRD5    V_SRD5(1U)
+
+#define S_SRD6    7
+#define V_SRD6(x) ((x) << S_SRD6)
+#define F_SRD6    V_SRD6(1U)
+
+#define S_SRD7    6
+#define V_SRD7(x) ((x) << S_SRD7)
+#define F_SRD7    V_SRD7(1U)
+
+#define S_SRD8    5
+#define V_SRD8(x) ((x) << S_SRD8)
+#define F_SRD8    V_SRD8(1U)
+
+#define S_CRRE    3
+#define V_CRRE(x) ((x) << S_CRRE)
+#define F_CRRE    V_CRRE(1U)
+
+#define S_CRMC    0
+#define M_CRMC    0x7U
+#define V_CRMC(x) ((x) << S_CRMC)
+#define G_CRMC(x) (((x) >> S_CRMC) & M_CRMC)
+
+#define A_PCIE_CORE_UTL_STATUS 0x5904
+
+#define S_USBP    31
+#define V_USBP(x) ((x) << S_USBP)
+#define F_USBP    V_USBP(1U)
+
+#define S_UPEP    30
+#define V_UPEP(x) ((x) << S_UPEP)
+#define F_UPEP    V_UPEP(1U)
+
+#define S_RCEP    29
+#define V_RCEP(x) ((x) << S_RCEP)
+#define F_RCEP    V_RCEP(1U)
+
+#define S_EPEP    28
+#define V_EPEP(x) ((x) << S_EPEP)
+#define F_EPEP    V_EPEP(1U)
+
+#define S_USBS    27
+#define V_USBS(x) ((x) << S_USBS)
+#define F_USBS    V_USBS(1U)
+
+#define S_UPES    26
+#define V_UPES(x) ((x) << S_UPES)
+#define F_UPES    V_UPES(1U)
+
+#define S_RCES    25
+#define V_RCES(x) ((x) << S_RCES)
+#define F_RCES    V_RCES(1U)
+
+#define S_EPES    24
+#define V_EPES(x) ((x) << S_EPES)
+#define F_EPES    V_EPES(1U)
+
+#define A_PCIE_CORE_UTL_SYSTEM_BUS_AGENT_STATUS 0x5908
+
+#define S_RNPP    31
+#define V_RNPP(x) ((x) << S_RNPP)
+#define F_RNPP    V_RNPP(1U)
+
+#define S_RPCP    29
+#define V_RPCP(x) ((x) << S_RPCP)
+#define F_RPCP    V_RPCP(1U)
+
+#define S_RCIP    27
+#define V_RCIP(x) ((x) << S_RCIP)
+#define F_RCIP    V_RCIP(1U)
+
+#define S_RCCP    26
+#define V_RCCP(x) ((x) << S_RCCP)
+#define F_RCCP    V_RCCP(1U)
+
+#define S_RFTP    23
+#define V_RFTP(x) ((x) << S_RFTP)
+#define F_RFTP    V_RFTP(1U)
+
+#define S_PTRP    20
+#define V_PTRP(x) ((x) << S_PTRP)
+#define F_PTRP    V_PTRP(1U)
+
+#define A_PCIE_CORE_UTL_SYSTEM_BUS_AGENT_ERROR_SEVERITY 0x590c
+
+#define S_RNPS    31
+#define V_RNPS(x) ((x) << S_RNPS)
+#define F_RNPS    V_RNPS(1U)
+
+#define S_RPCS    29
+#define V_RPCS(x) ((x) << S_RPCS)
+#define F_RPCS    V_RPCS(1U)
+
+#define S_RCIS    27
+#define V_RCIS(x) ((x) << S_RCIS)
+#define F_RCIS    V_RCIS(1U)
+
+#define S_RCCS    26
+#define V_RCCS(x) ((x) << S_RCCS)
+#define F_RCCS    V_RCCS(1U)
+
+#define S_RFTS    23
+#define V_RFTS(x) ((x) << S_RFTS)
+#define F_RFTS    V_RFTS(1U)
+
+#define A_PCIE_CORE_UTL_SYSTEM_BUS_AGENT_INTERRUPT_ENABLE 0x5910
+
+#define S_RNPI    31
+#define V_RNPI(x) ((x) << S_RNPI)
+#define F_RNPI    V_RNPI(1U)
+
+#define S_RPCI    29
+#define V_RPCI(x) ((x) << S_RPCI)
+#define F_RPCI    V_RPCI(1U)
+
+#define S_RCII    27
+#define V_RCII(x) ((x) << S_RCII)
+#define F_RCII    V_RCII(1U)
+
+#define S_RCCI    26
+#define V_RCCI(x) ((x) << S_RCCI)
+#define F_RCCI    V_RCCI(1U)
+
+#define S_RFTI    23
+#define V_RFTI(x) ((x) << S_RFTI)
+#define F_RFTI    V_RFTI(1U)
+
+#define A_PCIE_CORE_SYSTEM_BUS_BURST_SIZE_CONFIGURATION 0x5920
+
+#define S_SBRS    28
+#define M_SBRS    0x7U
+#define V_SBRS(x) ((x) << S_SBRS)
+#define G_SBRS(x) (((x) >> S_SBRS) & M_SBRS)
+
+#define S_OTWS    20
+#define M_OTWS    0x7U
+#define V_OTWS(x) ((x) << S_OTWS)
+#define G_OTWS(x) (((x) >> S_OTWS) & M_OTWS)
+
+#define A_PCIE_CORE_REVISION_ID 0x5924
+
+#define S_RVID    20
+#define M_RVID    0xfffU
+#define V_RVID(x) ((x) << S_RVID)
+#define G_RVID(x) (((x) >> S_RVID) & M_RVID)
+
+#define S_BRVN    12
+#define M_BRVN    0xffU
+#define V_BRVN(x) ((x) << S_BRVN)
+#define G_BRVN(x) (((x) >> S_BRVN) & M_BRVN)
+
+#define A_PCIE_CORE_OUTBOUND_POSTED_HEADER_BUFFER_ALLOCATION 0x5960
+
+#define S_OP0H    24
+#define M_OP0H    0xfU
+#define V_OP0H(x) ((x) << S_OP0H)
+#define G_OP0H(x) (((x) >> S_OP0H) & M_OP0H)
+
+#define S_OP1H    16
+#define M_OP1H    0xfU
+#define V_OP1H(x) ((x) << S_OP1H)
+#define G_OP1H(x) (((x) >> S_OP1H) & M_OP1H)
+
+#define S_OP2H    8
+#define M_OP2H    0xfU
+#define V_OP2H(x) ((x) << S_OP2H)
+#define G_OP2H(x) (((x) >> S_OP2H) & M_OP2H)
+
+#define S_OP3H    0
+#define M_OP3H    0xfU
+#define V_OP3H(x) ((x) << S_OP3H)
+#define G_OP3H(x) (((x) >> S_OP3H) & M_OP3H)
+
+#define A_PCIE_CORE_OUTBOUND_POSTED_DATA_BUFFER_ALLOCATION 0x5968
+
+#define S_OP0D    24
+#define M_OP0D    0x7fU
+#define V_OP0D(x) ((x) << S_OP0D)
+#define G_OP0D(x) (((x) >> S_OP0D) & M_OP0D)
+
+#define S_OP1D    16
+#define M_OP1D    0x7fU
+#define V_OP1D(x) ((x) << S_OP1D)
+#define G_OP1D(x) (((x) >> S_OP1D) & M_OP1D)
+
+#define S_OP2D    8
+#define M_OP2D    0x7fU
+#define V_OP2D(x) ((x) << S_OP2D)
+#define G_OP2D(x) (((x) >> S_OP2D) & M_OP2D)
+
+#define S_OP3D    0
+#define M_OP3D    0x7fU
+#define V_OP3D(x) ((x) << S_OP3D)
+#define G_OP3D(x) (((x) >> S_OP3D) & M_OP3D)
+
+#define A_PCIE_CORE_INBOUND_POSTED_HEADER_BUFFER_ALLOCATION 0x5970
+
+#define S_IP0H    24
+#define M_IP0H    0x3fU
+#define V_IP0H(x) ((x) << S_IP0H)
+#define G_IP0H(x) (((x) >> S_IP0H) & M_IP0H)
+
+#define S_IP1H    16
+#define M_IP1H    0x3fU
+#define V_IP1H(x) ((x) << S_IP1H)
+#define G_IP1H(x) (((x) >> S_IP1H) & M_IP1H)
+
+#define S_IP2H    8
+#define M_IP2H    0x3fU
+#define V_IP2H(x) ((x) << S_IP2H)
+#define G_IP2H(x) (((x) >> S_IP2H) & M_IP2H)
+
+#define S_IP3H    0
+#define M_IP3H    0x3fU
+#define V_IP3H(x) ((x) << S_IP3H)
+#define G_IP3H(x) (((x) >> S_IP3H) & M_IP3H)
+
+#define A_PCIE_CORE_INBOUND_POSTED_DATA_BUFFER_ALLOCATION 0x5978
+
+#define S_IP0D    24
+#define M_IP0D    0xffU
+#define V_IP0D(x) ((x) << S_IP0D)
+#define G_IP0D(x) (((x) >> S_IP0D) & M_IP0D)
+
+#define S_IP1D    16
+#define M_IP1D    0xffU
+#define V_IP1D(x) ((x) << S_IP1D)
+#define G_IP1D(x) (((x) >> S_IP1D) & M_IP1D)
+
+#define S_IP2D    8
+#define M_IP2D    0xffU
+#define V_IP2D(x) ((x) << S_IP2D)
+#define G_IP2D(x) (((x) >> S_IP2D) & M_IP2D)
+
+#define S_IP3D    0
+#define M_IP3D    0xffU
+#define V_IP3D(x) ((x) << S_IP3D)
+#define G_IP3D(x) (((x) >> S_IP3D) & M_IP3D)
+
+#define A_PCIE_CORE_OUTBOUND_NON_POSTED_BUFFER_ALLOCATION 0x5980
+
+#define S_ON0H    24
+#define M_ON0H    0xfU
+#define V_ON0H(x) ((x) << S_ON0H)
+#define G_ON0H(x) (((x) >> S_ON0H) & M_ON0H)
+
+#define S_ON1H    16
+#define M_ON1H    0xfU
+#define V_ON1H(x) ((x) << S_ON1H)
+#define G_ON1H(x) (((x) >> S_ON1H) & M_ON1H)
+
+#define S_ON2H    8
+#define M_ON2H    0xfU
+#define V_ON2H(x) ((x) << S_ON2H)
+#define G_ON2H(x) (((x) >> S_ON2H) & M_ON2H)
+
+#define S_ON3H    0
+#define M_ON3H    0xfU
+#define V_ON3H(x) ((x) << S_ON3H)
+#define G_ON3H(x) (((x) >> S_ON3H) & M_ON3H)
+
+#define A_PCIE_CORE_INBOUND_NON_POSTED_REQUESTS_BUFFER_ALLOCATION 0x5988
+
+#define S_IN0H    24
+#define M_IN0H    0x3fU
+#define V_IN0H(x) ((x) << S_IN0H)
+#define G_IN0H(x) (((x) >> S_IN0H) & M_IN0H)
+
+#define S_IN1H    16
+#define M_IN1H    0x3fU
+#define V_IN1H(x) ((x) << S_IN1H)
+#define G_IN1H(x) (((x) >> S_IN1H) & M_IN1H)
+
+#define S_IN2H    8
+#define M_IN2H    0x3fU
+#define V_IN2H(x) ((x) << S_IN2H)
+#define G_IN2H(x) (((x) >> S_IN2H) & M_IN2H)
+
+#define S_IN3H    0
+#define M_IN3H    0x3fU
+#define V_IN3H(x) ((x) << S_IN3H)
+#define G_IN3H(x) (((x) >> S_IN3H) & M_IN3H)
+
+#define A_PCIE_CORE_PCI_EXPRESS_TAGS_ALLOCATION 0x5990
+
+#define S_OC0T    24
+#define M_OC0T    0xffU
+#define V_OC0T(x) ((x) << S_OC0T)
+#define G_OC0T(x) (((x) >> S_OC0T) & M_OC0T)
+
+#define S_OC1T    16
+#define M_OC1T    0xffU
+#define V_OC1T(x) ((x) << S_OC1T)
+#define G_OC1T(x) (((x) >> S_OC1T) & M_OC1T)
+
+#define S_OC2T    8
+#define M_OC2T    0xffU
+#define V_OC2T(x) ((x) << S_OC2T)
+#define G_OC2T(x) (((x) >> S_OC2T) & M_OC2T)
+
+#define S_OC3T    0
+#define M_OC3T    0xffU
+#define V_OC3T(x) ((x) << S_OC3T)
+#define G_OC3T(x) (((x) >> S_OC3T) & M_OC3T)
+
+#define A_PCIE_CORE_GBIF_READ_TAGS_ALLOCATION 0x5998
+
+#define S_IC0T    24
+#define M_IC0T    0x3fU
+#define V_IC0T(x) ((x) << S_IC0T)
+#define G_IC0T(x) (((x) >> S_IC0T) & M_IC0T)
+
+#define S_IC1T    16
+#define M_IC1T    0x3fU
+#define V_IC1T(x) ((x) << S_IC1T)
+#define G_IC1T(x) (((x) >> S_IC1T) & M_IC1T)
+
+#define S_IC2T    8
+#define M_IC2T    0x3fU
+#define V_IC2T(x) ((x) << S_IC2T)
+#define G_IC2T(x) (((x) >> S_IC2T) & M_IC2T)
+
+#define S_IC3T    0
+#define M_IC3T    0x3fU
+#define V_IC3T(x) ((x) << S_IC3T)
+#define G_IC3T(x) (((x) >> S_IC3T) & M_IC3T)
+
+#define A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_CONTROL 0x59a0
+
+#define S_VRB0    31
+#define V_VRB0(x) ((x) << S_VRB0)
+#define F_VRB0    V_VRB0(1U)
+
+#define S_VRB1    30
+#define V_VRB1(x) ((x) << S_VRB1)
+#define F_VRB1    V_VRB1(1U)
+
+#define S_VRB2    29
+#define V_VRB2(x) ((x) << S_VRB2)
+#define F_VRB2    V_VRB2(1U)
+
+#define S_VRB3    28
+#define V_VRB3(x) ((x) << S_VRB3)
+#define F_VRB3    V_VRB3(1U)
+
+#define S_PSFE    26
+#define V_PSFE(x) ((x) << S_PSFE)
+#define F_PSFE    V_PSFE(1U)
+
+#define S_RVDE    25
+#define V_RVDE(x) ((x) << S_RVDE)
+#define F_RVDE    V_RVDE(1U)
+
+#define S_TXE0    23
+#define V_TXE0(x) ((x) << S_TXE0)
+#define F_TXE0    V_TXE0(1U)
+
+#define S_TXE1    22
+#define V_TXE1(x) ((x) << S_TXE1)
+#define F_TXE1    V_TXE1(1U)
+
+#define S_TXE2    21
+#define V_TXE2(x) ((x) << S_TXE2)
+#define F_TXE2    V_TXE2(1U)
+
+#define S_TXE3    20
+#define V_TXE3(x) ((x) << S_TXE3)
+#define F_TXE3    V_TXE3(1U)
+
+#define S_RPAM    13
+#define V_RPAM(x) ((x) << S_RPAM)
+#define F_RPAM    V_RPAM(1U)
+
+#define S_RTOS    4
+#define M_RTOS    0xfU
+#define V_RTOS(x) ((x) << S_RTOS)
+#define G_RTOS(x) (((x) >> S_RTOS) & M_RTOS)
+
+#define A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_STATUS 0x59a4
+
+#define S_TPCP    30
+#define V_TPCP(x) ((x) << S_TPCP)
+#define F_TPCP    V_TPCP(1U)
+
+#define S_TNPP    29
+#define V_TNPP(x) ((x) << S_TNPP)
+#define F_TNPP    V_TNPP(1U)
+
+#define S_TFTP    28
+#define V_TFTP(x) ((x) << S_TFTP)
+#define F_TFTP    V_TFTP(1U)
+
+#define S_TCAP    27
+#define V_TCAP(x) ((x) << S_TCAP)
+#define F_TCAP    V_TCAP(1U)
+
+#define S_TCIP    26
+#define V_TCIP(x) ((x) << S_TCIP)
+#define F_TCIP    V_TCIP(1U)
+
+#define S_RCAP    25
+#define V_RCAP(x) ((x) << S_RCAP)
+#define F_RCAP    V_RCAP(1U)
+
+#define S_PLUP    23
+#define V_PLUP(x) ((x) << S_PLUP)
+#define F_PLUP    V_PLUP(1U)
+
+#define S_PLDN    22
+#define V_PLDN(x) ((x) << S_PLDN)
+#define F_PLDN    V_PLDN(1U)
+
+#define S_OTDD    21
+#define V_OTDD(x) ((x) << S_OTDD)
+#define F_OTDD    V_OTDD(1U)
+
+#define S_GTRP    20
+#define V_GTRP(x) ((x) << S_GTRP)
+#define F_GTRP    V_GTRP(1U)
+
+#define S_RDPE    18
+#define V_RDPE(x) ((x) << S_RDPE)
+#define F_RDPE    V_RDPE(1U)
+
+#define S_TDCE    17
+#define V_TDCE(x) ((x) << S_TDCE)
+#define F_TDCE    V_TDCE(1U)
+
+#define S_TDUE    16
+#define V_TDUE(x) ((x) << S_TDUE)
+#define F_TDUE    V_TDUE(1U)
+
+#define A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_ERROR_SEVERITY 0x59a8
+
+#define S_TPCS    30
+#define V_TPCS(x) ((x) << S_TPCS)
+#define F_TPCS    V_TPCS(1U)
+
+#define S_TNPS    29
+#define V_TNPS(x) ((x) << S_TNPS)
+#define F_TNPS    V_TNPS(1U)
+
+#define S_TFTS    28
+#define V_TFTS(x) ((x) << S_TFTS)
+#define F_TFTS    V_TFTS(1U)
+
+#define S_TCAS    27
+#define V_TCAS(x) ((x) << S_TCAS)
+#define F_TCAS    V_TCAS(1U)
+
+#define S_TCIS    26
+#define V_TCIS(x) ((x) << S_TCIS)
+#define F_TCIS    V_TCIS(1U)
+
+#define S_RCAS    25
+#define V_RCAS(x) ((x) << S_RCAS)
+#define F_RCAS    V_RCAS(1U)
+
+#define S_PLUS    23
+#define V_PLUS(x) ((x) << S_PLUS)
+#define F_PLUS    V_PLUS(1U)
+
+#define S_PLDS    22
+#define V_PLDS(x) ((x) << S_PLDS)
+#define F_PLDS    V_PLDS(1U)
+
+#define S_OTDS    21
+#define V_OTDS(x) ((x) << S_OTDS)
+#define F_OTDS    V_OTDS(1U)
+
+#define S_RDPS    18
+#define V_RDPS(x) ((x) << S_RDPS)
+#define F_RDPS    V_RDPS(1U)
+
+#define S_TDCS    17
+#define V_TDCS(x) ((x) << S_TDCS)
+#define F_TDCS    V_TDCS(1U)
+
+#define S_TDUS    16
+#define V_TDUS(x) ((x) << S_TDUS)
+#define F_TDUS    V_TDUS(1U)
+
+#define A_PCIE_CORE_UTL_PCI_EXPRESS_PORT_INTERRUPT_ENABLE 0x59ac
+
+#define S_TPCI    30
+#define V_TPCI(x) ((x) << S_TPCI)
+#define F_TPCI    V_TPCI(1U)
+
+#define S_TNPI    29
+#define V_TNPI(x) ((x) << S_TNPI)
+#define F_TNPI    V_TNPI(1U)
+
+#define S_TFTI    28
+#define V_TFTI(x) ((x) << S_TFTI)
+#define F_TFTI    V_TFTI(1U)
+
+#define S_TCAI    27
+#define V_TCAI(x) ((x) << S_TCAI)
+#define F_TCAI    V_TCAI(1U)
+
+#define S_TCII    26
+#define V_TCII(x) ((x) << S_TCII)
+#define F_TCII    V_TCII(1U)
+
+#define S_RCAI    25
+#define V_RCAI(x) ((x) << S_RCAI)
+#define F_RCAI    V_RCAI(1U)
+
+#define S_PLUI    23
+#define V_PLUI(x) ((x) << S_PLUI)
+#define F_PLUI    V_PLUI(1U)
+
+#define S_PLDI    22
+#define V_PLDI(x) ((x) << S_PLDI)
+#define F_PLDI    V_PLDI(1U)
+
+#define S_OTDI    21
+#define V_OTDI(x) ((x) << S_OTDI)
+#define F_OTDI    V_OTDI(1U)
+
+#define A_PCIE_CORE_ROOT_COMPLEX_STATUS 0x59b0
+
+#define S_RLCE    31
+#define V_RLCE(x) ((x) << S_RLCE)
+#define F_RLCE    V_RLCE(1U)
+
+#define S_RLNE    30
+#define V_RLNE(x) ((x) << S_RLNE)
+#define F_RLNE    V_RLNE(1U)
+
+#define S_RLFE    29
+#define V_RLFE(x) ((x) << S_RLFE)
+#define F_RLFE    V_RLFE(1U)
+
+#define S_RCPE    25
+#define V_RCPE(x) ((x) << S_RCPE)
+#define F_RCPE    V_RCPE(1U)
+
+#define S_RCTO    24
+#define V_RCTO(x) ((x) << S_RCTO)
+#define F_RCTO    V_RCTO(1U)
+
+#define S_PINA    23
+#define V_PINA(x) ((x) << S_PINA)
+#define F_PINA    V_PINA(1U)
+
+#define S_PINB    22
+#define V_PINB(x) ((x) << S_PINB)
+#define F_PINB    V_PINB(1U)
+
+#define S_PINC    21
+#define V_PINC(x) ((x) << S_PINC)
+#define F_PINC    V_PINC(1U)
+
+#define S_PIND    20
+#define V_PIND(x) ((x) << S_PIND)
+#define F_PIND    V_PIND(1U)
+
+#define S_ALER    19
+#define V_ALER(x) ((x) << S_ALER)
+#define F_ALER    V_ALER(1U)
+
+#define S_CRSE    18
+#define V_CRSE(x) ((x) << S_CRSE)
+#define F_CRSE    V_CRSE(1U)
+
+#define A_PCIE_CORE_ROOT_COMPLEX_ERROR_SEVERITY 0x59b4
+
+#define S_RLCS    31
+#define V_RLCS(x) ((x) << S_RLCS)
+#define F_RLCS    V_RLCS(1U)
+
+#define S_RLNS    30
+#define V_RLNS(x) ((x) << S_RLNS)
+#define F_RLNS    V_RLNS(1U)
+
+#define S_RLFS    29
+#define V_RLFS(x) ((x) << S_RLFS)
+#define F_RLFS    V_RLFS(1U)
+
+#define S_RCPS    25
+#define V_RCPS(x) ((x) << S_RCPS)
+#define F_RCPS    V_RCPS(1U)
+
+#define S_RCTS    24
+#define V_RCTS(x) ((x) << S_RCTS)
+#define F_RCTS    V_RCTS(1U)
+
+#define S_PAAS    23
+#define V_PAAS(x) ((x) << S_PAAS)
+#define F_PAAS    V_PAAS(1U)
+
+#define S_PABS    22
+#define V_PABS(x) ((x) << S_PABS)
+#define F_PABS    V_PABS(1U)
+
+#define S_PACS    21
+#define V_PACS(x) ((x) << S_PACS)
+#define F_PACS    V_PACS(1U)
+
+#define S_PADS    20
+#define V_PADS(x) ((x) << S_PADS)
+#define F_PADS    V_PADS(1U)
+
+#define S_ALES    19
+#define V_ALES(x) ((x) << S_ALES)
+#define F_ALES    V_ALES(1U)
+
+#define S_CRSS    18
+#define V_CRSS(x) ((x) << S_CRSS)
+#define F_CRSS    V_CRSS(1U)
+
+#define A_PCIE_CORE_ROOT_COMPLEX_INTERRUPT_ENABLE 0x59b8
+
+#define S_RLCI    31
+#define V_RLCI(x) ((x) << S_RLCI)
+#define F_RLCI    V_RLCI(1U)
+
+#define S_RLNI    30
+#define V_RLNI(x) ((x) << S_RLNI)
+#define F_RLNI    V_RLNI(1U)
+
+#define S_RLFI    29
+#define V_RLFI(x) ((x) << S_RLFI)
+#define F_RLFI    V_RLFI(1U)
+
+#define S_RCPI    25
+#define V_RCPI(x) ((x) << S_RCPI)
+#define F_RCPI    V_RCPI(1U)
+
+#define S_RCTI    24
+#define V_RCTI(x) ((x) << S_RCTI)
+#define F_RCTI    V_RCTI(1U)
+
+#define S_PAAI    23
+#define V_PAAI(x) ((x) << S_PAAI)
+#define F_PAAI    V_PAAI(1U)
+
+#define S_PABI    22
+#define V_PABI(x) ((x) << S_PABI)
+#define F_PABI    V_PABI(1U)
+
+#define S_PACI    21
+#define V_PACI(x) ((x) << S_PACI)
+#define F_PACI    V_PACI(1U)
+
+#define S_PADI    20
+#define V_PADI(x) ((x) << S_PADI)
+#define F_PADI    V_PADI(1U)
+
+#define S_ALEI    19
+#define V_ALEI(x) ((x) << S_ALEI)
+#define F_ALEI    V_ALEI(1U)
+
+#define S_CRSI    18
+#define V_CRSI(x) ((x) << S_CRSI)
+#define F_CRSI    V_CRSI(1U)
+
+#define A_PCIE_CORE_ENDPOINT_STATUS 0x59bc
+
+#define S_PTOM    31
+#define V_PTOM(x) ((x) << S_PTOM)
+#define F_PTOM    V_PTOM(1U)
+
+#define S_ALEA    29
+#define V_ALEA(x) ((x) << S_ALEA)
+#define F_ALEA    V_ALEA(1U)
+
+#define S_PMC0    23
+#define V_PMC0(x) ((x) << S_PMC0)
+#define F_PMC0    V_PMC0(1U)
+
+#define S_PMC1    22
+#define V_PMC1(x) ((x) << S_PMC1)
+#define F_PMC1    V_PMC1(1U)
+
+#define S_PMC2    21
+#define V_PMC2(x) ((x) << S_PMC2)
+#define F_PMC2    V_PMC2(1U)
+
+#define S_PMC3    20
+#define V_PMC3(x) ((x) << S_PMC3)
+#define F_PMC3    V_PMC3(1U)
+
+#define S_PMC4    19
+#define V_PMC4(x) ((x) << S_PMC4)
+#define F_PMC4    V_PMC4(1U)
+
+#define S_PMC5    18
+#define V_PMC5(x) ((x) << S_PMC5)
+#define F_PMC5    V_PMC5(1U)
+
+#define S_PMC6    17
+#define V_PMC6(x) ((x) << S_PMC6)
+#define F_PMC6    V_PMC6(1U)
+
+#define S_PMC7    16
+#define V_PMC7(x) ((x) << S_PMC7)
+#define F_PMC7    V_PMC7(1U)
+
+#define A_PCIE_CORE_ENDPOINT_ERROR_SEVERITY 0x59c0
+
+#define S_PTOS    31
+#define V_PTOS(x) ((x) << S_PTOS)
+#define F_PTOS    V_PTOS(1U)
+
+#define S_AENS    29
+#define V_AENS(x) ((x) << S_AENS)
+#define F_AENS    V_AENS(1U)
+
+#define S_PC0S    23
+#define V_PC0S(x) ((x) << S_PC0S)
+#define F_PC0S    V_PC0S(1U)
+
+#define S_PC1S    22
+#define V_PC1S(x) ((x) << S_PC1S)
+#define F_PC1S    V_PC1S(1U)
+
+#define S_PC2S    21
+#define V_PC2S(x) ((x) << S_PC2S)
+#define F_PC2S    V_PC2S(1U)
+
+#define S_PC3S    20
+#define V_PC3S(x) ((x) << S_PC3S)
+#define F_PC3S    V_PC3S(1U)
+
+#define S_PC4S    19
+#define V_PC4S(x) ((x) << S_PC4S)
+#define F_PC4S    V_PC4S(1U)
+
+#define S_PC5S    18
+#define V_PC5S(x) ((x) << S_PC5S)
+#define F_PC5S    V_PC5S(1U)
+
+#define S_PC6S    17
+#define V_PC6S(x) ((x) << S_PC6S)
+#define F_PC6S    V_PC6S(1U)
+
+#define S_PC7S    16
+#define V_PC7S(x) ((x) << S_PC7S)
+#define F_PC7S    V_PC7S(1U)
+
+#define S_PME0    15
+#define V_PME0(x) ((x) << S_PME0)
+#define F_PME0    V_PME0(1U)
+
+#define S_PME1    14
+#define V_PME1(x) ((x) << S_PME1)
+#define F_PME1    V_PME1(1U)
+
+#define S_PME2    13
+#define V_PME2(x) ((x) << S_PME2)
+#define F_PME2    V_PME2(1U)
+
+#define S_PME3    12
+#define V_PME3(x) ((x) << S_PME3)
+#define F_PME3    V_PME3(1U)
+
+#define S_PME4    11
+#define V_PME4(x) ((x) << S_PME4)
+#define F_PME4    V_PME4(1U)
+
+#define S_PME5    10
+#define V_PME5(x) ((x) << S_PME5)
+#define F_PME5    V_PME5(1U)
+
+#define S_PME6    9
+#define V_PME6(x) ((x) << S_PME6)
+#define F_PME6    V_PME6(1U)
+
+#define S_PME7    8
+#define V_PME7(x) ((x) << S_PME7)
+#define F_PME7    V_PME7(1U)
+
+#define A_PCIE_CORE_ENDPOINT_INTERRUPT_ENABLE 0x59c4
+
+#define S_PTOI    31
+#define V_PTOI(x) ((x) << S_PTOI)
+#define F_PTOI    V_PTOI(1U)
+
+#define S_AENI    29
+#define V_AENI(x) ((x) << S_AENI)
+#define F_AENI    V_AENI(1U)
+
+#define S_PC0I    23
+#define V_PC0I(x) ((x) << S_PC0I)
+#define F_PC0I    V_PC0I(1U)
+
+#define S_PC1I    22
+#define V_PC1I(x) ((x) << S_PC1I)
+#define F_PC1I    V_PC1I(1U)
+
+#define S_PC2I    21
+#define V_PC2I(x) ((x) << S_PC2I)
+#define F_PC2I    V_PC2I(1U)
+
+#define S_PC3I    20
+#define V_PC3I(x) ((x) << S_PC3I)
+#define F_PC3I    V_PC3I(1U)
+
+#define S_PC4I    19
+#define V_PC4I(x) ((x) << S_PC4I)
+#define F_PC4I    V_PC4I(1U)
+
+#define S_PC5I    18
+#define V_PC5I(x) ((x) << S_PC5I)
+#define F_PC5I    V_PC5I(1U)
+
+#define S_PC6I    17
+#define V_PC6I(x) ((x) << S_PC6I)
+#define F_PC6I    V_PC6I(1U)
+
+#define S_PC7I    16
+#define V_PC7I(x) ((x) << S_PC7I)
+#define F_PC7I    V_PC7I(1U)
+
+#define A_PCIE_CORE_PCI_POWER_MANAGEMENT_CONTROL_1 0x59c8
+
+#define S_TOAK    31
+#define V_TOAK(x) ((x) << S_TOAK)
+#define F_TOAK    V_TOAK(1U)
+
+#define S_L1RS    23
+#define V_L1RS(x) ((x) << S_L1RS)
+#define F_L1RS    V_L1RS(1U)
+
+#define S_L23S    22
+#define V_L23S(x) ((x) << S_L23S)
+#define F_L23S    V_L23S(1U)
+
+#define S_AL1S    21
+#define V_AL1S(x) ((x) << S_AL1S)
+#define F_AL1S    V_AL1S(1U)
+
+#define S_ALET    19
+#define V_ALET(x) ((x) << S_ALET)
+#define F_ALET    V_ALET(1U)
+
+#define A_PCIE_CORE_PCI_POWER_MANAGEMENT_CONTROL_2 0x59cc
+
+#define S_CPM0    30
+#define M_CPM0    0x3U
+#define V_CPM0(x) ((x) << S_CPM0)
+#define G_CPM0(x) (((x) >> S_CPM0) & M_CPM0)
+
+#define S_CPM1    28
+#define M_CPM1    0x3U
+#define V_CPM1(x) ((x) << S_CPM1)
+#define G_CPM1(x) (((x) >> S_CPM1) & M_CPM1)
+
+#define S_CPM2    26
+#define M_CPM2    0x3U
+#define V_CPM2(x) ((x) << S_CPM2)
+#define G_CPM2(x) (((x) >> S_CPM2) & M_CPM2)
+
+#define S_CPM3    24
+#define M_CPM3    0x3U
+#define V_CPM3(x) ((x) << S_CPM3)
+#define G_CPM3(x) (((x) >> S_CPM3) & M_CPM3)
+
+#define S_CPM4    22
+#define M_CPM4    0x3U
+#define V_CPM4(x) ((x) << S_CPM4)
+#define G_CPM4(x) (((x) >> S_CPM4) & M_CPM4)
+
+#define S_CPM5    20
+#define M_CPM5    0x3U
+#define V_CPM5(x) ((x) << S_CPM5)
+#define G_CPM5(x) (((x) >> S_CPM5) & M_CPM5)
+
+#define S_CPM6    18
+#define M_CPM6    0x3U
+#define V_CPM6(x) ((x) << S_CPM6)
+#define G_CPM6(x) (((x) >> S_CPM6) & M_CPM6)
+
+#define S_CPM7    16
+#define M_CPM7    0x3U
+#define V_CPM7(x) ((x) << S_CPM7)
+#define G_CPM7(x) (((x) >> S_CPM7) & M_CPM7)
+
+#define S_OPM0    14
+#define M_OPM0    0x3U
+#define V_OPM0(x) ((x) << S_OPM0)
+#define G_OPM0(x) (((x) >> S_OPM0) & M_OPM0)
+
+#define S_OPM1    12
+#define M_OPM1    0x3U
+#define V_OPM1(x) ((x) << S_OPM1)
+#define G_OPM1(x) (((x) >> S_OPM1) & M_OPM1)
+
+#define S_OPM2    10
+#define M_OPM2    0x3U
+#define V_OPM2(x) ((x) << S_OPM2)
+#define G_OPM2(x) (((x) >> S_OPM2) & M_OPM2)
+
+#define S_OPM3    8
+#define M_OPM3    0x3U
+#define V_OPM3(x) ((x) << S_OPM3)
+#define G_OPM3(x) (((x) >> S_OPM3) & M_OPM3)
+
+#define S_OPM4    6
+#define M_OPM4    0x3U
+#define V_OPM4(x) ((x) << S_OPM4)
+#define G_OPM4(x) (((x) >> S_OPM4) & M_OPM4)
+
+#define S_OPM5    4
+#define M_OPM5    0x3U
+#define V_OPM5(x) ((x) << S_OPM5)
+#define G_OPM5(x) (((x) >> S_OPM5) & M_OPM5)
+
+#define S_OPM6    2
+#define M_OPM6    0x3U
+#define V_OPM6(x) ((x) << S_OPM6)
+#define G_OPM6(x) (((x) >> S_OPM6) & M_OPM6)
+
+#define S_OPM7    0
+#define M_OPM7    0x3U
+#define V_OPM7(x) ((x) << S_OPM7)
+#define G_OPM7(x) (((x) >> S_OPM7) & M_OPM7)
+
+#define A_PCIE_CORE_GENERAL_PURPOSE_CONTROL_1 0x59d0
+#define A_PCIE_CORE_GENERAL_PURPOSE_CONTROL_2 0x59d4
+#define A_PCIE_REVISION 0x5a00
+#define A_PCIE_PDEBUG_INDEX 0x5a04
+
+#define S_PDEBUGSELH    16
+#define M_PDEBUGSELH    0x3fU
+#define V_PDEBUGSELH(x) ((x) << S_PDEBUGSELH)
+#define G_PDEBUGSELH(x) (((x) >> S_PDEBUGSELH) & M_PDEBUGSELH)
+
+#define S_PDEBUGSELL    0
+#define M_PDEBUGSELL    0x3fU
+#define V_PDEBUGSELL(x) ((x) << S_PDEBUGSELL)
+#define G_PDEBUGSELL(x) (((x) >> S_PDEBUGSELL) & M_PDEBUGSELL)
+
+#define A_PCIE_PDEBUG_DATA_HIGH 0x5a08
+#define A_PCIE_PDEBUG_DATA_LOW 0x5a0c
+#define A_PCIE_CDEBUG_INDEX 0x5a10
+
+#define S_CDEBUGSELH    16
+#define M_CDEBUGSELH    0xffU
+#define V_CDEBUGSELH(x) ((x) << S_CDEBUGSELH)
+#define G_CDEBUGSELH(x) (((x) >> S_CDEBUGSELH) & M_CDEBUGSELH)
+
+#define S_CDEBUGSELL    0
+#define M_CDEBUGSELL    0xffU
+#define V_CDEBUGSELL(x) ((x) << S_CDEBUGSELL)
+#define G_CDEBUGSELL(x) (((x) >> S_CDEBUGSELL) & M_CDEBUGSELL)
+
+#define A_PCIE_CDEBUG_DATA_HIGH 0x5a14
+#define A_PCIE_CDEBUG_DATA_LOW 0x5a18
+#define A_PCIE_DMAW_SOP_CNT 0x5a1c
+
+#define S_CH3    24
+#define M_CH3    0xffU
+#define V_CH3(x) ((x) << S_CH3)
+#define G_CH3(x) (((x) >> S_CH3) & M_CH3)
+
+#define S_CH2    16
+#define M_CH2    0xffU
+#define V_CH2(x) ((x) << S_CH2)
+#define G_CH2(x) (((x) >> S_CH2) & M_CH2)
+
+#define S_CH1    8
+#define M_CH1    0xffU
+#define V_CH1(x) ((x) << S_CH1)
+#define G_CH1(x) (((x) >> S_CH1) & M_CH1)
+
+#define S_CH0    0
+#define M_CH0    0xffU
+#define V_CH0(x) ((x) << S_CH0)
+#define G_CH0(x) (((x) >> S_CH0) & M_CH0)
+
+#define A_PCIE_DMAW_EOP_CNT 0x5a20
+#define A_PCIE_DMAR_REQ_CNT 0x5a24
+#define A_PCIE_DMAR_RSP_SOP_CNT 0x5a28
+#define A_PCIE_DMAR_RSP_EOP_CNT 0x5a2c
+#define A_PCIE_DMAR_RSP_ERR_CNT 0x5a30
+#define A_PCIE_DMAI_CNT 0x5a34
+#define A_PCIE_CMDW_CNT 0x5a38
+
+#define S_CH1_EOP    24
+#define M_CH1_EOP    0xffU
+#define V_CH1_EOP(x) ((x) << S_CH1_EOP)
+#define G_CH1_EOP(x) (((x) >> S_CH1_EOP) & M_CH1_EOP)
+
+#define S_CH1_SOP    16
+#define M_CH1_SOP    0xffU
+#define V_CH1_SOP(x) ((x) << S_CH1_SOP)
+#define G_CH1_SOP(x) (((x) >> S_CH1_SOP) & M_CH1_SOP)
+
+#define S_CH0_EOP    8
+#define M_CH0_EOP    0xffU
+#define V_CH0_EOP(x) ((x) << S_CH0_EOP)
+#define G_CH0_EOP(x) (((x) >> S_CH0_EOP) & M_CH0_EOP)
+
+#define S_CH0_SOP    0
+#define M_CH0_SOP    0xffU
+#define V_CH0_SOP(x) ((x) << S_CH0_SOP)
+#define G_CH0_SOP(x) (((x) >> S_CH0_SOP) & M_CH0_SOP)
+
+#define A_PCIE_CMDR_REQ_CNT 0x5a3c
+#define A_PCIE_CMDR_RSP_CNT 0x5a40
+#define A_PCIE_CMDR_RSP_ERR_CNT 0x5a44
+#define A_PCIE_HMA_REQ_CNT 0x5a48
+
+#define S_CH0_READ    16
+#define M_CH0_READ    0xffU
+#define V_CH0_READ(x) ((x) << S_CH0_READ)
+#define G_CH0_READ(x) (((x) >> S_CH0_READ) & M_CH0_READ)
+
+#define S_CH0_WEOP    8
+#define M_CH0_WEOP    0xffU
+#define V_CH0_WEOP(x) ((x) << S_CH0_WEOP)
+#define G_CH0_WEOP(x) (((x) >> S_CH0_WEOP) & M_CH0_WEOP)
+
+#define S_CH0_WSOP    0
+#define M_CH0_WSOP    0xffU
+#define V_CH0_WSOP(x) ((x) << S_CH0_WSOP)
+#define G_CH0_WSOP(x) (((x) >> S_CH0_WSOP) & M_CH0_WSOP)
+
+#define A_PCIE_HMA_RSP_CNT 0x5a4c
+#define A_PCIE_DMA10_RSP_FREE 0x5a50
+
+#define S_CH1_RSP_FREE    16
+#define M_CH1_RSP_FREE    0xfffU
+#define V_CH1_RSP_FREE(x) ((x) << S_CH1_RSP_FREE)
+#define G_CH1_RSP_FREE(x) (((x) >> S_CH1_RSP_FREE) & M_CH1_RSP_FREE)
+
+#define S_CH0_RSP_FREE    0
+#define M_CH0_RSP_FREE    0xfffU
+#define V_CH0_RSP_FREE(x) ((x) << S_CH0_RSP_FREE)
+#define G_CH0_RSP_FREE(x) (((x) >> S_CH0_RSP_FREE) & M_CH0_RSP_FREE)
+
+#define A_PCIE_DMA32_RSP_FREE 0x5a54
+
+#define S_CH3_RSP_FREE    16
+#define M_CH3_RSP_FREE    0xfffU
+#define V_CH3_RSP_FREE(x) ((x) << S_CH3_RSP_FREE)
+#define G_CH3_RSP_FREE(x) (((x) >> S_CH3_RSP_FREE) & M_CH3_RSP_FREE)
+
+#define S_CH2_RSP_FREE    0
+#define M_CH2_RSP_FREE    0xfffU
+#define V_CH2_RSP_FREE(x) ((x) << S_CH2_RSP_FREE)
+#define G_CH2_RSP_FREE(x) (((x) >> S_CH2_RSP_FREE) & M_CH2_RSP_FREE)
+
+#define A_PCIE_CMD_RSP_FREE 0x5a58
+
+#define S_CMD_CH1_RSP_FREE    16
+#define M_CMD_CH1_RSP_FREE    0x7fU
+#define V_CMD_CH1_RSP_FREE(x) ((x) << S_CMD_CH1_RSP_FREE)
+#define G_CMD_CH1_RSP_FREE(x) (((x) >> S_CMD_CH1_RSP_FREE) & M_CMD_CH1_RSP_FREE)
+
+#define S_CMD_CH0_RSP_FREE    0
+#define M_CMD_CH0_RSP_FREE    0x7fU
+#define V_CMD_CH0_RSP_FREE(x) ((x) << S_CMD_CH0_RSP_FREE)
+#define G_CMD_CH0_RSP_FREE(x) (((x) >> S_CMD_CH0_RSP_FREE) & M_CMD_CH0_RSP_FREE)
+
+#define A_PCIE_HMA_RSP_FREE 0x5a5c
+#define A_PCIE_BUS_MST_STAT_0 0x5a60
+#define A_PCIE_BUS_MST_STAT_1 0x5a64
+#define A_PCIE_BUS_MST_STAT_2 0x5a68
+#define A_PCIE_BUS_MST_STAT_3 0x5a6c
+#define A_PCIE_BUS_MST_STAT_4 0x5a70
+#define A_PCIE_BUS_MST_STAT_5 0x5a74
+#define A_PCIE_BUS_MST_STAT_6 0x5a78
+#define A_PCIE_BUS_MST_STAT_7 0x5a7c
+#define A_PCIE_RSP_ERR_STAT_0 0x5a80
+#define A_PCIE_RSP_ERR_STAT_1 0x5a84
+#define A_PCIE_RSP_ERR_STAT_2 0x5a88
+#define A_PCIE_RSP_ERR_STAT_3 0x5a8c
+#define A_PCIE_RSP_ERR_STAT_4 0x5a90
+#define A_PCIE_RSP_ERR_STAT_5 0x5a94
+#define A_PCIE_RSP_ERR_STAT_6 0x5a98
+#define A_PCIE_RSP_ERR_STAT_7 0x5a9c
+#define A_PCIE_MSI_EN_0 0x5aa0
+#define A_PCIE_MSI_EN_1 0x5aa4
+#define A_PCIE_MSI_EN_2 0x5aa8
+#define A_PCIE_MSI_EN_3 0x5aac
+#define A_PCIE_MSI_EN_4 0x5ab0
+#define A_PCIE_MSI_EN_5 0x5ab4
+#define A_PCIE_MSI_EN_6 0x5ab8
+#define A_PCIE_MSI_EN_7 0x5abc
+#define A_PCIE_MSIX_EN_0 0x5ac0
+#define A_PCIE_MSIX_EN_1 0x5ac4
+#define A_PCIE_MSIX_EN_2 0x5ac8
+#define A_PCIE_MSIX_EN_3 0x5acc
+#define A_PCIE_MSIX_EN_4 0x5ad0
+#define A_PCIE_MSIX_EN_5 0x5ad4
+#define A_PCIE_MSIX_EN_6 0x5ad8
+#define A_PCIE_MSIX_EN_7 0x5adc
+#define A_PCIE_DMA_BUF_CTL 0x5ae0
+
+#define S_BUFRDCNT    18
+#define M_BUFRDCNT    0x3fffU
+#define V_BUFRDCNT(x) ((x) << S_BUFRDCNT)
+#define G_BUFRDCNT(x) (((x) >> S_BUFRDCNT) & M_BUFRDCNT)
+
+#define S_BUFWRCNT    9
+#define M_BUFWRCNT    0x1ffU
+#define V_BUFWRCNT(x) ((x) << S_BUFWRCNT)
+#define G_BUFWRCNT(x) (((x) >> S_BUFWRCNT) & M_BUFWRCNT)
+
+#define S_MAXBUFWRREQ    0
+#define M_MAXBUFWRREQ    0x1ffU
+#define V_MAXBUFWRREQ(x) ((x) << S_MAXBUFWRREQ)
+#define G_MAXBUFWRREQ(x) (((x) >> S_MAXBUFWRREQ) & M_MAXBUFWRREQ)
+
+/* registers for module DBG */
+#define DBG_BASE_ADDR 0x6000
+
+#define A_DBG_DBG0_CFG 0x6000
+
+#define S_MODULESELECT    12
+#define M_MODULESELECT    0xffU
+#define V_MODULESELECT(x) ((x) << S_MODULESELECT)
+#define G_MODULESELECT(x) (((x) >> S_MODULESELECT) & M_MODULESELECT)
+
+#define S_REGSELECT    4
+#define M_REGSELECT    0xffU
+#define V_REGSELECT(x) ((x) << S_REGSELECT)
+#define G_REGSELECT(x) (((x) >> S_REGSELECT) & M_REGSELECT)
+
+#define S_CLKSELECT    0
+#define M_CLKSELECT    0xfU
+#define V_CLKSELECT(x) ((x) << S_CLKSELECT)
+#define G_CLKSELECT(x) (((x) >> S_CLKSELECT) & M_CLKSELECT)
+
+#define A_DBG_DBG0_EN 0x6004
+
+#define S_PORTEN_PONR    16
+#define V_PORTEN_PONR(x) ((x) << S_PORTEN_PONR)
+#define F_PORTEN_PONR    V_PORTEN_PONR(1U)
+
+#define S_PORTEN_POND    12
+#define V_PORTEN_POND(x) ((x) << S_PORTEN_POND)
+#define F_PORTEN_POND    V_PORTEN_POND(1U)
+
+#define S_SDRHALFWORD0    8
+#define V_SDRHALFWORD0(x) ((x) << S_SDRHALFWORD0)
+#define F_SDRHALFWORD0    V_SDRHALFWORD0(1U)
+
+#define S_DDREN    4
+#define V_DDREN(x) ((x) << S_DDREN)
+#define F_DDREN    V_DDREN(1U)
+
+#define S_DBG_PORTEN    0
+#define V_DBG_PORTEN(x) ((x) << S_DBG_PORTEN)
+#define F_DBG_PORTEN    V_DBG_PORTEN(1U)
+
+#define A_DBG_DBG1_CFG 0x6008
+#define A_DBG_DBG1_EN 0x600c
+#define A_DBG_GPIO_EN 0x6010
+
+#define S_GPIO15_OEN    31
+#define V_GPIO15_OEN(x) ((x) << S_GPIO15_OEN)
+#define F_GPIO15_OEN    V_GPIO15_OEN(1U)
+
+#define S_GPIO14_OEN    30
+#define V_GPIO14_OEN(x) ((x) << S_GPIO14_OEN)
+#define F_GPIO14_OEN    V_GPIO14_OEN(1U)
+
+#define S_GPIO13_OEN    29
+#define V_GPIO13_OEN(x) ((x) << S_GPIO13_OEN)
+#define F_GPIO13_OEN    V_GPIO13_OEN(1U)
+
+#define S_GPIO12_OEN    28
+#define V_GPIO12_OEN(x) ((x) << S_GPIO12_OEN)
+#define F_GPIO12_OEN    V_GPIO12_OEN(1U)
+
+#define S_GPIO11_OEN    27
+#define V_GPIO11_OEN(x) ((x) << S_GPIO11_OEN)
+#define F_GPIO11_OEN    V_GPIO11_OEN(1U)
+
+#define S_GPIO10_OEN    26
+#define V_GPIO10_OEN(x) ((x) << S_GPIO10_OEN)
+#define F_GPIO10_OEN    V_GPIO10_OEN(1U)
+
+#define S_GPIO9_OEN    25
+#define V_GPIO9_OEN(x) ((x) << S_GPIO9_OEN)
+#define F_GPIO9_OEN    V_GPIO9_OEN(1U)
+
+#define S_GPIO8_OEN    24
+#define V_GPIO8_OEN(x) ((x) << S_GPIO8_OEN)
+#define F_GPIO8_OEN    V_GPIO8_OEN(1U)
+
+#define S_GPIO7_OEN    23
+#define V_GPIO7_OEN(x) ((x) << S_GPIO7_OEN)
+#define F_GPIO7_OEN    V_GPIO7_OEN(1U)
+
+#define S_GPIO6_OEN    22
+#define V_GPIO6_OEN(x) ((x) << S_GPIO6_OEN)
+#define F_GPIO6_OEN    V_GPIO6_OEN(1U)
+
+#define S_GPIO5_OEN    21
+#define V_GPIO5_OEN(x) ((x) << S_GPIO5_OEN)
+#define F_GPIO5_OEN    V_GPIO5_OEN(1U)
+
+#define S_GPIO4_OEN    20
+#define V_GPIO4_OEN(x) ((x) << S_GPIO4_OEN)
+#define F_GPIO4_OEN    V_GPIO4_OEN(1U)
+
+#define S_GPIO3_OEN    19
+#define V_GPIO3_OEN(x) ((x) << S_GPIO3_OEN)
+#define F_GPIO3_OEN    V_GPIO3_OEN(1U)
+
+#define S_GPIO2_OEN    18
+#define V_GPIO2_OEN(x) ((x) << S_GPIO2_OEN)
+#define F_GPIO2_OEN    V_GPIO2_OEN(1U)
+
+#define S_GPIO1_OEN    17
+#define V_GPIO1_OEN(x) ((x) << S_GPIO1_OEN)
+#define F_GPIO1_OEN    V_GPIO1_OEN(1U)
+
+#define S_GPIO0_OEN    16
+#define V_GPIO0_OEN(x) ((x) << S_GPIO0_OEN)
+#define F_GPIO0_OEN    V_GPIO0_OEN(1U)
+
+#define S_GPIO15_OUT_VAL    15
+#define V_GPIO15_OUT_VAL(x) ((x) << S_GPIO15_OUT_VAL)
+#define F_GPIO15_OUT_VAL    V_GPIO15_OUT_VAL(1U)
+
+#define S_GPIO14_OUT_VAL    14
+#define V_GPIO14_OUT_VAL(x) ((x) << S_GPIO14_OUT_VAL)
+#define F_GPIO14_OUT_VAL    V_GPIO14_OUT_VAL(1U)
+
+#define S_GPIO13_OUT_VAL    13
+#define V_GPIO13_OUT_VAL(x) ((x) << S_GPIO13_OUT_VAL)
+#define F_GPIO13_OUT_VAL    V_GPIO13_OUT_VAL(1U)
+
+#define S_GPIO12_OUT_VAL    12
+#define V_GPIO12_OUT_VAL(x) ((x) << S_GPIO12_OUT_VAL)
+#define F_GPIO12_OUT_VAL    V_GPIO12_OUT_VAL(1U)
+
+#define S_GPIO11_OUT_VAL    11
+#define V_GPIO11_OUT_VAL(x) ((x) << S_GPIO11_OUT_VAL)
+#define F_GPIO11_OUT_VAL    V_GPIO11_OUT_VAL(1U)
+
+#define S_GPIO10_OUT_VAL    10
+#define V_GPIO10_OUT_VAL(x) ((x) << S_GPIO10_OUT_VAL)
+#define F_GPIO10_OUT_VAL    V_GPIO10_OUT_VAL(1U)
+
+#define S_GPIO9_OUT_VAL    9
+#define V_GPIO9_OUT_VAL(x) ((x) << S_GPIO9_OUT_VAL)
+#define F_GPIO9_OUT_VAL    V_GPIO9_OUT_VAL(1U)
+
+#define S_GPIO8_OUT_VAL    8
+#define V_GPIO8_OUT_VAL(x) ((x) << S_GPIO8_OUT_VAL)
+#define F_GPIO8_OUT_VAL    V_GPIO8_OUT_VAL(1U)
+
+#define S_GPIO7_OUT_VAL    7
+#define V_GPIO7_OUT_VAL(x) ((x) << S_GPIO7_OUT_VAL)
+#define F_GPIO7_OUT_VAL    V_GPIO7_OUT_VAL(1U)
+
+#define S_GPIO6_OUT_VAL    6
+#define V_GPIO6_OUT_VAL(x) ((x) << S_GPIO6_OUT_VAL)
+#define F_GPIO6_OUT_VAL    V_GPIO6_OUT_VAL(1U)
+
+#define S_GPIO5_OUT_VAL    5
+#define V_GPIO5_OUT_VAL(x) ((x) << S_GPIO5_OUT_VAL)
+#define F_GPIO5_OUT_VAL    V_GPIO5_OUT_VAL(1U)
+
+#define S_GPIO4_OUT_VAL    4
+#define V_GPIO4_OUT_VAL(x) ((x) << S_GPIO4_OUT_VAL)
+#define F_GPIO4_OUT_VAL    V_GPIO4_OUT_VAL(1U)
+
+#define S_GPIO3_OUT_VAL    3
+#define V_GPIO3_OUT_VAL(x) ((x) << S_GPIO3_OUT_VAL)
+#define F_GPIO3_OUT_VAL    V_GPIO3_OUT_VAL(1U)
+
+#define S_GPIO2_OUT_VAL    2
+#define V_GPIO2_OUT_VAL(x) ((x) << S_GPIO2_OUT_VAL)
+#define F_GPIO2_OUT_VAL    V_GPIO2_OUT_VAL(1U)
+
+#define S_GPIO1_OUT_VAL    1
+#define V_GPIO1_OUT_VAL(x) ((x) << S_GPIO1_OUT_VAL)
+#define F_GPIO1_OUT_VAL    V_GPIO1_OUT_VAL(1U)
+
+#define S_GPIO0_OUT_VAL    0
+#define V_GPIO0_OUT_VAL(x) ((x) << S_GPIO0_OUT_VAL)
+#define F_GPIO0_OUT_VAL    V_GPIO0_OUT_VAL(1U)
+
+#define A_DBG_GPIO_IN 0x6014
+
+#define S_GPIO15_CHG_DET    31
+#define V_GPIO15_CHG_DET(x) ((x) << S_GPIO15_CHG_DET)
+#define F_GPIO15_CHG_DET    V_GPIO15_CHG_DET(1U)
+
+#define S_GPIO14_CHG_DET    30
+#define V_GPIO14_CHG_DET(x) ((x) << S_GPIO14_CHG_DET)
+#define F_GPIO14_CHG_DET    V_GPIO14_CHG_DET(1U)
+
+#define S_GPIO13_CHG_DET    29
+#define V_GPIO13_CHG_DET(x) ((x) << S_GPIO13_CHG_DET)
+#define F_GPIO13_CHG_DET    V_GPIO13_CHG_DET(1U)
+
+#define S_GPIO12_CHG_DET    28
+#define V_GPIO12_CHG_DET(x) ((x) << S_GPIO12_CHG_DET)
+#define F_GPIO12_CHG_DET    V_GPIO12_CHG_DET(1U)
+
+#define S_GPIO11_CHG_DET    27
+#define V_GPIO11_CHG_DET(x) ((x) << S_GPIO11_CHG_DET)
+#define F_GPIO11_CHG_DET    V_GPIO11_CHG_DET(1U)
+
+#define S_GPIO10_CHG_DET    26
+#define V_GPIO10_CHG_DET(x) ((x) << S_GPIO10_CHG_DET)
+#define F_GPIO10_CHG_DET    V_GPIO10_CHG_DET(1U)
+
+#define S_GPIO9_CHG_DET    25
+#define V_GPIO9_CHG_DET(x) ((x) << S_GPIO9_CHG_DET)
+#define F_GPIO9_CHG_DET    V_GPIO9_CHG_DET(1U)
+
+#define S_GPIO8_CHG_DET    24
+#define V_GPIO8_CHG_DET(x) ((x) << S_GPIO8_CHG_DET)
+#define F_GPIO8_CHG_DET    V_GPIO8_CHG_DET(1U)
+
+#define S_GPIO7_CHG_DET    23
+#define V_GPIO7_CHG_DET(x) ((x) << S_GPIO7_CHG_DET)
+#define F_GPIO7_CHG_DET    V_GPIO7_CHG_DET(1U)
+
+#define S_GPIO6_CHG_DET    22
+#define V_GPIO6_CHG_DET(x) ((x) << S_GPIO6_CHG_DET)
+#define F_GPIO6_CHG_DET    V_GPIO6_CHG_DET(1U)
+
+#define S_GPIO5_CHG_DET    21
+#define V_GPIO5_CHG_DET(x) ((x) << S_GPIO5_CHG_DET)
+#define F_GPIO5_CHG_DET    V_GPIO5_CHG_DET(1U)
+
+#define S_GPIO4_CHG_DET    20
+#define V_GPIO4_CHG_DET(x) ((x) << S_GPIO4_CHG_DET)
+#define F_GPIO4_CHG_DET    V_GPIO4_CHG_DET(1U)
+
+#define S_GPIO3_CHG_DET    19
+#define V_GPIO3_CHG_DET(x) ((x) << S_GPIO3_CHG_DET)
+#define F_GPIO3_CHG_DET    V_GPIO3_CHG_DET(1U)
+
+#define S_GPIO2_CHG_DET    18
+#define V_GPIO2_CHG_DET(x) ((x) << S_GPIO2_CHG_DET)
+#define F_GPIO2_CHG_DET    V_GPIO2_CHG_DET(1U)
+
+#define S_GPIO1_CHG_DET    17
+#define V_GPIO1_CHG_DET(x) ((x) << S_GPIO1_CHG_DET)
+#define F_GPIO1_CHG_DET    V_GPIO1_CHG_DET(1U)
+
+#define S_GPIO0_CHG_DET    16
+#define V_GPIO0_CHG_DET(x) ((x) << S_GPIO0_CHG_DET)
+#define F_GPIO0_CHG_DET    V_GPIO0_CHG_DET(1U)
+
+#define S_GPIO15_IN    15
+#define V_GPIO15_IN(x) ((x) << S_GPIO15_IN)
+#define F_GPIO15_IN    V_GPIO15_IN(1U)
+
+#define S_GPIO14_IN    14
+#define V_GPIO14_IN(x) ((x) << S_GPIO14_IN)
+#define F_GPIO14_IN    V_GPIO14_IN(1U)
+
+#define S_GPIO13_IN    13
+#define V_GPIO13_IN(x) ((x) << S_GPIO13_IN)
+#define F_GPIO13_IN    V_GPIO13_IN(1U)
+
+#define S_GPIO12_IN    12
+#define V_GPIO12_IN(x) ((x) << S_GPIO12_IN)
+#define F_GPIO12_IN    V_GPIO12_IN(1U)
+
+#define S_GPIO11_IN    11
+#define V_GPIO11_IN(x) ((x) << S_GPIO11_IN)
+#define F_GPIO11_IN    V_GPIO11_IN(1U)
+
+#define S_GPIO10_IN    10
+#define V_GPIO10_IN(x) ((x) << S_GPIO10_IN)
+#define F_GPIO10_IN    V_GPIO10_IN(1U)
+
+#define S_GPIO9_IN    9
+#define V_GPIO9_IN(x) ((x) << S_GPIO9_IN)
+#define F_GPIO9_IN    V_GPIO9_IN(1U)
+
+#define S_GPIO8_IN    8
+#define V_GPIO8_IN(x) ((x) << S_GPIO8_IN)
+#define F_GPIO8_IN    V_GPIO8_IN(1U)
+
+#define S_GPIO7_IN    7
+#define V_GPIO7_IN(x) ((x) << S_GPIO7_IN)
+#define F_GPIO7_IN    V_GPIO7_IN(1U)
+
+#define S_GPIO6_IN    6
+#define V_GPIO6_IN(x) ((x) << S_GPIO6_IN)
+#define F_GPIO6_IN    V_GPIO6_IN(1U)
+
+#define S_GPIO5_IN    5
+#define V_GPIO5_IN(x) ((x) << S_GPIO5_IN)
+#define F_GPIO5_IN    V_GPIO5_IN(1U)
+
+#define S_GPIO4_IN    4
+#define V_GPIO4_IN(x) ((x) << S_GPIO4_IN)
+#define F_GPIO4_IN    V_GPIO4_IN(1U)
+
+#define S_GPIO3_IN    3
+#define V_GPIO3_IN(x) ((x) << S_GPIO3_IN)
+#define F_GPIO3_IN    V_GPIO3_IN(1U)
+
+#define S_GPIO2_IN    2
+#define V_GPIO2_IN(x) ((x) << S_GPIO2_IN)
+#define F_GPIO2_IN    V_GPIO2_IN(1U)
+
+#define S_GPIO1_IN    1
+#define V_GPIO1_IN(x) ((x) << S_GPIO1_IN)
+#define F_GPIO1_IN    V_GPIO1_IN(1U)
+
+#define S_GPIO0_IN    0
+#define V_GPIO0_IN(x) ((x) << S_GPIO0_IN)
+#define F_GPIO0_IN    V_GPIO0_IN(1U)
+
+#define A_DBG_INT_ENABLE 0x6018
+
+#define S_IBM_FDL_FAIL_INT_ENBL    25
+#define V_IBM_FDL_FAIL_INT_ENBL(x) ((x) << S_IBM_FDL_FAIL_INT_ENBL)
+#define F_IBM_FDL_FAIL_INT_ENBL    V_IBM_FDL_FAIL_INT_ENBL(1U)
+
+#define S_ARM_FAIL_INT_ENBL    24
+#define V_ARM_FAIL_INT_ENBL(x) ((x) << S_ARM_FAIL_INT_ENBL)
+#define F_ARM_FAIL_INT_ENBL    V_ARM_FAIL_INT_ENBL(1U)
+
+#define S_ARM_ERROR_OUT_INT_ENBL    23
+#define V_ARM_ERROR_OUT_INT_ENBL(x) ((x) << S_ARM_ERROR_OUT_INT_ENBL)
+#define F_ARM_ERROR_OUT_INT_ENBL    V_ARM_ERROR_OUT_INT_ENBL(1U)
+
+#define S_PLL_LOCK_LOST_INT_ENBL    22
+#define V_PLL_LOCK_LOST_INT_ENBL(x) ((x) << S_PLL_LOCK_LOST_INT_ENBL)
+#define F_PLL_LOCK_LOST_INT_ENBL    V_PLL_LOCK_LOST_INT_ENBL(1U)
+
+#define S_C_LOCK    21
+#define V_C_LOCK(x) ((x) << S_C_LOCK)
+#define F_C_LOCK    V_C_LOCK(1U)
+
+#define S_M_LOCK    20
+#define V_M_LOCK(x) ((x) << S_M_LOCK)
+#define F_M_LOCK    V_M_LOCK(1U)
+
+#define S_U_LOCK    19
+#define V_U_LOCK(x) ((x) << S_U_LOCK)
+#define F_U_LOCK    V_U_LOCK(1U)
+
+#define S_PCIE_LOCK    18
+#define V_PCIE_LOCK(x) ((x) << S_PCIE_LOCK)
+#define F_PCIE_LOCK    V_PCIE_LOCK(1U)
+
+#define S_KX_LOCK    17
+#define V_KX_LOCK(x) ((x) << S_KX_LOCK)
+#define F_KX_LOCK    V_KX_LOCK(1U)
+
+#define S_KR_LOCK    16
+#define V_KR_LOCK(x) ((x) << S_KR_LOCK)
+#define F_KR_LOCK    V_KR_LOCK(1U)
+
+#define S_GPIO15    15
+#define V_GPIO15(x) ((x) << S_GPIO15)
+#define F_GPIO15    V_GPIO15(1U)
+
+#define S_GPIO14    14
+#define V_GPIO14(x) ((x) << S_GPIO14)
+#define F_GPIO14    V_GPIO14(1U)
+
+#define S_GPIO13    13
+#define V_GPIO13(x) ((x) << S_GPIO13)
+#define F_GPIO13    V_GPIO13(1U)
+
+#define S_GPIO12    12
+#define V_GPIO12(x) ((x) << S_GPIO12)
+#define F_GPIO12    V_GPIO12(1U)
+
+#define S_GPIO11    11
+#define V_GPIO11(x) ((x) << S_GPIO11)
+#define F_GPIO11    V_GPIO11(1U)
+
+#define S_GPIO10    10
+#define V_GPIO10(x) ((x) << S_GPIO10)
+#define F_GPIO10    V_GPIO10(1U)
+
+#define S_GPIO9    9
+#define V_GPIO9(x) ((x) << S_GPIO9)
+#define F_GPIO9    V_GPIO9(1U)
+
+#define S_GPIO8    8
+#define V_GPIO8(x) ((x) << S_GPIO8)
+#define F_GPIO8    V_GPIO8(1U)
+
+#define S_GPIO7    7
+#define V_GPIO7(x) ((x) << S_GPIO7)
+#define F_GPIO7    V_GPIO7(1U)
+
+#define S_GPIO6    6
+#define V_GPIO6(x) ((x) << S_GPIO6)
+#define F_GPIO6    V_GPIO6(1U)
+
+#define S_GPIO5    5
+#define V_GPIO5(x) ((x) << S_GPIO5)
+#define F_GPIO5    V_GPIO5(1U)
+
+#define S_GPIO4    4
+#define V_GPIO4(x) ((x) << S_GPIO4)
+#define F_GPIO4    V_GPIO4(1U)
+
+#define S_GPIO3    3
+#define V_GPIO3(x) ((x) << S_GPIO3)
+#define F_GPIO3    V_GPIO3(1U)
+
+#define S_GPIO2    2
+#define V_GPIO2(x) ((x) << S_GPIO2)
+#define F_GPIO2    V_GPIO2(1U)
+
+#define S_GPIO1    1
+#define V_GPIO1(x) ((x) << S_GPIO1)
+#define F_GPIO1    V_GPIO1(1U)
+
+#define S_GPIO0    0
+#define V_GPIO0(x) ((x) << S_GPIO0)
+#define F_GPIO0    V_GPIO0(1U)
+
+#define A_DBG_INT_CAUSE 0x601c
+
+#define S_IBM_FDL_FAIL_INT_CAUSE    25
+#define V_IBM_FDL_FAIL_INT_CAUSE(x) ((x) << S_IBM_FDL_FAIL_INT_CAUSE)
+#define F_IBM_FDL_FAIL_INT_CAUSE    V_IBM_FDL_FAIL_INT_CAUSE(1U)
+
+#define S_ARM_FAIL_INT_CAUSE    24
+#define V_ARM_FAIL_INT_CAUSE(x) ((x) << S_ARM_FAIL_INT_CAUSE)
+#define F_ARM_FAIL_INT_CAUSE    V_ARM_FAIL_INT_CAUSE(1U)
+
+#define S_ARM_ERROR_OUT_INT_CAUSE    23
+#define V_ARM_ERROR_OUT_INT_CAUSE(x) ((x) << S_ARM_ERROR_OUT_INT_CAUSE)
+#define F_ARM_ERROR_OUT_INT_CAUSE    V_ARM_ERROR_OUT_INT_CAUSE(1U)
+
+#define S_PLL_LOCK_LOST_INT_CAUSE    22
+#define V_PLL_LOCK_LOST_INT_CAUSE(x) ((x) << S_PLL_LOCK_LOST_INT_CAUSE)
+#define F_PLL_LOCK_LOST_INT_CAUSE    V_PLL_LOCK_LOST_INT_CAUSE(1U)
+
+#define A_DBG_DBG0_RST_VALUE 0x6020
+
+#define S_DEBUGDATA    0
+#define M_DEBUGDATA    0xffffU
+#define V_DEBUGDATA(x) ((x) << S_DEBUGDATA)
+#define G_DEBUGDATA(x) (((x) >> S_DEBUGDATA) & M_DEBUGDATA)
+
+#define A_DBG_OVERWRSERCFG_EN 0x6024
+
+#define S_OVERWRSERCFG_EN    0
+#define V_OVERWRSERCFG_EN(x) ((x) << S_OVERWRSERCFG_EN)
+#define F_OVERWRSERCFG_EN    V_OVERWRSERCFG_EN(1U)
+
+#define A_DBG_PLL_OCLK_PAD_EN 0x6028
+
+#define S_PCIE_OCLK_EN    20
+#define V_PCIE_OCLK_EN(x) ((x) << S_PCIE_OCLK_EN)
+#define F_PCIE_OCLK_EN    V_PCIE_OCLK_EN(1U)
+
+#define S_KX_OCLK_EN    16
+#define V_KX_OCLK_EN(x) ((x) << S_KX_OCLK_EN)
+#define F_KX_OCLK_EN    V_KX_OCLK_EN(1U)
+
+#define S_U_OCLK_EN    12
+#define V_U_OCLK_EN(x) ((x) << S_U_OCLK_EN)
+#define F_U_OCLK_EN    V_U_OCLK_EN(1U)
+
+#define S_KR_OCLK_EN    8
+#define V_KR_OCLK_EN(x) ((x) << S_KR_OCLK_EN)
+#define F_KR_OCLK_EN    V_KR_OCLK_EN(1U)
+
+#define S_M_OCLK_EN    4
+#define V_M_OCLK_EN(x) ((x) << S_M_OCLK_EN)
+#define F_M_OCLK_EN    V_M_OCLK_EN(1U)
+
+#define S_C_OCLK_EN    0
+#define V_C_OCLK_EN(x) ((x) << S_C_OCLK_EN)
+#define F_C_OCLK_EN    V_C_OCLK_EN(1U)
+
+#define A_DBG_PLL_LOCK 0x602c
+
+#define S_PLL_P_LOCK    20
+#define V_PLL_P_LOCK(x) ((x) << S_PLL_P_LOCK)
+#define F_PLL_P_LOCK    V_PLL_P_LOCK(1U)
+
+#define S_PLL_KX_LOCK    16
+#define V_PLL_KX_LOCK(x) ((x) << S_PLL_KX_LOCK)
+#define F_PLL_KX_LOCK    V_PLL_KX_LOCK(1U)
+
+#define S_PLL_U_LOCK    12
+#define V_PLL_U_LOCK(x) ((x) << S_PLL_U_LOCK)
+#define F_PLL_U_LOCK    V_PLL_U_LOCK(1U)
+
+#define S_PLL_KR_LOCK    8
+#define V_PLL_KR_LOCK(x) ((x) << S_PLL_KR_LOCK)
+#define F_PLL_KR_LOCK    V_PLL_KR_LOCK(1U)
+
+#define S_PLL_M_LOCK    4
+#define V_PLL_M_LOCK(x) ((x) << S_PLL_M_LOCK)
+#define F_PLL_M_LOCK    V_PLL_M_LOCK(1U)
+
+#define S_PLL_C_LOCK    0
+#define V_PLL_C_LOCK(x) ((x) << S_PLL_C_LOCK)
+#define F_PLL_C_LOCK    V_PLL_C_LOCK(1U)
+
+#define A_DBG_GPIO_ACT_LOW 0x6030
+
+#define S_P_LOCK_ACT_LOW    21
+#define V_P_LOCK_ACT_LOW(x) ((x) << S_P_LOCK_ACT_LOW)
+#define F_P_LOCK_ACT_LOW    V_P_LOCK_ACT_LOW(1U)
+
+#define S_C_LOCK_ACT_LOW    20
+#define V_C_LOCK_ACT_LOW(x) ((x) << S_C_LOCK_ACT_LOW)
+#define F_C_LOCK_ACT_LOW    V_C_LOCK_ACT_LOW(1U)
+
+#define S_M_LOCK_ACT_LOW    19
+#define V_M_LOCK_ACT_LOW(x) ((x) << S_M_LOCK_ACT_LOW)
+#define F_M_LOCK_ACT_LOW    V_M_LOCK_ACT_LOW(1U)
+
+#define S_U_LOCK_ACT_LOW    18
+#define V_U_LOCK_ACT_LOW(x) ((x) << S_U_LOCK_ACT_LOW)
+#define F_U_LOCK_ACT_LOW    V_U_LOCK_ACT_LOW(1U)
+
+#define S_KR_LOCK_ACT_LOW    17
+#define V_KR_LOCK_ACT_LOW(x) ((x) << S_KR_LOCK_ACT_LOW)
+#define F_KR_LOCK_ACT_LOW    V_KR_LOCK_ACT_LOW(1U)
+
+#define S_KX_LOCK_ACT_LOW    16
+#define V_KX_LOCK_ACT_LOW(x) ((x) << S_KX_LOCK_ACT_LOW)
+#define F_KX_LOCK_ACT_LOW    V_KX_LOCK_ACT_LOW(1U)
+
+#define S_GPIO15_ACT_LOW    15
+#define V_GPIO15_ACT_LOW(x) ((x) << S_GPIO15_ACT_LOW)
+#define F_GPIO15_ACT_LOW    V_GPIO15_ACT_LOW(1U)
+
+#define S_GPIO14_ACT_LOW    14
+#define V_GPIO14_ACT_LOW(x) ((x) << S_GPIO14_ACT_LOW)
+#define F_GPIO14_ACT_LOW    V_GPIO14_ACT_LOW(1U)
+
+#define S_GPIO13_ACT_LOW    13
+#define V_GPIO13_ACT_LOW(x) ((x) << S_GPIO13_ACT_LOW)
+#define F_GPIO13_ACT_LOW    V_GPIO13_ACT_LOW(1U)
+
+#define S_GPIO12_ACT_LOW    12
+#define V_GPIO12_ACT_LOW(x) ((x) << S_GPIO12_ACT_LOW)
+#define F_GPIO12_ACT_LOW    V_GPIO12_ACT_LOW(1U)
+
+#define S_GPIO11_ACT_LOW    11
+#define V_GPIO11_ACT_LOW(x) ((x) << S_GPIO11_ACT_LOW)
+#define F_GPIO11_ACT_LOW    V_GPIO11_ACT_LOW(1U)
+
+#define S_GPIO10_ACT_LOW    10
+#define V_GPIO10_ACT_LOW(x) ((x) << S_GPIO10_ACT_LOW)
+#define F_GPIO10_ACT_LOW    V_GPIO10_ACT_LOW(1U)
+
+#define S_GPIO9_ACT_LOW    9
+#define V_GPIO9_ACT_LOW(x) ((x) << S_GPIO9_ACT_LOW)
+#define F_GPIO9_ACT_LOW    V_GPIO9_ACT_LOW(1U)
+
+#define S_GPIO8_ACT_LOW    8
+#define V_GPIO8_ACT_LOW(x) ((x) << S_GPIO8_ACT_LOW)
+#define F_GPIO8_ACT_LOW    V_GPIO8_ACT_LOW(1U)
+
+#define S_GPIO7_ACT_LOW    7
+#define V_GPIO7_ACT_LOW(x) ((x) << S_GPIO7_ACT_LOW)
+#define F_GPIO7_ACT_LOW    V_GPIO7_ACT_LOW(1U)
+
+#define S_GPIO6_ACT_LOW    6
+#define V_GPIO6_ACT_LOW(x) ((x) << S_GPIO6_ACT_LOW)
+#define F_GPIO6_ACT_LOW    V_GPIO6_ACT_LOW(1U)
+
+#define S_GPIO5_ACT_LOW    5
+#define V_GPIO5_ACT_LOW(x) ((x) << S_GPIO5_ACT_LOW)
+#define F_GPIO5_ACT_LOW    V_GPIO5_ACT_LOW(1U)
+
+#define S_GPIO4_ACT_LOW    4
+#define V_GPIO4_ACT_LOW(x) ((x) << S_GPIO4_ACT_LOW)
+#define F_GPIO4_ACT_LOW    V_GPIO4_ACT_LOW(1U)
+
+#define S_GPIO3_ACT_LOW    3
+#define V_GPIO3_ACT_LOW(x) ((x) << S_GPIO3_ACT_LOW)
+#define F_GPIO3_ACT_LOW    V_GPIO3_ACT_LOW(1U)
+
+#define S_GPIO2_ACT_LOW    2
+#define V_GPIO2_ACT_LOW(x) ((x) << S_GPIO2_ACT_LOW)
+#define F_GPIO2_ACT_LOW    V_GPIO2_ACT_LOW(1U)
+
+#define S_GPIO1_ACT_LOW    1
+#define V_GPIO1_ACT_LOW(x) ((x) << S_GPIO1_ACT_LOW)
+#define F_GPIO1_ACT_LOW    V_GPIO1_ACT_LOW(1U)
+
+#define S_GPIO0_ACT_LOW    0
+#define V_GPIO0_ACT_LOW(x) ((x) << S_GPIO0_ACT_LOW)
+#define F_GPIO0_ACT_LOW    V_GPIO0_ACT_LOW(1U)
+
+#define A_DBG_EFUSE_BYTE0_3 0x6034
+#define A_DBG_EFUSE_BYTE4_7 0x6038
+#define A_DBG_EFUSE_BYTE8_11 0x603c
+#define A_DBG_EFUSE_BYTE12_15 0x6040
+#define A_DBG_STATIC_U_PLL_CONF 0x6044
+
+#define S_STATIC_U_PLL_MULT    23
+#define M_STATIC_U_PLL_MULT    0x1ffU
+#define V_STATIC_U_PLL_MULT(x) ((x) << S_STATIC_U_PLL_MULT)
+#define G_STATIC_U_PLL_MULT(x) (((x) >> S_STATIC_U_PLL_MULT) & M_STATIC_U_PLL_MULT)
+
+#define S_STATIC_U_PLL_PREDIV    18
+#define M_STATIC_U_PLL_PREDIV    0x1fU
+#define V_STATIC_U_PLL_PREDIV(x) ((x) << S_STATIC_U_PLL_PREDIV)
+#define G_STATIC_U_PLL_PREDIV(x) (((x) >> S_STATIC_U_PLL_PREDIV) & M_STATIC_U_PLL_PREDIV)
+
+#define S_STATIC_U_PLL_RANGEA    14
+#define M_STATIC_U_PLL_RANGEA    0xfU
+#define V_STATIC_U_PLL_RANGEA(x) ((x) << S_STATIC_U_PLL_RANGEA)
+#define G_STATIC_U_PLL_RANGEA(x) (((x) >> S_STATIC_U_PLL_RANGEA) & M_STATIC_U_PLL_RANGEA)
+
+#define S_STATIC_U_PLL_RANGEB    10
+#define M_STATIC_U_PLL_RANGEB    0xfU
+#define V_STATIC_U_PLL_RANGEB(x) ((x) << S_STATIC_U_PLL_RANGEB)
+#define G_STATIC_U_PLL_RANGEB(x) (((x) >> S_STATIC_U_PLL_RANGEB) & M_STATIC_U_PLL_RANGEB)
+
+#define S_STATIC_U_PLL_TUNE    0
+#define M_STATIC_U_PLL_TUNE    0x3ffU
+#define V_STATIC_U_PLL_TUNE(x) ((x) << S_STATIC_U_PLL_TUNE)
+#define G_STATIC_U_PLL_TUNE(x) (((x) >> S_STATIC_U_PLL_TUNE) & M_STATIC_U_PLL_TUNE)
+
+#define A_DBG_STATIC_C_PLL_CONF 0x6048
+
+#define S_STATIC_C_PLL_MULT    23
+#define M_STATIC_C_PLL_MULT    0x1ffU
+#define V_STATIC_C_PLL_MULT(x) ((x) << S_STATIC_C_PLL_MULT)
+#define G_STATIC_C_PLL_MULT(x) (((x) >> S_STATIC_C_PLL_MULT) & M_STATIC_C_PLL_MULT)
+
+#define S_STATIC_C_PLL_PREDIV    18
+#define M_STATIC_C_PLL_PREDIV    0x1fU
+#define V_STATIC_C_PLL_PREDIV(x) ((x) << S_STATIC_C_PLL_PREDIV)
+#define G_STATIC_C_PLL_PREDIV(x) (((x) >> S_STATIC_C_PLL_PREDIV) & M_STATIC_C_PLL_PREDIV)
+
+#define S_STATIC_C_PLL_RANGEA    14
+#define M_STATIC_C_PLL_RANGEA    0xfU
+#define V_STATIC_C_PLL_RANGEA(x) ((x) << S_STATIC_C_PLL_RANGEA)
+#define G_STATIC_C_PLL_RANGEA(x) (((x) >> S_STATIC_C_PLL_RANGEA) & M_STATIC_C_PLL_RANGEA)
+
+#define S_STATIC_C_PLL_RANGEB    10
+#define M_STATIC_C_PLL_RANGEB    0xfU
+#define V_STATIC_C_PLL_RANGEB(x) ((x) << S_STATIC_C_PLL_RANGEB)
+#define G_STATIC_C_PLL_RANGEB(x) (((x) >> S_STATIC_C_PLL_RANGEB) & M_STATIC_C_PLL_RANGEB)
+
+#define S_STATIC_C_PLL_TUNE    0
+#define M_STATIC_C_PLL_TUNE    0x3ffU
+#define V_STATIC_C_PLL_TUNE(x) ((x) << S_STATIC_C_PLL_TUNE)
+#define G_STATIC_C_PLL_TUNE(x) (((x) >> S_STATIC_C_PLL_TUNE) & M_STATIC_C_PLL_TUNE)
+
+#define A_DBG_STATIC_M_PLL_CONF 0x604c
+
+#define S_STATIC_M_PLL_MULT    23
+#define M_STATIC_M_PLL_MULT    0x1ffU
+#define V_STATIC_M_PLL_MULT(x) ((x) << S_STATIC_M_PLL_MULT)
+#define G_STATIC_M_PLL_MULT(x) (((x) >> S_STATIC_M_PLL_MULT) & M_STATIC_M_PLL_MULT)
+
+#define S_STATIC_M_PLL_PREDIV    18
+#define M_STATIC_M_PLL_PREDIV    0x1fU
+#define V_STATIC_M_PLL_PREDIV(x) ((x) << S_STATIC_M_PLL_PREDIV)
+#define G_STATIC_M_PLL_PREDIV(x) (((x) >> S_STATIC_M_PLL_PREDIV) & M_STATIC_M_PLL_PREDIV)
+
+#define S_STATIC_M_PLL_RANGEA    14
+#define M_STATIC_M_PLL_RANGEA    0xfU
+#define V_STATIC_M_PLL_RANGEA(x) ((x) << S_STATIC_M_PLL_RANGEA)
+#define G_STATIC_M_PLL_RANGEA(x) (((x) >> S_STATIC_M_PLL_RANGEA) & M_STATIC_M_PLL_RANGEA)
+
+#define S_STATIC_M_PLL_RANGEB    10
+#define M_STATIC_M_PLL_RANGEB    0xfU
+#define V_STATIC_M_PLL_RANGEB(x) ((x) << S_STATIC_M_PLL_RANGEB)
+#define G_STATIC_M_PLL_RANGEB(x) (((x) >> S_STATIC_M_PLL_RANGEB) & M_STATIC_M_PLL_RANGEB)
+
+#define S_STATIC_M_PLL_TUNE    0
+#define M_STATIC_M_PLL_TUNE    0x3ffU
+#define V_STATIC_M_PLL_TUNE(x) ((x) << S_STATIC_M_PLL_TUNE)
+#define G_STATIC_M_PLL_TUNE(x) (((x) >> S_STATIC_M_PLL_TUNE) & M_STATIC_M_PLL_TUNE)
+
+#define A_DBG_STATIC_KX_PLL_CONF 0x6050
+
+#define S_STATIC_KX_PLL_C    21
+#define M_STATIC_KX_PLL_C    0xffU
+#define V_STATIC_KX_PLL_C(x) ((x) << S_STATIC_KX_PLL_C)
+#define G_STATIC_KX_PLL_C(x) (((x) >> S_STATIC_KX_PLL_C) & M_STATIC_KX_PLL_C)
+
+#define S_STATIC_KX_PLL_M    15
+#define M_STATIC_KX_PLL_M    0x3fU
+#define V_STATIC_KX_PLL_M(x) ((x) << S_STATIC_KX_PLL_M)
+#define G_STATIC_KX_PLL_M(x) (((x) >> S_STATIC_KX_PLL_M) & M_STATIC_KX_PLL_M)
+
+#define S_STATIC_KX_PLL_N1    11
+#define M_STATIC_KX_PLL_N1    0xfU
+#define V_STATIC_KX_PLL_N1(x) ((x) << S_STATIC_KX_PLL_N1)
+#define G_STATIC_KX_PLL_N1(x) (((x) >> S_STATIC_KX_PLL_N1) & M_STATIC_KX_PLL_N1)
+
+#define S_STATIC_KX_PLL_N2    7
+#define M_STATIC_KX_PLL_N2    0xfU
+#define V_STATIC_KX_PLL_N2(x) ((x) << S_STATIC_KX_PLL_N2)
+#define G_STATIC_KX_PLL_N2(x) (((x) >> S_STATIC_KX_PLL_N2) & M_STATIC_KX_PLL_N2)
+
+#define S_STATIC_KX_PLL_N3    3
+#define M_STATIC_KX_PLL_N3    0xfU
+#define V_STATIC_KX_PLL_N3(x) ((x) << S_STATIC_KX_PLL_N3)
+#define G_STATIC_KX_PLL_N3(x) (((x) >> S_STATIC_KX_PLL_N3) & M_STATIC_KX_PLL_N3)
+
+#define S_STATIC_KX_PLL_P    0
+#define M_STATIC_KX_PLL_P    0x7U
+#define V_STATIC_KX_PLL_P(x) ((x) << S_STATIC_KX_PLL_P)
+#define G_STATIC_KX_PLL_P(x) (((x) >> S_STATIC_KX_PLL_P) & M_STATIC_KX_PLL_P)
+
+#define A_DBG_STATIC_KR_PLL_CONF 0x6054
+
+#define S_STATIC_KR_PLL_C    21
+#define M_STATIC_KR_PLL_C    0xffU
+#define V_STATIC_KR_PLL_C(x) ((x) << S_STATIC_KR_PLL_C)
+#define G_STATIC_KR_PLL_C(x) (((x) >> S_STATIC_KR_PLL_C) & M_STATIC_KR_PLL_C)
+
+#define S_STATIC_KR_PLL_M    15
+#define M_STATIC_KR_PLL_M    0x3fU
+#define V_STATIC_KR_PLL_M(x) ((x) << S_STATIC_KR_PLL_M)
+#define G_STATIC_KR_PLL_M(x) (((x) >> S_STATIC_KR_PLL_M) & M_STATIC_KR_PLL_M)
+
+#define S_STATIC_KR_PLL_N1    11
+#define M_STATIC_KR_PLL_N1    0xfU
+#define V_STATIC_KR_PLL_N1(x) ((x) << S_STATIC_KR_PLL_N1)
+#define G_STATIC_KR_PLL_N1(x) (((x) >> S_STATIC_KR_PLL_N1) & M_STATIC_KR_PLL_N1)
+
+#define S_STATIC_KR_PLL_N2    7
+#define M_STATIC_KR_PLL_N2    0xfU
+#define V_STATIC_KR_PLL_N2(x) ((x) << S_STATIC_KR_PLL_N2)
+#define G_STATIC_KR_PLL_N2(x) (((x) >> S_STATIC_KR_PLL_N2) & M_STATIC_KR_PLL_N2)
+
+#define S_STATIC_KR_PLL_N3    3
+#define M_STATIC_KR_PLL_N3    0xfU
+#define V_STATIC_KR_PLL_N3(x) ((x) << S_STATIC_KR_PLL_N3)
+#define G_STATIC_KR_PLL_N3(x) (((x) >> S_STATIC_KR_PLL_N3) & M_STATIC_KR_PLL_N3)
+
+#define S_STATIC_KR_PLL_P    0
+#define M_STATIC_KR_PLL_P    0x7U
+#define V_STATIC_KR_PLL_P(x) ((x) << S_STATIC_KR_PLL_P)
+#define G_STATIC_KR_PLL_P(x) (((x) >> S_STATIC_KR_PLL_P) & M_STATIC_KR_PLL_P)
+
+#define A_DBG_EXTRA_STATIC_BITS_CONF 0x6058
+
+#define S_STATIC_M_PLL_RESET    30
+#define V_STATIC_M_PLL_RESET(x) ((x) << S_STATIC_M_PLL_RESET)
+#define F_STATIC_M_PLL_RESET    V_STATIC_M_PLL_RESET(1U)
+
+#define S_STATIC_M_PLL_SLEEP    29
+#define V_STATIC_M_PLL_SLEEP(x) ((x) << S_STATIC_M_PLL_SLEEP)
+#define F_STATIC_M_PLL_SLEEP    V_STATIC_M_PLL_SLEEP(1U)
+
+#define S_STATIC_M_PLL_BYPASS    28
+#define V_STATIC_M_PLL_BYPASS(x) ((x) << S_STATIC_M_PLL_BYPASS)
+#define F_STATIC_M_PLL_BYPASS    V_STATIC_M_PLL_BYPASS(1U)
+
+#define S_STATIC_MPLL_CLK_SEL    27
+#define V_STATIC_MPLL_CLK_SEL(x) ((x) << S_STATIC_MPLL_CLK_SEL)
+#define F_STATIC_MPLL_CLK_SEL    V_STATIC_MPLL_CLK_SEL(1U)
+
+#define S_STATIC_U_PLL_SLEEP    26
+#define V_STATIC_U_PLL_SLEEP(x) ((x) << S_STATIC_U_PLL_SLEEP)
+#define F_STATIC_U_PLL_SLEEP    V_STATIC_U_PLL_SLEEP(1U)
+
+#define S_STATIC_C_PLL_SLEEP    25
+#define V_STATIC_C_PLL_SLEEP(x) ((x) << S_STATIC_C_PLL_SLEEP)
+#define F_STATIC_C_PLL_SLEEP    V_STATIC_C_PLL_SLEEP(1U)
+
+#define S_STATIC_LVDS_CLKOUT_SEL    23
+#define M_STATIC_LVDS_CLKOUT_SEL    0x3U
+#define V_STATIC_LVDS_CLKOUT_SEL(x) ((x) << S_STATIC_LVDS_CLKOUT_SEL)
+#define G_STATIC_LVDS_CLKOUT_SEL(x) (((x) >> S_STATIC_LVDS_CLKOUT_SEL) & M_STATIC_LVDS_CLKOUT_SEL)
+
+#define S_STATIC_LVDS_CLKOUT_EN    22
+#define V_STATIC_LVDS_CLKOUT_EN(x) ((x) << S_STATIC_LVDS_CLKOUT_EN)
+#define F_STATIC_LVDS_CLKOUT_EN    V_STATIC_LVDS_CLKOUT_EN(1U)
+
+#define S_STATIC_CCLK_FREQ_SEL    20
+#define M_STATIC_CCLK_FREQ_SEL    0x3U
+#define V_STATIC_CCLK_FREQ_SEL(x) ((x) << S_STATIC_CCLK_FREQ_SEL)
+#define G_STATIC_CCLK_FREQ_SEL(x) (((x) >> S_STATIC_CCLK_FREQ_SEL) & M_STATIC_CCLK_FREQ_SEL)
+
+#define S_STATIC_UCLK_FREQ_SEL    18
+#define M_STATIC_UCLK_FREQ_SEL    0x3U
+#define V_STATIC_UCLK_FREQ_SEL(x) ((x) << S_STATIC_UCLK_FREQ_SEL)
+#define G_STATIC_UCLK_FREQ_SEL(x) (((x) >> S_STATIC_UCLK_FREQ_SEL) & M_STATIC_UCLK_FREQ_SEL)
+
+#define S_EXPHYCLK_SEL_EN    17
+#define V_EXPHYCLK_SEL_EN(x) ((x) << S_EXPHYCLK_SEL_EN)
+#define F_EXPHYCLK_SEL_EN    V_EXPHYCLK_SEL_EN(1U)
+
+#define S_EXPHYCLK_SEL    15
+#define M_EXPHYCLK_SEL    0x3U
+#define V_EXPHYCLK_SEL(x) ((x) << S_EXPHYCLK_SEL)
+#define G_EXPHYCLK_SEL(x) (((x) >> S_EXPHYCLK_SEL) & M_EXPHYCLK_SEL)
+
+#define S_STATIC_U_PLL_BYPASS    14
+#define V_STATIC_U_PLL_BYPASS(x) ((x) << S_STATIC_U_PLL_BYPASS)
+#define F_STATIC_U_PLL_BYPASS    V_STATIC_U_PLL_BYPASS(1U)
+
+#define S_STATIC_C_PLL_BYPASS    13
+#define V_STATIC_C_PLL_BYPASS(x) ((x) << S_STATIC_C_PLL_BYPASS)
+#define F_STATIC_C_PLL_BYPASS    V_STATIC_C_PLL_BYPASS(1U)
+
+#define S_STATIC_KR_PLL_BYPASS    12
+#define V_STATIC_KR_PLL_BYPASS(x) ((x) << S_STATIC_KR_PLL_BYPASS)
+#define F_STATIC_KR_PLL_BYPASS    V_STATIC_KR_PLL_BYPASS(1U)
+
+#define S_STATIC_KX_PLL_BYPASS    11
+#define V_STATIC_KX_PLL_BYPASS(x) ((x) << S_STATIC_KX_PLL_BYPASS)
+#define F_STATIC_KX_PLL_BYPASS    V_STATIC_KX_PLL_BYPASS(1U)
+
+#define S_STATIC_KX_PLL_V    7
+#define M_STATIC_KX_PLL_V    0xfU
+#define V_STATIC_KX_PLL_V(x) ((x) << S_STATIC_KX_PLL_V)
+#define G_STATIC_KX_PLL_V(x) (((x) >> S_STATIC_KX_PLL_V) & M_STATIC_KX_PLL_V)
+
+#define S_STATIC_KR_PLL_V    3
+#define M_STATIC_KR_PLL_V    0xfU
+#define V_STATIC_KR_PLL_V(x) ((x) << S_STATIC_KR_PLL_V)
+#define G_STATIC_KR_PLL_V(x) (((x) >> S_STATIC_KR_PLL_V) & M_STATIC_KR_PLL_V)
+
+#define S_PSRO_SEL    0
+#define M_PSRO_SEL    0x7U
+#define V_PSRO_SEL(x) ((x) << S_PSRO_SEL)
+#define G_PSRO_SEL(x) (((x) >> S_PSRO_SEL) & M_PSRO_SEL)
+
+#define A_DBG_STATIC_OCLK_MUXSEL_CONF 0x605c
+
+#define S_M_OCLK_MUXSEL    12
+#define V_M_OCLK_MUXSEL(x) ((x) << S_M_OCLK_MUXSEL)
+#define F_M_OCLK_MUXSEL    V_M_OCLK_MUXSEL(1U)
+
+#define S_C_OCLK_MUXSEL    10
+#define M_C_OCLK_MUXSEL    0x3U
+#define V_C_OCLK_MUXSEL(x) ((x) << S_C_OCLK_MUXSEL)
+#define G_C_OCLK_MUXSEL(x) (((x) >> S_C_OCLK_MUXSEL) & M_C_OCLK_MUXSEL)
+
+#define S_U_OCLK_MUXSEL    8
+#define M_U_OCLK_MUXSEL    0x3U
+#define V_U_OCLK_MUXSEL(x) ((x) << S_U_OCLK_MUXSEL)
+#define G_U_OCLK_MUXSEL(x) (((x) >> S_U_OCLK_MUXSEL) & M_U_OCLK_MUXSEL)
+
+#define S_P_OCLK_MUXSEL    6
+#define M_P_OCLK_MUXSEL    0x3U
+#define V_P_OCLK_MUXSEL(x) ((x) << S_P_OCLK_MUXSEL)
+#define G_P_OCLK_MUXSEL(x) (((x) >> S_P_OCLK_MUXSEL) & M_P_OCLK_MUXSEL)
+
+#define S_KX_OCLK_MUXSEL    3
+#define M_KX_OCLK_MUXSEL    0x7U
+#define V_KX_OCLK_MUXSEL(x) ((x) << S_KX_OCLK_MUXSEL)
+#define G_KX_OCLK_MUXSEL(x) (((x) >> S_KX_OCLK_MUXSEL) & M_KX_OCLK_MUXSEL)
+
+#define S_KR_OCLK_MUXSEL    0
+#define M_KR_OCLK_MUXSEL    0x7U
+#define V_KR_OCLK_MUXSEL(x) ((x) << S_KR_OCLK_MUXSEL)
+#define G_KR_OCLK_MUXSEL(x) (((x) >> S_KR_OCLK_MUXSEL) & M_KR_OCLK_MUXSEL)
+
+#define A_DBG_TRACE0_CONF_COMPREG0 0x6060
+#define A_DBG_TRACE0_CONF_COMPREG1 0x6064
+#define A_DBG_TRACE1_CONF_COMPREG0 0x6068
+#define A_DBG_TRACE1_CONF_COMPREG1 0x606c
+#define A_DBG_TRACE0_CONF_MASKREG0 0x6070
+#define A_DBG_TRACE0_CONF_MASKREG1 0x6074
+#define A_DBG_TRACE1_CONF_MASKREG0 0x6078
+#define A_DBG_TRACE1_CONF_MASKREG1 0x607c
+#define A_DBG_TRACE_COUNTER 0x6080
+
+#define S_COUNTER1    16
+#define M_COUNTER1    0xffffU
+#define V_COUNTER1(x) ((x) << S_COUNTER1)
+#define G_COUNTER1(x) (((x) >> S_COUNTER1) & M_COUNTER1)
+
+#define S_COUNTER0    0
+#define M_COUNTER0    0xffffU
+#define V_COUNTER0(x) ((x) << S_COUNTER0)
+#define G_COUNTER0(x) (((x) >> S_COUNTER0) & M_COUNTER0)
+
+#define A_DBG_STATIC_REFCLK_PERIOD 0x6084
+
+#define S_STATIC_REFCLK_PERIOD    0
+#define M_STATIC_REFCLK_PERIOD    0xffffU
+#define V_STATIC_REFCLK_PERIOD(x) ((x) << S_STATIC_REFCLK_PERIOD)
+#define G_STATIC_REFCLK_PERIOD(x) (((x) >> S_STATIC_REFCLK_PERIOD) & M_STATIC_REFCLK_PERIOD)
+
+#define A_DBG_TRACE_CONF 0x6088
+
+#define S_DBG_TRACE_OPERATE_WITH_TRG    5
+#define V_DBG_TRACE_OPERATE_WITH_TRG(x) ((x) << S_DBG_TRACE_OPERATE_WITH_TRG)
+#define F_DBG_TRACE_OPERATE_WITH_TRG    V_DBG_TRACE_OPERATE_WITH_TRG(1U)
+
+#define S_DBG_TRACE_OPERATE_EN    4
+#define V_DBG_TRACE_OPERATE_EN(x) ((x) << S_DBG_TRACE_OPERATE_EN)
+#define F_DBG_TRACE_OPERATE_EN    V_DBG_TRACE_OPERATE_EN(1U)
+
+#define S_DBG_OPERATE_INDV_COMBINED    3
+#define V_DBG_OPERATE_INDV_COMBINED(x) ((x) << S_DBG_OPERATE_INDV_COMBINED)
+#define F_DBG_OPERATE_INDV_COMBINED    V_DBG_OPERATE_INDV_COMBINED(1U)
+
+#define S_DBG_OPERATE_ORDER_OF_TRIGGER    2
+#define V_DBG_OPERATE_ORDER_OF_TRIGGER(x) ((x) << S_DBG_OPERATE_ORDER_OF_TRIGGER)
+#define F_DBG_OPERATE_ORDER_OF_TRIGGER    V_DBG_OPERATE_ORDER_OF_TRIGGER(1U)
+
+#define S_DBG_OPERATE_SGL_DBL_TRIGGER    1
+#define V_DBG_OPERATE_SGL_DBL_TRIGGER(x) ((x) << S_DBG_OPERATE_SGL_DBL_TRIGGER)
+#define F_DBG_OPERATE_SGL_DBL_TRIGGER    V_DBG_OPERATE_SGL_DBL_TRIGGER(1U)
+
+#define S_DBG_OPERATE0_OR_1    0
+#define V_DBG_OPERATE0_OR_1(x) ((x) << S_DBG_OPERATE0_OR_1)
+#define F_DBG_OPERATE0_OR_1    V_DBG_OPERATE0_OR_1(1U)
+
+#define A_DBG_TRACE_RDEN 0x608c
+
+#define S_RD_ADDR1    10
+#define M_RD_ADDR1    0xffU
+#define V_RD_ADDR1(x) ((x) << S_RD_ADDR1)
+#define G_RD_ADDR1(x) (((x) >> S_RD_ADDR1) & M_RD_ADDR1)
+
+#define S_RD_ADDR0    2
+#define M_RD_ADDR0    0xffU
+#define V_RD_ADDR0(x) ((x) << S_RD_ADDR0)
+#define G_RD_ADDR0(x) (((x) >> S_RD_ADDR0) & M_RD_ADDR0)
+
+#define S_RD_EN1    1
+#define V_RD_EN1(x) ((x) << S_RD_EN1)
+#define F_RD_EN1    V_RD_EN1(1U)
+
+#define S_RD_EN0    0
+#define V_RD_EN0(x) ((x) << S_RD_EN0)
+#define F_RD_EN0    V_RD_EN0(1U)
+
+#define A_DBG_TRACE_WRADDR 0x6090
+
+#define S_WR_POINTER_ADDR1    16
+#define M_WR_POINTER_ADDR1    0xffU
+#define V_WR_POINTER_ADDR1(x) ((x) << S_WR_POINTER_ADDR1)
+#define G_WR_POINTER_ADDR1(x) (((x) >> S_WR_POINTER_ADDR1) & M_WR_POINTER_ADDR1)
+
+#define S_WR_POINTER_ADDR0    0
+#define M_WR_POINTER_ADDR0    0xffU
+#define V_WR_POINTER_ADDR0(x) ((x) << S_WR_POINTER_ADDR0)
+#define G_WR_POINTER_ADDR0(x) (((x) >> S_WR_POINTER_ADDR0) & M_WR_POINTER_ADDR0)
+
+#define A_DBG_TRACE0_DATA_OUT 0x6094
+#define A_DBG_TRACE1_DATA_OUT 0x6098
+#define A_DBG_PVT_REG_CALIBRATE_CTL 0x6100
+
+#define S_HALT_CALIBRATE    1
+#define V_HALT_CALIBRATE(x) ((x) << S_HALT_CALIBRATE)
+#define F_HALT_CALIBRATE    V_HALT_CALIBRATE(1U)
+
+#define S_RESET_CALIBRATE    0
+#define V_RESET_CALIBRATE(x) ((x) << S_RESET_CALIBRATE)
+#define F_RESET_CALIBRATE    V_RESET_CALIBRATE(1U)
+
+#define A_DBG_PVT_REG_UPDATE_CTL 0x6104
+
+#define S_FAST_UPDATE    8
+#define V_FAST_UPDATE(x) ((x) << S_FAST_UPDATE)
+#define F_FAST_UPDATE    V_FAST_UPDATE(1U)
+
+#define S_FORCE_REG_IN_VALUE    2
+#define V_FORCE_REG_IN_VALUE(x) ((x) << S_FORCE_REG_IN_VALUE)
+#define F_FORCE_REG_IN_VALUE    V_FORCE_REG_IN_VALUE(1U)
+
+#define S_HALT_UPDATE    1
+#define V_HALT_UPDATE(x) ((x) << S_HALT_UPDATE)
+#define F_HALT_UPDATE    V_HALT_UPDATE(1U)
+
+#define A_DBG_PVT_REG_LAST_MEASUREMENT 0x6108
+
+#define S_LAST_MEASUREMENT_SELECT    8
+#define M_LAST_MEASUREMENT_SELECT    0x3U
+#define V_LAST_MEASUREMENT_SELECT(x) ((x) << S_LAST_MEASUREMENT_SELECT)
+#define G_LAST_MEASUREMENT_SELECT(x) (((x) >> S_LAST_MEASUREMENT_SELECT) & M_LAST_MEASUREMENT_SELECT)
+
+#define S_LAST_MEASUREMENT_RESULT_BANK_B    4
+#define M_LAST_MEASUREMENT_RESULT_BANK_B    0xfU
+#define V_LAST_MEASUREMENT_RESULT_BANK_B(x) ((x) << S_LAST_MEASUREMENT_RESULT_BANK_B)
+#define G_LAST_MEASUREMENT_RESULT_BANK_B(x) (((x) >> S_LAST_MEASUREMENT_RESULT_BANK_B) & M_LAST_MEASUREMENT_RESULT_BANK_B)
+
+#define S_LAST_MEASUREMENT_RESULT_BANK_A    0
+#define M_LAST_MEASUREMENT_RESULT_BANK_A    0xfU
+#define V_LAST_MEASUREMENT_RESULT_BANK_A(x) ((x) << S_LAST_MEASUREMENT_RESULT_BANK_A)
+#define G_LAST_MEASUREMENT_RESULT_BANK_A(x) (((x) >> S_LAST_MEASUREMENT_RESULT_BANK_A) & M_LAST_MEASUREMENT_RESULT_BANK_A)
+
+#define A_DBG_PVT_REG_DRVN 0x610c
+
+#define S_PVT_REG_DRVN_EN    8
+#define V_PVT_REG_DRVN_EN(x) ((x) << S_PVT_REG_DRVN_EN)
+#define F_PVT_REG_DRVN_EN    V_PVT_REG_DRVN_EN(1U)
+
+#define S_PVT_REG_DRVN_B    4
+#define M_PVT_REG_DRVN_B    0xfU
+#define V_PVT_REG_DRVN_B(x) ((x) << S_PVT_REG_DRVN_B)
+#define G_PVT_REG_DRVN_B(x) (((x) >> S_PVT_REG_DRVN_B) & M_PVT_REG_DRVN_B)
+
+#define S_PVT_REG_DRVN_A    0
+#define M_PVT_REG_DRVN_A    0xfU
+#define V_PVT_REG_DRVN_A(x) ((x) << S_PVT_REG_DRVN_A)
+#define G_PVT_REG_DRVN_A(x) (((x) >> S_PVT_REG_DRVN_A) & M_PVT_REG_DRVN_A)
+
+#define A_DBG_PVT_REG_DRVP 0x6110
+
+#define S_PVT_REG_DRVP_EN    8
+#define V_PVT_REG_DRVP_EN(x) ((x) << S_PVT_REG_DRVP_EN)
+#define F_PVT_REG_DRVP_EN    V_PVT_REG_DRVP_EN(1U)
+
+#define S_PVT_REG_DRVP_B    4
+#define M_PVT_REG_DRVP_B    0xfU
+#define V_PVT_REG_DRVP_B(x) ((x) << S_PVT_REG_DRVP_B)
+#define G_PVT_REG_DRVP_B(x) (((x) >> S_PVT_REG_DRVP_B) & M_PVT_REG_DRVP_B)
+
+#define S_PVT_REG_DRVP_A    0
+#define M_PVT_REG_DRVP_A    0xfU
+#define V_PVT_REG_DRVP_A(x) ((x) << S_PVT_REG_DRVP_A)
+#define G_PVT_REG_DRVP_A(x) (((x) >> S_PVT_REG_DRVP_A) & M_PVT_REG_DRVP_A)
+
+#define A_DBG_PVT_REG_TERMN 0x6114
+
+#define S_PVT_REG_TERMN_EN    8
+#define V_PVT_REG_TERMN_EN(x) ((x) << S_PVT_REG_TERMN_EN)
+#define F_PVT_REG_TERMN_EN    V_PVT_REG_TERMN_EN(1U)
+
+#define S_PVT_REG_TERMN_B    4
+#define M_PVT_REG_TERMN_B    0xfU
+#define V_PVT_REG_TERMN_B(x) ((x) << S_PVT_REG_TERMN_B)
+#define G_PVT_REG_TERMN_B(x) (((x) >> S_PVT_REG_TERMN_B) & M_PVT_REG_TERMN_B)
+
+#define S_PVT_REG_TERMN_A    0
+#define M_PVT_REG_TERMN_A    0xfU
+#define V_PVT_REG_TERMN_A(x) ((x) << S_PVT_REG_TERMN_A)
+#define G_PVT_REG_TERMN_A(x) (((x) >> S_PVT_REG_TERMN_A) & M_PVT_REG_TERMN_A)
+
+#define A_DBG_PVT_REG_TERMP 0x6118
+
+#define S_PVT_REG_TERMP_EN    8
+#define V_PVT_REG_TERMP_EN(x) ((x) << S_PVT_REG_TERMP_EN)
+#define F_PVT_REG_TERMP_EN    V_PVT_REG_TERMP_EN(1U)
+
+#define S_PVT_REG_TERMP_B    4
+#define M_PVT_REG_TERMP_B    0xfU
+#define V_PVT_REG_TERMP_B(x) ((x) << S_PVT_REG_TERMP_B)
+#define G_PVT_REG_TERMP_B(x) (((x) >> S_PVT_REG_TERMP_B) & M_PVT_REG_TERMP_B)
+
+#define S_PVT_REG_TERMP_A    0
+#define M_PVT_REG_TERMP_A    0xfU
+#define V_PVT_REG_TERMP_A(x) ((x) << S_PVT_REG_TERMP_A)
+#define G_PVT_REG_TERMP_A(x) (((x) >> S_PVT_REG_TERMP_A) & M_PVT_REG_TERMP_A)
+
+#define A_DBG_PVT_REG_THRESHOLD 0x611c
+
+#define S_PVT_CALIBRATION_DONE    8
+#define V_PVT_CALIBRATION_DONE(x) ((x) << S_PVT_CALIBRATION_DONE)
+#define F_PVT_CALIBRATION_DONE    V_PVT_CALIBRATION_DONE(1U)
+
+#define S_THRESHOLD_TERMP_MAX_SYNC    7
+#define V_THRESHOLD_TERMP_MAX_SYNC(x) ((x) << S_THRESHOLD_TERMP_MAX_SYNC)
+#define F_THRESHOLD_TERMP_MAX_SYNC    V_THRESHOLD_TERMP_MAX_SYNC(1U)
+
+#define S_THRESHOLD_TERMP_MIN_SYNC    6
+#define V_THRESHOLD_TERMP_MIN_SYNC(x) ((x) << S_THRESHOLD_TERMP_MIN_SYNC)
+#define F_THRESHOLD_TERMP_MIN_SYNC    V_THRESHOLD_TERMP_MIN_SYNC(1U)
+
+#define S_THRESHOLD_TERMN_MAX_SYNC    5
+#define V_THRESHOLD_TERMN_MAX_SYNC(x) ((x) << S_THRESHOLD_TERMN_MAX_SYNC)
+#define F_THRESHOLD_TERMN_MAX_SYNC    V_THRESHOLD_TERMN_MAX_SYNC(1U)
+
+#define S_THRESHOLD_TERMN_MIN_SYNC    4
+#define V_THRESHOLD_TERMN_MIN_SYNC(x) ((x) << S_THRESHOLD_TERMN_MIN_SYNC)
+#define F_THRESHOLD_TERMN_MIN_SYNC    V_THRESHOLD_TERMN_MIN_SYNC(1U)
+
+#define S_THRESHOLD_DRVP_MAX_SYNC    3
+#define V_THRESHOLD_DRVP_MAX_SYNC(x) ((x) << S_THRESHOLD_DRVP_MAX_SYNC)
+#define F_THRESHOLD_DRVP_MAX_SYNC    V_THRESHOLD_DRVP_MAX_SYNC(1U)
+
+#define S_THRESHOLD_DRVP_MIN_SYNC    2
+#define V_THRESHOLD_DRVP_MIN_SYNC(x) ((x) << S_THRESHOLD_DRVP_MIN_SYNC)
+#define F_THRESHOLD_DRVP_MIN_SYNC    V_THRESHOLD_DRVP_MIN_SYNC(1U)
+
+#define S_THRESHOLD_DRVN_MAX_SYNC    1
+#define V_THRESHOLD_DRVN_MAX_SYNC(x) ((x) << S_THRESHOLD_DRVN_MAX_SYNC)
+#define F_THRESHOLD_DRVN_MAX_SYNC    V_THRESHOLD_DRVN_MAX_SYNC(1U)
+
+#define S_THRESHOLD_DRVN_MIN_SYNC    0
+#define V_THRESHOLD_DRVN_MIN_SYNC(x) ((x) << S_THRESHOLD_DRVN_MIN_SYNC)
+#define F_THRESHOLD_DRVN_MIN_SYNC    V_THRESHOLD_DRVN_MIN_SYNC(1U)
+
+#define A_DBG_PVT_REG_IN_TERMP 0x6120
+
+#define S_REG_IN_TERMP_B    4
+#define M_REG_IN_TERMP_B    0xfU
+#define V_REG_IN_TERMP_B(x) ((x) << S_REG_IN_TERMP_B)
+#define G_REG_IN_TERMP_B(x) (((x) >> S_REG_IN_TERMP_B) & M_REG_IN_TERMP_B)
+
+#define S_REG_IN_TERMP_A    0
+#define M_REG_IN_TERMP_A    0xfU
+#define V_REG_IN_TERMP_A(x) ((x) << S_REG_IN_TERMP_A)
+#define G_REG_IN_TERMP_A(x) (((x) >> S_REG_IN_TERMP_A) & M_REG_IN_TERMP_A)
+
+#define A_DBG_PVT_REG_IN_TERMN 0x6124
+
+#define S_REG_IN_TERMN_B    4
+#define M_REG_IN_TERMN_B    0xfU
+#define V_REG_IN_TERMN_B(x) ((x) << S_REG_IN_TERMN_B)
+#define G_REG_IN_TERMN_B(x) (((x) >> S_REG_IN_TERMN_B) & M_REG_IN_TERMN_B)
+
+#define S_REG_IN_TERMN_A    0
+#define M_REG_IN_TERMN_A    0xfU
+#define V_REG_IN_TERMN_A(x) ((x) << S_REG_IN_TERMN_A)
+#define G_REG_IN_TERMN_A(x) (((x) >> S_REG_IN_TERMN_A) & M_REG_IN_TERMN_A)
+
+#define A_DBG_PVT_REG_IN_DRVP 0x6128
+
+#define S_REG_IN_DRVP_B    4
+#define M_REG_IN_DRVP_B    0xfU
+#define V_REG_IN_DRVP_B(x) ((x) << S_REG_IN_DRVP_B)
+#define G_REG_IN_DRVP_B(x) (((x) >> S_REG_IN_DRVP_B) & M_REG_IN_DRVP_B)
+
+#define S_REG_IN_DRVP_A    0
+#define M_REG_IN_DRVP_A    0xfU
+#define V_REG_IN_DRVP_A(x) ((x) << S_REG_IN_DRVP_A)
+#define G_REG_IN_DRVP_A(x) (((x) >> S_REG_IN_DRVP_A) & M_REG_IN_DRVP_A)
+
+#define A_DBG_PVT_REG_IN_DRVN 0x612c
+
+#define S_REG_IN_DRVN_B    4
+#define M_REG_IN_DRVN_B    0xfU
+#define V_REG_IN_DRVN_B(x) ((x) << S_REG_IN_DRVN_B)
+#define G_REG_IN_DRVN_B(x) (((x) >> S_REG_IN_DRVN_B) & M_REG_IN_DRVN_B)
+
+#define S_REG_IN_DRVN_A    0
+#define M_REG_IN_DRVN_A    0xfU
+#define V_REG_IN_DRVN_A(x) ((x) << S_REG_IN_DRVN_A)
+#define G_REG_IN_DRVN_A(x) (((x) >> S_REG_IN_DRVN_A) & M_REG_IN_DRVN_A)
+
+#define A_DBG_PVT_REG_OUT_TERMP 0x6130
+
+#define S_REG_OUT_TERMP_B    4
+#define M_REG_OUT_TERMP_B    0xfU
+#define V_REG_OUT_TERMP_B(x) ((x) << S_REG_OUT_TERMP_B)
+#define G_REG_OUT_TERMP_B(x) (((x) >> S_REG_OUT_TERMP_B) & M_REG_OUT_TERMP_B)
+
+#define S_REG_OUT_TERMP_A    0
+#define M_REG_OUT_TERMP_A    0xfU
+#define V_REG_OUT_TERMP_A(x) ((x) << S_REG_OUT_TERMP_A)
+#define G_REG_OUT_TERMP_A(x) (((x) >> S_REG_OUT_TERMP_A) & M_REG_OUT_TERMP_A)
+
+#define A_DBG_PVT_REG_OUT_TERMN 0x6134
+
+#define S_REG_OUT_TERMN_B    4
+#define M_REG_OUT_TERMN_B    0xfU
+#define V_REG_OUT_TERMN_B(x) ((x) << S_REG_OUT_TERMN_B)
+#define G_REG_OUT_TERMN_B(x) (((x) >> S_REG_OUT_TERMN_B) & M_REG_OUT_TERMN_B)
+
+#define S_REG_OUT_TERMN_A    0
+#define M_REG_OUT_TERMN_A    0xfU
+#define V_REG_OUT_TERMN_A(x) ((x) << S_REG_OUT_TERMN_A)
+#define G_REG_OUT_TERMN_A(x) (((x) >> S_REG_OUT_TERMN_A) & M_REG_OUT_TERMN_A)
+
+#define A_DBG_PVT_REG_OUT_DRVP 0x6138
+
+#define S_REG_OUT_DRVP_B    4
+#define M_REG_OUT_DRVP_B    0xfU
+#define V_REG_OUT_DRVP_B(x) ((x) << S_REG_OUT_DRVP_B)
+#define G_REG_OUT_DRVP_B(x) (((x) >> S_REG_OUT_DRVP_B) & M_REG_OUT_DRVP_B)
+
+#define S_REG_OUT_DRVP_A    0
+#define M_REG_OUT_DRVP_A    0xfU
+#define V_REG_OUT_DRVP_A(x) ((x) << S_REG_OUT_DRVP_A)
+#define G_REG_OUT_DRVP_A(x) (((x) >> S_REG_OUT_DRVP_A) & M_REG_OUT_DRVP_A)
+
+#define A_DBG_PVT_REG_OUT_DRVN 0x613c
+
+#define S_REG_OUT_DRVN_B    4
+#define M_REG_OUT_DRVN_B    0xfU
+#define V_REG_OUT_DRVN_B(x) ((x) << S_REG_OUT_DRVN_B)
+#define G_REG_OUT_DRVN_B(x) (((x) >> S_REG_OUT_DRVN_B) & M_REG_OUT_DRVN_B)
+
+#define S_REG_OUT_DRVN_A    0
+#define M_REG_OUT_DRVN_A    0xfU
+#define V_REG_OUT_DRVN_A(x) ((x) << S_REG_OUT_DRVN_A)
+#define G_REG_OUT_DRVN_A(x) (((x) >> S_REG_OUT_DRVN_A) & M_REG_OUT_DRVN_A)
+
+#define A_DBG_PVT_REG_HISTORY_TERMP 0x6140
+
+#define S_TERMP_B_HISTORY    4
+#define M_TERMP_B_HISTORY    0xfU
+#define V_TERMP_B_HISTORY(x) ((x) << S_TERMP_B_HISTORY)
+#define G_TERMP_B_HISTORY(x) (((x) >> S_TERMP_B_HISTORY) & M_TERMP_B_HISTORY)
+
+#define S_TERMP_A_HISTORY    0
+#define M_TERMP_A_HISTORY    0xfU
+#define V_TERMP_A_HISTORY(x) ((x) << S_TERMP_A_HISTORY)
+#define G_TERMP_A_HISTORY(x) (((x) >> S_TERMP_A_HISTORY) & M_TERMP_A_HISTORY)
+
+#define A_DBG_PVT_REG_HISTORY_TERMN 0x6144
+
+#define S_TERMN_B_HISTORY    4
+#define M_TERMN_B_HISTORY    0xfU
+#define V_TERMN_B_HISTORY(x) ((x) << S_TERMN_B_HISTORY)
+#define G_TERMN_B_HISTORY(x) (((x) >> S_TERMN_B_HISTORY) & M_TERMN_B_HISTORY)
+
+#define S_TERMN_A_HISTORY    0
+#define M_TERMN_A_HISTORY    0xfU
+#define V_TERMN_A_HISTORY(x) ((x) << S_TERMN_A_HISTORY)
+#define G_TERMN_A_HISTORY(x) (((x) >> S_TERMN_A_HISTORY) & M_TERMN_A_HISTORY)
+
+#define A_DBG_PVT_REG_HISTORY_DRVP 0x6148
+
+#define S_DRVP_B_HISTORY    4
+#define M_DRVP_B_HISTORY    0xfU
+#define V_DRVP_B_HISTORY(x) ((x) << S_DRVP_B_HISTORY)
+#define G_DRVP_B_HISTORY(x) (((x) >> S_DRVP_B_HISTORY) & M_DRVP_B_HISTORY)
+
+#define S_DRVP_A_HISTORY    0
+#define M_DRVP_A_HISTORY    0xfU
+#define V_DRVP_A_HISTORY(x) ((x) << S_DRVP_A_HISTORY)
+#define G_DRVP_A_HISTORY(x) (((x) >> S_DRVP_A_HISTORY) & M_DRVP_A_HISTORY)
+
+#define A_DBG_PVT_REG_HISTORY_DRVN 0x614c
+
+#define S_DRVN_B_HISTORY    4
+#define M_DRVN_B_HISTORY    0xfU
+#define V_DRVN_B_HISTORY(x) ((x) << S_DRVN_B_HISTORY)
+#define G_DRVN_B_HISTORY(x) (((x) >> S_DRVN_B_HISTORY) & M_DRVN_B_HISTORY)
+
+#define S_DRVN_A_HISTORY    0
+#define M_DRVN_A_HISTORY    0xfU
+#define V_DRVN_A_HISTORY(x) ((x) << S_DRVN_A_HISTORY)
+#define G_DRVN_A_HISTORY(x) (((x) >> S_DRVN_A_HISTORY) & M_DRVN_A_HISTORY)
+
+#define A_DBG_PVT_REG_SAMPLE_WAIT_CLKS 0x6150
+
+#define S_SAMPLE_WAIT_CLKS    0
+#define M_SAMPLE_WAIT_CLKS    0x1fU
+#define V_SAMPLE_WAIT_CLKS(x) ((x) << S_SAMPLE_WAIT_CLKS)
+#define G_SAMPLE_WAIT_CLKS(x) (((x) >> S_SAMPLE_WAIT_CLKS) & M_SAMPLE_WAIT_CLKS)
+
+/* registers for module MC */
+#define MC_BASE_ADDR 0x6200
+
+#define A_MC_PCTL_SCFG 0x6200
+
+#define S_RKINF_EN    5
+#define V_RKINF_EN(x) ((x) << S_RKINF_EN)
+#define F_RKINF_EN    V_RKINF_EN(1U)
+
+#define S_DUAL_PCTL_EN    4
+#define V_DUAL_PCTL_EN(x) ((x) << S_DUAL_PCTL_EN)
+#define F_DUAL_PCTL_EN    V_DUAL_PCTL_EN(1U)
+
+#define S_SLAVE_MODE    3
+#define V_SLAVE_MODE(x) ((x) << S_SLAVE_MODE)
+#define F_SLAVE_MODE    V_SLAVE_MODE(1U)
+
+#define S_LOOPBACK_EN    1
+#define V_LOOPBACK_EN(x) ((x) << S_LOOPBACK_EN)
+#define F_LOOPBACK_EN    V_LOOPBACK_EN(1U)
+
+#define S_HW_LOW_POWER_EN    0
+#define V_HW_LOW_POWER_EN(x) ((x) << S_HW_LOW_POWER_EN)
+#define F_HW_LOW_POWER_EN    V_HW_LOW_POWER_EN(1U)
+
+#define A_MC_PCTL_SCTL 0x6204
+
+#define S_STATE_CMD    0
+#define M_STATE_CMD    0x7U
+#define V_STATE_CMD(x) ((x) << S_STATE_CMD)
+#define G_STATE_CMD(x) (((x) >> S_STATE_CMD) & M_STATE_CMD)
+
+#define A_MC_PCTL_STAT 0x6208
+
+#define S_CTL_STAT    0
+#define M_CTL_STAT    0x7U
+#define V_CTL_STAT(x) ((x) << S_CTL_STAT)
+#define G_CTL_STAT(x) (((x) >> S_CTL_STAT) & M_CTL_STAT)
+
+#define A_MC_PCTL_MCMD 0x6240
+
+#define S_START_CMD    31
+#define V_START_CMD(x) ((x) << S_START_CMD)
+#define F_START_CMD    V_START_CMD(1U)
+
+#define S_CMD_ADD_DEL    24
+#define M_CMD_ADD_DEL    0xfU
+#define V_CMD_ADD_DEL(x) ((x) << S_CMD_ADD_DEL)
+#define G_CMD_ADD_DEL(x) (((x) >> S_CMD_ADD_DEL) & M_CMD_ADD_DEL)
+
+#define S_RANK_SEL    20
+#define M_RANK_SEL    0xfU
+#define V_RANK_SEL(x) ((x) << S_RANK_SEL)
+#define G_RANK_SEL(x) (((x) >> S_RANK_SEL) & M_RANK_SEL)
+
+#define S_BANK_ADDR    17
+#define M_BANK_ADDR    0x7U
+#define V_BANK_ADDR(x) ((x) << S_BANK_ADDR)
+#define G_BANK_ADDR(x) (((x) >> S_BANK_ADDR) & M_BANK_ADDR)
+
+#define S_CMD_ADDR    4
+#define M_CMD_ADDR    0x1fffU
+#define V_CMD_ADDR(x) ((x) << S_CMD_ADDR)
+#define G_CMD_ADDR(x) (((x) >> S_CMD_ADDR) & M_CMD_ADDR)
+
+#define S_CMD_OPCODE    0
+#define M_CMD_OPCODE    0x7U
+#define V_CMD_OPCODE(x) ((x) << S_CMD_OPCODE)
+#define G_CMD_OPCODE(x) (((x) >> S_CMD_OPCODE) & M_CMD_OPCODE)
+
+#define A_MC_PCTL_POWCTL 0x6244
+
+#define S_POWER_UP_START    0
+#define V_POWER_UP_START(x) ((x) << S_POWER_UP_START)
+#define F_POWER_UP_START    V_POWER_UP_START(1U)
+
+#define A_MC_PCTL_POWSTAT 0x6248
+
+#define S_PHY_CALIBDONE    1
+#define V_PHY_CALIBDONE(x) ((x) << S_PHY_CALIBDONE)
+#define F_PHY_CALIBDONE    V_PHY_CALIBDONE(1U)
+
+#define S_POWER_UP_DONE    0
+#define V_POWER_UP_DONE(x) ((x) << S_POWER_UP_DONE)
+#define F_POWER_UP_DONE    V_POWER_UP_DONE(1U)
+
+#define A_MC_PCTL_MCFG 0x6280
+
+#define S_TFAW_CFG    18
+#define M_TFAW_CFG    0x3U
+#define V_TFAW_CFG(x) ((x) << S_TFAW_CFG)
+#define G_TFAW_CFG(x) (((x) >> S_TFAW_CFG) & M_TFAW_CFG)
+
+#define S_PD_EXIT_MODE    17
+#define V_PD_EXIT_MODE(x) ((x) << S_PD_EXIT_MODE)
+#define F_PD_EXIT_MODE    V_PD_EXIT_MODE(1U)
+
+#define S_PD_TYPE    16
+#define V_PD_TYPE(x) ((x) << S_PD_TYPE)
+#define F_PD_TYPE    V_PD_TYPE(1U)
+
+#define S_PD_IDLE    8
+#define M_PD_IDLE    0xffU
+#define V_PD_IDLE(x) ((x) << S_PD_IDLE)
+#define G_PD_IDLE(x) (((x) >> S_PD_IDLE) & M_PD_IDLE)
+
+#define S_PAGE_POLICY    6
+#define M_PAGE_POLICY    0x3U
+#define V_PAGE_POLICY(x) ((x) << S_PAGE_POLICY)
+#define G_PAGE_POLICY(x) (((x) >> S_PAGE_POLICY) & M_PAGE_POLICY)
+
+#define S_DDR3_EN    5
+#define V_DDR3_EN(x) ((x) << S_DDR3_EN)
+#define F_DDR3_EN    V_DDR3_EN(1U)
+
+#define S_TWO_T_EN    3
+#define V_TWO_T_EN(x) ((x) << S_TWO_T_EN)
+#define F_TWO_T_EN    V_TWO_T_EN(1U)
+
+#define S_BL8INT_EN    2
+#define V_BL8INT_EN(x) ((x) << S_BL8INT_EN)
+#define F_BL8INT_EN    V_BL8INT_EN(1U)
+
+#define S_MEM_BL    0
+#define V_MEM_BL(x) ((x) << S_MEM_BL)
+#define F_MEM_BL    V_MEM_BL(1U)
+
+#define A_MC_PCTL_PPCFG 0x6284
+
+#define S_RPMEM_DIS    1
+#define M_RPMEM_DIS    0xffU
+#define V_RPMEM_DIS(x) ((x) << S_RPMEM_DIS)
+#define G_RPMEM_DIS(x) (((x) >> S_RPMEM_DIS) & M_RPMEM_DIS)
+
+#define S_PPMEM_EN    0
+#define V_PPMEM_EN(x) ((x) << S_PPMEM_EN)
+#define F_PPMEM_EN    V_PPMEM_EN(1U)
+
+#define A_MC_PCTL_MSTAT 0x6288
+
+#define S_POWER_DOWN    0
+#define V_POWER_DOWN(x) ((x) << S_POWER_DOWN)
+#define F_POWER_DOWN    V_POWER_DOWN(1U)
+
+#define A_MC_PCTL_ODTCFG 0x628c
+
+#define S_RANK3_ODT_DEFAULT    28
+#define V_RANK3_ODT_DEFAULT(x) ((x) << S_RANK3_ODT_DEFAULT)
+#define F_RANK3_ODT_DEFAULT    V_RANK3_ODT_DEFAULT(1U)
+
+#define S_RANK3_ODT_WRITE_SEL    27
+#define V_RANK3_ODT_WRITE_SEL(x) ((x) << S_RANK3_ODT_WRITE_SEL)
+#define F_RANK3_ODT_WRITE_SEL    V_RANK3_ODT_WRITE_SEL(1U)
+
+#define S_RANK3_ODT_WRITE_NSE    26
+#define V_RANK3_ODT_WRITE_NSE(x) ((x) << S_RANK3_ODT_WRITE_NSE)
+#define F_RANK3_ODT_WRITE_NSE    V_RANK3_ODT_WRITE_NSE(1U)
+
+#define S_RANK3_ODT_READ_SEL    25
+#define V_RANK3_ODT_READ_SEL(x) ((x) << S_RANK3_ODT_READ_SEL)
+#define F_RANK3_ODT_READ_SEL    V_RANK3_ODT_READ_SEL(1U)
+
+#define S_RANK3_ODT_READ_NSEL    24
+#define V_RANK3_ODT_READ_NSEL(x) ((x) << S_RANK3_ODT_READ_NSEL)
+#define F_RANK3_ODT_READ_NSEL    V_RANK3_ODT_READ_NSEL(1U)
+
+#define S_RANK2_ODT_DEFAULT    20
+#define V_RANK2_ODT_DEFAULT(x) ((x) << S_RANK2_ODT_DEFAULT)
+#define F_RANK2_ODT_DEFAULT    V_RANK2_ODT_DEFAULT(1U)
+
+#define S_RANK2_ODT_WRITE_SEL    19
+#define V_RANK2_ODT_WRITE_SEL(x) ((x) << S_RANK2_ODT_WRITE_SEL)
+#define F_RANK2_ODT_WRITE_SEL    V_RANK2_ODT_WRITE_SEL(1U)
+
+#define S_RANK2_ODT_WRITE_NSEL    18
+#define V_RANK2_ODT_WRITE_NSEL(x) ((x) << S_RANK2_ODT_WRITE_NSEL)
+#define F_RANK2_ODT_WRITE_NSEL    V_RANK2_ODT_WRITE_NSEL(1U)
+
+#define S_RANK2_ODT_READ_SEL    17
+#define V_RANK2_ODT_READ_SEL(x) ((x) << S_RANK2_ODT_READ_SEL)
+#define F_RANK2_ODT_READ_SEL    V_RANK2_ODT_READ_SEL(1U)
+
+#define S_RANK2_ODT_READ_NSEL    16
+#define V_RANK2_ODT_READ_NSEL(x) ((x) << S_RANK2_ODT_READ_NSEL)
+#define F_RANK2_ODT_READ_NSEL    V_RANK2_ODT_READ_NSEL(1U)
+
+#define S_RANK1_ODT_DEFAULT    12
+#define V_RANK1_ODT_DEFAULT(x) ((x) << S_RANK1_ODT_DEFAULT)
+#define F_RANK1_ODT_DEFAULT    V_RANK1_ODT_DEFAULT(1U)
+
+#define S_RANK1_ODT_WRITE_SEL    11
+#define V_RANK1_ODT_WRITE_SEL(x) ((x) << S_RANK1_ODT_WRITE_SEL)
+#define F_RANK1_ODT_WRITE_SEL    V_RANK1_ODT_WRITE_SEL(1U)
+
+#define S_RANK1_ODT_WRITE_NSEL    10
+#define V_RANK1_ODT_WRITE_NSEL(x) ((x) << S_RANK1_ODT_WRITE_NSEL)
+#define F_RANK1_ODT_WRITE_NSEL    V_RANK1_ODT_WRITE_NSEL(1U)
+
+#define S_RANK1_ODT_READ_SEL    9
+#define V_RANK1_ODT_READ_SEL(x) ((x) << S_RANK1_ODT_READ_SEL)
+#define F_RANK1_ODT_READ_SEL    V_RANK1_ODT_READ_SEL(1U)
+
+#define S_RANK1_ODT_READ_NSEL    8
+#define V_RANK1_ODT_READ_NSEL(x) ((x) << S_RANK1_ODT_READ_NSEL)
+#define F_RANK1_ODT_READ_NSEL    V_RANK1_ODT_READ_NSEL(1U)
+
+#define S_RANK0_ODT_DEFAULT    4
+#define V_RANK0_ODT_DEFAULT(x) ((x) << S_RANK0_ODT_DEFAULT)
+#define F_RANK0_ODT_DEFAULT    V_RANK0_ODT_DEFAULT(1U)
+
+#define S_RANK0_ODT_WRITE_SEL    3
+#define V_RANK0_ODT_WRITE_SEL(x) ((x) << S_RANK0_ODT_WRITE_SEL)
+#define F_RANK0_ODT_WRITE_SEL    V_RANK0_ODT_WRITE_SEL(1U)
+
+#define S_RANK0_ODT_WRITE_NSEL    2
+#define V_RANK0_ODT_WRITE_NSEL(x) ((x) << S_RANK0_ODT_WRITE_NSEL)
+#define F_RANK0_ODT_WRITE_NSEL    V_RANK0_ODT_WRITE_NSEL(1U)
+
+#define S_RANK0_ODT_READ_SEL    1
+#define V_RANK0_ODT_READ_SEL(x) ((x) << S_RANK0_ODT_READ_SEL)
+#define F_RANK0_ODT_READ_SEL    V_RANK0_ODT_READ_SEL(1U)
+
+#define S_RANK0_ODT_READ_NSEL    0
+#define V_RANK0_ODT_READ_NSEL(x) ((x) << S_RANK0_ODT_READ_NSEL)
+#define F_RANK0_ODT_READ_NSEL    V_RANK0_ODT_READ_NSEL(1U)
+
+#define A_MC_PCTL_DQSECFG 0x6290
+
+#define S_DV_ALAT    20
+#define M_DV_ALAT    0xfU
+#define V_DV_ALAT(x) ((x) << S_DV_ALAT)
+#define G_DV_ALAT(x) (((x) >> S_DV_ALAT) & M_DV_ALAT)
+
+#define S_DV_ALEN    16
+#define M_DV_ALEN    0x3U
+#define V_DV_ALEN(x) ((x) << S_DV_ALEN)
+#define G_DV_ALEN(x) (((x) >> S_DV_ALEN) & M_DV_ALEN)
+
+#define S_DSE_ALAT    12
+#define M_DSE_ALAT    0xfU
+#define V_DSE_ALAT(x) ((x) << S_DSE_ALAT)
+#define G_DSE_ALAT(x) (((x) >> S_DSE_ALAT) & M_DSE_ALAT)
+
+#define S_DSE_ALEN    8
+#define M_DSE_ALEN    0x3U
+#define V_DSE_ALEN(x) ((x) << S_DSE_ALEN)
+#define G_DSE_ALEN(x) (((x) >> S_DSE_ALEN) & M_DSE_ALEN)
+
+#define S_QSE_ALAT    4
+#define M_QSE_ALAT    0xfU
+#define V_QSE_ALAT(x) ((x) << S_QSE_ALAT)
+#define G_QSE_ALAT(x) (((x) >> S_QSE_ALAT) & M_QSE_ALAT)
+
+#define S_QSE_ALEN    0
+#define M_QSE_ALEN    0x3U
+#define V_QSE_ALEN(x) ((x) << S_QSE_ALEN)
+#define G_QSE_ALEN(x) (((x) >> S_QSE_ALEN) & M_QSE_ALEN)
+
+#define A_MC_PCTL_DTUPDES 0x6294
+
+#define S_DTU_RD_MISSING    13
+#define V_DTU_RD_MISSING(x) ((x) << S_DTU_RD_MISSING)
+#define F_DTU_RD_MISSING    V_DTU_RD_MISSING(1U)
+
+#define S_DTU_EAFFL    9
+#define M_DTU_EAFFL    0xfU
+#define V_DTU_EAFFL(x) ((x) << S_DTU_EAFFL)
+#define G_DTU_EAFFL(x) (((x) >> S_DTU_EAFFL) & M_DTU_EAFFL)
+
+#define S_DTU_RANDOM_ERROR    8
+#define V_DTU_RANDOM_ERROR(x) ((x) << S_DTU_RANDOM_ERROR)
+#define F_DTU_RANDOM_ERROR    V_DTU_RANDOM_ERROR(1U)
+
+#define S_DTU_ERROR_B7    7
+#define V_DTU_ERROR_B7(x) ((x) << S_DTU_ERROR_B7)
+#define F_DTU_ERROR_B7    V_DTU_ERROR_B7(1U)
+
+#define S_DTU_ERR_B6    6
+#define V_DTU_ERR_B6(x) ((x) << S_DTU_ERR_B6)
+#define F_DTU_ERR_B6    V_DTU_ERR_B6(1U)
+
+#define S_DTU_ERR_B5    5
+#define V_DTU_ERR_B5(x) ((x) << S_DTU_ERR_B5)
+#define F_DTU_ERR_B5    V_DTU_ERR_B5(1U)
+
+#define S_DTU_ERR_B4    4
+#define V_DTU_ERR_B4(x) ((x) << S_DTU_ERR_B4)
+#define F_DTU_ERR_B4    V_DTU_ERR_B4(1U)
+
+#define S_DTU_ERR_B3    3
+#define V_DTU_ERR_B3(x) ((x) << S_DTU_ERR_B3)
+#define F_DTU_ERR_B3    V_DTU_ERR_B3(1U)
+
+#define S_DTU_ERR_B2    2
+#define V_DTU_ERR_B2(x) ((x) << S_DTU_ERR_B2)
+#define F_DTU_ERR_B2    V_DTU_ERR_B2(1U)
+
+#define S_DTU_ERR_B1    1
+#define V_DTU_ERR_B1(x) ((x) << S_DTU_ERR_B1)
+#define F_DTU_ERR_B1    V_DTU_ERR_B1(1U)
+
+#define S_DTU_ERR_B0    0
+#define V_DTU_ERR_B0(x) ((x) << S_DTU_ERR_B0)
+#define F_DTU_ERR_B0    V_DTU_ERR_B0(1U)
+
+#define A_MC_PCTL_DTUNA 0x6298
+#define A_MC_PCTL_DTUNE 0x629c
+#define A_MC_PCTL_DTUPRDO 0x62a0
+
+#define S_DTU_ALLBITS_1    16
+#define M_DTU_ALLBITS_1    0xffffU
+#define V_DTU_ALLBITS_1(x) ((x) << S_DTU_ALLBITS_1)
+#define G_DTU_ALLBITS_1(x) (((x) >> S_DTU_ALLBITS_1) & M_DTU_ALLBITS_1)
+
+#define S_DTU_ALLBITS_0    0
+#define M_DTU_ALLBITS_0    0xffffU
+#define V_DTU_ALLBITS_0(x) ((x) << S_DTU_ALLBITS_0)
+#define G_DTU_ALLBITS_0(x) (((x) >> S_DTU_ALLBITS_0) & M_DTU_ALLBITS_0)
+
+#define A_MC_PCTL_DTUPRD1 0x62a4
+
+#define S_DTU_ALLBITS_3    16
+#define M_DTU_ALLBITS_3    0xffffU
+#define V_DTU_ALLBITS_3(x) ((x) << S_DTU_ALLBITS_3)
+#define G_DTU_ALLBITS_3(x) (((x) >> S_DTU_ALLBITS_3) & M_DTU_ALLBITS_3)
+
+#define S_DTU_ALLBITS_2    0
+#define M_DTU_ALLBITS_2    0xffffU
+#define V_DTU_ALLBITS_2(x) ((x) << S_DTU_ALLBITS_2)
+#define G_DTU_ALLBITS_2(x) (((x) >> S_DTU_ALLBITS_2) & M_DTU_ALLBITS_2)
+
+#define A_MC_PCTL_DTUPRD2 0x62a8
+
+#define S_DTU_ALLBITS_5    16
+#define M_DTU_ALLBITS_5    0xffffU
+#define V_DTU_ALLBITS_5(x) ((x) << S_DTU_ALLBITS_5)
+#define G_DTU_ALLBITS_5(x) (((x) >> S_DTU_ALLBITS_5) & M_DTU_ALLBITS_5)
+
+#define S_DTU_ALLBITS_4    0
+#define M_DTU_ALLBITS_4    0xffffU
+#define V_DTU_ALLBITS_4(x) ((x) << S_DTU_ALLBITS_4)
+#define G_DTU_ALLBITS_4(x) (((x) >> S_DTU_ALLBITS_4) & M_DTU_ALLBITS_4)
+
+#define A_MC_PCTL_DTUPRD3 0x62ac
+
+#define S_DTU_ALLBITS_7    16
+#define M_DTU_ALLBITS_7    0xffffU
+#define V_DTU_ALLBITS_7(x) ((x) << S_DTU_ALLBITS_7)
+#define G_DTU_ALLBITS_7(x) (((x) >> S_DTU_ALLBITS_7) & M_DTU_ALLBITS_7)
+
+#define S_DTU_ALLBITS_6    0
+#define M_DTU_ALLBITS_6    0xffffU
+#define V_DTU_ALLBITS_6(x) ((x) << S_DTU_ALLBITS_6)
+#define G_DTU_ALLBITS_6(x) (((x) >> S_DTU_ALLBITS_6) & M_DTU_ALLBITS_6)
+
+#define A_MC_PCTL_DTUAWDT 0x62b0
+
+#define S_NUMBER_RANKS    9
+#define M_NUMBER_RANKS    0x3U
+#define V_NUMBER_RANKS(x) ((x) << S_NUMBER_RANKS)
+#define G_NUMBER_RANKS(x) (((x) >> S_NUMBER_RANKS) & M_NUMBER_RANKS)
+
+#define S_ROW_ADDR_WIDTH    6
+#define M_ROW_ADDR_WIDTH    0x3U
+#define V_ROW_ADDR_WIDTH(x) ((x) << S_ROW_ADDR_WIDTH)
+#define G_ROW_ADDR_WIDTH(x) (((x) >> S_ROW_ADDR_WIDTH) & M_ROW_ADDR_WIDTH)
+
+#define S_BANK_ADDR_WIDTH    3
+#define M_BANK_ADDR_WIDTH    0x3U
+#define V_BANK_ADDR_WIDTH(x) ((x) << S_BANK_ADDR_WIDTH)
+#define G_BANK_ADDR_WIDTH(x) (((x) >> S_BANK_ADDR_WIDTH) & M_BANK_ADDR_WIDTH)
+
+#define S_COLUMN_ADDR_WIDTH    0
+#define M_COLUMN_ADDR_WIDTH    0x3U
+#define V_COLUMN_ADDR_WIDTH(x) ((x) << S_COLUMN_ADDR_WIDTH)
+#define G_COLUMN_ADDR_WIDTH(x) (((x) >> S_COLUMN_ADDR_WIDTH) & M_COLUMN_ADDR_WIDTH)
+
+#define A_MC_PCTL_TOGCNT1U 0x62c0
+
+#define S_TOGGLE_COUNTER_1U    0
+#define M_TOGGLE_COUNTER_1U    0x3ffU
+#define V_TOGGLE_COUNTER_1U(x) ((x) << S_TOGGLE_COUNTER_1U)
+#define G_TOGGLE_COUNTER_1U(x) (((x) >> S_TOGGLE_COUNTER_1U) & M_TOGGLE_COUNTER_1U)
+
+#define A_MC_PCTL_TINIT 0x62c4
+
+#define S_T_INIT    0
+#define M_T_INIT    0x1ffU
+#define V_T_INIT(x) ((x) << S_T_INIT)
+#define G_T_INIT(x) (((x) >> S_T_INIT) & M_T_INIT)
+
+#define A_MC_PCTL_TRSTH 0x62c8
+
+#define S_T_RSTH    0
+#define M_T_RSTH    0x3ffU
+#define V_T_RSTH(x) ((x) << S_T_RSTH)
+#define G_T_RSTH(x) (((x) >> S_T_RSTH) & M_T_RSTH)
+
+#define A_MC_PCTL_TOGCNT100N 0x62cc
+
+#define S_TOGGLE_COUNTER_100N    0
+#define M_TOGGLE_COUNTER_100N    0x7fU
+#define V_TOGGLE_COUNTER_100N(x) ((x) << S_TOGGLE_COUNTER_100N)
+#define G_TOGGLE_COUNTER_100N(x) (((x) >> S_TOGGLE_COUNTER_100N) & M_TOGGLE_COUNTER_100N)
+
+#define A_MC_PCTL_TREFI 0x62d0
+
+#define S_T_REFI    0
+#define M_T_REFI    0xffU
+#define V_T_REFI(x) ((x) << S_T_REFI)
+#define G_T_REFI(x) (((x) >> S_T_REFI) & M_T_REFI)
+
+#define A_MC_PCTL_TMRD 0x62d4
+
+#define S_T_MRD    0
+#define M_T_MRD    0x7U
+#define V_T_MRD(x) ((x) << S_T_MRD)
+#define G_T_MRD(x) (((x) >> S_T_MRD) & M_T_MRD)
+
+#define A_MC_PCTL_TRFC 0x62d8
+
+#define S_T_RFC    0
+#define M_T_RFC    0xffU
+#define V_T_RFC(x) ((x) << S_T_RFC)
+#define G_T_RFC(x) (((x) >> S_T_RFC) & M_T_RFC)
+
+#define A_MC_PCTL_TRP 0x62dc
+
+#define S_T_RP    0
+#define M_T_RP    0xfU
+#define V_T_RP(x) ((x) << S_T_RP)
+#define G_T_RP(x) (((x) >> S_T_RP) & M_T_RP)
+
+#define A_MC_PCTL_TRTW 0x62e0
+
+#define S_T_RTW    0
+#define M_T_RTW    0x7U
+#define V_T_RTW(x) ((x) << S_T_RTW)
+#define G_T_RTW(x) (((x) >> S_T_RTW) & M_T_RTW)
+
+#define A_MC_PCTL_TAL 0x62e4
+
+#define S_T_AL    0
+#define M_T_AL    0xfU
+#define V_T_AL(x) ((x) << S_T_AL)
+#define G_T_AL(x) (((x) >> S_T_AL) & M_T_AL)
+
+#define A_MC_PCTL_TCL 0x62e8
+
+#define S_T_CL    0
+#define M_T_CL    0xfU
+#define V_T_CL(x) ((x) << S_T_CL)
+#define G_T_CL(x) (((x) >> S_T_CL) & M_T_CL)
+
+#define A_MC_PCTL_TCWL 0x62ec
+
+#define S_T_CWL    0
+#define M_T_CWL    0xfU
+#define V_T_CWL(x) ((x) << S_T_CWL)
+#define G_T_CWL(x) (((x) >> S_T_CWL) & M_T_CWL)
+
+#define A_MC_PCTL_TRAS 0x62f0
+
+#define S_T_RAS    0
+#define M_T_RAS    0x3fU
+#define V_T_RAS(x) ((x) << S_T_RAS)
+#define G_T_RAS(x) (((x) >> S_T_RAS) & M_T_RAS)
+
+#define A_MC_PCTL_TRC 0x62f4
+
+#define S_T_RC    0
+#define M_T_RC    0x3fU
+#define V_T_RC(x) ((x) << S_T_RC)
+#define G_T_RC(x) (((x) >> S_T_RC) & M_T_RC)
+
+#define A_MC_PCTL_TRCD 0x62f8
+
+#define S_T_RCD    0
+#define M_T_RCD    0xfU
+#define V_T_RCD(x) ((x) << S_T_RCD)
+#define G_T_RCD(x) (((x) >> S_T_RCD) & M_T_RCD)
+
+#define A_MC_PCTL_TRRD 0x62fc
+
+#define S_T_RRD    0
+#define M_T_RRD    0xfU
+#define V_T_RRD(x) ((x) << S_T_RRD)
+#define G_T_RRD(x) (((x) >> S_T_RRD) & M_T_RRD)
+
+#define A_MC_PCTL_TRTP 0x6300
+
+#define S_T_RTP    0
+#define M_T_RTP    0x7U
+#define V_T_RTP(x) ((x) << S_T_RTP)
+#define G_T_RTP(x) (((x) >> S_T_RTP) & M_T_RTP)
+
+#define A_MC_PCTL_TWR 0x6304
+
+#define S_T_WR    0
+#define M_T_WR    0x7U
+#define V_T_WR(x) ((x) << S_T_WR)
+#define G_T_WR(x) (((x) >> S_T_WR) & M_T_WR)
+
+#define A_MC_PCTL_TWTR 0x6308
+
+#define S_T_WTR    0
+#define M_T_WTR    0x7U
+#define V_T_WTR(x) ((x) << S_T_WTR)
+#define G_T_WTR(x) (((x) >> S_T_WTR) & M_T_WTR)
+
+#define A_MC_PCTL_TEXSR 0x630c
+
+#define S_T_EXSR    0
+#define M_T_EXSR    0x3ffU
+#define V_T_EXSR(x) ((x) << S_T_EXSR)
+#define G_T_EXSR(x) (((x) >> S_T_EXSR) & M_T_EXSR)
+
+#define A_MC_PCTL_TXP 0x6310
+
+#define S_T_XP    0
+#define M_T_XP    0x7U
+#define V_T_XP(x) ((x) << S_T_XP)
+#define G_T_XP(x) (((x) >> S_T_XP) & M_T_XP)
+
+#define A_MC_PCTL_TXPDLL 0x6314
+
+#define S_T_XPDLL    0
+#define M_T_XPDLL    0x3fU
+#define V_T_XPDLL(x) ((x) << S_T_XPDLL)
+#define G_T_XPDLL(x) (((x) >> S_T_XPDLL) & M_T_XPDLL)
+
+#define A_MC_PCTL_TZQCS 0x6318
+
+#define S_T_ZQCS    0
+#define M_T_ZQCS    0x7fU
+#define V_T_ZQCS(x) ((x) << S_T_ZQCS)
+#define G_T_ZQCS(x) (((x) >> S_T_ZQCS) & M_T_ZQCS)
+
+#define A_MC_PCTL_TZQCSI 0x631c
+
+#define S_T_ZQCSI    0
+#define M_T_ZQCSI    0xfffU
+#define V_T_ZQCSI(x) ((x) << S_T_ZQCSI)
+#define G_T_ZQCSI(x) (((x) >> S_T_ZQCSI) & M_T_ZQCSI)
+
+#define A_MC_PCTL_TDQS 0x6320
+
+#define S_T_DQS    0
+#define M_T_DQS    0x7U
+#define V_T_DQS(x) ((x) << S_T_DQS)
+#define G_T_DQS(x) (((x) >> S_T_DQS) & M_T_DQS)
+
+#define A_MC_PCTL_TCKSRE 0x6324
+
+#define S_T_CKSRE    0
+#define M_T_CKSRE    0xfU
+#define V_T_CKSRE(x) ((x) << S_T_CKSRE)
+#define G_T_CKSRE(x) (((x) >> S_T_CKSRE) & M_T_CKSRE)
+
+#define A_MC_PCTL_TCKSRX 0x6328
+
+#define S_T_CKSRX    0
+#define M_T_CKSRX    0xfU
+#define V_T_CKSRX(x) ((x) << S_T_CKSRX)
+#define G_T_CKSRX(x) (((x) >> S_T_CKSRX) & M_T_CKSRX)
+
+#define A_MC_PCTL_TCKE 0x632c
+
+#define S_T_CKE    0
+#define M_T_CKE    0x7U
+#define V_T_CKE(x) ((x) << S_T_CKE)
+#define G_T_CKE(x) (((x) >> S_T_CKE) & M_T_CKE)
+
+#define A_MC_PCTL_TMOD 0x6330
+
+#define S_T_MOD    0
+#define M_T_MOD    0xfU
+#define V_T_MOD(x) ((x) << S_T_MOD)
+#define G_T_MOD(x) (((x) >> S_T_MOD) & M_T_MOD)
+
+#define A_MC_PCTL_TRSTL 0x6334
+
+#define S_RSTHOLD    0
+#define M_RSTHOLD    0x7fU
+#define V_RSTHOLD(x) ((x) << S_RSTHOLD)
+#define G_RSTHOLD(x) (((x) >> S_RSTHOLD) & M_RSTHOLD)
+
+#define A_MC_PCTL_TZQCL 0x6338
+
+#define S_T_ZQCL    0
+#define M_T_ZQCL    0x3ffU
+#define V_T_ZQCL(x) ((x) << S_T_ZQCL)
+#define G_T_ZQCL(x) (((x) >> S_T_ZQCL) & M_T_ZQCL)
+
+#define A_MC_PCTL_DWLCFG0 0x6370
+
+#define S_T_ADWL_VEC    0
+#define M_T_ADWL_VEC    0x1ffU
+#define V_T_ADWL_VEC(x) ((x) << S_T_ADWL_VEC)
+#define G_T_ADWL_VEC(x) (((x) >> S_T_ADWL_VEC) & M_T_ADWL_VEC)
+
+#define A_MC_PCTL_DWLCFG1 0x6374
+#define A_MC_PCTL_DWLCFG2 0x6378
+#define A_MC_PCTL_DWLCFG3 0x637c
+#define A_MC_PCTL_ECCCFG 0x6380
+
+#define S_INLINE_SYN_EN    4
+#define V_INLINE_SYN_EN(x) ((x) << S_INLINE_SYN_EN)
+#define F_INLINE_SYN_EN    V_INLINE_SYN_EN(1U)
+
+#define S_ECC_EN    3
+#define V_ECC_EN(x) ((x) << S_ECC_EN)
+#define F_ECC_EN    V_ECC_EN(1U)
+
+#define S_ECC_INTR_EN    2
+#define V_ECC_INTR_EN(x) ((x) << S_ECC_INTR_EN)
+#define F_ECC_INTR_EN    V_ECC_INTR_EN(1U)
+
+#define A_MC_PCTL_ECCTST 0x6384
+
+#define S_ECC_TEST_MASK    0
+#define M_ECC_TEST_MASK    0xffU
+#define V_ECC_TEST_MASK(x) ((x) << S_ECC_TEST_MASK)
+#define G_ECC_TEST_MASK(x) (((x) >> S_ECC_TEST_MASK) & M_ECC_TEST_MASK)
+
+#define A_MC_PCTL_ECCCLR 0x6388
+
+#define S_CLR_ECC_LOG    1
+#define V_CLR_ECC_LOG(x) ((x) << S_CLR_ECC_LOG)
+#define F_CLR_ECC_LOG    V_CLR_ECC_LOG(1U)
+
+#define S_CLR_ECC_INTR    0
+#define V_CLR_ECC_INTR(x) ((x) << S_CLR_ECC_INTR)
+#define F_CLR_ECC_INTR    V_CLR_ECC_INTR(1U)
+
+#define A_MC_PCTL_ECCLOG 0x638c
+#define A_MC_PCTL_DTUWACTL 0x6400
+
+#define S_DTU_WR_RANK    30
+#define M_DTU_WR_RANK    0x3U
+#define V_DTU_WR_RANK(x) ((x) << S_DTU_WR_RANK)
+#define G_DTU_WR_RANK(x) (((x) >> S_DTU_WR_RANK) & M_DTU_WR_RANK)
+
+#define S_DTU_WR_ROW    13
+#define M_DTU_WR_ROW    0x1ffffU
+#define V_DTU_WR_ROW(x) ((x) << S_DTU_WR_ROW)
+#define G_DTU_WR_ROW(x) (((x) >> S_DTU_WR_ROW) & M_DTU_WR_ROW)
+
+#define S_DTU_WR_BANK    10
+#define M_DTU_WR_BANK    0x7U
+#define V_DTU_WR_BANK(x) ((x) << S_DTU_WR_BANK)
+#define G_DTU_WR_BANK(x) (((x) >> S_DTU_WR_BANK) & M_DTU_WR_BANK)
+
+#define S_DTU_WR_COL    0
+#define M_DTU_WR_COL    0x3ffU
+#define V_DTU_WR_COL(x) ((x) << S_DTU_WR_COL)
+#define G_DTU_WR_COL(x) (((x) >> S_DTU_WR_COL) & M_DTU_WR_COL)
+
+#define A_MC_PCTL_DTURACTL 0x6404
+
+#define S_DTU_RD_RANK    30
+#define M_DTU_RD_RANK    0x3U
+#define V_DTU_RD_RANK(x) ((x) << S_DTU_RD_RANK)
+#define G_DTU_RD_RANK(x) (((x) >> S_DTU_RD_RANK) & M_DTU_RD_RANK)
+
+#define S_DTU_RD_ROW    13
+#define M_DTU_RD_ROW    0x1ffffU
+#define V_DTU_RD_ROW(x) ((x) << S_DTU_RD_ROW)
+#define G_DTU_RD_ROW(x) (((x) >> S_DTU_RD_ROW) & M_DTU_RD_ROW)
+
+#define S_DTU_RD_BANK    10
+#define M_DTU_RD_BANK    0x7U
+#define V_DTU_RD_BANK(x) ((x) << S_DTU_RD_BANK)
+#define G_DTU_RD_BANK(x) (((x) >> S_DTU_RD_BANK) & M_DTU_RD_BANK)
+
+#define S_DTU_RD_COL    0
+#define M_DTU_RD_COL    0x3ffU
+#define V_DTU_RD_COL(x) ((x) << S_DTU_RD_COL)
+#define G_DTU_RD_COL(x) (((x) >> S_DTU_RD_COL) & M_DTU_RD_COL)
+
+#define A_MC_PCTL_DTUCFG 0x6408
+
+#define S_DTU_ROW_INCREMENTS    16
+#define M_DTU_ROW_INCREMENTS    0x7fU
+#define V_DTU_ROW_INCREMENTS(x) ((x) << S_DTU_ROW_INCREMENTS)
+#define G_DTU_ROW_INCREMENTS(x) (((x) >> S_DTU_ROW_INCREMENTS) & M_DTU_ROW_INCREMENTS)
+
+#define S_DTU_WR_MULTI_RD    15
+#define V_DTU_WR_MULTI_RD(x) ((x) << S_DTU_WR_MULTI_RD)
+#define F_DTU_WR_MULTI_RD    V_DTU_WR_MULTI_RD(1U)
+
+#define S_DTU_DATA_MASK_EN    14
+#define V_DTU_DATA_MASK_EN(x) ((x) << S_DTU_DATA_MASK_EN)
+#define F_DTU_DATA_MASK_EN    V_DTU_DATA_MASK_EN(1U)
+
+#define S_DTU_TARGET_LANE    10
+#define M_DTU_TARGET_LANE    0xfU
+#define V_DTU_TARGET_LANE(x) ((x) << S_DTU_TARGET_LANE)
+#define G_DTU_TARGET_LANE(x) (((x) >> S_DTU_TARGET_LANE) & M_DTU_TARGET_LANE)
+
+#define S_DTU_GENERATE_RANDOM    9
+#define V_DTU_GENERATE_RANDOM(x) ((x) << S_DTU_GENERATE_RANDOM)
+#define F_DTU_GENERATE_RANDOM    V_DTU_GENERATE_RANDOM(1U)
+
+#define S_DTU_INCR_BANKS    8
+#define V_DTU_INCR_BANKS(x) ((x) << S_DTU_INCR_BANKS)
+#define F_DTU_INCR_BANKS    V_DTU_INCR_BANKS(1U)
+
+#define S_DTU_INCR_COLS    7
+#define V_DTU_INCR_COLS(x) ((x) << S_DTU_INCR_COLS)
+#define F_DTU_INCR_COLS    V_DTU_INCR_COLS(1U)
+
+#define S_DTU_NALEN    1
+#define M_DTU_NALEN    0x3fU
+#define V_DTU_NALEN(x) ((x) << S_DTU_NALEN)
+#define G_DTU_NALEN(x) (((x) >> S_DTU_NALEN) & M_DTU_NALEN)
+
+#define S_DTU_ENABLE    0
+#define V_DTU_ENABLE(x) ((x) << S_DTU_ENABLE)
+#define F_DTU_ENABLE    V_DTU_ENABLE(1U)
+
+#define A_MC_PCTL_DTUECTL 0x640c
+
+#define S_WR_MULTI_RD_RST    2
+#define V_WR_MULTI_RD_RST(x) ((x) << S_WR_MULTI_RD_RST)
+#define F_WR_MULTI_RD_RST    V_WR_MULTI_RD_RST(1U)
+
+#define S_RUN_ERROR_REPORTS    1
+#define V_RUN_ERROR_REPORTS(x) ((x) << S_RUN_ERROR_REPORTS)
+#define F_RUN_ERROR_REPORTS    V_RUN_ERROR_REPORTS(1U)
+
+#define S_RUN_DTU    0
+#define V_RUN_DTU(x) ((x) << S_RUN_DTU)
+#define F_RUN_DTU    V_RUN_DTU(1U)
+
+#define A_MC_PCTL_DTUWD0 0x6410
+
+#define S_DTU_WR_BYTE3    24
+#define M_DTU_WR_BYTE3    0xffU
+#define V_DTU_WR_BYTE3(x) ((x) << S_DTU_WR_BYTE3)
+#define G_DTU_WR_BYTE3(x) (((x) >> S_DTU_WR_BYTE3) & M_DTU_WR_BYTE3)
+
+#define S_DTU_WR_BYTE2    16
+#define M_DTU_WR_BYTE2    0xffU
+#define V_DTU_WR_BYTE2(x) ((x) << S_DTU_WR_BYTE2)
+#define G_DTU_WR_BYTE2(x) (((x) >> S_DTU_WR_BYTE2) & M_DTU_WR_BYTE2)
+
+#define S_DTU_WR_BYTE1    8
+#define M_DTU_WR_BYTE1    0xffU
+#define V_DTU_WR_BYTE1(x) ((x) << S_DTU_WR_BYTE1)
+#define G_DTU_WR_BYTE1(x) (((x) >> S_DTU_WR_BYTE1) & M_DTU_WR_BYTE1)
+
+#define S_DTU_WR_BYTE0    0
+#define M_DTU_WR_BYTE0    0xffU
+#define V_DTU_WR_BYTE0(x) ((x) << S_DTU_WR_BYTE0)
+#define G_DTU_WR_BYTE0(x) (((x) >> S_DTU_WR_BYTE0) & M_DTU_WR_BYTE0)
+
+#define A_MC_PCTL_DTUWD1 0x6414
+
+#define S_DTU_WR_BYTE7    24
+#define M_DTU_WR_BYTE7    0xffU
+#define V_DTU_WR_BYTE7(x) ((x) << S_DTU_WR_BYTE7)
+#define G_DTU_WR_BYTE7(x) (((x) >> S_DTU_WR_BYTE7) & M_DTU_WR_BYTE7)
+
+#define S_DTU_WR_BYTE6    16
+#define M_DTU_WR_BYTE6    0xffU
+#define V_DTU_WR_BYTE6(x) ((x) << S_DTU_WR_BYTE6)
+#define G_DTU_WR_BYTE6(x) (((x) >> S_DTU_WR_BYTE6) & M_DTU_WR_BYTE6)
+
+#define S_DTU_WR_BYTE5    8
+#define M_DTU_WR_BYTE5    0xffU
+#define V_DTU_WR_BYTE5(x) ((x) << S_DTU_WR_BYTE5)
+#define G_DTU_WR_BYTE5(x) (((x) >> S_DTU_WR_BYTE5) & M_DTU_WR_BYTE5)
+
+#define S_DTU_WR_BYTE4    0
+#define M_DTU_WR_BYTE4    0xffU
+#define V_DTU_WR_BYTE4(x) ((x) << S_DTU_WR_BYTE4)
+#define G_DTU_WR_BYTE4(x) (((x) >> S_DTU_WR_BYTE4) & M_DTU_WR_BYTE4)
+
+#define A_MC_PCTL_DTUWD2 0x6418
+
+#define S_DTU_WR_BYTE11    24
+#define M_DTU_WR_BYTE11    0xffU
+#define V_DTU_WR_BYTE11(x) ((x) << S_DTU_WR_BYTE11)
+#define G_DTU_WR_BYTE11(x) (((x) >> S_DTU_WR_BYTE11) & M_DTU_WR_BYTE11)
+
+#define S_DTU_WR_BYTE10    16
+#define M_DTU_WR_BYTE10    0xffU
+#define V_DTU_WR_BYTE10(x) ((x) << S_DTU_WR_BYTE10)
+#define G_DTU_WR_BYTE10(x) (((x) >> S_DTU_WR_BYTE10) & M_DTU_WR_BYTE10)
+
+#define S_DTU_WR_BYTE9    8
+#define M_DTU_WR_BYTE9    0xffU
+#define V_DTU_WR_BYTE9(x) ((x) << S_DTU_WR_BYTE9)
+#define G_DTU_WR_BYTE9(x) (((x) >> S_DTU_WR_BYTE9) & M_DTU_WR_BYTE9)
+
+#define S_DTU_WR_BYTE8    0
+#define M_DTU_WR_BYTE8    0xffU
+#define V_DTU_WR_BYTE8(x) ((x) << S_DTU_WR_BYTE8)
+#define G_DTU_WR_BYTE8(x) (((x) >> S_DTU_WR_BYTE8) & M_DTU_WR_BYTE8)
+
+#define A_MC_PCTL_DTUWD3 0x641c
+
+#define S_DTU_WR_BYTE15    24
+#define M_DTU_WR_BYTE15    0xffU
+#define V_DTU_WR_BYTE15(x) ((x) << S_DTU_WR_BYTE15)
+#define G_DTU_WR_BYTE15(x) (((x) >> S_DTU_WR_BYTE15) & M_DTU_WR_BYTE15)
+
+#define S_DTU_WR_BYTE14    16
+#define M_DTU_WR_BYTE14    0xffU
+#define V_DTU_WR_BYTE14(x) ((x) << S_DTU_WR_BYTE14)
+#define G_DTU_WR_BYTE14(x) (((x) >> S_DTU_WR_BYTE14) & M_DTU_WR_BYTE14)
+
+#define S_DTU_WR_BYTE13    8
+#define M_DTU_WR_BYTE13    0xffU
+#define V_DTU_WR_BYTE13(x) ((x) << S_DTU_WR_BYTE13)
+#define G_DTU_WR_BYTE13(x) (((x) >> S_DTU_WR_BYTE13) & M_DTU_WR_BYTE13)
+
+#define S_DTU_WR_BYTE12    0
+#define M_DTU_WR_BYTE12    0xffU
+#define V_DTU_WR_BYTE12(x) ((x) << S_DTU_WR_BYTE12)
+#define G_DTU_WR_BYTE12(x) (((x) >> S_DTU_WR_BYTE12) & M_DTU_WR_BYTE12)
+
+#define A_MC_PCTL_DTUWDM 0x6420
+
+#define S_DM_WR_BYTE0    0
+#define M_DM_WR_BYTE0    0xffffU
+#define V_DM_WR_BYTE0(x) ((x) << S_DM_WR_BYTE0)
+#define G_DM_WR_BYTE0(x) (((x) >> S_DM_WR_BYTE0) & M_DM_WR_BYTE0)
+
+#define A_MC_PCTL_DTURD0 0x6424
+
+#define S_DTU_RD_BYTE3    24
+#define M_DTU_RD_BYTE3    0xffU
+#define V_DTU_RD_BYTE3(x) ((x) << S_DTU_RD_BYTE3)
+#define G_DTU_RD_BYTE3(x) (((x) >> S_DTU_RD_BYTE3) & M_DTU_RD_BYTE3)
+
+#define S_DTU_RD_BYTE2    16
+#define M_DTU_RD_BYTE2    0xffU
+#define V_DTU_RD_BYTE2(x) ((x) << S_DTU_RD_BYTE2)
+#define G_DTU_RD_BYTE2(x) (((x) >> S_DTU_RD_BYTE2) & M_DTU_RD_BYTE2)
+
+#define S_DTU_RD_BYTE1    8
+#define M_DTU_RD_BYTE1    0xffU
+#define V_DTU_RD_BYTE1(x) ((x) << S_DTU_RD_BYTE1)
+#define G_DTU_RD_BYTE1(x) (((x) >> S_DTU_RD_BYTE1) & M_DTU_RD_BYTE1)
+
+#define S_DTU_RD_BYTE0    0
+#define M_DTU_RD_BYTE0    0xffU
+#define V_DTU_RD_BYTE0(x) ((x) << S_DTU_RD_BYTE0)
+#define G_DTU_RD_BYTE0(x) (((x) >> S_DTU_RD_BYTE0) & M_DTU_RD_BYTE0)
+
+#define A_MC_PCTL_DTURD1 0x6428
+
+#define S_DTU_RD_BYTE7    24
+#define M_DTU_RD_BYTE7    0xffU
+#define V_DTU_RD_BYTE7(x) ((x) << S_DTU_RD_BYTE7)
+#define G_DTU_RD_BYTE7(x) (((x) >> S_DTU_RD_BYTE7) & M_DTU_RD_BYTE7)
+
+#define S_DTU_RD_BYTE6    16
+#define M_DTU_RD_BYTE6    0xffU
+#define V_DTU_RD_BYTE6(x) ((x) << S_DTU_RD_BYTE6)
+#define G_DTU_RD_BYTE6(x) (((x) >> S_DTU_RD_BYTE6) & M_DTU_RD_BYTE6)
+
+#define S_DTU_RD_BYTE5    8
+#define M_DTU_RD_BYTE5    0xffU
+#define V_DTU_RD_BYTE5(x) ((x) << S_DTU_RD_BYTE5)
+#define G_DTU_RD_BYTE5(x) (((x) >> S_DTU_RD_BYTE5) & M_DTU_RD_BYTE5)
+
+#define S_DTU_RD_BYTE4    0
+#define M_DTU_RD_BYTE4    0xffU
+#define V_DTU_RD_BYTE4(x) ((x) << S_DTU_RD_BYTE4)
+#define G_DTU_RD_BYTE4(x) (((x) >> S_DTU_RD_BYTE4) & M_DTU_RD_BYTE4)
+
+#define A_MC_PCTL_DTURD2 0x642c
+
+#define S_DTU_RD_BYTE11    24
+#define M_DTU_RD_BYTE11    0xffU
+#define V_DTU_RD_BYTE11(x) ((x) << S_DTU_RD_BYTE11)
+#define G_DTU_RD_BYTE11(x) (((x) >> S_DTU_RD_BYTE11) & M_DTU_RD_BYTE11)
+
+#define S_DTU_RD_BYTE10    16
+#define M_DTU_RD_BYTE10    0xffU
+#define V_DTU_RD_BYTE10(x) ((x) << S_DTU_RD_BYTE10)
+#define G_DTU_RD_BYTE10(x) (((x) >> S_DTU_RD_BYTE10) & M_DTU_RD_BYTE10)
+
+#define S_DTU_RD_BYTE9    8
+#define M_DTU_RD_BYTE9    0xffU
+#define V_DTU_RD_BYTE9(x) ((x) << S_DTU_RD_BYTE9)
+#define G_DTU_RD_BYTE9(x) (((x) >> S_DTU_RD_BYTE9) & M_DTU_RD_BYTE9)
+
+#define S_DTU_RD_BYTE8    0
+#define M_DTU_RD_BYTE8    0xffU
+#define V_DTU_RD_BYTE8(x) ((x) << S_DTU_RD_BYTE8)
+#define G_DTU_RD_BYTE8(x) (((x) >> S_DTU_RD_BYTE8) & M_DTU_RD_BYTE8)
+
+#define A_MC_PCTL_DTURD3 0x6430
+
+#define S_DTU_RD_BYTE15    24
+#define M_DTU_RD_BYTE15    0xffU
+#define V_DTU_RD_BYTE15(x) ((x) << S_DTU_RD_BYTE15)
+#define G_DTU_RD_BYTE15(x) (((x) >> S_DTU_RD_BYTE15) & M_DTU_RD_BYTE15)
+
+#define S_DTU_RD_BYTE14    16
+#define M_DTU_RD_BYTE14    0xffU
+#define V_DTU_RD_BYTE14(x) ((x) << S_DTU_RD_BYTE14)
+#define G_DTU_RD_BYTE14(x) (((x) >> S_DTU_RD_BYTE14) & M_DTU_RD_BYTE14)
+
+#define S_DTU_RD_BYTE13    8
+#define M_DTU_RD_BYTE13    0xffU
+#define V_DTU_RD_BYTE13(x) ((x) << S_DTU_RD_BYTE13)
+#define G_DTU_RD_BYTE13(x) (((x) >> S_DTU_RD_BYTE13) & M_DTU_RD_BYTE13)
+
+#define S_DTU_RD_BYTE12    0
+#define M_DTU_RD_BYTE12    0xffU
+#define V_DTU_RD_BYTE12(x) ((x) << S_DTU_RD_BYTE12)
+#define G_DTU_RD_BYTE12(x) (((x) >> S_DTU_RD_BYTE12) & M_DTU_RD_BYTE12)
+
+#define A_MC_DTULFSRWD 0x6434
+#define A_MC_PCTL_DTULFSRRD 0x6438
+#define A_MC_PCTL_DTUEAF 0x643c
+
+#define S_EA_RANK    30
+#define M_EA_RANK    0x3U
+#define V_EA_RANK(x) ((x) << S_EA_RANK)
+#define G_EA_RANK(x) (((x) >> S_EA_RANK) & M_EA_RANK)
+
+#define S_EA_ROW    13
+#define M_EA_ROW    0x1ffffU
+#define V_EA_ROW(x) ((x) << S_EA_ROW)
+#define G_EA_ROW(x) (((x) >> S_EA_ROW) & M_EA_ROW)
+
+#define S_EA_BANK    10
+#define M_EA_BANK    0x7U
+#define V_EA_BANK(x) ((x) << S_EA_BANK)
+#define G_EA_BANK(x) (((x) >> S_EA_BANK) & M_EA_BANK)
+
+#define S_EA_COLUMN    0
+#define M_EA_COLUMN    0x3ffU
+#define V_EA_COLUMN(x) ((x) << S_EA_COLUMN)
+#define G_EA_COLUMN(x) (((x) >> S_EA_COLUMN) & M_EA_COLUMN)
+
+#define A_MC_PCTL_PHYPVTCFG 0x6500
+
+#define S_PVT_UPD_REQ_EN    15
+#define V_PVT_UPD_REQ_EN(x) ((x) << S_PVT_UPD_REQ_EN)
+#define F_PVT_UPD_REQ_EN    V_PVT_UPD_REQ_EN(1U)
+
+#define S_PVT_UPD_TRIG_POL    14
+#define V_PVT_UPD_TRIG_POL(x) ((x) << S_PVT_UPD_TRIG_POL)
+#define F_PVT_UPD_TRIG_POL    V_PVT_UPD_TRIG_POL(1U)
+
+#define S_PVT_UPD_TRIG_TYPE    12
+#define V_PVT_UPD_TRIG_TYPE(x) ((x) << S_PVT_UPD_TRIG_TYPE)
+#define F_PVT_UPD_TRIG_TYPE    V_PVT_UPD_TRIG_TYPE(1U)
+
+#define S_PVT_UPD_DONE_POL    10
+#define V_PVT_UPD_DONE_POL(x) ((x) << S_PVT_UPD_DONE_POL)
+#define F_PVT_UPD_DONE_POL    V_PVT_UPD_DONE_POL(1U)
+
+#define S_PVT_UPD_DONE_TYPE    8
+#define M_PVT_UPD_DONE_TYPE    0x3U
+#define V_PVT_UPD_DONE_TYPE(x) ((x) << S_PVT_UPD_DONE_TYPE)
+#define G_PVT_UPD_DONE_TYPE(x) (((x) >> S_PVT_UPD_DONE_TYPE) & M_PVT_UPD_DONE_TYPE)
+
+#define S_PHY_UPD_REQ_EN    7
+#define V_PHY_UPD_REQ_EN(x) ((x) << S_PHY_UPD_REQ_EN)
+#define F_PHY_UPD_REQ_EN    V_PHY_UPD_REQ_EN(1U)
+
+#define S_PHY_UPD_TRIG_POL    6
+#define V_PHY_UPD_TRIG_POL(x) ((x) << S_PHY_UPD_TRIG_POL)
+#define F_PHY_UPD_TRIG_POL    V_PHY_UPD_TRIG_POL(1U)
+
+#define S_PHY_UPD_TRIG_TYPE    4
+#define V_PHY_UPD_TRIG_TYPE(x) ((x) << S_PHY_UPD_TRIG_TYPE)
+#define F_PHY_UPD_TRIG_TYPE    V_PHY_UPD_TRIG_TYPE(1U)
+
+#define S_PHY_UPD_DONE_POL    2
+#define V_PHY_UPD_DONE_POL(x) ((x) << S_PHY_UPD_DONE_POL)
+#define F_PHY_UPD_DONE_POL    V_PHY_UPD_DONE_POL(1U)
+
+#define S_PHY_UPD_DONE_TYPE    0
+#define M_PHY_UPD_DONE_TYPE    0x3U
+#define V_PHY_UPD_DONE_TYPE(x) ((x) << S_PHY_UPD_DONE_TYPE)
+#define G_PHY_UPD_DONE_TYPE(x) (((x) >> S_PHY_UPD_DONE_TYPE) & M_PHY_UPD_DONE_TYPE)
+
+#define A_MC_PCTL_PHYPVTSTAT 0x6504
+
+#define S_I_PVT_UPD_TRIG    5
+#define V_I_PVT_UPD_TRIG(x) ((x) << S_I_PVT_UPD_TRIG)
+#define F_I_PVT_UPD_TRIG    V_I_PVT_UPD_TRIG(1U)
+
+#define S_I_PVT_UPD_DONE    4
+#define V_I_PVT_UPD_DONE(x) ((x) << S_I_PVT_UPD_DONE)
+#define F_I_PVT_UPD_DONE    V_I_PVT_UPD_DONE(1U)
+
+#define S_I_PHY_UPD_TRIG    1
+#define V_I_PHY_UPD_TRIG(x) ((x) << S_I_PHY_UPD_TRIG)
+#define F_I_PHY_UPD_TRIG    V_I_PHY_UPD_TRIG(1U)
+
+#define S_I_PHY_UPD_DONE    0
+#define V_I_PHY_UPD_DONE(x) ((x) << S_I_PHY_UPD_DONE)
+#define F_I_PHY_UPD_DONE    V_I_PHY_UPD_DONE(1U)
+
+#define A_MC_PCTL_PHYTUPDON 0x6508
+
+#define S_PHY_T_UPDON    0
+#define M_PHY_T_UPDON    0xffU
+#define V_PHY_T_UPDON(x) ((x) << S_PHY_T_UPDON)
+#define G_PHY_T_UPDON(x) (((x) >> S_PHY_T_UPDON) & M_PHY_T_UPDON)
+
+#define A_MC_PCTL_PHYTUPDDLY 0x650c
+
+#define S_PHY_T_UPDDLY    0
+#define M_PHY_T_UPDDLY    0xfU
+#define V_PHY_T_UPDDLY(x) ((x) << S_PHY_T_UPDDLY)
+#define G_PHY_T_UPDDLY(x) (((x) >> S_PHY_T_UPDDLY) & M_PHY_T_UPDDLY)
+
+#define A_MC_PCTL_PVTTUPON 0x6510
+
+#define S_PVT_T_UPDON    0
+#define M_PVT_T_UPDON    0xffU
+#define V_PVT_T_UPDON(x) ((x) << S_PVT_T_UPDON)
+#define G_PVT_T_UPDON(x) (((x) >> S_PVT_T_UPDON) & M_PVT_T_UPDON)
+
+#define A_MC_PCTL_PVTTUPDDLY 0x6514
+
+#define S_PVT_T_UPDDLY    0
+#define M_PVT_T_UPDDLY    0xfU
+#define V_PVT_T_UPDDLY(x) ((x) << S_PVT_T_UPDDLY)
+#define G_PVT_T_UPDDLY(x) (((x) >> S_PVT_T_UPDDLY) & M_PVT_T_UPDDLY)
+
+#define A_MC_PCTL_PHYPVTUPDI 0x6518
+
+#define S_PHYPVT_T_UPDI    0
+#define M_PHYPVT_T_UPDI    0xffU
+#define V_PHYPVT_T_UPDI(x) ((x) << S_PHYPVT_T_UPDI)
+#define G_PHYPVT_T_UPDI(x) (((x) >> S_PHYPVT_T_UPDI) & M_PHYPVT_T_UPDI)
+
+#define A_MC_PCTL_PHYIOCRV1 0x651c
+
+#define S_BYTE_OE_CTL    16
+#define M_BYTE_OE_CTL    0x3U
+#define V_BYTE_OE_CTL(x) ((x) << S_BYTE_OE_CTL)
+#define G_BYTE_OE_CTL(x) (((x) >> S_BYTE_OE_CTL) & M_BYTE_OE_CTL)
+
+#define S_DYN_SOC_ODT_ALAT    12
+#define M_DYN_SOC_ODT_ALAT    0xfU
+#define V_DYN_SOC_ODT_ALAT(x) ((x) << S_DYN_SOC_ODT_ALAT)
+#define G_DYN_SOC_ODT_ALAT(x) (((x) >> S_DYN_SOC_ODT_ALAT) & M_DYN_SOC_ODT_ALAT)
+
+#define S_DYN_SOC_ODT_ATEN    8
+#define M_DYN_SOC_ODT_ATEN    0x3U
+#define V_DYN_SOC_ODT_ATEN(x) ((x) << S_DYN_SOC_ODT_ATEN)
+#define G_DYN_SOC_ODT_ATEN(x) (((x) >> S_DYN_SOC_ODT_ATEN) & M_DYN_SOC_ODT_ATEN)
+
+#define S_DYN_SOC_ODT    2
+#define V_DYN_SOC_ODT(x) ((x) << S_DYN_SOC_ODT)
+#define F_DYN_SOC_ODT    V_DYN_SOC_ODT(1U)
+
+#define S_SOC_ODT_EN    0
+#define V_SOC_ODT_EN(x) ((x) << S_SOC_ODT_EN)
+#define F_SOC_ODT_EN    V_SOC_ODT_EN(1U)
+
+#define A_MC_PCTL_PHYTUPDWAIT 0x6520
+
+#define S_PHY_T_UPDWAIT    0
+#define M_PHY_T_UPDWAIT    0x3fU
+#define V_PHY_T_UPDWAIT(x) ((x) << S_PHY_T_UPDWAIT)
+#define G_PHY_T_UPDWAIT(x) (((x) >> S_PHY_T_UPDWAIT) & M_PHY_T_UPDWAIT)
+
+#define A_MC_PCTL_PVTTUPDWAIT 0x6524
+
+#define S_PVT_T_UPDWAIT    0
+#define M_PVT_T_UPDWAIT    0x3fU
+#define V_PVT_T_UPDWAIT(x) ((x) << S_PVT_T_UPDWAIT)
+#define G_PVT_T_UPDWAIT(x) (((x) >> S_PVT_T_UPDWAIT) & M_PVT_T_UPDWAIT)
+
+#define A_MC_DDR3PHYAC_GCR 0x6a00
+
+#define S_WLRANK    8
+#define M_WLRANK    0x3U
+#define V_WLRANK(x) ((x) << S_WLRANK)
+#define G_WLRANK(x) (((x) >> S_WLRANK) & M_WLRANK)
+
+#define S_FDEPTH    6
+#define M_FDEPTH    0x3U
+#define V_FDEPTH(x) ((x) << S_FDEPTH)
+#define G_FDEPTH(x) (((x) >> S_FDEPTH) & M_FDEPTH)
+
+#define S_LPFDEPTH    4
+#define M_LPFDEPTH    0x3U
+#define V_LPFDEPTH(x) ((x) << S_LPFDEPTH)
+#define G_LPFDEPTH(x) (((x) >> S_LPFDEPTH) & M_LPFDEPTH)
+
+#define S_LPFEN    3
+#define V_LPFEN(x) ((x) << S_LPFEN)
+#define F_LPFEN    V_LPFEN(1U)
+
+#define S_WL    2
+#define V_WL(x) ((x) << S_WL)
+#define F_WL    V_WL(1U)
+
+#define S_CAL    1
+#define V_CAL(x) ((x) << S_CAL)
+#define F_CAL    V_CAL(1U)
+
+#define S_MDLEN    0
+#define V_MDLEN(x) ((x) << S_MDLEN)
+#define F_MDLEN    V_MDLEN(1U)
+
+#define A_MC_DDR3PHYAC_RCR0 0x6a04
+
+#define S_OCPONR    8
+#define V_OCPONR(x) ((x) << S_OCPONR)
+#define F_OCPONR    V_OCPONR(1U)
+
+#define S_OCPOND    7
+#define V_OCPOND(x) ((x) << S_OCPOND)
+#define F_OCPOND    V_OCPOND(1U)
+
+#define S_OCOEN    6
+#define V_OCOEN(x) ((x) << S_OCOEN)
+#define F_OCOEN    V_OCOEN(1U)
+
+#define S_CKEPONR    5
+#define V_CKEPONR(x) ((x) << S_CKEPONR)
+#define F_CKEPONR    V_CKEPONR(1U)
+
+#define S_CKEPOND    4
+#define V_CKEPOND(x) ((x) << S_CKEPOND)
+#define F_CKEPOND    V_CKEPOND(1U)
+
+#define S_CKEOEN    3
+#define V_CKEOEN(x) ((x) << S_CKEOEN)
+#define F_CKEOEN    V_CKEOEN(1U)
+
+#define S_CKPONR    2
+#define V_CKPONR(x) ((x) << S_CKPONR)
+#define F_CKPONR    V_CKPONR(1U)
+
+#define S_CKPOND    1
+#define V_CKPOND(x) ((x) << S_CKPOND)
+#define F_CKPOND    V_CKPOND(1U)
+
+#define S_CKOEN    0
+#define V_CKOEN(x) ((x) << S_CKOEN)
+#define F_CKOEN    V_CKOEN(1U)
+
+#define A_MC_DDR3PHYAC_ACCR 0x6a14
+
+#define S_ACPONR    8
+#define V_ACPONR(x) ((x) << S_ACPONR)
+#define F_ACPONR    V_ACPONR(1U)
+
+#define S_ACPOND    7
+#define V_ACPOND(x) ((x) << S_ACPOND)
+#define F_ACPOND    V_ACPOND(1U)
+
+#define S_ACOEN    6
+#define V_ACOEN(x) ((x) << S_ACOEN)
+#define F_ACOEN    V_ACOEN(1U)
+
+#define S_CK5PONR    5
+#define V_CK5PONR(x) ((x) << S_CK5PONR)
+#define F_CK5PONR    V_CK5PONR(1U)
+
+#define S_CK5POND    4
+#define V_CK5POND(x) ((x) << S_CK5POND)
+#define F_CK5POND    V_CK5POND(1U)
+
+#define S_CK5OEN    3
+#define V_CK5OEN(x) ((x) << S_CK5OEN)
+#define F_CK5OEN    V_CK5OEN(1U)
+
+#define S_CK4PONR    2
+#define V_CK4PONR(x) ((x) << S_CK4PONR)
+#define F_CK4PONR    V_CK4PONR(1U)
+
+#define S_CK4POND    1
+#define V_CK4POND(x) ((x) << S_CK4POND)
+#define F_CK4POND    V_CK4POND(1U)
+
+#define S_CK4OEN    0
+#define V_CK4OEN(x) ((x) << S_CK4OEN)
+#define F_CK4OEN    V_CK4OEN(1U)
+
+#define A_MC_DDR3PHYAC_GSR 0x6a18
+
+#define S_WLERR    4
+#define V_WLERR(x) ((x) << S_WLERR)
+#define F_WLERR    V_WLERR(1U)
+
+#define S_INIT    3
+#define V_INIT(x) ((x) << S_INIT)
+#define F_INIT    V_INIT(1U)
+
+#define S_ACCAL    0
+#define V_ACCAL(x) ((x) << S_ACCAL)
+#define F_ACCAL    V_ACCAL(1U)
+
+#define A_MC_DDR3PHYAC_ECSR 0x6a1c
+
+#define S_WLDEC    1
+#define V_WLDEC(x) ((x) << S_WLDEC)
+#define F_WLDEC    V_WLDEC(1U)
+
+#define S_WLINC    0
+#define V_WLINC(x) ((x) << S_WLINC)
+#define F_WLINC    V_WLINC(1U)
+
+#define A_MC_DDR3PHYAC_OCSR 0x6a20
+#define A_MC_DDR3PHYAC_MDIPR 0x6a24
+
+#define S_PRD    0
+#define M_PRD    0x3ffU
+#define V_PRD(x) ((x) << S_PRD)
+#define G_PRD(x) (((x) >> S_PRD) & M_PRD)
+
+#define A_MC_DDR3PHYAC_MDTPR 0x6a28
+#define A_MC_DDR3PHYAC_MDPPR0 0x6a2c
+#define A_MC_DDR3PHYAC_MDPPR1 0x6a30
+#define A_MC_DDR3PHYAC_PMBDR0 0x6a34
+
+#define S_DFLTDLY    0
+#define M_DFLTDLY    0x7fU
+#define V_DFLTDLY(x) ((x) << S_DFLTDLY)
+#define G_DFLTDLY(x) (((x) >> S_DFLTDLY) & M_DFLTDLY)
+
+#define A_MC_DDR3PHYAC_PMBDR1 0x6a38
+#define A_MC_DDR3PHYAC_ACR 0x6a60
+
+#define S_TSEL    9
+#define V_TSEL(x) ((x) << S_TSEL)
+#define F_TSEL    V_TSEL(1U)
+
+#define S_ISEL    7
+#define M_ISEL    0x3U
+#define V_ISEL(x) ((x) << S_ISEL)
+#define G_ISEL(x) (((x) >> S_ISEL) & M_ISEL)
+
+#define S_CALBYP    2
+#define V_CALBYP(x) ((x) << S_CALBYP)
+#define F_CALBYP    V_CALBYP(1U)
+
+#define S_SDRSELINV    1
+#define V_SDRSELINV(x) ((x) << S_SDRSELINV)
+#define F_SDRSELINV    V_SDRSELINV(1U)
+
+#define S_CKINV    0
+#define V_CKINV(x) ((x) << S_CKINV)
+#define F_CKINV    V_CKINV(1U)
+
+#define A_MC_DDR3PHYAC_PSCR 0x6a64
+
+#define S_PSCALE    0
+#define M_PSCALE    0x3ffU
+#define V_PSCALE(x) ((x) << S_PSCALE)
+#define G_PSCALE(x) (((x) >> S_PSCALE) & M_PSCALE)
+
+#define A_MC_DDR3PHYAC_PRCR 0x6a68
+
+#define S_PHYINIT    9
+#define V_PHYINIT(x) ((x) << S_PHYINIT)
+#define F_PHYINIT    V_PHYINIT(1U)
+
+#define S_PHYHRST    7
+#define V_PHYHRST(x) ((x) << S_PHYHRST)
+#define F_PHYHRST    V_PHYHRST(1U)
+
+#define S_RSTCLKS    3
+#define M_RSTCLKS    0xfU
+#define V_RSTCLKS(x) ((x) << S_RSTCLKS)
+#define G_RSTCLKS(x) (((x) >> S_RSTCLKS) & M_RSTCLKS)
+
+#define S_PLLPD    2
+#define V_PLLPD(x) ((x) << S_PLLPD)
+#define F_PLLPD    V_PLLPD(1U)
+
+#define S_PLLRST    1
+#define V_PLLRST(x) ((x) << S_PLLRST)
+#define F_PLLRST    V_PLLRST(1U)
+
+#define S_PHYRST    0
+#define V_PHYRST(x) ((x) << S_PHYRST)
+#define F_PHYRST    V_PHYRST(1U)
+
+#define A_MC_DDR3PHYAC_PLLCR0 0x6a6c
+
+#define S_RSTCXKS    4
+#define M_RSTCXKS    0x1fU
+#define V_RSTCXKS(x) ((x) << S_RSTCXKS)
+#define G_RSTCXKS(x) (((x) >> S_RSTCXKS) & M_RSTCXKS)
+
+#define S_ICPSEL    3
+#define V_ICPSEL(x) ((x) << S_ICPSEL)
+#define F_ICPSEL    V_ICPSEL(1U)
+
+#define S_TESTA    0
+#define M_TESTA    0x7U
+#define V_TESTA(x) ((x) << S_TESTA)
+#define G_TESTA(x) (((x) >> S_TESTA) & M_TESTA)
+
+#define A_MC_DDR3PHYAC_PLLCR1 0x6a70
+
+#define S_BYPASS    9
+#define V_BYPASS(x) ((x) << S_BYPASS)
+#define F_BYPASS    V_BYPASS(1U)
+
+#define S_BDIV    3
+#define M_BDIV    0x3U
+#define V_BDIV(x) ((x) << S_BDIV)
+#define G_BDIV(x) (((x) >> S_BDIV) & M_BDIV)
+
+#define S_TESTD    0
+#define M_TESTD    0x7U
+#define V_TESTD(x) ((x) << S_TESTD)
+#define G_TESTD(x) (((x) >> S_TESTD) & M_TESTD)
+
+#define A_MC_DDR3PHYAC_CLKENR 0x6a78
+
+#define S_CKCLKEN    3
+#define M_CKCLKEN    0x3fU
+#define V_CKCLKEN(x) ((x) << S_CKCLKEN)
+#define G_CKCLKEN(x) (((x) >> S_CKCLKEN) & M_CKCLKEN)
+
+#define S_HDRCLKEN    2
+#define V_HDRCLKEN(x) ((x) << S_HDRCLKEN)
+#define F_HDRCLKEN    V_HDRCLKEN(1U)
+
+#define S_SDRCLKEN    1
+#define V_SDRCLKEN(x) ((x) << S_SDRCLKEN)
+#define F_SDRCLKEN    V_SDRCLKEN(1U)
+
+#define S_DDRCLKEN    0
+#define V_DDRCLKEN(x) ((x) << S_DDRCLKEN)
+#define F_DDRCLKEN    V_DDRCLKEN(1U)
+
+#define A_MC_DDR3PHYDATX8_GCR 0x6b00
+
+#define S_PONR    6
+#define V_PONR(x) ((x) << S_PONR)
+#define F_PONR    V_PONR(1U)
+
+#define S_POND    5
+#define V_POND(x) ((x) << S_POND)
+#define F_POND    V_POND(1U)
+
+#define S_RDBDVT    4
+#define V_RDBDVT(x) ((x) << S_RDBDVT)
+#define F_RDBDVT    V_RDBDVT(1U)
+
+#define S_WDBDVT    3
+#define V_WDBDVT(x) ((x) << S_WDBDVT)
+#define F_WDBDVT    V_WDBDVT(1U)
+
+#define S_RDSDVT    2
+#define V_RDSDVT(x) ((x) << S_RDSDVT)
+#define F_RDSDVT    V_RDSDVT(1U)
+
+#define S_WDSDVT    1
+#define V_WDSDVT(x) ((x) << S_WDSDVT)
+#define F_WDSDVT    V_WDSDVT(1U)
+
+#define S_WLSDVT    0
+#define V_WLSDVT(x) ((x) << S_WLSDVT)
+#define F_WLSDVT    V_WLSDVT(1U)
+
+#define A_MC_DDR3PHYDATX8_WDSDR 0x6b04
+
+#define S_WDSDR_DLY    0
+#define M_WDSDR_DLY    0x3ffU
+#define V_WDSDR_DLY(x) ((x) << S_WDSDR_DLY)
+#define G_WDSDR_DLY(x) (((x) >> S_WDSDR_DLY) & M_WDSDR_DLY)
+
+#define A_MC_DDR3PHYDATX8_WLDPR 0x6b08
+#define A_MC_DDR3PHYDATX8_WLDR 0x6b0c
+
+#define S_WL_DLY    0
+#define M_WL_DLY    0x3ffU
+#define V_WL_DLY(x) ((x) << S_WL_DLY)
+#define G_WL_DLY(x) (((x) >> S_WL_DLY) & M_WL_DLY)
+
+#define A_MC_DDR3PHYDATX8_WDBDR0 0x6b1c
+
+#define S_DLY    0
+#define M_DLY    0x7fU
+#define V_DLY(x) ((x) << S_DLY)
+#define G_DLY(x) (((x) >> S_DLY) & M_DLY)
+
+#define A_MC_DDR3PHYDATX8_WDBDR1 0x6b20
+#define A_MC_DDR3PHYDATX8_WDBDR2 0x6b24
+#define A_MC_DDR3PHYDATX8_WDBDR3 0x6b28
+#define A_MC_DDR3PHYDATX8_WDBDR4 0x6b2c
+#define A_MC_DDR3PHYDATX8_WDBDR5 0x6b30
+#define A_MC_DDR3PHYDATX8_WDBDR6 0x6b34
+#define A_MC_DDR3PHYDATX8_WDBDR7 0x6b38
+#define A_MC_DDR3PHYDATX8_WDBDR8 0x6b3c
+#define A_MC_DDR3PHYDATX8_WDBDMR 0x6b40
+
+#define S_MAXDLY    0
+#define M_MAXDLY    0x7fU
+#define V_MAXDLY(x) ((x) << S_MAXDLY)
+#define G_MAXDLY(x) (((x) >> S_MAXDLY) & M_MAXDLY)
+
+#define A_MC_DDR3PHYDATX8_RDSDR 0x6b44
+
+#define S_RDSDR_DLY    0
+#define M_RDSDR_DLY    0x3ffU
+#define V_RDSDR_DLY(x) ((x) << S_RDSDR_DLY)
+#define G_RDSDR_DLY(x) (((x) >> S_RDSDR_DLY) & M_RDSDR_DLY)
+
+#define A_MC_DDR3PHYDATX8_RDBDR0 0x6b48
+#define A_MC_DDR3PHYDATX8_RDBDR1 0x6b4c
+#define A_MC_DDR3PHYDATX8_RDBDR2 0x6b50
+#define A_MC_DDR3PHYDATX8_RDBDR3 0x6b54
+#define A_MC_DDR3PHYDATX8_RDBDR4 0x6b58
+#define A_MC_DDR3PHYDATX8_RDBDR5 0x6b5c
+#define A_MC_DDR3PHYDATX8_RDBDR6 0x6b60
+#define A_MC_DDR3PHYDATX8_RDBDR7 0x6b64
+#define A_MC_DDR3PHYDATX8_RDBDMR 0x6b68
+#define A_MC_DDR3PHYDATX8_PMBDR0 0x6b6c
+#define A_MC_DDR3PHYDATX8_PMBDR1 0x6b70
+#define A_MC_DDR3PHYDATX8_PMBDR2 0x6b74
+#define A_MC_DDR3PHYDATX8_PMBDR3 0x6b78
+#define A_MC_DDR3PHYDATX8_WDBDPR 0x6b7c
+
+#define S_DP_DLY    0
+#define M_DP_DLY    0x1ffU
+#define V_DP_DLY(x) ((x) << S_DP_DLY)
+#define G_DP_DLY(x) (((x) >> S_DP_DLY) & M_DP_DLY)
+
+#define A_MC_DDR3PHYDATX8_RDBDPR 0x6b80
+#define A_MC_DDR3PHYDATX8_GSR 0x6b84
+
+#define S_WLDONE    3
+#define V_WLDONE(x) ((x) << S_WLDONE)
+#define F_WLDONE    V_WLDONE(1U)
+
+#define S_WLCAL    2
+#define V_WLCAL(x) ((x) << S_WLCAL)
+#define F_WLCAL    V_WLCAL(1U)
+
+#define S_READ    1
+#define V_READ(x) ((x) << S_READ)
+#define F_READ    V_READ(1U)
+
+#define S_RDQSCAL    0
+#define V_RDQSCAL(x) ((x) << S_RDQSCAL)
+#define F_RDQSCAL    V_RDQSCAL(1U)
+
+#define A_MC_DDR3PHYDATX8_ACR 0x6bf0
+
+#define S_PHYHSRST    9
+#define V_PHYHSRST(x) ((x) << S_PHYHSRST)
+#define F_PHYHSRST    V_PHYHSRST(1U)
+
+#define S_WLSTEP    8
+#define V_WLSTEP(x) ((x) << S_WLSTEP)
+#define F_WLSTEP    V_WLSTEP(1U)
+
+#define S_SDR_SEL_INV    2
+#define V_SDR_SEL_INV(x) ((x) << S_SDR_SEL_INV)
+#define F_SDR_SEL_INV    V_SDR_SEL_INV(1U)
+
+#define S_DDRSELINV    1
+#define V_DDRSELINV(x) ((x) << S_DDRSELINV)
+#define F_DDRSELINV    V_DDRSELINV(1U)
+
+#define S_DSINV    0
+#define V_DSINV(x) ((x) << S_DSINV)
+#define F_DSINV    V_DSINV(1U)
+
+#define A_MC_DDR3PHYDATX8_RSR 0x6bf4
+
+#define S_WLRANKSEL    9
+#define V_WLRANKSEL(x) ((x) << S_WLRANKSEL)
+#define F_WLRANKSEL    V_WLRANKSEL(1U)
+
+#define S_RANK    0
+#define M_RANK    0x3U
+#define V_RANK(x) ((x) << S_RANK)
+#define G_RANK(x) (((x) >> S_RANK) & M_RANK)
+
+#define A_MC_DDR3PHYDATX8_CLKENR 0x6bf8
+
+#define S_DTOSEL    8
+#define M_DTOSEL    0x3U
+#define V_DTOSEL(x) ((x) << S_DTOSEL)
+#define G_DTOSEL(x) (((x) >> S_DTOSEL) & M_DTOSEL)
+
+#define A_MC_PVT_REG_CALIBRATE_CTL 0x7400
+#define A_MC_PVT_REG_UPDATE_CTL 0x7404
+#define A_MC_PVT_REG_LAST_MEASUREMENT 0x7408
+#define A_MC_PVT_REG_DRVN 0x740c
+#define A_MC_PVT_REG_DRVP 0x7410
+#define A_MC_PVT_REG_TERMN 0x7414
+#define A_MC_PVT_REG_TERMP 0x7418
+#define A_MC_PVT_REG_THRESHOLD 0x741c
+#define A_MC_PVT_REG_IN_TERMP 0x7420
+#define A_MC_PVT_REG_IN_TERMN 0x7424
+#define A_MC_PVT_REG_IN_DRVP 0x7428
+#define A_MC_PVT_REG_IN_DRVN 0x742c
+#define A_MC_PVT_REG_OUT_TERMP 0x7430
+#define A_MC_PVT_REG_OUT_TERMN 0x7434
+#define A_MC_PVT_REG_OUT_DRVP 0x7438
+#define A_MC_PVT_REG_OUT_DRVN 0x743c
+#define A_MC_PVT_REG_HISTORY_TERMP 0x7440
+#define A_MC_PVT_REG_HISTORY_TERMN 0x7444
+#define A_MC_PVT_REG_HISTORY_DRVP 0x7448
+#define A_MC_PVT_REG_HISTORY_DRVN 0x744c
+#define A_MC_PVT_REG_SAMPLE_WAIT_CLKS 0x7450
+#define A_MC_DDRPHY_RST_CTRL 0x7500
+
+#define S_DDRIO_ENABLE    1
+#define V_DDRIO_ENABLE(x) ((x) << S_DDRIO_ENABLE)
+#define F_DDRIO_ENABLE    V_DDRIO_ENABLE(1U)
+
+#define S_PHY_RST_N    0
+#define V_PHY_RST_N(x) ((x) << S_PHY_RST_N)
+#define F_PHY_RST_N    V_PHY_RST_N(1U)
+
+#define A_MC_PERFORMANCE_CTRL 0x7504
+
+#define S_STALL_CHK_BIT    2
+#define V_STALL_CHK_BIT(x) ((x) << S_STALL_CHK_BIT)
+#define F_STALL_CHK_BIT    V_STALL_CHK_BIT(1U)
+
+#define S_DDR3_BRC_MODE    1
+#define V_DDR3_BRC_MODE(x) ((x) << S_DDR3_BRC_MODE)
+#define F_DDR3_BRC_MODE    V_DDR3_BRC_MODE(1U)
+
+#define S_RMW_PERF_CTRL    0
+#define V_RMW_PERF_CTRL(x) ((x) << S_RMW_PERF_CTRL)
+#define F_RMW_PERF_CTRL    V_RMW_PERF_CTRL(1U)
+
+#define A_MC_ECC_CTRL 0x7508
+
+#define S_ECC_BYPASS_BIST    1
+#define V_ECC_BYPASS_BIST(x) ((x) << S_ECC_BYPASS_BIST)
+#define F_ECC_BYPASS_BIST    V_ECC_BYPASS_BIST(1U)
+
+#define S_ECC_DISABLE    0
+#define V_ECC_DISABLE(x) ((x) << S_ECC_DISABLE)
+#define F_ECC_DISABLE    V_ECC_DISABLE(1U)
+
+#define A_MC_PAR_ENABLE 0x750c
+
+#define S_ECC_UE_PAR_ENABLE    3
+#define V_ECC_UE_PAR_ENABLE(x) ((x) << S_ECC_UE_PAR_ENABLE)
+#define F_ECC_UE_PAR_ENABLE    V_ECC_UE_PAR_ENABLE(1U)
+
+#define S_ECC_CE_PAR_ENABLE    2
+#define V_ECC_CE_PAR_ENABLE(x) ((x) << S_ECC_CE_PAR_ENABLE)
+#define F_ECC_CE_PAR_ENABLE    V_ECC_CE_PAR_ENABLE(1U)
+
+#define S_PERR_REG_INT_ENABLE    1
+#define V_PERR_REG_INT_ENABLE(x) ((x) << S_PERR_REG_INT_ENABLE)
+#define F_PERR_REG_INT_ENABLE    V_PERR_REG_INT_ENABLE(1U)
+
+#define S_PERR_BLK_INT_ENABLE    0
+#define V_PERR_BLK_INT_ENABLE(x) ((x) << S_PERR_BLK_INT_ENABLE)
+#define F_PERR_BLK_INT_ENABLE    V_PERR_BLK_INT_ENABLE(1U)
+
+#define A_MC_PAR_CAUSE 0x7510
+
+#define S_ECC_UE_PAR_CAUSE    3
+#define V_ECC_UE_PAR_CAUSE(x) ((x) << S_ECC_UE_PAR_CAUSE)
+#define F_ECC_UE_PAR_CAUSE    V_ECC_UE_PAR_CAUSE(1U)
+
+#define S_ECC_CE_PAR_CAUSE    2
+#define V_ECC_CE_PAR_CAUSE(x) ((x) << S_ECC_CE_PAR_CAUSE)
+#define F_ECC_CE_PAR_CAUSE    V_ECC_CE_PAR_CAUSE(1U)
+
+#define S_FIFOR_PAR_CAUSE    1
+#define V_FIFOR_PAR_CAUSE(x) ((x) << S_FIFOR_PAR_CAUSE)
+#define F_FIFOR_PAR_CAUSE    V_FIFOR_PAR_CAUSE(1U)
+
+#define S_RDATA_FIFOR_PAR_CAUSE    0
+#define V_RDATA_FIFOR_PAR_CAUSE(x) ((x) << S_RDATA_FIFOR_PAR_CAUSE)
+#define F_RDATA_FIFOR_PAR_CAUSE    V_RDATA_FIFOR_PAR_CAUSE(1U)
+
+#define A_MC_INT_ENABLE 0x7514
+
+#define S_ECC_UE_INT_ENABLE    2
+#define V_ECC_UE_INT_ENABLE(x) ((x) << S_ECC_UE_INT_ENABLE)
+#define F_ECC_UE_INT_ENABLE    V_ECC_UE_INT_ENABLE(1U)
+
+#define S_ECC_CE_INT_ENABLE    1
+#define V_ECC_CE_INT_ENABLE(x) ((x) << S_ECC_CE_INT_ENABLE)
+#define F_ECC_CE_INT_ENABLE    V_ECC_CE_INT_ENABLE(1U)
+
+#define S_PERR_INT_ENABLE    0
+#define V_PERR_INT_ENABLE(x) ((x) << S_PERR_INT_ENABLE)
+#define F_PERR_INT_ENABLE    V_PERR_INT_ENABLE(1U)
+
+#define A_MC_INT_CAUSE 0x7518
+
+#define S_ECC_UE_INT_CAUSE    2
+#define V_ECC_UE_INT_CAUSE(x) ((x) << S_ECC_UE_INT_CAUSE)
+#define F_ECC_UE_INT_CAUSE    V_ECC_UE_INT_CAUSE(1U)
+
+#define S_ECC_CE_INT_CAUSE    1
+#define V_ECC_CE_INT_CAUSE(x) ((x) << S_ECC_CE_INT_CAUSE)
+#define F_ECC_CE_INT_CAUSE    V_ECC_CE_INT_CAUSE(1U)
+
+#define S_PERR_INT_CAUSE    0
+#define V_PERR_INT_CAUSE(x) ((x) << S_PERR_INT_CAUSE)
+#define F_PERR_INT_CAUSE    V_PERR_INT_CAUSE(1U)
+
+#define A_MC_ECC_STATUS 0x751c
+
+#define S_ECC_CECNT    16
+#define M_ECC_CECNT    0xffffU
+#define V_ECC_CECNT(x) ((x) << S_ECC_CECNT)
+#define G_ECC_CECNT(x) (((x) >> S_ECC_CECNT) & M_ECC_CECNT)
+
+#define S_ECC_UECNT    0
+#define M_ECC_UECNT    0xffffU
+#define V_ECC_UECNT(x) ((x) << S_ECC_UECNT)
+#define G_ECC_UECNT(x) (((x) >> S_ECC_UECNT) & M_ECC_UECNT)
+
+#define A_MC_PHY_CTRL 0x7520
+
+#define S_CTLPHYRR    0
+#define V_CTLPHYRR(x) ((x) << S_CTLPHYRR)
+#define F_CTLPHYRR    V_CTLPHYRR(1U)
+
+#define A_MC_STATIC_CFG_STATUS 0x7524
+
+#define S_STATIC_MODE    9
+#define V_STATIC_MODE(x) ((x) << S_STATIC_MODE)
+#define F_STATIC_MODE    V_STATIC_MODE(1U)
+
+#define S_STATIC_DEN    6
+#define M_STATIC_DEN    0x7U
+#define V_STATIC_DEN(x) ((x) << S_STATIC_DEN)
+#define G_STATIC_DEN(x) (((x) >> S_STATIC_DEN) & M_STATIC_DEN)
+
+#define S_STATIC_ORG    5
+#define V_STATIC_ORG(x) ((x) << S_STATIC_ORG)
+#define F_STATIC_ORG    V_STATIC_ORG(1U)
+
+#define S_STATIC_RKS    4
+#define V_STATIC_RKS(x) ((x) << S_STATIC_RKS)
+#define F_STATIC_RKS    V_STATIC_RKS(1U)
+
+#define S_STATIC_WIDTH    1
+#define M_STATIC_WIDTH    0x7U
+#define V_STATIC_WIDTH(x) ((x) << S_STATIC_WIDTH)
+#define G_STATIC_WIDTH(x) (((x) >> S_STATIC_WIDTH) & M_STATIC_WIDTH)
+
+#define S_STATIC_SLOW    0
+#define V_STATIC_SLOW(x) ((x) << S_STATIC_SLOW)
+#define F_STATIC_SLOW    V_STATIC_SLOW(1U)
+
+#define A_MC_CORE_PCTL_STAT 0x7528
+
+#define S_PCTL_ACCESS_STAT    0
+#define M_PCTL_ACCESS_STAT    0x7U
+#define V_PCTL_ACCESS_STAT(x) ((x) << S_PCTL_ACCESS_STAT)
+#define G_PCTL_ACCESS_STAT(x) (((x) >> S_PCTL_ACCESS_STAT) & M_PCTL_ACCESS_STAT)
+
+#define A_MC_DEBUG_CNT 0x752c
+
+#define S_WDATA_OCNT    8
+#define M_WDATA_OCNT    0x1fU
+#define V_WDATA_OCNT(x) ((x) << S_WDATA_OCNT)
+#define G_WDATA_OCNT(x) (((x) >> S_WDATA_OCNT) & M_WDATA_OCNT)
+
+#define S_RDATA_OCNT    0
+#define M_RDATA_OCNT    0x1fU
+#define V_RDATA_OCNT(x) ((x) << S_RDATA_OCNT)
+#define G_RDATA_OCNT(x) (((x) >> S_RDATA_OCNT) & M_RDATA_OCNT)
+
+#define A_MC_BONUS 0x7530
+#define A_MC_BIST_CMD 0x7600
+
+#define S_START_BIST    31
+#define V_START_BIST(x) ((x) << S_START_BIST)
+#define F_START_BIST    V_START_BIST(1U)
+
+#define S_BIST_CMD_GAP    8
+#define M_BIST_CMD_GAP    0xffU
+#define V_BIST_CMD_GAP(x) ((x) << S_BIST_CMD_GAP)
+#define G_BIST_CMD_GAP(x) (((x) >> S_BIST_CMD_GAP) & M_BIST_CMD_GAP)
+
+#define S_BIST_OPCODE    0
+#define M_BIST_OPCODE    0x3U
+#define V_BIST_OPCODE(x) ((x) << S_BIST_OPCODE)
+#define G_BIST_OPCODE(x) (((x) >> S_BIST_OPCODE) & M_BIST_OPCODE)
+
+#define A_MC_BIST_CMD_ADDR 0x7604
+#define A_MC_BIST_CMD_LEN 0x7608
+#define A_MC_BIST_DATA_PATTERN 0x760c
+
+#define S_BIST_DATA_TYPE    0
+#define M_BIST_DATA_TYPE    0xfU
+#define V_BIST_DATA_TYPE(x) ((x) << S_BIST_DATA_TYPE)
+#define G_BIST_DATA_TYPE(x) (((x) >> S_BIST_DATA_TYPE) & M_BIST_DATA_TYPE)
+
+#define A_MC_BIST_USER_WDATA0 0x7614
+#define A_MC_BIST_USER_WDATA1 0x7618
+#define A_MC_BIST_USER_WDATA2 0x761c
+
+#define S_USER_DATA2    0
+#define M_USER_DATA2    0xffU
+#define V_USER_DATA2(x) ((x) << S_USER_DATA2)
+#define G_USER_DATA2(x) (((x) >> S_USER_DATA2) & M_USER_DATA2)
+
+#define A_MC_BIST_NUM_ERR 0x7680
+#define A_MC_BIST_ERR_FIRST_ADDR 0x7684
+#define A_MC_BIST_STATUS_RDATA 0x7688
+
+/* registers for module MA */
+#define MA_BASE_ADDR 0x7700
+
+#define A_MA_CLIENT0_RD_LATENCY_THRESHOLD 0x7700
+
+#define S_THRESHOLD1    17
+#define M_THRESHOLD1    0x7fffU
+#define V_THRESHOLD1(x) ((x) << S_THRESHOLD1)
+#define G_THRESHOLD1(x) (((x) >> S_THRESHOLD1) & M_THRESHOLD1)
+
+#define S_THRESHOLD1_EN    16
+#define V_THRESHOLD1_EN(x) ((x) << S_THRESHOLD1_EN)
+#define F_THRESHOLD1_EN    V_THRESHOLD1_EN(1U)
+
+#define S_THRESHOLD0    1
+#define M_THRESHOLD0    0x7fffU
+#define V_THRESHOLD0(x) ((x) << S_THRESHOLD0)
+#define G_THRESHOLD0(x) (((x) >> S_THRESHOLD0) & M_THRESHOLD0)
+
+#define S_THRESHOLD0_EN    0
+#define V_THRESHOLD0_EN(x) ((x) << S_THRESHOLD0_EN)
+#define F_THRESHOLD0_EN    V_THRESHOLD0_EN(1U)
+
+#define A_MA_CLIENT0_WR_LATENCY_THRESHOLD 0x7704
+#define A_MA_CLIENT1_RD_LATENCY_THRESHOLD 0x7708
+#define A_MA_CLIENT1_WR_LATENCY_THRESHOLD 0x770c
+#define A_MA_CLIENT2_RD_LATENCY_THRESHOLD 0x7710
+#define A_MA_CLIENT2_WR_LATENCY_THRESHOLD 0x7714
+#define A_MA_CLIENT3_RD_LATENCY_THRESHOLD 0x7718
+#define A_MA_CLIENT3_WR_LATENCY_THRESHOLD 0x771c
+#define A_MA_CLIENT4_RD_LATENCY_THRESHOLD 0x7720
+#define A_MA_CLIENT4_WR_LATENCY_THRESHOLD 0x7724
+#define A_MA_CLIENT5_RD_LATENCY_THRESHOLD 0x7728
+#define A_MA_CLIENT5_WR_LATENCY_THRESHOLD 0x772c
+#define A_MA_CLIENT6_RD_LATENCY_THRESHOLD 0x7730
+#define A_MA_CLIENT6_WR_LATENCY_THRESHOLD 0x7734
+#define A_MA_CLIENT7_RD_LATENCY_THRESHOLD 0x7738
+#define A_MA_CLIENT7_WR_LATENCY_THRESHOLD 0x773c
+#define A_MA_CLIENT8_RD_LATENCY_THRESHOLD 0x7740
+#define A_MA_CLIENT8_WR_LATENCY_THRESHOLD 0x7744
+#define A_MA_CLIENT9_RD_LATENCY_THRESHOLD 0x7748
+#define A_MA_CLIENT9_WR_LATENCY_THRESHOLD 0x774c
+#define A_MA_CLIENT10_RD_LATENCY_THRESHOLD 0x7750
+#define A_MA_CLIENT10_WR_LATENCY_THRESHOLD 0x7754
+#define A_MA_CLIENT11_RD_LATENCY_THRESHOLD 0x7758
+#define A_MA_CLIENT11_WR_LATENCY_THRESHOLD 0x775c
+#define A_MA_CLIENT12_RD_LATENCY_THRESHOLD 0x7760
+#define A_MA_CLIENT12_WR_LATENCY_THRESHOLD 0x7764
+#define A_MA_SGE_TH0_DEBUG_CNT 0x7768
+
+#define S_DBG_READ_DATA_CNT    24
+#define M_DBG_READ_DATA_CNT    0xffU
+#define V_DBG_READ_DATA_CNT(x) ((x) << S_DBG_READ_DATA_CNT)
+#define G_DBG_READ_DATA_CNT(x) (((x) >> S_DBG_READ_DATA_CNT) & M_DBG_READ_DATA_CNT)
+
+#define S_DBG_READ_REQ_CNT    16
+#define M_DBG_READ_REQ_CNT    0xffU
+#define V_DBG_READ_REQ_CNT(x) ((x) << S_DBG_READ_REQ_CNT)
+#define G_DBG_READ_REQ_CNT(x) (((x) >> S_DBG_READ_REQ_CNT) & M_DBG_READ_REQ_CNT)
+
+#define S_DBG_WRITE_DATA_CNT    8
+#define M_DBG_WRITE_DATA_CNT    0xffU
+#define V_DBG_WRITE_DATA_CNT(x) ((x) << S_DBG_WRITE_DATA_CNT)
+#define G_DBG_WRITE_DATA_CNT(x) (((x) >> S_DBG_WRITE_DATA_CNT) & M_DBG_WRITE_DATA_CNT)
+
+#define S_DBG_WRITE_REQ_CNT    0
+#define M_DBG_WRITE_REQ_CNT    0xffU
+#define V_DBG_WRITE_REQ_CNT(x) ((x) << S_DBG_WRITE_REQ_CNT)
+#define G_DBG_WRITE_REQ_CNT(x) (((x) >> S_DBG_WRITE_REQ_CNT) & M_DBG_WRITE_REQ_CNT)
+
+#define A_MA_SGE_TH1_DEBUG_CNT 0x776c
+#define A_MA_ULPTX_DEBUG_CNT 0x7770
+#define A_MA_ULPRX_DEBUG_CNT 0x7774
+#define A_MA_ULPTXRX_DEBUG_CNT 0x7778
+#define A_MA_TP_TH0_DEBUG_CNT 0x777c
+#define A_MA_TP_TH1_DEBUG_CNT 0x7780
+#define A_MA_LE_DEBUG_CNT 0x7784
+#define A_MA_CIM_DEBUG_CNT 0x7788
+#define A_MA_PCIE_DEBUG_CNT 0x778c
+#define A_MA_PMTX_DEBUG_CNT 0x7790
+#define A_MA_PMRX_DEBUG_CNT 0x7794
+#define A_MA_HMA_DEBUG_CNT 0x7798
+#define A_MA_EDRAM0_BAR 0x77c0
+
+#define S_EDRAM0_BASE    16
+#define M_EDRAM0_BASE    0xfffU
+#define V_EDRAM0_BASE(x) ((x) << S_EDRAM0_BASE)
+#define G_EDRAM0_BASE(x) (((x) >> S_EDRAM0_BASE) & M_EDRAM0_BASE)
+
+#define S_EDRAM0_SIZE    0
+#define M_EDRAM0_SIZE    0xfffU
+#define V_EDRAM0_SIZE(x) ((x) << S_EDRAM0_SIZE)
+#define G_EDRAM0_SIZE(x) (((x) >> S_EDRAM0_SIZE) & M_EDRAM0_SIZE)
+
+#define A_MA_EDRAM1_BAR 0x77c4
+
+#define S_EDRAM1_BASE    16
+#define M_EDRAM1_BASE    0xfffU
+#define V_EDRAM1_BASE(x) ((x) << S_EDRAM1_BASE)
+#define G_EDRAM1_BASE(x) (((x) >> S_EDRAM1_BASE) & M_EDRAM1_BASE)
+
+#define S_EDRAM1_SIZE    0
+#define M_EDRAM1_SIZE    0xfffU
+#define V_EDRAM1_SIZE(x) ((x) << S_EDRAM1_SIZE)
+#define G_EDRAM1_SIZE(x) (((x) >> S_EDRAM1_SIZE) & M_EDRAM1_SIZE)
+
+#define A_MA_EXT_MEMORY_BAR 0x77c8
+
+#define S_EXT_MEM_BASE    16
+#define M_EXT_MEM_BASE    0xfffU
+#define V_EXT_MEM_BASE(x) ((x) << S_EXT_MEM_BASE)
+#define G_EXT_MEM_BASE(x) (((x) >> S_EXT_MEM_BASE) & M_EXT_MEM_BASE)
+
+#define S_EXT_MEM_SIZE    0
+#define M_EXT_MEM_SIZE    0xfffU
+#define V_EXT_MEM_SIZE(x) ((x) << S_EXT_MEM_SIZE)
+#define G_EXT_MEM_SIZE(x) (((x) >> S_EXT_MEM_SIZE) & M_EXT_MEM_SIZE)
+
+#define A_MA_HOST_MEMORY_BAR 0x77cc
+
+#define S_HMA_BASE    16
+#define M_HMA_BASE    0xfffU
+#define V_HMA_BASE(x) ((x) << S_HMA_BASE)
+#define G_HMA_BASE(x) (((x) >> S_HMA_BASE) & M_HMA_BASE)
+
+#define S_HMA_SIZE    0
+#define M_HMA_SIZE    0xfffU
+#define V_HMA_SIZE(x) ((x) << S_HMA_SIZE)
+#define G_HMA_SIZE(x) (((x) >> S_HMA_SIZE) & M_HMA_SIZE)
+
+#define A_MA_EXT_MEM_PAGE_SIZE 0x77d0
+
+#define S_BRC_MODE    2
+#define V_BRC_MODE(x) ((x) << S_BRC_MODE)
+#define F_BRC_MODE    V_BRC_MODE(1U)
+
+#define S_EXT_MEM_PAGE_SIZE    0
+#define M_EXT_MEM_PAGE_SIZE    0x3U
+#define V_EXT_MEM_PAGE_SIZE(x) ((x) << S_EXT_MEM_PAGE_SIZE)
+#define G_EXT_MEM_PAGE_SIZE(x) (((x) >> S_EXT_MEM_PAGE_SIZE) & M_EXT_MEM_PAGE_SIZE)
+
+#define A_MA_ARB_CTRL 0x77d4
+
+#define S_DIS_PAGE_HINT    1
+#define V_DIS_PAGE_HINT(x) ((x) << S_DIS_PAGE_HINT)
+#define F_DIS_PAGE_HINT    V_DIS_PAGE_HINT(1U)
+
+#define S_DIS_ADV_ARB    0
+#define V_DIS_ADV_ARB(x) ((x) << S_DIS_ADV_ARB)
+#define F_DIS_ADV_ARB    V_DIS_ADV_ARB(1U)
+
+#define A_MA_TARGET_MEM_ENABLE 0x77d8
+
+#define S_HMA_ENABLE    3
+#define V_HMA_ENABLE(x) ((x) << S_HMA_ENABLE)
+#define F_HMA_ENABLE    V_HMA_ENABLE(1U)
+
+#define S_EXT_MEM_ENABLE    2
+#define V_EXT_MEM_ENABLE(x) ((x) << S_EXT_MEM_ENABLE)
+#define F_EXT_MEM_ENABLE    V_EXT_MEM_ENABLE(1U)
+
+#define S_EDRAM1_ENABLE    1
+#define V_EDRAM1_ENABLE(x) ((x) << S_EDRAM1_ENABLE)
+#define F_EDRAM1_ENABLE    V_EDRAM1_ENABLE(1U)
+
+#define S_EDRAM0_ENABLE    0
+#define V_EDRAM0_ENABLE(x) ((x) << S_EDRAM0_ENABLE)
+#define F_EDRAM0_ENABLE    V_EDRAM0_ENABLE(1U)
+
+#define A_MA_INT_ENABLE 0x77dc
+
+#define S_MEM_PERR_INT_ENABLE    1
+#define V_MEM_PERR_INT_ENABLE(x) ((x) << S_MEM_PERR_INT_ENABLE)
+#define F_MEM_PERR_INT_ENABLE    V_MEM_PERR_INT_ENABLE(1U)
+
+#define S_MEM_WRAP_INT_ENABLE    0
+#define V_MEM_WRAP_INT_ENABLE(x) ((x) << S_MEM_WRAP_INT_ENABLE)
+#define F_MEM_WRAP_INT_ENABLE    V_MEM_WRAP_INT_ENABLE(1U)
+
+#define A_MA_INT_CAUSE 0x77e0
+
+#define S_MEM_PERR_INT_CAUSE    1
+#define V_MEM_PERR_INT_CAUSE(x) ((x) << S_MEM_PERR_INT_CAUSE)
+#define F_MEM_PERR_INT_CAUSE    V_MEM_PERR_INT_CAUSE(1U)
+
+#define S_MEM_WRAP_INT_CAUSE    0
+#define V_MEM_WRAP_INT_CAUSE(x) ((x) << S_MEM_WRAP_INT_CAUSE)
+#define F_MEM_WRAP_INT_CAUSE    V_MEM_WRAP_INT_CAUSE(1U)
+
+#define A_MA_INT_WRAP_STATUS 0x77e4
+
+#define S_MEM_WRAP_ADDRESS    4
+#define M_MEM_WRAP_ADDRESS    0xfffffffU
+#define V_MEM_WRAP_ADDRESS(x) ((x) << S_MEM_WRAP_ADDRESS)
+#define G_MEM_WRAP_ADDRESS(x) (((x) >> S_MEM_WRAP_ADDRESS) & M_MEM_WRAP_ADDRESS)
+
+#define S_MEM_WRAP_CLIENT_NUM    0
+#define M_MEM_WRAP_CLIENT_NUM    0xfU
+#define V_MEM_WRAP_CLIENT_NUM(x) ((x) << S_MEM_WRAP_CLIENT_NUM)
+#define G_MEM_WRAP_CLIENT_NUM(x) (((x) >> S_MEM_WRAP_CLIENT_NUM) & M_MEM_WRAP_CLIENT_NUM)
+
+#define A_MA_TP_THREAD1_MAPPER 0x77e8
+
+#define S_TP_THREAD1_EN    0
+#define M_TP_THREAD1_EN    0xffU
+#define V_TP_THREAD1_EN(x) ((x) << S_TP_THREAD1_EN)
+#define G_TP_THREAD1_EN(x) (((x) >> S_TP_THREAD1_EN) & M_TP_THREAD1_EN)
+
+#define A_MA_SGE_THREAD1_MAPPER 0x77ec
+
+#define S_SGE_THREAD1_EN    0
+#define M_SGE_THREAD1_EN    0xffU
+#define V_SGE_THREAD1_EN(x) ((x) << S_SGE_THREAD1_EN)
+#define G_SGE_THREAD1_EN(x) (((x) >> S_SGE_THREAD1_EN) & M_SGE_THREAD1_EN)
+
+#define A_MA_PARITY_ERROR_ENABLE 0x77f0
+
+#define S_TP_DMARBT_PAR_ERROR_EN    31
+#define V_TP_DMARBT_PAR_ERROR_EN(x) ((x) << S_TP_DMARBT_PAR_ERROR_EN)
+#define F_TP_DMARBT_PAR_ERROR_EN    V_TP_DMARBT_PAR_ERROR_EN(1U)
+
+#define S_LOGIC_FIFO_PAR_ERROR_EN    30
+#define V_LOGIC_FIFO_PAR_ERROR_EN(x) ((x) << S_LOGIC_FIFO_PAR_ERROR_EN)
+#define F_LOGIC_FIFO_PAR_ERROR_EN    V_LOGIC_FIFO_PAR_ERROR_EN(1U)
+
+#define S_ARB3_PAR_WRQUEUE_ERROR_EN    29
+#define V_ARB3_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_ARB3_PAR_WRQUEUE_ERROR_EN)
+#define F_ARB3_PAR_WRQUEUE_ERROR_EN    V_ARB3_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_ARB2_PAR_WRQUEUE_ERROR_EN    28
+#define V_ARB2_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_ARB2_PAR_WRQUEUE_ERROR_EN)
+#define F_ARB2_PAR_WRQUEUE_ERROR_EN    V_ARB2_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_ARB1_PAR_WRQUEUE_ERROR_EN    27
+#define V_ARB1_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_ARB1_PAR_WRQUEUE_ERROR_EN)
+#define F_ARB1_PAR_WRQUEUE_ERROR_EN    V_ARB1_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_ARB0_PAR_WRQUEUE_ERROR_EN    26
+#define V_ARB0_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_ARB0_PAR_WRQUEUE_ERROR_EN)
+#define F_ARB0_PAR_WRQUEUE_ERROR_EN    V_ARB0_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_ARB3_PAR_RDQUEUE_ERROR_EN    25
+#define V_ARB3_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_ARB3_PAR_RDQUEUE_ERROR_EN)
+#define F_ARB3_PAR_RDQUEUE_ERROR_EN    V_ARB3_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_ARB2_PAR_RDQUEUE_ERROR_EN    24
+#define V_ARB2_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_ARB2_PAR_RDQUEUE_ERROR_EN)
+#define F_ARB2_PAR_RDQUEUE_ERROR_EN    V_ARB2_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_ARB1_PAR_RDQUEUE_ERROR_EN    23
+#define V_ARB1_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_ARB1_PAR_RDQUEUE_ERROR_EN)
+#define F_ARB1_PAR_RDQUEUE_ERROR_EN    V_ARB1_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_ARB0_PAR_RDQUEUE_ERROR_EN    22
+#define V_ARB0_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_ARB0_PAR_RDQUEUE_ERROR_EN)
+#define F_ARB0_PAR_RDQUEUE_ERROR_EN    V_ARB0_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL10_PAR_WRQUEUE_ERROR_EN    21
+#define V_CL10_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL10_PAR_WRQUEUE_ERROR_EN)
+#define F_CL10_PAR_WRQUEUE_ERROR_EN    V_CL10_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL9_PAR_WRQUEUE_ERROR_EN    20
+#define V_CL9_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL9_PAR_WRQUEUE_ERROR_EN)
+#define F_CL9_PAR_WRQUEUE_ERROR_EN    V_CL9_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL8_PAR_WRQUEUE_ERROR_EN    19
+#define V_CL8_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL8_PAR_WRQUEUE_ERROR_EN)
+#define F_CL8_PAR_WRQUEUE_ERROR_EN    V_CL8_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL7_PAR_WRQUEUE_ERROR_EN    18
+#define V_CL7_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL7_PAR_WRQUEUE_ERROR_EN)
+#define F_CL7_PAR_WRQUEUE_ERROR_EN    V_CL7_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL6_PAR_WRQUEUE_ERROR_EN    17
+#define V_CL6_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL6_PAR_WRQUEUE_ERROR_EN)
+#define F_CL6_PAR_WRQUEUE_ERROR_EN    V_CL6_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL5_PAR_WRQUEUE_ERROR_EN    16
+#define V_CL5_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL5_PAR_WRQUEUE_ERROR_EN)
+#define F_CL5_PAR_WRQUEUE_ERROR_EN    V_CL5_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL4_PAR_WRQUEUE_ERROR_EN    15
+#define V_CL4_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL4_PAR_WRQUEUE_ERROR_EN)
+#define F_CL4_PAR_WRQUEUE_ERROR_EN    V_CL4_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL3_PAR_WRQUEUE_ERROR_EN    14
+#define V_CL3_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL3_PAR_WRQUEUE_ERROR_EN)
+#define F_CL3_PAR_WRQUEUE_ERROR_EN    V_CL3_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL2_PAR_WRQUEUE_ERROR_EN    13
+#define V_CL2_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL2_PAR_WRQUEUE_ERROR_EN)
+#define F_CL2_PAR_WRQUEUE_ERROR_EN    V_CL2_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL1_PAR_WRQUEUE_ERROR_EN    12
+#define V_CL1_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL1_PAR_WRQUEUE_ERROR_EN)
+#define F_CL1_PAR_WRQUEUE_ERROR_EN    V_CL1_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL0_PAR_WRQUEUE_ERROR_EN    11
+#define V_CL0_PAR_WRQUEUE_ERROR_EN(x) ((x) << S_CL0_PAR_WRQUEUE_ERROR_EN)
+#define F_CL0_PAR_WRQUEUE_ERROR_EN    V_CL0_PAR_WRQUEUE_ERROR_EN(1U)
+
+#define S_CL10_PAR_RDQUEUE_ERROR_EN    10
+#define V_CL10_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL10_PAR_RDQUEUE_ERROR_EN)
+#define F_CL10_PAR_RDQUEUE_ERROR_EN    V_CL10_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL9_PAR_RDQUEUE_ERROR_EN    9
+#define V_CL9_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL9_PAR_RDQUEUE_ERROR_EN)
+#define F_CL9_PAR_RDQUEUE_ERROR_EN    V_CL9_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL8_PAR_RDQUEUE_ERROR_EN    8
+#define V_CL8_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL8_PAR_RDQUEUE_ERROR_EN)
+#define F_CL8_PAR_RDQUEUE_ERROR_EN    V_CL8_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL7_PAR_RDQUEUE_ERROR_EN    7
+#define V_CL7_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL7_PAR_RDQUEUE_ERROR_EN)
+#define F_CL7_PAR_RDQUEUE_ERROR_EN    V_CL7_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL6_PAR_RDQUEUE_ERROR_EN    6
+#define V_CL6_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL6_PAR_RDQUEUE_ERROR_EN)
+#define F_CL6_PAR_RDQUEUE_ERROR_EN    V_CL6_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL5_PAR_RDQUEUE_ERROR_EN    5
+#define V_CL5_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL5_PAR_RDQUEUE_ERROR_EN)
+#define F_CL5_PAR_RDQUEUE_ERROR_EN    V_CL5_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL4_PAR_RDQUEUE_ERROR_EN    4
+#define V_CL4_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL4_PAR_RDQUEUE_ERROR_EN)
+#define F_CL4_PAR_RDQUEUE_ERROR_EN    V_CL4_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL3_PAR_RDQUEUE_ERROR_EN    3
+#define V_CL3_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL3_PAR_RDQUEUE_ERROR_EN)
+#define F_CL3_PAR_RDQUEUE_ERROR_EN    V_CL3_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL2_PAR_RDQUEUE_ERROR_EN    2
+#define V_CL2_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL2_PAR_RDQUEUE_ERROR_EN)
+#define F_CL2_PAR_RDQUEUE_ERROR_EN    V_CL2_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL1_PAR_RDQUEUE_ERROR_EN    1
+#define V_CL1_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL1_PAR_RDQUEUE_ERROR_EN)
+#define F_CL1_PAR_RDQUEUE_ERROR_EN    V_CL1_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define S_CL0_PAR_RDQUEUE_ERROR_EN    0
+#define V_CL0_PAR_RDQUEUE_ERROR_EN(x) ((x) << S_CL0_PAR_RDQUEUE_ERROR_EN)
+#define F_CL0_PAR_RDQUEUE_ERROR_EN    V_CL0_PAR_RDQUEUE_ERROR_EN(1U)
+
+#define A_MA_PARITY_ERROR_STATUS 0x77f4
+
+#define S_TP_DMARBT_PAR_ERROR    31
+#define V_TP_DMARBT_PAR_ERROR(x) ((x) << S_TP_DMARBT_PAR_ERROR)
+#define F_TP_DMARBT_PAR_ERROR    V_TP_DMARBT_PAR_ERROR(1U)
+
+#define S_LOGIC_FIFO_PAR_ERROR    30
+#define V_LOGIC_FIFO_PAR_ERROR(x) ((x) << S_LOGIC_FIFO_PAR_ERROR)
+#define F_LOGIC_FIFO_PAR_ERROR    V_LOGIC_FIFO_PAR_ERROR(1U)
+
+#define S_ARB3_PAR_WRQUEUE_ERROR    29
+#define V_ARB3_PAR_WRQUEUE_ERROR(x) ((x) << S_ARB3_PAR_WRQUEUE_ERROR)
+#define F_ARB3_PAR_WRQUEUE_ERROR    V_ARB3_PAR_WRQUEUE_ERROR(1U)
+
+#define S_ARB2_PAR_WRQUEUE_ERROR    28
+#define V_ARB2_PAR_WRQUEUE_ERROR(x) ((x) << S_ARB2_PAR_WRQUEUE_ERROR)
+#define F_ARB2_PAR_WRQUEUE_ERROR    V_ARB2_PAR_WRQUEUE_ERROR(1U)
+
+#define S_ARB1_PAR_WRQUEUE_ERROR    27
+#define V_ARB1_PAR_WRQUEUE_ERROR(x) ((x) << S_ARB1_PAR_WRQUEUE_ERROR)
+#define F_ARB1_PAR_WRQUEUE_ERROR    V_ARB1_PAR_WRQUEUE_ERROR(1U)
+
+#define S_ARB0_PAR_WRQUEUE_ERROR    26
+#define V_ARB0_PAR_WRQUEUE_ERROR(x) ((x) << S_ARB0_PAR_WRQUEUE_ERROR)
+#define F_ARB0_PAR_WRQUEUE_ERROR    V_ARB0_PAR_WRQUEUE_ERROR(1U)
+
+#define S_ARB3_PAR_RDQUEUE_ERROR    25
+#define V_ARB3_PAR_RDQUEUE_ERROR(x) ((x) << S_ARB3_PAR_RDQUEUE_ERROR)
+#define F_ARB3_PAR_RDQUEUE_ERROR    V_ARB3_PAR_RDQUEUE_ERROR(1U)
+
+#define S_ARB2_PAR_RDQUEUE_ERROR    24
+#define V_ARB2_PAR_RDQUEUE_ERROR(x) ((x) << S_ARB2_PAR_RDQUEUE_ERROR)
+#define F_ARB2_PAR_RDQUEUE_ERROR    V_ARB2_PAR_RDQUEUE_ERROR(1U)
+
+#define S_ARB1_PAR_RDQUEUE_ERROR    23
+#define V_ARB1_PAR_RDQUEUE_ERROR(x) ((x) << S_ARB1_PAR_RDQUEUE_ERROR)
+#define F_ARB1_PAR_RDQUEUE_ERROR    V_ARB1_PAR_RDQUEUE_ERROR(1U)
+
+#define S_ARB0_PAR_RDQUEUE_ERROR    22
+#define V_ARB0_PAR_RDQUEUE_ERROR(x) ((x) << S_ARB0_PAR_RDQUEUE_ERROR)
+#define F_ARB0_PAR_RDQUEUE_ERROR    V_ARB0_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL10_PAR_WRQUEUE_ERROR    21
+#define V_CL10_PAR_WRQUEUE_ERROR(x) ((x) << S_CL10_PAR_WRQUEUE_ERROR)
+#define F_CL10_PAR_WRQUEUE_ERROR    V_CL10_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL9_PAR_WRQUEUE_ERROR    20
+#define V_CL9_PAR_WRQUEUE_ERROR(x) ((x) << S_CL9_PAR_WRQUEUE_ERROR)
+#define F_CL9_PAR_WRQUEUE_ERROR    V_CL9_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL8_PAR_WRQUEUE_ERROR    19
+#define V_CL8_PAR_WRQUEUE_ERROR(x) ((x) << S_CL8_PAR_WRQUEUE_ERROR)
+#define F_CL8_PAR_WRQUEUE_ERROR    V_CL8_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL7_PAR_WRQUEUE_ERROR    18
+#define V_CL7_PAR_WRQUEUE_ERROR(x) ((x) << S_CL7_PAR_WRQUEUE_ERROR)
+#define F_CL7_PAR_WRQUEUE_ERROR    V_CL7_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL6_PAR_WRQUEUE_ERROR    17
+#define V_CL6_PAR_WRQUEUE_ERROR(x) ((x) << S_CL6_PAR_WRQUEUE_ERROR)
+#define F_CL6_PAR_WRQUEUE_ERROR    V_CL6_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL5_PAR_WRQUEUE_ERROR    16
+#define V_CL5_PAR_WRQUEUE_ERROR(x) ((x) << S_CL5_PAR_WRQUEUE_ERROR)
+#define F_CL5_PAR_WRQUEUE_ERROR    V_CL5_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL4_PAR_WRQUEUE_ERROR    15
+#define V_CL4_PAR_WRQUEUE_ERROR(x) ((x) << S_CL4_PAR_WRQUEUE_ERROR)
+#define F_CL4_PAR_WRQUEUE_ERROR    V_CL4_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL3_PAR_WRQUEUE_ERROR    14
+#define V_CL3_PAR_WRQUEUE_ERROR(x) ((x) << S_CL3_PAR_WRQUEUE_ERROR)
+#define F_CL3_PAR_WRQUEUE_ERROR    V_CL3_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL2_PAR_WRQUEUE_ERROR    13
+#define V_CL2_PAR_WRQUEUE_ERROR(x) ((x) << S_CL2_PAR_WRQUEUE_ERROR)
+#define F_CL2_PAR_WRQUEUE_ERROR    V_CL2_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL1_PAR_WRQUEUE_ERROR    12
+#define V_CL1_PAR_WRQUEUE_ERROR(x) ((x) << S_CL1_PAR_WRQUEUE_ERROR)
+#define F_CL1_PAR_WRQUEUE_ERROR    V_CL1_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL0_PAR_WRQUEUE_ERROR    11
+#define V_CL0_PAR_WRQUEUE_ERROR(x) ((x) << S_CL0_PAR_WRQUEUE_ERROR)
+#define F_CL0_PAR_WRQUEUE_ERROR    V_CL0_PAR_WRQUEUE_ERROR(1U)
+
+#define S_CL10_PAR_RDQUEUE_ERROR    10
+#define V_CL10_PAR_RDQUEUE_ERROR(x) ((x) << S_CL10_PAR_RDQUEUE_ERROR)
+#define F_CL10_PAR_RDQUEUE_ERROR    V_CL10_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL9_PAR_RDQUEUE_ERROR    9
+#define V_CL9_PAR_RDQUEUE_ERROR(x) ((x) << S_CL9_PAR_RDQUEUE_ERROR)
+#define F_CL9_PAR_RDQUEUE_ERROR    V_CL9_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL8_PAR_RDQUEUE_ERROR    8
+#define V_CL8_PAR_RDQUEUE_ERROR(x) ((x) << S_CL8_PAR_RDQUEUE_ERROR)
+#define F_CL8_PAR_RDQUEUE_ERROR    V_CL8_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL7_PAR_RDQUEUE_ERROR    7
+#define V_CL7_PAR_RDQUEUE_ERROR(x) ((x) << S_CL7_PAR_RDQUEUE_ERROR)
+#define F_CL7_PAR_RDQUEUE_ERROR    V_CL7_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL6_PAR_RDQUEUE_ERROR    6
+#define V_CL6_PAR_RDQUEUE_ERROR(x) ((x) << S_CL6_PAR_RDQUEUE_ERROR)
+#define F_CL6_PAR_RDQUEUE_ERROR    V_CL6_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL5_PAR_RDQUEUE_ERROR    5
+#define V_CL5_PAR_RDQUEUE_ERROR(x) ((x) << S_CL5_PAR_RDQUEUE_ERROR)
+#define F_CL5_PAR_RDQUEUE_ERROR    V_CL5_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL4_PAR_RDQUEUE_ERROR    4
+#define V_CL4_PAR_RDQUEUE_ERROR(x) ((x) << S_CL4_PAR_RDQUEUE_ERROR)
+#define F_CL4_PAR_RDQUEUE_ERROR    V_CL4_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL3_PAR_RDQUEUE_ERROR    3
+#define V_CL3_PAR_RDQUEUE_ERROR(x) ((x) << S_CL3_PAR_RDQUEUE_ERROR)
+#define F_CL3_PAR_RDQUEUE_ERROR    V_CL3_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL2_PAR_RDQUEUE_ERROR    2
+#define V_CL2_PAR_RDQUEUE_ERROR(x) ((x) << S_CL2_PAR_RDQUEUE_ERROR)
+#define F_CL2_PAR_RDQUEUE_ERROR    V_CL2_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL1_PAR_RDQUEUE_ERROR    1
+#define V_CL1_PAR_RDQUEUE_ERROR(x) ((x) << S_CL1_PAR_RDQUEUE_ERROR)
+#define F_CL1_PAR_RDQUEUE_ERROR    V_CL1_PAR_RDQUEUE_ERROR(1U)
+
+#define S_CL0_PAR_RDQUEUE_ERROR    0
+#define V_CL0_PAR_RDQUEUE_ERROR(x) ((x) << S_CL0_PAR_RDQUEUE_ERROR)
+#define F_CL0_PAR_RDQUEUE_ERROR    V_CL0_PAR_RDQUEUE_ERROR(1U)
+
+#define A_MA_SGE_PCIE_COHERANCY_CTRL 0x77f8
+
+#define S_BONUS_REG    6
+#define M_BONUS_REG    0x3ffffffU
+#define V_BONUS_REG(x) ((x) << S_BONUS_REG)
+#define G_BONUS_REG(x) (((x) >> S_BONUS_REG) & M_BONUS_REG)
+
+#define S_COHERANCY_CMD_TYPE    4
+#define M_COHERANCY_CMD_TYPE    0x3U
+#define V_COHERANCY_CMD_TYPE(x) ((x) << S_COHERANCY_CMD_TYPE)
+#define G_COHERANCY_CMD_TYPE(x) (((x) >> S_COHERANCY_CMD_TYPE) & M_COHERANCY_CMD_TYPE)
+
+#define S_COHERANCY_THREAD_NUM    1
+#define M_COHERANCY_THREAD_NUM    0x7U
+#define V_COHERANCY_THREAD_NUM(x) ((x) << S_COHERANCY_THREAD_NUM)
+#define G_COHERANCY_THREAD_NUM(x) (((x) >> S_COHERANCY_THREAD_NUM) & M_COHERANCY_THREAD_NUM)
+
+#define S_COHERANCY_ENABLE    0
+#define V_COHERANCY_ENABLE(x) ((x) << S_COHERANCY_ENABLE)
+#define F_COHERANCY_ENABLE    V_COHERANCY_ENABLE(1U)
+
+#define A_MA_ERROR_ENABLE 0x77fc
+
+#define S_UE_ENABLE    0
+#define V_UE_ENABLE(x) ((x) << S_UE_ENABLE)
+#define F_UE_ENABLE    V_UE_ENABLE(1U)
+
+/* registers for module EDC_0 */
+#define EDC_0_BASE_ADDR 0x7900
+
+#define A_EDC_REF 0x7900
+
+#define S_EDC_INST_NUM    18
+#define V_EDC_INST_NUM(x) ((x) << S_EDC_INST_NUM)
+#define F_EDC_INST_NUM    V_EDC_INST_NUM(1U)
+
+#define S_ENABLE_PERF    17
+#define V_ENABLE_PERF(x) ((x) << S_ENABLE_PERF)
+#define F_ENABLE_PERF    V_ENABLE_PERF(1U)
+
+#define S_ECC_BYPASS    16
+#define V_ECC_BYPASS(x) ((x) << S_ECC_BYPASS)
+#define F_ECC_BYPASS    V_ECC_BYPASS(1U)
+
+#define S_REFFREQ    0
+#define M_REFFREQ    0xffffU
+#define V_REFFREQ(x) ((x) << S_REFFREQ)
+#define G_REFFREQ(x) (((x) >> S_REFFREQ) & M_REFFREQ)
+
+#define A_EDC_BIST_CMD 0x7904
+#define A_EDC_BIST_CMD_ADDR 0x7908
+#define A_EDC_BIST_CMD_LEN 0x790c
+#define A_EDC_BIST_DATA_PATTERN 0x7910
+#define A_EDC_BIST_USER_WDATA0 0x7914
+#define A_EDC_BIST_USER_WDATA1 0x7918
+#define A_EDC_BIST_USER_WDATA2 0x791c
+#define A_EDC_BIST_NUM_ERR 0x7920
+#define A_EDC_BIST_ERR_FIRST_ADDR 0x7924
+#define A_EDC_BIST_STATUS_RDATA 0x7928
+#define A_EDC_PAR_ENABLE 0x7970
+
+#define S_ECC_UE    2
+#define V_ECC_UE(x) ((x) << S_ECC_UE)
+#define F_ECC_UE    V_ECC_UE(1U)
+
+#define S_ECC_CE    1
+#define V_ECC_CE(x) ((x) << S_ECC_CE)
+#define F_ECC_CE    V_ECC_CE(1U)
+
+#define A_EDC_INT_ENABLE 0x7974
+#define A_EDC_INT_CAUSE 0x7978
+
+#define S_ECC_UE_PAR    5
+#define V_ECC_UE_PAR(x) ((x) << S_ECC_UE_PAR)
+#define F_ECC_UE_PAR    V_ECC_UE_PAR(1U)
+
+#define S_ECC_CE_PAR    4
+#define V_ECC_CE_PAR(x) ((x) << S_ECC_CE_PAR)
+#define F_ECC_CE_PAR    V_ECC_CE_PAR(1U)
+
+#define S_PERR_PAR_CAUSE    3
+#define V_PERR_PAR_CAUSE(x) ((x) << S_PERR_PAR_CAUSE)
+#define F_PERR_PAR_CAUSE    V_PERR_PAR_CAUSE(1U)
+
+#define A_EDC_ECC_STATUS 0x797c
+
+/* registers for module EDC_1 */
+#define EDC_1_BASE_ADDR 0x7980
+
+/* registers for module HMA */
+#define HMA_BASE_ADDR 0x7a00
+
+/* registers for module CIM */
+#define CIM_BASE_ADDR 0x7b00
+
+#define A_CIM_VF_EXT_MAILBOX_CTRL 0x0
+
+#define S_VFMBGENERIC    4
+#define M_VFMBGENERIC    0xfU
+#define V_VFMBGENERIC(x) ((x) << S_VFMBGENERIC)
+#define G_VFMBGENERIC(x) (((x) >> S_VFMBGENERIC) & M_VFMBGENERIC)
+
+#define A_CIM_VF_EXT_MAILBOX_STATUS 0x4
+
+#define S_MBVFREADY    0
+#define V_MBVFREADY(x) ((x) << S_MBVFREADY)
+#define F_MBVFREADY    V_MBVFREADY(1U)
+
+#define A_CIM_PF_MAILBOX_DATA 0x240
+#define A_CIM_PF_MAILBOX_CTRL 0x280
+
+#define S_MBGENERIC    4
+#define M_MBGENERIC    0xfffffffU
+#define V_MBGENERIC(x) ((x) << S_MBGENERIC)
+#define G_MBGENERIC(x) (((x) >> S_MBGENERIC) & M_MBGENERIC)
+
+#define S_MBMSGVALID    3
+#define V_MBMSGVALID(x) ((x) << S_MBMSGVALID)
+#define F_MBMSGVALID    V_MBMSGVALID(1U)
+
+#define S_MBINTREQ    2
+#define V_MBINTREQ(x) ((x) << S_MBINTREQ)
+#define F_MBINTREQ    V_MBINTREQ(1U)
+
+#define S_MBOWNER    0
+#define M_MBOWNER    0x3U
+#define V_MBOWNER(x) ((x) << S_MBOWNER)
+#define G_MBOWNER(x) (((x) >> S_MBOWNER) & M_MBOWNER)
+
+#define A_CIM_PF_MAILBOX_ACC_STATUS 0x284
+
+#define S_MBWRBUSY    31
+#define V_MBWRBUSY(x) ((x) << S_MBWRBUSY)
+#define F_MBWRBUSY    V_MBWRBUSY(1U)
+
+#define A_CIM_PF_HOST_INT_ENABLE 0x288
+
+#define S_MBMSGRDYINTEN    19
+#define V_MBMSGRDYINTEN(x) ((x) << S_MBMSGRDYINTEN)
+#define F_MBMSGRDYINTEN    V_MBMSGRDYINTEN(1U)
+
+#define A_CIM_PF_HOST_INT_CAUSE 0x28c
+
+#define S_MBMSGRDYINT    19
+#define V_MBMSGRDYINT(x) ((x) << S_MBMSGRDYINT)
+#define F_MBMSGRDYINT    V_MBMSGRDYINT(1U)
+
+#define A_CIM_BOOT_CFG 0x7b00
+
+#define S_BOOTADDR    8
+#define M_BOOTADDR    0xffffffU
+#define V_BOOTADDR(x) ((x) << S_BOOTADDR)
+#define G_BOOTADDR(x) (((x) >> S_BOOTADDR) & M_BOOTADDR)
+
+#define S_UPGEN    2
+#define M_UPGEN    0x3fU
+#define V_UPGEN(x) ((x) << S_UPGEN)
+#define G_UPGEN(x) (((x) >> S_UPGEN) & M_UPGEN)
+
+#define S_BOOTSDRAM    1
+#define V_BOOTSDRAM(x) ((x) << S_BOOTSDRAM)
+#define F_BOOTSDRAM    V_BOOTSDRAM(1U)
+
+#define S_UPCRST    0
+#define V_UPCRST(x) ((x) << S_UPCRST)
+#define F_UPCRST    V_UPCRST(1U)
+
+#define A_CIM_FLASH_BASE_ADDR 0x7b04
+
+#define S_FLASHBASEADDR    6
+#define M_FLASHBASEADDR    0x3ffffU
+#define V_FLASHBASEADDR(x) ((x) << S_FLASHBASEADDR)
+#define G_FLASHBASEADDR(x) (((x) >> S_FLASHBASEADDR) & M_FLASHBASEADDR)
+
+#define A_CIM_FLASH_ADDR_SIZE 0x7b08
+
+#define S_FLASHADDRSIZE    4
+#define M_FLASHADDRSIZE    0xfffffU
+#define V_FLASHADDRSIZE(x) ((x) << S_FLASHADDRSIZE)
+#define G_FLASHADDRSIZE(x) (((x) >> S_FLASHADDRSIZE) & M_FLASHADDRSIZE)
+
+#define A_CIM_EEPROM_BASE_ADDR 0x7b0c
+
+#define S_EEPROMBASEADDR    6
+#define M_EEPROMBASEADDR    0x3ffffU
+#define V_EEPROMBASEADDR(x) ((x) << S_EEPROMBASEADDR)
+#define G_EEPROMBASEADDR(x) (((x) >> S_EEPROMBASEADDR) & M_EEPROMBASEADDR)
+
+#define A_CIM_EEPROM_ADDR_SIZE 0x7b10
+
+#define S_EEPROMADDRSIZE    4
+#define M_EEPROMADDRSIZE    0xfffffU
+#define V_EEPROMADDRSIZE(x) ((x) << S_EEPROMADDRSIZE)
+#define G_EEPROMADDRSIZE(x) (((x) >> S_EEPROMADDRSIZE) & M_EEPROMADDRSIZE)
+
+#define A_CIM_SDRAM_BASE_ADDR 0x7b14
+
+#define S_SDRAMBASEADDR    6
+#define M_SDRAMBASEADDR    0x3ffffffU
+#define V_SDRAMBASEADDR(x) ((x) << S_SDRAMBASEADDR)
+#define G_SDRAMBASEADDR(x) (((x) >> S_SDRAMBASEADDR) & M_SDRAMBASEADDR)
+
+#define A_CIM_SDRAM_ADDR_SIZE 0x7b18
+
+#define S_SDRAMADDRSIZE    4
+#define M_SDRAMADDRSIZE    0xfffffffU
+#define V_SDRAMADDRSIZE(x) ((x) << S_SDRAMADDRSIZE)
+#define G_SDRAMADDRSIZE(x) (((x) >> S_SDRAMADDRSIZE) & M_SDRAMADDRSIZE)
+
+#define A_CIM_EXTMEM2_BASE_ADDR 0x7b1c
+
+#define S_EXTMEM2BASEADDR    6
+#define M_EXTMEM2BASEADDR    0x3ffffffU
+#define V_EXTMEM2BASEADDR(x) ((x) << S_EXTMEM2BASEADDR)
+#define G_EXTMEM2BASEADDR(x) (((x) >> S_EXTMEM2BASEADDR) & M_EXTMEM2BASEADDR)
+
+#define A_CIM_EXTMEM2_ADDR_SIZE 0x7b20
+
+#define S_EXTMEM2ADDRSIZE    4
+#define M_EXTMEM2ADDRSIZE    0xfffffffU
+#define V_EXTMEM2ADDRSIZE(x) ((x) << S_EXTMEM2ADDRSIZE)
+#define G_EXTMEM2ADDRSIZE(x) (((x) >> S_EXTMEM2ADDRSIZE) & M_EXTMEM2ADDRSIZE)
+
+#define A_CIM_UP_SPARE_INT 0x7b24
+
+#define S_TDEBUGINT    4
+#define V_TDEBUGINT(x) ((x) << S_TDEBUGINT)
+#define F_TDEBUGINT    V_TDEBUGINT(1U)
+
+#define S_BOOTVECSEL    3
+#define V_BOOTVECSEL(x) ((x) << S_BOOTVECSEL)
+#define F_BOOTVECSEL    V_BOOTVECSEL(1U)
+
+#define S_UPSPAREINT    0
+#define M_UPSPAREINT    0x7U
+#define V_UPSPAREINT(x) ((x) << S_UPSPAREINT)
+#define G_UPSPAREINT(x) (((x) >> S_UPSPAREINT) & M_UPSPAREINT)
+
+#define A_CIM_HOST_INT_ENABLE 0x7b28
+
+#define S_TIEQOUTPARERRINTEN    20
+#define V_TIEQOUTPARERRINTEN(x) ((x) << S_TIEQOUTPARERRINTEN)
+#define F_TIEQOUTPARERRINTEN    V_TIEQOUTPARERRINTEN(1U)
+
+#define S_TIEQINPARERRINTEN    19
+#define V_TIEQINPARERRINTEN(x) ((x) << S_TIEQINPARERRINTEN)
+#define F_TIEQINPARERRINTEN    V_TIEQINPARERRINTEN(1U)
+
+#define S_MBHOSTPARERR    18
+#define V_MBHOSTPARERR(x) ((x) << S_MBHOSTPARERR)
+#define F_MBHOSTPARERR    V_MBHOSTPARERR(1U)
+
+#define S_MBUPPARERR    17
+#define V_MBUPPARERR(x) ((x) << S_MBUPPARERR)
+#define F_MBUPPARERR    V_MBUPPARERR(1U)
+
+#define S_IBQTP0PARERR    16
+#define V_IBQTP0PARERR(x) ((x) << S_IBQTP0PARERR)
+#define F_IBQTP0PARERR    V_IBQTP0PARERR(1U)
+
+#define S_IBQTP1PARERR    15
+#define V_IBQTP1PARERR(x) ((x) << S_IBQTP1PARERR)
+#define F_IBQTP1PARERR    V_IBQTP1PARERR(1U)
+
+#define S_IBQULPPARERR    14
+#define V_IBQULPPARERR(x) ((x) << S_IBQULPPARERR)
+#define F_IBQULPPARERR    V_IBQULPPARERR(1U)
+
+#define S_IBQSGELOPARERR    13
+#define V_IBQSGELOPARERR(x) ((x) << S_IBQSGELOPARERR)
+#define F_IBQSGELOPARERR    V_IBQSGELOPARERR(1U)
+
+#define S_IBQSGEHIPARERR    12
+#define V_IBQSGEHIPARERR(x) ((x) << S_IBQSGEHIPARERR)
+#define F_IBQSGEHIPARERR    V_IBQSGEHIPARERR(1U)
+
+#define S_IBQNCSIPARERR    11
+#define V_IBQNCSIPARERR(x) ((x) << S_IBQNCSIPARERR)
+#define F_IBQNCSIPARERR    V_IBQNCSIPARERR(1U)
+
+#define S_OBQULP0PARERR    10
+#define V_OBQULP0PARERR(x) ((x) << S_OBQULP0PARERR)
+#define F_OBQULP0PARERR    V_OBQULP0PARERR(1U)
+
+#define S_OBQULP1PARERR    9
+#define V_OBQULP1PARERR(x) ((x) << S_OBQULP1PARERR)
+#define F_OBQULP1PARERR    V_OBQULP1PARERR(1U)
+
+#define S_OBQULP2PARERR    8
+#define V_OBQULP2PARERR(x) ((x) << S_OBQULP2PARERR)
+#define F_OBQULP2PARERR    V_OBQULP2PARERR(1U)
+
+#define S_OBQULP3PARERR    7
+#define V_OBQULP3PARERR(x) ((x) << S_OBQULP3PARERR)
+#define F_OBQULP3PARERR    V_OBQULP3PARERR(1U)
+
+#define S_OBQSGEPARERR    6
+#define V_OBQSGEPARERR(x) ((x) << S_OBQSGEPARERR)
+#define F_OBQSGEPARERR    V_OBQSGEPARERR(1U)
+
+#define S_OBQNCSIPARERR    5
+#define V_OBQNCSIPARERR(x) ((x) << S_OBQNCSIPARERR)
+#define F_OBQNCSIPARERR    V_OBQNCSIPARERR(1U)
+
+#define S_TIMER1INTEN    3
+#define V_TIMER1INTEN(x) ((x) << S_TIMER1INTEN)
+#define F_TIMER1INTEN    V_TIMER1INTEN(1U)
+
+#define S_TIMER0INTEN    2
+#define V_TIMER0INTEN(x) ((x) << S_TIMER0INTEN)
+#define F_TIMER0INTEN    V_TIMER0INTEN(1U)
+
+#define S_PREFDROPINTEN    1
+#define V_PREFDROPINTEN(x) ((x) << S_PREFDROPINTEN)
+#define F_PREFDROPINTEN    V_PREFDROPINTEN(1U)
+
+#define A_CIM_HOST_INT_CAUSE 0x7b2c
+
+#define S_TIEQOUTPARERRINT    20
+#define V_TIEQOUTPARERRINT(x) ((x) << S_TIEQOUTPARERRINT)
+#define F_TIEQOUTPARERRINT    V_TIEQOUTPARERRINT(1U)
+
+#define S_TIEQINPARERRINT    19
+#define V_TIEQINPARERRINT(x) ((x) << S_TIEQINPARERRINT)
+#define F_TIEQINPARERRINT    V_TIEQINPARERRINT(1U)
+
+#define S_TIMER1INT    3
+#define V_TIMER1INT(x) ((x) << S_TIMER1INT)
+#define F_TIMER1INT    V_TIMER1INT(1U)
+
+#define S_TIMER0INT    2
+#define V_TIMER0INT(x) ((x) << S_TIMER0INT)
+#define F_TIMER0INT    V_TIMER0INT(1U)
+
+#define S_PREFDROPINT    1
+#define V_PREFDROPINT(x) ((x) << S_PREFDROPINT)
+#define F_PREFDROPINT    V_PREFDROPINT(1U)
+
+#define S_UPACCNONZERO    0
+#define V_UPACCNONZERO(x) ((x) << S_UPACCNONZERO)
+#define F_UPACCNONZERO    V_UPACCNONZERO(1U)
+
+#define A_CIM_HOST_UPACC_INT_ENABLE 0x7b30
+
+#define S_EEPROMWRINTEN    30
+#define V_EEPROMWRINTEN(x) ((x) << S_EEPROMWRINTEN)
+#define F_EEPROMWRINTEN    V_EEPROMWRINTEN(1U)
+
+#define S_TIMEOUTMAINTEN    29
+#define V_TIMEOUTMAINTEN(x) ((x) << S_TIMEOUTMAINTEN)
+#define F_TIMEOUTMAINTEN    V_TIMEOUTMAINTEN(1U)
+
+#define S_TIMEOUTINTEN    28
+#define V_TIMEOUTINTEN(x) ((x) << S_TIMEOUTINTEN)
+#define F_TIMEOUTINTEN    V_TIMEOUTINTEN(1U)
+
+#define S_RSPOVRLOOKUPINTEN    27
+#define V_RSPOVRLOOKUPINTEN(x) ((x) << S_RSPOVRLOOKUPINTEN)
+#define F_RSPOVRLOOKUPINTEN    V_RSPOVRLOOKUPINTEN(1U)
+
+#define S_REQOVRLOOKUPINTEN    26
+#define V_REQOVRLOOKUPINTEN(x) ((x) << S_REQOVRLOOKUPINTEN)
+#define F_REQOVRLOOKUPINTEN    V_REQOVRLOOKUPINTEN(1U)
+
+#define S_BLKWRPLINTEN    25
+#define V_BLKWRPLINTEN(x) ((x) << S_BLKWRPLINTEN)
+#define F_BLKWRPLINTEN    V_BLKWRPLINTEN(1U)
+
+#define S_BLKRDPLINTEN    24
+#define V_BLKRDPLINTEN(x) ((x) << S_BLKRDPLINTEN)
+#define F_BLKRDPLINTEN    V_BLKRDPLINTEN(1U)
+
+#define S_SGLWRPLINTEN    23
+#define V_SGLWRPLINTEN(x) ((x) << S_SGLWRPLINTEN)
+#define F_SGLWRPLINTEN    V_SGLWRPLINTEN(1U)
+
+#define S_SGLRDPLINTEN    22
+#define V_SGLRDPLINTEN(x) ((x) << S_SGLRDPLINTEN)
+#define F_SGLRDPLINTEN    V_SGLRDPLINTEN(1U)
+
+#define S_BLKWRCTLINTEN    21
+#define V_BLKWRCTLINTEN(x) ((x) << S_BLKWRCTLINTEN)
+#define F_BLKWRCTLINTEN    V_BLKWRCTLINTEN(1U)
+
+#define S_BLKRDCTLINTEN    20
+#define V_BLKRDCTLINTEN(x) ((x) << S_BLKRDCTLINTEN)
+#define F_BLKRDCTLINTEN    V_BLKRDCTLINTEN(1U)
+
+#define S_SGLWRCTLINTEN    19
+#define V_SGLWRCTLINTEN(x) ((x) << S_SGLWRCTLINTEN)
+#define F_SGLWRCTLINTEN    V_SGLWRCTLINTEN(1U)
+
+#define S_SGLRDCTLINTEN    18
+#define V_SGLRDCTLINTEN(x) ((x) << S_SGLRDCTLINTEN)
+#define F_SGLRDCTLINTEN    V_SGLRDCTLINTEN(1U)
+
+#define S_BLKWREEPROMINTEN    17
+#define V_BLKWREEPROMINTEN(x) ((x) << S_BLKWREEPROMINTEN)
+#define F_BLKWREEPROMINTEN    V_BLKWREEPROMINTEN(1U)
+
+#define S_BLKRDEEPROMINTEN    16
+#define V_BLKRDEEPROMINTEN(x) ((x) << S_BLKRDEEPROMINTEN)
+#define F_BLKRDEEPROMINTEN    V_BLKRDEEPROMINTEN(1U)
+
+#define S_SGLWREEPROMINTEN    15
+#define V_SGLWREEPROMINTEN(x) ((x) << S_SGLWREEPROMINTEN)
+#define F_SGLWREEPROMINTEN    V_SGLWREEPROMINTEN(1U)
+
+#define S_SGLRDEEPROMINTEN    14
+#define V_SGLRDEEPROMINTEN(x) ((x) << S_SGLRDEEPROMINTEN)
+#define F_SGLRDEEPROMINTEN    V_SGLRDEEPROMINTEN(1U)
+
+#define S_BLKWRFLASHINTEN    13
+#define V_BLKWRFLASHINTEN(x) ((x) << S_BLKWRFLASHINTEN)
+#define F_BLKWRFLASHINTEN    V_BLKWRFLASHINTEN(1U)
+
+#define S_BLKRDFLASHINTEN    12
+#define V_BLKRDFLASHINTEN(x) ((x) << S_BLKRDFLASHINTEN)
+#define F_BLKRDFLASHINTEN    V_BLKRDFLASHINTEN(1U)
+
+#define S_SGLWRFLASHINTEN    11
+#define V_SGLWRFLASHINTEN(x) ((x) << S_SGLWRFLASHINTEN)
+#define F_SGLWRFLASHINTEN    V_SGLWRFLASHINTEN(1U)
+
+#define S_SGLRDFLASHINTEN    10
+#define V_SGLRDFLASHINTEN(x) ((x) << S_SGLRDFLASHINTEN)
+#define F_SGLRDFLASHINTEN    V_SGLRDFLASHINTEN(1U)
+
+#define S_BLKWRBOOTINTEN    9
+#define V_BLKWRBOOTINTEN(x) ((x) << S_BLKWRBOOTINTEN)
+#define F_BLKWRBOOTINTEN    V_BLKWRBOOTINTEN(1U)
+
+#define S_BLKRDBOOTINTEN    8
+#define V_BLKRDBOOTINTEN(x) ((x) << S_BLKRDBOOTINTEN)
+#define F_BLKRDBOOTINTEN    V_BLKRDBOOTINTEN(1U)
+
+#define S_SGLWRBOOTINTEN    7
+#define V_SGLWRBOOTINTEN(x) ((x) << S_SGLWRBOOTINTEN)
+#define F_SGLWRBOOTINTEN    V_SGLWRBOOTINTEN(1U)
+
+#define S_SGLRDBOOTINTEN    6
+#define V_SGLRDBOOTINTEN(x) ((x) << S_SGLRDBOOTINTEN)
+#define F_SGLRDBOOTINTEN    V_SGLRDBOOTINTEN(1U)
+
+#define S_ILLWRBEINTEN    5
+#define V_ILLWRBEINTEN(x) ((x) << S_ILLWRBEINTEN)
+#define F_ILLWRBEINTEN    V_ILLWRBEINTEN(1U)
+
+#define S_ILLRDBEINTEN    4
+#define V_ILLRDBEINTEN(x) ((x) << S_ILLRDBEINTEN)
+#define F_ILLRDBEINTEN    V_ILLRDBEINTEN(1U)
+
+#define S_ILLRDINTEN    3
+#define V_ILLRDINTEN(x) ((x) << S_ILLRDINTEN)
+#define F_ILLRDINTEN    V_ILLRDINTEN(1U)
+
+#define S_ILLWRINTEN    2
+#define V_ILLWRINTEN(x) ((x) << S_ILLWRINTEN)
+#define F_ILLWRINTEN    V_ILLWRINTEN(1U)
+
+#define S_ILLTRANSINTEN    1
+#define V_ILLTRANSINTEN(x) ((x) << S_ILLTRANSINTEN)
+#define F_ILLTRANSINTEN    V_ILLTRANSINTEN(1U)
+
+#define S_RSVDSPACEINTEN    0
+#define V_RSVDSPACEINTEN(x) ((x) << S_RSVDSPACEINTEN)
+#define F_RSVDSPACEINTEN    V_RSVDSPACEINTEN(1U)
+
+#define A_CIM_HOST_UPACC_INT_CAUSE 0x7b34
+
+#define S_EEPROMWRINT    30
+#define V_EEPROMWRINT(x) ((x) << S_EEPROMWRINT)
+#define F_EEPROMWRINT    V_EEPROMWRINT(1U)
+
+#define S_TIMEOUTMAINT    29
+#define V_TIMEOUTMAINT(x) ((x) << S_TIMEOUTMAINT)
+#define F_TIMEOUTMAINT    V_TIMEOUTMAINT(1U)
+
+#define S_TIMEOUTINT    28
+#define V_TIMEOUTINT(x) ((x) << S_TIMEOUTINT)
+#define F_TIMEOUTINT    V_TIMEOUTINT(1U)
+
+#define S_RSPOVRLOOKUPINT    27
+#define V_RSPOVRLOOKUPINT(x) ((x) << S_RSPOVRLOOKUPINT)
+#define F_RSPOVRLOOKUPINT    V_RSPOVRLOOKUPINT(1U)
+
+#define S_REQOVRLOOKUPINT    26
+#define V_REQOVRLOOKUPINT(x) ((x) << S_REQOVRLOOKUPINT)
+#define F_REQOVRLOOKUPINT    V_REQOVRLOOKUPINT(1U)
+
+#define S_BLKWRPLINT    25
+#define V_BLKWRPLINT(x) ((x) << S_BLKWRPLINT)
+#define F_BLKWRPLINT    V_BLKWRPLINT(1U)
+
+#define S_BLKRDPLINT    24
+#define V_BLKRDPLINT(x) ((x) << S_BLKRDPLINT)
+#define F_BLKRDPLINT    V_BLKRDPLINT(1U)
+
+#define S_SGLWRPLINT    23
+#define V_SGLWRPLINT(x) ((x) << S_SGLWRPLINT)
+#define F_SGLWRPLINT    V_SGLWRPLINT(1U)
+
+#define S_SGLRDPLINT    22
+#define V_SGLRDPLINT(x) ((x) << S_SGLRDPLINT)
+#define F_SGLRDPLINT    V_SGLRDPLINT(1U)
+
+#define S_BLKWRCTLINT    21
+#define V_BLKWRCTLINT(x) ((x) << S_BLKWRCTLINT)
+#define F_BLKWRCTLINT    V_BLKWRCTLINT(1U)
+
+#define S_BLKRDCTLINT    20
+#define V_BLKRDCTLINT(x) ((x) << S_BLKRDCTLINT)
+#define F_BLKRDCTLINT    V_BLKRDCTLINT(1U)
+
+#define S_SGLWRCTLINT    19
+#define V_SGLWRCTLINT(x) ((x) << S_SGLWRCTLINT)
+#define F_SGLWRCTLINT    V_SGLWRCTLINT(1U)
+
+#define S_SGLRDCTLINT    18
+#define V_SGLRDCTLINT(x) ((x) << S_SGLRDCTLINT)
+#define F_SGLRDCTLINT    V_SGLRDCTLINT(1U)
+
+#define S_BLKWREEPROMINT    17
+#define V_BLKWREEPROMINT(x) ((x) << S_BLKWREEPROMINT)
+#define F_BLKWREEPROMINT    V_BLKWREEPROMINT(1U)
+
+#define S_BLKRDEEPROMINT    16
+#define V_BLKRDEEPROMINT(x) ((x) << S_BLKRDEEPROMINT)
+#define F_BLKRDEEPROMINT    V_BLKRDEEPROMINT(1U)
+
+#define S_SGLWREEPROMINT    15
+#define V_SGLWREEPROMINT(x) ((x) << S_SGLWREEPROMINT)
+#define F_SGLWREEPROMINT    V_SGLWREEPROMINT(1U)
+
+#define S_SGLRDEEPROMINT    14
+#define V_SGLRDEEPROMINT(x) ((x) << S_SGLRDEEPROMINT)
+#define F_SGLRDEEPROMINT    V_SGLRDEEPROMINT(1U)
+
+#define S_BLKWRFLASHINT    13
+#define V_BLKWRFLASHINT(x) ((x) << S_BLKWRFLASHINT)
+#define F_BLKWRFLASHINT    V_BLKWRFLASHINT(1U)
+
+#define S_BLKRDFLASHINT    12
+#define V_BLKRDFLASHINT(x) ((x) << S_BLKRDFLASHINT)
+#define F_BLKRDFLASHINT    V_BLKRDFLASHINT(1U)
+
+#define S_SGLWRFLASHINT    11
+#define V_SGLWRFLASHINT(x) ((x) << S_SGLWRFLASHINT)
+#define F_SGLWRFLASHINT    V_SGLWRFLASHINT(1U)
+
+#define S_SGLRDFLASHINT    10
+#define V_SGLRDFLASHINT(x) ((x) << S_SGLRDFLASHINT)
+#define F_SGLRDFLASHINT    V_SGLRDFLASHINT(1U)
+
+#define S_BLKWRBOOTINT    9
+#define V_BLKWRBOOTINT(x) ((x) << S_BLKWRBOOTINT)
+#define F_BLKWRBOOTINT    V_BLKWRBOOTINT(1U)
+
+#define S_BLKRDBOOTINT    8
+#define V_BLKRDBOOTINT(x) ((x) << S_BLKRDBOOTINT)
+#define F_BLKRDBOOTINT    V_BLKRDBOOTINT(1U)
+
+#define S_SGLWRBOOTINT    7
+#define V_SGLWRBOOTINT(x) ((x) << S_SGLWRBOOTINT)
+#define F_SGLWRBOOTINT    V_SGLWRBOOTINT(1U)
+
+#define S_SGLRDBOOTINT    6
+#define V_SGLRDBOOTINT(x) ((x) << S_SGLRDBOOTINT)
+#define F_SGLRDBOOTINT    V_SGLRDBOOTINT(1U)
+
+#define S_ILLWRBEINT    5
+#define V_ILLWRBEINT(x) ((x) << S_ILLWRBEINT)
+#define F_ILLWRBEINT    V_ILLWRBEINT(1U)
+
+#define S_ILLRDBEINT    4
+#define V_ILLRDBEINT(x) ((x) << S_ILLRDBEINT)
+#define F_ILLRDBEINT    V_ILLRDBEINT(1U)
+
+#define S_ILLRDINT    3
+#define V_ILLRDINT(x) ((x) << S_ILLRDINT)
+#define F_ILLRDINT    V_ILLRDINT(1U)
+
+#define S_ILLWRINT    2
+#define V_ILLWRINT(x) ((x) << S_ILLWRINT)
+#define F_ILLWRINT    V_ILLWRINT(1U)
+
+#define S_ILLTRANSINT    1
+#define V_ILLTRANSINT(x) ((x) << S_ILLTRANSINT)
+#define F_ILLTRANSINT    V_ILLTRANSINT(1U)
+
+#define S_RSVDSPACEINT    0
+#define V_RSVDSPACEINT(x) ((x) << S_RSVDSPACEINT)
+#define F_RSVDSPACEINT    V_RSVDSPACEINT(1U)
+
+#define A_CIM_UP_INT_ENABLE 0x7b38
+
+#define S_MSTPLINTEN    4
+#define V_MSTPLINTEN(x) ((x) << S_MSTPLINTEN)
+#define F_MSTPLINTEN    V_MSTPLINTEN(1U)
+
+#define A_CIM_UP_INT_CAUSE 0x7b3c
+
+#define S_MSTPLINT    4
+#define V_MSTPLINT(x) ((x) << S_MSTPLINT)
+#define F_MSTPLINT    V_MSTPLINT(1U)
+
+#define A_CIM_UP_ACC_INT_ENABLE 0x7b40
+#define A_CIM_UP_ACC_INT_CAUSE 0x7b44
+#define A_CIM_QUEUE_CONFIG_REF 0x7b48
+
+#define S_OBQSELECT    4
+#define V_OBQSELECT(x) ((x) << S_OBQSELECT)
+#define F_OBQSELECT    V_OBQSELECT(1U)
+
+#define S_IBQSELECT    3
+#define V_IBQSELECT(x) ((x) << S_IBQSELECT)
+#define F_IBQSELECT    V_IBQSELECT(1U)
+
+#define S_QUENUMSELECT    0
+#define M_QUENUMSELECT    0x7U
+#define V_QUENUMSELECT(x) ((x) << S_QUENUMSELECT)
+#define G_QUENUMSELECT(x) (((x) >> S_QUENUMSELECT) & M_QUENUMSELECT)
+
+#define A_CIM_QUEUE_CONFIG_CTRL 0x7b4c
+
+#define S_CIMQSIZE    24
+#define M_CIMQSIZE    0x3fU
+#define V_CIMQSIZE(x) ((x) << S_CIMQSIZE)
+#define G_CIMQSIZE(x) (((x) >> S_CIMQSIZE) & M_CIMQSIZE)
+
+#define S_CIMQBASE    16
+#define M_CIMQBASE    0x3fU
+#define V_CIMQBASE(x) ((x) << S_CIMQBASE)
+#define G_CIMQBASE(x) (((x) >> S_CIMQBASE) & M_CIMQBASE)
+
+#define S_CIMQDBG8BEN    9
+#define V_CIMQDBG8BEN(x) ((x) << S_CIMQDBG8BEN)
+#define F_CIMQDBG8BEN    V_CIMQDBG8BEN(1U)
+
+#define S_QUEFULLTHRSH    0
+#define M_QUEFULLTHRSH    0x1ffU
+#define V_QUEFULLTHRSH(x) ((x) << S_QUEFULLTHRSH)
+#define G_QUEFULLTHRSH(x) (((x) >> S_QUEFULLTHRSH) & M_QUEFULLTHRSH)
+
+#define A_CIM_HOST_ACC_CTRL 0x7b50
+
+#define S_HOSTBUSY    17
+#define V_HOSTBUSY(x) ((x) << S_HOSTBUSY)
+#define F_HOSTBUSY    V_HOSTBUSY(1U)
+
+#define S_HOSTWRITE    16
+#define V_HOSTWRITE(x) ((x) << S_HOSTWRITE)
+#define F_HOSTWRITE    V_HOSTWRITE(1U)
+
+#define S_HOSTADDR    0
+#define M_HOSTADDR    0xffffU
+#define V_HOSTADDR(x) ((x) << S_HOSTADDR)
+#define G_HOSTADDR(x) (((x) >> S_HOSTADDR) & M_HOSTADDR)
+
+#define A_CIM_HOST_ACC_DATA 0x7b54
+#define A_CIM_CDEBUGDATA 0x7b58
+
+#define S_CDEBUGDATAH    16
+#define M_CDEBUGDATAH    0xffffU
+#define V_CDEBUGDATAH(x) ((x) << S_CDEBUGDATAH)
+#define G_CDEBUGDATAH(x) (((x) >> S_CDEBUGDATAH) & M_CDEBUGDATAH)
+
+#define S_CDEBUGDATAL    0
+#define M_CDEBUGDATAL    0xffffU
+#define V_CDEBUGDATAL(x) ((x) << S_CDEBUGDATAL)
+#define G_CDEBUGDATAL(x) (((x) >> S_CDEBUGDATAL) & M_CDEBUGDATAL)
+
+#define A_CIM_IBQ_DBG_CFG 0x7b60
+
+#define S_IBQDBGADDR    16
+#define M_IBQDBGADDR    0xfffU
+#define V_IBQDBGADDR(x) ((x) << S_IBQDBGADDR)
+#define G_IBQDBGADDR(x) (((x) >> S_IBQDBGADDR) & M_IBQDBGADDR)
+
+#define S_IBQDBGWR    2
+#define V_IBQDBGWR(x) ((x) << S_IBQDBGWR)
+#define F_IBQDBGWR    V_IBQDBGWR(1U)
+
+#define S_IBQDBGBUSY    1
+#define V_IBQDBGBUSY(x) ((x) << S_IBQDBGBUSY)
+#define F_IBQDBGBUSY    V_IBQDBGBUSY(1U)
+
+#define S_IBQDBGEN    0
+#define V_IBQDBGEN(x) ((x) << S_IBQDBGEN)
+#define F_IBQDBGEN    V_IBQDBGEN(1U)
+
+#define A_CIM_OBQ_DBG_CFG 0x7b64
+
+#define S_OBQDBGADDR    16
+#define M_OBQDBGADDR    0xfffU
+#define V_OBQDBGADDR(x) ((x) << S_OBQDBGADDR)
+#define G_OBQDBGADDR(x) (((x) >> S_OBQDBGADDR) & M_OBQDBGADDR)
+
+#define S_OBQDBGWR    2
+#define V_OBQDBGWR(x) ((x) << S_OBQDBGWR)
+#define F_OBQDBGWR    V_OBQDBGWR(1U)
+
+#define S_OBQDBGBUSY    1
+#define V_OBQDBGBUSY(x) ((x) << S_OBQDBGBUSY)
+#define F_OBQDBGBUSY    V_OBQDBGBUSY(1U)
+
+#define S_OBQDBGEN    0
+#define V_OBQDBGEN(x) ((x) << S_OBQDBGEN)
+#define F_OBQDBGEN    V_OBQDBGEN(1U)
+
+#define A_CIM_IBQ_DBG_DATA 0x7b68
+#define A_CIM_OBQ_DBG_DATA 0x7b6c
+#define A_CIM_DEBUGCFG 0x7b70
+
+#define S_POLADBGRDPTR    23
+#define M_POLADBGRDPTR    0x1ffU
+#define V_POLADBGRDPTR(x) ((x) << S_POLADBGRDPTR)
+#define G_POLADBGRDPTR(x) (((x) >> S_POLADBGRDPTR) & M_POLADBGRDPTR)
+
+#define S_PILADBGRDPTR    14
+#define M_PILADBGRDPTR    0x1ffU
+#define V_PILADBGRDPTR(x) ((x) << S_PILADBGRDPTR)
+#define G_PILADBGRDPTR(x) (((x) >> S_PILADBGRDPTR) & M_PILADBGRDPTR)
+
+#define S_LAMASKTRIG    13
+#define V_LAMASKTRIG(x) ((x) << S_LAMASKTRIG)
+#define F_LAMASKTRIG    V_LAMASKTRIG(1U)
+
+#define S_LADBGEN    12
+#define V_LADBGEN(x) ((x) << S_LADBGEN)
+#define F_LADBGEN    V_LADBGEN(1U)
+
+#define S_LAFILLONCE    11
+#define V_LAFILLONCE(x) ((x) << S_LAFILLONCE)
+#define F_LAFILLONCE    V_LAFILLONCE(1U)
+
+#define S_LAMASKSTOP    10
+#define V_LAMASKSTOP(x) ((x) << S_LAMASKSTOP)
+#define F_LAMASKSTOP    V_LAMASKSTOP(1U)
+
+#define S_DEBUGSELH    5
+#define M_DEBUGSELH    0x1fU
+#define V_DEBUGSELH(x) ((x) << S_DEBUGSELH)
+#define G_DEBUGSELH(x) (((x) >> S_DEBUGSELH) & M_DEBUGSELH)
+
+#define S_DEBUGSELL    0
+#define M_DEBUGSELL    0x1fU
+#define V_DEBUGSELL(x) ((x) << S_DEBUGSELL)
+#define G_DEBUGSELL(x) (((x) >> S_DEBUGSELL) & M_DEBUGSELL)
+
+#define A_CIM_DEBUGSTS 0x7b74
+
+#define S_LARESET    31
+#define V_LARESET(x) ((x) << S_LARESET)
+#define F_LARESET    V_LARESET(1U)
+
+#define S_POLADBGWRPTR    16
+#define M_POLADBGWRPTR    0x1ffU
+#define V_POLADBGWRPTR(x) ((x) << S_POLADBGWRPTR)
+#define G_POLADBGWRPTR(x) (((x) >> S_POLADBGWRPTR) & M_POLADBGWRPTR)
+
+#define S_PILADBGWRPTR    0
+#define M_PILADBGWRPTR    0x1ffU
+#define V_PILADBGWRPTR(x) ((x) << S_PILADBGWRPTR)
+#define G_PILADBGWRPTR(x) (((x) >> S_PILADBGWRPTR) & M_PILADBGWRPTR)
+
+#define A_CIM_PO_LA_DEBUGDATA 0x7b78
+#define A_CIM_PI_LA_DEBUGDATA 0x7b7c
+#define A_CIM_PO_LA_MADEBUGDATA 0x7b80
+#define A_CIM_PI_LA_MADEBUGDATA 0x7b84
+#define A_CIM_PO_LA_PIFSMDEBUGDATA 0x7b8c
+#define A_CIM_MEM_ZONE0_VA 0x7b90
+
+#define S_MEM_ZONE_VA    4
+#define M_MEM_ZONE_VA    0xfffffffU
+#define V_MEM_ZONE_VA(x) ((x) << S_MEM_ZONE_VA)
+#define G_MEM_ZONE_VA(x) (((x) >> S_MEM_ZONE_VA) & M_MEM_ZONE_VA)
+
+#define A_CIM_MEM_ZONE0_BA 0x7b94
+
+#define S_MEM_ZONE_BA    6
+#define M_MEM_ZONE_BA    0x3ffffffU
+#define V_MEM_ZONE_BA(x) ((x) << S_MEM_ZONE_BA)
+#define G_MEM_ZONE_BA(x) (((x) >> S_MEM_ZONE_BA) & M_MEM_ZONE_BA)
+
+#define S_PBT_ENABLE    5
+#define V_PBT_ENABLE(x) ((x) << S_PBT_ENABLE)
+#define F_PBT_ENABLE    V_PBT_ENABLE(1U)
+
+#define S_ZONE_DST    0
+#define M_ZONE_DST    0x3U
+#define V_ZONE_DST(x) ((x) << S_ZONE_DST)
+#define G_ZONE_DST(x) (((x) >> S_ZONE_DST) & M_ZONE_DST)
+
+#define A_CIM_MEM_ZONE0_LEN 0x7b98
+
+#define S_MEM_ZONE_LEN    4
+#define M_MEM_ZONE_LEN    0xfffffffU
+#define V_MEM_ZONE_LEN(x) ((x) << S_MEM_ZONE_LEN)
+#define G_MEM_ZONE_LEN(x) (((x) >> S_MEM_ZONE_LEN) & M_MEM_ZONE_LEN)
+
+#define A_CIM_MEM_ZONE1_VA 0x7b9c
+#define A_CIM_MEM_ZONE1_BA 0x7ba0
+#define A_CIM_MEM_ZONE1_LEN 0x7ba4
+#define A_CIM_MEM_ZONE2_VA 0x7ba8
+#define A_CIM_MEM_ZONE2_BA 0x7bac
+#define A_CIM_MEM_ZONE2_LEN 0x7bb0
+#define A_CIM_MEM_ZONE3_VA 0x7bb4
+#define A_CIM_MEM_ZONE3_BA 0x7bb8
+#define A_CIM_MEM_ZONE3_LEN 0x7bbc
+#define A_CIM_MEM_ZONE4_VA 0x7bc0
+#define A_CIM_MEM_ZONE4_BA 0x7bc4
+#define A_CIM_MEM_ZONE4_LEN 0x7bc8
+#define A_CIM_MEM_ZONE5_VA 0x7bcc
+#define A_CIM_MEM_ZONE5_BA 0x7bd0
+#define A_CIM_MEM_ZONE5_LEN 0x7bd4
+#define A_CIM_MEM_ZONE6_VA 0x7bd8
+#define A_CIM_MEM_ZONE6_BA 0x7bdc
+#define A_CIM_MEM_ZONE6_LEN 0x7be0
+#define A_CIM_MEM_ZONE7_VA 0x7be4
+#define A_CIM_MEM_ZONE7_BA 0x7be8
+#define A_CIM_MEM_ZONE7_LEN 0x7bec
+#define A_CIM_BOOT_LEN 0x7bf0
+
+#define S_BOOTLEN    4
+#define M_BOOTLEN    0xfffffffU
+#define V_BOOTLEN(x) ((x) << S_BOOTLEN)
+#define G_BOOTLEN(x) (((x) >> S_BOOTLEN) & M_BOOTLEN)
+
+#define A_CIM_GLB_TIMER_CTL 0x7bf4
+
+#define S_TIMER1EN    4
+#define V_TIMER1EN(x) ((x) << S_TIMER1EN)
+#define F_TIMER1EN    V_TIMER1EN(1U)
+
+#define S_TIMER0EN    3
+#define V_TIMER0EN(x) ((x) << S_TIMER0EN)
+#define F_TIMER0EN    V_TIMER0EN(1U)
+
+#define S_TIMEREN    1
+#define V_TIMEREN(x) ((x) << S_TIMEREN)
+#define F_TIMEREN    V_TIMEREN(1U)
+
+#define A_CIM_GLB_TIMER 0x7bf8
+#define A_CIM_GLB_TIMER_TICK 0x7bfc
+
+#define S_GLBLTTICK    0
+#define M_GLBLTTICK    0xffffU
+#define V_GLBLTTICK(x) ((x) << S_GLBLTTICK)
+#define G_GLBLTTICK(x) (((x) >> S_GLBLTTICK) & M_GLBLTTICK)
+
+#define A_CIM_TIMER0 0x7c00
+#define A_CIM_TIMER1 0x7c04
+#define A_CIM_DEBUG_ADDR_TIMEOUT 0x7c08
+
+#define S_DADDRTIMEOUT    2
+#define M_DADDRTIMEOUT    0x3fffffffU
+#define V_DADDRTIMEOUT(x) ((x) << S_DADDRTIMEOUT)
+#define G_DADDRTIMEOUT(x) (((x) >> S_DADDRTIMEOUT) & M_DADDRTIMEOUT)
+
+#define A_CIM_DEBUG_ADDR_ILLEGAL 0x7c0c
+
+#define S_DADDRILLEGAL    2
+#define M_DADDRILLEGAL    0x3fffffffU
+#define V_DADDRILLEGAL(x) ((x) << S_DADDRILLEGAL)
+#define G_DADDRILLEGAL(x) (((x) >> S_DADDRILLEGAL) & M_DADDRILLEGAL)
+
+#define A_CIM_DEBUG_PIF_CAUSE_MASK 0x7c10
+
+#define S_DPIFHOSTMASK    0
+#define M_DPIFHOSTMASK    0x1fffffU
+#define V_DPIFHOSTMASK(x) ((x) << S_DPIFHOSTMASK)
+#define G_DPIFHOSTMASK(x) (((x) >> S_DPIFHOSTMASK) & M_DPIFHOSTMASK)
+
+#define A_CIM_DEBUG_PIF_UPACC_CAUSE_MASK 0x7c14
+
+#define S_DPIFHUPAMASK    0
+#define M_DPIFHUPAMASK    0x7fffffffU
+#define V_DPIFHUPAMASK(x) ((x) << S_DPIFHUPAMASK)
+#define G_DPIFHUPAMASK(x) (((x) >> S_DPIFHUPAMASK) & M_DPIFHUPAMASK)
+
+#define A_CIM_DEBUG_UP_CAUSE_MASK 0x7c18
+
+#define S_DUPMASK    0
+#define M_DUPMASK    0x1fffffU
+#define V_DUPMASK(x) ((x) << S_DUPMASK)
+#define G_DUPMASK(x) (((x) >> S_DUPMASK) & M_DUPMASK)
+
+#define A_CIM_DEBUG_UP_UPACC_CAUSE_MASK 0x7c1c
+
+#define S_DUPUACCMASK    0
+#define M_DUPUACCMASK    0x7fffffffU
+#define V_DUPUACCMASK(x) ((x) << S_DUPUACCMASK)
+#define G_DUPUACCMASK(x) (((x) >> S_DUPUACCMASK) & M_DUPUACCMASK)
+
+#define A_CIM_PERR_INJECT 0x7c20
+#define A_CIM_PERR_ENABLE 0x7c24
+
+#define S_PERREN    0
+#define M_PERREN    0x1fffffU
+#define V_PERREN(x) ((x) << S_PERREN)
+#define G_PERREN(x) (((x) >> S_PERREN) & M_PERREN)
+
+#define A_CIM_EEPROM_BUSY_BIT 0x7c28
+
+#define S_EEPROMBUSY    0
+#define V_EEPROMBUSY(x) ((x) << S_EEPROMBUSY)
+#define F_EEPROMBUSY    V_EEPROMBUSY(1U)
+
+#define A_CIM_MA_TIMER_EN 0x7c2c
+
+#define S_MA_TIMER_ENABLE    0
+#define V_MA_TIMER_ENABLE(x) ((x) << S_MA_TIMER_ENABLE)
+#define F_MA_TIMER_ENABLE    V_MA_TIMER_ENABLE(1U)
+
+#define A_CIM_UP_PO_SINGLE_OUTSTANDING 0x7c30
+
+#define S_UP_PO_SINGLE_OUTSTANDING    0
+#define V_UP_PO_SINGLE_OUTSTANDING(x) ((x) << S_UP_PO_SINGLE_OUTSTANDING)
+#define F_UP_PO_SINGLE_OUTSTANDING    V_UP_PO_SINGLE_OUTSTANDING(1U)
+
+#define A_CIM_CIM_DEBUG_SPARE 0x7c34
+#define A_CIM_UP_OPERATION_FREQ 0x7c38
+
+/* registers for module TP */
+#define TP_BASE_ADDR 0x7d00
+
+#define A_TP_IN_CONFIG 0x7d00
+
+#define S_TCPOPTPARSERDISCH3    27
+#define V_TCPOPTPARSERDISCH3(x) ((x) << S_TCPOPTPARSERDISCH3)
+#define F_TCPOPTPARSERDISCH3    V_TCPOPTPARSERDISCH3(1U)
+
+#define S_TCPOPTPARSERDISCH2    26
+#define V_TCPOPTPARSERDISCH2(x) ((x) << S_TCPOPTPARSERDISCH2)
+#define F_TCPOPTPARSERDISCH2    V_TCPOPTPARSERDISCH2(1U)
+
+#define S_TCPOPTPARSERDISCH1    25
+#define V_TCPOPTPARSERDISCH1(x) ((x) << S_TCPOPTPARSERDISCH1)
+#define F_TCPOPTPARSERDISCH1    V_TCPOPTPARSERDISCH1(1U)
+
+#define S_TCPOPTPARSERDISCH0    24
+#define V_TCPOPTPARSERDISCH0(x) ((x) << S_TCPOPTPARSERDISCH0)
+#define F_TCPOPTPARSERDISCH0    V_TCPOPTPARSERDISCH0(1U)
+
+#define S_CRCPASSPRT3    23
+#define V_CRCPASSPRT3(x) ((x) << S_CRCPASSPRT3)
+#define F_CRCPASSPRT3    V_CRCPASSPRT3(1U)
+
+#define S_CRCPASSPRT2    22
+#define V_CRCPASSPRT2(x) ((x) << S_CRCPASSPRT2)
+#define F_CRCPASSPRT2    V_CRCPASSPRT2(1U)
+
+#define S_CRCPASSPRT1    21
+#define V_CRCPASSPRT1(x) ((x) << S_CRCPASSPRT1)
+#define F_CRCPASSPRT1    V_CRCPASSPRT1(1U)
+
+#define S_CRCPASSPRT0    20
+#define V_CRCPASSPRT0(x) ((x) << S_CRCPASSPRT0)
+#define F_CRCPASSPRT0    V_CRCPASSPRT0(1U)
+
+#define S_VEPAMODE    19
+#define V_VEPAMODE(x) ((x) << S_VEPAMODE)
+#define F_VEPAMODE    V_VEPAMODE(1U)
+
+#define S_FIPUPEN    18
+#define V_FIPUPEN(x) ((x) << S_FIPUPEN)
+#define F_FIPUPEN    V_FIPUPEN(1U)
+
+#define S_FCOEUPEN    17
+#define V_FCOEUPEN(x) ((x) << S_FCOEUPEN)
+#define F_FCOEUPEN    V_FCOEUPEN(1U)
+
+#define S_FCOEENABLE    16
+#define V_FCOEENABLE(x) ((x) << S_FCOEENABLE)
+#define F_FCOEENABLE    V_FCOEENABLE(1U)
+
+#define S_IPV6ENABLE    15
+#define V_IPV6ENABLE(x) ((x) << S_IPV6ENABLE)
+#define F_IPV6ENABLE    V_IPV6ENABLE(1U)
+
+#define S_NICMODE    14
+#define V_NICMODE(x) ((x) << S_NICMODE)
+#define F_NICMODE    V_NICMODE(1U)
+
+#define S_ECHECKSUMCHECKTCP    13
+#define V_ECHECKSUMCHECKTCP(x) ((x) << S_ECHECKSUMCHECKTCP)
+#define F_ECHECKSUMCHECKTCP    V_ECHECKSUMCHECKTCP(1U)
+
+#define S_ECHECKSUMCHECKIP    12
+#define V_ECHECKSUMCHECKIP(x) ((x) << S_ECHECKSUMCHECKIP)
+#define F_ECHECKSUMCHECKIP    V_ECHECKSUMCHECKIP(1U)
+
+#define S_EREPORTUDPHDRLEN    11
+#define V_EREPORTUDPHDRLEN(x) ((x) << S_EREPORTUDPHDRLEN)
+#define F_EREPORTUDPHDRLEN    V_EREPORTUDPHDRLEN(1U)
+
+#define S_IN_ECPL    10
+#define V_IN_ECPL(x) ((x) << S_IN_ECPL)
+#define F_IN_ECPL    V_IN_ECPL(1U)
+
+#define S_VNTAGENABLE    9
+#define V_VNTAGENABLE(x) ((x) << S_VNTAGENABLE)
+#define F_VNTAGENABLE    V_VNTAGENABLE(1U)
+
+#define S_IN_EETH    8
+#define V_IN_EETH(x) ((x) << S_IN_EETH)
+#define F_IN_EETH    V_IN_EETH(1U)
+
+#define S_CCHECKSUMCHECKTCP    6
+#define V_CCHECKSUMCHECKTCP(x) ((x) << S_CCHECKSUMCHECKTCP)
+#define F_CCHECKSUMCHECKTCP    V_CCHECKSUMCHECKTCP(1U)
+
+#define S_CCHECKSUMCHECKIP    5
+#define V_CCHECKSUMCHECKIP(x) ((x) << S_CCHECKSUMCHECKIP)
+#define F_CCHECKSUMCHECKIP    V_CCHECKSUMCHECKIP(1U)
+
+#define S_CTAG    4
+#define V_CTAG(x) ((x) << S_CTAG)
+#define F_CTAG    V_CTAG(1U)
+
+#define S_IN_CCPL    3
+#define V_IN_CCPL(x) ((x) << S_IN_CCPL)
+#define F_IN_CCPL    V_IN_CCPL(1U)
+
+#define S_IN_CETH    1
+#define V_IN_CETH(x) ((x) << S_IN_CETH)
+#define F_IN_CETH    V_IN_CETH(1U)
+
+#define S_CTUNNEL    0
+#define V_CTUNNEL(x) ((x) << S_CTUNNEL)
+#define F_CTUNNEL    V_CTUNNEL(1U)
+
+#define A_TP_OUT_CONFIG 0x7d04
+
+#define S_PORTQFCEN    28
+#define M_PORTQFCEN    0xfU
+#define V_PORTQFCEN(x) ((x) << S_PORTQFCEN)
+#define G_PORTQFCEN(x) (((x) >> S_PORTQFCEN) & M_PORTQFCEN)
+
+#define S_EPKTDISTCHN3    23
+#define V_EPKTDISTCHN3(x) ((x) << S_EPKTDISTCHN3)
+#define F_EPKTDISTCHN3    V_EPKTDISTCHN3(1U)
+
+#define S_EPKTDISTCHN2    22
+#define V_EPKTDISTCHN2(x) ((x) << S_EPKTDISTCHN2)
+#define F_EPKTDISTCHN2    V_EPKTDISTCHN2(1U)
+
+#define S_EPKTDISTCHN1    21
+#define V_EPKTDISTCHN1(x) ((x) << S_EPKTDISTCHN1)
+#define F_EPKTDISTCHN1    V_EPKTDISTCHN1(1U)
+
+#define S_EPKTDISTCHN0    20
+#define V_EPKTDISTCHN0(x) ((x) << S_EPKTDISTCHN0)
+#define F_EPKTDISTCHN0    V_EPKTDISTCHN0(1U)
+
+#define S_TTLMODE    19
+#define V_TTLMODE(x) ((x) << S_TTLMODE)
+#define F_TTLMODE    V_TTLMODE(1U)
+
+#define S_EQFCDMAC    18
+#define V_EQFCDMAC(x) ((x) << S_EQFCDMAC)
+#define F_EQFCDMAC    V_EQFCDMAC(1U)
+
+#define S_ELPBKINCMPSSTAT    17
+#define V_ELPBKINCMPSSTAT(x) ((x) << S_ELPBKINCMPSSTAT)
+#define F_ELPBKINCMPSSTAT    V_ELPBKINCMPSSTAT(1U)
+
+#define S_IPIDSPLITMODE    16
+#define V_IPIDSPLITMODE(x) ((x) << S_IPIDSPLITMODE)
+#define F_IPIDSPLITMODE    V_IPIDSPLITMODE(1U)
+
+#define S_VLANEXTENABLEPORT3    15
+#define V_VLANEXTENABLEPORT3(x) ((x) << S_VLANEXTENABLEPORT3)
+#define F_VLANEXTENABLEPORT3    V_VLANEXTENABLEPORT3(1U)
+
+#define S_VLANEXTENABLEPORT2    14
+#define V_VLANEXTENABLEPORT2(x) ((x) << S_VLANEXTENABLEPORT2)
+#define F_VLANEXTENABLEPORT2    V_VLANEXTENABLEPORT2(1U)
+
+#define S_VLANEXTENABLEPORT1    13
+#define V_VLANEXTENABLEPORT1(x) ((x) << S_VLANEXTENABLEPORT1)
+#define F_VLANEXTENABLEPORT1    V_VLANEXTENABLEPORT1(1U)
+
+#define S_VLANEXTENABLEPORT0    12
+#define V_VLANEXTENABLEPORT0(x) ((x) << S_VLANEXTENABLEPORT0)
+#define F_VLANEXTENABLEPORT0    V_VLANEXTENABLEPORT0(1U)
+
+#define S_ECHECKSUMINSERTTCP    11
+#define V_ECHECKSUMINSERTTCP(x) ((x) << S_ECHECKSUMINSERTTCP)
+#define F_ECHECKSUMINSERTTCP    V_ECHECKSUMINSERTTCP(1U)
+
+#define S_ECHECKSUMINSERTIP    10
+#define V_ECHECKSUMINSERTIP(x) ((x) << S_ECHECKSUMINSERTIP)
+#define F_ECHECKSUMINSERTIP    V_ECHECKSUMINSERTIP(1U)
+
+#define S_ECPL    8
+#define V_ECPL(x) ((x) << S_ECPL)
+#define F_ECPL    V_ECPL(1U)
+
+#define S_EPRIORITY    7
+#define V_EPRIORITY(x) ((x) << S_EPRIORITY)
+#define F_EPRIORITY    V_EPRIORITY(1U)
+
+#define S_EETHERNET    6
+#define V_EETHERNET(x) ((x) << S_EETHERNET)
+#define F_EETHERNET    V_EETHERNET(1U)
+
+#define S_CCHECKSUMINSERTTCP    5
+#define V_CCHECKSUMINSERTTCP(x) ((x) << S_CCHECKSUMINSERTTCP)
+#define F_CCHECKSUMINSERTTCP    V_CCHECKSUMINSERTTCP(1U)
+
+#define S_CCHECKSUMINSERTIP    4
+#define V_CCHECKSUMINSERTIP(x) ((x) << S_CCHECKSUMINSERTIP)
+#define F_CCHECKSUMINSERTIP    V_CCHECKSUMINSERTIP(1U)
+
+#define S_CCPL    2
+#define V_CCPL(x) ((x) << S_CCPL)
+#define F_CCPL    V_CCPL(1U)
+
+#define S_CETHERNET    0
+#define V_CETHERNET(x) ((x) << S_CETHERNET)
+#define F_CETHERNET    V_CETHERNET(1U)
+
+#define A_TP_GLOBAL_CONFIG 0x7d08
+
+#define S_SYNCOOKIEPARAMS    26
+#define M_SYNCOOKIEPARAMS    0x3fU
+#define V_SYNCOOKIEPARAMS(x) ((x) << S_SYNCOOKIEPARAMS)
+#define G_SYNCOOKIEPARAMS(x) (((x) >> S_SYNCOOKIEPARAMS) & M_SYNCOOKIEPARAMS)
+
+#define S_RXFLOWCONTROLDISABLE    25
+#define V_RXFLOWCONTROLDISABLE(x) ((x) << S_RXFLOWCONTROLDISABLE)
+#define F_RXFLOWCONTROLDISABLE    V_RXFLOWCONTROLDISABLE(1U)
+
+#define S_TXPACINGENABLE    24
+#define V_TXPACINGENABLE(x) ((x) << S_TXPACINGENABLE)
+#define F_TXPACINGENABLE    V_TXPACINGENABLE(1U)
+
+#define S_ATTACKFILTERENABLE    23
+#define V_ATTACKFILTERENABLE(x) ((x) << S_ATTACKFILTERENABLE)
+#define F_ATTACKFILTERENABLE    V_ATTACKFILTERENABLE(1U)
+
+#define S_SYNCOOKIENOOPTIONS    22
+#define V_SYNCOOKIENOOPTIONS(x) ((x) << S_SYNCOOKIENOOPTIONS)
+#define F_SYNCOOKIENOOPTIONS    V_SYNCOOKIENOOPTIONS(1U)
+
+#define S_PROTECTEDMODE    21
+#define V_PROTECTEDMODE(x) ((x) << S_PROTECTEDMODE)
+#define F_PROTECTEDMODE    V_PROTECTEDMODE(1U)
+
+#define S_PINGDROP    20
+#define V_PINGDROP(x) ((x) << S_PINGDROP)
+#define F_PINGDROP    V_PINGDROP(1U)
+
+#define S_FRAGMENTDROP    19
+#define V_FRAGMENTDROP(x) ((x) << S_FRAGMENTDROP)
+#define F_FRAGMENTDROP    V_FRAGMENTDROP(1U)
+
+#define S_FIVETUPLELOOKUP    17
+#define M_FIVETUPLELOOKUP    0x3U
+#define V_FIVETUPLELOOKUP(x) ((x) << S_FIVETUPLELOOKUP)
+#define G_FIVETUPLELOOKUP(x) (((x) >> S_FIVETUPLELOOKUP) & M_FIVETUPLELOOKUP)
+
+#define S_OFDMPSSTATS    16
+#define V_OFDMPSSTATS(x) ((x) << S_OFDMPSSTATS)
+#define F_OFDMPSSTATS    V_OFDMPSSTATS(1U)
+
+#define S_DONTFRAGMENT    15
+#define V_DONTFRAGMENT(x) ((x) << S_DONTFRAGMENT)
+#define F_DONTFRAGMENT    V_DONTFRAGMENT(1U)
+
+#define S_IPIDENTSPLIT    14
+#define V_IPIDENTSPLIT(x) ((x) << S_IPIDENTSPLIT)
+#define F_IPIDENTSPLIT    V_IPIDENTSPLIT(1U)
+
+#define S_IPCHECKSUMOFFLOAD    13
+#define V_IPCHECKSUMOFFLOAD(x) ((x) << S_IPCHECKSUMOFFLOAD)
+#define F_IPCHECKSUMOFFLOAD    V_IPCHECKSUMOFFLOAD(1U)
+
+#define S_UDPCHECKSUMOFFLOAD    12
+#define V_UDPCHECKSUMOFFLOAD(x) ((x) << S_UDPCHECKSUMOFFLOAD)
+#define F_UDPCHECKSUMOFFLOAD    V_UDPCHECKSUMOFFLOAD(1U)
+
+#define S_TCPCHECKSUMOFFLOAD    11
+#define V_TCPCHECKSUMOFFLOAD(x) ((x) << S_TCPCHECKSUMOFFLOAD)
+#define F_TCPCHECKSUMOFFLOAD    V_TCPCHECKSUMOFFLOAD(1U)
+
+#define S_RSSLOOPBACKENABLE    10
+#define V_RSSLOOPBACKENABLE(x) ((x) << S_RSSLOOPBACKENABLE)
+#define F_RSSLOOPBACKENABLE    V_RSSLOOPBACKENABLE(1U)
+
+#define S_TCAMSERVERUSE    8
+#define M_TCAMSERVERUSE    0x3U
+#define V_TCAMSERVERUSE(x) ((x) << S_TCAMSERVERUSE)
+#define G_TCAMSERVERUSE(x) (((x) >> S_TCAMSERVERUSE) & M_TCAMSERVERUSE)
+
+#define S_IPTTL    0
+#define M_IPTTL    0xffU
+#define V_IPTTL(x) ((x) << S_IPTTL)
+#define G_IPTTL(x) (((x) >> S_IPTTL) & M_IPTTL)
+
+#define A_TP_DB_CONFIG 0x7d0c
+
+#define S_DBMAXOPCNT    24
+#define M_DBMAXOPCNT    0xffU
+#define V_DBMAXOPCNT(x) ((x) << S_DBMAXOPCNT)
+#define G_DBMAXOPCNT(x) (((x) >> S_DBMAXOPCNT) & M_DBMAXOPCNT)
+
+#define S_CXMAXOPCNTDISABLE    23
+#define V_CXMAXOPCNTDISABLE(x) ((x) << S_CXMAXOPCNTDISABLE)
+#define F_CXMAXOPCNTDISABLE    V_CXMAXOPCNTDISABLE(1U)
+
+#define S_CXMAXOPCNT    16
+#define M_CXMAXOPCNT    0x7fU
+#define V_CXMAXOPCNT(x) ((x) << S_CXMAXOPCNT)
+#define G_CXMAXOPCNT(x) (((x) >> S_CXMAXOPCNT) & M_CXMAXOPCNT)
+
+#define S_TXMAXOPCNTDISABLE    15
+#define V_TXMAXOPCNTDISABLE(x) ((x) << S_TXMAXOPCNTDISABLE)
+#define F_TXMAXOPCNTDISABLE    V_TXMAXOPCNTDISABLE(1U)
+
+#define S_TXMAXOPCNT    8
+#define M_TXMAXOPCNT    0x7fU
+#define V_TXMAXOPCNT(x) ((x) << S_TXMAXOPCNT)
+#define G_TXMAXOPCNT(x) (((x) >> S_TXMAXOPCNT) & M_TXMAXOPCNT)
+
+#define S_RXMAXOPCNTDISABLE    7
+#define V_RXMAXOPCNTDISABLE(x) ((x) << S_RXMAXOPCNTDISABLE)
+#define F_RXMAXOPCNTDISABLE    V_RXMAXOPCNTDISABLE(1U)
+
+#define S_RXMAXOPCNT    0
+#define M_RXMAXOPCNT    0x7fU
+#define V_RXMAXOPCNT(x) ((x) << S_RXMAXOPCNT)
+#define G_RXMAXOPCNT(x) (((x) >> S_RXMAXOPCNT) & M_RXMAXOPCNT)
+
+#define A_TP_CMM_TCB_BASE 0x7d10
+#define A_TP_CMM_MM_BASE 0x7d14
+#define A_TP_CMM_TIMER_BASE 0x7d18
+#define A_TP_CMM_MM_FLST_SIZE 0x7d1c
+
+#define S_RXPOOLSIZE    16
+#define M_RXPOOLSIZE    0xffffU
+#define V_RXPOOLSIZE(x) ((x) << S_RXPOOLSIZE)
+#define G_RXPOOLSIZE(x) (((x) >> S_RXPOOLSIZE) & M_RXPOOLSIZE)
+
+#define S_TXPOOLSIZE    0
+#define M_TXPOOLSIZE    0xffffU
+#define V_TXPOOLSIZE(x) ((x) << S_TXPOOLSIZE)
+#define G_TXPOOLSIZE(x) (((x) >> S_TXPOOLSIZE) & M_TXPOOLSIZE)
+
+#define A_TP_PMM_TX_BASE 0x7d20
+#define A_TP_PMM_DEFRAG_BASE 0x7d24
+#define A_TP_PMM_RX_BASE 0x7d28
+#define A_TP_PMM_RX_PAGE_SIZE 0x7d2c
+#define A_TP_PMM_RX_MAX_PAGE 0x7d30
+
+#define S_PMRXNUMCHN    31
+#define V_PMRXNUMCHN(x) ((x) << S_PMRXNUMCHN)
+#define F_PMRXNUMCHN    V_PMRXNUMCHN(1U)
+
+#define S_PMRXMAXPAGE    0
+#define M_PMRXMAXPAGE    0x1fffffU
+#define V_PMRXMAXPAGE(x) ((x) << S_PMRXMAXPAGE)
+#define G_PMRXMAXPAGE(x) (((x) >> S_PMRXMAXPAGE) & M_PMRXMAXPAGE)
+
+#define A_TP_PMM_TX_PAGE_SIZE 0x7d34
+#define A_TP_PMM_TX_MAX_PAGE 0x7d38
+
+#define S_PMTXNUMCHN    30
+#define M_PMTXNUMCHN    0x3U
+#define V_PMTXNUMCHN(x) ((x) << S_PMTXNUMCHN)
+#define G_PMTXNUMCHN(x) (((x) >> S_PMTXNUMCHN) & M_PMTXNUMCHN)
+
+#define S_PMTXMAXPAGE    0
+#define M_PMTXMAXPAGE    0x1fffffU
+#define V_PMTXMAXPAGE(x) ((x) << S_PMTXMAXPAGE)
+#define G_PMTXMAXPAGE(x) (((x) >> S_PMTXMAXPAGE) & M_PMTXMAXPAGE)
+
+#define A_TP_TCP_OPTIONS 0x7d40
+
+#define S_MTUDEFAULT    16
+#define M_MTUDEFAULT    0xffffU
+#define V_MTUDEFAULT(x) ((x) << S_MTUDEFAULT)
+#define G_MTUDEFAULT(x) (((x) >> S_MTUDEFAULT) & M_MTUDEFAULT)
+
+#define S_MTUENABLE    10
+#define V_MTUENABLE(x) ((x) << S_MTUENABLE)
+#define F_MTUENABLE    V_MTUENABLE(1U)
+
+#define S_SACKTX    9
+#define V_SACKTX(x) ((x) << S_SACKTX)
+#define F_SACKTX    V_SACKTX(1U)
+
+#define S_SACKRX    8
+#define V_SACKRX(x) ((x) << S_SACKRX)
+#define F_SACKRX    V_SACKRX(1U)
+
+#define S_SACKMODE    4
+#define M_SACKMODE    0x3U
+#define V_SACKMODE(x) ((x) << S_SACKMODE)
+#define G_SACKMODE(x) (((x) >> S_SACKMODE) & M_SACKMODE)
+
+#define S_WINDOWSCALEMODE    2
+#define M_WINDOWSCALEMODE    0x3U
+#define V_WINDOWSCALEMODE(x) ((x) << S_WINDOWSCALEMODE)
+#define G_WINDOWSCALEMODE(x) (((x) >> S_WINDOWSCALEMODE) & M_WINDOWSCALEMODE)
+
+#define S_TIMESTAMPSMODE    0
+#define M_TIMESTAMPSMODE    0x3U
+#define V_TIMESTAMPSMODE(x) ((x) << S_TIMESTAMPSMODE)
+#define G_TIMESTAMPSMODE(x) (((x) >> S_TIMESTAMPSMODE) & M_TIMESTAMPSMODE)
+
+#define A_TP_DACK_CONFIG 0x7d44
+
+#define S_AUTOSTATE3    30
+#define M_AUTOSTATE3    0x3U
+#define V_AUTOSTATE3(x) ((x) << S_AUTOSTATE3)
+#define G_AUTOSTATE3(x) (((x) >> S_AUTOSTATE3) & M_AUTOSTATE3)
+
+#define S_AUTOSTATE2    28
+#define M_AUTOSTATE2    0x3U
+#define V_AUTOSTATE2(x) ((x) << S_AUTOSTATE2)
+#define G_AUTOSTATE2(x) (((x) >> S_AUTOSTATE2) & M_AUTOSTATE2)
+
+#define S_AUTOSTATE1    26
+#define M_AUTOSTATE1    0x3U
+#define V_AUTOSTATE1(x) ((x) << S_AUTOSTATE1)
+#define G_AUTOSTATE1(x) (((x) >> S_AUTOSTATE1) & M_AUTOSTATE1)
+
+#define S_BYTETHRESHOLD    8
+#define M_BYTETHRESHOLD    0x3ffffU
+#define V_BYTETHRESHOLD(x) ((x) << S_BYTETHRESHOLD)
+#define G_BYTETHRESHOLD(x) (((x) >> S_BYTETHRESHOLD) & M_BYTETHRESHOLD)
+
+#define S_MSSTHRESHOLD    4
+#define M_MSSTHRESHOLD    0x7U
+#define V_MSSTHRESHOLD(x) ((x) << S_MSSTHRESHOLD)
+#define G_MSSTHRESHOLD(x) (((x) >> S_MSSTHRESHOLD) & M_MSSTHRESHOLD)
+
+#define S_AUTOCAREFUL    2
+#define V_AUTOCAREFUL(x) ((x) << S_AUTOCAREFUL)
+#define F_AUTOCAREFUL    V_AUTOCAREFUL(1U)
+
+#define S_AUTOENABLE    1
+#define V_AUTOENABLE(x) ((x) << S_AUTOENABLE)
+#define F_AUTOENABLE    V_AUTOENABLE(1U)
+
+#define S_MODE    0
+#define V_MODE(x) ((x) << S_MODE)
+#define F_MODE    V_MODE(1U)
+
+#define A_TP_PC_CONFIG 0x7d48
+
+#define S_CMCACHEDISABLE    31
+#define V_CMCACHEDISABLE(x) ((x) << S_CMCACHEDISABLE)
+#define F_CMCACHEDISABLE    V_CMCACHEDISABLE(1U)
+
+#define S_ENABLEOCSPIFULL    30
+#define V_ENABLEOCSPIFULL(x) ((x) << S_ENABLEOCSPIFULL)
+#define F_ENABLEOCSPIFULL    V_ENABLEOCSPIFULL(1U)
+
+#define S_ENABLEFLMERRORDDP    29
+#define V_ENABLEFLMERRORDDP(x) ((x) << S_ENABLEFLMERRORDDP)
+#define F_ENABLEFLMERRORDDP    V_ENABLEFLMERRORDDP(1U)
+
+#define S_LOCKTID    28
+#define V_LOCKTID(x) ((x) << S_LOCKTID)
+#define F_LOCKTID    V_LOCKTID(1U)
+
+#define S_DISABLEINVPEND    27
+#define V_DISABLEINVPEND(x) ((x) << S_DISABLEINVPEND)
+#define F_DISABLEINVPEND    V_DISABLEINVPEND(1U)
+
+#define S_ENABLEFILTERCOUNT    26
+#define V_ENABLEFILTERCOUNT(x) ((x) << S_ENABLEFILTERCOUNT)
+#define F_ENABLEFILTERCOUNT    V_ENABLEFILTERCOUNT(1U)
+
+#define S_RDDPCONGEN    25
+#define V_RDDPCONGEN(x) ((x) << S_RDDPCONGEN)
+#define F_RDDPCONGEN    V_RDDPCONGEN(1U)
+
+#define S_ENABLEONFLYPDU    24
+#define V_ENABLEONFLYPDU(x) ((x) << S_ENABLEONFLYPDU)
+#define F_ENABLEONFLYPDU    V_ENABLEONFLYPDU(1U)
+
+#define S_ENABLEMINRCVWND    23
+#define V_ENABLEMINRCVWND(x) ((x) << S_ENABLEMINRCVWND)
+#define F_ENABLEMINRCVWND    V_ENABLEMINRCVWND(1U)
+
+#define S_ENABLEMAXRCVWND    22
+#define V_ENABLEMAXRCVWND(x) ((x) << S_ENABLEMAXRCVWND)
+#define F_ENABLEMAXRCVWND    V_ENABLEMAXRCVWND(1U)
+
+#define S_TXDATAACKRATEENABLE    21
+#define V_TXDATAACKRATEENABLE(x) ((x) << S_TXDATAACKRATEENABLE)
+#define F_TXDATAACKRATEENABLE    V_TXDATAACKRATEENABLE(1U)
+
+#define S_TXDEFERENABLE    20
+#define V_TXDEFERENABLE(x) ((x) << S_TXDEFERENABLE)
+#define F_TXDEFERENABLE    V_TXDEFERENABLE(1U)
+
+#define S_RXCONGESTIONMODE    19
+#define V_RXCONGESTIONMODE(x) ((x) << S_RXCONGESTIONMODE)
+#define F_RXCONGESTIONMODE    V_RXCONGESTIONMODE(1U)
+
+#define S_HEARBEATONCEDACK    18
+#define V_HEARBEATONCEDACK(x) ((x) << S_HEARBEATONCEDACK)
+#define F_HEARBEATONCEDACK    V_HEARBEATONCEDACK(1U)
+
+#define S_HEARBEATONCEHEAP    17
+#define V_HEARBEATONCEHEAP(x) ((x) << S_HEARBEATONCEHEAP)
+#define F_HEARBEATONCEHEAP    V_HEARBEATONCEHEAP(1U)
+
+#define S_HEARBEATDACK    16
+#define V_HEARBEATDACK(x) ((x) << S_HEARBEATDACK)
+#define F_HEARBEATDACK    V_HEARBEATDACK(1U)
+
+#define S_TXCONGESTIONMODE    15
+#define V_TXCONGESTIONMODE(x) ((x) << S_TXCONGESTIONMODE)
+#define F_TXCONGESTIONMODE    V_TXCONGESTIONMODE(1U)
+
+#define S_ACCEPTLATESTRCVADV    14
+#define V_ACCEPTLATESTRCVADV(x) ((x) << S_ACCEPTLATESTRCVADV)
+#define F_ACCEPTLATESTRCVADV    V_ACCEPTLATESTRCVADV(1U)
+
+#define S_DISABLESYNDATA    13
+#define V_DISABLESYNDATA(x) ((x) << S_DISABLESYNDATA)
+#define F_DISABLESYNDATA    V_DISABLESYNDATA(1U)
+
+#define S_DISABLEWINDOWPSH    12
+#define V_DISABLEWINDOWPSH(x) ((x) << S_DISABLEWINDOWPSH)
+#define F_DISABLEWINDOWPSH    V_DISABLEWINDOWPSH(1U)
+
+#define S_DISABLEFINOLDDATA    11
+#define V_DISABLEFINOLDDATA(x) ((x) << S_DISABLEFINOLDDATA)
+#define F_DISABLEFINOLDDATA    V_DISABLEFINOLDDATA(1U)
+
+#define S_ENABLEFLMERROR    10
+#define V_ENABLEFLMERROR(x) ((x) << S_ENABLEFLMERROR)
+#define F_ENABLEFLMERROR    V_ENABLEFLMERROR(1U)
+
+#define S_ENABLEOPTMTU    9
+#define V_ENABLEOPTMTU(x) ((x) << S_ENABLEOPTMTU)
+#define F_ENABLEOPTMTU    V_ENABLEOPTMTU(1U)
+
+#define S_FILTERPEERFIN    8
+#define V_FILTERPEERFIN(x) ((x) << S_FILTERPEERFIN)
+#define F_FILTERPEERFIN    V_FILTERPEERFIN(1U)
+
+#define S_ENABLEFEEDBACKSEND    7
+#define V_ENABLEFEEDBACKSEND(x) ((x) << S_ENABLEFEEDBACKSEND)
+#define F_ENABLEFEEDBACKSEND    V_ENABLEFEEDBACKSEND(1U)
+
+#define S_ENABLERDMAERROR    6
+#define V_ENABLERDMAERROR(x) ((x) << S_ENABLERDMAERROR)
+#define F_ENABLERDMAERROR    V_ENABLERDMAERROR(1U)
+
+#define S_ENABLEDDPFLOWCONTROL    5
+#define V_ENABLEDDPFLOWCONTROL(x) ((x) << S_ENABLEDDPFLOWCONTROL)
+#define F_ENABLEDDPFLOWCONTROL    V_ENABLEDDPFLOWCONTROL(1U)
+
+#define S_DISABLEHELDFIN    4
+#define V_DISABLEHELDFIN(x) ((x) << S_DISABLEHELDFIN)
+#define F_DISABLEHELDFIN    V_DISABLEHELDFIN(1U)
+
+#define S_ENABLEOFDOVLAN    3
+#define V_ENABLEOFDOVLAN(x) ((x) << S_ENABLEOFDOVLAN)
+#define F_ENABLEOFDOVLAN    V_ENABLEOFDOVLAN(1U)
+
+#define S_DISABLETIMEWAIT    2
+#define V_DISABLETIMEWAIT(x) ((x) << S_DISABLETIMEWAIT)
+#define F_DISABLETIMEWAIT    V_DISABLETIMEWAIT(1U)
+
+#define S_ENABLEVLANCHECK    1
+#define V_ENABLEVLANCHECK(x) ((x) << S_ENABLEVLANCHECK)
+#define F_ENABLEVLANCHECK    V_ENABLEVLANCHECK(1U)
+
+#define S_TXDATAACKPAGEENABLE    0
+#define V_TXDATAACKPAGEENABLE(x) ((x) << S_TXDATAACKPAGEENABLE)
+#define F_TXDATAACKPAGEENABLE    V_TXDATAACKPAGEENABLE(1U)
+
+#define A_TP_PC_CONFIG2 0x7d4c
+
+#define S_ENABLEMTUVFMODE    31
+#define V_ENABLEMTUVFMODE(x) ((x) << S_ENABLEMTUVFMODE)
+#define F_ENABLEMTUVFMODE    V_ENABLEMTUVFMODE(1U)
+
+#define S_ENABLEMIBVFMODE    30
+#define V_ENABLEMIBVFMODE(x) ((x) << S_ENABLEMIBVFMODE)
+#define F_ENABLEMIBVFMODE    V_ENABLEMIBVFMODE(1U)
+
+#define S_DISABLELBKCHECK    29
+#define V_DISABLELBKCHECK(x) ((x) << S_DISABLELBKCHECK)
+#define F_DISABLELBKCHECK    V_DISABLELBKCHECK(1U)
+
+#define S_ENABLEURGDDPOFF    28
+#define V_ENABLEURGDDPOFF(x) ((x) << S_ENABLEURGDDPOFF)
+#define F_ENABLEURGDDPOFF    V_ENABLEURGDDPOFF(1U)
+
+#define S_ENABLEFILTERLPBK    27
+#define V_ENABLEFILTERLPBK(x) ((x) << S_ENABLEFILTERLPBK)
+#define F_ENABLEFILTERLPBK    V_ENABLEFILTERLPBK(1U)
+
+#define S_DISABLETBLMMGR    26
+#define V_DISABLETBLMMGR(x) ((x) << S_DISABLETBLMMGR)
+#define F_DISABLETBLMMGR    V_DISABLETBLMMGR(1U)
+
+#define S_CNGRECSNDNXT    25
+#define V_CNGRECSNDNXT(x) ((x) << S_CNGRECSNDNXT)
+#define F_CNGRECSNDNXT    V_CNGRECSNDNXT(1U)
+
+#define S_ENABLELBKCHN    24
+#define V_ENABLELBKCHN(x) ((x) << S_ENABLELBKCHN)
+#define F_ENABLELBKCHN    V_ENABLELBKCHN(1U)
+
+#define S_ENABLELROECN    23
+#define V_ENABLELROECN(x) ((x) << S_ENABLELROECN)
+#define F_ENABLELROECN    V_ENABLELROECN(1U)
+
+#define S_ENABLEPCMDCHECK    22
+#define V_ENABLEPCMDCHECK(x) ((x) << S_ENABLEPCMDCHECK)
+#define F_ENABLEPCMDCHECK    V_ENABLEPCMDCHECK(1U)
+
+#define S_ENABLEELBKAFULL    21
+#define V_ENABLEELBKAFULL(x) ((x) << S_ENABLEELBKAFULL)
+#define F_ENABLEELBKAFULL    V_ENABLEELBKAFULL(1U)
+
+#define S_ENABLECLBKAFULL    20
+#define V_ENABLECLBKAFULL(x) ((x) << S_ENABLECLBKAFULL)
+#define F_ENABLECLBKAFULL    V_ENABLECLBKAFULL(1U)
+
+#define S_ENABLEOESPIFULL    19
+#define V_ENABLEOESPIFULL(x) ((x) << S_ENABLEOESPIFULL)
+#define F_ENABLEOESPIFULL    V_ENABLEOESPIFULL(1U)
+
+#define S_DISABLEHITCHECK    18
+#define V_DISABLEHITCHECK(x) ((x) << S_DISABLEHITCHECK)
+#define F_DISABLEHITCHECK    V_DISABLEHITCHECK(1U)
+
+#define S_ENABLERSSERRCHECK    17
+#define V_ENABLERSSERRCHECK(x) ((x) << S_ENABLERSSERRCHECK)
+#define F_ENABLERSSERRCHECK    V_ENABLERSSERRCHECK(1U)
+
+#define S_DISABLENEWPSHFLAG    16
+#define V_DISABLENEWPSHFLAG(x) ((x) << S_DISABLENEWPSHFLAG)
+#define F_DISABLENEWPSHFLAG    V_DISABLENEWPSHFLAG(1U)
+
+#define S_ENABLERDDPRCVADVCLR    15
+#define V_ENABLERDDPRCVADVCLR(x) ((x) << S_ENABLERDDPRCVADVCLR)
+#define F_ENABLERDDPRCVADVCLR    V_ENABLERDDPRCVADVCLR(1U)
+
+#define S_ENABLETXDATAARPMISS    14
+#define V_ENABLETXDATAARPMISS(x) ((x) << S_ENABLETXDATAARPMISS)
+#define F_ENABLETXDATAARPMISS    V_ENABLETXDATAARPMISS(1U)
+
+#define S_ENABLEARPMISS    13
+#define V_ENABLEARPMISS(x) ((x) << S_ENABLEARPMISS)
+#define F_ENABLEARPMISS    V_ENABLEARPMISS(1U)
+
+#define S_ENABLERSTPAWS    12
+#define V_ENABLERSTPAWS(x) ((x) << S_ENABLERSTPAWS)
+#define F_ENABLERSTPAWS    V_ENABLERSTPAWS(1U)
+
+#define S_ENABLEIPV6RSS    11
+#define V_ENABLEIPV6RSS(x) ((x) << S_ENABLEIPV6RSS)
+#define F_ENABLEIPV6RSS    V_ENABLEIPV6RSS(1U)
+
+#define S_ENABLENONOFDHYBRSS    10
+#define V_ENABLENONOFDHYBRSS(x) ((x) << S_ENABLENONOFDHYBRSS)
+#define F_ENABLENONOFDHYBRSS    V_ENABLENONOFDHYBRSS(1U)
+
+#define S_ENABLEUDP4TUPRSS    9
+#define V_ENABLEUDP4TUPRSS(x) ((x) << S_ENABLEUDP4TUPRSS)
+#define F_ENABLEUDP4TUPRSS    V_ENABLEUDP4TUPRSS(1U)
+
+#define S_ENABLERXPKTTMSTPRSS    8
+#define V_ENABLERXPKTTMSTPRSS(x) ((x) << S_ENABLERXPKTTMSTPRSS)
+#define F_ENABLERXPKTTMSTPRSS    V_ENABLERXPKTTMSTPRSS(1U)
+
+#define S_ENABLEEPCMDAFULL    7
+#define V_ENABLEEPCMDAFULL(x) ((x) << S_ENABLEEPCMDAFULL)
+#define F_ENABLEEPCMDAFULL    V_ENABLEEPCMDAFULL(1U)
+
+#define S_ENABLECPCMDAFULL    6
+#define V_ENABLECPCMDAFULL(x) ((x) << S_ENABLECPCMDAFULL)
+#define F_ENABLECPCMDAFULL    V_ENABLECPCMDAFULL(1U)
+
+#define S_ENABLEEHDRAFULL    5
+#define V_ENABLEEHDRAFULL(x) ((x) << S_ENABLEEHDRAFULL)
+#define F_ENABLEEHDRAFULL    V_ENABLEEHDRAFULL(1U)
+
+#define S_ENABLECHDRAFULL    4
+#define V_ENABLECHDRAFULL(x) ((x) << S_ENABLECHDRAFULL)
+#define F_ENABLECHDRAFULL    V_ENABLECHDRAFULL(1U)
+
+#define S_ENABLEEMACAFULL    3
+#define V_ENABLEEMACAFULL(x) ((x) << S_ENABLEEMACAFULL)
+#define F_ENABLEEMACAFULL    V_ENABLEEMACAFULL(1U)
+
+#define S_ENABLENONOFDTIDRSS    2
+#define V_ENABLENONOFDTIDRSS(x) ((x) << S_ENABLENONOFDTIDRSS)
+#define F_ENABLENONOFDTIDRSS    V_ENABLENONOFDTIDRSS(1U)
+
+#define S_ENABLENONOFDTCBRSS    1
+#define V_ENABLENONOFDTCBRSS(x) ((x) << S_ENABLENONOFDTCBRSS)
+#define F_ENABLENONOFDTCBRSS    V_ENABLENONOFDTCBRSS(1U)
+
+#define S_ENABLETNLOFDCLOSED    0
+#define V_ENABLETNLOFDCLOSED(x) ((x) << S_ENABLETNLOFDCLOSED)
+#define F_ENABLETNLOFDCLOSED    V_ENABLETNLOFDCLOSED(1U)
+
+#define A_TP_TCP_BACKOFF_REG0 0x7d50
+
+#define S_TIMERBACKOFFINDEX3    24
+#define M_TIMERBACKOFFINDEX3    0xffU
+#define V_TIMERBACKOFFINDEX3(x) ((x) << S_TIMERBACKOFFINDEX3)
+#define G_TIMERBACKOFFINDEX3(x) (((x) >> S_TIMERBACKOFFINDEX3) & M_TIMERBACKOFFINDEX3)
+
+#define S_TIMERBACKOFFINDEX2    16
+#define M_TIMERBACKOFFINDEX2    0xffU
+#define V_TIMERBACKOFFINDEX2(x) ((x) << S_TIMERBACKOFFINDEX2)
+#define G_TIMERBACKOFFINDEX2(x) (((x) >> S_TIMERBACKOFFINDEX2) & M_TIMERBACKOFFINDEX2)
+
+#define S_TIMERBACKOFFINDEX1    8
+#define M_TIMERBACKOFFINDEX1    0xffU
+#define V_TIMERBACKOFFINDEX1(x) ((x) << S_TIMERBACKOFFINDEX1)
+#define G_TIMERBACKOFFINDEX1(x) (((x) >> S_TIMERBACKOFFINDEX1) & M_TIMERBACKOFFINDEX1)
+
+#define S_TIMERBACKOFFINDEX0    0
+#define M_TIMERBACKOFFINDEX0    0xffU
+#define V_TIMERBACKOFFINDEX0(x) ((x) << S_TIMERBACKOFFINDEX0)
+#define G_TIMERBACKOFFINDEX0(x) (((x) >> S_TIMERBACKOFFINDEX0) & M_TIMERBACKOFFINDEX0)
+
+#define A_TP_TCP_BACKOFF_REG1 0x7d54
+
+#define S_TIMERBACKOFFINDEX7    24
+#define M_TIMERBACKOFFINDEX7    0xffU
+#define V_TIMERBACKOFFINDEX7(x) ((x) << S_TIMERBACKOFFINDEX7)
+#define G_TIMERBACKOFFINDEX7(x) (((x) >> S_TIMERBACKOFFINDEX7) & M_TIMERBACKOFFINDEX7)
+
+#define S_TIMERBACKOFFINDEX6    16
+#define M_TIMERBACKOFFINDEX6    0xffU
+#define V_TIMERBACKOFFINDEX6(x) ((x) << S_TIMERBACKOFFINDEX6)
+#define G_TIMERBACKOFFINDEX6(x) (((x) >> S_TIMERBACKOFFINDEX6) & M_TIMERBACKOFFINDEX6)
+
+#define S_TIMERBACKOFFINDEX5    8
+#define M_TIMERBACKOFFINDEX5    0xffU
+#define V_TIMERBACKOFFINDEX5(x) ((x) << S_TIMERBACKOFFINDEX5)
+#define G_TIMERBACKOFFINDEX5(x) (((x) >> S_TIMERBACKOFFINDEX5) & M_TIMERBACKOFFINDEX5)
+
+#define S_TIMERBACKOFFINDEX4    0
+#define M_TIMERBACKOFFINDEX4    0xffU
+#define V_TIMERBACKOFFINDEX4(x) ((x) << S_TIMERBACKOFFINDEX4)
+#define G_TIMERBACKOFFINDEX4(x) (((x) >> S_TIMERBACKOFFINDEX4) & M_TIMERBACKOFFINDEX4)
+
+#define A_TP_TCP_BACKOFF_REG2 0x7d58
+
+#define S_TIMERBACKOFFINDEX11    24
+#define M_TIMERBACKOFFINDEX11    0xffU
+#define V_TIMERBACKOFFINDEX11(x) ((x) << S_TIMERBACKOFFINDEX11)
+#define G_TIMERBACKOFFINDEX11(x) (((x) >> S_TIMERBACKOFFINDEX11) & M_TIMERBACKOFFINDEX11)
+
+#define S_TIMERBACKOFFINDEX10    16
+#define M_TIMERBACKOFFINDEX10    0xffU
+#define V_TIMERBACKOFFINDEX10(x) ((x) << S_TIMERBACKOFFINDEX10)
+#define G_TIMERBACKOFFINDEX10(x) (((x) >> S_TIMERBACKOFFINDEX10) & M_TIMERBACKOFFINDEX10)
+
+#define S_TIMERBACKOFFINDEX9    8
+#define M_TIMERBACKOFFINDEX9    0xffU
+#define V_TIMERBACKOFFINDEX9(x) ((x) << S_TIMERBACKOFFINDEX9)
+#define G_TIMERBACKOFFINDEX9(x) (((x) >> S_TIMERBACKOFFINDEX9) & M_TIMERBACKOFFINDEX9)
+
+#define S_TIMERBACKOFFINDEX8    0
+#define M_TIMERBACKOFFINDEX8    0xffU
+#define V_TIMERBACKOFFINDEX8(x) ((x) << S_TIMERBACKOFFINDEX8)
+#define G_TIMERBACKOFFINDEX8(x) (((x) >> S_TIMERBACKOFFINDEX8) & M_TIMERBACKOFFINDEX8)
+
+#define A_TP_TCP_BACKOFF_REG3 0x7d5c
+
+#define S_TIMERBACKOFFINDEX15    24
+#define M_TIMERBACKOFFINDEX15    0xffU
+#define V_TIMERBACKOFFINDEX15(x) ((x) << S_TIMERBACKOFFINDEX15)
+#define G_TIMERBACKOFFINDEX15(x) (((x) >> S_TIMERBACKOFFINDEX15) & M_TIMERBACKOFFINDEX15)
+
+#define S_TIMERBACKOFFINDEX14    16
+#define M_TIMERBACKOFFINDEX14    0xffU
+#define V_TIMERBACKOFFINDEX14(x) ((x) << S_TIMERBACKOFFINDEX14)
+#define G_TIMERBACKOFFINDEX14(x) (((x) >> S_TIMERBACKOFFINDEX14) & M_TIMERBACKOFFINDEX14)
+
+#define S_TIMERBACKOFFINDEX13    8
+#define M_TIMERBACKOFFINDEX13    0xffU
+#define V_TIMERBACKOFFINDEX13(x) ((x) << S_TIMERBACKOFFINDEX13)
+#define G_TIMERBACKOFFINDEX13(x) (((x) >> S_TIMERBACKOFFINDEX13) & M_TIMERBACKOFFINDEX13)
+
+#define S_TIMERBACKOFFINDEX12    0
+#define M_TIMERBACKOFFINDEX12    0xffU
+#define V_TIMERBACKOFFINDEX12(x) ((x) << S_TIMERBACKOFFINDEX12)
+#define G_TIMERBACKOFFINDEX12(x) (((x) >> S_TIMERBACKOFFINDEX12) & M_TIMERBACKOFFINDEX12)
+
+#define A_TP_PARA_REG0 0x7d60
+
+#define S_INITCWNDIDLE    27
+#define V_INITCWNDIDLE(x) ((x) << S_INITCWNDIDLE)
+#define F_INITCWNDIDLE    V_INITCWNDIDLE(1U)
+
+#define S_INITCWND    24
+#define M_INITCWND    0x7U
+#define V_INITCWND(x) ((x) << S_INITCWND)
+#define G_INITCWND(x) (((x) >> S_INITCWND) & M_INITCWND)
+
+#define S_DUPACKTHRESH    20
+#define M_DUPACKTHRESH    0xfU
+#define V_DUPACKTHRESH(x) ((x) << S_DUPACKTHRESH)
+#define G_DUPACKTHRESH(x) (((x) >> S_DUPACKTHRESH) & M_DUPACKTHRESH)
+
+#define S_CPLERRENABLE    12
+#define V_CPLERRENABLE(x) ((x) << S_CPLERRENABLE)
+#define F_CPLERRENABLE    V_CPLERRENABLE(1U)
+
+#define S_FASTTNLCNT    11
+#define V_FASTTNLCNT(x) ((x) << S_FASTTNLCNT)
+#define F_FASTTNLCNT    V_FASTTNLCNT(1U)
+
+#define S_FASTTBLCNT    10
+#define V_FASTTBLCNT(x) ((x) << S_FASTTBLCNT)
+#define F_FASTTBLCNT    V_FASTTBLCNT(1U)
+
+#define S_TPTCAMKEY    9
+#define V_TPTCAMKEY(x) ((x) << S_TPTCAMKEY)
+#define F_TPTCAMKEY    V_TPTCAMKEY(1U)
+
+#define S_SWSMODE    8
+#define V_SWSMODE(x) ((x) << S_SWSMODE)
+#define F_SWSMODE    V_SWSMODE(1U)
+
+#define S_TSMPMODE    6
+#define M_TSMPMODE    0x3U
+#define V_TSMPMODE(x) ((x) << S_TSMPMODE)
+#define G_TSMPMODE(x) (((x) >> S_TSMPMODE) & M_TSMPMODE)
+
+#define S_BYTECOUNTLIMIT    4
+#define M_BYTECOUNTLIMIT    0x3U
+#define V_BYTECOUNTLIMIT(x) ((x) << S_BYTECOUNTLIMIT)
+#define G_BYTECOUNTLIMIT(x) (((x) >> S_BYTECOUNTLIMIT) & M_BYTECOUNTLIMIT)
+
+#define S_SWSSHOVE    3
+#define V_SWSSHOVE(x) ((x) << S_SWSSHOVE)
+#define F_SWSSHOVE    V_SWSSHOVE(1U)
+
+#define S_TBLTIMER    2
+#define V_TBLTIMER(x) ((x) << S_TBLTIMER)
+#define F_TBLTIMER    V_TBLTIMER(1U)
+
+#define S_RXTPACE    1
+#define V_RXTPACE(x) ((x) << S_RXTPACE)
+#define F_RXTPACE    V_RXTPACE(1U)
+
+#define S_SWSTIMER    0
+#define V_SWSTIMER(x) ((x) << S_SWSTIMER)
+#define F_SWSTIMER    V_SWSTIMER(1U)
+
+#define A_TP_PARA_REG1 0x7d64
+
+#define S_INITRWND    16
+#define M_INITRWND    0xffffU
+#define V_INITRWND(x) ((x) << S_INITRWND)
+#define G_INITRWND(x) (((x) >> S_INITRWND) & M_INITRWND)
+
+#define S_INITIALSSTHRESH    0
+#define M_INITIALSSTHRESH    0xffffU
+#define V_INITIALSSTHRESH(x) ((x) << S_INITIALSSTHRESH)
+#define G_INITIALSSTHRESH(x) (((x) >> S_INITIALSSTHRESH) & M_INITIALSSTHRESH)
+
+#define A_TP_PARA_REG2 0x7d68
+
+#define S_MAXRXDATA    16
+#define M_MAXRXDATA    0xffffU
+#define V_MAXRXDATA(x) ((x) << S_MAXRXDATA)
+#define G_MAXRXDATA(x) (((x) >> S_MAXRXDATA) & M_MAXRXDATA)
+
+#define S_RXCOALESCESIZE    0
+#define M_RXCOALESCESIZE    0xffffU
+#define V_RXCOALESCESIZE(x) ((x) << S_RXCOALESCESIZE)
+#define G_RXCOALESCESIZE(x) (((x) >> S_RXCOALESCESIZE) & M_RXCOALESCESIZE)
+
+#define A_TP_PARA_REG3 0x7d6c
+
+#define S_ENABLETNLCNGLPBK    31
+#define V_ENABLETNLCNGLPBK(x) ((x) << S_ENABLETNLCNGLPBK)
+#define F_ENABLETNLCNGLPBK    V_ENABLETNLCNGLPBK(1U)
+
+#define S_ENABLETNLCNGFIFO    30
+#define V_ENABLETNLCNGFIFO(x) ((x) << S_ENABLETNLCNGFIFO)
+#define F_ENABLETNLCNGFIFO    V_ENABLETNLCNGFIFO(1U)
+
+#define S_ENABLETNLCNGHDR    29
+#define V_ENABLETNLCNGHDR(x) ((x) << S_ENABLETNLCNGHDR)
+#define F_ENABLETNLCNGHDR    V_ENABLETNLCNGHDR(1U)
+
+#define S_ENABLETNLCNGSGE    28
+#define V_ENABLETNLCNGSGE(x) ((x) << S_ENABLETNLCNGSGE)
+#define F_ENABLETNLCNGSGE    V_ENABLETNLCNGSGE(1U)
+
+#define S_RXMACCHECK    27
+#define V_RXMACCHECK(x) ((x) << S_RXMACCHECK)
+#define F_RXMACCHECK    V_RXMACCHECK(1U)
+
+#define S_RXSYNFILTER    26
+#define V_RXSYNFILTER(x) ((x) << S_RXSYNFILTER)
+#define F_RXSYNFILTER    V_RXSYNFILTER(1U)
+
+#define S_CNGCTRLECN    25
+#define V_CNGCTRLECN(x) ((x) << S_CNGCTRLECN)
+#define F_CNGCTRLECN    V_CNGCTRLECN(1U)
+
+#define S_RXDDPOFFINIT    24
+#define V_RXDDPOFFINIT(x) ((x) << S_RXDDPOFFINIT)
+#define F_RXDDPOFFINIT    V_RXDDPOFFINIT(1U)
+
+#define S_TUNNELCNGDROP3    23
+#define V_TUNNELCNGDROP3(x) ((x) << S_TUNNELCNGDROP3)
+#define F_TUNNELCNGDROP3    V_TUNNELCNGDROP3(1U)
+
+#define S_TUNNELCNGDROP2    22
+#define V_TUNNELCNGDROP2(x) ((x) << S_TUNNELCNGDROP2)
+#define F_TUNNELCNGDROP2    V_TUNNELCNGDROP2(1U)
+
+#define S_TUNNELCNGDROP1    21
+#define V_TUNNELCNGDROP1(x) ((x) << S_TUNNELCNGDROP1)
+#define F_TUNNELCNGDROP1    V_TUNNELCNGDROP1(1U)
+
+#define S_TUNNELCNGDROP0    20
+#define V_TUNNELCNGDROP0(x) ((x) << S_TUNNELCNGDROP0)
+#define F_TUNNELCNGDROP0    V_TUNNELCNGDROP0(1U)
+
+#define S_TXDATAACKIDX    16
+#define M_TXDATAACKIDX    0xfU
+#define V_TXDATAACKIDX(x) ((x) << S_TXDATAACKIDX)
+#define G_TXDATAACKIDX(x) (((x) >> S_TXDATAACKIDX) & M_TXDATAACKIDX)
+
+#define S_RXFRAGENABLE    12
+#define M_RXFRAGENABLE    0x7U
+#define V_RXFRAGENABLE(x) ((x) << S_RXFRAGENABLE)
+#define G_RXFRAGENABLE(x) (((x) >> S_RXFRAGENABLE) & M_RXFRAGENABLE)
+
+#define S_TXPACEFIXEDSTRICT    11
+#define V_TXPACEFIXEDSTRICT(x) ((x) << S_TXPACEFIXEDSTRICT)
+#define F_TXPACEFIXEDSTRICT    V_TXPACEFIXEDSTRICT(1U)
+
+#define S_TXPACEAUTOSTRICT    10
+#define V_TXPACEAUTOSTRICT(x) ((x) << S_TXPACEAUTOSTRICT)
+#define F_TXPACEAUTOSTRICT    V_TXPACEAUTOSTRICT(1U)
+
+#define S_TXPACEFIXED    9
+#define V_TXPACEFIXED(x) ((x) << S_TXPACEFIXED)
+#define F_TXPACEFIXED    V_TXPACEFIXED(1U)
+
+#define S_TXPACEAUTO    8
+#define V_TXPACEAUTO(x) ((x) << S_TXPACEAUTO)
+#define F_TXPACEAUTO    V_TXPACEAUTO(1U)
+
+#define S_RXCHNTUNNEL    7
+#define V_RXCHNTUNNEL(x) ((x) << S_RXCHNTUNNEL)
+#define F_RXCHNTUNNEL    V_RXCHNTUNNEL(1U)
+
+#define S_RXURGTUNNEL    6
+#define V_RXURGTUNNEL(x) ((x) << S_RXURGTUNNEL)
+#define F_RXURGTUNNEL    V_RXURGTUNNEL(1U)
+
+#define S_RXURGMODE    5
+#define V_RXURGMODE(x) ((x) << S_RXURGMODE)
+#define F_RXURGMODE    V_RXURGMODE(1U)
+
+#define S_TXURGMODE    4
+#define V_TXURGMODE(x) ((x) << S_TXURGMODE)
+#define F_TXURGMODE    V_TXURGMODE(1U)
+
+#define S_CNGCTRLMODE    2
+#define M_CNGCTRLMODE    0x3U
+#define V_CNGCTRLMODE(x) ((x) << S_CNGCTRLMODE)
+#define G_CNGCTRLMODE(x) (((x) >> S_CNGCTRLMODE) & M_CNGCTRLMODE)
+
+#define S_RXCOALESCEENABLE    1
+#define V_RXCOALESCEENABLE(x) ((x) << S_RXCOALESCEENABLE)
+#define F_RXCOALESCEENABLE    V_RXCOALESCEENABLE(1U)
+
+#define S_RXCOALESCEPSHEN    0
+#define V_RXCOALESCEPSHEN(x) ((x) << S_RXCOALESCEPSHEN)
+#define F_RXCOALESCEPSHEN    V_RXCOALESCEPSHEN(1U)
+
+#define A_TP_PARA_REG4 0x7d70
+
+#define S_HIGHSPEEDCFG    24
+#define M_HIGHSPEEDCFG    0xffU
+#define V_HIGHSPEEDCFG(x) ((x) << S_HIGHSPEEDCFG)
+#define G_HIGHSPEEDCFG(x) (((x) >> S_HIGHSPEEDCFG) & M_HIGHSPEEDCFG)
+
+#define S_NEWRENOCFG    16
+#define M_NEWRENOCFG    0xffU
+#define V_NEWRENOCFG(x) ((x) << S_NEWRENOCFG)
+#define G_NEWRENOCFG(x) (((x) >> S_NEWRENOCFG) & M_NEWRENOCFG)
+
+#define S_TAHOECFG    8
+#define M_TAHOECFG    0xffU
+#define V_TAHOECFG(x) ((x) << S_TAHOECFG)
+#define G_TAHOECFG(x) (((x) >> S_TAHOECFG) & M_TAHOECFG)
+
+#define S_RENOCFG    0
+#define M_RENOCFG    0xffU
+#define V_RENOCFG(x) ((x) << S_RENOCFG)
+#define G_RENOCFG(x) (((x) >> S_RENOCFG) & M_RENOCFG)
+
+#define A_TP_PARA_REG5 0x7d74
+
+#define S_INDICATESIZE    16
+#define M_INDICATESIZE    0xffffU
+#define V_INDICATESIZE(x) ((x) << S_INDICATESIZE)
+#define G_INDICATESIZE(x) (((x) >> S_INDICATESIZE) & M_INDICATESIZE)
+
+#define S_MAXPROXYSIZE    12
+#define M_MAXPROXYSIZE    0xfU
+#define V_MAXPROXYSIZE(x) ((x) << S_MAXPROXYSIZE)
+#define G_MAXPROXYSIZE(x) (((x) >> S_MAXPROXYSIZE) & M_MAXPROXYSIZE)
+
+#define S_ENABLEREADPDU    11
+#define V_ENABLEREADPDU(x) ((x) << S_ENABLEREADPDU)
+#define F_ENABLEREADPDU    V_ENABLEREADPDU(1U)
+
+#define S_RXREADAHEAD    10
+#define V_RXREADAHEAD(x) ((x) << S_RXREADAHEAD)
+#define F_RXREADAHEAD    V_RXREADAHEAD(1U)
+
+#define S_EMPTYRQENABLE    9
+#define V_EMPTYRQENABLE(x) ((x) << S_EMPTYRQENABLE)
+#define F_EMPTYRQENABLE    V_EMPTYRQENABLE(1U)
+
+#define S_SCHDENABLE    8
+#define V_SCHDENABLE(x) ((x) << S_SCHDENABLE)
+#define F_SCHDENABLE    V_SCHDENABLE(1U)
+
+#define S_REARMDDPOFFSET    4
+#define V_REARMDDPOFFSET(x) ((x) << S_REARMDDPOFFSET)
+#define F_REARMDDPOFFSET    V_REARMDDPOFFSET(1U)
+
+#define S_RESETDDPOFFSET    3
+#define V_RESETDDPOFFSET(x) ((x) << S_RESETDDPOFFSET)
+#define F_RESETDDPOFFSET    V_RESETDDPOFFSET(1U)
+
+#define S_ONFLYDDPENABLE    2
+#define V_ONFLYDDPENABLE(x) ((x) << S_ONFLYDDPENABLE)
+#define F_ONFLYDDPENABLE    V_ONFLYDDPENABLE(1U)
+
+#define S_DACKTIMERSPIN    1
+#define V_DACKTIMERSPIN(x) ((x) << S_DACKTIMERSPIN)
+#define F_DACKTIMERSPIN    V_DACKTIMERSPIN(1U)
+
+#define S_PUSHTIMERENABLE    0
+#define V_PUSHTIMERENABLE(x) ((x) << S_PUSHTIMERENABLE)
+#define F_PUSHTIMERENABLE    V_PUSHTIMERENABLE(1U)
+
+#define A_TP_PARA_REG6 0x7d78
+
+#define S_TXPDUSIZEADJ    24
+#define M_TXPDUSIZEADJ    0xffU
+#define V_TXPDUSIZEADJ(x) ((x) << S_TXPDUSIZEADJ)
+#define G_TXPDUSIZEADJ(x) (((x) >> S_TXPDUSIZEADJ) & M_TXPDUSIZEADJ)
+
+#define S_LIMITEDTRANSMIT    20
+#define M_LIMITEDTRANSMIT    0xfU
+#define V_LIMITEDTRANSMIT(x) ((x) << S_LIMITEDTRANSMIT)
+#define G_LIMITEDTRANSMIT(x) (((x) >> S_LIMITEDTRANSMIT) & M_LIMITEDTRANSMIT)
+
+#define S_ENABLECSAV    19
+#define V_ENABLECSAV(x) ((x) << S_ENABLECSAV)
+#define F_ENABLECSAV    V_ENABLECSAV(1U)
+
+#define S_ENABLEDEFERPDU    18
+#define V_ENABLEDEFERPDU(x) ((x) << S_ENABLEDEFERPDU)
+#define F_ENABLEDEFERPDU    V_ENABLEDEFERPDU(1U)
+
+#define S_ENABLEFLUSH    17
+#define V_ENABLEFLUSH(x) ((x) << S_ENABLEFLUSH)
+#define F_ENABLEFLUSH    V_ENABLEFLUSH(1U)
+
+#define S_ENABLEBYTEPERSIST    16
+#define V_ENABLEBYTEPERSIST(x) ((x) << S_ENABLEBYTEPERSIST)
+#define F_ENABLEBYTEPERSIST    V_ENABLEBYTEPERSIST(1U)
+
+#define S_DISABLETMOCNG    15
+#define V_DISABLETMOCNG(x) ((x) << S_DISABLETMOCNG)
+#define F_DISABLETMOCNG    V_DISABLETMOCNG(1U)
+
+#define S_TXREADAHEAD    14
+#define V_TXREADAHEAD(x) ((x) << S_TXREADAHEAD)
+#define F_TXREADAHEAD    V_TXREADAHEAD(1U)
+
+#define S_ALLOWEXEPTION    13
+#define V_ALLOWEXEPTION(x) ((x) << S_ALLOWEXEPTION)
+#define F_ALLOWEXEPTION    V_ALLOWEXEPTION(1U)
+
+#define S_ENABLEDEFERACK    12
+#define V_ENABLEDEFERACK(x) ((x) << S_ENABLEDEFERACK)
+#define F_ENABLEDEFERACK    V_ENABLEDEFERACK(1U)
+
+#define S_ENABLEESND    11
+#define V_ENABLEESND(x) ((x) << S_ENABLEESND)
+#define F_ENABLEESND    V_ENABLEESND(1U)
+
+#define S_ENABLECSND    10
+#define V_ENABLECSND(x) ((x) << S_ENABLECSND)
+#define F_ENABLECSND    V_ENABLECSND(1U)
+
+#define S_ENABLEPDUE    9
+#define V_ENABLEPDUE(x) ((x) << S_ENABLEPDUE)
+#define F_ENABLEPDUE    V_ENABLEPDUE(1U)
+
+#define S_ENABLEPDUC    8
+#define V_ENABLEPDUC(x) ((x) << S_ENABLEPDUC)
+#define F_ENABLEPDUC    V_ENABLEPDUC(1U)
+
+#define S_ENABLEBUFI    7
+#define V_ENABLEBUFI(x) ((x) << S_ENABLEBUFI)
+#define F_ENABLEBUFI    V_ENABLEBUFI(1U)
+
+#define S_ENABLEBUFE    6
+#define V_ENABLEBUFE(x) ((x) << S_ENABLEBUFE)
+#define F_ENABLEBUFE    V_ENABLEBUFE(1U)
+
+#define S_ENABLEDEFER    5
+#define V_ENABLEDEFER(x) ((x) << S_ENABLEDEFER)
+#define F_ENABLEDEFER    V_ENABLEDEFER(1U)
+
+#define S_ENABLECLEARRXMTOOS    4
+#define V_ENABLECLEARRXMTOOS(x) ((x) << S_ENABLECLEARRXMTOOS)
+#define F_ENABLECLEARRXMTOOS    V_ENABLECLEARRXMTOOS(1U)
+
+#define S_DISABLEPDUCNG    3
+#define V_DISABLEPDUCNG(x) ((x) << S_DISABLEPDUCNG)
+#define F_DISABLEPDUCNG    V_DISABLEPDUCNG(1U)
+
+#define S_DISABLEPDUTIMEOUT    2
+#define V_DISABLEPDUTIMEOUT(x) ((x) << S_DISABLEPDUTIMEOUT)
+#define F_DISABLEPDUTIMEOUT    V_DISABLEPDUTIMEOUT(1U)
+
+#define S_DISABLEPDURXMT    1
+#define V_DISABLEPDURXMT(x) ((x) << S_DISABLEPDURXMT)
+#define F_DISABLEPDURXMT    V_DISABLEPDURXMT(1U)
+
+#define S_DISABLEPDUXMT    0
+#define V_DISABLEPDUXMT(x) ((x) << S_DISABLEPDUXMT)
+#define F_DISABLEPDUXMT    V_DISABLEPDUXMT(1U)
+
+#define A_TP_PARA_REG7 0x7d7c
+
+#define S_PMMAXXFERLEN1    16
+#define M_PMMAXXFERLEN1    0xffffU
+#define V_PMMAXXFERLEN1(x) ((x) << S_PMMAXXFERLEN1)
+#define G_PMMAXXFERLEN1(x) (((x) >> S_PMMAXXFERLEN1) & M_PMMAXXFERLEN1)
+
+#define S_PMMAXXFERLEN0    0
+#define M_PMMAXXFERLEN0    0xffffU
+#define V_PMMAXXFERLEN0(x) ((x) << S_PMMAXXFERLEN0)
+#define G_PMMAXXFERLEN0(x) (((x) >> S_PMMAXXFERLEN0) & M_PMMAXXFERLEN0)
+
+#define A_TP_ENG_CONFIG 0x7d80
+
+#define S_TABLELATENCYDONE    28
+#define M_TABLELATENCYDONE    0xfU
+#define V_TABLELATENCYDONE(x) ((x) << S_TABLELATENCYDONE)
+#define G_TABLELATENCYDONE(x) (((x) >> S_TABLELATENCYDONE) & M_TABLELATENCYDONE)
+
+#define S_TABLELATENCYSTART    24
+#define M_TABLELATENCYSTART    0xfU
+#define V_TABLELATENCYSTART(x) ((x) << S_TABLELATENCYSTART)
+#define G_TABLELATENCYSTART(x) (((x) >> S_TABLELATENCYSTART) & M_TABLELATENCYSTART)
+
+#define S_ENGINELATENCYDELTA    16
+#define M_ENGINELATENCYDELTA    0xfU
+#define V_ENGINELATENCYDELTA(x) ((x) << S_ENGINELATENCYDELTA)
+#define G_ENGINELATENCYDELTA(x) (((x) >> S_ENGINELATENCYDELTA) & M_ENGINELATENCYDELTA)
+
+#define S_ENGINELATENCYMMGR    12
+#define M_ENGINELATENCYMMGR    0xfU
+#define V_ENGINELATENCYMMGR(x) ((x) << S_ENGINELATENCYMMGR)
+#define G_ENGINELATENCYMMGR(x) (((x) >> S_ENGINELATENCYMMGR) & M_ENGINELATENCYMMGR)
+
+#define S_ENGINELATENCYWIREIP6    8
+#define M_ENGINELATENCYWIREIP6    0xfU
+#define V_ENGINELATENCYWIREIP6(x) ((x) << S_ENGINELATENCYWIREIP6)
+#define G_ENGINELATENCYWIREIP6(x) (((x) >> S_ENGINELATENCYWIREIP6) & M_ENGINELATENCYWIREIP6)
+
+#define S_ENGINELATENCYWIRE    4
+#define M_ENGINELATENCYWIRE    0xfU
+#define V_ENGINELATENCYWIRE(x) ((x) << S_ENGINELATENCYWIRE)
+#define G_ENGINELATENCYWIRE(x) (((x) >> S_ENGINELATENCYWIRE) & M_ENGINELATENCYWIRE)
+
+#define S_ENGINELATENCYBASE    0
+#define M_ENGINELATENCYBASE    0xfU
+#define V_ENGINELATENCYBASE(x) ((x) << S_ENGINELATENCYBASE)
+#define G_ENGINELATENCYBASE(x) (((x) >> S_ENGINELATENCYBASE) & M_ENGINELATENCYBASE)
+
+#define A_TP_ERR_CONFIG 0x7d8c
+
+#define S_TNLERRORPING    30
+#define V_TNLERRORPING(x) ((x) << S_TNLERRORPING)
+#define F_TNLERRORPING    V_TNLERRORPING(1U)
+
+#define S_TNLERRORCSUM    29
+#define V_TNLERRORCSUM(x) ((x) << S_TNLERRORCSUM)
+#define F_TNLERRORCSUM    V_TNLERRORCSUM(1U)
+
+#define S_TNLERRORCSUMIP    28
+#define V_TNLERRORCSUMIP(x) ((x) << S_TNLERRORCSUMIP)
+#define F_TNLERRORCSUMIP    V_TNLERRORCSUMIP(1U)
+
+#define S_TNLERRORTCPOPT    25
+#define V_TNLERRORTCPOPT(x) ((x) << S_TNLERRORTCPOPT)
+#define F_TNLERRORTCPOPT    V_TNLERRORTCPOPT(1U)
+
+#define S_TNLERRORPKTLEN    24
+#define V_TNLERRORPKTLEN(x) ((x) << S_TNLERRORPKTLEN)
+#define F_TNLERRORPKTLEN    V_TNLERRORPKTLEN(1U)
+
+#define S_TNLERRORTCPHDRLEN    23
+#define V_TNLERRORTCPHDRLEN(x) ((x) << S_TNLERRORTCPHDRLEN)
+#define F_TNLERRORTCPHDRLEN    V_TNLERRORTCPHDRLEN(1U)
+
+#define S_TNLERRORIPHDRLEN    22
+#define V_TNLERRORIPHDRLEN(x) ((x) << S_TNLERRORIPHDRLEN)
+#define F_TNLERRORIPHDRLEN    V_TNLERRORIPHDRLEN(1U)
+
+#define S_TNLERRORETHHDRLEN    21
+#define V_TNLERRORETHHDRLEN(x) ((x) << S_TNLERRORETHHDRLEN)
+#define F_TNLERRORETHHDRLEN    V_TNLERRORETHHDRLEN(1U)
+
+#define S_TNLERRORATTACK    20
+#define V_TNLERRORATTACK(x) ((x) << S_TNLERRORATTACK)
+#define F_TNLERRORATTACK    V_TNLERRORATTACK(1U)
+
+#define S_TNLERRORFRAG    19
+#define V_TNLERRORFRAG(x) ((x) << S_TNLERRORFRAG)
+#define F_TNLERRORFRAG    V_TNLERRORFRAG(1U)
+
+#define S_TNLERRORIPVER    18
+#define V_TNLERRORIPVER(x) ((x) << S_TNLERRORIPVER)
+#define F_TNLERRORIPVER    V_TNLERRORIPVER(1U)
+
+#define S_TNLERRORMAC    17
+#define V_TNLERRORMAC(x) ((x) << S_TNLERRORMAC)
+#define F_TNLERRORMAC    V_TNLERRORMAC(1U)
+
+#define S_TNLERRORANY    16
+#define V_TNLERRORANY(x) ((x) << S_TNLERRORANY)
+#define F_TNLERRORANY    V_TNLERRORANY(1U)
+
+#define S_DROPERRORPING    14
+#define V_DROPERRORPING(x) ((x) << S_DROPERRORPING)
+#define F_DROPERRORPING    V_DROPERRORPING(1U)
+
+#define S_DROPERRORCSUM    13
+#define V_DROPERRORCSUM(x) ((x) << S_DROPERRORCSUM)
+#define F_DROPERRORCSUM    V_DROPERRORCSUM(1U)
+
+#define S_DROPERRORCSUMIP    12
+#define V_DROPERRORCSUMIP(x) ((x) << S_DROPERRORCSUMIP)
+#define F_DROPERRORCSUMIP    V_DROPERRORCSUMIP(1U)
+
+#define S_DROPERRORTCPOPT    9
+#define V_DROPERRORTCPOPT(x) ((x) << S_DROPERRORTCPOPT)
+#define F_DROPERRORTCPOPT    V_DROPERRORTCPOPT(1U)
+
+#define S_DROPERRORPKTLEN    8
+#define V_DROPERRORPKTLEN(x) ((x) << S_DROPERRORPKTLEN)
+#define F_DROPERRORPKTLEN    V_DROPERRORPKTLEN(1U)
+
+#define S_DROPERRORTCPHDRLEN    7
+#define V_DROPERRORTCPHDRLEN(x) ((x) << S_DROPERRORTCPHDRLEN)
+#define F_DROPERRORTCPHDRLEN    V_DROPERRORTCPHDRLEN(1U)
+
+#define S_DROPERRORIPHDRLEN    6
+#define V_DROPERRORIPHDRLEN(x) ((x) << S_DROPERRORIPHDRLEN)
+#define F_DROPERRORIPHDRLEN    V_DROPERRORIPHDRLEN(1U)
+
+#define S_DROPERRORETHHDRLEN    5
+#define V_DROPERRORETHHDRLEN(x) ((x) << S_DROPERRORETHHDRLEN)
+#define F_DROPERRORETHHDRLEN    V_DROPERRORETHHDRLEN(1U)
+
+#define S_DROPERRORATTACK    4
+#define V_DROPERRORATTACK(x) ((x) << S_DROPERRORATTACK)
+#define F_DROPERRORATTACK    V_DROPERRORATTACK(1U)
+
+#define S_DROPERRORFRAG    3
+#define V_DROPERRORFRAG(x) ((x) << S_DROPERRORFRAG)
+#define F_DROPERRORFRAG    V_DROPERRORFRAG(1U)
+
+#define S_DROPERRORIPVER    2
+#define V_DROPERRORIPVER(x) ((x) << S_DROPERRORIPVER)
+#define F_DROPERRORIPVER    V_DROPERRORIPVER(1U)
+
+#define S_DROPERRORMAC    1
+#define V_DROPERRORMAC(x) ((x) << S_DROPERRORMAC)
+#define F_DROPERRORMAC    V_DROPERRORMAC(1U)
+
+#define S_DROPERRORANY    0
+#define V_DROPERRORANY(x) ((x) << S_DROPERRORANY)
+#define F_DROPERRORANY    V_DROPERRORANY(1U)
+
+#define A_TP_TIMER_RESOLUTION 0x7d90
+
+#define S_TIMERRESOLUTION    16
+#define M_TIMERRESOLUTION    0xffU
+#define V_TIMERRESOLUTION(x) ((x) << S_TIMERRESOLUTION)
+#define G_TIMERRESOLUTION(x) (((x) >> S_TIMERRESOLUTION) & M_TIMERRESOLUTION)
+
+#define S_TIMESTAMPRESOLUTION    8
+#define M_TIMESTAMPRESOLUTION    0xffU
+#define V_TIMESTAMPRESOLUTION(x) ((x) << S_TIMESTAMPRESOLUTION)
+#define G_TIMESTAMPRESOLUTION(x) (((x) >> S_TIMESTAMPRESOLUTION) & M_TIMESTAMPRESOLUTION)
+
+#define S_DELAYEDACKRESOLUTION    0
+#define M_DELAYEDACKRESOLUTION    0xffU
+#define V_DELAYEDACKRESOLUTION(x) ((x) << S_DELAYEDACKRESOLUTION)
+#define G_DELAYEDACKRESOLUTION(x) (((x) >> S_DELAYEDACKRESOLUTION) & M_DELAYEDACKRESOLUTION)
+
+#define A_TP_MSL 0x7d94
+
+#define S_MSL    0
+#define M_MSL    0x3fffffffU
+#define V_MSL(x) ((x) << S_MSL)
+#define G_MSL(x) (((x) >> S_MSL) & M_MSL)
+
+#define A_TP_RXT_MIN 0x7d98
+
+#define S_RXTMIN    0
+#define M_RXTMIN    0x3fffffffU
+#define V_RXTMIN(x) ((x) << S_RXTMIN)
+#define G_RXTMIN(x) (((x) >> S_RXTMIN) & M_RXTMIN)
+
+#define A_TP_RXT_MAX 0x7d9c
+
+#define S_RXTMAX    0
+#define M_RXTMAX    0x3fffffffU
+#define V_RXTMAX(x) ((x) << S_RXTMAX)
+#define G_RXTMAX(x) (((x) >> S_RXTMAX) & M_RXTMAX)
+
+#define A_TP_PERS_MIN 0x7da0
+
+#define S_PERSMIN    0
+#define M_PERSMIN    0x3fffffffU
+#define V_PERSMIN(x) ((x) << S_PERSMIN)
+#define G_PERSMIN(x) (((x) >> S_PERSMIN) & M_PERSMIN)
+
+#define A_TP_PERS_MAX 0x7da4
+
+#define S_PERSMAX    0
+#define M_PERSMAX    0x3fffffffU
+#define V_PERSMAX(x) ((x) << S_PERSMAX)
+#define G_PERSMAX(x) (((x) >> S_PERSMAX) & M_PERSMAX)
+
+#define A_TP_KEEP_IDLE 0x7da8
+
+#define S_KEEPALIVEIDLE    0
+#define M_KEEPALIVEIDLE    0x3fffffffU
+#define V_KEEPALIVEIDLE(x) ((x) << S_KEEPALIVEIDLE)
+#define G_KEEPALIVEIDLE(x) (((x) >> S_KEEPALIVEIDLE) & M_KEEPALIVEIDLE)
+
+#define A_TP_KEEP_INTVL 0x7dac
+
+#define S_KEEPALIVEINTVL    0
+#define M_KEEPALIVEINTVL    0x3fffffffU
+#define V_KEEPALIVEINTVL(x) ((x) << S_KEEPALIVEINTVL)
+#define G_KEEPALIVEINTVL(x) (((x) >> S_KEEPALIVEINTVL) & M_KEEPALIVEINTVL)
+
+#define A_TP_INIT_SRTT 0x7db0
+
+#define S_MAXRTT    16
+#define M_MAXRTT    0xffffU
+#define V_MAXRTT(x) ((x) << S_MAXRTT)
+#define G_MAXRTT(x) (((x) >> S_MAXRTT) & M_MAXRTT)
+
+#define S_INITSRTT    0
+#define M_INITSRTT    0xffffU
+#define V_INITSRTT(x) ((x) << S_INITSRTT)
+#define G_INITSRTT(x) (((x) >> S_INITSRTT) & M_INITSRTT)
+
+#define A_TP_DACK_TIMER 0x7db4
+
+#define S_DACKTIME    0
+#define M_DACKTIME    0xfffU
+#define V_DACKTIME(x) ((x) << S_DACKTIME)
+#define G_DACKTIME(x) (((x) >> S_DACKTIME) & M_DACKTIME)
+
+#define A_TP_FINWAIT2_TIMER 0x7db8
+
+#define S_FINWAIT2TIME    0
+#define M_FINWAIT2TIME    0x3fffffffU
+#define V_FINWAIT2TIME(x) ((x) << S_FINWAIT2TIME)
+#define G_FINWAIT2TIME(x) (((x) >> S_FINWAIT2TIME) & M_FINWAIT2TIME)
+
+#define A_TP_FAST_FINWAIT2_TIMER 0x7dbc
+
+#define S_FASTFINWAIT2TIME    0
+#define M_FASTFINWAIT2TIME    0x3fffffffU
+#define V_FASTFINWAIT2TIME(x) ((x) << S_FASTFINWAIT2TIME)
+#define G_FASTFINWAIT2TIME(x) (((x) >> S_FASTFINWAIT2TIME) & M_FASTFINWAIT2TIME)
+
+#define A_TP_SHIFT_CNT 0x7dc0
+
+#define S_SYNSHIFTMAX    24
+#define M_SYNSHIFTMAX    0xffU
+#define V_SYNSHIFTMAX(x) ((x) << S_SYNSHIFTMAX)
+#define G_SYNSHIFTMAX(x) (((x) >> S_SYNSHIFTMAX) & M_SYNSHIFTMAX)
+
+#define S_RXTSHIFTMAXR1    20
+#define M_RXTSHIFTMAXR1    0xfU
+#define V_RXTSHIFTMAXR1(x) ((x) << S_RXTSHIFTMAXR1)
+#define G_RXTSHIFTMAXR1(x) (((x) >> S_RXTSHIFTMAXR1) & M_RXTSHIFTMAXR1)
+
+#define S_RXTSHIFTMAXR2    16
+#define M_RXTSHIFTMAXR2    0xfU
+#define V_RXTSHIFTMAXR2(x) ((x) << S_RXTSHIFTMAXR2)
+#define G_RXTSHIFTMAXR2(x) (((x) >> S_RXTSHIFTMAXR2) & M_RXTSHIFTMAXR2)
+
+#define S_PERSHIFTBACKOFFMAX    12
+#define M_PERSHIFTBACKOFFMAX    0xfU
+#define V_PERSHIFTBACKOFFMAX(x) ((x) << S_PERSHIFTBACKOFFMAX)
+#define G_PERSHIFTBACKOFFMAX(x) (((x) >> S_PERSHIFTBACKOFFMAX) & M_PERSHIFTBACKOFFMAX)
+
+#define S_PERSHIFTMAX    8
+#define M_PERSHIFTMAX    0xfU
+#define V_PERSHIFTMAX(x) ((x) << S_PERSHIFTMAX)
+#define G_PERSHIFTMAX(x) (((x) >> S_PERSHIFTMAX) & M_PERSHIFTMAX)
+
+#define S_KEEPALIVEMAXR1    4
+#define M_KEEPALIVEMAXR1    0xfU
+#define V_KEEPALIVEMAXR1(x) ((x) << S_KEEPALIVEMAXR1)
+#define G_KEEPALIVEMAXR1(x) (((x) >> S_KEEPALIVEMAXR1) & M_KEEPALIVEMAXR1)
+
+#define S_KEEPALIVEMAXR2    0
+#define M_KEEPALIVEMAXR2    0xfU
+#define V_KEEPALIVEMAXR2(x) ((x) << S_KEEPALIVEMAXR2)
+#define G_KEEPALIVEMAXR2(x) (((x) >> S_KEEPALIVEMAXR2) & M_KEEPALIVEMAXR2)
+
+#define A_TP_TM_CONFIG 0x7dc4
+
+#define S_CMTIMERMAXNUM    0
+#define M_CMTIMERMAXNUM    0x7U
+#define V_CMTIMERMAXNUM(x) ((x) << S_CMTIMERMAXNUM)
+#define G_CMTIMERMAXNUM(x) (((x) >> S_CMTIMERMAXNUM) & M_CMTIMERMAXNUM)
+
+#define A_TP_TIME_LO 0x7dc8
+#define A_TP_TIME_HI 0x7dcc
+#define A_TP_PORT_MTU_0 0x7dd0
+
+#define S_PORT1MTUVALUE    16
+#define M_PORT1MTUVALUE    0xffffU
+#define V_PORT1MTUVALUE(x) ((x) << S_PORT1MTUVALUE)
+#define G_PORT1MTUVALUE(x) (((x) >> S_PORT1MTUVALUE) & M_PORT1MTUVALUE)
+
+#define S_PORT0MTUVALUE    0
+#define M_PORT0MTUVALUE    0xffffU
+#define V_PORT0MTUVALUE(x) ((x) << S_PORT0MTUVALUE)
+#define G_PORT0MTUVALUE(x) (((x) >> S_PORT0MTUVALUE) & M_PORT0MTUVALUE)
+
+#define A_TP_PORT_MTU_1 0x7dd4
+
+#define S_PORT3MTUVALUE    16
+#define M_PORT3MTUVALUE    0xffffU
+#define V_PORT3MTUVALUE(x) ((x) << S_PORT3MTUVALUE)
+#define G_PORT3MTUVALUE(x) (((x) >> S_PORT3MTUVALUE) & M_PORT3MTUVALUE)
+
+#define S_PORT2MTUVALUE    0
+#define M_PORT2MTUVALUE    0xffffU
+#define V_PORT2MTUVALUE(x) ((x) << S_PORT2MTUVALUE)
+#define G_PORT2MTUVALUE(x) (((x) >> S_PORT2MTUVALUE) & M_PORT2MTUVALUE)
+
+#define A_TP_PACE_TABLE 0x7dd8
+#define A_TP_CCTRL_TABLE 0x7ddc
+
+#define S_ROWINDEX    16
+#define M_ROWINDEX    0xffffU
+#define V_ROWINDEX(x) ((x) << S_ROWINDEX)
+#define G_ROWINDEX(x) (((x) >> S_ROWINDEX) & M_ROWINDEX)
+
+#define S_ROWVALUE    0
+#define M_ROWVALUE    0xffffU
+#define V_ROWVALUE(x) ((x) << S_ROWVALUE)
+#define G_ROWVALUE(x) (((x) >> S_ROWVALUE) & M_ROWVALUE)
+
+#define A_TP_MTU_TABLE 0x7de4
+
+#define S_MTUINDEX    24
+#define M_MTUINDEX    0xffU
+#define V_MTUINDEX(x) ((x) << S_MTUINDEX)
+#define G_MTUINDEX(x) (((x) >> S_MTUINDEX) & M_MTUINDEX)
+
+#define S_MTUWIDTH    16
+#define M_MTUWIDTH    0xfU
+#define V_MTUWIDTH(x) ((x) << S_MTUWIDTH)
+#define G_MTUWIDTH(x) (((x) >> S_MTUWIDTH) & M_MTUWIDTH)
+
+#define S_MTUVALUE    0
+#define M_MTUVALUE    0x3fffU
+#define V_MTUVALUE(x) ((x) << S_MTUVALUE)
+#define G_MTUVALUE(x) (((x) >> S_MTUVALUE) & M_MTUVALUE)
+
+#define A_TP_ULP_TABLE 0x7de8
+
+#define S_ULPTYPE7FIELD    28
+#define M_ULPTYPE7FIELD    0xfU
+#define V_ULPTYPE7FIELD(x) ((x) << S_ULPTYPE7FIELD)
+#define G_ULPTYPE7FIELD(x) (((x) >> S_ULPTYPE7FIELD) & M_ULPTYPE7FIELD)
+
+#define S_ULPTYPE6FIELD    24
+#define M_ULPTYPE6FIELD    0xfU
+#define V_ULPTYPE6FIELD(x) ((x) << S_ULPTYPE6FIELD)
+#define G_ULPTYPE6FIELD(x) (((x) >> S_ULPTYPE6FIELD) & M_ULPTYPE6FIELD)
+
+#define S_ULPTYPE5FIELD    20
+#define M_ULPTYPE5FIELD    0xfU
+#define V_ULPTYPE5FIELD(x) ((x) << S_ULPTYPE5FIELD)
+#define G_ULPTYPE5FIELD(x) (((x) >> S_ULPTYPE5FIELD) & M_ULPTYPE5FIELD)
+
+#define S_ULPTYPE4FIELD    16
+#define M_ULPTYPE4FIELD    0xfU
+#define V_ULPTYPE4FIELD(x) ((x) << S_ULPTYPE4FIELD)
+#define G_ULPTYPE4FIELD(x) (((x) >> S_ULPTYPE4FIELD) & M_ULPTYPE4FIELD)
+
+#define S_ULPTYPE3FIELD    12
+#define M_ULPTYPE3FIELD    0xfU
+#define V_ULPTYPE3FIELD(x) ((x) << S_ULPTYPE3FIELD)
+#define G_ULPTYPE3FIELD(x) (((x) >> S_ULPTYPE3FIELD) & M_ULPTYPE3FIELD)
+
+#define S_ULPTYPE2FIELD    8
+#define M_ULPTYPE2FIELD    0xfU
+#define V_ULPTYPE2FIELD(x) ((x) << S_ULPTYPE2FIELD)
+#define G_ULPTYPE2FIELD(x) (((x) >> S_ULPTYPE2FIELD) & M_ULPTYPE2FIELD)
+
+#define S_ULPTYPE1FIELD    4
+#define M_ULPTYPE1FIELD    0xfU
+#define V_ULPTYPE1FIELD(x) ((x) << S_ULPTYPE1FIELD)
+#define G_ULPTYPE1FIELD(x) (((x) >> S_ULPTYPE1FIELD) & M_ULPTYPE1FIELD)
+
+#define S_ULPTYPE0FIELD    0
+#define M_ULPTYPE0FIELD    0xfU
+#define V_ULPTYPE0FIELD(x) ((x) << S_ULPTYPE0FIELD)
+#define G_ULPTYPE0FIELD(x) (((x) >> S_ULPTYPE0FIELD) & M_ULPTYPE0FIELD)
+
+#define A_TP_RSS_LKP_TABLE 0x7dec
+
+#define S_LKPTBLROWVLD    31
+#define V_LKPTBLROWVLD(x) ((x) << S_LKPTBLROWVLD)
+#define F_LKPTBLROWVLD    V_LKPTBLROWVLD(1U)
+
+#define S_LKPTBLROWIDX    20
+#define M_LKPTBLROWIDX    0x3ffU
+#define V_LKPTBLROWIDX(x) ((x) << S_LKPTBLROWIDX)
+#define G_LKPTBLROWIDX(x) (((x) >> S_LKPTBLROWIDX) & M_LKPTBLROWIDX)
+
+#define S_LKPTBLQUEUE1    10
+#define M_LKPTBLQUEUE1    0x3ffU
+#define V_LKPTBLQUEUE1(x) ((x) << S_LKPTBLQUEUE1)
+#define G_LKPTBLQUEUE1(x) (((x) >> S_LKPTBLQUEUE1) & M_LKPTBLQUEUE1)
+
+#define S_LKPTBLQUEUE0    0
+#define M_LKPTBLQUEUE0    0x3ffU
+#define V_LKPTBLQUEUE0(x) ((x) << S_LKPTBLQUEUE0)
+#define G_LKPTBLQUEUE0(x) (((x) >> S_LKPTBLQUEUE0) & M_LKPTBLQUEUE0)
+
+#define A_TP_RSS_CONFIG 0x7df0
+
+#define S_TNL4TUPENIPV6    31
+#define V_TNL4TUPENIPV6(x) ((x) << S_TNL4TUPENIPV6)
+#define F_TNL4TUPENIPV6    V_TNL4TUPENIPV6(1U)
+
+#define S_TNL2TUPENIPV6    30
+#define V_TNL2TUPENIPV6(x) ((x) << S_TNL2TUPENIPV6)
+#define F_TNL2TUPENIPV6    V_TNL2TUPENIPV6(1U)
+
+#define S_TNL4TUPENIPV4    29
+#define V_TNL4TUPENIPV4(x) ((x) << S_TNL4TUPENIPV4)
+#define F_TNL4TUPENIPV4    V_TNL4TUPENIPV4(1U)
+
+#define S_TNL2TUPENIPV4    28
+#define V_TNL2TUPENIPV4(x) ((x) << S_TNL2TUPENIPV4)
+#define F_TNL2TUPENIPV4    V_TNL2TUPENIPV4(1U)
+
+#define S_TNLTCPSEL    27
+#define V_TNLTCPSEL(x) ((x) << S_TNLTCPSEL)
+#define F_TNLTCPSEL    V_TNLTCPSEL(1U)
+
+#define S_TNLIP6SEL    26
+#define V_TNLIP6SEL(x) ((x) << S_TNLIP6SEL)
+#define F_TNLIP6SEL    V_TNLIP6SEL(1U)
+
+#define S_TNLVRTSEL    25
+#define V_TNLVRTSEL(x) ((x) << S_TNLVRTSEL)
+#define F_TNLVRTSEL    V_TNLVRTSEL(1U)
+
+#define S_TNLMAPEN    24
+#define V_TNLMAPEN(x) ((x) << S_TNLMAPEN)
+#define F_TNLMAPEN    V_TNLMAPEN(1U)
+
+#define S_OFDHASHSAVE    19
+#define V_OFDHASHSAVE(x) ((x) << S_OFDHASHSAVE)
+#define F_OFDHASHSAVE    V_OFDHASHSAVE(1U)
+
+#define S_OFDVRTSEL    18
+#define V_OFDVRTSEL(x) ((x) << S_OFDVRTSEL)
+#define F_OFDVRTSEL    V_OFDVRTSEL(1U)
+
+#define S_OFDMAPEN    17
+#define V_OFDMAPEN(x) ((x) << S_OFDMAPEN)
+#define F_OFDMAPEN    V_OFDMAPEN(1U)
+
+#define S_OFDLKPEN    16
+#define V_OFDLKPEN(x) ((x) << S_OFDLKPEN)
+#define F_OFDLKPEN    V_OFDLKPEN(1U)
+
+#define S_SYN4TUPENIPV6    15
+#define V_SYN4TUPENIPV6(x) ((x) << S_SYN4TUPENIPV6)
+#define F_SYN4TUPENIPV6    V_SYN4TUPENIPV6(1U)
+
+#define S_SYN2TUPENIPV6    14
+#define V_SYN2TUPENIPV6(x) ((x) << S_SYN2TUPENIPV6)
+#define F_SYN2TUPENIPV6    V_SYN2TUPENIPV6(1U)
+
+#define S_SYN4TUPENIPV4    13
+#define V_SYN4TUPENIPV4(x) ((x) << S_SYN4TUPENIPV4)
+#define F_SYN4TUPENIPV4    V_SYN4TUPENIPV4(1U)
+
+#define S_SYN2TUPENIPV4    12
+#define V_SYN2TUPENIPV4(x) ((x) << S_SYN2TUPENIPV4)
+#define F_SYN2TUPENIPV4    V_SYN2TUPENIPV4(1U)
+
+#define S_SYNIP6SEL    11
+#define V_SYNIP6SEL(x) ((x) << S_SYNIP6SEL)
+#define F_SYNIP6SEL    V_SYNIP6SEL(1U)
+
+#define S_SYNVRTSEL    10
+#define V_SYNVRTSEL(x) ((x) << S_SYNVRTSEL)
+#define F_SYNVRTSEL    V_SYNVRTSEL(1U)
+
+#define S_SYNMAPEN    9
+#define V_SYNMAPEN(x) ((x) << S_SYNMAPEN)
+#define F_SYNMAPEN    V_SYNMAPEN(1U)
+
+#define S_SYNLKPEN    8
+#define V_SYNLKPEN(x) ((x) << S_SYNLKPEN)
+#define F_SYNLKPEN    V_SYNLKPEN(1U)
+
+#define S_CHANNELENABLE    7
+#define V_CHANNELENABLE(x) ((x) << S_CHANNELENABLE)
+#define F_CHANNELENABLE    V_CHANNELENABLE(1U)
+
+#define S_PORTENABLE    6
+#define V_PORTENABLE(x) ((x) << S_PORTENABLE)
+#define F_PORTENABLE    V_PORTENABLE(1U)
+
+#define S_TNLALLLOOKUP    5
+#define V_TNLALLLOOKUP(x) ((x) << S_TNLALLLOOKUP)
+#define F_TNLALLLOOKUP    V_TNLALLLOOKUP(1U)
+
+#define S_VIRTENABLE    4
+#define V_VIRTENABLE(x) ((x) << S_VIRTENABLE)
+#define F_VIRTENABLE    V_VIRTENABLE(1U)
+
+#define S_CONGESTIONENABLE    3
+#define V_CONGESTIONENABLE(x) ((x) << S_CONGESTIONENABLE)
+#define F_CONGESTIONENABLE    V_CONGESTIONENABLE(1U)
+
+#define S_HASHTOEPLITZ    2
+#define V_HASHTOEPLITZ(x) ((x) << S_HASHTOEPLITZ)
+#define F_HASHTOEPLITZ    V_HASHTOEPLITZ(1U)
+
+#define S_UDPENABLE    1
+#define V_UDPENABLE(x) ((x) << S_UDPENABLE)
+#define F_UDPENABLE    V_UDPENABLE(1U)
+
+#define S_DISABLE    0
+#define V_DISABLE(x) ((x) << S_DISABLE)
+#define F_DISABLE    V_DISABLE(1U)
+
+#define A_TP_RSS_CONFIG_TNL 0x7df4
+
+#define S_MASKSIZE    28
+#define M_MASKSIZE    0xfU
+#define V_MASKSIZE(x) ((x) << S_MASKSIZE)
+#define G_MASKSIZE(x) (((x) >> S_MASKSIZE) & M_MASKSIZE)
+
+#define S_MASKFILTER    16
+#define M_MASKFILTER    0x7ffU
+#define V_MASKFILTER(x) ((x) << S_MASKFILTER)
+#define G_MASKFILTER(x) (((x) >> S_MASKFILTER) & M_MASKFILTER)
+
+#define S_USEWIRECH    0
+#define V_USEWIRECH(x) ((x) << S_USEWIRECH)
+#define F_USEWIRECH    V_USEWIRECH(1U)
+
+#define A_TP_RSS_CONFIG_OFD 0x7df8
+
+#define S_RRCPLMAPEN    20
+#define V_RRCPLMAPEN(x) ((x) << S_RRCPLMAPEN)
+#define F_RRCPLMAPEN    V_RRCPLMAPEN(1U)
+
+#define S_RRCPLQUEWIDTH    16
+#define M_RRCPLQUEWIDTH    0xfU
+#define V_RRCPLQUEWIDTH(x) ((x) << S_RRCPLQUEWIDTH)
+#define G_RRCPLQUEWIDTH(x) (((x) >> S_RRCPLQUEWIDTH) & M_RRCPLQUEWIDTH)
+
+#define A_TP_RSS_CONFIG_SYN 0x7dfc
+#define A_TP_RSS_CONFIG_VRT 0x7e00
+
+#define S_VFRDRG    25
+#define V_VFRDRG(x) ((x) << S_VFRDRG)
+#define F_VFRDRG    V_VFRDRG(1U)
+
+#define S_VFRDEN    24
+#define V_VFRDEN(x) ((x) << S_VFRDEN)
+#define F_VFRDEN    V_VFRDEN(1U)
+
+#define S_VFPERREN    23
+#define V_VFPERREN(x) ((x) << S_VFPERREN)
+#define F_VFPERREN    V_VFPERREN(1U)
+
+#define S_KEYPERREN    22
+#define V_KEYPERREN(x) ((x) << S_KEYPERREN)
+#define F_KEYPERREN    V_KEYPERREN(1U)
+
+#define S_DISABLEVLAN    21
+#define V_DISABLEVLAN(x) ((x) << S_DISABLEVLAN)
+#define F_DISABLEVLAN    V_DISABLEVLAN(1U)
+
+#define S_ENABLEUP0    20
+#define V_ENABLEUP0(x) ((x) << S_ENABLEUP0)
+#define F_ENABLEUP0    V_ENABLEUP0(1U)
+
+#define S_HASHDELAY    16
+#define M_HASHDELAY    0xfU
+#define V_HASHDELAY(x) ((x) << S_HASHDELAY)
+#define G_HASHDELAY(x) (((x) >> S_HASHDELAY) & M_HASHDELAY)
+
+#define S_VFWRADDR    8
+#define M_VFWRADDR    0x7fU
+#define V_VFWRADDR(x) ((x) << S_VFWRADDR)
+#define G_VFWRADDR(x) (((x) >> S_VFWRADDR) & M_VFWRADDR)
+
+#define S_KEYMODE    6
+#define M_KEYMODE    0x3U
+#define V_KEYMODE(x) ((x) << S_KEYMODE)
+#define G_KEYMODE(x) (((x) >> S_KEYMODE) & M_KEYMODE)
+
+#define S_VFWREN    5
+#define V_VFWREN(x) ((x) << S_VFWREN)
+#define F_VFWREN    V_VFWREN(1U)
+
+#define S_KEYWREN    4
+#define V_KEYWREN(x) ((x) << S_KEYWREN)
+#define F_KEYWREN    V_KEYWREN(1U)
+
+#define S_KEYWRADDR    0
+#define M_KEYWRADDR    0xfU
+#define V_KEYWRADDR(x) ((x) << S_KEYWRADDR)
+#define G_KEYWRADDR(x) (((x) >> S_KEYWRADDR) & M_KEYWRADDR)
+
+#define A_TP_RSS_CONFIG_CNG 0x7e04
+
+#define S_CHNCOUNT3    31
+#define V_CHNCOUNT3(x) ((x) << S_CHNCOUNT3)
+#define F_CHNCOUNT3    V_CHNCOUNT3(1U)
+
+#define S_CHNCOUNT2    30
+#define V_CHNCOUNT2(x) ((x) << S_CHNCOUNT2)
+#define F_CHNCOUNT2    V_CHNCOUNT2(1U)
+
+#define S_CHNCOUNT1    29
+#define V_CHNCOUNT1(x) ((x) << S_CHNCOUNT1)
+#define F_CHNCOUNT1    V_CHNCOUNT1(1U)
+
+#define S_CHNCOUNT0    28
+#define V_CHNCOUNT0(x) ((x) << S_CHNCOUNT0)
+#define F_CHNCOUNT0    V_CHNCOUNT0(1U)
+
+#define S_CHNUNDFLOW3    27
+#define V_CHNUNDFLOW3(x) ((x) << S_CHNUNDFLOW3)
+#define F_CHNUNDFLOW3    V_CHNUNDFLOW3(1U)
+
+#define S_CHNUNDFLOW2    26
+#define V_CHNUNDFLOW2(x) ((x) << S_CHNUNDFLOW2)
+#define F_CHNUNDFLOW2    V_CHNUNDFLOW2(1U)
+
+#define S_CHNUNDFLOW1    25
+#define V_CHNUNDFLOW1(x) ((x) << S_CHNUNDFLOW1)
+#define F_CHNUNDFLOW1    V_CHNUNDFLOW1(1U)
+
+#define S_CHNUNDFLOW0    24
+#define V_CHNUNDFLOW0(x) ((x) << S_CHNUNDFLOW0)
+#define F_CHNUNDFLOW0    V_CHNUNDFLOW0(1U)
+
+#define S_CHNOVRFLOW3    23
+#define V_CHNOVRFLOW3(x) ((x) << S_CHNOVRFLOW3)
+#define F_CHNOVRFLOW3    V_CHNOVRFLOW3(1U)
+
+#define S_CHNOVRFLOW2    22
+#define V_CHNOVRFLOW2(x) ((x) << S_CHNOVRFLOW2)
+#define F_CHNOVRFLOW2    V_CHNOVRFLOW2(1U)
+
+#define S_CHNOVRFLOW1    21
+#define V_CHNOVRFLOW1(x) ((x) << S_CHNOVRFLOW1)
+#define F_CHNOVRFLOW1    V_CHNOVRFLOW1(1U)
+
+#define S_CHNOVRFLOW0    20
+#define V_CHNOVRFLOW0(x) ((x) << S_CHNOVRFLOW0)
+#define F_CHNOVRFLOW0    V_CHNOVRFLOW0(1U)
+
+#define S_RSTCHN3    19
+#define V_RSTCHN3(x) ((x) << S_RSTCHN3)
+#define F_RSTCHN3    V_RSTCHN3(1U)
+
+#define S_RSTCHN2    18
+#define V_RSTCHN2(x) ((x) << S_RSTCHN2)
+#define F_RSTCHN2    V_RSTCHN2(1U)
+
+#define S_RSTCHN1    17
+#define V_RSTCHN1(x) ((x) << S_RSTCHN1)
+#define F_RSTCHN1    V_RSTCHN1(1U)
+
+#define S_RSTCHN0    16
+#define V_RSTCHN0(x) ((x) << S_RSTCHN0)
+#define F_RSTCHN0    V_RSTCHN0(1U)
+
+#define S_UPDVLD    15
+#define V_UPDVLD(x) ((x) << S_UPDVLD)
+#define F_UPDVLD    V_UPDVLD(1U)
+
+#define S_XOFF    14
+#define V_XOFF(x) ((x) << S_XOFF)
+#define F_XOFF    V_XOFF(1U)
+
+#define S_UPDCHN3    13
+#define V_UPDCHN3(x) ((x) << S_UPDCHN3)
+#define F_UPDCHN3    V_UPDCHN3(1U)
+
+#define S_UPDCHN2    12
+#define V_UPDCHN2(x) ((x) << S_UPDCHN2)
+#define F_UPDCHN2    V_UPDCHN2(1U)
+
+#define S_UPDCHN1    11
+#define V_UPDCHN1(x) ((x) << S_UPDCHN1)
+#define F_UPDCHN1    V_UPDCHN1(1U)
+
+#define S_UPDCHN0    10
+#define V_UPDCHN0(x) ((x) << S_UPDCHN0)
+#define F_UPDCHN0    V_UPDCHN0(1U)
+
+#define S_QUEUE    0
+#define M_QUEUE    0x3ffU
+#define V_QUEUE(x) ((x) << S_QUEUE)
+#define G_QUEUE(x) (((x) >> S_QUEUE) & M_QUEUE)
+
+#define A_TP_LA_TABLE_0 0x7e10
+
+#define S_VIRTPORT1TABLE    16
+#define M_VIRTPORT1TABLE    0xffffU
+#define V_VIRTPORT1TABLE(x) ((x) << S_VIRTPORT1TABLE)
+#define G_VIRTPORT1TABLE(x) (((x) >> S_VIRTPORT1TABLE) & M_VIRTPORT1TABLE)
+
+#define S_VIRTPORT0TABLE    0
+#define M_VIRTPORT0TABLE    0xffffU
+#define V_VIRTPORT0TABLE(x) ((x) << S_VIRTPORT0TABLE)
+#define G_VIRTPORT0TABLE(x) (((x) >> S_VIRTPORT0TABLE) & M_VIRTPORT0TABLE)
+
+#define A_TP_LA_TABLE_1 0x7e14
+
+#define S_VIRTPORT3TABLE    16
+#define M_VIRTPORT3TABLE    0xffffU
+#define V_VIRTPORT3TABLE(x) ((x) << S_VIRTPORT3TABLE)
+#define G_VIRTPORT3TABLE(x) (((x) >> S_VIRTPORT3TABLE) & M_VIRTPORT3TABLE)
+
+#define S_VIRTPORT2TABLE    0
+#define M_VIRTPORT2TABLE    0xffffU
+#define V_VIRTPORT2TABLE(x) ((x) << S_VIRTPORT2TABLE)
+#define G_VIRTPORT2TABLE(x) (((x) >> S_VIRTPORT2TABLE) & M_VIRTPORT2TABLE)
+
+#define A_TP_TM_PIO_ADDR 0x7e18
+#define A_TP_TM_PIO_DATA 0x7e1c
+#define A_TP_MOD_CONFIG 0x7e24
+
+#define S_RXCHANNELWEIGHT1    24
+#define M_RXCHANNELWEIGHT1    0xffU
+#define V_RXCHANNELWEIGHT1(x) ((x) << S_RXCHANNELWEIGHT1)
+#define G_RXCHANNELWEIGHT1(x) (((x) >> S_RXCHANNELWEIGHT1) & M_RXCHANNELWEIGHT1)
+
+#define S_RXCHANNELWEIGHT0    16
+#define M_RXCHANNELWEIGHT0    0xffU
+#define V_RXCHANNELWEIGHT0(x) ((x) << S_RXCHANNELWEIGHT0)
+#define G_RXCHANNELWEIGHT0(x) (((x) >> S_RXCHANNELWEIGHT0) & M_RXCHANNELWEIGHT0)
+
+#define S_TIMERMODE    8
+#define M_TIMERMODE    0xffU
+#define V_TIMERMODE(x) ((x) << S_TIMERMODE)
+#define G_TIMERMODE(x) (((x) >> S_TIMERMODE) & M_TIMERMODE)
+
+#define S_TXCHANNELXOFFEN    0
+#define M_TXCHANNELXOFFEN    0xfU
+#define V_TXCHANNELXOFFEN(x) ((x) << S_TXCHANNELXOFFEN)
+#define G_TXCHANNELXOFFEN(x) (((x) >> S_TXCHANNELXOFFEN) & M_TXCHANNELXOFFEN)
+
+#define A_TP_TX_MOD_QUEUE_REQ_MAP 0x7e28
+
+#define S_RX_MOD_WEIGHT    24
+#define M_RX_MOD_WEIGHT    0xffU
+#define V_RX_MOD_WEIGHT(x) ((x) << S_RX_MOD_WEIGHT)
+#define G_RX_MOD_WEIGHT(x) (((x) >> S_RX_MOD_WEIGHT) & M_RX_MOD_WEIGHT)
+
+#define S_TX_MOD_WEIGHT    16
+#define M_TX_MOD_WEIGHT    0xffU
+#define V_TX_MOD_WEIGHT(x) ((x) << S_TX_MOD_WEIGHT)
+#define G_TX_MOD_WEIGHT(x) (((x) >> S_TX_MOD_WEIGHT) & M_TX_MOD_WEIGHT)
+
+#define S_TX_MOD_QUEUE_REQ_MAP    0
+#define M_TX_MOD_QUEUE_REQ_MAP    0xffffU
+#define V_TX_MOD_QUEUE_REQ_MAP(x) ((x) << S_TX_MOD_QUEUE_REQ_MAP)
+#define G_TX_MOD_QUEUE_REQ_MAP(x) (((x) >> S_TX_MOD_QUEUE_REQ_MAP) & M_TX_MOD_QUEUE_REQ_MAP)
+
+#define A_TP_TX_MOD_QUEUE_WEIGHT1 0x7e2c
+
+#define S_TX_MODQ_WEIGHT7    24
+#define M_TX_MODQ_WEIGHT7    0xffU
+#define V_TX_MODQ_WEIGHT7(x) ((x) << S_TX_MODQ_WEIGHT7)
+#define G_TX_MODQ_WEIGHT7(x) (((x) >> S_TX_MODQ_WEIGHT7) & M_TX_MODQ_WEIGHT7)
+
+#define S_TX_MODQ_WEIGHT6    16
+#define M_TX_MODQ_WEIGHT6    0xffU
+#define V_TX_MODQ_WEIGHT6(x) ((x) << S_TX_MODQ_WEIGHT6)
+#define G_TX_MODQ_WEIGHT6(x) (((x) >> S_TX_MODQ_WEIGHT6) & M_TX_MODQ_WEIGHT6)
+
+#define S_TX_MODQ_WEIGHT5    8
+#define M_TX_MODQ_WEIGHT5    0xffU
+#define V_TX_MODQ_WEIGHT5(x) ((x) << S_TX_MODQ_WEIGHT5)
+#define G_TX_MODQ_WEIGHT5(x) (((x) >> S_TX_MODQ_WEIGHT5) & M_TX_MODQ_WEIGHT5)
+
+#define S_TX_MODQ_WEIGHT4    0
+#define M_TX_MODQ_WEIGHT4    0xffU
+#define V_TX_MODQ_WEIGHT4(x) ((x) << S_TX_MODQ_WEIGHT4)
+#define G_TX_MODQ_WEIGHT4(x) (((x) >> S_TX_MODQ_WEIGHT4) & M_TX_MODQ_WEIGHT4)
+
+#define A_TP_TX_MOD_QUEUE_WEIGHT0 0x7e30
+
+#define S_TX_MODQ_WEIGHT3    24
+#define M_TX_MODQ_WEIGHT3    0xffU
+#define V_TX_MODQ_WEIGHT3(x) ((x) << S_TX_MODQ_WEIGHT3)
+#define G_TX_MODQ_WEIGHT3(x) (((x) >> S_TX_MODQ_WEIGHT3) & M_TX_MODQ_WEIGHT3)
+
+#define S_TX_MODQ_WEIGHT2    16
+#define M_TX_MODQ_WEIGHT2    0xffU
+#define V_TX_MODQ_WEIGHT2(x) ((x) << S_TX_MODQ_WEIGHT2)
+#define G_TX_MODQ_WEIGHT2(x) (((x) >> S_TX_MODQ_WEIGHT2) & M_TX_MODQ_WEIGHT2)
+
+#define S_TX_MODQ_WEIGHT1    8
+#define M_TX_MODQ_WEIGHT1    0xffU
+#define V_TX_MODQ_WEIGHT1(x) ((x) << S_TX_MODQ_WEIGHT1)
+#define G_TX_MODQ_WEIGHT1(x) (((x) >> S_TX_MODQ_WEIGHT1) & M_TX_MODQ_WEIGHT1)
+
+#define S_TX_MODQ_WEIGHT0    0
+#define M_TX_MODQ_WEIGHT0    0xffU
+#define V_TX_MODQ_WEIGHT0(x) ((x) << S_TX_MODQ_WEIGHT0)
+#define G_TX_MODQ_WEIGHT0(x) (((x) >> S_TX_MODQ_WEIGHT0) & M_TX_MODQ_WEIGHT0)
+
+#define A_TP_TX_MOD_CHANNEL_WEIGHT 0x7e34
+#define A_TP_MOD_RATE_LIMIT 0x7e38
+
+#define S_RX_MOD_RATE_LIMIT_INC    24
+#define M_RX_MOD_RATE_LIMIT_INC    0xffU
+#define V_RX_MOD_RATE_LIMIT_INC(x) ((x) << S_RX_MOD_RATE_LIMIT_INC)
+#define G_RX_MOD_RATE_LIMIT_INC(x) (((x) >> S_RX_MOD_RATE_LIMIT_INC) & M_RX_MOD_RATE_LIMIT_INC)
+
+#define S_RX_MOD_RATE_LIMIT_TICK    16
+#define M_RX_MOD_RATE_LIMIT_TICK    0xffU
+#define V_RX_MOD_RATE_LIMIT_TICK(x) ((x) << S_RX_MOD_RATE_LIMIT_TICK)
+#define G_RX_MOD_RATE_LIMIT_TICK(x) (((x) >> S_RX_MOD_RATE_LIMIT_TICK) & M_RX_MOD_RATE_LIMIT_TICK)
+
+#define S_TX_MOD_RATE_LIMIT_INC    8
+#define M_TX_MOD_RATE_LIMIT_INC    0xffU
+#define V_TX_MOD_RATE_LIMIT_INC(x) ((x) << S_TX_MOD_RATE_LIMIT_INC)
+#define G_TX_MOD_RATE_LIMIT_INC(x) (((x) >> S_TX_MOD_RATE_LIMIT_INC) & M_TX_MOD_RATE_LIMIT_INC)
+
+#define S_TX_MOD_RATE_LIMIT_TICK    0
+#define M_TX_MOD_RATE_LIMIT_TICK    0xffU
+#define V_TX_MOD_RATE_LIMIT_TICK(x) ((x) << S_TX_MOD_RATE_LIMIT_TICK)
+#define G_TX_MOD_RATE_LIMIT_TICK(x) (((x) >> S_TX_MOD_RATE_LIMIT_TICK) & M_TX_MOD_RATE_LIMIT_TICK)
+
+#define A_TP_PIO_ADDR 0x7e40
+#define A_TP_PIO_DATA 0x7e44
+#define A_TP_RESET 0x7e4c
+
+#define S_FLSTINITENABLE    1
+#define V_FLSTINITENABLE(x) ((x) << S_FLSTINITENABLE)
+#define F_FLSTINITENABLE    V_FLSTINITENABLE(1U)
+
+#define S_TPRESET    0
+#define V_TPRESET(x) ((x) << S_TPRESET)
+#define F_TPRESET    V_TPRESET(1U)
+
+#define A_TP_MIB_INDEX 0x7e50
+#define A_TP_MIB_DATA 0x7e54
+#define A_TP_SYNC_TIME_HI 0x7e58
+#define A_TP_SYNC_TIME_LO 0x7e5c
+#define A_TP_CMM_MM_RX_FLST_BASE 0x7e60
+#define A_TP_CMM_MM_TX_FLST_BASE 0x7e64
+#define A_TP_CMM_MM_PS_FLST_BASE 0x7e68
+#define A_TP_CMM_MM_MAX_PSTRUCT 0x7e6c
+
+#define S_CMMAXPSTRUCT    0
+#define M_CMMAXPSTRUCT    0x1fffffU
+#define V_CMMAXPSTRUCT(x) ((x) << S_CMMAXPSTRUCT)
+#define G_CMMAXPSTRUCT(x) (((x) >> S_CMMAXPSTRUCT) & M_CMMAXPSTRUCT)
+
+#define A_TP_INT_ENABLE 0x7e70
+
+#define S_FLMTXFLSTEMPTY    30
+#define V_FLMTXFLSTEMPTY(x) ((x) << S_FLMTXFLSTEMPTY)
+#define F_FLMTXFLSTEMPTY    V_FLMTXFLSTEMPTY(1U)
+
+#define S_RSSLKPPERR    29
+#define V_RSSLKPPERR(x) ((x) << S_RSSLKPPERR)
+#define F_RSSLKPPERR    V_RSSLKPPERR(1U)
+
+#define S_FLMPERRSET    28
+#define V_FLMPERRSET(x) ((x) << S_FLMPERRSET)
+#define F_FLMPERRSET    V_FLMPERRSET(1U)
+
+#define S_PROTOCOLSRAMPERR    27
+#define V_PROTOCOLSRAMPERR(x) ((x) << S_PROTOCOLSRAMPERR)
+#define F_PROTOCOLSRAMPERR    V_PROTOCOLSRAMPERR(1U)
+
+#define S_ARPLUTPERR    26
+#define V_ARPLUTPERR(x) ((x) << S_ARPLUTPERR)
+#define F_ARPLUTPERR    V_ARPLUTPERR(1U)
+
+#define S_CMRCFOPPERR    25
+#define V_CMRCFOPPERR(x) ((x) << S_CMRCFOPPERR)
+#define F_CMRCFOPPERR    V_CMRCFOPPERR(1U)
+
+#define S_CMCACHEPERR    24
+#define V_CMCACHEPERR(x) ((x) << S_CMCACHEPERR)
+#define F_CMCACHEPERR    V_CMCACHEPERR(1U)
+
+#define S_CMRCFDATAPERR    23
+#define V_CMRCFDATAPERR(x) ((x) << S_CMRCFDATAPERR)
+#define F_CMRCFDATAPERR    V_CMRCFDATAPERR(1U)
+
+#define S_DBL2TLUTPERR    22
+#define V_DBL2TLUTPERR(x) ((x) << S_DBL2TLUTPERR)
+#define F_DBL2TLUTPERR    V_DBL2TLUTPERR(1U)
+
+#define S_DBTXTIDPERR    21
+#define V_DBTXTIDPERR(x) ((x) << S_DBTXTIDPERR)
+#define F_DBTXTIDPERR    V_DBTXTIDPERR(1U)
+
+#define S_DBEXTPERR    20
+#define V_DBEXTPERR(x) ((x) << S_DBEXTPERR)
+#define F_DBEXTPERR    V_DBEXTPERR(1U)
+
+#define S_DBOPPERR    19
+#define V_DBOPPERR(x) ((x) << S_DBOPPERR)
+#define F_DBOPPERR    V_DBOPPERR(1U)
+
+#define S_TMCACHEPERR    18
+#define V_TMCACHEPERR(x) ((x) << S_TMCACHEPERR)
+#define F_TMCACHEPERR    V_TMCACHEPERR(1U)
+
+#define S_ETPOUTCPLFIFOPERR    17
+#define V_ETPOUTCPLFIFOPERR(x) ((x) << S_ETPOUTCPLFIFOPERR)
+#define F_ETPOUTCPLFIFOPERR    V_ETPOUTCPLFIFOPERR(1U)
+
+#define S_ETPOUTTCPFIFOPERR    16
+#define V_ETPOUTTCPFIFOPERR(x) ((x) << S_ETPOUTTCPFIFOPERR)
+#define F_ETPOUTTCPFIFOPERR    V_ETPOUTTCPFIFOPERR(1U)
+
+#define S_ETPOUTIPFIFOPERR    15
+#define V_ETPOUTIPFIFOPERR(x) ((x) << S_ETPOUTIPFIFOPERR)
+#define F_ETPOUTIPFIFOPERR    V_ETPOUTIPFIFOPERR(1U)
+
+#define S_ETPOUTETHFIFOPERR    14
+#define V_ETPOUTETHFIFOPERR(x) ((x) << S_ETPOUTETHFIFOPERR)
+#define F_ETPOUTETHFIFOPERR    V_ETPOUTETHFIFOPERR(1U)
+
+#define S_ETPINCPLFIFOPERR    13
+#define V_ETPINCPLFIFOPERR(x) ((x) << S_ETPINCPLFIFOPERR)
+#define F_ETPINCPLFIFOPERR    V_ETPINCPLFIFOPERR(1U)
+
+#define S_ETPINTCPOPTFIFOPERR    12
+#define V_ETPINTCPOPTFIFOPERR(x) ((x) << S_ETPINTCPOPTFIFOPERR)
+#define F_ETPINTCPOPTFIFOPERR    V_ETPINTCPOPTFIFOPERR(1U)
+
+#define S_ETPINTCPFIFOPERR    11
+#define V_ETPINTCPFIFOPERR(x) ((x) << S_ETPINTCPFIFOPERR)
+#define F_ETPINTCPFIFOPERR    V_ETPINTCPFIFOPERR(1U)
+
+#define S_ETPINIPFIFOPERR    10
+#define V_ETPINIPFIFOPERR(x) ((x) << S_ETPINIPFIFOPERR)
+#define F_ETPINIPFIFOPERR    V_ETPINIPFIFOPERR(1U)
+
+#define S_ETPINETHFIFOPERR    9
+#define V_ETPINETHFIFOPERR(x) ((x) << S_ETPINETHFIFOPERR)
+#define F_ETPINETHFIFOPERR    V_ETPINETHFIFOPERR(1U)
+
+#define S_CTPOUTCPLFIFOPERR    8
+#define V_CTPOUTCPLFIFOPERR(x) ((x) << S_CTPOUTCPLFIFOPERR)
+#define F_CTPOUTCPLFIFOPERR    V_CTPOUTCPLFIFOPERR(1U)
+
+#define S_CTPOUTTCPFIFOPERR    7
+#define V_CTPOUTTCPFIFOPERR(x) ((x) << S_CTPOUTTCPFIFOPERR)
+#define F_CTPOUTTCPFIFOPERR    V_CTPOUTTCPFIFOPERR(1U)
+
+#define S_CTPOUTIPFIFOPERR    6
+#define V_CTPOUTIPFIFOPERR(x) ((x) << S_CTPOUTIPFIFOPERR)
+#define F_CTPOUTIPFIFOPERR    V_CTPOUTIPFIFOPERR(1U)
+
+#define S_CTPOUTETHFIFOPERR    5
+#define V_CTPOUTETHFIFOPERR(x) ((x) << S_CTPOUTETHFIFOPERR)
+#define F_CTPOUTETHFIFOPERR    V_CTPOUTETHFIFOPERR(1U)
+
+#define S_CTPINCPLFIFOPERR    4
+#define V_CTPINCPLFIFOPERR(x) ((x) << S_CTPINCPLFIFOPERR)
+#define F_CTPINCPLFIFOPERR    V_CTPINCPLFIFOPERR(1U)
+
+#define S_CTPINTCPOPFIFOPERR    3
+#define V_CTPINTCPOPFIFOPERR(x) ((x) << S_CTPINTCPOPFIFOPERR)
+#define F_CTPINTCPOPFIFOPERR    V_CTPINTCPOPFIFOPERR(1U)
+
+#define S_PDUFBKFIFOPERR    2
+#define V_PDUFBKFIFOPERR(x) ((x) << S_PDUFBKFIFOPERR)
+#define F_PDUFBKFIFOPERR    V_PDUFBKFIFOPERR(1U)
+
+#define S_CMOPEXTFIFOPERR    1
+#define V_CMOPEXTFIFOPERR(x) ((x) << S_CMOPEXTFIFOPERR)
+#define F_CMOPEXTFIFOPERR    V_CMOPEXTFIFOPERR(1U)
+
+#define S_DELINVFIFOPERR    0
+#define V_DELINVFIFOPERR(x) ((x) << S_DELINVFIFOPERR)
+#define F_DELINVFIFOPERR    V_DELINVFIFOPERR(1U)
+
+#define A_TP_INT_CAUSE 0x7e74
+#define A_TP_PER_ENABLE 0x7e78
+#define A_TP_FLM_FREE_PS_CNT 0x7e80
+
+#define S_FREEPSTRUCTCOUNT    0
+#define M_FREEPSTRUCTCOUNT    0x1fffffU
+#define V_FREEPSTRUCTCOUNT(x) ((x) << S_FREEPSTRUCTCOUNT)
+#define G_FREEPSTRUCTCOUNT(x) (((x) >> S_FREEPSTRUCTCOUNT) & M_FREEPSTRUCTCOUNT)
+
+#define A_TP_FLM_FREE_RX_CNT 0x7e84
+
+#define S_FREERXPAGECHN    28
+#define V_FREERXPAGECHN(x) ((x) << S_FREERXPAGECHN)
+#define F_FREERXPAGECHN    V_FREERXPAGECHN(1U)
+
+#define S_FREERXPAGECOUNT    0
+#define M_FREERXPAGECOUNT    0x1fffffU
+#define V_FREERXPAGECOUNT(x) ((x) << S_FREERXPAGECOUNT)
+#define G_FREERXPAGECOUNT(x) (((x) >> S_FREERXPAGECOUNT) & M_FREERXPAGECOUNT)
+
+#define A_TP_FLM_FREE_TX_CNT 0x7e88
+
+#define S_FREETXPAGECHN    28
+#define M_FREETXPAGECHN    0x3U
+#define V_FREETXPAGECHN(x) ((x) << S_FREETXPAGECHN)
+#define G_FREETXPAGECHN(x) (((x) >> S_FREETXPAGECHN) & M_FREETXPAGECHN)
+
+#define S_FREETXPAGECOUNT    0
+#define M_FREETXPAGECOUNT    0x1fffffU
+#define V_FREETXPAGECOUNT(x) ((x) << S_FREETXPAGECOUNT)
+#define G_FREETXPAGECOUNT(x) (((x) >> S_FREETXPAGECOUNT) & M_FREETXPAGECOUNT)
+
+#define A_TP_TM_HEAP_PUSH_CNT 0x7e8c
+#define A_TP_TM_HEAP_POP_CNT 0x7e90
+#define A_TP_TM_DACK_PUSH_CNT 0x7e94
+#define A_TP_TM_DACK_POP_CNT 0x7e98
+#define A_TP_TM_MOD_PUSH_CNT 0x7e9c
+#define A_TP_MOD_POP_CNT 0x7ea0
+#define A_TP_TIMER_SEPARATOR 0x7ea4
+
+#define S_TIMERSEPARATOR    16
+#define M_TIMERSEPARATOR    0xffffU
+#define V_TIMERSEPARATOR(x) ((x) << S_TIMERSEPARATOR)
+#define G_TIMERSEPARATOR(x) (((x) >> S_TIMERSEPARATOR) & M_TIMERSEPARATOR)
+
+#define S_DISABLETIMEFREEZE    0
+#define V_DISABLETIMEFREEZE(x) ((x) << S_DISABLETIMEFREEZE)
+#define F_DISABLETIMEFREEZE    V_DISABLETIMEFREEZE(1U)
+
+#define A_TP_DEBUG_FLAGS 0x7eac
+
+#define S_RXTIMERDACKFIRST    26
+#define V_RXTIMERDACKFIRST(x) ((x) << S_RXTIMERDACKFIRST)
+#define F_RXTIMERDACKFIRST    V_RXTIMERDACKFIRST(1U)
+
+#define S_RXTIMERDACK    25
+#define V_RXTIMERDACK(x) ((x) << S_RXTIMERDACK)
+#define F_RXTIMERDACK    V_RXTIMERDACK(1U)
+
+#define S_RXTIMERHEARTBEAT    24
+#define V_RXTIMERHEARTBEAT(x) ((x) << S_RXTIMERHEARTBEAT)
+#define F_RXTIMERHEARTBEAT    V_RXTIMERHEARTBEAT(1U)
+
+#define S_RXPAWSDROP    23
+#define V_RXPAWSDROP(x) ((x) << S_RXPAWSDROP)
+#define F_RXPAWSDROP    V_RXPAWSDROP(1U)
+
+#define S_RXURGDATADROP    22
+#define V_RXURGDATADROP(x) ((x) << S_RXURGDATADROP)
+#define F_RXURGDATADROP    V_RXURGDATADROP(1U)
+
+#define S_RXFUTUREDATA    21
+#define V_RXFUTUREDATA(x) ((x) << S_RXFUTUREDATA)
+#define F_RXFUTUREDATA    V_RXFUTUREDATA(1U)
+
+#define S_RXRCVRXMDATA    20
+#define V_RXRCVRXMDATA(x) ((x) << S_RXRCVRXMDATA)
+#define F_RXRCVRXMDATA    V_RXRCVRXMDATA(1U)
+
+#define S_RXRCVOOODATAFIN    19
+#define V_RXRCVOOODATAFIN(x) ((x) << S_RXRCVOOODATAFIN)
+#define F_RXRCVOOODATAFIN    V_RXRCVOOODATAFIN(1U)
+
+#define S_RXRCVOOODATA    18
+#define V_RXRCVOOODATA(x) ((x) << S_RXRCVOOODATA)
+#define F_RXRCVOOODATA    V_RXRCVOOODATA(1U)
+
+#define S_RXRCVWNDZERO    17
+#define V_RXRCVWNDZERO(x) ((x) << S_RXRCVWNDZERO)
+#define F_RXRCVWNDZERO    V_RXRCVWNDZERO(1U)
+
+#define S_RXRCVWNDLTMSS    16
+#define V_RXRCVWNDLTMSS(x) ((x) << S_RXRCVWNDLTMSS)
+#define F_RXRCVWNDLTMSS    V_RXRCVWNDLTMSS(1U)
+
+#define S_TXDUPACKINC    11
+#define V_TXDUPACKINC(x) ((x) << S_TXDUPACKINC)
+#define F_TXDUPACKINC    V_TXDUPACKINC(1U)
+
+#define S_TXRXMURG    10
+#define V_TXRXMURG(x) ((x) << S_TXRXMURG)
+#define F_TXRXMURG    V_TXRXMURG(1U)
+
+#define S_TXRXMFIN    9
+#define V_TXRXMFIN(x) ((x) << S_TXRXMFIN)
+#define F_TXRXMFIN    V_TXRXMFIN(1U)
+
+#define S_TXRXMSYN    8
+#define V_TXRXMSYN(x) ((x) << S_TXRXMSYN)
+#define F_TXRXMSYN    V_TXRXMSYN(1U)
+
+#define S_TXRXMNEWRENO    7
+#define V_TXRXMNEWRENO(x) ((x) << S_TXRXMNEWRENO)
+#define F_TXRXMNEWRENO    V_TXRXMNEWRENO(1U)
+
+#define S_TXRXMFAST    6
+#define V_TXRXMFAST(x) ((x) << S_TXRXMFAST)
+#define F_TXRXMFAST    V_TXRXMFAST(1U)
+
+#define S_TXRXMTIMER    5
+#define V_TXRXMTIMER(x) ((x) << S_TXRXMTIMER)
+#define F_TXRXMTIMER    V_TXRXMTIMER(1U)
+
+#define S_TXRXMTIMERKEEPALIVE    4
+#define V_TXRXMTIMERKEEPALIVE(x) ((x) << S_TXRXMTIMERKEEPALIVE)
+#define F_TXRXMTIMERKEEPALIVE    V_TXRXMTIMERKEEPALIVE(1U)
+
+#define S_TXRXMTIMERPERSIST    3
+#define V_TXRXMTIMERPERSIST(x) ((x) << S_TXRXMTIMERPERSIST)
+#define F_TXRXMTIMERPERSIST    V_TXRXMTIMERPERSIST(1U)
+
+#define S_TXRCVADVSHRUNK    2
+#define V_TXRCVADVSHRUNK(x) ((x) << S_TXRCVADVSHRUNK)
+#define F_TXRCVADVSHRUNK    V_TXRCVADVSHRUNK(1U)
+
+#define S_TXRCVADVZERO    1
+#define V_TXRCVADVZERO(x) ((x) << S_TXRCVADVZERO)
+#define F_TXRCVADVZERO    V_TXRCVADVZERO(1U)
+
+#define S_TXRCVADVLTMSS    0
+#define V_TXRCVADVLTMSS(x) ((x) << S_TXRCVADVLTMSS)
+#define F_TXRCVADVLTMSS    V_TXRCVADVLTMSS(1U)
+
+#define A_TP_RX_SCHED 0x7eb0
+
+#define S_RXCOMMITRESET1    31
+#define V_RXCOMMITRESET1(x) ((x) << S_RXCOMMITRESET1)
+#define F_RXCOMMITRESET1    V_RXCOMMITRESET1(1U)
+
+#define S_RXCOMMITRESET0    30
+#define V_RXCOMMITRESET0(x) ((x) << S_RXCOMMITRESET0)
+#define F_RXCOMMITRESET0    V_RXCOMMITRESET0(1U)
+
+#define S_RXFORCECONG1    29
+#define V_RXFORCECONG1(x) ((x) << S_RXFORCECONG1)
+#define F_RXFORCECONG1    V_RXFORCECONG1(1U)
+
+#define S_RXFORCECONG0    28
+#define V_RXFORCECONG0(x) ((x) << S_RXFORCECONG0)
+#define F_RXFORCECONG0    V_RXFORCECONG0(1U)
+
+#define S_ENABLELPBKFULL1    26
+#define M_ENABLELPBKFULL1    0x3U
+#define V_ENABLELPBKFULL1(x) ((x) << S_ENABLELPBKFULL1)
+#define G_ENABLELPBKFULL1(x) (((x) >> S_ENABLELPBKFULL1) & M_ENABLELPBKFULL1)
+
+#define S_ENABLELPBKFULL0    24
+#define M_ENABLELPBKFULL0    0x3U
+#define V_ENABLELPBKFULL0(x) ((x) << S_ENABLELPBKFULL0)
+#define G_ENABLELPBKFULL0(x) (((x) >> S_ENABLELPBKFULL0) & M_ENABLELPBKFULL0)
+
+#define S_ENABLEFIFOFULL1    22
+#define M_ENABLEFIFOFULL1    0x3U
+#define V_ENABLEFIFOFULL1(x) ((x) << S_ENABLEFIFOFULL1)
+#define G_ENABLEFIFOFULL1(x) (((x) >> S_ENABLEFIFOFULL1) & M_ENABLEFIFOFULL1)
+
+#define S_ENABLEPCMDFULL1    20
+#define M_ENABLEPCMDFULL1    0x3U
+#define V_ENABLEPCMDFULL1(x) ((x) << S_ENABLEPCMDFULL1)
+#define G_ENABLEPCMDFULL1(x) (((x) >> S_ENABLEPCMDFULL1) & M_ENABLEPCMDFULL1)
+
+#define S_ENABLEHDRFULL1    18
+#define M_ENABLEHDRFULL1    0x3U
+#define V_ENABLEHDRFULL1(x) ((x) << S_ENABLEHDRFULL1)
+#define G_ENABLEHDRFULL1(x) (((x) >> S_ENABLEHDRFULL1) & M_ENABLEHDRFULL1)
+
+#define S_ENABLEFIFOFULL0    16
+#define M_ENABLEFIFOFULL0    0x3U
+#define V_ENABLEFIFOFULL0(x) ((x) << S_ENABLEFIFOFULL0)
+#define G_ENABLEFIFOFULL0(x) (((x) >> S_ENABLEFIFOFULL0) & M_ENABLEFIFOFULL0)
+
+#define S_ENABLEPCMDFULL0    14
+#define M_ENABLEPCMDFULL0    0x3U
+#define V_ENABLEPCMDFULL0(x) ((x) << S_ENABLEPCMDFULL0)
+#define G_ENABLEPCMDFULL0(x) (((x) >> S_ENABLEPCMDFULL0) & M_ENABLEPCMDFULL0)
+
+#define S_ENABLEHDRFULL0    12
+#define M_ENABLEHDRFULL0    0x3U
+#define V_ENABLEHDRFULL0(x) ((x) << S_ENABLEHDRFULL0)
+#define G_ENABLEHDRFULL0(x) (((x) >> S_ENABLEHDRFULL0) & M_ENABLEHDRFULL0)
+
+#define S_COMMITLIMIT1    6
+#define M_COMMITLIMIT1    0x3fU
+#define V_COMMITLIMIT1(x) ((x) << S_COMMITLIMIT1)
+#define G_COMMITLIMIT1(x) (((x) >> S_COMMITLIMIT1) & M_COMMITLIMIT1)
+
+#define S_COMMITLIMIT0    0
+#define M_COMMITLIMIT0    0x3fU
+#define V_COMMITLIMIT0(x) ((x) << S_COMMITLIMIT0)
+#define G_COMMITLIMIT0(x) (((x) >> S_COMMITLIMIT0) & M_COMMITLIMIT0)
+
+#define A_TP_TX_SCHED 0x7eb4
+
+#define S_COMMITRESET3    31
+#define V_COMMITRESET3(x) ((x) << S_COMMITRESET3)
+#define F_COMMITRESET3    V_COMMITRESET3(1U)
+
+#define S_COMMITRESET2    30
+#define V_COMMITRESET2(x) ((x) << S_COMMITRESET2)
+#define F_COMMITRESET2    V_COMMITRESET2(1U)
+
+#define S_COMMITRESET1    29
+#define V_COMMITRESET1(x) ((x) << S_COMMITRESET1)
+#define F_COMMITRESET1    V_COMMITRESET1(1U)
+
+#define S_COMMITRESET0    28
+#define V_COMMITRESET0(x) ((x) << S_COMMITRESET0)
+#define F_COMMITRESET0    V_COMMITRESET0(1U)
+
+#define S_FORCECONG3    27
+#define V_FORCECONG3(x) ((x) << S_FORCECONG3)
+#define F_FORCECONG3    V_FORCECONG3(1U)
+
+#define S_FORCECONG2    26
+#define V_FORCECONG2(x) ((x) << S_FORCECONG2)
+#define F_FORCECONG2    V_FORCECONG2(1U)
+
+#define S_FORCECONG1    25
+#define V_FORCECONG1(x) ((x) << S_FORCECONG1)
+#define F_FORCECONG1    V_FORCECONG1(1U)
+
+#define S_FORCECONG0    24
+#define V_FORCECONG0(x) ((x) << S_FORCECONG0)
+#define F_FORCECONG0    V_FORCECONG0(1U)
+
+#define S_COMMITLIMIT3    18
+#define M_COMMITLIMIT3    0x3fU
+#define V_COMMITLIMIT3(x) ((x) << S_COMMITLIMIT3)
+#define G_COMMITLIMIT3(x) (((x) >> S_COMMITLIMIT3) & M_COMMITLIMIT3)
+
+#define S_COMMITLIMIT2    12
+#define M_COMMITLIMIT2    0x3fU
+#define V_COMMITLIMIT2(x) ((x) << S_COMMITLIMIT2)
+#define G_COMMITLIMIT2(x) (((x) >> S_COMMITLIMIT2) & M_COMMITLIMIT2)
+
+#define A_TP_FX_SCHED 0x7eb8
+
+#define S_TXCHNXOFF3    19
+#define V_TXCHNXOFF3(x) ((x) << S_TXCHNXOFF3)
+#define F_TXCHNXOFF3    V_TXCHNXOFF3(1U)
+
+#define S_TXCHNXOFF2    18
+#define V_TXCHNXOFF2(x) ((x) << S_TXCHNXOFF2)
+#define F_TXCHNXOFF2    V_TXCHNXOFF2(1U)
+
+#define S_TXCHNXOFF1    17
+#define V_TXCHNXOFF1(x) ((x) << S_TXCHNXOFF1)
+#define F_TXCHNXOFF1    V_TXCHNXOFF1(1U)
+
+#define S_TXCHNXOFF0    16
+#define V_TXCHNXOFF0(x) ((x) << S_TXCHNXOFF0)
+#define F_TXCHNXOFF0    V_TXCHNXOFF0(1U)
+
+#define S_TXMODXOFF7    15
+#define V_TXMODXOFF7(x) ((x) << S_TXMODXOFF7)
+#define F_TXMODXOFF7    V_TXMODXOFF7(1U)
+
+#define S_TXMODXOFF6    14
+#define V_TXMODXOFF6(x) ((x) << S_TXMODXOFF6)
+#define F_TXMODXOFF6    V_TXMODXOFF6(1U)
+
+#define S_TXMODXOFF5    13
+#define V_TXMODXOFF5(x) ((x) << S_TXMODXOFF5)
+#define F_TXMODXOFF5    V_TXMODXOFF5(1U)
+
+#define S_TXMODXOFF4    12
+#define V_TXMODXOFF4(x) ((x) << S_TXMODXOFF4)
+#define F_TXMODXOFF4    V_TXMODXOFF4(1U)
+
+#define S_TXMODXOFF3    11
+#define V_TXMODXOFF3(x) ((x) << S_TXMODXOFF3)
+#define F_TXMODXOFF3    V_TXMODXOFF3(1U)
+
+#define S_TXMODXOFF2    10
+#define V_TXMODXOFF2(x) ((x) << S_TXMODXOFF2)
+#define F_TXMODXOFF2    V_TXMODXOFF2(1U)
+
+#define S_TXMODXOFF1    9
+#define V_TXMODXOFF1(x) ((x) << S_TXMODXOFF1)
+#define F_TXMODXOFF1    V_TXMODXOFF1(1U)
+
+#define S_TXMODXOFF0    8
+#define V_TXMODXOFF0(x) ((x) << S_TXMODXOFF0)
+#define F_TXMODXOFF0    V_TXMODXOFF0(1U)
+
+#define S_RXCHNXOFF3    7
+#define V_RXCHNXOFF3(x) ((x) << S_RXCHNXOFF3)
+#define F_RXCHNXOFF3    V_RXCHNXOFF3(1U)
+
+#define S_RXCHNXOFF2    6
+#define V_RXCHNXOFF2(x) ((x) << S_RXCHNXOFF2)
+#define F_RXCHNXOFF2    V_RXCHNXOFF2(1U)
+
+#define S_RXCHNXOFF1    5
+#define V_RXCHNXOFF1(x) ((x) << S_RXCHNXOFF1)
+#define F_RXCHNXOFF1    V_RXCHNXOFF1(1U)
+
+#define S_RXCHNXOFF0    4
+#define V_RXCHNXOFF0(x) ((x) << S_RXCHNXOFF0)
+#define F_RXCHNXOFF0    V_RXCHNXOFF0(1U)
+
+#define S_RXMODXOFF1    1
+#define V_RXMODXOFF1(x) ((x) << S_RXMODXOFF1)
+#define F_RXMODXOFF1    V_RXMODXOFF1(1U)
+
+#define S_RXMODXOFF0    0
+#define V_RXMODXOFF0(x) ((x) << S_RXMODXOFF0)
+#define F_RXMODXOFF0    V_RXMODXOFF0(1U)
+
+#define A_TP_TX_ORATE 0x7ebc
+
+#define S_OFDRATE3    24
+#define M_OFDRATE3    0xffU
+#define V_OFDRATE3(x) ((x) << S_OFDRATE3)
+#define G_OFDRATE3(x) (((x) >> S_OFDRATE3) & M_OFDRATE3)
+
+#define S_OFDRATE2    16
+#define M_OFDRATE2    0xffU
+#define V_OFDRATE2(x) ((x) << S_OFDRATE2)
+#define G_OFDRATE2(x) (((x) >> S_OFDRATE2) & M_OFDRATE2)
+
+#define S_OFDRATE1    8
+#define M_OFDRATE1    0xffU
+#define V_OFDRATE1(x) ((x) << S_OFDRATE1)
+#define G_OFDRATE1(x) (((x) >> S_OFDRATE1) & M_OFDRATE1)
+
+#define S_OFDRATE0    0
+#define M_OFDRATE0    0xffU
+#define V_OFDRATE0(x) ((x) << S_OFDRATE0)
+#define G_OFDRATE0(x) (((x) >> S_OFDRATE0) & M_OFDRATE0)
+
+#define A_TP_IX_SCHED0 0x7ec0
+#define A_TP_IX_SCHED1 0x7ec4
+#define A_TP_IX_SCHED2 0x7ec8
+#define A_TP_IX_SCHED3 0x7ecc
+#define A_TP_TX_TRATE 0x7ed0
+
+#define S_TNLRATE3    24
+#define M_TNLRATE3    0xffU
+#define V_TNLRATE3(x) ((x) << S_TNLRATE3)
+#define G_TNLRATE3(x) (((x) >> S_TNLRATE3) & M_TNLRATE3)
+
+#define S_TNLRATE2    16
+#define M_TNLRATE2    0xffU
+#define V_TNLRATE2(x) ((x) << S_TNLRATE2)
+#define G_TNLRATE2(x) (((x) >> S_TNLRATE2) & M_TNLRATE2)
+
+#define S_TNLRATE1    8
+#define M_TNLRATE1    0xffU
+#define V_TNLRATE1(x) ((x) << S_TNLRATE1)
+#define G_TNLRATE1(x) (((x) >> S_TNLRATE1) & M_TNLRATE1)
+
+#define S_TNLRATE0    0
+#define M_TNLRATE0    0xffU
+#define V_TNLRATE0(x) ((x) << S_TNLRATE0)
+#define G_TNLRATE0(x) (((x) >> S_TNLRATE0) & M_TNLRATE0)
+
+#define A_TP_DBG_LA_CONFIG 0x7ed4
+
+#define S_DBGLAOPCENABLE    24
+#define M_DBGLAOPCENABLE    0xffU
+#define V_DBGLAOPCENABLE(x) ((x) << S_DBGLAOPCENABLE)
+#define G_DBGLAOPCENABLE(x) (((x) >> S_DBGLAOPCENABLE) & M_DBGLAOPCENABLE)
+
+#define S_DBGLAWHLF    23
+#define V_DBGLAWHLF(x) ((x) << S_DBGLAWHLF)
+#define F_DBGLAWHLF    V_DBGLAWHLF(1U)
+
+#define S_DBGLAWPTR    16
+#define M_DBGLAWPTR    0x7fU
+#define V_DBGLAWPTR(x) ((x) << S_DBGLAWPTR)
+#define G_DBGLAWPTR(x) (((x) >> S_DBGLAWPTR) & M_DBGLAWPTR)
+
+#define S_DBGLAMODE    14
+#define M_DBGLAMODE    0x3U
+#define V_DBGLAMODE(x) ((x) << S_DBGLAMODE)
+#define G_DBGLAMODE(x) (((x) >> S_DBGLAMODE) & M_DBGLAMODE)
+
+#define S_DBGLAFATALFREEZE    13
+#define V_DBGLAFATALFREEZE(x) ((x) << S_DBGLAFATALFREEZE)
+#define F_DBGLAFATALFREEZE    V_DBGLAFATALFREEZE(1U)
+
+#define S_DBGLAENABLE    12
+#define V_DBGLAENABLE(x) ((x) << S_DBGLAENABLE)
+#define F_DBGLAENABLE    V_DBGLAENABLE(1U)
+
+#define S_DBGLARPTR    0
+#define M_DBGLARPTR    0x7fU
+#define V_DBGLARPTR(x) ((x) << S_DBGLARPTR)
+#define G_DBGLARPTR(x) (((x) >> S_DBGLARPTR) & M_DBGLARPTR)
+
+#define A_TP_DBG_LA_DATAL 0x7ed8
+#define A_TP_DBG_LA_DATAH 0x7edc
+#define A_TP_PROTOCOL_CNTRL 0x7ee8
+
+#define S_WRITEENABLE    31
+#define V_WRITEENABLE(x) ((x) << S_WRITEENABLE)
+#define F_WRITEENABLE    V_WRITEENABLE(1U)
+
+#define S_TCAMENABLE    10
+#define V_TCAMENABLE(x) ((x) << S_TCAMENABLE)
+#define F_TCAMENABLE    V_TCAMENABLE(1U)
+
+#define S_BLOCKSELECT    8
+#define M_BLOCKSELECT    0x3U
+#define V_BLOCKSELECT(x) ((x) << S_BLOCKSELECT)
+#define G_BLOCKSELECT(x) (((x) >> S_BLOCKSELECT) & M_BLOCKSELECT)
+
+#define S_LINEADDRESS    1
+#define M_LINEADDRESS    0x7fU
+#define V_LINEADDRESS(x) ((x) << S_LINEADDRESS)
+#define G_LINEADDRESS(x) (((x) >> S_LINEADDRESS) & M_LINEADDRESS)
+
+#define S_REQUESTDONE    0
+#define V_REQUESTDONE(x) ((x) << S_REQUESTDONE)
+#define F_REQUESTDONE    V_REQUESTDONE(1U)
+
+#define A_TP_PROTOCOL_DATA0 0x7eec
+#define A_TP_PROTOCOL_DATA1 0x7ef0
+#define A_TP_PROTOCOL_DATA2 0x7ef4
+#define A_TP_PROTOCOL_DATA3 0x7ef8
+#define A_TP_PROTOCOL_DATA4 0x7efc
+
+#define S_PROTOCOLDATAFIELD    0
+#define M_PROTOCOLDATAFIELD    0xfU
+#define V_PROTOCOLDATAFIELD(x) ((x) << S_PROTOCOLDATAFIELD)
+#define G_PROTOCOLDATAFIELD(x) (((x) >> S_PROTOCOLDATAFIELD) & M_PROTOCOLDATAFIELD)
+
+#define A_TP_TX_MOD_Q7_Q6_TIMER_SEPARATOR 0x0
+
+#define S_TXTIMERSEPQ7    16
+#define M_TXTIMERSEPQ7    0xffffU
+#define V_TXTIMERSEPQ7(x) ((x) << S_TXTIMERSEPQ7)
+#define G_TXTIMERSEPQ7(x) (((x) >> S_TXTIMERSEPQ7) & M_TXTIMERSEPQ7)
+
+#define S_TXTIMERSEPQ6    0
+#define M_TXTIMERSEPQ6    0xffffU
+#define V_TXTIMERSEPQ6(x) ((x) << S_TXTIMERSEPQ6)
+#define G_TXTIMERSEPQ6(x) (((x) >> S_TXTIMERSEPQ6) & M_TXTIMERSEPQ6)
+
+#define A_TP_TX_MOD_Q5_Q4_TIMER_SEPARATOR 0x1
+
+#define S_TXTIMERSEPQ5    16
+#define M_TXTIMERSEPQ5    0xffffU
+#define V_TXTIMERSEPQ5(x) ((x) << S_TXTIMERSEPQ5)
+#define G_TXTIMERSEPQ5(x) (((x) >> S_TXTIMERSEPQ5) & M_TXTIMERSEPQ5)
+
+#define S_TXTIMERSEPQ4    0
+#define M_TXTIMERSEPQ4    0xffffU
+#define V_TXTIMERSEPQ4(x) ((x) << S_TXTIMERSEPQ4)
+#define G_TXTIMERSEPQ4(x) (((x) >> S_TXTIMERSEPQ4) & M_TXTIMERSEPQ4)
+
+#define A_TP_TX_MOD_Q3_Q2_TIMER_SEPARATOR 0x2
+
+#define S_TXTIMERSEPQ3    16
+#define M_TXTIMERSEPQ3    0xffffU
+#define V_TXTIMERSEPQ3(x) ((x) << S_TXTIMERSEPQ3)
+#define G_TXTIMERSEPQ3(x) (((x) >> S_TXTIMERSEPQ3) & M_TXTIMERSEPQ3)
+
+#define S_TXTIMERSEPQ2    0
+#define M_TXTIMERSEPQ2    0xffffU
+#define V_TXTIMERSEPQ2(x) ((x) << S_TXTIMERSEPQ2)
+#define G_TXTIMERSEPQ2(x) (((x) >> S_TXTIMERSEPQ2) & M_TXTIMERSEPQ2)
+
+#define A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR 0x3
+
+#define S_TXTIMERSEPQ1    16
+#define M_TXTIMERSEPQ1    0xffffU
+#define V_TXTIMERSEPQ1(x) ((x) << S_TXTIMERSEPQ1)
+#define G_TXTIMERSEPQ1(x) (((x) >> S_TXTIMERSEPQ1) & M_TXTIMERSEPQ1)
+
+#define S_TXTIMERSEPQ0    0
+#define M_TXTIMERSEPQ0    0xffffU
+#define V_TXTIMERSEPQ0(x) ((x) << S_TXTIMERSEPQ0)
+#define G_TXTIMERSEPQ0(x) (((x) >> S_TXTIMERSEPQ0) & M_TXTIMERSEPQ0)
+
+#define A_TP_RX_MOD_Q1_Q0_TIMER_SEPARATOR 0x4
+
+#define S_RXTIMERSEPQ1    16
+#define M_RXTIMERSEPQ1    0xffffU
+#define V_RXTIMERSEPQ1(x) ((x) << S_RXTIMERSEPQ1)
+#define G_RXTIMERSEPQ1(x) (((x) >> S_RXTIMERSEPQ1) & M_RXTIMERSEPQ1)
+
+#define S_RXTIMERSEPQ0    0
+#define M_RXTIMERSEPQ0    0xffffU
+#define V_RXTIMERSEPQ0(x) ((x) << S_RXTIMERSEPQ0)
+#define G_RXTIMERSEPQ0(x) (((x) >> S_RXTIMERSEPQ0) & M_RXTIMERSEPQ0)
+
+#define A_TP_TX_MOD_Q7_Q6_RATE_LIMIT 0x5
+
+#define S_TXRATEINCQ7    24
+#define M_TXRATEINCQ7    0xffU
+#define V_TXRATEINCQ7(x) ((x) << S_TXRATEINCQ7)
+#define G_TXRATEINCQ7(x) (((x) >> S_TXRATEINCQ7) & M_TXRATEINCQ7)
+
+#define S_TXRATETCKQ7    16
+#define M_TXRATETCKQ7    0xffU
+#define V_TXRATETCKQ7(x) ((x) << S_TXRATETCKQ7)
+#define G_TXRATETCKQ7(x) (((x) >> S_TXRATETCKQ7) & M_TXRATETCKQ7)
+
+#define S_TXRATEINCQ6    8
+#define M_TXRATEINCQ6    0xffU
+#define V_TXRATEINCQ6(x) ((x) << S_TXRATEINCQ6)
+#define G_TXRATEINCQ6(x) (((x) >> S_TXRATEINCQ6) & M_TXRATEINCQ6)
+
+#define S_TXRATETCKQ6    0
+#define M_TXRATETCKQ6    0xffU
+#define V_TXRATETCKQ6(x) ((x) << S_TXRATETCKQ6)
+#define G_TXRATETCKQ6(x) (((x) >> S_TXRATETCKQ6) & M_TXRATETCKQ6)
+
+#define A_TP_TX_MOD_Q5_Q4_RATE_LIMIT 0x6
+
+#define S_TXRATEINCQ5    24
+#define M_TXRATEINCQ5    0xffU
+#define V_TXRATEINCQ5(x) ((x) << S_TXRATEINCQ5)
+#define G_TXRATEINCQ5(x) (((x) >> S_TXRATEINCQ5) & M_TXRATEINCQ5)
+
+#define S_TXRATETCKQ5    16
+#define M_TXRATETCKQ5    0xffU
+#define V_TXRATETCKQ5(x) ((x) << S_TXRATETCKQ5)
+#define G_TXRATETCKQ5(x) (((x) >> S_TXRATETCKQ5) & M_TXRATETCKQ5)
+
+#define S_TXRATEINCQ4    8
+#define M_TXRATEINCQ4    0xffU
+#define V_TXRATEINCQ4(x) ((x) << S_TXRATEINCQ4)
+#define G_TXRATEINCQ4(x) (((x) >> S_TXRATEINCQ4) & M_TXRATEINCQ4)
+
+#define S_TXRATETCKQ4    0
+#define M_TXRATETCKQ4    0xffU
+#define V_TXRATETCKQ4(x) ((x) << S_TXRATETCKQ4)
+#define G_TXRATETCKQ4(x) (((x) >> S_TXRATETCKQ4) & M_TXRATETCKQ4)
+
+#define A_TP_TX_MOD_Q3_Q2_RATE_LIMIT 0x7
+
+#define S_TXRATEINCQ3    24
+#define M_TXRATEINCQ3    0xffU
+#define V_TXRATEINCQ3(x) ((x) << S_TXRATEINCQ3)
+#define G_TXRATEINCQ3(x) (((x) >> S_TXRATEINCQ3) & M_TXRATEINCQ3)
+
+#define S_TXRATETCKQ3    16
+#define M_TXRATETCKQ3    0xffU
+#define V_TXRATETCKQ3(x) ((x) << S_TXRATETCKQ3)
+#define G_TXRATETCKQ3(x) (((x) >> S_TXRATETCKQ3) & M_TXRATETCKQ3)
+
+#define S_TXRATEINCQ2    8
+#define M_TXRATEINCQ2    0xffU
+#define V_TXRATEINCQ2(x) ((x) << S_TXRATEINCQ2)
+#define G_TXRATEINCQ2(x) (((x) >> S_TXRATEINCQ2) & M_TXRATEINCQ2)
+
+#define S_TXRATETCKQ2    0
+#define M_TXRATETCKQ2    0xffU
+#define V_TXRATETCKQ2(x) ((x) << S_TXRATETCKQ2)
+#define G_TXRATETCKQ2(x) (((x) >> S_TXRATETCKQ2) & M_TXRATETCKQ2)
+
+#define A_TP_TX_MOD_Q1_Q0_RATE_LIMIT 0x8
+
+#define S_TXRATEINCQ1    24
+#define M_TXRATEINCQ1    0xffU
+#define V_TXRATEINCQ1(x) ((x) << S_TXRATEINCQ1)
+#define G_TXRATEINCQ1(x) (((x) >> S_TXRATEINCQ1) & M_TXRATEINCQ1)
+
+#define S_TXRATETCKQ1    16
+#define M_TXRATETCKQ1    0xffU
+#define V_TXRATETCKQ1(x) ((x) << S_TXRATETCKQ1)
+#define G_TXRATETCKQ1(x) (((x) >> S_TXRATETCKQ1) & M_TXRATETCKQ1)
+
+#define S_TXRATEINCQ0    8
+#define M_TXRATEINCQ0    0xffU
+#define V_TXRATEINCQ0(x) ((x) << S_TXRATEINCQ0)
+#define G_TXRATEINCQ0(x) (((x) >> S_TXRATEINCQ0) & M_TXRATEINCQ0)
+
+#define S_TXRATETCKQ0    0
+#define M_TXRATETCKQ0    0xffU
+#define V_TXRATETCKQ0(x) ((x) << S_TXRATETCKQ0)
+#define G_TXRATETCKQ0(x) (((x) >> S_TXRATETCKQ0) & M_TXRATETCKQ0)
+
+#define A_TP_RX_MOD_Q1_Q0_RATE_LIMIT 0x9
+
+#define S_RXRATEINCQ1    24
+#define M_RXRATEINCQ1    0xffU
+#define V_RXRATEINCQ1(x) ((x) << S_RXRATEINCQ1)
+#define G_RXRATEINCQ1(x) (((x) >> S_RXRATEINCQ1) & M_RXRATEINCQ1)
+
+#define S_RXRATETCKQ1    16
+#define M_RXRATETCKQ1    0xffU
+#define V_RXRATETCKQ1(x) ((x) << S_RXRATETCKQ1)
+#define G_RXRATETCKQ1(x) (((x) >> S_RXRATETCKQ1) & M_RXRATETCKQ1)
+
+#define S_RXRATEINCQ0    8
+#define M_RXRATEINCQ0    0xffU
+#define V_RXRATEINCQ0(x) ((x) << S_RXRATEINCQ0)
+#define G_RXRATEINCQ0(x) (((x) >> S_RXRATEINCQ0) & M_RXRATEINCQ0)
+
+#define S_RXRATETCKQ0    0
+#define M_RXRATETCKQ0    0xffU
+#define V_RXRATETCKQ0(x) ((x) << S_RXRATETCKQ0)
+#define G_RXRATETCKQ0(x) (((x) >> S_RXRATETCKQ0) & M_RXRATETCKQ0)
+
+#define A_TP_TX_MOD_C3_C2_RATE_LIMIT 0xa
+#define A_TP_TX_MOD_C1_C0_RATE_LIMIT 0xb
+#define A_TP_RX_SCHED_MAP 0x20
+
+#define S_RXMAPCHANNEL3    24
+#define M_RXMAPCHANNEL3    0xffU
+#define V_RXMAPCHANNEL3(x) ((x) << S_RXMAPCHANNEL3)
+#define G_RXMAPCHANNEL3(x) (((x) >> S_RXMAPCHANNEL3) & M_RXMAPCHANNEL3)
+
+#define S_RXMAPCHANNEL2    16
+#define M_RXMAPCHANNEL2    0xffU
+#define V_RXMAPCHANNEL2(x) ((x) << S_RXMAPCHANNEL2)
+#define G_RXMAPCHANNEL2(x) (((x) >> S_RXMAPCHANNEL2) & M_RXMAPCHANNEL2)
+
+#define S_RXMAPCHANNEL1    8
+#define M_RXMAPCHANNEL1    0xffU
+#define V_RXMAPCHANNEL1(x) ((x) << S_RXMAPCHANNEL1)
+#define G_RXMAPCHANNEL1(x) (((x) >> S_RXMAPCHANNEL1) & M_RXMAPCHANNEL1)
+
+#define S_RXMAPCHANNEL0    0
+#define M_RXMAPCHANNEL0    0xffU
+#define V_RXMAPCHANNEL0(x) ((x) << S_RXMAPCHANNEL0)
+#define G_RXMAPCHANNEL0(x) (((x) >> S_RXMAPCHANNEL0) & M_RXMAPCHANNEL0)
+
+#define A_TP_RX_SCHED_SGE 0x21
+
+#define S_RXSGEMOD1    12
+#define M_RXSGEMOD1    0xfU
+#define V_RXSGEMOD1(x) ((x) << S_RXSGEMOD1)
+#define G_RXSGEMOD1(x) (((x) >> S_RXSGEMOD1) & M_RXSGEMOD1)
+
+#define S_RXSGEMOD0    8
+#define M_RXSGEMOD0    0xfU
+#define V_RXSGEMOD0(x) ((x) << S_RXSGEMOD0)
+#define G_RXSGEMOD0(x) (((x) >> S_RXSGEMOD0) & M_RXSGEMOD0)
+
+#define S_RXSGECHANNEL3    3
+#define V_RXSGECHANNEL3(x) ((x) << S_RXSGECHANNEL3)
+#define F_RXSGECHANNEL3    V_RXSGECHANNEL3(1U)
+
+#define S_RXSGECHANNEL2    2
+#define V_RXSGECHANNEL2(x) ((x) << S_RXSGECHANNEL2)
+#define F_RXSGECHANNEL2    V_RXSGECHANNEL2(1U)
+
+#define S_RXSGECHANNEL1    1
+#define V_RXSGECHANNEL1(x) ((x) << S_RXSGECHANNEL1)
+#define F_RXSGECHANNEL1    V_RXSGECHANNEL1(1U)
+
+#define S_RXSGECHANNEL0    0
+#define V_RXSGECHANNEL0(x) ((x) << S_RXSGECHANNEL0)
+#define F_RXSGECHANNEL0    V_RXSGECHANNEL0(1U)
+
+#define A_TP_TX_SCHED_MAP 0x22
+
+#define S_TXMAPCHANNEL3    12
+#define M_TXMAPCHANNEL3    0xfU
+#define V_TXMAPCHANNEL3(x) ((x) << S_TXMAPCHANNEL3)
+#define G_TXMAPCHANNEL3(x) (((x) >> S_TXMAPCHANNEL3) & M_TXMAPCHANNEL3)
+
+#define S_TXMAPCHANNEL2    8
+#define M_TXMAPCHANNEL2    0xfU
+#define V_TXMAPCHANNEL2(x) ((x) << S_TXMAPCHANNEL2)
+#define G_TXMAPCHANNEL2(x) (((x) >> S_TXMAPCHANNEL2) & M_TXMAPCHANNEL2)
+
+#define S_TXMAPCHANNEL1    4
+#define M_TXMAPCHANNEL1    0xfU
+#define V_TXMAPCHANNEL1(x) ((x) << S_TXMAPCHANNEL1)
+#define G_TXMAPCHANNEL1(x) (((x) >> S_TXMAPCHANNEL1) & M_TXMAPCHANNEL1)
+
+#define S_TXMAPCHANNEL0    0
+#define M_TXMAPCHANNEL0    0xfU
+#define V_TXMAPCHANNEL0(x) ((x) << S_TXMAPCHANNEL0)
+#define G_TXMAPCHANNEL0(x) (((x) >> S_TXMAPCHANNEL0) & M_TXMAPCHANNEL0)
+
+#define A_TP_TX_SCHED_HDR 0x23
+
+#define S_TXMAPHDRCHANNEL7    28
+#define M_TXMAPHDRCHANNEL7    0xfU
+#define V_TXMAPHDRCHANNEL7(x) ((x) << S_TXMAPHDRCHANNEL7)
+#define G_TXMAPHDRCHANNEL7(x) (((x) >> S_TXMAPHDRCHANNEL7) & M_TXMAPHDRCHANNEL7)
+
+#define S_TXMAPHDRCHANNEL6    24
+#define M_TXMAPHDRCHANNEL6    0xfU
+#define V_TXMAPHDRCHANNEL6(x) ((x) << S_TXMAPHDRCHANNEL6)
+#define G_TXMAPHDRCHANNEL6(x) (((x) >> S_TXMAPHDRCHANNEL6) & M_TXMAPHDRCHANNEL6)
+
+#define S_TXMAPHDRCHANNEL5    20
+#define M_TXMAPHDRCHANNEL5    0xfU
+#define V_TXMAPHDRCHANNEL5(x) ((x) << S_TXMAPHDRCHANNEL5)
+#define G_TXMAPHDRCHANNEL5(x) (((x) >> S_TXMAPHDRCHANNEL5) & M_TXMAPHDRCHANNEL5)
+
+#define S_TXMAPHDRCHANNEL4    16
+#define M_TXMAPHDRCHANNEL4    0xfU
+#define V_TXMAPHDRCHANNEL4(x) ((x) << S_TXMAPHDRCHANNEL4)
+#define G_TXMAPHDRCHANNEL4(x) (((x) >> S_TXMAPHDRCHANNEL4) & M_TXMAPHDRCHANNEL4)
+
+#define S_TXMAPHDRCHANNEL3    12
+#define M_TXMAPHDRCHANNEL3    0xfU
+#define V_TXMAPHDRCHANNEL3(x) ((x) << S_TXMAPHDRCHANNEL3)
+#define G_TXMAPHDRCHANNEL3(x) (((x) >> S_TXMAPHDRCHANNEL3) & M_TXMAPHDRCHANNEL3)
+
+#define S_TXMAPHDRCHANNEL2    8
+#define M_TXMAPHDRCHANNEL2    0xfU
+#define V_TXMAPHDRCHANNEL2(x) ((x) << S_TXMAPHDRCHANNEL2)
+#define G_TXMAPHDRCHANNEL2(x) (((x) >> S_TXMAPHDRCHANNEL2) & M_TXMAPHDRCHANNEL2)
+
+#define S_TXMAPHDRCHANNEL1    4
+#define M_TXMAPHDRCHANNEL1    0xfU
+#define V_TXMAPHDRCHANNEL1(x) ((x) << S_TXMAPHDRCHANNEL1)
+#define G_TXMAPHDRCHANNEL1(x) (((x) >> S_TXMAPHDRCHANNEL1) & M_TXMAPHDRCHANNEL1)
+
+#define S_TXMAPHDRCHANNEL0    0
+#define M_TXMAPHDRCHANNEL0    0xfU
+#define V_TXMAPHDRCHANNEL0(x) ((x) << S_TXMAPHDRCHANNEL0)
+#define G_TXMAPHDRCHANNEL0(x) (((x) >> S_TXMAPHDRCHANNEL0) & M_TXMAPHDRCHANNEL0)
+
+#define A_TP_TX_SCHED_FIFO 0x24
+
+#define S_TXMAPFIFOCHANNEL7    28
+#define M_TXMAPFIFOCHANNEL7    0xfU
+#define V_TXMAPFIFOCHANNEL7(x) ((x) << S_TXMAPFIFOCHANNEL7)
+#define G_TXMAPFIFOCHANNEL7(x) (((x) >> S_TXMAPFIFOCHANNEL7) & M_TXMAPFIFOCHANNEL7)
+
+#define S_TXMAPFIFOCHANNEL6    24
+#define M_TXMAPFIFOCHANNEL6    0xfU
+#define V_TXMAPFIFOCHANNEL6(x) ((x) << S_TXMAPFIFOCHANNEL6)
+#define G_TXMAPFIFOCHANNEL6(x) (((x) >> S_TXMAPFIFOCHANNEL6) & M_TXMAPFIFOCHANNEL6)
+
+#define S_TXMAPFIFOCHANNEL5    20
+#define M_TXMAPFIFOCHANNEL5    0xfU
+#define V_TXMAPFIFOCHANNEL5(x) ((x) << S_TXMAPFIFOCHANNEL5)
+#define G_TXMAPFIFOCHANNEL5(x) (((x) >> S_TXMAPFIFOCHANNEL5) & M_TXMAPFIFOCHANNEL5)
+
+#define S_TXMAPFIFOCHANNEL4    16
+#define M_TXMAPFIFOCHANNEL4    0xfU
+#define V_TXMAPFIFOCHANNEL4(x) ((x) << S_TXMAPFIFOCHANNEL4)
+#define G_TXMAPFIFOCHANNEL4(x) (((x) >> S_TXMAPFIFOCHANNEL4) & M_TXMAPFIFOCHANNEL4)
+
+#define S_TXMAPFIFOCHANNEL3    12
+#define M_TXMAPFIFOCHANNEL3    0xfU
+#define V_TXMAPFIFOCHANNEL3(x) ((x) << S_TXMAPFIFOCHANNEL3)
+#define G_TXMAPFIFOCHANNEL3(x) (((x) >> S_TXMAPFIFOCHANNEL3) & M_TXMAPFIFOCHANNEL3)
+
+#define S_TXMAPFIFOCHANNEL2    8
+#define M_TXMAPFIFOCHANNEL2    0xfU
+#define V_TXMAPFIFOCHANNEL2(x) ((x) << S_TXMAPFIFOCHANNEL2)
+#define G_TXMAPFIFOCHANNEL2(x) (((x) >> S_TXMAPFIFOCHANNEL2) & M_TXMAPFIFOCHANNEL2)
+
+#define S_TXMAPFIFOCHANNEL1    4
+#define M_TXMAPFIFOCHANNEL1    0xfU
+#define V_TXMAPFIFOCHANNEL1(x) ((x) << S_TXMAPFIFOCHANNEL1)
+#define G_TXMAPFIFOCHANNEL1(x) (((x) >> S_TXMAPFIFOCHANNEL1) & M_TXMAPFIFOCHANNEL1)
+
+#define S_TXMAPFIFOCHANNEL0    0
+#define M_TXMAPFIFOCHANNEL0    0xfU
+#define V_TXMAPFIFOCHANNEL0(x) ((x) << S_TXMAPFIFOCHANNEL0)
+#define G_TXMAPFIFOCHANNEL0(x) (((x) >> S_TXMAPFIFOCHANNEL0) & M_TXMAPFIFOCHANNEL0)
+
+#define A_TP_TX_SCHED_PCMD 0x25
+
+#define S_TXMAPPCMDCHANNEL7    28
+#define M_TXMAPPCMDCHANNEL7    0xfU
+#define V_TXMAPPCMDCHANNEL7(x) ((x) << S_TXMAPPCMDCHANNEL7)
+#define G_TXMAPPCMDCHANNEL7(x) (((x) >> S_TXMAPPCMDCHANNEL7) & M_TXMAPPCMDCHANNEL7)
+
+#define S_TXMAPPCMDCHANNEL6    24
+#define M_TXMAPPCMDCHANNEL6    0xfU
+#define V_TXMAPPCMDCHANNEL6(x) ((x) << S_TXMAPPCMDCHANNEL6)
+#define G_TXMAPPCMDCHANNEL6(x) (((x) >> S_TXMAPPCMDCHANNEL6) & M_TXMAPPCMDCHANNEL6)
+
+#define S_TXMAPPCMDCHANNEL5    20
+#define M_TXMAPPCMDCHANNEL5    0xfU
+#define V_TXMAPPCMDCHANNEL5(x) ((x) << S_TXMAPPCMDCHANNEL5)
+#define G_TXMAPPCMDCHANNEL5(x) (((x) >> S_TXMAPPCMDCHANNEL5) & M_TXMAPPCMDCHANNEL5)
+
+#define S_TXMAPPCMDCHANNEL4    16
+#define M_TXMAPPCMDCHANNEL4    0xfU
+#define V_TXMAPPCMDCHANNEL4(x) ((x) << S_TXMAPPCMDCHANNEL4)
+#define G_TXMAPPCMDCHANNEL4(x) (((x) >> S_TXMAPPCMDCHANNEL4) & M_TXMAPPCMDCHANNEL4)
+
+#define S_TXMAPPCMDCHANNEL3    12
+#define M_TXMAPPCMDCHANNEL3    0xfU
+#define V_TXMAPPCMDCHANNEL3(x) ((x) << S_TXMAPPCMDCHANNEL3)
+#define G_TXMAPPCMDCHANNEL3(x) (((x) >> S_TXMAPPCMDCHANNEL3) & M_TXMAPPCMDCHANNEL3)
+
+#define S_TXMAPPCMDCHANNEL2    8
+#define M_TXMAPPCMDCHANNEL2    0xfU
+#define V_TXMAPPCMDCHANNEL2(x) ((x) << S_TXMAPPCMDCHANNEL2)
+#define G_TXMAPPCMDCHANNEL2(x) (((x) >> S_TXMAPPCMDCHANNEL2) & M_TXMAPPCMDCHANNEL2)
+
+#define S_TXMAPPCMDCHANNEL1    4
+#define M_TXMAPPCMDCHANNEL1    0xfU
+#define V_TXMAPPCMDCHANNEL1(x) ((x) << S_TXMAPPCMDCHANNEL1)
+#define G_TXMAPPCMDCHANNEL1(x) (((x) >> S_TXMAPPCMDCHANNEL1) & M_TXMAPPCMDCHANNEL1)
+
+#define S_TXMAPPCMDCHANNEL0    0
+#define M_TXMAPPCMDCHANNEL0    0xfU
+#define V_TXMAPPCMDCHANNEL0(x) ((x) << S_TXMAPPCMDCHANNEL0)
+#define G_TXMAPPCMDCHANNEL0(x) (((x) >> S_TXMAPPCMDCHANNEL0) & M_TXMAPPCMDCHANNEL0)
+
+#define A_TP_TX_SCHED_LPBK 0x26
+
+#define S_TXMAPLPBKCHANNEL7    28
+#define M_TXMAPLPBKCHANNEL7    0xfU
+#define V_TXMAPLPBKCHANNEL7(x) ((x) << S_TXMAPLPBKCHANNEL7)
+#define G_TXMAPLPBKCHANNEL7(x) (((x) >> S_TXMAPLPBKCHANNEL7) & M_TXMAPLPBKCHANNEL7)
+
+#define S_TXMAPLPBKCHANNEL6    24
+#define M_TXMAPLPBKCHANNEL6    0xfU
+#define V_TXMAPLPBKCHANNEL6(x) ((x) << S_TXMAPLPBKCHANNEL6)
+#define G_TXMAPLPBKCHANNEL6(x) (((x) >> S_TXMAPLPBKCHANNEL6) & M_TXMAPLPBKCHANNEL6)
+
+#define S_TXMAPLPBKCHANNEL5    20
+#define M_TXMAPLPBKCHANNEL5    0xfU
+#define V_TXMAPLPBKCHANNEL5(x) ((x) << S_TXMAPLPBKCHANNEL5)
+#define G_TXMAPLPBKCHANNEL5(x) (((x) >> S_TXMAPLPBKCHANNEL5) & M_TXMAPLPBKCHANNEL5)
+
+#define S_TXMAPLPBKCHANNEL4    16
+#define M_TXMAPLPBKCHANNEL4    0xfU
+#define V_TXMAPLPBKCHANNEL4(x) ((x) << S_TXMAPLPBKCHANNEL4)
+#define G_TXMAPLPBKCHANNEL4(x) (((x) >> S_TXMAPLPBKCHANNEL4) & M_TXMAPLPBKCHANNEL4)
+
+#define S_TXMAPLPBKCHANNEL3    12
+#define M_TXMAPLPBKCHANNEL3    0xfU
+#define V_TXMAPLPBKCHANNEL3(x) ((x) << S_TXMAPLPBKCHANNEL3)
+#define G_TXMAPLPBKCHANNEL3(x) (((x) >> S_TXMAPLPBKCHANNEL3) & M_TXMAPLPBKCHANNEL3)
+
+#define S_TXMAPLPBKCHANNEL2    8
+#define M_TXMAPLPBKCHANNEL2    0xfU
+#define V_TXMAPLPBKCHANNEL2(x) ((x) << S_TXMAPLPBKCHANNEL2)
+#define G_TXMAPLPBKCHANNEL2(x) (((x) >> S_TXMAPLPBKCHANNEL2) & M_TXMAPLPBKCHANNEL2)
+
+#define S_TXMAPLPBKCHANNEL1    4
+#define M_TXMAPLPBKCHANNEL1    0xfU
+#define V_TXMAPLPBKCHANNEL1(x) ((x) << S_TXMAPLPBKCHANNEL1)
+#define G_TXMAPLPBKCHANNEL1(x) (((x) >> S_TXMAPLPBKCHANNEL1) & M_TXMAPLPBKCHANNEL1)
+
+#define S_TXMAPLPBKCHANNEL0    0
+#define M_TXMAPLPBKCHANNEL0    0xfU
+#define V_TXMAPLPBKCHANNEL0(x) ((x) << S_TXMAPLPBKCHANNEL0)
+#define G_TXMAPLPBKCHANNEL0(x) (((x) >> S_TXMAPLPBKCHANNEL0) & M_TXMAPLPBKCHANNEL0)
+
+#define A_TP_CHANNEL_MAP 0x27
+
+#define S_RXMAPCHANNELELN    16
+#define M_RXMAPCHANNELELN    0xfU
+#define V_RXMAPCHANNELELN(x) ((x) << S_RXMAPCHANNELELN)
+#define G_RXMAPCHANNELELN(x) (((x) >> S_RXMAPCHANNELELN) & M_RXMAPCHANNELELN)
+
+#define S_RXMAPE2LCHANNEL3    14
+#define M_RXMAPE2LCHANNEL3    0x3U
+#define V_RXMAPE2LCHANNEL3(x) ((x) << S_RXMAPE2LCHANNEL3)
+#define G_RXMAPE2LCHANNEL3(x) (((x) >> S_RXMAPE2LCHANNEL3) & M_RXMAPE2LCHANNEL3)
+
+#define S_RXMAPE2LCHANNEL2    12
+#define M_RXMAPE2LCHANNEL2    0x3U
+#define V_RXMAPE2LCHANNEL2(x) ((x) << S_RXMAPE2LCHANNEL2)
+#define G_RXMAPE2LCHANNEL2(x) (((x) >> S_RXMAPE2LCHANNEL2) & M_RXMAPE2LCHANNEL2)
+
+#define S_RXMAPE2LCHANNEL1    10
+#define M_RXMAPE2LCHANNEL1    0x3U
+#define V_RXMAPE2LCHANNEL1(x) ((x) << S_RXMAPE2LCHANNEL1)
+#define G_RXMAPE2LCHANNEL1(x) (((x) >> S_RXMAPE2LCHANNEL1) & M_RXMAPE2LCHANNEL1)
+
+#define S_RXMAPE2LCHANNEL0    8
+#define M_RXMAPE2LCHANNEL0    0x3U
+#define V_RXMAPE2LCHANNEL0(x) ((x) << S_RXMAPE2LCHANNEL0)
+#define G_RXMAPE2LCHANNEL0(x) (((x) >> S_RXMAPE2LCHANNEL0) & M_RXMAPE2LCHANNEL0)
+
+#define S_RXMAPC2CCHANNEL3    7
+#define V_RXMAPC2CCHANNEL3(x) ((x) << S_RXMAPC2CCHANNEL3)
+#define F_RXMAPC2CCHANNEL3    V_RXMAPC2CCHANNEL3(1U)
+
+#define S_RXMAPC2CCHANNEL2    6
+#define V_RXMAPC2CCHANNEL2(x) ((x) << S_RXMAPC2CCHANNEL2)
+#define F_RXMAPC2CCHANNEL2    V_RXMAPC2CCHANNEL2(1U)
+
+#define S_RXMAPC2CCHANNEL1    5
+#define V_RXMAPC2CCHANNEL1(x) ((x) << S_RXMAPC2CCHANNEL1)
+#define F_RXMAPC2CCHANNEL1    V_RXMAPC2CCHANNEL1(1U)
+
+#define S_RXMAPC2CCHANNEL0    4
+#define V_RXMAPC2CCHANNEL0(x) ((x) << S_RXMAPC2CCHANNEL0)
+#define F_RXMAPC2CCHANNEL0    V_RXMAPC2CCHANNEL0(1U)
+
+#define S_RXMAPE2CCHANNEL3    3
+#define V_RXMAPE2CCHANNEL3(x) ((x) << S_RXMAPE2CCHANNEL3)
+#define F_RXMAPE2CCHANNEL3    V_RXMAPE2CCHANNEL3(1U)
+
+#define S_RXMAPE2CCHANNEL2    2
+#define V_RXMAPE2CCHANNEL2(x) ((x) << S_RXMAPE2CCHANNEL2)
+#define F_RXMAPE2CCHANNEL2    V_RXMAPE2CCHANNEL2(1U)
+
+#define S_RXMAPE2CCHANNEL1    1
+#define V_RXMAPE2CCHANNEL1(x) ((x) << S_RXMAPE2CCHANNEL1)
+#define F_RXMAPE2CCHANNEL1    V_RXMAPE2CCHANNEL1(1U)
+
+#define S_RXMAPE2CCHANNEL0    0
+#define V_RXMAPE2CCHANNEL0(x) ((x) << S_RXMAPE2CCHANNEL0)
+#define F_RXMAPE2CCHANNEL0    V_RXMAPE2CCHANNEL0(1U)
+
+#define A_TP_RX_LPBK 0x28
+#define A_TP_TX_LPBK 0x29
+#define A_TP_TX_SCHED_PPP 0x2a
+
+#define S_TXPPPENPORT3    24
+#define M_TXPPPENPORT3    0xffU
+#define V_TXPPPENPORT3(x) ((x) << S_TXPPPENPORT3)
+#define G_TXPPPENPORT3(x) (((x) >> S_TXPPPENPORT3) & M_TXPPPENPORT3)
+
+#define S_TXPPPENPORT2    16
+#define M_TXPPPENPORT2    0xffU
+#define V_TXPPPENPORT2(x) ((x) << S_TXPPPENPORT2)
+#define G_TXPPPENPORT2(x) (((x) >> S_TXPPPENPORT2) & M_TXPPPENPORT2)
+
+#define S_TXPPPENPORT1    8
+#define M_TXPPPENPORT1    0xffU
+#define V_TXPPPENPORT1(x) ((x) << S_TXPPPENPORT1)
+#define G_TXPPPENPORT1(x) (((x) >> S_TXPPPENPORT1) & M_TXPPPENPORT1)
+
+#define S_TXPPPENPORT0    0
+#define M_TXPPPENPORT0    0xffU
+#define V_TXPPPENPORT0(x) ((x) << S_TXPPPENPORT0)
+#define G_TXPPPENPORT0(x) (((x) >> S_TXPPPENPORT0) & M_TXPPPENPORT0)
+
+#define A_TP_IPMI_CFG1 0x2e
+
+#define S_VLANENABLE    31
+#define V_VLANENABLE(x) ((x) << S_VLANENABLE)
+#define F_VLANENABLE    V_VLANENABLE(1U)
+
+#define S_PRIMARYPORTENABLE    30
+#define V_PRIMARYPORTENABLE(x) ((x) << S_PRIMARYPORTENABLE)
+#define F_PRIMARYPORTENABLE    V_PRIMARYPORTENABLE(1U)
+
+#define S_SECUREPORTENABLE    29
+#define V_SECUREPORTENABLE(x) ((x) << S_SECUREPORTENABLE)
+#define F_SECUREPORTENABLE    V_SECUREPORTENABLE(1U)
+
+#define S_ARPENABLE    28
+#define V_ARPENABLE(x) ((x) << S_ARPENABLE)
+#define F_ARPENABLE    V_ARPENABLE(1U)
+
+#define S_IPMI_VLAN    0
+#define M_IPMI_VLAN    0xffffU
+#define V_IPMI_VLAN(x) ((x) << S_IPMI_VLAN)
+#define G_IPMI_VLAN(x) (((x) >> S_IPMI_VLAN) & M_IPMI_VLAN)
+
+#define A_TP_IPMI_CFG2 0x2f
+
+#define S_SECUREPORT    16
+#define M_SECUREPORT    0xffffU
+#define V_SECUREPORT(x) ((x) << S_SECUREPORT)
+#define G_SECUREPORT(x) (((x) >> S_SECUREPORT) & M_SECUREPORT)
+
+#define S_PRIMARYPORT    0
+#define M_PRIMARYPORT    0xffffU
+#define V_PRIMARYPORT(x) ((x) << S_PRIMARYPORT)
+#define G_PRIMARYPORT(x) (((x) >> S_PRIMARYPORT) & M_PRIMARYPORT)
+
+#define A_TP_RSS_PF0_CONFIG 0x30
+
+#define S_MAPENABLE    31
+#define V_MAPENABLE(x) ((x) << S_MAPENABLE)
+#define F_MAPENABLE    V_MAPENABLE(1U)
+
+#define S_CHNENABLE    30
+#define V_CHNENABLE(x) ((x) << S_CHNENABLE)
+#define F_CHNENABLE    V_CHNENABLE(1U)
+
+#define S_PRTENABLE    29
+#define V_PRTENABLE(x) ((x) << S_PRTENABLE)
+#define F_PRTENABLE    V_PRTENABLE(1U)
+
+#define S_UDPFOURTUPEN    28
+#define V_UDPFOURTUPEN(x) ((x) << S_UDPFOURTUPEN)
+#define F_UDPFOURTUPEN    V_UDPFOURTUPEN(1U)
+
+#define S_IP6FOURTUPEN    27
+#define V_IP6FOURTUPEN(x) ((x) << S_IP6FOURTUPEN)
+#define F_IP6FOURTUPEN    V_IP6FOURTUPEN(1U)
+
+#define S_IP6TWOTUPEN    26
+#define V_IP6TWOTUPEN(x) ((x) << S_IP6TWOTUPEN)
+#define F_IP6TWOTUPEN    V_IP6TWOTUPEN(1U)
+
+#define S_IP4FOURTUPEN    25
+#define V_IP4FOURTUPEN(x) ((x) << S_IP4FOURTUPEN)
+#define F_IP4FOURTUPEN    V_IP4FOURTUPEN(1U)
+
+#define S_IP4TWOTUPEN    24
+#define V_IP4TWOTUPEN(x) ((x) << S_IP4TWOTUPEN)
+#define F_IP4TWOTUPEN    V_IP4TWOTUPEN(1U)
+
+#define S_IVFWIDTH    20
+#define M_IVFWIDTH    0xfU
+#define V_IVFWIDTH(x) ((x) << S_IVFWIDTH)
+#define G_IVFWIDTH(x) (((x) >> S_IVFWIDTH) & M_IVFWIDTH)
+
+#define S_CH1DEFAULTQUEUE    10
+#define M_CH1DEFAULTQUEUE    0x3ffU
+#define V_CH1DEFAULTQUEUE(x) ((x) << S_CH1DEFAULTQUEUE)
+#define G_CH1DEFAULTQUEUE(x) (((x) >> S_CH1DEFAULTQUEUE) & M_CH1DEFAULTQUEUE)
+
+#define S_CH0DEFAULTQUEUE    0
+#define M_CH0DEFAULTQUEUE    0x3ffU
+#define V_CH0DEFAULTQUEUE(x) ((x) << S_CH0DEFAULTQUEUE)
+#define G_CH0DEFAULTQUEUE(x) (((x) >> S_CH0DEFAULTQUEUE) & M_CH0DEFAULTQUEUE)
+
+#define A_TP_RSS_PF1_CONFIG 0x31
+#define A_TP_RSS_PF2_CONFIG 0x32
+#define A_TP_RSS_PF3_CONFIG 0x33
+#define A_TP_RSS_PF4_CONFIG 0x34
+#define A_TP_RSS_PF5_CONFIG 0x35
+#define A_TP_RSS_PF6_CONFIG 0x36
+#define A_TP_RSS_PF7_CONFIG 0x37
+#define A_TP_RSS_PF_MAP 0x38
+
+#define S_LKPIDXSIZE    24
+#define M_LKPIDXSIZE    0x3U
+#define V_LKPIDXSIZE(x) ((x) << S_LKPIDXSIZE)
+#define G_LKPIDXSIZE(x) (((x) >> S_LKPIDXSIZE) & M_LKPIDXSIZE)
+
+#define S_PF7LKPIDX    21
+#define M_PF7LKPIDX    0x7U
+#define V_PF7LKPIDX(x) ((x) << S_PF7LKPIDX)
+#define G_PF7LKPIDX(x) (((x) >> S_PF7LKPIDX) & M_PF7LKPIDX)
+
+#define S_PF6LKPIDX    18
+#define M_PF6LKPIDX    0x7U
+#define V_PF6LKPIDX(x) ((x) << S_PF6LKPIDX)
+#define G_PF6LKPIDX(x) (((x) >> S_PF6LKPIDX) & M_PF6LKPIDX)
+
+#define S_PF5LKPIDX    15
+#define M_PF5LKPIDX    0x7U
+#define V_PF5LKPIDX(x) ((x) << S_PF5LKPIDX)
+#define G_PF5LKPIDX(x) (((x) >> S_PF5LKPIDX) & M_PF5LKPIDX)
+
+#define S_PF4LKPIDX    12
+#define M_PF4LKPIDX    0x7U
+#define V_PF4LKPIDX(x) ((x) << S_PF4LKPIDX)
+#define G_PF4LKPIDX(x) (((x) >> S_PF4LKPIDX) & M_PF4LKPIDX)
+
+#define S_PF3LKPIDX    9
+#define M_PF3LKPIDX    0x7U
+#define V_PF3LKPIDX(x) ((x) << S_PF3LKPIDX)
+#define G_PF3LKPIDX(x) (((x) >> S_PF3LKPIDX) & M_PF3LKPIDX)
+
+#define S_PF2LKPIDX    6
+#define M_PF2LKPIDX    0x7U
+#define V_PF2LKPIDX(x) ((x) << S_PF2LKPIDX)
+#define G_PF2LKPIDX(x) (((x) >> S_PF2LKPIDX) & M_PF2LKPIDX)
+
+#define S_PF1LKPIDX    3
+#define M_PF1LKPIDX    0x7U
+#define V_PF1LKPIDX(x) ((x) << S_PF1LKPIDX)
+#define G_PF1LKPIDX(x) (((x) >> S_PF1LKPIDX) & M_PF1LKPIDX)
+
+#define S_PF0LKPIDX    0
+#define M_PF0LKPIDX    0x7U
+#define V_PF0LKPIDX(x) ((x) << S_PF0LKPIDX)
+#define G_PF0LKPIDX(x) (((x) >> S_PF0LKPIDX) & M_PF0LKPIDX)
+
+#define A_TP_RSS_PF_MSK 0x39
+
+#define S_PF7MSKSIZE    28
+#define M_PF7MSKSIZE    0xfU
+#define V_PF7MSKSIZE(x) ((x) << S_PF7MSKSIZE)
+#define G_PF7MSKSIZE(x) (((x) >> S_PF7MSKSIZE) & M_PF7MSKSIZE)
+
+#define S_PF6MSKSIZE    24
+#define M_PF6MSKSIZE    0xfU
+#define V_PF6MSKSIZE(x) ((x) << S_PF6MSKSIZE)
+#define G_PF6MSKSIZE(x) (((x) >> S_PF6MSKSIZE) & M_PF6MSKSIZE)
+
+#define S_PF5MSKSIZE    20
+#define M_PF5MSKSIZE    0xfU
+#define V_PF5MSKSIZE(x) ((x) << S_PF5MSKSIZE)
+#define G_PF5MSKSIZE(x) (((x) >> S_PF5MSKSIZE) & M_PF5MSKSIZE)
+
+#define S_PF4MSKSIZE    16
+#define M_PF4MSKSIZE    0xfU
+#define V_PF4MSKSIZE(x) ((x) << S_PF4MSKSIZE)
+#define G_PF4MSKSIZE(x) (((x) >> S_PF4MSKSIZE) & M_PF4MSKSIZE)
+
+#define S_PF3MSKSIZE    12
+#define M_PF3MSKSIZE    0xfU
+#define V_PF3MSKSIZE(x) ((x) << S_PF3MSKSIZE)
+#define G_PF3MSKSIZE(x) (((x) >> S_PF3MSKSIZE) & M_PF3MSKSIZE)
+
+#define S_PF2MSKSIZE    8
+#define M_PF2MSKSIZE    0xfU
+#define V_PF2MSKSIZE(x) ((x) << S_PF2MSKSIZE)
+#define G_PF2MSKSIZE(x) (((x) >> S_PF2MSKSIZE) & M_PF2MSKSIZE)
+
+#define S_PF1MSKSIZE    4
+#define M_PF1MSKSIZE    0xfU
+#define V_PF1MSKSIZE(x) ((x) << S_PF1MSKSIZE)
+#define G_PF1MSKSIZE(x) (((x) >> S_PF1MSKSIZE) & M_PF1MSKSIZE)
+
+#define S_PF0MSKSIZE    0
+#define M_PF0MSKSIZE    0xfU
+#define V_PF0MSKSIZE(x) ((x) << S_PF0MSKSIZE)
+#define G_PF0MSKSIZE(x) (((x) >> S_PF0MSKSIZE) & M_PF0MSKSIZE)
+
+#define A_TP_RSS_VFL_CONFIG 0x3a
+#define A_TP_RSS_VFH_CONFIG 0x3b
+
+#define S_ENABLEUDPHASH    31
+#define V_ENABLEUDPHASH(x) ((x) << S_ENABLEUDPHASH)
+#define F_ENABLEUDPHASH    V_ENABLEUDPHASH(1U)
+
+#define S_VFUPEN    30
+#define V_VFUPEN(x) ((x) << S_VFUPEN)
+#define F_VFUPEN    V_VFUPEN(1U)
+
+#define S_VFVLNEX    28
+#define V_VFVLNEX(x) ((x) << S_VFVLNEX)
+#define F_VFVLNEX    V_VFVLNEX(1U)
+
+#define S_VFPRTEN    27
+#define V_VFPRTEN(x) ((x) << S_VFPRTEN)
+#define F_VFPRTEN    V_VFPRTEN(1U)
+
+#define S_VFCHNEN    26
+#define V_VFCHNEN(x) ((x) << S_VFCHNEN)
+#define F_VFCHNEN    V_VFCHNEN(1U)
+
+#define S_DEFAULTQUEUE    16
+#define M_DEFAULTQUEUE    0x3ffU
+#define V_DEFAULTQUEUE(x) ((x) << S_DEFAULTQUEUE)
+#define G_DEFAULTQUEUE(x) (((x) >> S_DEFAULTQUEUE) & M_DEFAULTQUEUE)
+
+#define S_VFLKPIDX    8
+#define M_VFLKPIDX    0xffU
+#define V_VFLKPIDX(x) ((x) << S_VFLKPIDX)
+#define G_VFLKPIDX(x) (((x) >> S_VFLKPIDX) & M_VFLKPIDX)
+
+#define S_VFIP6FOURTUPEN    7
+#define V_VFIP6FOURTUPEN(x) ((x) << S_VFIP6FOURTUPEN)
+#define F_VFIP6FOURTUPEN    V_VFIP6FOURTUPEN(1U)
+
+#define S_VFIP6TWOTUPEN    6
+#define V_VFIP6TWOTUPEN(x) ((x) << S_VFIP6TWOTUPEN)
+#define F_VFIP6TWOTUPEN    V_VFIP6TWOTUPEN(1U)
+
+#define S_VFIP4FOURTUPEN    5
+#define V_VFIP4FOURTUPEN(x) ((x) << S_VFIP4FOURTUPEN)
+#define F_VFIP4FOURTUPEN    V_VFIP4FOURTUPEN(1U)
+
+#define S_VFIP4TWOTUPEN    4
+#define V_VFIP4TWOTUPEN(x) ((x) << S_VFIP4TWOTUPEN)
+#define F_VFIP4TWOTUPEN    V_VFIP4TWOTUPEN(1U)
+
+#define S_KEYINDEX    0
+#define M_KEYINDEX    0xfU
+#define V_KEYINDEX(x) ((x) << S_KEYINDEX)
+#define G_KEYINDEX(x) (((x) >> S_KEYINDEX) & M_KEYINDEX)
+
+#define A_TP_RSS_SECRET_KEY0 0x40
+#define A_TP_RSS_SECRET_KEY1 0x41
+#define A_TP_RSS_SECRET_KEY2 0x42
+#define A_TP_RSS_SECRET_KEY3 0x43
+#define A_TP_RSS_SECRET_KEY4 0x44
+#define A_TP_RSS_SECRET_KEY5 0x45
+#define A_TP_RSS_SECRET_KEY6 0x46
+#define A_TP_RSS_SECRET_KEY7 0x47
+#define A_TP_RSS_SECRET_KEY8 0x48
+#define A_TP_RSS_SECRET_KEY9 0x49
+#define A_TP_ETHER_TYPE_VL 0x50
+
+#define S_CQFCTYPE    16
+#define M_CQFCTYPE    0xffffU
+#define V_CQFCTYPE(x) ((x) << S_CQFCTYPE)
+#define G_CQFCTYPE(x) (((x) >> S_CQFCTYPE) & M_CQFCTYPE)
+
+#define S_VLANTYPE    0
+#define M_VLANTYPE    0xffffU
+#define V_VLANTYPE(x) ((x) << S_VLANTYPE)
+#define G_VLANTYPE(x) (((x) >> S_VLANTYPE) & M_VLANTYPE)
+
+#define A_TP_ETHER_TYPE_IP 0x51
+
+#define S_IPV6TYPE    16
+#define M_IPV6TYPE    0xffffU
+#define V_IPV6TYPE(x) ((x) << S_IPV6TYPE)
+#define G_IPV6TYPE(x) (((x) >> S_IPV6TYPE) & M_IPV6TYPE)
+
+#define S_IPV4TYPE    0
+#define M_IPV4TYPE    0xffffU
+#define V_IPV4TYPE(x) ((x) << S_IPV4TYPE)
+#define G_IPV4TYPE(x) (((x) >> S_IPV4TYPE) & M_IPV4TYPE)
+
+#define A_TP_DBG_CLEAR 0x60
+#define A_TP_DBG_CORE_HDR0 0x61
+
+#define S_E_TCP_OP_SRDY    16
+#define V_E_TCP_OP_SRDY(x) ((x) << S_E_TCP_OP_SRDY)
+#define F_E_TCP_OP_SRDY    V_E_TCP_OP_SRDY(1U)
+
+#define S_E_PLD_TXZEROP_SRDY    15
+#define V_E_PLD_TXZEROP_SRDY(x) ((x) << S_E_PLD_TXZEROP_SRDY)
+#define F_E_PLD_TXZEROP_SRDY    V_E_PLD_TXZEROP_SRDY(1U)
+
+#define S_E_PLD_RX_SRDY    14
+#define V_E_PLD_RX_SRDY(x) ((x) << S_E_PLD_RX_SRDY)
+#define F_E_PLD_RX_SRDY    V_E_PLD_RX_SRDY(1U)
+
+#define S_E_RX_ERROR_SRDY    13
+#define V_E_RX_ERROR_SRDY(x) ((x) << S_E_RX_ERROR_SRDY)
+#define F_E_RX_ERROR_SRDY    V_E_RX_ERROR_SRDY(1U)
+
+#define S_E_RX_ISS_SRDY    12
+#define V_E_RX_ISS_SRDY(x) ((x) << S_E_RX_ISS_SRDY)
+#define F_E_RX_ISS_SRDY    V_E_RX_ISS_SRDY(1U)
+
+#define S_C_TCP_OP_SRDY    11
+#define V_C_TCP_OP_SRDY(x) ((x) << S_C_TCP_OP_SRDY)
+#define F_C_TCP_OP_SRDY    V_C_TCP_OP_SRDY(1U)
+
+#define S_C_PLD_TXZEROP_SRDY    10
+#define V_C_PLD_TXZEROP_SRDY(x) ((x) << S_C_PLD_TXZEROP_SRDY)
+#define F_C_PLD_TXZEROP_SRDY    V_C_PLD_TXZEROP_SRDY(1U)
+
+#define S_C_PLD_RX_SRDY    9
+#define V_C_PLD_RX_SRDY(x) ((x) << S_C_PLD_RX_SRDY)
+#define F_C_PLD_RX_SRDY    V_C_PLD_RX_SRDY(1U)
+
+#define S_C_RX_ERROR_SRDY    8
+#define V_C_RX_ERROR_SRDY(x) ((x) << S_C_RX_ERROR_SRDY)
+#define F_C_RX_ERROR_SRDY    V_C_RX_ERROR_SRDY(1U)
+
+#define S_C_RX_ISS_SRDY    7
+#define V_C_RX_ISS_SRDY(x) ((x) << S_C_RX_ISS_SRDY)
+#define F_C_RX_ISS_SRDY    V_C_RX_ISS_SRDY(1U)
+
+#define S_E_CPL5_TXVALID    6
+#define V_E_CPL5_TXVALID(x) ((x) << S_E_CPL5_TXVALID)
+#define F_E_CPL5_TXVALID    V_E_CPL5_TXVALID(1U)
+
+#define S_E_ETH_TXVALID    5
+#define V_E_ETH_TXVALID(x) ((x) << S_E_ETH_TXVALID)
+#define F_E_ETH_TXVALID    V_E_ETH_TXVALID(1U)
+
+#define S_E_IP_TXVALID    4
+#define V_E_IP_TXVALID(x) ((x) << S_E_IP_TXVALID)
+#define F_E_IP_TXVALID    V_E_IP_TXVALID(1U)
+
+#define S_E_TCP_TXVALID    3
+#define V_E_TCP_TXVALID(x) ((x) << S_E_TCP_TXVALID)
+#define F_E_TCP_TXVALID    V_E_TCP_TXVALID(1U)
+
+#define S_C_CPL5_RXVALID    2
+#define V_C_CPL5_RXVALID(x) ((x) << S_C_CPL5_RXVALID)
+#define F_C_CPL5_RXVALID    V_C_CPL5_RXVALID(1U)
+
+#define S_C_CPL5_TXVALID    1
+#define V_C_CPL5_TXVALID(x) ((x) << S_C_CPL5_TXVALID)
+#define F_C_CPL5_TXVALID    V_C_CPL5_TXVALID(1U)
+
+#define S_E_TCP_OPT_RXVALID    0
+#define V_E_TCP_OPT_RXVALID(x) ((x) << S_E_TCP_OPT_RXVALID)
+#define F_E_TCP_OPT_RXVALID    V_E_TCP_OPT_RXVALID(1U)
+
+#define A_TP_DBG_CORE_HDR1 0x62
+
+#define S_E_CPL5_TXFULL    6
+#define V_E_CPL5_TXFULL(x) ((x) << S_E_CPL5_TXFULL)
+#define F_E_CPL5_TXFULL    V_E_CPL5_TXFULL(1U)
+
+#define S_E_ETH_TXFULL    5
+#define V_E_ETH_TXFULL(x) ((x) << S_E_ETH_TXFULL)
+#define F_E_ETH_TXFULL    V_E_ETH_TXFULL(1U)
+
+#define S_E_IP_TXFULL    4
+#define V_E_IP_TXFULL(x) ((x) << S_E_IP_TXFULL)
+#define F_E_IP_TXFULL    V_E_IP_TXFULL(1U)
+
+#define S_E_TCP_TXFULL    3
+#define V_E_TCP_TXFULL(x) ((x) << S_E_TCP_TXFULL)
+#define F_E_TCP_TXFULL    V_E_TCP_TXFULL(1U)
+
+#define S_C_CPL5_RXFULL    2
+#define V_C_CPL5_RXFULL(x) ((x) << S_C_CPL5_RXFULL)
+#define F_C_CPL5_RXFULL    V_C_CPL5_RXFULL(1U)
+
+#define S_C_CPL5_TXFULL    1
+#define V_C_CPL5_TXFULL(x) ((x) << S_C_CPL5_TXFULL)
+#define F_C_CPL5_TXFULL    V_C_CPL5_TXFULL(1U)
+
+#define S_E_TCP_OPT_RXFULL    0
+#define V_E_TCP_OPT_RXFULL(x) ((x) << S_E_TCP_OPT_RXFULL)
+#define F_E_TCP_OPT_RXFULL    V_E_TCP_OPT_RXFULL(1U)
+
+#define A_TP_DBG_CORE_FATAL 0x63
+
+#define S_EMSGFATAL    31
+#define V_EMSGFATAL(x) ((x) << S_EMSGFATAL)
+#define F_EMSGFATAL    V_EMSGFATAL(1U)
+
+#define S_CMSGFATAL    30
+#define V_CMSGFATAL(x) ((x) << S_CMSGFATAL)
+#define F_CMSGFATAL    V_CMSGFATAL(1U)
+
+#define S_PAWSFATAL    29
+#define V_PAWSFATAL(x) ((x) << S_PAWSFATAL)
+#define F_PAWSFATAL    V_PAWSFATAL(1U)
+
+#define S_SRAMFATAL    28
+#define V_SRAMFATAL(x) ((x) << S_SRAMFATAL)
+#define F_SRAMFATAL    V_SRAMFATAL(1U)
+
+#define S_EPCMDCONG    24
+#define M_EPCMDCONG    0xfU
+#define V_EPCMDCONG(x) ((x) << S_EPCMDCONG)
+#define G_EPCMDCONG(x) (((x) >> S_EPCMDCONG) & M_EPCMDCONG)
+
+#define S_CPCMDCONG    22
+#define M_CPCMDCONG    0x3U
+#define V_CPCMDCONG(x) ((x) << S_CPCMDCONG)
+#define G_CPCMDCONG(x) (((x) >> S_CPCMDCONG) & M_CPCMDCONG)
+
+#define S_CPCMDLENFATAL    21
+#define V_CPCMDLENFATAL(x) ((x) << S_CPCMDLENFATAL)
+#define F_CPCMDLENFATAL    V_CPCMDLENFATAL(1U)
+
+#define S_EPCMDLENFATAL    20
+#define V_EPCMDLENFATAL(x) ((x) << S_EPCMDLENFATAL)
+#define F_EPCMDLENFATAL    V_EPCMDLENFATAL(1U)
+
+#define S_CPCMDVALID    16
+#define M_CPCMDVALID    0xfU
+#define V_CPCMDVALID(x) ((x) << S_CPCMDVALID)
+#define G_CPCMDVALID(x) (((x) >> S_CPCMDVALID) & M_CPCMDVALID)
+
+#define S_CPCMDAFULL    12
+#define M_CPCMDAFULL    0xfU
+#define V_CPCMDAFULL(x) ((x) << S_CPCMDAFULL)
+#define G_CPCMDAFULL(x) (((x) >> S_CPCMDAFULL) & M_CPCMDAFULL)
+
+#define S_EPCMDVALID    10
+#define M_EPCMDVALID    0x3U
+#define V_EPCMDVALID(x) ((x) << S_EPCMDVALID)
+#define G_EPCMDVALID(x) (((x) >> S_EPCMDVALID) & M_EPCMDVALID)
+
+#define S_EPCMDAFULL    8
+#define M_EPCMDAFULL    0x3U
+#define V_EPCMDAFULL(x) ((x) << S_EPCMDAFULL)
+#define G_EPCMDAFULL(x) (((x) >> S_EPCMDAFULL) & M_EPCMDAFULL)
+
+#define S_CPCMDEOIFATAL    7
+#define V_CPCMDEOIFATAL(x) ((x) << S_CPCMDEOIFATAL)
+#define F_CPCMDEOIFATAL    V_CPCMDEOIFATAL(1U)
+
+#define S_CMDBRQFATAL    4
+#define V_CMDBRQFATAL(x) ((x) << S_CMDBRQFATAL)
+#define F_CMDBRQFATAL    V_CMDBRQFATAL(1U)
+
+#define S_CNONZEROPPOPCNT    2
+#define M_CNONZEROPPOPCNT    0x3U
+#define V_CNONZEROPPOPCNT(x) ((x) << S_CNONZEROPPOPCNT)
+#define G_CNONZEROPPOPCNT(x) (((x) >> S_CNONZEROPPOPCNT) & M_CNONZEROPPOPCNT)
+
+#define S_CPCMDEOICNT    0
+#define M_CPCMDEOICNT    0x3U
+#define V_CPCMDEOICNT(x) ((x) << S_CPCMDEOICNT)
+#define G_CPCMDEOICNT(x) (((x) >> S_CPCMDEOICNT) & M_CPCMDEOICNT)
+
+#define A_TP_DBG_CORE_OUT 0x64
+
+#define S_CCPLENC    26
+#define V_CCPLENC(x) ((x) << S_CCPLENC)
+#define F_CCPLENC    V_CCPLENC(1U)
+
+#define S_CWRCPLPKT    25
+#define V_CWRCPLPKT(x) ((x) << S_CWRCPLPKT)
+#define F_CWRCPLPKT    V_CWRCPLPKT(1U)
+
+#define S_CWRETHPKT    24
+#define V_CWRETHPKT(x) ((x) << S_CWRETHPKT)
+#define F_CWRETHPKT    V_CWRETHPKT(1U)
+
+#define S_CWRIPPKT    23
+#define V_CWRIPPKT(x) ((x) << S_CWRIPPKT)
+#define F_CWRIPPKT    V_CWRIPPKT(1U)
+
+#define S_CWRTCPPKT    22
+#define V_CWRTCPPKT(x) ((x) << S_CWRTCPPKT)
+#define F_CWRTCPPKT    V_CWRTCPPKT(1U)
+
+#define S_CWRZEROP    21
+#define V_CWRZEROP(x) ((x) << S_CWRZEROP)
+#define F_CWRZEROP    V_CWRZEROP(1U)
+
+#define S_CCPLTXFULL    20
+#define V_CCPLTXFULL(x) ((x) << S_CCPLTXFULL)
+#define F_CCPLTXFULL    V_CCPLTXFULL(1U)
+
+#define S_CETHTXFULL    19
+#define V_CETHTXFULL(x) ((x) << S_CETHTXFULL)
+#define F_CETHTXFULL    V_CETHTXFULL(1U)
+
+#define S_CIPTXFULL    18
+#define V_CIPTXFULL(x) ((x) << S_CIPTXFULL)
+#define F_CIPTXFULL    V_CIPTXFULL(1U)
+
+#define S_CTCPTXFULL    17
+#define V_CTCPTXFULL(x) ((x) << S_CTCPTXFULL)
+#define F_CTCPTXFULL    V_CTCPTXFULL(1U)
+
+#define S_CPLDTXZEROPDRDY    16
+#define V_CPLDTXZEROPDRDY(x) ((x) << S_CPLDTXZEROPDRDY)
+#define F_CPLDTXZEROPDRDY    V_CPLDTXZEROPDRDY(1U)
+
+#define S_ECPLENC    10
+#define V_ECPLENC(x) ((x) << S_ECPLENC)
+#define F_ECPLENC    V_ECPLENC(1U)
+
+#define S_EWRCPLPKT    9
+#define V_EWRCPLPKT(x) ((x) << S_EWRCPLPKT)
+#define F_EWRCPLPKT    V_EWRCPLPKT(1U)
+
+#define S_EWRETHPKT    8
+#define V_EWRETHPKT(x) ((x) << S_EWRETHPKT)
+#define F_EWRETHPKT    V_EWRETHPKT(1U)
+
+#define S_EWRIPPKT    7
+#define V_EWRIPPKT(x) ((x) << S_EWRIPPKT)
+#define F_EWRIPPKT    V_EWRIPPKT(1U)
+
+#define S_EWRTCPPKT    6
+#define V_EWRTCPPKT(x) ((x) << S_EWRTCPPKT)
+#define F_EWRTCPPKT    V_EWRTCPPKT(1U)
+
+#define S_EWRZEROP    5
+#define V_EWRZEROP(x) ((x) << S_EWRZEROP)
+#define F_EWRZEROP    V_EWRZEROP(1U)
+
+#define S_ECPLTXFULL    4
+#define V_ECPLTXFULL(x) ((x) << S_ECPLTXFULL)
+#define F_ECPLTXFULL    V_ECPLTXFULL(1U)
+
+#define S_EETHTXFULL    3
+#define V_EETHTXFULL(x) ((x) << S_EETHTXFULL)
+#define F_EETHTXFULL    V_EETHTXFULL(1U)
+
+#define S_EIPTXFULL    2
+#define V_EIPTXFULL(x) ((x) << S_EIPTXFULL)
+#define F_EIPTXFULL    V_EIPTXFULL(1U)
+
+#define S_ETCPTXFULL    1
+#define V_ETCPTXFULL(x) ((x) << S_ETCPTXFULL)
+#define F_ETCPTXFULL    V_ETCPTXFULL(1U)
+
+#define S_EPLDTXZEROPDRDY    0
+#define V_EPLDTXZEROPDRDY(x) ((x) << S_EPLDTXZEROPDRDY)
+#define F_EPLDTXZEROPDRDY    V_EPLDTXZEROPDRDY(1U)
+
+#define A_TP_DBG_CORE_TID 0x65
+
+#define S_LINENUMBER    24
+#define M_LINENUMBER    0x7fU
+#define V_LINENUMBER(x) ((x) << S_LINENUMBER)
+#define G_LINENUMBER(x) (((x) >> S_LINENUMBER) & M_LINENUMBER)
+
+#define S_SPURIOUSMSG    23
+#define V_SPURIOUSMSG(x) ((x) << S_SPURIOUSMSG)
+#define F_SPURIOUSMSG    V_SPURIOUSMSG(1U)
+
+#define S_SYNLEARNED    20
+#define V_SYNLEARNED(x) ((x) << S_SYNLEARNED)
+#define F_SYNLEARNED    V_SYNLEARNED(1U)
+
+#define S_TIDVALUE    0
+#define M_TIDVALUE    0xfffffU
+#define V_TIDVALUE(x) ((x) << S_TIDVALUE)
+#define G_TIDVALUE(x) (((x) >> S_TIDVALUE) & M_TIDVALUE)
+
+#define A_TP_DBG_ENG_RES0 0x66
+
+#define S_RESOURCESREADY    31
+#define V_RESOURCESREADY(x) ((x) << S_RESOURCESREADY)
+#define F_RESOURCESREADY    V_RESOURCESREADY(1U)
+
+#define S_RCFOPCODEOUTSRDY    30
+#define V_RCFOPCODEOUTSRDY(x) ((x) << S_RCFOPCODEOUTSRDY)
+#define F_RCFOPCODEOUTSRDY    V_RCFOPCODEOUTSRDY(1U)
+
+#define S_RCFDATAOUTSRDY    29
+#define V_RCFDATAOUTSRDY(x) ((x) << S_RCFDATAOUTSRDY)
+#define F_RCFDATAOUTSRDY    V_RCFDATAOUTSRDY(1U)
+
+#define S_FLUSHINPUTMSG    28
+#define V_FLUSHINPUTMSG(x) ((x) << S_FLUSHINPUTMSG)
+#define F_FLUSHINPUTMSG    V_FLUSHINPUTMSG(1U)
+
+#define S_RCFOPSRCOUT    26
+#define M_RCFOPSRCOUT    0x3U
+#define V_RCFOPSRCOUT(x) ((x) << S_RCFOPSRCOUT)
+#define G_RCFOPSRCOUT(x) (((x) >> S_RCFOPSRCOUT) & M_RCFOPSRCOUT)
+
+#define S_C_MSG    25
+#define V_C_MSG(x) ((x) << S_C_MSG)
+#define F_C_MSG    V_C_MSG(1U)
+
+#define S_E_MSG    24
+#define V_E_MSG(x) ((x) << S_E_MSG)
+#define F_E_MSG    V_E_MSG(1U)
+
+#define S_RCFOPCODEOUT    20
+#define M_RCFOPCODEOUT    0xfU
+#define V_RCFOPCODEOUT(x) ((x) << S_RCFOPCODEOUT)
+#define G_RCFOPCODEOUT(x) (((x) >> S_RCFOPCODEOUT) & M_RCFOPCODEOUT)
+
+#define S_EFFRCFOPCODEOUT    16
+#define M_EFFRCFOPCODEOUT    0xfU
+#define V_EFFRCFOPCODEOUT(x) ((x) << S_EFFRCFOPCODEOUT)
+#define G_EFFRCFOPCODEOUT(x) (((x) >> S_EFFRCFOPCODEOUT) & M_EFFRCFOPCODEOUT)
+
+#define S_SEENRESOURCESREADY    15
+#define V_SEENRESOURCESREADY(x) ((x) << S_SEENRESOURCESREADY)
+#define F_SEENRESOURCESREADY    V_SEENRESOURCESREADY(1U)
+
+#define S_RESOURCESREADYCOPY    14
+#define V_RESOURCESREADYCOPY(x) ((x) << S_RESOURCESREADYCOPY)
+#define F_RESOURCESREADYCOPY    V_RESOURCESREADYCOPY(1U)
+
+#define S_OPCODEWAITSFORDATA    13
+#define V_OPCODEWAITSFORDATA(x) ((x) << S_OPCODEWAITSFORDATA)
+#define F_OPCODEWAITSFORDATA    V_OPCODEWAITSFORDATA(1U)
+
+#define S_CPLDRXSRDY    12
+#define V_CPLDRXSRDY(x) ((x) << S_CPLDRXSRDY)
+#define F_CPLDRXSRDY    V_CPLDRXSRDY(1U)
+
+#define S_CPLDRXZEROPSRDY    11
+#define V_CPLDRXZEROPSRDY(x) ((x) << S_CPLDRXZEROPSRDY)
+#define F_CPLDRXZEROPSRDY    V_CPLDRXZEROPSRDY(1U)
+
+#define S_EPLDRXZEROPSRDY    10
+#define V_EPLDRXZEROPSRDY(x) ((x) << S_EPLDRXZEROPSRDY)
+#define F_EPLDRXZEROPSRDY    V_EPLDRXZEROPSRDY(1U)
+
+#define S_ERXERRORSRDY    9
+#define V_ERXERRORSRDY(x) ((x) << S_ERXERRORSRDY)
+#define F_ERXERRORSRDY    V_ERXERRORSRDY(1U)
+
+#define S_EPLDRXSRDY    8
+#define V_EPLDRXSRDY(x) ((x) << S_EPLDRXSRDY)
+#define F_EPLDRXSRDY    V_EPLDRXSRDY(1U)
+
+#define S_CRXBUSY    7
+#define V_CRXBUSY(x) ((x) << S_CRXBUSY)
+#define F_CRXBUSY    V_CRXBUSY(1U)
+
+#define S_ERXBUSY    6
+#define V_ERXBUSY(x) ((x) << S_ERXBUSY)
+#define F_ERXBUSY    V_ERXBUSY(1U)
+
+#define S_TIMERINSERTBUSY    5
+#define V_TIMERINSERTBUSY(x) ((x) << S_TIMERINSERTBUSY)
+#define F_TIMERINSERTBUSY    V_TIMERINSERTBUSY(1U)
+
+#define S_WCFBUSY    4
+#define V_WCFBUSY(x) ((x) << S_WCFBUSY)
+#define F_WCFBUSY    V_WCFBUSY(1U)
+
+#define S_CTXBUSY    3
+#define V_CTXBUSY(x) ((x) << S_CTXBUSY)
+#define F_CTXBUSY    V_CTXBUSY(1U)
+
+#define S_CPCMDBUSY    2
+#define V_CPCMDBUSY(x) ((x) << S_CPCMDBUSY)
+#define F_CPCMDBUSY    V_CPCMDBUSY(1U)
+
+#define S_ETXBUSY    1
+#define V_ETXBUSY(x) ((x) << S_ETXBUSY)
+#define F_ETXBUSY    V_ETXBUSY(1U)
+
+#define S_EPCMDBUSY    0
+#define V_EPCMDBUSY(x) ((x) << S_EPCMDBUSY)
+#define F_EPCMDBUSY    V_EPCMDBUSY(1U)
+
+#define A_TP_DBG_ENG_RES1 0x67
+
+#define S_RXCPLSRDY    31
+#define V_RXCPLSRDY(x) ((x) << S_RXCPLSRDY)
+#define F_RXCPLSRDY    V_RXCPLSRDY(1U)
+
+#define S_RXOPTSRDY    30
+#define V_RXOPTSRDY(x) ((x) << S_RXOPTSRDY)
+#define F_RXOPTSRDY    V_RXOPTSRDY(1U)
+
+#define S_RXPLDLENSRDY    29
+#define V_RXPLDLENSRDY(x) ((x) << S_RXPLDLENSRDY)
+#define F_RXPLDLENSRDY    V_RXPLDLENSRDY(1U)
+
+#define S_RXNOTBUSY    28
+#define V_RXNOTBUSY(x) ((x) << S_RXNOTBUSY)
+#define F_RXNOTBUSY    V_RXNOTBUSY(1U)
+
+#define S_CPLCMDIN    20
+#define M_CPLCMDIN    0xffU
+#define V_CPLCMDIN(x) ((x) << S_CPLCMDIN)
+#define G_CPLCMDIN(x) (((x) >> S_CPLCMDIN) & M_CPLCMDIN)
+
+#define S_RCFPTIDSRDY    19
+#define V_RCFPTIDSRDY(x) ((x) << S_RCFPTIDSRDY)
+#define F_RCFPTIDSRDY    V_RCFPTIDSRDY(1U)
+
+#define S_EPDUHDRSRDY    18
+#define V_EPDUHDRSRDY(x) ((x) << S_EPDUHDRSRDY)
+#define F_EPDUHDRSRDY    V_EPDUHDRSRDY(1U)
+
+#define S_TUNNELPKTREG    17
+#define V_TUNNELPKTREG(x) ((x) << S_TUNNELPKTREG)
+#define F_TUNNELPKTREG    V_TUNNELPKTREG(1U)
+
+#define S_TXPKTCSUMSRDY    16
+#define V_TXPKTCSUMSRDY(x) ((x) << S_TXPKTCSUMSRDY)
+#define F_TXPKTCSUMSRDY    V_TXPKTCSUMSRDY(1U)
+
+#define S_TABLEACCESSLATENCY    12
+#define M_TABLEACCESSLATENCY    0xfU
+#define V_TABLEACCESSLATENCY(x) ((x) << S_TABLEACCESSLATENCY)
+#define G_TABLEACCESSLATENCY(x) (((x) >> S_TABLEACCESSLATENCY) & M_TABLEACCESSLATENCY)
+
+#define S_MMGRDONE    11
+#define V_MMGRDONE(x) ((x) << S_MMGRDONE)
+#define F_MMGRDONE    V_MMGRDONE(1U)
+
+#define S_SEENMMGRDONE    10
+#define V_SEENMMGRDONE(x) ((x) << S_SEENMMGRDONE)
+#define F_SEENMMGRDONE    V_SEENMMGRDONE(1U)
+
+#define S_RXERRORSRDY    9
+#define V_RXERRORSRDY(x) ((x) << S_RXERRORSRDY)
+#define F_RXERRORSRDY    V_RXERRORSRDY(1U)
+
+#define S_RCFOPTIONSTCPSRDY    8
+#define V_RCFOPTIONSTCPSRDY(x) ((x) << S_RCFOPTIONSTCPSRDY)
+#define F_RCFOPTIONSTCPSRDY    V_RCFOPTIONSTCPSRDY(1U)
+
+#define S_ENGINESTATE    6
+#define M_ENGINESTATE    0x3U
+#define V_ENGINESTATE(x) ((x) << S_ENGINESTATE)
+#define G_ENGINESTATE(x) (((x) >> S_ENGINESTATE) & M_ENGINESTATE)
+
+#define S_TABLEACCESINCREMENT    5
+#define V_TABLEACCESINCREMENT(x) ((x) << S_TABLEACCESINCREMENT)
+#define F_TABLEACCESINCREMENT    V_TABLEACCESINCREMENT(1U)
+
+#define S_TABLEACCESCOMPLETE    4
+#define V_TABLEACCESCOMPLETE(x) ((x) << S_TABLEACCESCOMPLETE)
+#define F_TABLEACCESCOMPLETE    V_TABLEACCESCOMPLETE(1U)
+
+#define S_RCFOPCODEOUTUSABLE    3
+#define V_RCFOPCODEOUTUSABLE(x) ((x) << S_RCFOPCODEOUTUSABLE)
+#define F_RCFOPCODEOUTUSABLE    V_RCFOPCODEOUTUSABLE(1U)
+
+#define S_RCFDATAOUTUSABLE    2
+#define V_RCFDATAOUTUSABLE(x) ((x) << S_RCFDATAOUTUSABLE)
+#define F_RCFDATAOUTUSABLE    V_RCFDATAOUTUSABLE(1U)
+
+#define S_RCFDATAWAITAFTERRD    1
+#define V_RCFDATAWAITAFTERRD(x) ((x) << S_RCFDATAWAITAFTERRD)
+#define F_RCFDATAWAITAFTERRD    V_RCFDATAWAITAFTERRD(1U)
+
+#define S_RCFDATACMRDY    0
+#define V_RCFDATACMRDY(x) ((x) << S_RCFDATACMRDY)
+#define F_RCFDATACMRDY    V_RCFDATACMRDY(1U)
+
+#define A_TP_DBG_ENG_RES2 0x68
+
+#define S_CPLCMDRAW    24
+#define M_CPLCMDRAW    0xffU
+#define V_CPLCMDRAW(x) ((x) << S_CPLCMDRAW)
+#define G_CPLCMDRAW(x) (((x) >> S_CPLCMDRAW) & M_CPLCMDRAW)
+
+#define S_RXMACPORT    20
+#define M_RXMACPORT    0xfU
+#define V_RXMACPORT(x) ((x) << S_RXMACPORT)
+#define G_RXMACPORT(x) (((x) >> S_RXMACPORT) & M_RXMACPORT)
+
+#define S_TXECHANNEL    18
+#define M_TXECHANNEL    0x3U
+#define V_TXECHANNEL(x) ((x) << S_TXECHANNEL)
+#define G_TXECHANNEL(x) (((x) >> S_TXECHANNEL) & M_TXECHANNEL)
+
+#define S_RXECHANNEL    16
+#define M_RXECHANNEL    0x3U
+#define V_RXECHANNEL(x) ((x) << S_RXECHANNEL)
+#define G_RXECHANNEL(x) (((x) >> S_RXECHANNEL) & M_RXECHANNEL)
+
+#define S_CDATAOUT    15
+#define V_CDATAOUT(x) ((x) << S_CDATAOUT)
+#define F_CDATAOUT    V_CDATAOUT(1U)
+
+#define S_CREADPDU    14
+#define V_CREADPDU(x) ((x) << S_CREADPDU)
+#define F_CREADPDU    V_CREADPDU(1U)
+
+#define S_EDATAOUT    13
+#define V_EDATAOUT(x) ((x) << S_EDATAOUT)
+#define F_EDATAOUT    V_EDATAOUT(1U)
+
+#define S_EREADPDU    12
+#define V_EREADPDU(x) ((x) << S_EREADPDU)
+#define F_EREADPDU    V_EREADPDU(1U)
+
+#define S_ETCPOPSRDY    11
+#define V_ETCPOPSRDY(x) ((x) << S_ETCPOPSRDY)
+#define F_ETCPOPSRDY    V_ETCPOPSRDY(1U)
+
+#define S_CTCPOPSRDY    10
+#define V_CTCPOPSRDY(x) ((x) << S_CTCPOPSRDY)
+#define F_CTCPOPSRDY    V_CTCPOPSRDY(1U)
+
+#define S_CPKTOUT    9
+#define V_CPKTOUT(x) ((x) << S_CPKTOUT)
+#define F_CPKTOUT    V_CPKTOUT(1U)
+
+#define S_CMDBRSPSRDY    8
+#define V_CMDBRSPSRDY(x) ((x) << S_CMDBRSPSRDY)
+#define F_CMDBRSPSRDY    V_CMDBRSPSRDY(1U)
+
+#define S_RXPSTRUCTSFULL    6
+#define M_RXPSTRUCTSFULL    0x3U
+#define V_RXPSTRUCTSFULL(x) ((x) << S_RXPSTRUCTSFULL)
+#define G_RXPSTRUCTSFULL(x) (((x) >> S_RXPSTRUCTSFULL) & M_RXPSTRUCTSFULL)
+
+#define S_RXPAGEPOOLFULL    4
+#define M_RXPAGEPOOLFULL    0x3U
+#define V_RXPAGEPOOLFULL(x) ((x) << S_RXPAGEPOOLFULL)
+#define G_RXPAGEPOOLFULL(x) (((x) >> S_RXPAGEPOOLFULL) & M_RXPAGEPOOLFULL)
+
+#define S_RCFREASONOUT    0
+#define M_RCFREASONOUT    0xfU
+#define V_RCFREASONOUT(x) ((x) << S_RCFREASONOUT)
+#define G_RCFREASONOUT(x) (((x) >> S_RCFREASONOUT) & M_RCFREASONOUT)
+
+#define A_TP_DBG_CORE_PCMD 0x69
+
+#define S_CPCMDEOPCNT    30
+#define M_CPCMDEOPCNT    0x3U
+#define V_CPCMDEOPCNT(x) ((x) << S_CPCMDEOPCNT)
+#define G_CPCMDEOPCNT(x) (((x) >> S_CPCMDEOPCNT) & M_CPCMDEOPCNT)
+
+#define S_CPCMDLENSAVE    16
+#define M_CPCMDLENSAVE    0x3fffU
+#define V_CPCMDLENSAVE(x) ((x) << S_CPCMDLENSAVE)
+#define G_CPCMDLENSAVE(x) (((x) >> S_CPCMDLENSAVE) & M_CPCMDLENSAVE)
+
+#define S_EPCMDEOPCNT    14
+#define M_EPCMDEOPCNT    0x3U
+#define V_EPCMDEOPCNT(x) ((x) << S_EPCMDEOPCNT)
+#define G_EPCMDEOPCNT(x) (((x) >> S_EPCMDEOPCNT) & M_EPCMDEOPCNT)
+
+#define S_EPCMDLENSAVE    0
+#define M_EPCMDLENSAVE    0x3fffU
+#define V_EPCMDLENSAVE(x) ((x) << S_EPCMDLENSAVE)
+#define G_EPCMDLENSAVE(x) (((x) >> S_EPCMDLENSAVE) & M_EPCMDLENSAVE)
+
+#define A_TP_DBG_SCHED_TX 0x6a
+
+#define S_TXCHNXOFF    28
+#define M_TXCHNXOFF    0xfU
+#define V_TXCHNXOFF(x) ((x) << S_TXCHNXOFF)
+#define G_TXCHNXOFF(x) (((x) >> S_TXCHNXOFF) & M_TXCHNXOFF)
+
+#define S_TXFIFOCNG    24
+#define M_TXFIFOCNG    0xfU
+#define V_TXFIFOCNG(x) ((x) << S_TXFIFOCNG)
+#define G_TXFIFOCNG(x) (((x) >> S_TXFIFOCNG) & M_TXFIFOCNG)
+
+#define S_TXPCMDCNG    20
+#define M_TXPCMDCNG    0xfU
+#define V_TXPCMDCNG(x) ((x) << S_TXPCMDCNG)
+#define G_TXPCMDCNG(x) (((x) >> S_TXPCMDCNG) & M_TXPCMDCNG)
+
+#define S_TXLPBKCNG    16
+#define M_TXLPBKCNG    0xfU
+#define V_TXLPBKCNG(x) ((x) << S_TXLPBKCNG)
+#define G_TXLPBKCNG(x) (((x) >> S_TXLPBKCNG) & M_TXLPBKCNG)
+
+#define S_TXHDRCNG    8
+#define M_TXHDRCNG    0xffU
+#define V_TXHDRCNG(x) ((x) << S_TXHDRCNG)
+#define G_TXHDRCNG(x) (((x) >> S_TXHDRCNG) & M_TXHDRCNG)
+
+#define S_TXMODXOFF    0
+#define M_TXMODXOFF    0xffU
+#define V_TXMODXOFF(x) ((x) << S_TXMODXOFF)
+#define G_TXMODXOFF(x) (((x) >> S_TXMODXOFF) & M_TXMODXOFF)
+
+#define A_TP_DBG_SCHED_RX 0x6b
+
+#define S_RXCHNXOFF    28
+#define M_RXCHNXOFF    0xfU
+#define V_RXCHNXOFF(x) ((x) << S_RXCHNXOFF)
+#define G_RXCHNXOFF(x) (((x) >> S_RXCHNXOFF) & M_RXCHNXOFF)
+
+#define S_RXSGECNG    24
+#define M_RXSGECNG    0xfU
+#define V_RXSGECNG(x) ((x) << S_RXSGECNG)
+#define G_RXSGECNG(x) (((x) >> S_RXSGECNG) & M_RXSGECNG)
+
+#define S_RXFIFOCNG    22
+#define M_RXFIFOCNG    0x3U
+#define V_RXFIFOCNG(x) ((x) << S_RXFIFOCNG)
+#define G_RXFIFOCNG(x) (((x) >> S_RXFIFOCNG) & M_RXFIFOCNG)
+
+#define S_RXPCMDCNG    20
+#define M_RXPCMDCNG    0x3U
+#define V_RXPCMDCNG(x) ((x) << S_RXPCMDCNG)
+#define G_RXPCMDCNG(x) (((x) >> S_RXPCMDCNG) & M_RXPCMDCNG)
+
+#define S_RXLPBKCNG    16
+#define M_RXLPBKCNG    0xfU
+#define V_RXLPBKCNG(x) ((x) << S_RXLPBKCNG)
+#define G_RXLPBKCNG(x) (((x) >> S_RXLPBKCNG) & M_RXLPBKCNG)
+
+#define S_RXHDRCNG    8
+#define M_RXHDRCNG    0xfU
+#define V_RXHDRCNG(x) ((x) << S_RXHDRCNG)
+#define G_RXHDRCNG(x) (((x) >> S_RXHDRCNG) & M_RXHDRCNG)
+
+#define S_RXMODXOFF    0
+#define M_RXMODXOFF    0x3U
+#define V_RXMODXOFF(x) ((x) << S_RXMODXOFF)
+#define G_RXMODXOFF(x) (((x) >> S_RXMODXOFF) & M_RXMODXOFF)
+
+#define A_TP_TX_DROP_CFG_CH0 0x12b
+
+#define S_TIMERENABLED    31
+#define V_TIMERENABLED(x) ((x) << S_TIMERENABLED)
+#define F_TIMERENABLED    V_TIMERENABLED(1U)
+
+#define S_TIMERERRORENABLE    30
+#define V_TIMERERRORENABLE(x) ((x) << S_TIMERERRORENABLE)
+#define F_TIMERERRORENABLE    V_TIMERERRORENABLE(1U)
+
+#define S_TIMERTHRESHOLD    4
+#define M_TIMERTHRESHOLD    0x3ffffffU
+#define V_TIMERTHRESHOLD(x) ((x) << S_TIMERTHRESHOLD)
+#define G_TIMERTHRESHOLD(x) (((x) >> S_TIMERTHRESHOLD) & M_TIMERTHRESHOLD)
+
+#define S_PACKETDROPS    0
+#define M_PACKETDROPS    0xfU
+#define V_PACKETDROPS(x) ((x) << S_PACKETDROPS)
+#define G_PACKETDROPS(x) (((x) >> S_PACKETDROPS) & M_PACKETDROPS)
+
+#define A_TP_TX_DROP_CFG_CH1 0x12c
+#define A_TP_TX_DROP_CNT_CH0 0x12d
+
+#define S_TXDROPCNTCH0SENT    16
+#define M_TXDROPCNTCH0SENT    0xffffU
+#define V_TXDROPCNTCH0SENT(x) ((x) << S_TXDROPCNTCH0SENT)
+#define G_TXDROPCNTCH0SENT(x) (((x) >> S_TXDROPCNTCH0SENT) & M_TXDROPCNTCH0SENT)
+
+#define S_TXDROPCNTCH0RCVD    0
+#define M_TXDROPCNTCH0RCVD    0xffffU
+#define V_TXDROPCNTCH0RCVD(x) ((x) << S_TXDROPCNTCH0RCVD)
+#define G_TXDROPCNTCH0RCVD(x) (((x) >> S_TXDROPCNTCH0RCVD) & M_TXDROPCNTCH0RCVD)
+
+#define A_TP_TX_DROP_CNT_CH1 0x12e
+
+#define S_TXDROPCNTCH1SENT    16
+#define M_TXDROPCNTCH1SENT    0xffffU
+#define V_TXDROPCNTCH1SENT(x) ((x) << S_TXDROPCNTCH1SENT)
+#define G_TXDROPCNTCH1SENT(x) (((x) >> S_TXDROPCNTCH1SENT) & M_TXDROPCNTCH1SENT)
+
+#define S_TXDROPCNTCH1RCVD    0
+#define M_TXDROPCNTCH1RCVD    0xffffU
+#define V_TXDROPCNTCH1RCVD(x) ((x) << S_TXDROPCNTCH1RCVD)
+#define G_TXDROPCNTCH1RCVD(x) (((x) >> S_TXDROPCNTCH1RCVD) & M_TXDROPCNTCH1RCVD)
+
+#define A_TP_TX_DROP_MODE 0x12f
+
+#define S_TXDROPMODECH3    3
+#define V_TXDROPMODECH3(x) ((x) << S_TXDROPMODECH3)
+#define F_TXDROPMODECH3    V_TXDROPMODECH3(1U)
+
+#define S_TXDROPMODECH2    2
+#define V_TXDROPMODECH2(x) ((x) << S_TXDROPMODECH2)
+#define F_TXDROPMODECH2    V_TXDROPMODECH2(1U)
+
+#define S_TXDROPMODECH1    1
+#define V_TXDROPMODECH1(x) ((x) << S_TXDROPMODECH1)
+#define F_TXDROPMODECH1    V_TXDROPMODECH1(1U)
+
+#define S_TXDROPMODECH0    0
+#define V_TXDROPMODECH0(x) ((x) << S_TXDROPMODECH0)
+#define F_TXDROPMODECH0    V_TXDROPMODECH0(1U)
+
+#define A_TP_DBG_ESIDE_PKT0 0x130
+
+#define S_ETXSOPCNT    28
+#define M_ETXSOPCNT    0xfU
+#define V_ETXSOPCNT(x) ((x) << S_ETXSOPCNT)
+#define G_ETXSOPCNT(x) (((x) >> S_ETXSOPCNT) & M_ETXSOPCNT)
+
+#define S_ETXEOPCNT    24
+#define M_ETXEOPCNT    0xfU
+#define V_ETXEOPCNT(x) ((x) << S_ETXEOPCNT)
+#define G_ETXEOPCNT(x) (((x) >> S_ETXEOPCNT) & M_ETXEOPCNT)
+
+#define S_ETXPLDSOPCNT    20
+#define M_ETXPLDSOPCNT    0xfU
+#define V_ETXPLDSOPCNT(x) ((x) << S_ETXPLDSOPCNT)
+#define G_ETXPLDSOPCNT(x) (((x) >> S_ETXPLDSOPCNT) & M_ETXPLDSOPCNT)
+
+#define S_ETXPLDEOPCNT    16
+#define M_ETXPLDEOPCNT    0xfU
+#define V_ETXPLDEOPCNT(x) ((x) << S_ETXPLDEOPCNT)
+#define G_ETXPLDEOPCNT(x) (((x) >> S_ETXPLDEOPCNT) & M_ETXPLDEOPCNT)
+
+#define S_ERXSOPCNT    12
+#define M_ERXSOPCNT    0xfU
+#define V_ERXSOPCNT(x) ((x) << S_ERXSOPCNT)
+#define G_ERXSOPCNT(x) (((x) >> S_ERXSOPCNT) & M_ERXSOPCNT)
+
+#define S_ERXEOPCNT    8
+#define M_ERXEOPCNT    0xfU
+#define V_ERXEOPCNT(x) ((x) << S_ERXEOPCNT)
+#define G_ERXEOPCNT(x) (((x) >> S_ERXEOPCNT) & M_ERXEOPCNT)
+
+#define S_ERXPLDSOPCNT    4
+#define M_ERXPLDSOPCNT    0xfU
+#define V_ERXPLDSOPCNT(x) ((x) << S_ERXPLDSOPCNT)
+#define G_ERXPLDSOPCNT(x) (((x) >> S_ERXPLDSOPCNT) & M_ERXPLDSOPCNT)
+
+#define S_ERXPLDEOPCNT    0
+#define M_ERXPLDEOPCNT    0xfU
+#define V_ERXPLDEOPCNT(x) ((x) << S_ERXPLDEOPCNT)
+#define G_ERXPLDEOPCNT(x) (((x) >> S_ERXPLDEOPCNT) & M_ERXPLDEOPCNT)
+
+#define A_TP_DBG_ESIDE_PKT1 0x131
+#define A_TP_DBG_ESIDE_PKT2 0x132
+#define A_TP_DBG_ESIDE_PKT3 0x133
+#define A_TP_DBG_ESIDE_FIFO0 0x134
+
+#define S_PLDRXCSUMVALID1    31
+#define V_PLDRXCSUMVALID1(x) ((x) << S_PLDRXCSUMVALID1)
+#define F_PLDRXCSUMVALID1    V_PLDRXCSUMVALID1(1U)
+
+#define S_PLDRXZEROPSRDY1    30
+#define V_PLDRXZEROPSRDY1(x) ((x) << S_PLDRXZEROPSRDY1)
+#define F_PLDRXZEROPSRDY1    V_PLDRXZEROPSRDY1(1U)
+
+#define S_PLDRXVALID1    29
+#define V_PLDRXVALID1(x) ((x) << S_PLDRXVALID1)
+#define F_PLDRXVALID1    V_PLDRXVALID1(1U)
+
+#define S_TCPRXVALID1    28
+#define V_TCPRXVALID1(x) ((x) << S_TCPRXVALID1)
+#define F_TCPRXVALID1    V_TCPRXVALID1(1U)
+
+#define S_IPRXVALID1    27
+#define V_IPRXVALID1(x) ((x) << S_IPRXVALID1)
+#define F_IPRXVALID1    V_IPRXVALID1(1U)
+
+#define S_ETHRXVALID1    26
+#define V_ETHRXVALID1(x) ((x) << S_ETHRXVALID1)
+#define F_ETHRXVALID1    V_ETHRXVALID1(1U)
+
+#define S_CPLRXVALID1    25
+#define V_CPLRXVALID1(x) ((x) << S_CPLRXVALID1)
+#define F_CPLRXVALID1    V_CPLRXVALID1(1U)
+
+#define S_FSTATIC1    24
+#define V_FSTATIC1(x) ((x) << S_FSTATIC1)
+#define F_FSTATIC1    V_FSTATIC1(1U)
+
+#define S_ERRORSRDY1    23
+#define V_ERRORSRDY1(x) ((x) << S_ERRORSRDY1)
+#define F_ERRORSRDY1    V_ERRORSRDY1(1U)
+
+#define S_PLDTXSRDY1    22
+#define V_PLDTXSRDY1(x) ((x) << S_PLDTXSRDY1)
+#define F_PLDTXSRDY1    V_PLDTXSRDY1(1U)
+
+#define S_DBVLD1    21
+#define V_DBVLD1(x) ((x) << S_DBVLD1)
+#define F_DBVLD1    V_DBVLD1(1U)
+
+#define S_PLDTXVALID1    20
+#define V_PLDTXVALID1(x) ((x) << S_PLDTXVALID1)
+#define F_PLDTXVALID1    V_PLDTXVALID1(1U)
+
+#define S_ETXVALID1    19
+#define V_ETXVALID1(x) ((x) << S_ETXVALID1)
+#define F_ETXVALID1    V_ETXVALID1(1U)
+
+#define S_ETXFULL1    18
+#define V_ETXFULL1(x) ((x) << S_ETXFULL1)
+#define F_ETXFULL1    V_ETXFULL1(1U)
+
+#define S_ERXVALID1    17
+#define V_ERXVALID1(x) ((x) << S_ERXVALID1)
+#define F_ERXVALID1    V_ERXVALID1(1U)
+
+#define S_ERXFULL1    16
+#define V_ERXFULL1(x) ((x) << S_ERXFULL1)
+#define F_ERXFULL1    V_ERXFULL1(1U)
+
+#define S_PLDRXCSUMVALID0    15
+#define V_PLDRXCSUMVALID0(x) ((x) << S_PLDRXCSUMVALID0)
+#define F_PLDRXCSUMVALID0    V_PLDRXCSUMVALID0(1U)
+
+#define S_PLDRXZEROPSRDY0    14
+#define V_PLDRXZEROPSRDY0(x) ((x) << S_PLDRXZEROPSRDY0)
+#define F_PLDRXZEROPSRDY0    V_PLDRXZEROPSRDY0(1U)
+
+#define S_PLDRXVALID0    13
+#define V_PLDRXVALID0(x) ((x) << S_PLDRXVALID0)
+#define F_PLDRXVALID0    V_PLDRXVALID0(1U)
+
+#define S_TCPRXVALID0    12
+#define V_TCPRXVALID0(x) ((x) << S_TCPRXVALID0)
+#define F_TCPRXVALID0    V_TCPRXVALID0(1U)
+
+#define S_IPRXVALID0    11
+#define V_IPRXVALID0(x) ((x) << S_IPRXVALID0)
+#define F_IPRXVALID0    V_IPRXVALID0(1U)
+
+#define S_ETHRXVALID0    10
+#define V_ETHRXVALID0(x) ((x) << S_ETHRXVALID0)
+#define F_ETHRXVALID0    V_ETHRXVALID0(1U)
+
+#define S_CPLRXVALID0    9
+#define V_CPLRXVALID0(x) ((x) << S_CPLRXVALID0)
+#define F_CPLRXVALID0    V_CPLRXVALID0(1U)
+
+#define S_FSTATIC0    8
+#define V_FSTATIC0(x) ((x) << S_FSTATIC0)
+#define F_FSTATIC0    V_FSTATIC0(1U)
+
+#define S_ERRORSRDY0    7
+#define V_ERRORSRDY0(x) ((x) << S_ERRORSRDY0)
+#define F_ERRORSRDY0    V_ERRORSRDY0(1U)
+
+#define S_PLDTXSRDY0    6
+#define V_PLDTXSRDY0(x) ((x) << S_PLDTXSRDY0)
+#define F_PLDTXSRDY0    V_PLDTXSRDY0(1U)
+
+#define S_DBVLD0    5
+#define V_DBVLD0(x) ((x) << S_DBVLD0)
+#define F_DBVLD0    V_DBVLD0(1U)
+
+#define S_PLDTXVALID0    4
+#define V_PLDTXVALID0(x) ((x) << S_PLDTXVALID0)
+#define F_PLDTXVALID0    V_PLDTXVALID0(1U)
+
+#define S_ETXVALID0    3
+#define V_ETXVALID0(x) ((x) << S_ETXVALID0)
+#define F_ETXVALID0    V_ETXVALID0(1U)
+
+#define S_ETXFULL0    2
+#define V_ETXFULL0(x) ((x) << S_ETXFULL0)
+#define F_ETXFULL0    V_ETXFULL0(1U)
+
+#define S_ERXVALID0    1
+#define V_ERXVALID0(x) ((x) << S_ERXVALID0)
+#define F_ERXVALID0    V_ERXVALID0(1U)
+
+#define S_ERXFULL0    0
+#define V_ERXFULL0(x) ((x) << S_ERXFULL0)
+#define F_ERXFULL0    V_ERXFULL0(1U)
+
+#define A_TP_DBG_ESIDE_FIFO1 0x135
+
+#define S_PLDRXCSUMVALID3    31
+#define V_PLDRXCSUMVALID3(x) ((x) << S_PLDRXCSUMVALID3)
+#define F_PLDRXCSUMVALID3    V_PLDRXCSUMVALID3(1U)
+
+#define S_PLDRXZEROPSRDY3    30
+#define V_PLDRXZEROPSRDY3(x) ((x) << S_PLDRXZEROPSRDY3)
+#define F_PLDRXZEROPSRDY3    V_PLDRXZEROPSRDY3(1U)
+
+#define S_PLDRXVALID3    29
+#define V_PLDRXVALID3(x) ((x) << S_PLDRXVALID3)
+#define F_PLDRXVALID3    V_PLDRXVALID3(1U)
+
+#define S_TCPRXVALID3    28
+#define V_TCPRXVALID3(x) ((x) << S_TCPRXVALID3)
+#define F_TCPRXVALID3    V_TCPRXVALID3(1U)
+
+#define S_IPRXVALID3    27
+#define V_IPRXVALID3(x) ((x) << S_IPRXVALID3)
+#define F_IPRXVALID3    V_IPRXVALID3(1U)
+
+#define S_ETHRXVALID3    26
+#define V_ETHRXVALID3(x) ((x) << S_ETHRXVALID3)
+#define F_ETHRXVALID3    V_ETHRXVALID3(1U)
+
+#define S_CPLRXVALID3    25
+#define V_CPLRXVALID3(x) ((x) << S_CPLRXVALID3)
+#define F_CPLRXVALID3    V_CPLRXVALID3(1U)
+
+#define S_FSTATIC3    24
+#define V_FSTATIC3(x) ((x) << S_FSTATIC3)
+#define F_FSTATIC3    V_FSTATIC3(1U)
+
+#define S_ERRORSRDY3    23
+#define V_ERRORSRDY3(x) ((x) << S_ERRORSRDY3)
+#define F_ERRORSRDY3    V_ERRORSRDY3(1U)
+
+#define S_PLDTXSRDY3    22
+#define V_PLDTXSRDY3(x) ((x) << S_PLDTXSRDY3)
+#define F_PLDTXSRDY3    V_PLDTXSRDY3(1U)
+
+#define S_DBVLD3    21
+#define V_DBVLD3(x) ((x) << S_DBVLD3)
+#define F_DBVLD3    V_DBVLD3(1U)
+
+#define S_PLDTXVALID3    20
+#define V_PLDTXVALID3(x) ((x) << S_PLDTXVALID3)
+#define F_PLDTXVALID3    V_PLDTXVALID3(1U)
+
+#define S_ETXVALID3    19
+#define V_ETXVALID3(x) ((x) << S_ETXVALID3)
+#define F_ETXVALID3    V_ETXVALID3(1U)
+
+#define S_ETXFULL3    18
+#define V_ETXFULL3(x) ((x) << S_ETXFULL3)
+#define F_ETXFULL3    V_ETXFULL3(1U)
+
+#define S_ERXVALID3    17
+#define V_ERXVALID3(x) ((x) << S_ERXVALID3)
+#define F_ERXVALID3    V_ERXVALID3(1U)
+
+#define S_ERXFULL3    16
+#define V_ERXFULL3(x) ((x) << S_ERXFULL3)
+#define F_ERXFULL3    V_ERXFULL3(1U)
+
+#define S_PLDRXCSUMVALID2    15
+#define V_PLDRXCSUMVALID2(x) ((x) << S_PLDRXCSUMVALID2)
+#define F_PLDRXCSUMVALID2    V_PLDRXCSUMVALID2(1U)
+
+#define S_PLDRXZEROPSRDY2    14
+#define V_PLDRXZEROPSRDY2(x) ((x) << S_PLDRXZEROPSRDY2)
+#define F_PLDRXZEROPSRDY2    V_PLDRXZEROPSRDY2(1U)
+
+#define S_PLDRXVALID2    13
+#define V_PLDRXVALID2(x) ((x) << S_PLDRXVALID2)
+#define F_PLDRXVALID2    V_PLDRXVALID2(1U)
+
+#define S_TCPRXVALID2    12
+#define V_TCPRXVALID2(x) ((x) << S_TCPRXVALID2)
+#define F_TCPRXVALID2    V_TCPRXVALID2(1U)
+
+#define S_IPRXVALID2    11
+#define V_IPRXVALID2(x) ((x) << S_IPRXVALID2)
+#define F_IPRXVALID2    V_IPRXVALID2(1U)
+
+#define S_ETHRXVALID2    10
+#define V_ETHRXVALID2(x) ((x) << S_ETHRXVALID2)
+#define F_ETHRXVALID2    V_ETHRXVALID2(1U)
+
+#define S_CPLRXVALID2    9
+#define V_CPLRXVALID2(x) ((x) << S_CPLRXVALID2)
+#define F_CPLRXVALID2    V_CPLRXVALID2(1U)
+
+#define S_FSTATIC2    8
+#define V_FSTATIC2(x) ((x) << S_FSTATIC2)
+#define F_FSTATIC2    V_FSTATIC2(1U)
+
+#define S_ERRORSRDY2    7
+#define V_ERRORSRDY2(x) ((x) << S_ERRORSRDY2)
+#define F_ERRORSRDY2    V_ERRORSRDY2(1U)
+
+#define S_PLDTXSRDY2    6
+#define V_PLDTXSRDY2(x) ((x) << S_PLDTXSRDY2)
+#define F_PLDTXSRDY2    V_PLDTXSRDY2(1U)
+
+#define S_DBVLD2    5
+#define V_DBVLD2(x) ((x) << S_DBVLD2)
+#define F_DBVLD2    V_DBVLD2(1U)
+
+#define S_PLDTXVALID2    4
+#define V_PLDTXVALID2(x) ((x) << S_PLDTXVALID2)
+#define F_PLDTXVALID2    V_PLDTXVALID2(1U)
+
+#define S_ETXVALID2    3
+#define V_ETXVALID2(x) ((x) << S_ETXVALID2)
+#define F_ETXVALID2    V_ETXVALID2(1U)
+
+#define S_ETXFULL2    2
+#define V_ETXFULL2(x) ((x) << S_ETXFULL2)
+#define F_ETXFULL2    V_ETXFULL2(1U)
+
+#define S_ERXVALID2    1
+#define V_ERXVALID2(x) ((x) << S_ERXVALID2)
+#define F_ERXVALID2    V_ERXVALID2(1U)
+
+#define S_ERXFULL2    0
+#define V_ERXFULL2(x) ((x) << S_ERXFULL2)
+#define F_ERXFULL2    V_ERXFULL2(1U)
+
+#define A_TP_DBG_ESIDE_DISP0 0x136
+
+#define S_RESRDY    31
+#define V_RESRDY(x) ((x) << S_RESRDY)
+#define F_RESRDY    V_RESRDY(1U)
+
+#define S_STATE    28
+#define M_STATE    0x7U
+#define V_STATE(x) ((x) << S_STATE)
+#define G_STATE(x) (((x) >> S_STATE) & M_STATE)
+
+#define S_FIFOCPL5RXVALID    27
+#define V_FIFOCPL5RXVALID(x) ((x) << S_FIFOCPL5RXVALID)
+#define F_FIFOCPL5RXVALID    V_FIFOCPL5RXVALID(1U)
+
+#define S_FIFOETHRXVALID    26
+#define V_FIFOETHRXVALID(x) ((x) << S_FIFOETHRXVALID)
+#define F_FIFOETHRXVALID    V_FIFOETHRXVALID(1U)
+
+#define S_FIFOETHRXSOCP    25
+#define V_FIFOETHRXSOCP(x) ((x) << S_FIFOETHRXSOCP)
+#define F_FIFOETHRXSOCP    V_FIFOETHRXSOCP(1U)
+
+#define S_FIFOPLDRXZEROP    24
+#define V_FIFOPLDRXZEROP(x) ((x) << S_FIFOPLDRXZEROP)
+#define F_FIFOPLDRXZEROP    V_FIFOPLDRXZEROP(1U)
+
+#define S_PLDRXVALID    23
+#define V_PLDRXVALID(x) ((x) << S_PLDRXVALID)
+#define F_PLDRXVALID    V_PLDRXVALID(1U)
+
+#define S_FIFOPLDRXZEROP_SRDY    22
+#define V_FIFOPLDRXZEROP_SRDY(x) ((x) << S_FIFOPLDRXZEROP_SRDY)
+#define F_FIFOPLDRXZEROP_SRDY    V_FIFOPLDRXZEROP_SRDY(1U)
+
+#define S_FIFOIPRXVALID    21
+#define V_FIFOIPRXVALID(x) ((x) << S_FIFOIPRXVALID)
+#define F_FIFOIPRXVALID    V_FIFOIPRXVALID(1U)
+
+#define S_FIFOTCPRXVALID    20
+#define V_FIFOTCPRXVALID(x) ((x) << S_FIFOTCPRXVALID)
+#define F_FIFOTCPRXVALID    V_FIFOTCPRXVALID(1U)
+
+#define S_PLDRXCSUMVALID    19
+#define V_PLDRXCSUMVALID(x) ((x) << S_PLDRXCSUMVALID)
+#define F_PLDRXCSUMVALID    V_PLDRXCSUMVALID(1U)
+
+#define S_FIFOIPCSUMSRDY    18
+#define V_FIFOIPCSUMSRDY(x) ((x) << S_FIFOIPCSUMSRDY)
+#define F_FIFOIPCSUMSRDY    V_FIFOIPCSUMSRDY(1U)
+
+#define S_FIFOIPPSEUDOCSUMSRDY    17
+#define V_FIFOIPPSEUDOCSUMSRDY(x) ((x) << S_FIFOIPPSEUDOCSUMSRDY)
+#define F_FIFOIPPSEUDOCSUMSRDY    V_FIFOIPPSEUDOCSUMSRDY(1U)
+
+#define S_FIFOTCPCSUMSRDY    16
+#define V_FIFOTCPCSUMSRDY(x) ((x) << S_FIFOTCPCSUMSRDY)
+#define F_FIFOTCPCSUMSRDY    V_FIFOTCPCSUMSRDY(1U)
+
+#define S_ESTATIC4    12
+#define M_ESTATIC4    0xfU
+#define V_ESTATIC4(x) ((x) << S_ESTATIC4)
+#define G_ESTATIC4(x) (((x) >> S_ESTATIC4) & M_ESTATIC4)
+
+#define S_FIFOCPLSOCPCNT    10
+#define M_FIFOCPLSOCPCNT    0x3U
+#define V_FIFOCPLSOCPCNT(x) ((x) << S_FIFOCPLSOCPCNT)
+#define G_FIFOCPLSOCPCNT(x) (((x) >> S_FIFOCPLSOCPCNT) & M_FIFOCPLSOCPCNT)
+
+#define S_FIFOETHSOCPCNT    8
+#define M_FIFOETHSOCPCNT    0x3U
+#define V_FIFOETHSOCPCNT(x) ((x) << S_FIFOETHSOCPCNT)
+#define G_FIFOETHSOCPCNT(x) (((x) >> S_FIFOETHSOCPCNT) & M_FIFOETHSOCPCNT)
+
+#define S_FIFOIPSOCPCNT    6
+#define M_FIFOIPSOCPCNT    0x3U
+#define V_FIFOIPSOCPCNT(x) ((x) << S_FIFOIPSOCPCNT)
+#define G_FIFOIPSOCPCNT(x) (((x) >> S_FIFOIPSOCPCNT) & M_FIFOIPSOCPCNT)
+
+#define S_FIFOTCPSOCPCNT    4
+#define M_FIFOTCPSOCPCNT    0x3U
+#define V_FIFOTCPSOCPCNT(x) ((x) << S_FIFOTCPSOCPCNT)
+#define G_FIFOTCPSOCPCNT(x) (((x) >> S_FIFOTCPSOCPCNT) & M_FIFOTCPSOCPCNT)
+
+#define S_PLD_RXZEROP_CNT    2
+#define M_PLD_RXZEROP_CNT    0x3U
+#define V_PLD_RXZEROP_CNT(x) ((x) << S_PLD_RXZEROP_CNT)
+#define G_PLD_RXZEROP_CNT(x) (((x) >> S_PLD_RXZEROP_CNT) & M_PLD_RXZEROP_CNT)
+
+#define S_ESTATIC6    1
+#define V_ESTATIC6(x) ((x) << S_ESTATIC6)
+#define F_ESTATIC6    V_ESTATIC6(1U)
+
+#define S_TXFULL    0
+#define V_TXFULL(x) ((x) << S_TXFULL)
+#define F_TXFULL    V_TXFULL(1U)
+
+#define A_TP_DBG_ESIDE_DISP1 0x137
+#define A_TP_MAC_MATCH_MAP0 0x138
+
+#define S_MAPVALUEWR    16
+#define M_MAPVALUEWR    0xffU
+#define V_MAPVALUEWR(x) ((x) << S_MAPVALUEWR)
+#define G_MAPVALUEWR(x) (((x) >> S_MAPVALUEWR) & M_MAPVALUEWR)
+
+#define S_MAPINDEX    2
+#define M_MAPINDEX    0x1ffU
+#define V_MAPINDEX(x) ((x) << S_MAPINDEX)
+#define G_MAPINDEX(x) (((x) >> S_MAPINDEX) & M_MAPINDEX)
+
+#define S_MAPREAD    1
+#define V_MAPREAD(x) ((x) << S_MAPREAD)
+#define F_MAPREAD    V_MAPREAD(1U)
+
+#define S_MAPWRITE    0
+#define V_MAPWRITE(x) ((x) << S_MAPWRITE)
+#define F_MAPWRITE    V_MAPWRITE(1U)
+
+#define A_TP_MAC_MATCH_MAP1 0x139
+
+#define S_MAPVALUERD    0
+#define M_MAPVALUERD    0x1ffU
+#define V_MAPVALUERD(x) ((x) << S_MAPVALUERD)
+#define G_MAPVALUERD(x) (((x) >> S_MAPVALUERD) & M_MAPVALUERD)
+
+#define A_TP_DBG_ESIDE_DISP2 0x13a
+#define A_TP_DBG_ESIDE_DISP3 0x13b
+#define A_TP_DBG_ESIDE_HDR0 0x13c
+
+#define S_TCPSOPCNT    28
+#define M_TCPSOPCNT    0xfU
+#define V_TCPSOPCNT(x) ((x) << S_TCPSOPCNT)
+#define G_TCPSOPCNT(x) (((x) >> S_TCPSOPCNT) & M_TCPSOPCNT)
+
+#define S_TCPEOPCNT    24
+#define M_TCPEOPCNT    0xfU
+#define V_TCPEOPCNT(x) ((x) << S_TCPEOPCNT)
+#define G_TCPEOPCNT(x) (((x) >> S_TCPEOPCNT) & M_TCPEOPCNT)
+
+#define S_IPSOPCNT    20
+#define M_IPSOPCNT    0xfU
+#define V_IPSOPCNT(x) ((x) << S_IPSOPCNT)
+#define G_IPSOPCNT(x) (((x) >> S_IPSOPCNT) & M_IPSOPCNT)
+
+#define S_IPEOPCNT    16
+#define M_IPEOPCNT    0xfU
+#define V_IPEOPCNT(x) ((x) << S_IPEOPCNT)
+#define G_IPEOPCNT(x) (((x) >> S_IPEOPCNT) & M_IPEOPCNT)
+
+#define S_ETHSOPCNT    12
+#define M_ETHSOPCNT    0xfU
+#define V_ETHSOPCNT(x) ((x) << S_ETHSOPCNT)
+#define G_ETHSOPCNT(x) (((x) >> S_ETHSOPCNT) & M_ETHSOPCNT)
+
+#define S_ETHEOPCNT    8
+#define M_ETHEOPCNT    0xfU
+#define V_ETHEOPCNT(x) ((x) << S_ETHEOPCNT)
+#define G_ETHEOPCNT(x) (((x) >> S_ETHEOPCNT) & M_ETHEOPCNT)
+
+#define S_CPLSOPCNT    4
+#define M_CPLSOPCNT    0xfU
+#define V_CPLSOPCNT(x) ((x) << S_CPLSOPCNT)
+#define G_CPLSOPCNT(x) (((x) >> S_CPLSOPCNT) & M_CPLSOPCNT)
+
+#define S_CPLEOPCNT    0
+#define M_CPLEOPCNT    0xfU
+#define V_CPLEOPCNT(x) ((x) << S_CPLEOPCNT)
+#define G_CPLEOPCNT(x) (((x) >> S_CPLEOPCNT) & M_CPLEOPCNT)
+
+#define A_TP_DBG_ESIDE_HDR1 0x13d
+#define A_TP_DBG_ESIDE_HDR2 0x13e
+#define A_TP_DBG_ESIDE_HDR3 0x13f
+#define A_TP_VLAN_PRI_MAP 0x140
+
+#define S_FRAGMENTATION    9
+#define V_FRAGMENTATION(x) ((x) << S_FRAGMENTATION)
+#define F_FRAGMENTATION    V_FRAGMENTATION(1U)
+
+#define S_MPSHITTYPE    8
+#define V_MPSHITTYPE(x) ((x) << S_MPSHITTYPE)
+#define F_MPSHITTYPE    V_MPSHITTYPE(1U)
+
+#define S_MACMATCH    7
+#define V_MACMATCH(x) ((x) << S_MACMATCH)
+#define F_MACMATCH    V_MACMATCH(1U)
+
+#define S_ETHERTYPE    6
+#define V_ETHERTYPE(x) ((x) << S_ETHERTYPE)
+#define F_ETHERTYPE    V_ETHERTYPE(1U)
+
+#define S_PROTOCOL    5
+#define V_PROTOCOL(x) ((x) << S_PROTOCOL)
+#define F_PROTOCOL    V_PROTOCOL(1U)
+
+#define S_TOS    4
+#define V_TOS(x) ((x) << S_TOS)
+#define F_TOS    V_TOS(1U)
+
+#define S_VLAN    3
+#define V_VLAN(x) ((x) << S_VLAN)
+#define F_VLAN    V_VLAN(1U)
+
+#define S_VNIC_ID    2
+#define V_VNIC_ID(x) ((x) << S_VNIC_ID)
+#define F_VNIC_ID    V_VNIC_ID(1U)
+
+#define S_PORT    1
+#define V_PORT(x) ((x) << S_PORT)
+#define F_PORT    V_PORT(1U)
+
+#define S_FCOE    0
+#define V_FCOE(x) ((x) << S_FCOE)
+#define F_FCOE    V_FCOE(1U)
+
+#define A_TP_INGRESS_CONFIG 0x141
+
+#define S_OPAQUE_TYPE    16
+#define M_OPAQUE_TYPE    0xffffU
+#define V_OPAQUE_TYPE(x) ((x) << S_OPAQUE_TYPE)
+#define G_OPAQUE_TYPE(x) (((x) >> S_OPAQUE_TYPE) & M_OPAQUE_TYPE)
+
+#define S_OPAQUE_RM    15
+#define V_OPAQUE_RM(x) ((x) << S_OPAQUE_RM)
+#define F_OPAQUE_RM    V_OPAQUE_RM(1U)
+
+#define S_OPAQUE_HDR_SIZE    14
+#define V_OPAQUE_HDR_SIZE(x) ((x) << S_OPAQUE_HDR_SIZE)
+#define F_OPAQUE_HDR_SIZE    V_OPAQUE_HDR_SIZE(1U)
+
+#define S_OPAQUE_RM_MAC_IN_MAC    13
+#define V_OPAQUE_RM_MAC_IN_MAC(x) ((x) << S_OPAQUE_RM_MAC_IN_MAC)
+#define F_OPAQUE_RM_MAC_IN_MAC    V_OPAQUE_RM_MAC_IN_MAC(1U)
+
+#define S_FCOE_TARGET    12
+#define V_FCOE_TARGET(x) ((x) << S_FCOE_TARGET)
+#define F_FCOE_TARGET    V_FCOE_TARGET(1U)
+
+#define S_VNIC    11
+#define V_VNIC(x) ((x) << S_VNIC)
+#define F_VNIC    V_VNIC(1U)
+
+#define S_CSUM_HAS_PSEUDO_HDR    10
+#define V_CSUM_HAS_PSEUDO_HDR(x) ((x) << S_CSUM_HAS_PSEUDO_HDR)
+#define F_CSUM_HAS_PSEUDO_HDR    V_CSUM_HAS_PSEUDO_HDR(1U)
+
+#define S_RM_OVLAN    9
+#define V_RM_OVLAN(x) ((x) << S_RM_OVLAN)
+#define F_RM_OVLAN    V_RM_OVLAN(1U)
+
+#define S_LOOKUPEVERYPKT    8
+#define V_LOOKUPEVERYPKT(x) ((x) << S_LOOKUPEVERYPKT)
+#define F_LOOKUPEVERYPKT    V_LOOKUPEVERYPKT(1U)
+
+#define S_IPV6_EXT_HDR_SKIP    0
+#define M_IPV6_EXT_HDR_SKIP    0xffU
+#define V_IPV6_EXT_HDR_SKIP(x) ((x) << S_IPV6_EXT_HDR_SKIP)
+#define G_IPV6_EXT_HDR_SKIP(x) (((x) >> S_IPV6_EXT_HDR_SKIP) & M_IPV6_EXT_HDR_SKIP)
+
+#define A_TP_TX_DROP_CFG_CH2 0x142
+#define A_TP_TX_DROP_CFG_CH3 0x143
+#define A_TP_EGRESS_CONFIG 0x145
+
+#define S_REWRITEFORCETOSIZE    0
+#define V_REWRITEFORCETOSIZE(x) ((x) << S_REWRITEFORCETOSIZE)
+#define F_REWRITEFORCETOSIZE    V_REWRITEFORCETOSIZE(1U)
+
+#define A_TP_EHDR_CONFIG_LO 0x146
+
+#define S_CPLLIMIT    24
+#define M_CPLLIMIT    0xffU
+#define V_CPLLIMIT(x) ((x) << S_CPLLIMIT)
+#define G_CPLLIMIT(x) (((x) >> S_CPLLIMIT) & M_CPLLIMIT)
+
+#define S_ETHLIMIT    16
+#define M_ETHLIMIT    0xffU
+#define V_ETHLIMIT(x) ((x) << S_ETHLIMIT)
+#define G_ETHLIMIT(x) (((x) >> S_ETHLIMIT) & M_ETHLIMIT)
+
+#define S_IPLIMIT    8
+#define M_IPLIMIT    0xffU
+#define V_IPLIMIT(x) ((x) << S_IPLIMIT)
+#define G_IPLIMIT(x) (((x) >> S_IPLIMIT) & M_IPLIMIT)
+
+#define S_TCPLIMIT    0
+#define M_TCPLIMIT    0xffU
+#define V_TCPLIMIT(x) ((x) << S_TCPLIMIT)
+#define G_TCPLIMIT(x) (((x) >> S_TCPLIMIT) & M_TCPLIMIT)
+
+#define A_TP_EHDR_CONFIG_HI 0x147
+#define A_TP_DBG_ESIDE_INT 0x148
+
+#define S_ERXSOP2X    28
+#define M_ERXSOP2X    0xfU
+#define V_ERXSOP2X(x) ((x) << S_ERXSOP2X)
+#define G_ERXSOP2X(x) (((x) >> S_ERXSOP2X) & M_ERXSOP2X)
+
+#define S_ERXEOP2X    24
+#define M_ERXEOP2X    0xfU
+#define V_ERXEOP2X(x) ((x) << S_ERXEOP2X)
+#define G_ERXEOP2X(x) (((x) >> S_ERXEOP2X) & M_ERXEOP2X)
+
+#define S_ERXVALID2X    20
+#define M_ERXVALID2X    0xfU
+#define V_ERXVALID2X(x) ((x) << S_ERXVALID2X)
+#define G_ERXVALID2X(x) (((x) >> S_ERXVALID2X) & M_ERXVALID2X)
+
+#define S_ERXAFULL2X    16
+#define M_ERXAFULL2X    0xfU
+#define V_ERXAFULL2X(x) ((x) << S_ERXAFULL2X)
+#define G_ERXAFULL2X(x) (((x) >> S_ERXAFULL2X) & M_ERXAFULL2X)
+
+#define S_PLD2XTXVALID    12
+#define M_PLD2XTXVALID    0xfU
+#define V_PLD2XTXVALID(x) ((x) << S_PLD2XTXVALID)
+#define G_PLD2XTXVALID(x) (((x) >> S_PLD2XTXVALID) & M_PLD2XTXVALID)
+
+#define S_PLD2XTXAFULL    8
+#define M_PLD2XTXAFULL    0xfU
+#define V_PLD2XTXAFULL(x) ((x) << S_PLD2XTXAFULL)
+#define G_PLD2XTXAFULL(x) (((x) >> S_PLD2XTXAFULL) & M_PLD2XTXAFULL)
+
+#define S_ERRORSRDY    7
+#define V_ERRORSRDY(x) ((x) << S_ERRORSRDY)
+#define F_ERRORSRDY    V_ERRORSRDY(1U)
+
+#define S_ERRORDRDY    6
+#define V_ERRORDRDY(x) ((x) << S_ERRORDRDY)
+#define F_ERRORDRDY    V_ERRORDRDY(1U)
+
+#define S_TCPOPSRDY    5
+#define V_TCPOPSRDY(x) ((x) << S_TCPOPSRDY)
+#define F_TCPOPSRDY    V_TCPOPSRDY(1U)
+
+#define S_TCPOPDRDY    4
+#define V_TCPOPDRDY(x) ((x) << S_TCPOPDRDY)
+#define F_TCPOPDRDY    V_TCPOPDRDY(1U)
+
+#define S_PLDTXSRDY    3
+#define V_PLDTXSRDY(x) ((x) << S_PLDTXSRDY)
+#define F_PLDTXSRDY    V_PLDTXSRDY(1U)
+
+#define S_PLDTXDRDY    2
+#define V_PLDTXDRDY(x) ((x) << S_PLDTXDRDY)
+#define F_PLDTXDRDY    V_PLDTXDRDY(1U)
+
+#define S_TCPOPTTXVALID    1
+#define V_TCPOPTTXVALID(x) ((x) << S_TCPOPTTXVALID)
+#define F_TCPOPTTXVALID    V_TCPOPTTXVALID(1U)
+
+#define S_TCPOPTTXFULL    0
+#define V_TCPOPTTXFULL(x) ((x) << S_TCPOPTTXFULL)
+#define F_TCPOPTTXFULL    V_TCPOPTTXFULL(1U)
+
+#define A_TP_DBG_ESIDE_DEMUX 0x149
+
+#define S_EALLDONE    28
+#define M_EALLDONE    0xfU
+#define V_EALLDONE(x) ((x) << S_EALLDONE)
+#define G_EALLDONE(x) (((x) >> S_EALLDONE) & M_EALLDONE)
+
+#define S_EFIFOPLDDONE    24
+#define M_EFIFOPLDDONE    0xfU
+#define V_EFIFOPLDDONE(x) ((x) << S_EFIFOPLDDONE)
+#define G_EFIFOPLDDONE(x) (((x) >> S_EFIFOPLDDONE) & M_EFIFOPLDDONE)
+
+#define S_EDBDONE    20
+#define M_EDBDONE    0xfU
+#define V_EDBDONE(x) ((x) << S_EDBDONE)
+#define G_EDBDONE(x) (((x) >> S_EDBDONE) & M_EDBDONE)
+
+#define S_EISSFIFODONE    16
+#define M_EISSFIFODONE    0xfU
+#define V_EISSFIFODONE(x) ((x) << S_EISSFIFODONE)
+#define G_EISSFIFODONE(x) (((x) >> S_EISSFIFODONE) & M_EISSFIFODONE)
+
+#define S_EACKERRFIFODONE    12
+#define M_EACKERRFIFODONE    0xfU
+#define V_EACKERRFIFODONE(x) ((x) << S_EACKERRFIFODONE)
+#define G_EACKERRFIFODONE(x) (((x) >> S_EACKERRFIFODONE) & M_EACKERRFIFODONE)
+
+#define S_EFIFOERRORDONE    8
+#define M_EFIFOERRORDONE    0xfU
+#define V_EFIFOERRORDONE(x) ((x) << S_EFIFOERRORDONE)
+#define G_EFIFOERRORDONE(x) (((x) >> S_EFIFOERRORDONE) & M_EFIFOERRORDONE)
+
+#define S_ERXPKTATTRFIFOFDONE    4
+#define M_ERXPKTATTRFIFOFDONE    0xfU
+#define V_ERXPKTATTRFIFOFDONE(x) ((x) << S_ERXPKTATTRFIFOFDONE)
+#define G_ERXPKTATTRFIFOFDONE(x) (((x) >> S_ERXPKTATTRFIFOFDONE) & M_ERXPKTATTRFIFOFDONE)
+
+#define S_ETCPOPDONE    0
+#define M_ETCPOPDONE    0xfU
+#define V_ETCPOPDONE(x) ((x) << S_ETCPOPDONE)
+#define G_ETCPOPDONE(x) (((x) >> S_ETCPOPDONE) & M_ETCPOPDONE)
+
+#define A_TP_DBG_ESIDE_IN0 0x14a
+
+#define S_RXVALID    31
+#define V_RXVALID(x) ((x) << S_RXVALID)
+#define F_RXVALID    V_RXVALID(1U)
+
+#define S_RXFULL    30
+#define V_RXFULL(x) ((x) << S_RXFULL)
+#define F_RXFULL    V_RXFULL(1U)
+
+#define S_RXSOCP    29
+#define V_RXSOCP(x) ((x) << S_RXSOCP)
+#define F_RXSOCP    V_RXSOCP(1U)
+
+#define S_RXEOP    28
+#define V_RXEOP(x) ((x) << S_RXEOP)
+#define F_RXEOP    V_RXEOP(1U)
+
+#define S_RXVALID_I    27
+#define V_RXVALID_I(x) ((x) << S_RXVALID_I)
+#define F_RXVALID_I    V_RXVALID_I(1U)
+
+#define S_RXFULL_I    26
+#define V_RXFULL_I(x) ((x) << S_RXFULL_I)
+#define F_RXFULL_I    V_RXFULL_I(1U)
+
+#define S_RXSOCP_I    25
+#define V_RXSOCP_I(x) ((x) << S_RXSOCP_I)
+#define F_RXSOCP_I    V_RXSOCP_I(1U)
+
+#define S_RXEOP_I    24
+#define V_RXEOP_I(x) ((x) << S_RXEOP_I)
+#define F_RXEOP_I    V_RXEOP_I(1U)
+
+#define S_RXVALID_I2    23
+#define V_RXVALID_I2(x) ((x) << S_RXVALID_I2)
+#define F_RXVALID_I2    V_RXVALID_I2(1U)
+
+#define S_RXFULL_I2    22
+#define V_RXFULL_I2(x) ((x) << S_RXFULL_I2)
+#define F_RXFULL_I2    V_RXFULL_I2(1U)
+
+#define S_RXSOCP_I2    21
+#define V_RXSOCP_I2(x) ((x) << S_RXSOCP_I2)
+#define F_RXSOCP_I2    V_RXSOCP_I2(1U)
+
+#define S_RXEOP_I2    20
+#define V_RXEOP_I2(x) ((x) << S_RXEOP_I2)
+#define F_RXEOP_I2    V_RXEOP_I2(1U)
+
+#define S_CT_MPA_TXVALID_FIFO    19
+#define V_CT_MPA_TXVALID_FIFO(x) ((x) << S_CT_MPA_TXVALID_FIFO)
+#define F_CT_MPA_TXVALID_FIFO    V_CT_MPA_TXVALID_FIFO(1U)
+
+#define S_CT_MPA_TXFULL_FIFO    18
+#define V_CT_MPA_TXFULL_FIFO(x) ((x) << S_CT_MPA_TXFULL_FIFO)
+#define F_CT_MPA_TXFULL_FIFO    V_CT_MPA_TXFULL_FIFO(1U)
+
+#define S_CT_MPA_TXVALID    17
+#define V_CT_MPA_TXVALID(x) ((x) << S_CT_MPA_TXVALID)
+#define F_CT_MPA_TXVALID    V_CT_MPA_TXVALID(1U)
+
+#define S_CT_MPA_TXFULL    16
+#define V_CT_MPA_TXFULL(x) ((x) << S_CT_MPA_TXFULL)
+#define F_CT_MPA_TXFULL    V_CT_MPA_TXFULL(1U)
+
+#define S_RXVALID_BUF    15
+#define V_RXVALID_BUF(x) ((x) << S_RXVALID_BUF)
+#define F_RXVALID_BUF    V_RXVALID_BUF(1U)
+
+#define S_RXFULL_BUF    14
+#define V_RXFULL_BUF(x) ((x) << S_RXFULL_BUF)
+#define F_RXFULL_BUF    V_RXFULL_BUF(1U)
+
+#define S_PLD_TXVALID    13
+#define V_PLD_TXVALID(x) ((x) << S_PLD_TXVALID)
+#define F_PLD_TXVALID    V_PLD_TXVALID(1U)
+
+#define S_PLD_TXFULL    12
+#define V_PLD_TXFULL(x) ((x) << S_PLD_TXFULL)
+#define F_PLD_TXFULL    V_PLD_TXFULL(1U)
+
+#define S_ISS_FIFO_SRDY    11
+#define V_ISS_FIFO_SRDY(x) ((x) << S_ISS_FIFO_SRDY)
+#define F_ISS_FIFO_SRDY    V_ISS_FIFO_SRDY(1U)
+
+#define S_ISS_FIFO_DRDY    10
+#define V_ISS_FIFO_DRDY(x) ((x) << S_ISS_FIFO_DRDY)
+#define F_ISS_FIFO_DRDY    V_ISS_FIFO_DRDY(1U)
+
+#define S_CT_TCP_OP_ISS_SRDY    9
+#define V_CT_TCP_OP_ISS_SRDY(x) ((x) << S_CT_TCP_OP_ISS_SRDY)
+#define F_CT_TCP_OP_ISS_SRDY    V_CT_TCP_OP_ISS_SRDY(1U)
+
+#define S_CT_TCP_OP_ISS_DRDY    8
+#define V_CT_TCP_OP_ISS_DRDY(x) ((x) << S_CT_TCP_OP_ISS_DRDY)
+#define F_CT_TCP_OP_ISS_DRDY    V_CT_TCP_OP_ISS_DRDY(1U)
+
+#define S_P2CSUMERROR_SRDY    7
+#define V_P2CSUMERROR_SRDY(x) ((x) << S_P2CSUMERROR_SRDY)
+#define F_P2CSUMERROR_SRDY    V_P2CSUMERROR_SRDY(1U)
+
+#define S_P2CSUMERROR_DRDY    6
+#define V_P2CSUMERROR_DRDY(x) ((x) << S_P2CSUMERROR_DRDY)
+#define F_P2CSUMERROR_DRDY    V_P2CSUMERROR_DRDY(1U)
+
+#define S_FIFO_ERROR_SRDY    5
+#define V_FIFO_ERROR_SRDY(x) ((x) << S_FIFO_ERROR_SRDY)
+#define F_FIFO_ERROR_SRDY    V_FIFO_ERROR_SRDY(1U)
+
+#define S_FIFO_ERROR_DRDY    4
+#define V_FIFO_ERROR_DRDY(x) ((x) << S_FIFO_ERROR_DRDY)
+#define F_FIFO_ERROR_DRDY    V_FIFO_ERROR_DRDY(1U)
+
+#define S_PLD_SRDY    3
+#define V_PLD_SRDY(x) ((x) << S_PLD_SRDY)
+#define F_PLD_SRDY    V_PLD_SRDY(1U)
+
+#define S_PLD_DRDY    2
+#define V_PLD_DRDY(x) ((x) << S_PLD_DRDY)
+#define F_PLD_DRDY    V_PLD_DRDY(1U)
+
+#define S_RX_PKT_ATTR_SRDY    1
+#define V_RX_PKT_ATTR_SRDY(x) ((x) << S_RX_PKT_ATTR_SRDY)
+#define F_RX_PKT_ATTR_SRDY    V_RX_PKT_ATTR_SRDY(1U)
+
+#define S_RX_PKT_ATTR_DRDY    0
+#define V_RX_PKT_ATTR_DRDY(x) ((x) << S_RX_PKT_ATTR_DRDY)
+#define F_RX_PKT_ATTR_DRDY    V_RX_PKT_ATTR_DRDY(1U)
+
+#define A_TP_DBG_ESIDE_IN1 0x14b
+#define A_TP_DBG_ESIDE_IN2 0x14c
+#define A_TP_DBG_ESIDE_IN3 0x14d
+#define A_TP_DBG_ESIDE_FRM 0x14e
+
+#define S_ERX2XERROR    28
+#define M_ERX2XERROR    0xfU
+#define V_ERX2XERROR(x) ((x) << S_ERX2XERROR)
+#define G_ERX2XERROR(x) (((x) >> S_ERX2XERROR) & M_ERX2XERROR)
+
+#define S_EPLDTX2XERROR    24
+#define M_EPLDTX2XERROR    0xfU
+#define V_EPLDTX2XERROR(x) ((x) << S_EPLDTX2XERROR)
+#define G_EPLDTX2XERROR(x) (((x) >> S_EPLDTX2XERROR) & M_EPLDTX2XERROR)
+
+#define S_ETXERROR    20
+#define M_ETXERROR    0xfU
+#define V_ETXERROR(x) ((x) << S_ETXERROR)
+#define G_ETXERROR(x) (((x) >> S_ETXERROR) & M_ETXERROR)
+
+#define S_EPLDRXERROR    16
+#define M_EPLDRXERROR    0xfU
+#define V_EPLDRXERROR(x) ((x) << S_EPLDRXERROR)
+#define G_EPLDRXERROR(x) (((x) >> S_EPLDRXERROR) & M_EPLDRXERROR)
+
+#define S_ERXSIZEERROR3    12
+#define M_ERXSIZEERROR3    0xfU
+#define V_ERXSIZEERROR3(x) ((x) << S_ERXSIZEERROR3)
+#define G_ERXSIZEERROR3(x) (((x) >> S_ERXSIZEERROR3) & M_ERXSIZEERROR3)
+
+#define S_ERXSIZEERROR2    8
+#define M_ERXSIZEERROR2    0xfU
+#define V_ERXSIZEERROR2(x) ((x) << S_ERXSIZEERROR2)
+#define G_ERXSIZEERROR2(x) (((x) >> S_ERXSIZEERROR2) & M_ERXSIZEERROR2)
+
+#define S_ERXSIZEERROR1    4
+#define M_ERXSIZEERROR1    0xfU
+#define V_ERXSIZEERROR1(x) ((x) << S_ERXSIZEERROR1)
+#define G_ERXSIZEERROR1(x) (((x) >> S_ERXSIZEERROR1) & M_ERXSIZEERROR1)
+
+#define S_ERXSIZEERROR0    0
+#define M_ERXSIZEERROR0    0xfU
+#define V_ERXSIZEERROR0(x) ((x) << S_ERXSIZEERROR0)
+#define G_ERXSIZEERROR0(x) (((x) >> S_ERXSIZEERROR0) & M_ERXSIZEERROR0)
+
+#define A_TP_DBG_ESIDE_DRP 0x14f
+
+#define S_RXDROP3    24
+#define M_RXDROP3    0xffU
+#define V_RXDROP3(x) ((x) << S_RXDROP3)
+#define G_RXDROP3(x) (((x) >> S_RXDROP3) & M_RXDROP3)
+
+#define S_RXDROP2    16
+#define M_RXDROP2    0xffU
+#define V_RXDROP2(x) ((x) << S_RXDROP2)
+#define G_RXDROP2(x) (((x) >> S_RXDROP2) & M_RXDROP2)
+
+#define S_RXDROP1    8
+#define M_RXDROP1    0xffU
+#define V_RXDROP1(x) ((x) << S_RXDROP1)
+#define G_RXDROP1(x) (((x) >> S_RXDROP1) & M_RXDROP1)
+
+#define S_RXDROP0    0
+#define M_RXDROP0    0xffU
+#define V_RXDROP0(x) ((x) << S_RXDROP0)
+#define G_RXDROP0(x) (((x) >> S_RXDROP0) & M_RXDROP0)
+
+#define A_TP_DBG_ESIDE_TX 0x150
+
+#define S_ETXVALID    4
+#define M_ETXVALID    0xfU
+#define V_ETXVALID(x) ((x) << S_ETXVALID)
+#define G_ETXVALID(x) (((x) >> S_ETXVALID) & M_ETXVALID)
+
+#define S_ETXFULL    0
+#define M_ETXFULL    0xfU
+#define V_ETXFULL(x) ((x) << S_ETXFULL)
+#define G_ETXFULL(x) (((x) >> S_ETXFULL) & M_ETXFULL)
+
+#define A_TP_ESIDE_SVID_MASK 0x151
+#define A_TP_ESIDE_DVID_MASK 0x152
+#define A_TP_ESIDE_ALIGN_MASK 0x153
+
+#define S_USE_LOOP_BIT    24
+#define V_USE_LOOP_BIT(x) ((x) << S_USE_LOOP_BIT)
+#define F_USE_LOOP_BIT    V_USE_LOOP_BIT(1U)
+
+#define S_LOOP_OFFSET    16
+#define M_LOOP_OFFSET    0xffU
+#define V_LOOP_OFFSET(x) ((x) << S_LOOP_OFFSET)
+#define G_LOOP_OFFSET(x) (((x) >> S_LOOP_OFFSET) & M_LOOP_OFFSET)
+
+#define S_DVID_ID_OFFSET    8
+#define M_DVID_ID_OFFSET    0xffU
+#define V_DVID_ID_OFFSET(x) ((x) << S_DVID_ID_OFFSET)
+#define G_DVID_ID_OFFSET(x) (((x) >> S_DVID_ID_OFFSET) & M_DVID_ID_OFFSET)
+
+#define S_SVID_ID_OFFSET    0
+#define M_SVID_ID_OFFSET    0xffU
+#define V_SVID_ID_OFFSET(x) ((x) << S_SVID_ID_OFFSET)
+#define G_SVID_ID_OFFSET(x) (((x) >> S_SVID_ID_OFFSET) & M_SVID_ID_OFFSET)
+
+#define A_TP_DBG_CSIDE_RX0 0x230
+
+#define S_CRXSOPCNT    28
+#define M_CRXSOPCNT    0xfU
+#define V_CRXSOPCNT(x) ((x) << S_CRXSOPCNT)
+#define G_CRXSOPCNT(x) (((x) >> S_CRXSOPCNT) & M_CRXSOPCNT)
+
+#define S_CRXEOPCNT    24
+#define M_CRXEOPCNT    0xfU
+#define V_CRXEOPCNT(x) ((x) << S_CRXEOPCNT)
+#define G_CRXEOPCNT(x) (((x) >> S_CRXEOPCNT) & M_CRXEOPCNT)
+
+#define S_CRXPLDSOPCNT    20
+#define M_CRXPLDSOPCNT    0xfU
+#define V_CRXPLDSOPCNT(x) ((x) << S_CRXPLDSOPCNT)
+#define G_CRXPLDSOPCNT(x) (((x) >> S_CRXPLDSOPCNT) & M_CRXPLDSOPCNT)
+
+#define S_CRXPLDEOPCNT    16
+#define M_CRXPLDEOPCNT    0xfU
+#define V_CRXPLDEOPCNT(x) ((x) << S_CRXPLDEOPCNT)
+#define G_CRXPLDEOPCNT(x) (((x) >> S_CRXPLDEOPCNT) & M_CRXPLDEOPCNT)
+
+#define S_CRXARBSOPCNT    12
+#define M_CRXARBSOPCNT    0xfU
+#define V_CRXARBSOPCNT(x) ((x) << S_CRXARBSOPCNT)
+#define G_CRXARBSOPCNT(x) (((x) >> S_CRXARBSOPCNT) & M_CRXARBSOPCNT)
+
+#define S_CRXARBEOPCNT    8
+#define M_CRXARBEOPCNT    0xfU
+#define V_CRXARBEOPCNT(x) ((x) << S_CRXARBEOPCNT)
+#define G_CRXARBEOPCNT(x) (((x) >> S_CRXARBEOPCNT) & M_CRXARBEOPCNT)
+
+#define S_CRXCPLSOPCNT    4
+#define M_CRXCPLSOPCNT    0xfU
+#define V_CRXCPLSOPCNT(x) ((x) << S_CRXCPLSOPCNT)
+#define G_CRXCPLSOPCNT(x) (((x) >> S_CRXCPLSOPCNT) & M_CRXCPLSOPCNT)
+
+#define S_CRXCPLEOPCNT    0
+#define M_CRXCPLEOPCNT    0xfU
+#define V_CRXCPLEOPCNT(x) ((x) << S_CRXCPLEOPCNT)
+#define G_CRXCPLEOPCNT(x) (((x) >> S_CRXCPLEOPCNT) & M_CRXCPLEOPCNT)
+
+#define A_TP_DBG_CSIDE_RX1 0x231
+#define A_TP_DBG_CSIDE_RX2 0x232
+#define A_TP_DBG_CSIDE_RX3 0x233
+#define A_TP_DBG_CSIDE_TX0 0x234
+
+#define S_TXSOPCNT    28
+#define M_TXSOPCNT    0xfU
+#define V_TXSOPCNT(x) ((x) << S_TXSOPCNT)
+#define G_TXSOPCNT(x) (((x) >> S_TXSOPCNT) & M_TXSOPCNT)
+
+#define S_TXEOPCNT    24
+#define M_TXEOPCNT    0xfU
+#define V_TXEOPCNT(x) ((x) << S_TXEOPCNT)
+#define G_TXEOPCNT(x) (((x) >> S_TXEOPCNT) & M_TXEOPCNT)
+
+#define S_TXPLDSOPCNT    20
+#define M_TXPLDSOPCNT    0xfU
+#define V_TXPLDSOPCNT(x) ((x) << S_TXPLDSOPCNT)
+#define G_TXPLDSOPCNT(x) (((x) >> S_TXPLDSOPCNT) & M_TXPLDSOPCNT)
+
+#define S_TXPLDEOPCNT    16
+#define M_TXPLDEOPCNT    0xfU
+#define V_TXPLDEOPCNT(x) ((x) << S_TXPLDEOPCNT)
+#define G_TXPLDEOPCNT(x) (((x) >> S_TXPLDEOPCNT) & M_TXPLDEOPCNT)
+
+#define S_TXARBSOPCNT    12
+#define M_TXARBSOPCNT    0xfU
+#define V_TXARBSOPCNT(x) ((x) << S_TXARBSOPCNT)
+#define G_TXARBSOPCNT(x) (((x) >> S_TXARBSOPCNT) & M_TXARBSOPCNT)
+
+#define S_TXARBEOPCNT    8
+#define M_TXARBEOPCNT    0xfU
+#define V_TXARBEOPCNT(x) ((x) << S_TXARBEOPCNT)
+#define G_TXARBEOPCNT(x) (((x) >> S_TXARBEOPCNT) & M_TXARBEOPCNT)
+
+#define S_TXCPLSOPCNT    4
+#define M_TXCPLSOPCNT    0xfU
+#define V_TXCPLSOPCNT(x) ((x) << S_TXCPLSOPCNT)
+#define G_TXCPLSOPCNT(x) (((x) >> S_TXCPLSOPCNT) & M_TXCPLSOPCNT)
+
+#define S_TXCPLEOPCNT    0
+#define M_TXCPLEOPCNT    0xfU
+#define V_TXCPLEOPCNT(x) ((x) << S_TXCPLEOPCNT)
+#define G_TXCPLEOPCNT(x) (((x) >> S_TXCPLEOPCNT) & M_TXCPLEOPCNT)
+
+#define A_TP_DBG_CSIDE_TX1 0x235
+#define A_TP_DBG_CSIDE_TX2 0x236
+#define A_TP_DBG_CSIDE_TX3 0x237
+#define A_TP_DBG_CSIDE_FIFO0 0x238
+
+#define S_PLD_RXZEROP_SRDY1    31
+#define V_PLD_RXZEROP_SRDY1(x) ((x) << S_PLD_RXZEROP_SRDY1)
+#define F_PLD_RXZEROP_SRDY1    V_PLD_RXZEROP_SRDY1(1U)
+
+#define S_PLD_RXZEROP_DRDY1    30
+#define V_PLD_RXZEROP_DRDY1(x) ((x) << S_PLD_RXZEROP_DRDY1)
+#define F_PLD_RXZEROP_DRDY1    V_PLD_RXZEROP_DRDY1(1U)
+
+#define S_PLD_TXZEROP_SRDY1    29
+#define V_PLD_TXZEROP_SRDY1(x) ((x) << S_PLD_TXZEROP_SRDY1)
+#define F_PLD_TXZEROP_SRDY1    V_PLD_TXZEROP_SRDY1(1U)
+
+#define S_PLD_TXZEROP_DRDY1    28
+#define V_PLD_TXZEROP_DRDY1(x) ((x) << S_PLD_TXZEROP_DRDY1)
+#define F_PLD_TXZEROP_DRDY1    V_PLD_TXZEROP_DRDY1(1U)
+
+#define S_PLD_TX_SRDY1    27
+#define V_PLD_TX_SRDY1(x) ((x) << S_PLD_TX_SRDY1)
+#define F_PLD_TX_SRDY1    V_PLD_TX_SRDY1(1U)
+
+#define S_PLD_TX_DRDY1    26
+#define V_PLD_TX_DRDY1(x) ((x) << S_PLD_TX_DRDY1)
+#define F_PLD_TX_DRDY1    V_PLD_TX_DRDY1(1U)
+
+#define S_ERROR_SRDY1    25
+#define V_ERROR_SRDY1(x) ((x) << S_ERROR_SRDY1)
+#define F_ERROR_SRDY1    V_ERROR_SRDY1(1U)
+
+#define S_ERROR_DRDY1    24
+#define V_ERROR_DRDY1(x) ((x) << S_ERROR_DRDY1)
+#define F_ERROR_DRDY1    V_ERROR_DRDY1(1U)
+
+#define S_DB_VLD1    23
+#define V_DB_VLD1(x) ((x) << S_DB_VLD1)
+#define F_DB_VLD1    V_DB_VLD1(1U)
+
+#define S_DB_GT1    22
+#define V_DB_GT1(x) ((x) << S_DB_GT1)
+#define F_DB_GT1    V_DB_GT1(1U)
+
+#define S_TXVALID1    21
+#define V_TXVALID1(x) ((x) << S_TXVALID1)
+#define F_TXVALID1    V_TXVALID1(1U)
+
+#define S_TXFULL1    20
+#define V_TXFULL1(x) ((x) << S_TXFULL1)
+#define F_TXFULL1    V_TXFULL1(1U)
+
+#define S_PLD_TXVALID1    19
+#define V_PLD_TXVALID1(x) ((x) << S_PLD_TXVALID1)
+#define F_PLD_TXVALID1    V_PLD_TXVALID1(1U)
+
+#define S_PLD_TXFULL1    18
+#define V_PLD_TXFULL1(x) ((x) << S_PLD_TXFULL1)
+#define F_PLD_TXFULL1    V_PLD_TXFULL1(1U)
+
+#define S_CPL5_TXVALID1    17
+#define V_CPL5_TXVALID1(x) ((x) << S_CPL5_TXVALID1)
+#define F_CPL5_TXVALID1    V_CPL5_TXVALID1(1U)
+
+#define S_CPL5_TXFULL1    16
+#define V_CPL5_TXFULL1(x) ((x) << S_CPL5_TXFULL1)
+#define F_CPL5_TXFULL1    V_CPL5_TXFULL1(1U)
+
+#define S_PLD_RXZEROP_SRDY0    15
+#define V_PLD_RXZEROP_SRDY0(x) ((x) << S_PLD_RXZEROP_SRDY0)
+#define F_PLD_RXZEROP_SRDY0    V_PLD_RXZEROP_SRDY0(1U)
+
+#define S_PLD_RXZEROP_DRDY0    14
+#define V_PLD_RXZEROP_DRDY0(x) ((x) << S_PLD_RXZEROP_DRDY0)
+#define F_PLD_RXZEROP_DRDY0    V_PLD_RXZEROP_DRDY0(1U)
+
+#define S_PLD_TXZEROP_SRDY0    13
+#define V_PLD_TXZEROP_SRDY0(x) ((x) << S_PLD_TXZEROP_SRDY0)
+#define F_PLD_TXZEROP_SRDY0    V_PLD_TXZEROP_SRDY0(1U)
+
+#define S_PLD_TXZEROP_DRDY0    12
+#define V_PLD_TXZEROP_DRDY0(x) ((x) << S_PLD_TXZEROP_DRDY0)
+#define F_PLD_TXZEROP_DRDY0    V_PLD_TXZEROP_DRDY0(1U)
+
+#define S_PLD_TX_SRDY0    11
+#define V_PLD_TX_SRDY0(x) ((x) << S_PLD_TX_SRDY0)
+#define F_PLD_TX_SRDY0    V_PLD_TX_SRDY0(1U)
+
+#define S_PLD_TX_DRDY0    10
+#define V_PLD_TX_DRDY0(x) ((x) << S_PLD_TX_DRDY0)
+#define F_PLD_TX_DRDY0    V_PLD_TX_DRDY0(1U)
+
+#define S_ERROR_SRDY0    9
+#define V_ERROR_SRDY0(x) ((x) << S_ERROR_SRDY0)
+#define F_ERROR_SRDY0    V_ERROR_SRDY0(1U)
+
+#define S_ERROR_DRDY0    8
+#define V_ERROR_DRDY0(x) ((x) << S_ERROR_DRDY0)
+#define F_ERROR_DRDY0    V_ERROR_DRDY0(1U)
+
+#define S_DB_VLD0    7
+#define V_DB_VLD0(x) ((x) << S_DB_VLD0)
+#define F_DB_VLD0    V_DB_VLD0(1U)
+
+#define S_DB_GT0    6
+#define V_DB_GT0(x) ((x) << S_DB_GT0)
+#define F_DB_GT0    V_DB_GT0(1U)
+
+#define S_TXVALID0    5
+#define V_TXVALID0(x) ((x) << S_TXVALID0)
+#define F_TXVALID0    V_TXVALID0(1U)
+
+#define S_TXFULL0    4
+#define V_TXFULL0(x) ((x) << S_TXFULL0)
+#define F_TXFULL0    V_TXFULL0(1U)
+
+#define S_PLD_TXVALID0    3
+#define V_PLD_TXVALID0(x) ((x) << S_PLD_TXVALID0)
+#define F_PLD_TXVALID0    V_PLD_TXVALID0(1U)
+
+#define S_PLD_TXFULL0    2
+#define V_PLD_TXFULL0(x) ((x) << S_PLD_TXFULL0)
+#define F_PLD_TXFULL0    V_PLD_TXFULL0(1U)
+
+#define S_CPL5_TXVALID0    1
+#define V_CPL5_TXVALID0(x) ((x) << S_CPL5_TXVALID0)
+#define F_CPL5_TXVALID0    V_CPL5_TXVALID0(1U)
+
+#define S_CPL5_TXFULL0    0
+#define V_CPL5_TXFULL0(x) ((x) << S_CPL5_TXFULL0)
+#define F_CPL5_TXFULL0    V_CPL5_TXFULL0(1U)
+
+#define A_TP_DBG_CSIDE_FIFO1 0x239
+
+#define S_PLD_RXZEROP_SRDY3    31
+#define V_PLD_RXZEROP_SRDY3(x) ((x) << S_PLD_RXZEROP_SRDY3)
+#define F_PLD_RXZEROP_SRDY3    V_PLD_RXZEROP_SRDY3(1U)
+
+#define S_PLD_RXZEROP_DRDY3    30
+#define V_PLD_RXZEROP_DRDY3(x) ((x) << S_PLD_RXZEROP_DRDY3)
+#define F_PLD_RXZEROP_DRDY3    V_PLD_RXZEROP_DRDY3(1U)
+
+#define S_PLD_TXZEROP_SRDY3    29
+#define V_PLD_TXZEROP_SRDY3(x) ((x) << S_PLD_TXZEROP_SRDY3)
+#define F_PLD_TXZEROP_SRDY3    V_PLD_TXZEROP_SRDY3(1U)
+
+#define S_PLD_TXZEROP_DRDY3    28
+#define V_PLD_TXZEROP_DRDY3(x) ((x) << S_PLD_TXZEROP_DRDY3)
+#define F_PLD_TXZEROP_DRDY3    V_PLD_TXZEROP_DRDY3(1U)
+
+#define S_PLD_TX_SRDY3    27
+#define V_PLD_TX_SRDY3(x) ((x) << S_PLD_TX_SRDY3)
+#define F_PLD_TX_SRDY3    V_PLD_TX_SRDY3(1U)
+
+#define S_PLD_TX_DRDY3    26
+#define V_PLD_TX_DRDY3(x) ((x) << S_PLD_TX_DRDY3)
+#define F_PLD_TX_DRDY3    V_PLD_TX_DRDY3(1U)
+
+#define S_ERROR_SRDY3    25
+#define V_ERROR_SRDY3(x) ((x) << S_ERROR_SRDY3)
+#define F_ERROR_SRDY3    V_ERROR_SRDY3(1U)
+
+#define S_ERROR_DRDY3    24
+#define V_ERROR_DRDY3(x) ((x) << S_ERROR_DRDY3)
+#define F_ERROR_DRDY3    V_ERROR_DRDY3(1U)
+
+#define S_DB_VLD3    23
+#define V_DB_VLD3(x) ((x) << S_DB_VLD3)
+#define F_DB_VLD3    V_DB_VLD3(1U)
+
+#define S_DB_GT3    22
+#define V_DB_GT3(x) ((x) << S_DB_GT3)
+#define F_DB_GT3    V_DB_GT3(1U)
+
+#define S_TXVALID3    21
+#define V_TXVALID3(x) ((x) << S_TXVALID3)
+#define F_TXVALID3    V_TXVALID3(1U)
+
+#define S_TXFULL3    20
+#define V_TXFULL3(x) ((x) << S_TXFULL3)
+#define F_TXFULL3    V_TXFULL3(1U)
+
+#define S_PLD_TXVALID3    19
+#define V_PLD_TXVALID3(x) ((x) << S_PLD_TXVALID3)
+#define F_PLD_TXVALID3    V_PLD_TXVALID3(1U)
+
+#define S_PLD_TXFULL3    18
+#define V_PLD_TXFULL3(x) ((x) << S_PLD_TXFULL3)
+#define F_PLD_TXFULL3    V_PLD_TXFULL3(1U)
+
+#define S_CPL5_TXVALID3    17
+#define V_CPL5_TXVALID3(x) ((x) << S_CPL5_TXVALID3)
+#define F_CPL5_TXVALID3    V_CPL5_TXVALID3(1U)
+
+#define S_CPL5_TXFULL3    16
+#define V_CPL5_TXFULL3(x) ((x) << S_CPL5_TXFULL3)
+#define F_CPL5_TXFULL3    V_CPL5_TXFULL3(1U)
+
+#define S_PLD_RXZEROP_SRDY2    15
+#define V_PLD_RXZEROP_SRDY2(x) ((x) << S_PLD_RXZEROP_SRDY2)
+#define F_PLD_RXZEROP_SRDY2    V_PLD_RXZEROP_SRDY2(1U)
+
+#define S_PLD_RXZEROP_DRDY2    14
+#define V_PLD_RXZEROP_DRDY2(x) ((x) << S_PLD_RXZEROP_DRDY2)
+#define F_PLD_RXZEROP_DRDY2    V_PLD_RXZEROP_DRDY2(1U)
+
+#define S_PLD_TXZEROP_SRDY2    13
+#define V_PLD_TXZEROP_SRDY2(x) ((x) << S_PLD_TXZEROP_SRDY2)
+#define F_PLD_TXZEROP_SRDY2    V_PLD_TXZEROP_SRDY2(1U)
+
+#define S_PLD_TXZEROP_DRDY2    12
+#define V_PLD_TXZEROP_DRDY2(x) ((x) << S_PLD_TXZEROP_DRDY2)
+#define F_PLD_TXZEROP_DRDY2    V_PLD_TXZEROP_DRDY2(1U)
+
+#define S_PLD_TX_SRDY2    11
+#define V_PLD_TX_SRDY2(x) ((x) << S_PLD_TX_SRDY2)
+#define F_PLD_TX_SRDY2    V_PLD_TX_SRDY2(1U)
+
+#define S_PLD_TX_DRDY2    10
+#define V_PLD_TX_DRDY2(x) ((x) << S_PLD_TX_DRDY2)
+#define F_PLD_TX_DRDY2    V_PLD_TX_DRDY2(1U)
+
+#define S_ERROR_SRDY2    9
+#define V_ERROR_SRDY2(x) ((x) << S_ERROR_SRDY2)
+#define F_ERROR_SRDY2    V_ERROR_SRDY2(1U)
+
+#define S_ERROR_DRDY2    8
+#define V_ERROR_DRDY2(x) ((x) << S_ERROR_DRDY2)
+#define F_ERROR_DRDY2    V_ERROR_DRDY2(1U)
+
+#define S_DB_VLD2    7
+#define V_DB_VLD2(x) ((x) << S_DB_VLD2)
+#define F_DB_VLD2    V_DB_VLD2(1U)
+
+#define S_DB_GT2    6
+#define V_DB_GT2(x) ((x) << S_DB_GT2)
+#define F_DB_GT2    V_DB_GT2(1U)
+
+#define S_TXVALID2    5
+#define V_TXVALID2(x) ((x) << S_TXVALID2)
+#define F_TXVALID2    V_TXVALID2(1U)
+
+#define S_TXFULL2    4
+#define V_TXFULL2(x) ((x) << S_TXFULL2)
+#define F_TXFULL2    V_TXFULL2(1U)
+
+#define S_PLD_TXVALID2    3
+#define V_PLD_TXVALID2(x) ((x) << S_PLD_TXVALID2)
+#define F_PLD_TXVALID2    V_PLD_TXVALID2(1U)
+
+#define S_PLD_TXFULL2    2
+#define V_PLD_TXFULL2(x) ((x) << S_PLD_TXFULL2)
+#define F_PLD_TXFULL2    V_PLD_TXFULL2(1U)
+
+#define S_CPL5_TXVALID2    1
+#define V_CPL5_TXVALID2(x) ((x) << S_CPL5_TXVALID2)
+#define F_CPL5_TXVALID2    V_CPL5_TXVALID2(1U)
+
+#define S_CPL5_TXFULL2    0
+#define V_CPL5_TXFULL2(x) ((x) << S_CPL5_TXFULL2)
+#define F_CPL5_TXFULL2    V_CPL5_TXFULL2(1U)
+
+#define A_TP_DBG_CSIDE_DISP0 0x23a
+
+#define S_CPL5RXVALID    27
+#define V_CPL5RXVALID(x) ((x) << S_CPL5RXVALID)
+#define F_CPL5RXVALID    V_CPL5RXVALID(1U)
+
+#define S_CSTATIC1    26
+#define V_CSTATIC1(x) ((x) << S_CSTATIC1)
+#define F_CSTATIC1    V_CSTATIC1(1U)
+
+#define S_CSTATIC2    25
+#define V_CSTATIC2(x) ((x) << S_CSTATIC2)
+#define F_CSTATIC2    V_CSTATIC2(1U)
+
+#define S_PLD_RXZEROP    24
+#define V_PLD_RXZEROP(x) ((x) << S_PLD_RXZEROP)
+#define F_PLD_RXZEROP    V_PLD_RXZEROP(1U)
+
+#define S_DDP_IN_PROGRESS    23
+#define V_DDP_IN_PROGRESS(x) ((x) << S_DDP_IN_PROGRESS)
+#define F_DDP_IN_PROGRESS    V_DDP_IN_PROGRESS(1U)
+
+#define S_PLD_RXZEROP_SRDY    22
+#define V_PLD_RXZEROP_SRDY(x) ((x) << S_PLD_RXZEROP_SRDY)
+#define F_PLD_RXZEROP_SRDY    V_PLD_RXZEROP_SRDY(1U)
+
+#define S_CSTATIC3    21
+#define V_CSTATIC3(x) ((x) << S_CSTATIC3)
+#define F_CSTATIC3    V_CSTATIC3(1U)
+
+#define S_DDP_DRDY    20
+#define V_DDP_DRDY(x) ((x) << S_DDP_DRDY)
+#define F_DDP_DRDY    V_DDP_DRDY(1U)
+
+#define S_DDP_PRE_STATE    17
+#define M_DDP_PRE_STATE    0x7U
+#define V_DDP_PRE_STATE(x) ((x) << S_DDP_PRE_STATE)
+#define G_DDP_PRE_STATE(x) (((x) >> S_DDP_PRE_STATE) & M_DDP_PRE_STATE)
+
+#define S_DDP_SRDY    16
+#define V_DDP_SRDY(x) ((x) << S_DDP_SRDY)
+#define F_DDP_SRDY    V_DDP_SRDY(1U)
+
+#define S_DDP_MSG_CODE    12
+#define M_DDP_MSG_CODE    0xfU
+#define V_DDP_MSG_CODE(x) ((x) << S_DDP_MSG_CODE)
+#define G_DDP_MSG_CODE(x) (((x) >> S_DDP_MSG_CODE) & M_DDP_MSG_CODE)
+
+#define S_CPL5_SOCP_CNT    10
+#define M_CPL5_SOCP_CNT    0x3U
+#define V_CPL5_SOCP_CNT(x) ((x) << S_CPL5_SOCP_CNT)
+#define G_CPL5_SOCP_CNT(x) (((x) >> S_CPL5_SOCP_CNT) & M_CPL5_SOCP_CNT)
+
+#define S_CSTATIC4    4
+#define M_CSTATIC4    0x3fU
+#define V_CSTATIC4(x) ((x) << S_CSTATIC4)
+#define G_CSTATIC4(x) (((x) >> S_CSTATIC4) & M_CSTATIC4)
+
+#define S_CMD_SEL    1
+#define V_CMD_SEL(x) ((x) << S_CMD_SEL)
+#define F_CMD_SEL    V_CMD_SEL(1U)
+
+#define A_TP_DBG_CSIDE_DISP1 0x23b
+#define A_TP_DBG_CSIDE_DDP0 0x23c
+
+#define S_DDPMSGLATEST7    28
+#define M_DDPMSGLATEST7    0xfU
+#define V_DDPMSGLATEST7(x) ((x) << S_DDPMSGLATEST7)
+#define G_DDPMSGLATEST7(x) (((x) >> S_DDPMSGLATEST7) & M_DDPMSGLATEST7)
+
+#define S_DDPMSGLATEST6    24
+#define M_DDPMSGLATEST6    0xfU
+#define V_DDPMSGLATEST6(x) ((x) << S_DDPMSGLATEST6)
+#define G_DDPMSGLATEST6(x) (((x) >> S_DDPMSGLATEST6) & M_DDPMSGLATEST6)
+
+#define S_DDPMSGLATEST5    20
+#define M_DDPMSGLATEST5    0xfU
+#define V_DDPMSGLATEST5(x) ((x) << S_DDPMSGLATEST5)
+#define G_DDPMSGLATEST5(x) (((x) >> S_DDPMSGLATEST5) & M_DDPMSGLATEST5)
+
+#define S_DDPMSGLATEST4    16
+#define M_DDPMSGLATEST4    0xfU
+#define V_DDPMSGLATEST4(x) ((x) << S_DDPMSGLATEST4)
+#define G_DDPMSGLATEST4(x) (((x) >> S_DDPMSGLATEST4) & M_DDPMSGLATEST4)
+
+#define S_DDPMSGLATEST3    12
+#define M_DDPMSGLATEST3    0xfU
+#define V_DDPMSGLATEST3(x) ((x) << S_DDPMSGLATEST3)
+#define G_DDPMSGLATEST3(x) (((x) >> S_DDPMSGLATEST3) & M_DDPMSGLATEST3)
+
+#define S_DDPMSGLATEST2    8
+#define M_DDPMSGLATEST2    0xfU
+#define V_DDPMSGLATEST2(x) ((x) << S_DDPMSGLATEST2)
+#define G_DDPMSGLATEST2(x) (((x) >> S_DDPMSGLATEST2) & M_DDPMSGLATEST2)
+
+#define S_DDPMSGLATEST1    4
+#define M_DDPMSGLATEST1    0xfU
+#define V_DDPMSGLATEST1(x) ((x) << S_DDPMSGLATEST1)
+#define G_DDPMSGLATEST1(x) (((x) >> S_DDPMSGLATEST1) & M_DDPMSGLATEST1)
+
+#define S_DDPMSGLATEST0    0
+#define M_DDPMSGLATEST0    0xfU
+#define V_DDPMSGLATEST0(x) ((x) << S_DDPMSGLATEST0)
+#define G_DDPMSGLATEST0(x) (((x) >> S_DDPMSGLATEST0) & M_DDPMSGLATEST0)
+
+#define A_TP_DBG_CSIDE_DDP1 0x23d
+#define A_TP_DBG_CSIDE_FRM 0x23e
+
+#define S_CRX2XERROR    28
+#define M_CRX2XERROR    0xfU
+#define V_CRX2XERROR(x) ((x) << S_CRX2XERROR)
+#define G_CRX2XERROR(x) (((x) >> S_CRX2XERROR) & M_CRX2XERROR)
+
+#define S_CPLDTX2XERROR    24
+#define M_CPLDTX2XERROR    0xfU
+#define V_CPLDTX2XERROR(x) ((x) << S_CPLDTX2XERROR)
+#define G_CPLDTX2XERROR(x) (((x) >> S_CPLDTX2XERROR) & M_CPLDTX2XERROR)
+
+#define S_CTXERROR    22
+#define M_CTXERROR    0x3U
+#define V_CTXERROR(x) ((x) << S_CTXERROR)
+#define G_CTXERROR(x) (((x) >> S_CTXERROR) & M_CTXERROR)
+
+#define S_CPLDRXERROR    20
+#define M_CPLDRXERROR    0x3U
+#define V_CPLDRXERROR(x) ((x) << S_CPLDRXERROR)
+#define G_CPLDRXERROR(x) (((x) >> S_CPLDRXERROR) & M_CPLDRXERROR)
+
+#define S_CPLRXERROR    18
+#define M_CPLRXERROR    0x3U
+#define V_CPLRXERROR(x) ((x) << S_CPLRXERROR)
+#define G_CPLRXERROR(x) (((x) >> S_CPLRXERROR) & M_CPLRXERROR)
+
+#define S_CPLTXERROR    16
+#define M_CPLTXERROR    0x3U
+#define V_CPLTXERROR(x) ((x) << S_CPLTXERROR)
+#define G_CPLTXERROR(x) (((x) >> S_CPLTXERROR) & M_CPLTXERROR)
+
+#define S_CPRSERROR    0
+#define M_CPRSERROR    0xfU
+#define V_CPRSERROR(x) ((x) << S_CPRSERROR)
+#define G_CPRSERROR(x) (((x) >> S_CPRSERROR) & M_CPRSERROR)
+
+#define A_TP_DBG_CSIDE_INT 0x23f
+
+#define S_CRXVALID2X    28
+#define M_CRXVALID2X    0xfU
+#define V_CRXVALID2X(x) ((x) << S_CRXVALID2X)
+#define G_CRXVALID2X(x) (((x) >> S_CRXVALID2X) & M_CRXVALID2X)
+
+#define S_CRXAFULL2X    24
+#define M_CRXAFULL2X    0xfU
+#define V_CRXAFULL2X(x) ((x) << S_CRXAFULL2X)
+#define G_CRXAFULL2X(x) (((x) >> S_CRXAFULL2X) & M_CRXAFULL2X)
+
+#define S_CTXVALID2X    22
+#define M_CTXVALID2X    0x3U
+#define V_CTXVALID2X(x) ((x) << S_CTXVALID2X)
+#define G_CTXVALID2X(x) (((x) >> S_CTXVALID2X) & M_CTXVALID2X)
+
+#define S_CTXAFULL2X    20
+#define M_CTXAFULL2X    0x3U
+#define V_CTXAFULL2X(x) ((x) << S_CTXAFULL2X)
+#define G_CTXAFULL2X(x) (((x) >> S_CTXAFULL2X) & M_CTXAFULL2X)
+
+#define S_PLD2X_RXVALID    18
+#define M_PLD2X_RXVALID    0x3U
+#define V_PLD2X_RXVALID(x) ((x) << S_PLD2X_RXVALID)
+#define G_PLD2X_RXVALID(x) (((x) >> S_PLD2X_RXVALID) & M_PLD2X_RXVALID)
+
+#define S_PLD2X_RXAFULL    16
+#define M_PLD2X_RXAFULL    0x3U
+#define V_PLD2X_RXAFULL(x) ((x) << S_PLD2X_RXAFULL)
+#define G_PLD2X_RXAFULL(x) (((x) >> S_PLD2X_RXAFULL) & M_PLD2X_RXAFULL)
+
+#define S_CSIDE_DDP_VALID    14
+#define M_CSIDE_DDP_VALID    0x3U
+#define V_CSIDE_DDP_VALID(x) ((x) << S_CSIDE_DDP_VALID)
+#define G_CSIDE_DDP_VALID(x) (((x) >> S_CSIDE_DDP_VALID) & M_CSIDE_DDP_VALID)
+
+#define S_DDP_AFULL    12
+#define M_DDP_AFULL    0x3U
+#define V_DDP_AFULL(x) ((x) << S_DDP_AFULL)
+#define G_DDP_AFULL(x) (((x) >> S_DDP_AFULL) & M_DDP_AFULL)
+
+#define S_TRC_RXVALID    11
+#define V_TRC_RXVALID(x) ((x) << S_TRC_RXVALID)
+#define F_TRC_RXVALID    V_TRC_RXVALID(1U)
+
+#define S_TRC_RXFULL    10
+#define V_TRC_RXFULL(x) ((x) << S_TRC_RXFULL)
+#define F_TRC_RXFULL    V_TRC_RXFULL(1U)
+
+#define S_CPL5_TXVALID    9
+#define V_CPL5_TXVALID(x) ((x) << S_CPL5_TXVALID)
+#define F_CPL5_TXVALID    V_CPL5_TXVALID(1U)
+
+#define S_CPL5_TXFULL    8
+#define V_CPL5_TXFULL(x) ((x) << S_CPL5_TXFULL)
+#define F_CPL5_TXFULL    V_CPL5_TXFULL(1U)
+
+#define S_PLD2X_TXVALID    4
+#define M_PLD2X_TXVALID    0xfU
+#define V_PLD2X_TXVALID(x) ((x) << S_PLD2X_TXVALID)
+#define G_PLD2X_TXVALID(x) (((x) >> S_PLD2X_TXVALID) & M_PLD2X_TXVALID)
+
+#define S_PLD2X_TXAFULL    0
+#define M_PLD2X_TXAFULL    0xfU
+#define V_PLD2X_TXAFULL(x) ((x) << S_PLD2X_TXAFULL)
+#define G_PLD2X_TXAFULL(x) (((x) >> S_PLD2X_TXAFULL) & M_PLD2X_TXAFULL)
+
+#define A_TP_CHDR_CONFIG 0x240
+
+#define S_CH1HIGH    24
+#define M_CH1HIGH    0xffU
+#define V_CH1HIGH(x) ((x) << S_CH1HIGH)
+#define G_CH1HIGH(x) (((x) >> S_CH1HIGH) & M_CH1HIGH)
+
+#define S_CH1LOW    16
+#define M_CH1LOW    0xffU
+#define V_CH1LOW(x) ((x) << S_CH1LOW)
+#define G_CH1LOW(x) (((x) >> S_CH1LOW) & M_CH1LOW)
+
+#define S_CH0HIGH    8
+#define M_CH0HIGH    0xffU
+#define V_CH0HIGH(x) ((x) << S_CH0HIGH)
+#define G_CH0HIGH(x) (((x) >> S_CH0HIGH) & M_CH0HIGH)
+
+#define S_CH0LOW    0
+#define M_CH0LOW    0xffU
+#define V_CH0LOW(x) ((x) << S_CH0LOW)
+#define G_CH0LOW(x) (((x) >> S_CH0LOW) & M_CH0LOW)
+
+#define A_TP_UTRN_CONFIG 0x241
+
+#define S_CH2FIFOLIMIT    16
+#define M_CH2FIFOLIMIT    0xffU
+#define V_CH2FIFOLIMIT(x) ((x) << S_CH2FIFOLIMIT)
+#define G_CH2FIFOLIMIT(x) (((x) >> S_CH2FIFOLIMIT) & M_CH2FIFOLIMIT)
+
+#define S_CH1FIFOLIMIT    8
+#define M_CH1FIFOLIMIT    0xffU
+#define V_CH1FIFOLIMIT(x) ((x) << S_CH1FIFOLIMIT)
+#define G_CH1FIFOLIMIT(x) (((x) >> S_CH1FIFOLIMIT) & M_CH1FIFOLIMIT)
+
+#define S_CH0FIFOLIMIT    0
+#define M_CH0FIFOLIMIT    0xffU
+#define V_CH0FIFOLIMIT(x) ((x) << S_CH0FIFOLIMIT)
+#define G_CH0FIFOLIMIT(x) (((x) >> S_CH0FIFOLIMIT) & M_CH0FIFOLIMIT)
+
+#define A_TP_CDSP_CONFIG 0x242
+
+#define S_WRITEZEROEN    4
+#define V_WRITEZEROEN(x) ((x) << S_WRITEZEROEN)
+#define F_WRITEZEROEN    V_WRITEZEROEN(1U)
+
+#define S_WRITEZEROOP    0
+#define M_WRITEZEROOP    0xfU
+#define V_WRITEZEROOP(x) ((x) << S_WRITEZEROOP)
+#define G_WRITEZEROOP(x) (((x) >> S_WRITEZEROOP) & M_WRITEZEROOP)
+
+#define A_TP_TRC_CONFIG 0x244
+
+#define S_TRCRR    1
+#define V_TRCRR(x) ((x) << S_TRCRR)
+#define F_TRCRR    V_TRCRR(1U)
+
+#define S_TRCCH    0
+#define V_TRCCH(x) ((x) << S_TRCCH)
+#define F_TRCCH    V_TRCCH(1U)
+
+#define A_TP_TAG_CONFIG 0x245
+
+#define S_ETAGTYPE    16
+#define M_ETAGTYPE    0xffffU
+#define V_ETAGTYPE(x) ((x) << S_ETAGTYPE)
+#define G_ETAGTYPE(x) (((x) >> S_ETAGTYPE) & M_ETAGTYPE)
+
+#define A_TP_DBG_CSIDE_PRS 0x246
+
+#define S_CPRSSTATE3    24
+#define M_CPRSSTATE3    0x7U
+#define V_CPRSSTATE3(x) ((x) << S_CPRSSTATE3)
+#define G_CPRSSTATE3(x) (((x) >> S_CPRSSTATE3) & M_CPRSSTATE3)
+
+#define S_CPRSSTATE2    16
+#define M_CPRSSTATE2    0x7U
+#define V_CPRSSTATE2(x) ((x) << S_CPRSSTATE2)
+#define G_CPRSSTATE2(x) (((x) >> S_CPRSSTATE2) & M_CPRSSTATE2)
+
+#define S_CPRSSTATE1    8
+#define M_CPRSSTATE1    0x7U
+#define V_CPRSSTATE1(x) ((x) << S_CPRSSTATE1)
+#define G_CPRSSTATE1(x) (((x) >> S_CPRSSTATE1) & M_CPRSSTATE1)
+
+#define S_CPRSSTATE0    0
+#define M_CPRSSTATE0    0x7U
+#define V_CPRSSTATE0(x) ((x) << S_CPRSSTATE0)
+#define G_CPRSSTATE0(x) (((x) >> S_CPRSSTATE0) & M_CPRSSTATE0)
+
+#define A_TP_DBG_CSIDE_DEMUX 0x247
+
+#define S_CALLDONE    28
+#define M_CALLDONE    0xfU
+#define V_CALLDONE(x) ((x) << S_CALLDONE)
+#define G_CALLDONE(x) (((x) >> S_CALLDONE) & M_CALLDONE)
+
+#define S_CTCPL5DONE    24
+#define M_CTCPL5DONE    0xfU
+#define V_CTCPL5DONE(x) ((x) << S_CTCPL5DONE)
+#define G_CTCPL5DONE(x) (((x) >> S_CTCPL5DONE) & M_CTCPL5DONE)
+
+#define S_CTXZEROPDONE    20
+#define M_CTXZEROPDONE    0xfU
+#define V_CTXZEROPDONE(x) ((x) << S_CTXZEROPDONE)
+#define G_CTXZEROPDONE(x) (((x) >> S_CTXZEROPDONE) & M_CTXZEROPDONE)
+
+#define S_CPLDDONE    16
+#define M_CPLDDONE    0xfU
+#define V_CPLDDONE(x) ((x) << S_CPLDDONE)
+#define G_CPLDDONE(x) (((x) >> S_CPLDDONE) & M_CPLDDONE)
+
+#define S_CTTCPOPDONE    12
+#define M_CTTCPOPDONE    0xfU
+#define V_CTTCPOPDONE(x) ((x) << S_CTTCPOPDONE)
+#define G_CTTCPOPDONE(x) (((x) >> S_CTTCPOPDONE) & M_CTTCPOPDONE)
+
+#define S_CDBDONE    8
+#define M_CDBDONE    0xfU
+#define V_CDBDONE(x) ((x) << S_CDBDONE)
+#define G_CDBDONE(x) (((x) >> S_CDBDONE) & M_CDBDONE)
+
+#define S_CISSFIFODONE    4
+#define M_CISSFIFODONE    0xfU
+#define V_CISSFIFODONE(x) ((x) << S_CISSFIFODONE)
+#define G_CISSFIFODONE(x) (((x) >> S_CISSFIFODONE) & M_CISSFIFODONE)
+
+#define S_CTXPKTCSUMDONE    0
+#define M_CTXPKTCSUMDONE    0xfU
+#define V_CTXPKTCSUMDONE(x) ((x) << S_CTXPKTCSUMDONE)
+#define G_CTXPKTCSUMDONE(x) (((x) >> S_CTXPKTCSUMDONE) & M_CTXPKTCSUMDONE)
+
+#define A_TP_FIFO_CONFIG 0x8c0
+
+#define S_CH1_OUTPUT    27
+#define M_CH1_OUTPUT    0x1fU
+#define V_CH1_OUTPUT(x) ((x) << S_CH1_OUTPUT)
+#define G_CH1_OUTPUT(x) (((x) >> S_CH1_OUTPUT) & M_CH1_OUTPUT)
+
+#define S_CH2_OUTPUT    22
+#define M_CH2_OUTPUT    0x1fU
+#define V_CH2_OUTPUT(x) ((x) << S_CH2_OUTPUT)
+#define G_CH2_OUTPUT(x) (((x) >> S_CH2_OUTPUT) & M_CH2_OUTPUT)
+
+#define S_STROBE1    16
+#define V_STROBE1(x) ((x) << S_STROBE1)
+#define F_STROBE1    V_STROBE1(1U)
+
+#define S_CH1_INPUT    11
+#define M_CH1_INPUT    0x1fU
+#define V_CH1_INPUT(x) ((x) << S_CH1_INPUT)
+#define G_CH1_INPUT(x) (((x) >> S_CH1_INPUT) & M_CH1_INPUT)
+
+#define S_CH2_INPUT    6
+#define M_CH2_INPUT    0x1fU
+#define V_CH2_INPUT(x) ((x) << S_CH2_INPUT)
+#define G_CH2_INPUT(x) (((x) >> S_CH2_INPUT) & M_CH2_INPUT)
+
+#define S_CH3_INPUT    1
+#define M_CH3_INPUT    0x1fU
+#define V_CH3_INPUT(x) ((x) << S_CH3_INPUT)
+#define G_CH3_INPUT(x) (((x) >> S_CH3_INPUT) & M_CH3_INPUT)
+
+#define S_STROBE0    0
+#define V_STROBE0(x) ((x) << S_STROBE0)
+#define F_STROBE0    V_STROBE0(1U)
+
+#define A_TP_MIB_MAC_IN_ERR_0 0x0
+#define A_TP_MIB_MAC_IN_ERR_1 0x1
+#define A_TP_MIB_MAC_IN_ERR_2 0x2
+#define A_TP_MIB_MAC_IN_ERR_3 0x3
+#define A_TP_MIB_HDR_IN_ERR_0 0x4
+#define A_TP_MIB_HDR_IN_ERR_1 0x5
+#define A_TP_MIB_HDR_IN_ERR_2 0x6
+#define A_TP_MIB_HDR_IN_ERR_3 0x7
+#define A_TP_MIB_TCP_IN_ERR_0 0x8
+#define A_TP_MIB_TCP_IN_ERR_1 0x9
+#define A_TP_MIB_TCP_IN_ERR_2 0xa
+#define A_TP_MIB_TCP_IN_ERR_3 0xb
+#define A_TP_MIB_TCP_OUT_RST 0xc
+#define A_TP_MIB_TCP_IN_SEG_HI 0x10
+#define A_TP_MIB_TCP_IN_SEG_LO 0x11
+#define A_TP_MIB_TCP_OUT_SEG_HI 0x12
+#define A_TP_MIB_TCP_OUT_SEG_LO 0x13
+#define A_TP_MIB_TCP_RXT_SEG_HI 0x14
+#define A_TP_MIB_TCP_RXT_SEG_LO 0x15
+#define A_TP_MIB_TNL_CNG_DROP_0 0x18
+#define A_TP_MIB_TNL_CNG_DROP_1 0x19
+#define A_TP_MIB_TNL_CNG_DROP_2 0x1a
+#define A_TP_MIB_TNL_CNG_DROP_3 0x1b
+#define A_TP_MIB_OFD_CHN_DROP_0 0x1c
+#define A_TP_MIB_OFD_CHN_DROP_1 0x1d
+#define A_TP_MIB_OFD_CHN_DROP_2 0x1e
+#define A_TP_MIB_OFD_CHN_DROP_3 0x1f
+#define A_TP_MIB_TNL_OUT_PKT_0 0x20
+#define A_TP_MIB_TNL_OUT_PKT_1 0x21
+#define A_TP_MIB_TNL_OUT_PKT_2 0x22
+#define A_TP_MIB_TNL_OUT_PKT_3 0x23
+#define A_TP_MIB_TNL_IN_PKT_0 0x24
+#define A_TP_MIB_TNL_IN_PKT_1 0x25
+#define A_TP_MIB_TNL_IN_PKT_2 0x26
+#define A_TP_MIB_TNL_IN_PKT_3 0x27
+#define A_TP_MIB_TCP_V6IN_ERR_0 0x28
+#define A_TP_MIB_TCP_V6IN_ERR_1 0x29
+#define A_TP_MIB_TCP_V6IN_ERR_2 0x2a
+#define A_TP_MIB_TCP_V6IN_ERR_3 0x2b
+#define A_TP_MIB_TCP_V6OUT_RST 0x2c
+#define A_TP_MIB_TCP_V6IN_SEG_HI 0x30
+#define A_TP_MIB_TCP_V6IN_SEG_LO 0x31
+#define A_TP_MIB_TCP_V6OUT_SEG_HI 0x32
+#define A_TP_MIB_TCP_V6OUT_SEG_LO 0x33
+#define A_TP_MIB_TCP_V6RXT_SEG_HI 0x34
+#define A_TP_MIB_TCP_V6RXT_SEG_LO 0x35
+#define A_TP_MIB_OFD_ARP_DROP 0x36
+#define A_TP_MIB_OFD_DFR_DROP 0x37
+#define A_TP_MIB_CPL_IN_REQ_0 0x38
+#define A_TP_MIB_CPL_IN_REQ_1 0x39
+#define A_TP_MIB_CPL_IN_REQ_2 0x3a
+#define A_TP_MIB_CPL_IN_REQ_3 0x3b
+#define A_TP_MIB_CPL_OUT_RSP_0 0x3c
+#define A_TP_MIB_CPL_OUT_RSP_1 0x3d
+#define A_TP_MIB_CPL_OUT_RSP_2 0x3e
+#define A_TP_MIB_CPL_OUT_RSP_3 0x3f
+#define A_TP_MIB_TNL_LPBK_0 0x40
+#define A_TP_MIB_TNL_LPBK_1 0x41
+#define A_TP_MIB_TNL_LPBK_2 0x42
+#define A_TP_MIB_TNL_LPBK_3 0x43
+#define A_TP_MIB_TNL_DROP_0 0x44
+#define A_TP_MIB_TNL_DROP_1 0x45
+#define A_TP_MIB_TNL_DROP_2 0x46
+#define A_TP_MIB_TNL_DROP_3 0x47
+#define A_TP_MIB_FCOE_DDP_0 0x48
+#define A_TP_MIB_FCOE_DDP_1 0x49
+#define A_TP_MIB_FCOE_DDP_2 0x4a
+#define A_TP_MIB_FCOE_DDP_3 0x4b
+#define A_TP_MIB_FCOE_DROP_0 0x4c
+#define A_TP_MIB_FCOE_DROP_1 0x4d
+#define A_TP_MIB_FCOE_DROP_2 0x4e
+#define A_TP_MIB_FCOE_DROP_3 0x4f
+#define A_TP_MIB_FCOE_BYTE_0_HI 0x50
+#define A_TP_MIB_FCOE_BYTE_0_LO 0x51
+#define A_TP_MIB_FCOE_BYTE_1_HI 0x52
+#define A_TP_MIB_FCOE_BYTE_1_LO 0x53
+#define A_TP_MIB_FCOE_BYTE_2_HI 0x54
+#define A_TP_MIB_FCOE_BYTE_2_LO 0x55
+#define A_TP_MIB_FCOE_BYTE_3_HI 0x56
+#define A_TP_MIB_FCOE_BYTE_3_LO 0x57
+#define A_TP_MIB_OFD_VLN_DROP_0 0x58
+#define A_TP_MIB_OFD_VLN_DROP_1 0x59
+#define A_TP_MIB_OFD_VLN_DROP_2 0x5a
+#define A_TP_MIB_OFD_VLN_DROP_3 0x5b
+#define A_TP_MIB_USM_PKTS 0x5c
+#define A_TP_MIB_USM_DROP 0x5d
+#define A_TP_MIB_USM_BYTES_HI 0x5e
+#define A_TP_MIB_USM_BYTES_LO 0x5f
+#define A_TP_MIB_TID_DEL 0x60
+#define A_TP_MIB_TID_INV 0x61
+#define A_TP_MIB_TID_ACT 0x62
+#define A_TP_MIB_TID_PAS 0x63
+#define A_TP_MIB_RQE_DFR_MOD 0x64
+#define A_TP_MIB_RQE_DFR_PKT 0x65
+#define A_TP_MIB_CPL_OUT_ERR_0 0x68
+#define A_TP_MIB_CPL_OUT_ERR_1 0x69
+#define A_TP_MIB_CPL_OUT_ERR_2 0x6a
+#define A_TP_MIB_CPL_OUT_ERR_3 0x6b
+
+/* registers for module ULP_TX */
+#define ULP_TX_BASE_ADDR 0x8dc0
+
+#define A_ULP_TX_CONFIG 0x8dc0
+
+#define S_STAG_MIX_ENABLE    2
+#define V_STAG_MIX_ENABLE(x) ((x) << S_STAG_MIX_ENABLE)
+#define F_STAG_MIX_ENABLE    V_STAG_MIX_ENABLE(1U)
+
+#define S_STAGF_FIX_DISABLE    1
+#define V_STAGF_FIX_DISABLE(x) ((x) << S_STAGF_FIX_DISABLE)
+#define F_STAGF_FIX_DISABLE    V_STAGF_FIX_DISABLE(1U)
+
+#define S_EXTRA_TAG_INSERTION_ENABLE    0
+#define V_EXTRA_TAG_INSERTION_ENABLE(x) ((x) << S_EXTRA_TAG_INSERTION_ENABLE)
+#define F_EXTRA_TAG_INSERTION_ENABLE    V_EXTRA_TAG_INSERTION_ENABLE(1U)
+
+#define A_ULP_TX_PERR_INJECT 0x8dc4
+#define A_ULP_TX_INT_ENABLE 0x8dc8
+
+#define S_PBL_BOUND_ERR_CH3    31
+#define V_PBL_BOUND_ERR_CH3(x) ((x) << S_PBL_BOUND_ERR_CH3)
+#define F_PBL_BOUND_ERR_CH3    V_PBL_BOUND_ERR_CH3(1U)
+
+#define S_PBL_BOUND_ERR_CH2    30
+#define V_PBL_BOUND_ERR_CH2(x) ((x) << S_PBL_BOUND_ERR_CH2)
+#define F_PBL_BOUND_ERR_CH2    V_PBL_BOUND_ERR_CH2(1U)
+
+#define S_PBL_BOUND_ERR_CH1    29
+#define V_PBL_BOUND_ERR_CH1(x) ((x) << S_PBL_BOUND_ERR_CH1)
+#define F_PBL_BOUND_ERR_CH1    V_PBL_BOUND_ERR_CH1(1U)
+
+#define S_PBL_BOUND_ERR_CH0    28
+#define V_PBL_BOUND_ERR_CH0(x) ((x) << S_PBL_BOUND_ERR_CH0)
+#define F_PBL_BOUND_ERR_CH0    V_PBL_BOUND_ERR_CH0(1U)
+
+#define S_SGE2ULP_FIFO_PERR_SET3    27
+#define V_SGE2ULP_FIFO_PERR_SET3(x) ((x) << S_SGE2ULP_FIFO_PERR_SET3)
+#define F_SGE2ULP_FIFO_PERR_SET3    V_SGE2ULP_FIFO_PERR_SET3(1U)
+
+#define S_SGE2ULP_FIFO_PERR_SET2    26
+#define V_SGE2ULP_FIFO_PERR_SET2(x) ((x) << S_SGE2ULP_FIFO_PERR_SET2)
+#define F_SGE2ULP_FIFO_PERR_SET2    V_SGE2ULP_FIFO_PERR_SET2(1U)
+
+#define S_SGE2ULP_FIFO_PERR_SET1    25
+#define V_SGE2ULP_FIFO_PERR_SET1(x) ((x) << S_SGE2ULP_FIFO_PERR_SET1)
+#define F_SGE2ULP_FIFO_PERR_SET1    V_SGE2ULP_FIFO_PERR_SET1(1U)
+
+#define S_SGE2ULP_FIFO_PERR_SET0    24
+#define V_SGE2ULP_FIFO_PERR_SET0(x) ((x) << S_SGE2ULP_FIFO_PERR_SET0)
+#define F_SGE2ULP_FIFO_PERR_SET0    V_SGE2ULP_FIFO_PERR_SET0(1U)
+
+#define S_CIM2ULP_FIFO_PERR_SET3    23
+#define V_CIM2ULP_FIFO_PERR_SET3(x) ((x) << S_CIM2ULP_FIFO_PERR_SET3)
+#define F_CIM2ULP_FIFO_PERR_SET3    V_CIM2ULP_FIFO_PERR_SET3(1U)
+
+#define S_CIM2ULP_FIFO_PERR_SET2    22
+#define V_CIM2ULP_FIFO_PERR_SET2(x) ((x) << S_CIM2ULP_FIFO_PERR_SET2)
+#define F_CIM2ULP_FIFO_PERR_SET2    V_CIM2ULP_FIFO_PERR_SET2(1U)
+
+#define S_CIM2ULP_FIFO_PERR_SET1    21
+#define V_CIM2ULP_FIFO_PERR_SET1(x) ((x) << S_CIM2ULP_FIFO_PERR_SET1)
+#define F_CIM2ULP_FIFO_PERR_SET1    V_CIM2ULP_FIFO_PERR_SET1(1U)
+
+#define S_CIM2ULP_FIFO_PERR_SET0    20
+#define V_CIM2ULP_FIFO_PERR_SET0(x) ((x) << S_CIM2ULP_FIFO_PERR_SET0)
+#define F_CIM2ULP_FIFO_PERR_SET0    V_CIM2ULP_FIFO_PERR_SET0(1U)
+
+#define S_CQE_FIFO_PERR_SET3    19
+#define V_CQE_FIFO_PERR_SET3(x) ((x) << S_CQE_FIFO_PERR_SET3)
+#define F_CQE_FIFO_PERR_SET3    V_CQE_FIFO_PERR_SET3(1U)
+
+#define S_CQE_FIFO_PERR_SET2    18
+#define V_CQE_FIFO_PERR_SET2(x) ((x) << S_CQE_FIFO_PERR_SET2)
+#define F_CQE_FIFO_PERR_SET2    V_CQE_FIFO_PERR_SET2(1U)
+
+#define S_CQE_FIFO_PERR_SET1    17
+#define V_CQE_FIFO_PERR_SET1(x) ((x) << S_CQE_FIFO_PERR_SET1)
+#define F_CQE_FIFO_PERR_SET1    V_CQE_FIFO_PERR_SET1(1U)
+
+#define S_CQE_FIFO_PERR_SET0    16
+#define V_CQE_FIFO_PERR_SET0(x) ((x) << S_CQE_FIFO_PERR_SET0)
+#define F_CQE_FIFO_PERR_SET0    V_CQE_FIFO_PERR_SET0(1U)
+
+#define S_PBL_FIFO_PERR_SET3    15
+#define V_PBL_FIFO_PERR_SET3(x) ((x) << S_PBL_FIFO_PERR_SET3)
+#define F_PBL_FIFO_PERR_SET3    V_PBL_FIFO_PERR_SET3(1U)
+
+#define S_PBL_FIFO_PERR_SET2    14
+#define V_PBL_FIFO_PERR_SET2(x) ((x) << S_PBL_FIFO_PERR_SET2)
+#define F_PBL_FIFO_PERR_SET2    V_PBL_FIFO_PERR_SET2(1U)
+
+#define S_PBL_FIFO_PERR_SET1    13
+#define V_PBL_FIFO_PERR_SET1(x) ((x) << S_PBL_FIFO_PERR_SET1)
+#define F_PBL_FIFO_PERR_SET1    V_PBL_FIFO_PERR_SET1(1U)
+
+#define S_PBL_FIFO_PERR_SET0    12
+#define V_PBL_FIFO_PERR_SET0(x) ((x) << S_PBL_FIFO_PERR_SET0)
+#define F_PBL_FIFO_PERR_SET0    V_PBL_FIFO_PERR_SET0(1U)
+
+#define S_CMD_FIFO_PERR_SET3    11
+#define V_CMD_FIFO_PERR_SET3(x) ((x) << S_CMD_FIFO_PERR_SET3)
+#define F_CMD_FIFO_PERR_SET3    V_CMD_FIFO_PERR_SET3(1U)
+
+#define S_CMD_FIFO_PERR_SET2    10
+#define V_CMD_FIFO_PERR_SET2(x) ((x) << S_CMD_FIFO_PERR_SET2)
+#define F_CMD_FIFO_PERR_SET2    V_CMD_FIFO_PERR_SET2(1U)
+
+#define S_CMD_FIFO_PERR_SET1    9
+#define V_CMD_FIFO_PERR_SET1(x) ((x) << S_CMD_FIFO_PERR_SET1)
+#define F_CMD_FIFO_PERR_SET1    V_CMD_FIFO_PERR_SET1(1U)
+
+#define S_CMD_FIFO_PERR_SET0    8
+#define V_CMD_FIFO_PERR_SET0(x) ((x) << S_CMD_FIFO_PERR_SET0)
+#define F_CMD_FIFO_PERR_SET0    V_CMD_FIFO_PERR_SET0(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET3    7
+#define V_LSO_HDR_SRAM_PERR_SET3(x) ((x) << S_LSO_HDR_SRAM_PERR_SET3)
+#define F_LSO_HDR_SRAM_PERR_SET3    V_LSO_HDR_SRAM_PERR_SET3(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET2    6
+#define V_LSO_HDR_SRAM_PERR_SET2(x) ((x) << S_LSO_HDR_SRAM_PERR_SET2)
+#define F_LSO_HDR_SRAM_PERR_SET2    V_LSO_HDR_SRAM_PERR_SET2(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET1    5
+#define V_LSO_HDR_SRAM_PERR_SET1(x) ((x) << S_LSO_HDR_SRAM_PERR_SET1)
+#define F_LSO_HDR_SRAM_PERR_SET1    V_LSO_HDR_SRAM_PERR_SET1(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET0    4
+#define V_LSO_HDR_SRAM_PERR_SET0(x) ((x) << S_LSO_HDR_SRAM_PERR_SET0)
+#define F_LSO_HDR_SRAM_PERR_SET0    V_LSO_HDR_SRAM_PERR_SET0(1U)
+
+#define S_IMM_DATA_PERR_SET_CH3    3
+#define V_IMM_DATA_PERR_SET_CH3(x) ((x) << S_IMM_DATA_PERR_SET_CH3)
+#define F_IMM_DATA_PERR_SET_CH3    V_IMM_DATA_PERR_SET_CH3(1U)
+
+#define S_IMM_DATA_PERR_SET_CH2    2
+#define V_IMM_DATA_PERR_SET_CH2(x) ((x) << S_IMM_DATA_PERR_SET_CH2)
+#define F_IMM_DATA_PERR_SET_CH2    V_IMM_DATA_PERR_SET_CH2(1U)
+
+#define S_IMM_DATA_PERR_SET_CH1    1
+#define V_IMM_DATA_PERR_SET_CH1(x) ((x) << S_IMM_DATA_PERR_SET_CH1)
+#define F_IMM_DATA_PERR_SET_CH1    V_IMM_DATA_PERR_SET_CH1(1U)
+
+#define S_IMM_DATA_PERR_SET_CH0    0
+#define V_IMM_DATA_PERR_SET_CH0(x) ((x) << S_IMM_DATA_PERR_SET_CH0)
+#define F_IMM_DATA_PERR_SET_CH0    V_IMM_DATA_PERR_SET_CH0(1U)
+
+#define A_ULP_TX_INT_CAUSE 0x8dcc
+#define A_ULP_TX_PERR_ENABLE 0x8dd0
+#define A_ULP_TX_TPT_LLIMIT 0x8dd4
+#define A_ULP_TX_TPT_ULIMIT 0x8dd8
+#define A_ULP_TX_PBL_LLIMIT 0x8ddc
+#define A_ULP_TX_PBL_ULIMIT 0x8de0
+#define A_ULP_TX_CPL_ERR_OFFSET 0x8de4
+#define A_ULP_TX_CPL_ERR_MASK_L 0x8de8
+#define A_ULP_TX_CPL_ERR_MASK_H 0x8dec
+#define A_ULP_TX_CPL_ERR_VALUE_L 0x8df0
+#define A_ULP_TX_CPL_ERR_VALUE_H 0x8df4
+#define A_ULP_TX_CPL_PACK_SIZE1 0x8df8
+
+#define S_CH3SIZE1    24
+#define M_CH3SIZE1    0xffU
+#define V_CH3SIZE1(x) ((x) << S_CH3SIZE1)
+#define G_CH3SIZE1(x) (((x) >> S_CH3SIZE1) & M_CH3SIZE1)
+
+#define S_CH2SIZE1    16
+#define M_CH2SIZE1    0xffU
+#define V_CH2SIZE1(x) ((x) << S_CH2SIZE1)
+#define G_CH2SIZE1(x) (((x) >> S_CH2SIZE1) & M_CH2SIZE1)
+
+#define S_CH1SIZE1    8
+#define M_CH1SIZE1    0xffU
+#define V_CH1SIZE1(x) ((x) << S_CH1SIZE1)
+#define G_CH1SIZE1(x) (((x) >> S_CH1SIZE1) & M_CH1SIZE1)
+
+#define S_CH0SIZE1    0
+#define M_CH0SIZE1    0xffU
+#define V_CH0SIZE1(x) ((x) << S_CH0SIZE1)
+#define G_CH0SIZE1(x) (((x) >> S_CH0SIZE1) & M_CH0SIZE1)
+
+#define A_ULP_TX_CPL_PACK_SIZE2 0x8dfc
+
+#define S_CH3SIZE2    24
+#define M_CH3SIZE2    0xffU
+#define V_CH3SIZE2(x) ((x) << S_CH3SIZE2)
+#define G_CH3SIZE2(x) (((x) >> S_CH3SIZE2) & M_CH3SIZE2)
+
+#define S_CH2SIZE2    16
+#define M_CH2SIZE2    0xffU
+#define V_CH2SIZE2(x) ((x) << S_CH2SIZE2)
+#define G_CH2SIZE2(x) (((x) >> S_CH2SIZE2) & M_CH2SIZE2)
+
+#define S_CH1SIZE2    8
+#define M_CH1SIZE2    0xffU
+#define V_CH1SIZE2(x) ((x) << S_CH1SIZE2)
+#define G_CH1SIZE2(x) (((x) >> S_CH1SIZE2) & M_CH1SIZE2)
+
+#define S_CH0SIZE2    0
+#define M_CH0SIZE2    0xffU
+#define V_CH0SIZE2(x) ((x) << S_CH0SIZE2)
+#define G_CH0SIZE2(x) (((x) >> S_CH0SIZE2) & M_CH0SIZE2)
+
+#define A_ULP_TX_ERR_MSG2CIM 0x8e00
+#define A_ULP_TX_ERR_TABLE_BASE 0x8e04
+#define A_ULP_TX_ERR_CNT_CH0 0x8e10
+
+#define S_ERR_CNT0    0
+#define M_ERR_CNT0    0xfffffU
+#define V_ERR_CNT0(x) ((x) << S_ERR_CNT0)
+#define G_ERR_CNT0(x) (((x) >> S_ERR_CNT0) & M_ERR_CNT0)
+
+#define A_ULP_TX_ERR_CNT_CH1 0x8e14
+
+#define S_ERR_CNT1    0
+#define M_ERR_CNT1    0xfffffU
+#define V_ERR_CNT1(x) ((x) << S_ERR_CNT1)
+#define G_ERR_CNT1(x) (((x) >> S_ERR_CNT1) & M_ERR_CNT1)
+
+#define A_ULP_TX_ERR_CNT_CH2 0x8e18
+
+#define S_ERR_CNT2    0
+#define M_ERR_CNT2    0xfffffU
+#define V_ERR_CNT2(x) ((x) << S_ERR_CNT2)
+#define G_ERR_CNT2(x) (((x) >> S_ERR_CNT2) & M_ERR_CNT2)
+
+#define A_ULP_TX_ERR_CNT_CH3 0x8e1c
+
+#define S_ERR_CNT3    0
+#define M_ERR_CNT3    0xfffffU
+#define V_ERR_CNT3(x) ((x) << S_ERR_CNT3)
+#define G_ERR_CNT3(x) (((x) >> S_ERR_CNT3) & M_ERR_CNT3)
+
+#define A_ULP_TX_ULP2TP_BIST_CMD 0x8e30
+#define A_ULP_TX_ULP2TP_BIST_ERROR_CNT 0x8e34
+#define A_ULP_TX_FPGA_CMD_CTRL 0x8e38
+#define A_ULP_TX_FPGA_CMD_0 0x8e3c
+#define A_ULP_TX_FPGA_CMD_1 0x8e40
+#define A_ULP_TX_FPGA_CMD_2 0x8e44
+#define A_ULP_TX_FPGA_CMD_3 0x8e48
+#define A_ULP_TX_FPGA_CMD_4 0x8e4c
+#define A_ULP_TX_FPGA_CMD_5 0x8e50
+#define A_ULP_TX_FPGA_CMD_6 0x8e54
+#define A_ULP_TX_FPGA_CMD_7 0x8e58
+#define A_ULP_TX_FPGA_CMD_8 0x8e5c
+#define A_ULP_TX_FPGA_CMD_9 0x8e60
+#define A_ULP_TX_FPGA_CMD_10 0x8e64
+#define A_ULP_TX_FPGA_CMD_11 0x8e68
+#define A_ULP_TX_FPGA_CMD_12 0x8e6c
+#define A_ULP_TX_FPGA_CMD_13 0x8e70
+#define A_ULP_TX_FPGA_CMD_14 0x8e74
+#define A_ULP_TX_FPGA_CMD_15 0x8e78
+#define A_ULP_TX_SE_CNT_ERR 0x8ea0
+
+#define S_ERR_CH3    12
+#define M_ERR_CH3    0xfU
+#define V_ERR_CH3(x) ((x) << S_ERR_CH3)
+#define G_ERR_CH3(x) (((x) >> S_ERR_CH3) & M_ERR_CH3)
+
+#define S_ERR_CH2    8
+#define M_ERR_CH2    0xfU
+#define V_ERR_CH2(x) ((x) << S_ERR_CH2)
+#define G_ERR_CH2(x) (((x) >> S_ERR_CH2) & M_ERR_CH2)
+
+#define S_ERR_CH1    4
+#define M_ERR_CH1    0xfU
+#define V_ERR_CH1(x) ((x) << S_ERR_CH1)
+#define G_ERR_CH1(x) (((x) >> S_ERR_CH1) & M_ERR_CH1)
+
+#define S_ERR_CH0    0
+#define M_ERR_CH0    0xfU
+#define V_ERR_CH0(x) ((x) << S_ERR_CH0)
+#define G_ERR_CH0(x) (((x) >> S_ERR_CH0) & M_ERR_CH0)
+
+#define A_ULP_TX_SE_CNT_CLR 0x8ea4
+
+#define S_CLR_DROP    16
+#define M_CLR_DROP    0xfU
+#define V_CLR_DROP(x) ((x) << S_CLR_DROP)
+#define G_CLR_DROP(x) (((x) >> S_CLR_DROP) & M_CLR_DROP)
+
+#define S_CLR_CH3    12
+#define M_CLR_CH3    0xfU
+#define V_CLR_CH3(x) ((x) << S_CLR_CH3)
+#define G_CLR_CH3(x) (((x) >> S_CLR_CH3) & M_CLR_CH3)
+
+#define S_CLR_CH2    8
+#define M_CLR_CH2    0xfU
+#define V_CLR_CH2(x) ((x) << S_CLR_CH2)
+#define G_CLR_CH2(x) (((x) >> S_CLR_CH2) & M_CLR_CH2)
+
+#define S_CLR_CH1    4
+#define M_CLR_CH1    0xfU
+#define V_CLR_CH1(x) ((x) << S_CLR_CH1)
+#define G_CLR_CH1(x) (((x) >> S_CLR_CH1) & M_CLR_CH1)
+
+#define S_CLR_CH0    0
+#define M_CLR_CH0    0xfU
+#define V_CLR_CH0(x) ((x) << S_CLR_CH0)
+#define G_CLR_CH0(x) (((x) >> S_CLR_CH0) & M_CLR_CH0)
+
+#define A_ULP_TX_SE_CNT_CH0 0x8ea8
+
+#define S_SOP_CNT_ULP2TP    28
+#define M_SOP_CNT_ULP2TP    0xfU
+#define V_SOP_CNT_ULP2TP(x) ((x) << S_SOP_CNT_ULP2TP)
+#define G_SOP_CNT_ULP2TP(x) (((x) >> S_SOP_CNT_ULP2TP) & M_SOP_CNT_ULP2TP)
+
+#define S_EOP_CNT_ULP2TP    24
+#define M_EOP_CNT_ULP2TP    0xfU
+#define V_EOP_CNT_ULP2TP(x) ((x) << S_EOP_CNT_ULP2TP)
+#define G_EOP_CNT_ULP2TP(x) (((x) >> S_EOP_CNT_ULP2TP) & M_EOP_CNT_ULP2TP)
+
+#define S_SOP_CNT_LSO_IN    20
+#define M_SOP_CNT_LSO_IN    0xfU
+#define V_SOP_CNT_LSO_IN(x) ((x) << S_SOP_CNT_LSO_IN)
+#define G_SOP_CNT_LSO_IN(x) (((x) >> S_SOP_CNT_LSO_IN) & M_SOP_CNT_LSO_IN)
+
+#define S_EOP_CNT_LSO_IN    16
+#define M_EOP_CNT_LSO_IN    0xfU
+#define V_EOP_CNT_LSO_IN(x) ((x) << S_EOP_CNT_LSO_IN)
+#define G_EOP_CNT_LSO_IN(x) (((x) >> S_EOP_CNT_LSO_IN) & M_EOP_CNT_LSO_IN)
+
+#define S_SOP_CNT_ALG_IN    12
+#define M_SOP_CNT_ALG_IN    0xfU
+#define V_SOP_CNT_ALG_IN(x) ((x) << S_SOP_CNT_ALG_IN)
+#define G_SOP_CNT_ALG_IN(x) (((x) >> S_SOP_CNT_ALG_IN) & M_SOP_CNT_ALG_IN)
+
+#define S_EOP_CNT_ALG_IN    8
+#define M_EOP_CNT_ALG_IN    0xfU
+#define V_EOP_CNT_ALG_IN(x) ((x) << S_EOP_CNT_ALG_IN)
+#define G_EOP_CNT_ALG_IN(x) (((x) >> S_EOP_CNT_ALG_IN) & M_EOP_CNT_ALG_IN)
+
+#define S_SOP_CNT_CIM2ULP    4
+#define M_SOP_CNT_CIM2ULP    0xfU
+#define V_SOP_CNT_CIM2ULP(x) ((x) << S_SOP_CNT_CIM2ULP)
+#define G_SOP_CNT_CIM2ULP(x) (((x) >> S_SOP_CNT_CIM2ULP) & M_SOP_CNT_CIM2ULP)
+
+#define S_EOP_CNT_CIM2ULP    0
+#define M_EOP_CNT_CIM2ULP    0xfU
+#define V_EOP_CNT_CIM2ULP(x) ((x) << S_EOP_CNT_CIM2ULP)
+#define G_EOP_CNT_CIM2ULP(x) (((x) >> S_EOP_CNT_CIM2ULP) & M_EOP_CNT_CIM2ULP)
+
+#define A_ULP_TX_SE_CNT_CH1 0x8eac
+#define A_ULP_TX_SE_CNT_CH2 0x8eb0
+#define A_ULP_TX_SE_CNT_CH3 0x8eb4
+#define A_ULP_TX_DROP_CNT 0x8eb8
+
+#define S_DROP_CH3    12
+#define M_DROP_CH3    0xfU
+#define V_DROP_CH3(x) ((x) << S_DROP_CH3)
+#define G_DROP_CH3(x) (((x) >> S_DROP_CH3) & M_DROP_CH3)
+
+#define S_DROP_CH2    8
+#define M_DROP_CH2    0xfU
+#define V_DROP_CH2(x) ((x) << S_DROP_CH2)
+#define G_DROP_CH2(x) (((x) >> S_DROP_CH2) & M_DROP_CH2)
+
+#define S_DROP_CH1    4
+#define M_DROP_CH1    0xfU
+#define V_DROP_CH1(x) ((x) << S_DROP_CH1)
+#define G_DROP_CH1(x) (((x) >> S_DROP_CH1) & M_DROP_CH1)
+
+#define S_DROP_CH0    0
+#define M_DROP_CH0    0xfU
+#define V_DROP_CH0(x) ((x) << S_DROP_CH0)
+#define G_DROP_CH0(x) (((x) >> S_DROP_CH0) & M_DROP_CH0)
+
+#define A_ULP_TX_LA_RDPTR_0 0x8ec0
+#define A_ULP_TX_LA_RDDATA_0 0x8ec4
+#define A_ULP_TX_LA_WRPTR_0 0x8ec8
+#define A_ULP_TX_LA_RESERVED_0 0x8ecc
+#define A_ULP_TX_LA_RDPTR_1 0x8ed0
+#define A_ULP_TX_LA_RDDATA_1 0x8ed4
+#define A_ULP_TX_LA_WRPTR_1 0x8ed8
+#define A_ULP_TX_LA_RESERVED_1 0x8edc
+#define A_ULP_TX_LA_RDPTR_2 0x8ee0
+#define A_ULP_TX_LA_RDDATA_2 0x8ee4
+#define A_ULP_TX_LA_WRPTR_2 0x8ee8
+#define A_ULP_TX_LA_RESERVED_2 0x8eec
+#define A_ULP_TX_LA_RDPTR_3 0x8ef0
+#define A_ULP_TX_LA_RDDATA_3 0x8ef4
+#define A_ULP_TX_LA_WRPTR_3 0x8ef8
+#define A_ULP_TX_LA_RESERVED_3 0x8efc
+#define A_ULP_TX_LA_RDPTR_4 0x8f00
+#define A_ULP_TX_LA_RDDATA_4 0x8f04
+#define A_ULP_TX_LA_WRPTR_4 0x8f08
+#define A_ULP_TX_LA_RESERVED_4 0x8f0c
+#define A_ULP_TX_LA_RDPTR_5 0x8f10
+#define A_ULP_TX_LA_RDDATA_5 0x8f14
+#define A_ULP_TX_LA_WRPTR_5 0x8f18
+#define A_ULP_TX_LA_RESERVED_5 0x8f1c
+#define A_ULP_TX_LA_RDPTR_6 0x8f20
+#define A_ULP_TX_LA_RDDATA_6 0x8f24
+#define A_ULP_TX_LA_WRPTR_6 0x8f28
+#define A_ULP_TX_LA_RESERVED_6 0x8f2c
+#define A_ULP_TX_LA_RDPTR_7 0x8f30
+#define A_ULP_TX_LA_RDDATA_7 0x8f34
+#define A_ULP_TX_LA_WRPTR_7 0x8f38
+#define A_ULP_TX_LA_RESERVED_7 0x8f3c
+#define A_ULP_TX_LA_RDPTR_8 0x8f40
+#define A_ULP_TX_LA_RDDATA_8 0x8f44
+#define A_ULP_TX_LA_WRPTR_8 0x8f48
+#define A_ULP_TX_LA_RESERVED_8 0x8f4c
+#define A_ULP_TX_LA_RDPTR_9 0x8f50
+#define A_ULP_TX_LA_RDDATA_9 0x8f54
+#define A_ULP_TX_LA_WRPTR_9 0x8f58
+#define A_ULP_TX_LA_RESERVED_9 0x8f5c
+#define A_ULP_TX_LA_RDPTR_10 0x8f60
+#define A_ULP_TX_LA_RDDATA_10 0x8f64
+#define A_ULP_TX_LA_WRPTR_10 0x8f68
+#define A_ULP_TX_LA_RESERVED_10 0x8f6c
+
+/* registers for module PM_RX */
+#define PM_RX_BASE_ADDR 0x8fc0
+
+#define A_PM_RX_CFG 0x8fc0
+#define A_PM_RX_MODE 0x8fc4
+
+#define S_RX_USE_BUNDLE_LEN    4
+#define V_RX_USE_BUNDLE_LEN(x) ((x) << S_RX_USE_BUNDLE_LEN)
+#define F_RX_USE_BUNDLE_LEN    V_RX_USE_BUNDLE_LEN(1U)
+
+#define S_STAT_TO_CH    3
+#define V_STAT_TO_CH(x) ((x) << S_STAT_TO_CH)
+#define F_STAT_TO_CH    V_STAT_TO_CH(1U)
+
+#define S_STAT_FROM_CH    1
+#define M_STAT_FROM_CH    0x3U
+#define V_STAT_FROM_CH(x) ((x) << S_STAT_FROM_CH)
+#define G_STAT_FROM_CH(x) (((x) >> S_STAT_FROM_CH) & M_STAT_FROM_CH)
+
+#define S_PREFETCH_ENABLE    0
+#define V_PREFETCH_ENABLE(x) ((x) << S_PREFETCH_ENABLE)
+#define F_PREFETCH_ENABLE    V_PREFETCH_ENABLE(1U)
+
+#define A_PM_RX_STAT_CONFIG 0x8fc8
+#define A_PM_RX_STAT_COUNT 0x8fcc
+#define A_PM_RX_STAT_LSB 0x8fd0
+#define A_PM_RX_STAT_MSB 0x8fd4
+#define A_PM_RX_INT_ENABLE 0x8fd8
+
+#define S_ZERO_E_CMD_ERROR    22
+#define V_ZERO_E_CMD_ERROR(x) ((x) << S_ZERO_E_CMD_ERROR)
+#define F_ZERO_E_CMD_ERROR    V_ZERO_E_CMD_ERROR(1U)
+
+#define S_IESPI0_FIFO2X_RX_FRAMING_ERROR    21
+#define V_IESPI0_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_IESPI0_FIFO2X_RX_FRAMING_ERROR)
+#define F_IESPI0_FIFO2X_RX_FRAMING_ERROR    V_IESPI0_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI1_FIFO2X_RX_FRAMING_ERROR    20
+#define V_IESPI1_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_IESPI1_FIFO2X_RX_FRAMING_ERROR)
+#define F_IESPI1_FIFO2X_RX_FRAMING_ERROR    V_IESPI1_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI2_FIFO2X_RX_FRAMING_ERROR    19
+#define V_IESPI2_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_IESPI2_FIFO2X_RX_FRAMING_ERROR)
+#define F_IESPI2_FIFO2X_RX_FRAMING_ERROR    V_IESPI2_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI3_FIFO2X_RX_FRAMING_ERROR    18
+#define V_IESPI3_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_IESPI3_FIFO2X_RX_FRAMING_ERROR)
+#define F_IESPI3_FIFO2X_RX_FRAMING_ERROR    V_IESPI3_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI0_RX_FRAMING_ERROR    17
+#define V_IESPI0_RX_FRAMING_ERROR(x) ((x) << S_IESPI0_RX_FRAMING_ERROR)
+#define F_IESPI0_RX_FRAMING_ERROR    V_IESPI0_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI1_RX_FRAMING_ERROR    16
+#define V_IESPI1_RX_FRAMING_ERROR(x) ((x) << S_IESPI1_RX_FRAMING_ERROR)
+#define F_IESPI1_RX_FRAMING_ERROR    V_IESPI1_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI2_RX_FRAMING_ERROR    15
+#define V_IESPI2_RX_FRAMING_ERROR(x) ((x) << S_IESPI2_RX_FRAMING_ERROR)
+#define F_IESPI2_RX_FRAMING_ERROR    V_IESPI2_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI3_RX_FRAMING_ERROR    14
+#define V_IESPI3_RX_FRAMING_ERROR(x) ((x) << S_IESPI3_RX_FRAMING_ERROR)
+#define F_IESPI3_RX_FRAMING_ERROR    V_IESPI3_RX_FRAMING_ERROR(1U)
+
+#define S_IESPI0_TX_FRAMING_ERROR    13
+#define V_IESPI0_TX_FRAMING_ERROR(x) ((x) << S_IESPI0_TX_FRAMING_ERROR)
+#define F_IESPI0_TX_FRAMING_ERROR    V_IESPI0_TX_FRAMING_ERROR(1U)
+
+#define S_IESPI1_TX_FRAMING_ERROR    12
+#define V_IESPI1_TX_FRAMING_ERROR(x) ((x) << S_IESPI1_TX_FRAMING_ERROR)
+#define F_IESPI1_TX_FRAMING_ERROR    V_IESPI1_TX_FRAMING_ERROR(1U)
+
+#define S_IESPI2_TX_FRAMING_ERROR    11
+#define V_IESPI2_TX_FRAMING_ERROR(x) ((x) << S_IESPI2_TX_FRAMING_ERROR)
+#define F_IESPI2_TX_FRAMING_ERROR    V_IESPI2_TX_FRAMING_ERROR(1U)
+
+#define S_IESPI3_TX_FRAMING_ERROR    10
+#define V_IESPI3_TX_FRAMING_ERROR(x) ((x) << S_IESPI3_TX_FRAMING_ERROR)
+#define F_IESPI3_TX_FRAMING_ERROR    V_IESPI3_TX_FRAMING_ERROR(1U)
+
+#define S_OCSPI0_RX_FRAMING_ERROR    9
+#define V_OCSPI0_RX_FRAMING_ERROR(x) ((x) << S_OCSPI0_RX_FRAMING_ERROR)
+#define F_OCSPI0_RX_FRAMING_ERROR    V_OCSPI0_RX_FRAMING_ERROR(1U)
+
+#define S_OCSPI1_RX_FRAMING_ERROR    8
+#define V_OCSPI1_RX_FRAMING_ERROR(x) ((x) << S_OCSPI1_RX_FRAMING_ERROR)
+#define F_OCSPI1_RX_FRAMING_ERROR    V_OCSPI1_RX_FRAMING_ERROR(1U)
+
+#define S_OCSPI0_TX_FRAMING_ERROR    7
+#define V_OCSPI0_TX_FRAMING_ERROR(x) ((x) << S_OCSPI0_TX_FRAMING_ERROR)
+#define F_OCSPI0_TX_FRAMING_ERROR    V_OCSPI0_TX_FRAMING_ERROR(1U)
+
+#define S_OCSPI1_TX_FRAMING_ERROR    6
+#define V_OCSPI1_TX_FRAMING_ERROR(x) ((x) << S_OCSPI1_TX_FRAMING_ERROR)
+#define F_OCSPI1_TX_FRAMING_ERROR    V_OCSPI1_TX_FRAMING_ERROR(1U)
+
+#define S_OCSPI0_OFIFO2X_TX_FRAMING_ERROR    5
+#define V_OCSPI0_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OCSPI0_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OCSPI0_OFIFO2X_TX_FRAMING_ERROR    V_OCSPI0_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OCSPI1_OFIFO2X_TX_FRAMING_ERROR    4
+#define V_OCSPI1_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OCSPI1_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OCSPI1_OFIFO2X_TX_FRAMING_ERROR    V_OCSPI1_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OCSPI_PAR_ERROR    3
+#define V_OCSPI_PAR_ERROR(x) ((x) << S_OCSPI_PAR_ERROR)
+#define F_OCSPI_PAR_ERROR    V_OCSPI_PAR_ERROR(1U)
+
+#define S_DB_OPTIONS_PAR_ERROR    2
+#define V_DB_OPTIONS_PAR_ERROR(x) ((x) << S_DB_OPTIONS_PAR_ERROR)
+#define F_DB_OPTIONS_PAR_ERROR    V_DB_OPTIONS_PAR_ERROR(1U)
+
+#define S_IESPI_PAR_ERROR    1
+#define V_IESPI_PAR_ERROR(x) ((x) << S_IESPI_PAR_ERROR)
+#define F_IESPI_PAR_ERROR    V_IESPI_PAR_ERROR(1U)
+
+#define S_E_PCMD_PAR_ERROR    0
+#define V_E_PCMD_PAR_ERROR(x) ((x) << S_E_PCMD_PAR_ERROR)
+#define F_E_PCMD_PAR_ERROR    V_E_PCMD_PAR_ERROR(1U)
+
+#define A_PM_RX_INT_CAUSE 0x8fdc
+
+/* registers for module PM_TX */
+#define PM_TX_BASE_ADDR 0x8fe0
+
+#define A_PM_TX_CFG 0x8fe0
+
+#define S_CH3_OUTPUT    17
+#define M_CH3_OUTPUT    0x1fU
+#define V_CH3_OUTPUT(x) ((x) << S_CH3_OUTPUT)
+#define G_CH3_OUTPUT(x) (((x) >> S_CH3_OUTPUT) & M_CH3_OUTPUT)
+
+#define A_PM_TX_MODE 0x8fe4
+
+#define S_CONG_THRESH3    25
+#define M_CONG_THRESH3    0x7fU
+#define V_CONG_THRESH3(x) ((x) << S_CONG_THRESH3)
+#define G_CONG_THRESH3(x) (((x) >> S_CONG_THRESH3) & M_CONG_THRESH3)
+
+#define S_CONG_THRESH2    18
+#define M_CONG_THRESH2    0x7fU
+#define V_CONG_THRESH2(x) ((x) << S_CONG_THRESH2)
+#define G_CONG_THRESH2(x) (((x) >> S_CONG_THRESH2) & M_CONG_THRESH2)
+
+#define S_CONG_THRESH1    11
+#define M_CONG_THRESH1    0x7fU
+#define V_CONG_THRESH1(x) ((x) << S_CONG_THRESH1)
+#define G_CONG_THRESH1(x) (((x) >> S_CONG_THRESH1) & M_CONG_THRESH1)
+
+#define S_CONG_THRESH0    4
+#define M_CONG_THRESH0    0x7fU
+#define V_CONG_THRESH0(x) ((x) << S_CONG_THRESH0)
+#define G_CONG_THRESH0(x) (((x) >> S_CONG_THRESH0) & M_CONG_THRESH0)
+
+#define S_TX_USE_BUNDLE_LEN    3
+#define V_TX_USE_BUNDLE_LEN(x) ((x) << S_TX_USE_BUNDLE_LEN)
+#define F_TX_USE_BUNDLE_LEN    V_TX_USE_BUNDLE_LEN(1U)
+
+#define S_STAT_CHANNEL    1
+#define M_STAT_CHANNEL    0x3U
+#define V_STAT_CHANNEL(x) ((x) << S_STAT_CHANNEL)
+#define G_STAT_CHANNEL(x) (((x) >> S_STAT_CHANNEL) & M_STAT_CHANNEL)
+
+#define A_PM_TX_STAT_CONFIG 0x8fe8
+#define A_PM_TX_STAT_COUNT 0x8fec
+#define A_PM_TX_STAT_LSB 0x8ff0
+#define A_PM_TX_STAT_MSB 0x8ff4
+#define A_PM_TX_INT_ENABLE 0x8ff8
+
+#define S_PCMD_LEN_OVFL0    31
+#define V_PCMD_LEN_OVFL0(x) ((x) << S_PCMD_LEN_OVFL0)
+#define F_PCMD_LEN_OVFL0    V_PCMD_LEN_OVFL0(1U)
+
+#define S_PCMD_LEN_OVFL1    30
+#define V_PCMD_LEN_OVFL1(x) ((x) << S_PCMD_LEN_OVFL1)
+#define F_PCMD_LEN_OVFL1    V_PCMD_LEN_OVFL1(1U)
+
+#define S_PCMD_LEN_OVFL2    29
+#define V_PCMD_LEN_OVFL2(x) ((x) << S_PCMD_LEN_OVFL2)
+#define F_PCMD_LEN_OVFL2    V_PCMD_LEN_OVFL2(1U)
+
+#define S_ZERO_C_CMD_ERRO    28
+#define V_ZERO_C_CMD_ERRO(x) ((x) << S_ZERO_C_CMD_ERRO)
+#define F_ZERO_C_CMD_ERRO    V_ZERO_C_CMD_ERRO(1U)
+
+#define S_ICSPI0_FIFO2X_RX_FRAMING_ERROR    27
+#define V_ICSPI0_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_ICSPI0_FIFO2X_RX_FRAMING_ERROR)
+#define F_ICSPI0_FIFO2X_RX_FRAMING_ERROR    V_ICSPI0_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI1_FIFO2X_RX_FRAMING_ERROR    26
+#define V_ICSPI1_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_ICSPI1_FIFO2X_RX_FRAMING_ERROR)
+#define F_ICSPI1_FIFO2X_RX_FRAMING_ERROR    V_ICSPI1_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI2_FIFO2X_RX_FRAMING_ERROR    25
+#define V_ICSPI2_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_ICSPI2_FIFO2X_RX_FRAMING_ERROR)
+#define F_ICSPI2_FIFO2X_RX_FRAMING_ERROR    V_ICSPI2_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI3_FIFO2X_RX_FRAMING_ERROR    24
+#define V_ICSPI3_FIFO2X_RX_FRAMING_ERROR(x) ((x) << S_ICSPI3_FIFO2X_RX_FRAMING_ERROR)
+#define F_ICSPI3_FIFO2X_RX_FRAMING_ERROR    V_ICSPI3_FIFO2X_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI0_RX_FRAMING_ERROR    23
+#define V_ICSPI0_RX_FRAMING_ERROR(x) ((x) << S_ICSPI0_RX_FRAMING_ERROR)
+#define F_ICSPI0_RX_FRAMING_ERROR    V_ICSPI0_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI1_RX_FRAMING_ERROR    22
+#define V_ICSPI1_RX_FRAMING_ERROR(x) ((x) << S_ICSPI1_RX_FRAMING_ERROR)
+#define F_ICSPI1_RX_FRAMING_ERROR    V_ICSPI1_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI2_RX_FRAMING_ERROR    21
+#define V_ICSPI2_RX_FRAMING_ERROR(x) ((x) << S_ICSPI2_RX_FRAMING_ERROR)
+#define F_ICSPI2_RX_FRAMING_ERROR    V_ICSPI2_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI3_RX_FRAMING_ERROR    20
+#define V_ICSPI3_RX_FRAMING_ERROR(x) ((x) << S_ICSPI3_RX_FRAMING_ERROR)
+#define F_ICSPI3_RX_FRAMING_ERROR    V_ICSPI3_RX_FRAMING_ERROR(1U)
+
+#define S_ICSPI0_TX_FRAMING_ERROR    19
+#define V_ICSPI0_TX_FRAMING_ERROR(x) ((x) << S_ICSPI0_TX_FRAMING_ERROR)
+#define F_ICSPI0_TX_FRAMING_ERROR    V_ICSPI0_TX_FRAMING_ERROR(1U)
+
+#define S_ICSPI1_TX_FRAMING_ERROR    18
+#define V_ICSPI1_TX_FRAMING_ERROR(x) ((x) << S_ICSPI1_TX_FRAMING_ERROR)
+#define F_ICSPI1_TX_FRAMING_ERROR    V_ICSPI1_TX_FRAMING_ERROR(1U)
+
+#define S_ICSPI2_TX_FRAMING_ERROR    17
+#define V_ICSPI2_TX_FRAMING_ERROR(x) ((x) << S_ICSPI2_TX_FRAMING_ERROR)
+#define F_ICSPI2_TX_FRAMING_ERROR    V_ICSPI2_TX_FRAMING_ERROR(1U)
+
+#define S_ICSPI3_TX_FRAMING_ERROR    16
+#define V_ICSPI3_TX_FRAMING_ERROR(x) ((x) << S_ICSPI3_TX_FRAMING_ERROR)
+#define F_ICSPI3_TX_FRAMING_ERROR    V_ICSPI3_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI0_RX_FRAMING_ERROR    15
+#define V_OESPI0_RX_FRAMING_ERROR(x) ((x) << S_OESPI0_RX_FRAMING_ERROR)
+#define F_OESPI0_RX_FRAMING_ERROR    V_OESPI0_RX_FRAMING_ERROR(1U)
+
+#define S_OESPI1_RX_FRAMING_ERROR    14
+#define V_OESPI1_RX_FRAMING_ERROR(x) ((x) << S_OESPI1_RX_FRAMING_ERROR)
+#define F_OESPI1_RX_FRAMING_ERROR    V_OESPI1_RX_FRAMING_ERROR(1U)
+
+#define S_OESPI2_RX_FRAMING_ERROR    13
+#define V_OESPI2_RX_FRAMING_ERROR(x) ((x) << S_OESPI2_RX_FRAMING_ERROR)
+#define F_OESPI2_RX_FRAMING_ERROR    V_OESPI2_RX_FRAMING_ERROR(1U)
+
+#define S_OESPI3_RX_FRAMING_ERROR    12
+#define V_OESPI3_RX_FRAMING_ERROR(x) ((x) << S_OESPI3_RX_FRAMING_ERROR)
+#define F_OESPI3_RX_FRAMING_ERROR    V_OESPI3_RX_FRAMING_ERROR(1U)
+
+#define S_OESPI0_TX_FRAMING_ERROR    11
+#define V_OESPI0_TX_FRAMING_ERROR(x) ((x) << S_OESPI0_TX_FRAMING_ERROR)
+#define F_OESPI0_TX_FRAMING_ERROR    V_OESPI0_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI1_TX_FRAMING_ERROR    10
+#define V_OESPI1_TX_FRAMING_ERROR(x) ((x) << S_OESPI1_TX_FRAMING_ERROR)
+#define F_OESPI1_TX_FRAMING_ERROR    V_OESPI1_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI2_TX_FRAMING_ERROR    9
+#define V_OESPI2_TX_FRAMING_ERROR(x) ((x) << S_OESPI2_TX_FRAMING_ERROR)
+#define F_OESPI2_TX_FRAMING_ERROR    V_OESPI2_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI3_TX_FRAMING_ERROR    8
+#define V_OESPI3_TX_FRAMING_ERROR(x) ((x) << S_OESPI3_TX_FRAMING_ERROR)
+#define F_OESPI3_TX_FRAMING_ERROR    V_OESPI3_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI0_OFIFO2X_TX_FRAMING_ERROR    7
+#define V_OESPI0_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OESPI0_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OESPI0_OFIFO2X_TX_FRAMING_ERROR    V_OESPI0_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI1_OFIFO2X_TX_FRAMING_ERROR    6
+#define V_OESPI1_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OESPI1_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OESPI1_OFIFO2X_TX_FRAMING_ERROR    V_OESPI1_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI2_OFIFO2X_TX_FRAMING_ERROR    5
+#define V_OESPI2_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OESPI2_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OESPI2_OFIFO2X_TX_FRAMING_ERROR    V_OESPI2_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI3_OFIFO2X_TX_FRAMING_ERROR    4
+#define V_OESPI3_OFIFO2X_TX_FRAMING_ERROR(x) ((x) << S_OESPI3_OFIFO2X_TX_FRAMING_ERROR)
+#define F_OESPI3_OFIFO2X_TX_FRAMING_ERROR    V_OESPI3_OFIFO2X_TX_FRAMING_ERROR(1U)
+
+#define S_OESPI_PAR_ERROR    3
+#define V_OESPI_PAR_ERROR(x) ((x) << S_OESPI_PAR_ERROR)
+#define F_OESPI_PAR_ERROR    V_OESPI_PAR_ERROR(1U)
+
+#define S_ICSPI_PAR_ERROR    1
+#define V_ICSPI_PAR_ERROR(x) ((x) << S_ICSPI_PAR_ERROR)
+#define F_ICSPI_PAR_ERROR    V_ICSPI_PAR_ERROR(1U)
+
+#define S_C_PCMD_PAR_ERROR    0
+#define V_C_PCMD_PAR_ERROR(x) ((x) << S_C_PCMD_PAR_ERROR)
+#define F_C_PCMD_PAR_ERROR    V_C_PCMD_PAR_ERROR(1U)
+
+#define A_PM_TX_INT_CAUSE 0x8ffc
+
+#define S_ZERO_C_CMD_ERROR    28
+#define V_ZERO_C_CMD_ERROR(x) ((x) << S_ZERO_C_CMD_ERROR)
+#define F_ZERO_C_CMD_ERROR    V_ZERO_C_CMD_ERROR(1U)
+
+/* registers for module MPS */
+#define MPS_BASE_ADDR 0x9000
+
+#define A_MPS_PORT_CTL 0x0
+
+#define S_LPBKEN    31
+#define V_LPBKEN(x) ((x) << S_LPBKEN)
+#define F_LPBKEN    V_LPBKEN(1U)
+
+#define S_PORTTXEN    30
+#define V_PORTTXEN(x) ((x) << S_PORTTXEN)
+#define F_PORTTXEN    V_PORTTXEN(1U)
+
+#define S_PORTRXEN    29
+#define V_PORTRXEN(x) ((x) << S_PORTRXEN)
+#define F_PORTRXEN    V_PORTRXEN(1U)
+
+#define S_PPPEN    28
+#define V_PPPEN(x) ((x) << S_PPPEN)
+#define F_PPPEN    V_PPPEN(1U)
+
+#define S_FCSSTRIPEN    27
+#define V_FCSSTRIPEN(x) ((x) << S_FCSSTRIPEN)
+#define F_FCSSTRIPEN    V_FCSSTRIPEN(1U)
+
+#define S_PPPANDPAUSE    26
+#define V_PPPANDPAUSE(x) ((x) << S_PPPANDPAUSE)
+#define F_PPPANDPAUSE    V_PPPANDPAUSE(1U)
+
+#define S_PRIOPPPENMAP    16
+#define M_PRIOPPPENMAP    0xffU
+#define V_PRIOPPPENMAP(x) ((x) << S_PRIOPPPENMAP)
+#define G_PRIOPPPENMAP(x) (((x) >> S_PRIOPPPENMAP) & M_PRIOPPPENMAP)
+
+#define A_MPS_VF_CTL 0x0
+#define A_MPS_PORT_PAUSE_CTL 0x4
+
+#define S_TIMEUNIT    0
+#define M_TIMEUNIT    0xffffU
+#define V_TIMEUNIT(x) ((x) << S_TIMEUNIT)
+#define G_TIMEUNIT(x) (((x) >> S_TIMEUNIT) & M_TIMEUNIT)
+
+#define A_MPS_PORT_TX_PAUSE_CTL 0x8
+
+#define S_REGSENDOFF    24
+#define M_REGSENDOFF    0xffU
+#define V_REGSENDOFF(x) ((x) << S_REGSENDOFF)
+#define G_REGSENDOFF(x) (((x) >> S_REGSENDOFF) & M_REGSENDOFF)
+
+#define S_REGSENDON    16
+#define M_REGSENDON    0xffU
+#define V_REGSENDON(x) ((x) << S_REGSENDON)
+#define G_REGSENDON(x) (((x) >> S_REGSENDON) & M_REGSENDON)
+
+#define S_SGESENDEN    8
+#define M_SGESENDEN    0xffU
+#define V_SGESENDEN(x) ((x) << S_SGESENDEN)
+#define G_SGESENDEN(x) (((x) >> S_SGESENDEN) & M_SGESENDEN)
+
+#define S_RXSENDEN    0
+#define M_RXSENDEN    0xffU
+#define V_RXSENDEN(x) ((x) << S_RXSENDEN)
+#define G_RXSENDEN(x) (((x) >> S_RXSENDEN) & M_RXSENDEN)
+
+#define A_MPS_PORT_TX_PAUSE_CTL2 0xc
+
+#define S_XOFFDISABLE    0
+#define V_XOFFDISABLE(x) ((x) << S_XOFFDISABLE)
+#define F_XOFFDISABLE    V_XOFFDISABLE(1U)
+
+#define A_MPS_PORT_RX_PAUSE_CTL 0x10
+
+#define S_REGHALTON    8
+#define M_REGHALTON    0xffU
+#define V_REGHALTON(x) ((x) << S_REGHALTON)
+#define G_REGHALTON(x) (((x) >> S_REGHALTON) & M_REGHALTON)
+
+#define S_RXHALTEN    0
+#define M_RXHALTEN    0xffU
+#define V_RXHALTEN(x) ((x) << S_RXHALTEN)
+#define G_RXHALTEN(x) (((x) >> S_RXHALTEN) & M_RXHALTEN)
+
+#define A_MPS_PORT_TX_PAUSE_STATUS 0x14
+
+#define S_REGSENDING    16
+#define M_REGSENDING    0xffU
+#define V_REGSENDING(x) ((x) << S_REGSENDING)
+#define G_REGSENDING(x) (((x) >> S_REGSENDING) & M_REGSENDING)
+
+#define S_SGESENDING    8
+#define M_SGESENDING    0xffU
+#define V_SGESENDING(x) ((x) << S_SGESENDING)
+#define G_SGESENDING(x) (((x) >> S_SGESENDING) & M_SGESENDING)
+
+#define S_RXSENDING    0
+#define M_RXSENDING    0xffU
+#define V_RXSENDING(x) ((x) << S_RXSENDING)
+#define G_RXSENDING(x) (((x) >> S_RXSENDING) & M_RXSENDING)
+
+#define A_MPS_PORT_RX_PAUSE_STATUS 0x18
+
+#define S_REGHALTED    8
+#define M_REGHALTED    0xffU
+#define V_REGHALTED(x) ((x) << S_REGHALTED)
+#define G_REGHALTED(x) (((x) >> S_REGHALTED) & M_REGHALTED)
+
+#define S_RXHALTED    0
+#define M_RXHALTED    0xffU
+#define V_RXHALTED(x) ((x) << S_RXHALTED)
+#define G_RXHALTED(x) (((x) >> S_RXHALTED) & M_RXHALTED)
+
+#define A_MPS_PORT_TX_PAUSE_DEST_L 0x1c
+#define A_MPS_PORT_TX_PAUSE_DEST_H 0x20
+
+#define S_ADDR    0
+#define M_ADDR    0xffffU
+#define V_ADDR(x) ((x) << S_ADDR)
+#define G_ADDR(x) (((x) >> S_ADDR) & M_ADDR)
+
+#define A_MPS_PORT_TX_PAUSE_SOURCE_L 0x24
+#define A_MPS_PORT_TX_PAUSE_SOURCE_H 0x28
+#define A_MPS_PORT_PRTY_BUFFER_GROUP_MAP 0x2c
+
+#define S_PRTY7    14
+#define M_PRTY7    0x3U
+#define V_PRTY7(x) ((x) << S_PRTY7)
+#define G_PRTY7(x) (((x) >> S_PRTY7) & M_PRTY7)
+
+#define S_PRTY6    12
+#define M_PRTY6    0x3U
+#define V_PRTY6(x) ((x) << S_PRTY6)
+#define G_PRTY6(x) (((x) >> S_PRTY6) & M_PRTY6)
+
+#define S_PRTY5    10
+#define M_PRTY5    0x3U
+#define V_PRTY5(x) ((x) << S_PRTY5)
+#define G_PRTY5(x) (((x) >> S_PRTY5) & M_PRTY5)
+
+#define S_PRTY4    8
+#define M_PRTY4    0x3U
+#define V_PRTY4(x) ((x) << S_PRTY4)
+#define G_PRTY4(x) (((x) >> S_PRTY4) & M_PRTY4)
+
+#define S_PRTY3    6
+#define M_PRTY3    0x3U
+#define V_PRTY3(x) ((x) << S_PRTY3)
+#define G_PRTY3(x) (((x) >> S_PRTY3) & M_PRTY3)
+
+#define S_PRTY2    4
+#define M_PRTY2    0x3U
+#define V_PRTY2(x) ((x) << S_PRTY2)
+#define G_PRTY2(x) (((x) >> S_PRTY2) & M_PRTY2)
+
+#define S_PRTY1    2
+#define M_PRTY1    0x3U
+#define V_PRTY1(x) ((x) << S_PRTY1)
+#define G_PRTY1(x) (((x) >> S_PRTY1) & M_PRTY1)
+
+#define S_PRTY0    0
+#define M_PRTY0    0x3U
+#define V_PRTY0(x) ((x) << S_PRTY0)
+#define G_PRTY0(x) (((x) >> S_PRTY0) & M_PRTY0)
+
+#define A_MPS_VF_STAT_TX_VF_BCAST_BYTES_L 0x80
+#define A_MPS_VF_STAT_TX_VF_BCAST_BYTES_H 0x84
+#define A_MPS_VF_STAT_TX_VF_BCAST_FRAMES_L 0x88
+#define A_MPS_VF_STAT_TX_VF_BCAST_FRAMES_H 0x8c
+#define A_MPS_VF_STAT_TX_VF_MCAST_BYTES_L 0x90
+#define A_MPS_VF_STAT_TX_VF_MCAST_BYTES_H 0x94
+#define A_MPS_VF_STAT_TX_VF_MCAST_FRAMES_L 0x98
+#define A_MPS_VF_STAT_TX_VF_MCAST_FRAMES_H 0x9c
+#define A_MPS_VF_STAT_TX_VF_UCAST_BYTES_L 0xa0
+#define A_MPS_VF_STAT_TX_VF_UCAST_BYTES_H 0xa4
+#define A_MPS_VF_STAT_TX_VF_UCAST_FRAMES_L 0xa8
+#define A_MPS_VF_STAT_TX_VF_UCAST_FRAMES_H 0xac
+#define A_MPS_VF_STAT_TX_VF_DROP_FRAMES_L 0xb0
+#define A_MPS_VF_STAT_TX_VF_DROP_FRAMES_H 0xb4
+#define A_MPS_VF_STAT_TX_VF_OFFLOAD_BYTES_L 0xb8
+#define A_MPS_VF_STAT_TX_VF_OFFLOAD_BYTES_H 0xbc
+#define A_MPS_VF_STAT_TX_VF_OFFLOAD_FRAMES_L 0xc0
+#define A_MPS_VF_STAT_TX_VF_OFFLOAD_FRAMES_H 0xc4
+#define A_MPS_VF_STAT_RX_VF_BCAST_BYTES_L 0xc8
+#define A_MPS_VF_STAT_RX_VF_BCAST_BYTES_H 0xcc
+#define A_MPS_VF_STAT_RX_VF_BCAST_FRAMES_L 0xd0
+#define A_MPS_VF_STAT_RX_VF_BCAST_FRAMES_H 0xd4
+#define A_MPS_VF_STAT_RX_VF_MCAST_BYTES_L 0xd8
+#define A_MPS_VF_STAT_RX_VF_MCAST_BYTES_H 0xdc
+#define A_MPS_VF_STAT_RX_VF_MCAST_FRAMES_L 0xe0
+#define A_MPS_VF_STAT_RX_VF_MCAST_FRAMES_H 0xe4
+#define A_MPS_VF_STAT_RX_VF_UCAST_BYTES_L 0xe8
+#define A_MPS_VF_STAT_RX_VF_UCAST_BYTES_H 0xec
+#define A_MPS_VF_STAT_RX_VF_UCAST_FRAMES_L 0xf0
+#define A_MPS_VF_STAT_RX_VF_UCAST_FRAMES_H 0xf4
+#define A_MPS_VF_STAT_RX_VF_ERR_FRAMES_L 0xf8
+#define A_MPS_VF_STAT_RX_VF_ERR_FRAMES_H 0xfc
+#define A_MPS_PORT_RX_CTL 0x100
+
+#define S_NO_RPLCT_M    20
+#define V_NO_RPLCT_M(x) ((x) << S_NO_RPLCT_M)
+#define F_NO_RPLCT_M    V_NO_RPLCT_M(1U)
+
+#define S_RPLCT_SEL_L    18
+#define M_RPLCT_SEL_L    0x3U
+#define V_RPLCT_SEL_L(x) ((x) << S_RPLCT_SEL_L)
+#define G_RPLCT_SEL_L(x) (((x) >> S_RPLCT_SEL_L) & M_RPLCT_SEL_L)
+
+#define S_FLTR_VLAN_SEL    17
+#define V_FLTR_VLAN_SEL(x) ((x) << S_FLTR_VLAN_SEL)
+#define F_FLTR_VLAN_SEL    V_FLTR_VLAN_SEL(1U)
+
+#define S_PRIO_VLAN_SEL    16
+#define V_PRIO_VLAN_SEL(x) ((x) << S_PRIO_VLAN_SEL)
+#define F_PRIO_VLAN_SEL    V_PRIO_VLAN_SEL(1U)
+
+#define S_CHK_8023_LEN_M    15
+#define V_CHK_8023_LEN_M(x) ((x) << S_CHK_8023_LEN_M)
+#define F_CHK_8023_LEN_M    V_CHK_8023_LEN_M(1U)
+
+#define S_CHK_8023_LEN_L    14
+#define V_CHK_8023_LEN_L(x) ((x) << S_CHK_8023_LEN_L)
+#define F_CHK_8023_LEN_L    V_CHK_8023_LEN_L(1U)
+
+#define S_NIV_DROP    13
+#define V_NIV_DROP(x) ((x) << S_NIV_DROP)
+#define F_NIV_DROP    V_NIV_DROP(1U)
+
+#define S_NOV_DROP    12
+#define V_NOV_DROP(x) ((x) << S_NOV_DROP)
+#define F_NOV_DROP    V_NOV_DROP(1U)
+
+#define S_CLS_PRT    11
+#define V_CLS_PRT(x) ((x) << S_CLS_PRT)
+#define F_CLS_PRT    V_CLS_PRT(1U)
+
+#define S_RX_QFC_EN    10
+#define V_RX_QFC_EN(x) ((x) << S_RX_QFC_EN)
+#define F_RX_QFC_EN    V_RX_QFC_EN(1U)
+
+#define S_QFC_FWD_UP    9
+#define V_QFC_FWD_UP(x) ((x) << S_QFC_FWD_UP)
+#define F_QFC_FWD_UP    V_QFC_FWD_UP(1U)
+
+#define S_PPP_FWD_UP    8
+#define V_PPP_FWD_UP(x) ((x) << S_PPP_FWD_UP)
+#define F_PPP_FWD_UP    V_PPP_FWD_UP(1U)
+
+#define S_PAUSE_FWD_UP    7
+#define V_PAUSE_FWD_UP(x) ((x) << S_PAUSE_FWD_UP)
+#define F_PAUSE_FWD_UP    V_PAUSE_FWD_UP(1U)
+
+#define S_LPBK_BP    6
+#define V_LPBK_BP(x) ((x) << S_LPBK_BP)
+#define F_LPBK_BP    V_LPBK_BP(1U)
+
+#define S_PASS_NO_MATCH    5
+#define V_PASS_NO_MATCH(x) ((x) << S_PASS_NO_MATCH)
+#define F_PASS_NO_MATCH    V_PASS_NO_MATCH(1U)
+
+#define S_IVLAN_EN    4
+#define V_IVLAN_EN(x) ((x) << S_IVLAN_EN)
+#define F_IVLAN_EN    V_IVLAN_EN(1U)
+
+#define S_OVLAN_EN3    3
+#define V_OVLAN_EN3(x) ((x) << S_OVLAN_EN3)
+#define F_OVLAN_EN3    V_OVLAN_EN3(1U)
+
+#define S_OVLAN_EN2    2
+#define V_OVLAN_EN2(x) ((x) << S_OVLAN_EN2)
+#define F_OVLAN_EN2    V_OVLAN_EN2(1U)
+
+#define S_OVLAN_EN1    1
+#define V_OVLAN_EN1(x) ((x) << S_OVLAN_EN1)
+#define F_OVLAN_EN1    V_OVLAN_EN1(1U)
+
+#define S_OVLAN_EN0    0
+#define V_OVLAN_EN0(x) ((x) << S_OVLAN_EN0)
+#define F_OVLAN_EN0    V_OVLAN_EN0(1U)
+
+#define A_MPS_PORT_RX_MTU 0x104
+#define A_MPS_PORT_RX_PF_MAP 0x108
+#define A_MPS_PORT_RX_VF_MAP0 0x10c
+#define A_MPS_PORT_RX_VF_MAP1 0x110
+#define A_MPS_PORT_RX_VF_MAP2 0x114
+#define A_MPS_PORT_RX_VF_MAP3 0x118
+#define A_MPS_PORT_RX_IVLAN 0x11c
+
+#define S_IVLAN_ETYPE    0
+#define M_IVLAN_ETYPE    0xffffU
+#define V_IVLAN_ETYPE(x) ((x) << S_IVLAN_ETYPE)
+#define G_IVLAN_ETYPE(x) (((x) >> S_IVLAN_ETYPE) & M_IVLAN_ETYPE)
+
+#define A_MPS_PORT_RX_OVLAN0 0x120
+
+#define S_OVLAN_MASK    16
+#define M_OVLAN_MASK    0xffffU
+#define V_OVLAN_MASK(x) ((x) << S_OVLAN_MASK)
+#define G_OVLAN_MASK(x) (((x) >> S_OVLAN_MASK) & M_OVLAN_MASK)
+
+#define S_OVLAN_ETYPE    0
+#define M_OVLAN_ETYPE    0xffffU
+#define V_OVLAN_ETYPE(x) ((x) << S_OVLAN_ETYPE)
+#define G_OVLAN_ETYPE(x) (((x) >> S_OVLAN_ETYPE) & M_OVLAN_ETYPE)
+
+#define A_MPS_PORT_RX_OVLAN1 0x124
+#define A_MPS_PORT_RX_OVLAN2 0x128
+#define A_MPS_PORT_RX_OVLAN3 0x12c
+#define A_MPS_PORT_RX_RSS_HASH 0x130
+#define A_MPS_PORT_RX_RSS_CONTROL 0x134
+
+#define S_RSS_CTRL    16
+#define M_RSS_CTRL    0xffU
+#define V_RSS_CTRL(x) ((x) << S_RSS_CTRL)
+#define G_RSS_CTRL(x) (((x) >> S_RSS_CTRL) & M_RSS_CTRL)
+
+#define S_QUE_NUM    0
+#define M_QUE_NUM    0xffffU
+#define V_QUE_NUM(x) ((x) << S_QUE_NUM)
+#define G_QUE_NUM(x) (((x) >> S_QUE_NUM) & M_QUE_NUM)
+
+#define A_MPS_PORT_RX_CTL1 0x138
+
+#define S_FIXED_PFVF_MAC    13
+#define V_FIXED_PFVF_MAC(x) ((x) << S_FIXED_PFVF_MAC)
+#define F_FIXED_PFVF_MAC    V_FIXED_PFVF_MAC(1U)
+
+#define S_FIXED_PFVF_LPBK    12
+#define V_FIXED_PFVF_LPBK(x) ((x) << S_FIXED_PFVF_LPBK)
+#define F_FIXED_PFVF_LPBK    V_FIXED_PFVF_LPBK(1U)
+
+#define S_FIXED_PFVF_LPBK_OV    11
+#define V_FIXED_PFVF_LPBK_OV(x) ((x) << S_FIXED_PFVF_LPBK_OV)
+#define F_FIXED_PFVF_LPBK_OV    V_FIXED_PFVF_LPBK_OV(1U)
+
+#define S_FIXED_PF    8
+#define M_FIXED_PF    0x7U
+#define V_FIXED_PF(x) ((x) << S_FIXED_PF)
+#define G_FIXED_PF(x) (((x) >> S_FIXED_PF) & M_FIXED_PF)
+
+#define S_FIXED_VF_VLD    7
+#define V_FIXED_VF_VLD(x) ((x) << S_FIXED_VF_VLD)
+#define F_FIXED_VF_VLD    V_FIXED_VF_VLD(1U)
+
+#define S_FIXED_VF    0
+#define M_FIXED_VF    0x7fU
+#define V_FIXED_VF(x) ((x) << S_FIXED_VF)
+#define G_FIXED_VF(x) (((x) >> S_FIXED_VF) & M_FIXED_VF)
+
+#define A_MPS_PORT_RX_SPARE 0x13c
+#define A_MPS_PORT_TX_MAC_RELOAD_CH0 0x190
+
+#define S_CREDIT    0
+#define M_CREDIT    0xffffU
+#define V_CREDIT(x) ((x) << S_CREDIT)
+#define G_CREDIT(x) (((x) >> S_CREDIT) & M_CREDIT)
+
+#define A_MPS_PORT_TX_MAC_RELOAD_CH1 0x194
+#define A_MPS_PORT_TX_MAC_RELOAD_CH2 0x198
+#define A_MPS_PORT_TX_MAC_RELOAD_CH3 0x19c
+#define A_MPS_PORT_TX_MAC_RELOAD_CH4 0x1a0
+#define A_MPS_PORT_TX_LPBK_RELOAD_CH0 0x1a8
+#define A_MPS_PORT_TX_LPBK_RELOAD_CH1 0x1ac
+#define A_MPS_PORT_TX_LPBK_RELOAD_CH2 0x1b0
+#define A_MPS_PORT_TX_LPBK_RELOAD_CH3 0x1b4
+#define A_MPS_PORT_TX_LPBK_RELOAD_CH4 0x1b8
+#define A_MPS_PORT_TX_FIFO_CTL 0x1c4
+
+#define S_FIFOTH    5
+#define M_FIFOTH    0x1ffU
+#define V_FIFOTH(x) ((x) << S_FIFOTH)
+#define G_FIFOTH(x) (((x) >> S_FIFOTH) & M_FIFOTH)
+
+#define S_FIFOEN    4
+#define V_FIFOEN(x) ((x) << S_FIFOEN)
+#define F_FIFOEN    V_FIFOEN(1U)
+
+#define S_MAXPKTCNT    0
+#define M_MAXPKTCNT    0xfU
+#define V_MAXPKTCNT(x) ((x) << S_MAXPKTCNT)
+#define G_MAXPKTCNT(x) (((x) >> S_MAXPKTCNT) & M_MAXPKTCNT)
+
+#define A_MPS_PORT_FPGA_PAUSE_CTL 0x1c8
+#define A_MPS_PORT_CLS_HASH_SRAM 0x200
+
+#define S_VALID    20
+#define V_VALID(x) ((x) << S_VALID)
+#define F_VALID    V_VALID(1U)
+
+#define S_HASHPORTMAP    16
+#define M_HASHPORTMAP    0xfU
+#define V_HASHPORTMAP(x) ((x) << S_HASHPORTMAP)
+#define G_HASHPORTMAP(x) (((x) >> S_HASHPORTMAP) & M_HASHPORTMAP)
+
+#define S_MULTILISTEN    15
+#define V_MULTILISTEN(x) ((x) << S_MULTILISTEN)
+#define F_MULTILISTEN    V_MULTILISTEN(1U)
+
+#define S_PRIORITY    12
+#define M_PRIORITY    0x7U
+#define V_PRIORITY(x) ((x) << S_PRIORITY)
+#define G_PRIORITY(x) (((x) >> S_PRIORITY) & M_PRIORITY)
+
+#define S_REPLICATE    11
+#define V_REPLICATE(x) ((x) << S_REPLICATE)
+#define F_REPLICATE    V_REPLICATE(1U)
+
+#define S_PF    8
+#define M_PF    0x7U
+#define V_PF(x) ((x) << S_PF)
+#define G_PF(x) (((x) >> S_PF) & M_PF)
+
+#define S_VF_VALID    7
+#define V_VF_VALID(x) ((x) << S_VF_VALID)
+#define F_VF_VALID    V_VF_VALID(1U)
+
+#define S_VF    0
+#define M_VF    0x7fU
+#define V_VF(x) ((x) << S_VF)
+#define G_VF(x) (((x) >> S_VF) & M_VF)
+
+#define A_MPS_PF_CTL 0x2c0
+
+#define S_TXEN    1
+#define V_TXEN(x) ((x) << S_TXEN)
+#define F_TXEN    V_TXEN(1U)
+
+#define S_RXEN    0
+#define V_RXEN(x) ((x) << S_RXEN)
+#define F_RXEN    V_RXEN(1U)
+
+#define A_MPS_PF_TX_QINQ_VLAN 0x2e0
+
+#define S_PROTOCOLID    16
+#define M_PROTOCOLID    0xffffU
+#define V_PROTOCOLID(x) ((x) << S_PROTOCOLID)
+#define G_PROTOCOLID(x) (((x) >> S_PROTOCOLID) & M_PROTOCOLID)
+
+#define S_VLAN_PRIO    13
+#define M_VLAN_PRIO    0x7U
+#define V_VLAN_PRIO(x) ((x) << S_VLAN_PRIO)
+#define G_VLAN_PRIO(x) (((x) >> S_VLAN_PRIO) & M_VLAN_PRIO)
+
+#define S_CFI    12
+#define V_CFI(x) ((x) << S_CFI)
+#define F_CFI    V_CFI(1U)
+
+#define S_TAG    0
+#define M_TAG    0xfffU
+#define V_TAG(x) ((x) << S_TAG)
+#define G_TAG(x) (((x) >> S_TAG) & M_TAG)
+
+#define A_MPS_PF_STAT_TX_PF_BCAST_BYTES_L 0x300
+#define A_MPS_PF_STAT_TX_PF_BCAST_BYTES_H 0x304
+#define A_MPS_PORT_CLS_HASH_CTL 0x304
+
+#define S_UNICASTENABLE    31
+#define V_UNICASTENABLE(x) ((x) << S_UNICASTENABLE)
+#define F_UNICASTENABLE    V_UNICASTENABLE(1U)
+
+#define A_MPS_PF_STAT_TX_PF_BCAST_FRAMES_L 0x308
+#define A_MPS_PORT_CLS_PROMISCUOUS_CTL 0x308
+
+#define S_PROMISCEN    31
+#define V_PROMISCEN(x) ((x) << S_PROMISCEN)
+#define F_PROMISCEN    V_PROMISCEN(1U)
+
+#define A_MPS_PF_STAT_TX_PF_BCAST_FRAMES_H 0x30c
+#define A_MPS_PORT_CLS_BMC_MAC_ADDR_L 0x30c
+#define A_MPS_PF_STAT_TX_PF_MCAST_BYTES_L 0x310
+#define A_MPS_PORT_CLS_BMC_MAC_ADDR_H 0x310
+
+#define S_MATCHBOTH    17
+#define V_MATCHBOTH(x) ((x) << S_MATCHBOTH)
+#define F_MATCHBOTH    V_MATCHBOTH(1U)
+
+#define S_BMC_VLD    16
+#define V_BMC_VLD(x) ((x) << S_BMC_VLD)
+#define F_BMC_VLD    V_BMC_VLD(1U)
+
+#define A_MPS_PF_STAT_TX_PF_MCAST_BYTES_H 0x314
+#define A_MPS_PORT_CLS_BMC_VLAN 0x314
+
+#define S_BMC_VLAN_SEL    13
+#define V_BMC_VLAN_SEL(x) ((x) << S_BMC_VLAN_SEL)
+#define F_BMC_VLAN_SEL    V_BMC_VLAN_SEL(1U)
+
+#define S_VLAN_VLD    12
+#define V_VLAN_VLD(x) ((x) << S_VLAN_VLD)
+#define F_VLAN_VLD    V_VLAN_VLD(1U)
+
+#define A_MPS_PF_STAT_TX_PF_MCAST_FRAMES_L 0x318
+#define A_MPS_PORT_CLS_CTL 0x318
+
+#define S_PF_VLAN_SEL    0
+#define V_PF_VLAN_SEL(x) ((x) << S_PF_VLAN_SEL)
+#define F_PF_VLAN_SEL    V_PF_VLAN_SEL(1U)
+
+#define A_MPS_PF_STAT_TX_PF_MCAST_FRAMES_H 0x31c
+#define A_MPS_PF_STAT_TX_PF_UCAST_BYTES_L 0x320
+#define A_MPS_PF_STAT_TX_PF_UCAST_BYTES_H 0x324
+#define A_MPS_PF_STAT_TX_PF_UCAST_FRAMES_L 0x328
+#define A_MPS_PF_STAT_TX_PF_UCAST_FRAMES_H 0x32c
+#define A_MPS_PF_STAT_TX_PF_OFFLOAD_BYTES_L 0x330
+#define A_MPS_PF_STAT_TX_PF_OFFLOAD_BYTES_H 0x334
+#define A_MPS_PF_STAT_TX_PF_OFFLOAD_FRAMES_L 0x338
+#define A_MPS_PF_STAT_TX_PF_OFFLOAD_FRAMES_H 0x33c
+#define A_MPS_PF_STAT_RX_PF_BYTES_L 0x340
+#define A_MPS_PF_STAT_RX_PF_BYTES_H 0x344
+#define A_MPS_PF_STAT_RX_PF_FRAMES_L 0x348
+#define A_MPS_PF_STAT_RX_PF_FRAMES_H 0x34c
+#define A_MPS_PF_STAT_RX_PF_BCAST_BYTES_L 0x350
+#define A_MPS_PF_STAT_RX_PF_BCAST_BYTES_H 0x354
+#define A_MPS_PF_STAT_RX_PF_BCAST_FRAMES_L 0x358
+#define A_MPS_PF_STAT_RX_PF_BCAST_FRAMES_H 0x35c
+#define A_MPS_PF_STAT_RX_PF_MCAST_BYTES_L 0x360
+#define A_MPS_PF_STAT_RX_PF_MCAST_BYTES_H 0x364
+#define A_MPS_PF_STAT_RX_PF_MCAST_FRAMES_L 0x368
+#define A_MPS_PF_STAT_RX_PF_MCAST_FRAMES_H 0x36c
+#define A_MPS_PF_STAT_RX_PF_UCAST_BYTES_L 0x370
+#define A_MPS_PF_STAT_RX_PF_UCAST_BYTES_H 0x374
+#define A_MPS_PF_STAT_RX_PF_UCAST_FRAMES_L 0x378
+#define A_MPS_PF_STAT_RX_PF_UCAST_FRAMES_H 0x37c
+#define A_MPS_PF_STAT_RX_PF_ERR_FRAMES_L 0x380
+#define A_MPS_PF_STAT_RX_PF_ERR_FRAMES_H 0x384
+#define A_MPS_PORT_STAT_TX_PORT_BYTES_L 0x400
+#define A_MPS_PORT_STAT_TX_PORT_BYTES_H 0x404
+#define A_MPS_PORT_STAT_TX_PORT_FRAMES_L 0x408
+#define A_MPS_PORT_STAT_TX_PORT_FRAMES_H 0x40c
+#define A_MPS_PORT_STAT_TX_PORT_BCAST_L 0x410
+#define A_MPS_PORT_STAT_TX_PORT_BCAST_H 0x414
+#define A_MPS_PORT_STAT_TX_PORT_MCAST_L 0x418
+#define A_MPS_PORT_STAT_TX_PORT_MCAST_H 0x41c
+#define A_MPS_PORT_STAT_TX_PORT_UCAST_L 0x420
+#define A_MPS_PORT_STAT_TX_PORT_UCAST_H 0x424
+#define A_MPS_PORT_STAT_TX_PORT_ERROR_L 0x428
+#define A_MPS_PORT_STAT_TX_PORT_ERROR_H 0x42c
+#define A_MPS_PORT_STAT_TX_PORT_64B_L 0x430
+#define A_MPS_PORT_STAT_TX_PORT_64B_H 0x434
+#define A_MPS_PORT_STAT_TX_PORT_65B_127B_L 0x438
+#define A_MPS_PORT_STAT_TX_PORT_65B_127B_H 0x43c
+#define A_MPS_PORT_STAT_TX_PORT_128B_255B_L 0x440
+#define A_MPS_PORT_STAT_TX_PORT_128B_255B_H 0x444
+#define A_MPS_PORT_STAT_TX_PORT_256B_511B_L 0x448
+#define A_MPS_PORT_STAT_TX_PORT_256B_511B_H 0x44c
+#define A_MPS_PORT_STAT_TX_PORT_512B_1023B_L 0x450
+#define A_MPS_PORT_STAT_TX_PORT_512B_1023B_H 0x454
+#define A_MPS_PORT_STAT_TX_PORT_1024B_1518B_L 0x458
+#define A_MPS_PORT_STAT_TX_PORT_1024B_1518B_H 0x45c
+#define A_MPS_PORT_STAT_TX_PORT_1519B_MAX_L 0x460
+#define A_MPS_PORT_STAT_TX_PORT_1519B_MAX_H 0x464
+#define A_MPS_PORT_STAT_TX_PORT_DROP_L 0x468
+#define A_MPS_PORT_STAT_TX_PORT_DROP_H 0x46c
+#define A_MPS_PORT_STAT_TX_PORT_PAUSE_L 0x470
+#define A_MPS_PORT_STAT_TX_PORT_PAUSE_H 0x474
+#define A_MPS_PORT_STAT_TX_PORT_PPP0_L 0x478
+#define A_MPS_PORT_STAT_TX_PORT_PPP0_H 0x47c
+#define A_MPS_PORT_STAT_TX_PORT_PPP1_L 0x480
+#define A_MPS_PORT_STAT_TX_PORT_PPP1_H 0x484
+#define A_MPS_PORT_STAT_TX_PORT_PPP2_L 0x488
+#define A_MPS_PORT_STAT_TX_PORT_PPP2_H 0x48c
+#define A_MPS_PORT_STAT_TX_PORT_PPP3_L 0x490
+#define A_MPS_PORT_STAT_TX_PORT_PPP3_H 0x494
+#define A_MPS_PORT_STAT_TX_PORT_PPP4_L 0x498
+#define A_MPS_PORT_STAT_TX_PORT_PPP4_H 0x49c
+#define A_MPS_PORT_STAT_TX_PORT_PPP5_L 0x4a0
+#define A_MPS_PORT_STAT_TX_PORT_PPP5_H 0x4a4
+#define A_MPS_PORT_STAT_TX_PORT_PPP6_L 0x4a8
+#define A_MPS_PORT_STAT_TX_PORT_PPP6_H 0x4ac
+#define A_MPS_PORT_STAT_TX_PORT_PPP7_L 0x4b0
+#define A_MPS_PORT_STAT_TX_PORT_PPP7_H 0x4b4
+#define A_MPS_PORT_STAT_LB_PORT_BYTES_L 0x4c0
+#define A_MPS_PORT_STAT_LB_PORT_BYTES_H 0x4c4
+#define A_MPS_PORT_STAT_LB_PORT_FRAMES_L 0x4c8
+#define A_MPS_PORT_STAT_LB_PORT_FRAMES_H 0x4cc
+#define A_MPS_PORT_STAT_LB_PORT_BCAST_L 0x4d0
+#define A_MPS_PORT_STAT_LB_PORT_BCAST_H 0x4d4
+#define A_MPS_PORT_STAT_LB_PORT_MCAST_L 0x4d8
+#define A_MPS_PORT_STAT_LB_PORT_MCAST_H 0x4dc
+#define A_MPS_PORT_STAT_LB_PORT_UCAST_L 0x4e0
+#define A_MPS_PORT_STAT_LB_PORT_UCAST_H 0x4e4
+#define A_MPS_PORT_STAT_LB_PORT_ERROR_L 0x4e8
+#define A_MPS_PORT_STAT_LB_PORT_ERROR_H 0x4ec
+#define A_MPS_PORT_STAT_LB_PORT_64B_L 0x4f0
+#define A_MPS_PORT_STAT_LB_PORT_64B_H 0x4f4
+#define A_MPS_PORT_STAT_LB_PORT_65B_127B_L 0x4f8
+#define A_MPS_PORT_STAT_LB_PORT_65B_127B_H 0x4fc
+#define A_MPS_PORT_STAT_LB_PORT_128B_255B_L 0x500
+#define A_MPS_PORT_STAT_LB_PORT_128B_255B_H 0x504
+#define A_MPS_PORT_STAT_LB_PORT_256B_511B_L 0x508
+#define A_MPS_PORT_STAT_LB_PORT_256B_511B_H 0x50c
+#define A_MPS_PORT_STAT_LB_PORT_512B_1023B_L 0x510
+#define A_MPS_PORT_STAT_LB_PORT_512B_1023B_H 0x514
+#define A_MPS_PORT_STAT_LB_PORT_1024B_1518B_L 0x518
+#define A_MPS_PORT_STAT_LB_PORT_1024B_1518B_H 0x51c
+#define A_MPS_PORT_STAT_LB_PORT_1519B_MAX_L 0x520
+#define A_MPS_PORT_STAT_LB_PORT_1519B_MAX_H 0x524
+#define A_MPS_PORT_STAT_LB_PORT_DROP_FRAMES 0x528
+#define A_MPS_PORT_STAT_RX_PORT_BYTES_L 0x540
+#define A_MPS_PORT_STAT_RX_PORT_BYTES_H 0x544
+#define A_MPS_PORT_STAT_RX_PORT_FRAMES_L 0x548
+#define A_MPS_PORT_STAT_RX_PORT_FRAMES_H 0x54c
+#define A_MPS_PORT_STAT_RX_PORT_BCAST_L 0x550
+#define A_MPS_PORT_STAT_RX_PORT_BCAST_H 0x554
+#define A_MPS_PORT_STAT_RX_PORT_MCAST_L 0x558
+#define A_MPS_PORT_STAT_RX_PORT_MCAST_H 0x55c
+#define A_MPS_PORT_STAT_RX_PORT_UCAST_L 0x560
+#define A_MPS_PORT_STAT_RX_PORT_UCAST_H 0x564
+#define A_MPS_PORT_STAT_RX_PORT_MTU_ERROR_L 0x568
+#define A_MPS_PORT_STAT_RX_PORT_MTU_ERROR_H 0x56c
+#define A_MPS_PORT_STAT_RX_PORT_MTU_CRC_ERROR_L 0x570
+#define A_MPS_PORT_STAT_RX_PORT_MTU_CRC_ERROR_H 0x574
+#define A_MPS_PORT_STAT_RX_PORT_CRC_ERROR_L 0x578
+#define A_MPS_PORT_STAT_RX_PORT_CRC_ERROR_H 0x57c
+#define A_MPS_PORT_STAT_RX_PORT_LEN_ERROR_L 0x580
+#define A_MPS_PORT_STAT_RX_PORT_LEN_ERROR_H 0x584
+#define A_MPS_PORT_STAT_RX_PORT_SYM_ERROR_L 0x588
+#define A_MPS_PORT_STAT_RX_PORT_SYM_ERROR_H 0x58c
+#define A_MPS_PORT_STAT_RX_PORT_64B_L 0x590
+#define A_MPS_PORT_STAT_RX_PORT_64B_H 0x594
+#define A_MPS_PORT_STAT_RX_PORT_65B_127B_L 0x598
+#define A_MPS_PORT_STAT_RX_PORT_65B_127B_H 0x59c
+#define A_MPS_PORT_STAT_RX_PORT_128B_255B_L 0x5a0
+#define A_MPS_PORT_STAT_RX_PORT_128B_255B_H 0x5a4
+#define A_MPS_PORT_STAT_RX_PORT_256B_511B_L 0x5a8
+#define A_MPS_PORT_STAT_RX_PORT_256B_511B_H 0x5ac
+#define A_MPS_PORT_STAT_RX_PORT_512B_1023B_L 0x5b0
+#define A_MPS_PORT_STAT_RX_PORT_512B_1023B_H 0x5b4
+#define A_MPS_PORT_STAT_RX_PORT_1024B_1518B_L 0x5b8
+#define A_MPS_PORT_STAT_RX_PORT_1024B_1518B_H 0x5bc
+#define A_MPS_PORT_STAT_RX_PORT_1519B_MAX_L 0x5c0
+#define A_MPS_PORT_STAT_RX_PORT_1519B_MAX_H 0x5c4
+#define A_MPS_PORT_STAT_RX_PORT_PAUSE_L 0x5c8
+#define A_MPS_PORT_STAT_RX_PORT_PAUSE_H 0x5cc
+#define A_MPS_PORT_STAT_RX_PORT_PPP0_L 0x5d0
+#define A_MPS_PORT_STAT_RX_PORT_PPP0_H 0x5d4
+#define A_MPS_PORT_STAT_RX_PORT_PPP1_L 0x5d8
+#define A_MPS_PORT_STAT_RX_PORT_PPP1_H 0x5dc
+#define A_MPS_PORT_STAT_RX_PORT_PPP2_L 0x5e0
+#define A_MPS_PORT_STAT_RX_PORT_PPP2_H 0x5e4
+#define A_MPS_PORT_STAT_RX_PORT_PPP3_L 0x5e8
+#define A_MPS_PORT_STAT_RX_PORT_PPP3_H 0x5ec
+#define A_MPS_PORT_STAT_RX_PORT_PPP4_L 0x5f0
+#define A_MPS_PORT_STAT_RX_PORT_PPP4_H 0x5f4
+#define A_MPS_PORT_STAT_RX_PORT_PPP5_L 0x5f8
+#define A_MPS_PORT_STAT_RX_PORT_PPP5_H 0x5fc
+#define A_MPS_PORT_STAT_RX_PORT_PPP6_L 0x600
+#define A_MPS_PORT_STAT_RX_PORT_PPP6_H 0x604
+#define A_MPS_PORT_STAT_RX_PORT_PPP7_L 0x608
+#define A_MPS_PORT_STAT_RX_PORT_PPP7_H 0x60c
+#define A_MPS_PORT_STAT_RX_PORT_LESS_64B_L 0x610
+#define A_MPS_PORT_STAT_RX_PORT_LESS_64B_H 0x614
+#define A_MPS_CMN_CTL 0x9000
+
+#define S_DETECT8023    3
+#define V_DETECT8023(x) ((x) << S_DETECT8023)
+#define F_DETECT8023    V_DETECT8023(1U)
+
+#define S_VFDIRECTACCESS    2
+#define V_VFDIRECTACCESS(x) ((x) << S_VFDIRECTACCESS)
+#define F_VFDIRECTACCESS    V_VFDIRECTACCESS(1U)
+
+#define S_NUMPORTS    0
+#define M_NUMPORTS    0x3U
+#define V_NUMPORTS(x) ((x) << S_NUMPORTS)
+#define G_NUMPORTS(x) (((x) >> S_NUMPORTS) & M_NUMPORTS)
+
+#define A_MPS_INT_ENABLE 0x9004
+
+#define S_STATINTENB    5
+#define V_STATINTENB(x) ((x) << S_STATINTENB)
+#define F_STATINTENB    V_STATINTENB(1U)
+
+#define S_TXINTENB    4
+#define V_TXINTENB(x) ((x) << S_TXINTENB)
+#define F_TXINTENB    V_TXINTENB(1U)
+
+#define S_RXINTENB    3
+#define V_RXINTENB(x) ((x) << S_RXINTENB)
+#define F_RXINTENB    V_RXINTENB(1U)
+
+#define S_TRCINTENB    2
+#define V_TRCINTENB(x) ((x) << S_TRCINTENB)
+#define F_TRCINTENB    V_TRCINTENB(1U)
+
+#define S_CLSINTENB    1
+#define V_CLSINTENB(x) ((x) << S_CLSINTENB)
+#define F_CLSINTENB    V_CLSINTENB(1U)
+
+#define S_PLINTENB    0
+#define V_PLINTENB(x) ((x) << S_PLINTENB)
+#define F_PLINTENB    V_PLINTENB(1U)
+
+#define A_MPS_INT_CAUSE 0x9008
+
+#define S_STATINT    5
+#define V_STATINT(x) ((x) << S_STATINT)
+#define F_STATINT    V_STATINT(1U)
+
+#define S_TXINT    4
+#define V_TXINT(x) ((x) << S_TXINT)
+#define F_TXINT    V_TXINT(1U)
+
+#define S_RXINT    3
+#define V_RXINT(x) ((x) << S_RXINT)
+#define F_RXINT    V_RXINT(1U)
+
+#define S_TRCINT    2
+#define V_TRCINT(x) ((x) << S_TRCINT)
+#define F_TRCINT    V_TRCINT(1U)
+
+#define S_CLSINT    1
+#define V_CLSINT(x) ((x) << S_CLSINT)
+#define F_CLSINT    V_CLSINT(1U)
+
+#define S_PLINT    0
+#define V_PLINT(x) ((x) << S_PLINT)
+#define F_PLINT    V_PLINT(1U)
+
+#define A_MPS_VF_TX_CTL_31_0 0x9010
+#define A_MPS_VF_TX_CTL_63_32 0x9014
+#define A_MPS_VF_TX_CTL_95_64 0x9018
+#define A_MPS_VF_TX_CTL_127_96 0x901c
+#define A_MPS_VF_RX_CTL_31_0 0x9020
+#define A_MPS_VF_RX_CTL_63_32 0x9024
+#define A_MPS_VF_RX_CTL_95_64 0x9028
+#define A_MPS_VF_RX_CTL_127_96 0x902c
+#define A_MPS_TX_PAUSE_DURATION_BUF_GRP0 0x9030
+
+#define S_VALUE    0
+#define M_VALUE    0xffffU
+#define V_VALUE(x) ((x) << S_VALUE)
+#define G_VALUE(x) (((x) >> S_VALUE) & M_VALUE)
+
+#define A_MPS_TX_PAUSE_DURATION_BUF_GRP1 0x9034
+#define A_MPS_TX_PAUSE_DURATION_BUF_GRP2 0x9038
+#define A_MPS_TX_PAUSE_DURATION_BUF_GRP3 0x903c
+#define A_MPS_TX_PAUSE_RETRANS_BUF_GRP0 0x9040
+#define A_MPS_TX_PAUSE_RETRANS_BUF_GRP1 0x9044
+#define A_MPS_TX_PAUSE_RETRANS_BUF_GRP2 0x9048
+#define A_MPS_TX_PAUSE_RETRANS_BUF_GRP3 0x904c
+#define A_MPS_TP_CSIDE_MUX_CTL_P0 0x9050
+
+#define S_WEIGHT    0
+#define M_WEIGHT    0xfffU
+#define V_WEIGHT(x) ((x) << S_WEIGHT)
+#define G_WEIGHT(x) (((x) >> S_WEIGHT) & M_WEIGHT)
+
+#define A_MPS_TP_CSIDE_MUX_CTL_P1 0x9054
+#define A_MPS_WOL_CTL_MODE 0x9058
+
+#define S_WOL_MODE    0
+#define V_WOL_MODE(x) ((x) << S_WOL_MODE)
+#define F_WOL_MODE    V_WOL_MODE(1U)
+
+#define A_MPS_FPGA_DEBUG 0x9060
+
+#define S_LPBK_EN    8
+#define V_LPBK_EN(x) ((x) << S_LPBK_EN)
+#define F_LPBK_EN    V_LPBK_EN(1U)
+
+#define S_CH_MAP3    6
+#define M_CH_MAP3    0x3U
+#define V_CH_MAP3(x) ((x) << S_CH_MAP3)
+#define G_CH_MAP3(x) (((x) >> S_CH_MAP3) & M_CH_MAP3)
+
+#define S_CH_MAP2    4
+#define M_CH_MAP2    0x3U
+#define V_CH_MAP2(x) ((x) << S_CH_MAP2)
+#define G_CH_MAP2(x) (((x) >> S_CH_MAP2) & M_CH_MAP2)
+
+#define S_CH_MAP1    2
+#define M_CH_MAP1    0x3U
+#define V_CH_MAP1(x) ((x) << S_CH_MAP1)
+#define G_CH_MAP1(x) (((x) >> S_CH_MAP1) & M_CH_MAP1)
+
+#define S_CH_MAP0    0
+#define M_CH_MAP0    0x3U
+#define V_CH_MAP0(x) ((x) << S_CH_MAP0)
+#define G_CH_MAP0(x) (((x) >> S_CH_MAP0) & M_CH_MAP0)
+
+#define A_MPS_DEBUG_CTL 0x9068
+
+#define S_DBGMODECTL_H    11
+#define V_DBGMODECTL_H(x) ((x) << S_DBGMODECTL_H)
+#define F_DBGMODECTL_H    V_DBGMODECTL_H(1U)
+
+#define S_DBGSEL_H    6
+#define M_DBGSEL_H    0x1fU
+#define V_DBGSEL_H(x) ((x) << S_DBGSEL_H)
+#define G_DBGSEL_H(x) (((x) >> S_DBGSEL_H) & M_DBGSEL_H)
+
+#define S_DBGMODECTL_L    5
+#define V_DBGMODECTL_L(x) ((x) << S_DBGMODECTL_L)
+#define F_DBGMODECTL_L    V_DBGMODECTL_L(1U)
+
+#define S_DBGSEL_L    0
+#define M_DBGSEL_L    0x1fU
+#define V_DBGSEL_L(x) ((x) << S_DBGSEL_L)
+#define G_DBGSEL_L(x) (((x) >> S_DBGSEL_L) & M_DBGSEL_L)
+
+#define A_MPS_DEBUG_DATA_REG_L 0x906c
+#define A_MPS_DEBUG_DATA_REG_H 0x9070
+#define A_MPS_TOP_SPARE 0x9074
+
+#define S_TOPSPARE    12
+#define M_TOPSPARE    0xfffffU
+#define V_TOPSPARE(x) ((x) << S_TOPSPARE)
+#define G_TOPSPARE(x) (((x) >> S_TOPSPARE) & M_TOPSPARE)
+
+#define S_CHIKN_14463    8
+#define M_CHIKN_14463    0xfU
+#define V_CHIKN_14463(x) ((x) << S_CHIKN_14463)
+#define G_CHIKN_14463(x) (((x) >> S_CHIKN_14463) & M_CHIKN_14463)
+
+#define S_OVLANSELLPBK3    7
+#define V_OVLANSELLPBK3(x) ((x) << S_OVLANSELLPBK3)
+#define F_OVLANSELLPBK3    V_OVLANSELLPBK3(1U)
+
+#define S_OVLANSELLPBK2    6
+#define V_OVLANSELLPBK2(x) ((x) << S_OVLANSELLPBK2)
+#define F_OVLANSELLPBK2    V_OVLANSELLPBK2(1U)
+
+#define S_OVLANSELLPBK1    5
+#define V_OVLANSELLPBK1(x) ((x) << S_OVLANSELLPBK1)
+#define F_OVLANSELLPBK1    V_OVLANSELLPBK1(1U)
+
+#define S_OVLANSELLPBK0    4
+#define V_OVLANSELLPBK0(x) ((x) << S_OVLANSELLPBK0)
+#define F_OVLANSELLPBK0    V_OVLANSELLPBK0(1U)
+
+#define S_OVLANSELMAC3    3
+#define V_OVLANSELMAC3(x) ((x) << S_OVLANSELMAC3)
+#define F_OVLANSELMAC3    V_OVLANSELMAC3(1U)
+
+#define S_OVLANSELMAC2    2
+#define V_OVLANSELMAC2(x) ((x) << S_OVLANSELMAC2)
+#define F_OVLANSELMAC2    V_OVLANSELMAC2(1U)
+
+#define S_OVLANSELMAC1    1
+#define V_OVLANSELMAC1(x) ((x) << S_OVLANSELMAC1)
+#define F_OVLANSELMAC1    V_OVLANSELMAC1(1U)
+
+#define S_OVLANSELMAC0    0
+#define V_OVLANSELMAC0(x) ((x) << S_OVLANSELMAC0)
+#define F_OVLANSELMAC0    V_OVLANSELMAC0(1U)
+
+#define A_MPS_BUILD_REVISION 0x90fc
+#define A_MPS_TX_PRTY_SEL 0x9400
+
+#define S_CH4_PRTY    20
+#define M_CH4_PRTY    0x7U
+#define V_CH4_PRTY(x) ((x) << S_CH4_PRTY)
+#define G_CH4_PRTY(x) (((x) >> S_CH4_PRTY) & M_CH4_PRTY)
+
+#define S_CH3_PRTY    16
+#define M_CH3_PRTY    0x7U
+#define V_CH3_PRTY(x) ((x) << S_CH3_PRTY)
+#define G_CH3_PRTY(x) (((x) >> S_CH3_PRTY) & M_CH3_PRTY)
+
+#define S_CH2_PRTY    12
+#define M_CH2_PRTY    0x7U
+#define V_CH2_PRTY(x) ((x) << S_CH2_PRTY)
+#define G_CH2_PRTY(x) (((x) >> S_CH2_PRTY) & M_CH2_PRTY)
+
+#define S_CH1_PRTY    8
+#define M_CH1_PRTY    0x7U
+#define V_CH1_PRTY(x) ((x) << S_CH1_PRTY)
+#define G_CH1_PRTY(x) (((x) >> S_CH1_PRTY) & M_CH1_PRTY)
+
+#define S_CH0_PRTY    4
+#define M_CH0_PRTY    0x7U
+#define V_CH0_PRTY(x) ((x) << S_CH0_PRTY)
+#define G_CH0_PRTY(x) (((x) >> S_CH0_PRTY) & M_CH0_PRTY)
+
+#define S_TP_SOURCE    2
+#define M_TP_SOURCE    0x3U
+#define V_TP_SOURCE(x) ((x) << S_TP_SOURCE)
+#define G_TP_SOURCE(x) (((x) >> S_TP_SOURCE) & M_TP_SOURCE)
+
+#define S_NCSI_SOURCE    0
+#define M_NCSI_SOURCE    0x3U
+#define V_NCSI_SOURCE(x) ((x) << S_NCSI_SOURCE)
+#define G_NCSI_SOURCE(x) (((x) >> S_NCSI_SOURCE) & M_NCSI_SOURCE)
+
+#define A_MPS_TX_INT_ENABLE 0x9404
+
+#define S_PORTERR    16
+#define V_PORTERR(x) ((x) << S_PORTERR)
+#define F_PORTERR    V_PORTERR(1U)
+
+#define S_FRMERR    15
+#define V_FRMERR(x) ((x) << S_FRMERR)
+#define F_FRMERR    V_FRMERR(1U)
+
+#define S_SECNTERR    14
+#define V_SECNTERR(x) ((x) << S_SECNTERR)
+#define F_SECNTERR    V_SECNTERR(1U)
+
+#define S_BUBBLE    13
+#define V_BUBBLE(x) ((x) << S_BUBBLE)
+#define F_BUBBLE    V_BUBBLE(1U)
+
+#define S_TXDESCFIFO    9
+#define M_TXDESCFIFO    0xfU
+#define V_TXDESCFIFO(x) ((x) << S_TXDESCFIFO)
+#define G_TXDESCFIFO(x) (((x) >> S_TXDESCFIFO) & M_TXDESCFIFO)
+
+#define S_TXDATAFIFO    5
+#define M_TXDATAFIFO    0xfU
+#define V_TXDATAFIFO(x) ((x) << S_TXDATAFIFO)
+#define G_TXDATAFIFO(x) (((x) >> S_TXDATAFIFO) & M_TXDATAFIFO)
+
+#define S_NCSIFIFO    4
+#define V_NCSIFIFO(x) ((x) << S_NCSIFIFO)
+#define F_NCSIFIFO    V_NCSIFIFO(1U)
+
+#define S_TPFIFO    0
+#define M_TPFIFO    0xfU
+#define V_TPFIFO(x) ((x) << S_TPFIFO)
+#define G_TPFIFO(x) (((x) >> S_TPFIFO) & M_TPFIFO)
+
+#define A_MPS_TX_INT_CAUSE 0x9408
+#define A_MPS_TX_PERR_ENABLE 0x9410
+#define A_MPS_TX_PERR_INJECT 0x9414
+
+#define S_MPSTXMEMSEL    1
+#define M_MPSTXMEMSEL    0x1fU
+#define V_MPSTXMEMSEL(x) ((x) << S_MPSTXMEMSEL)
+#define G_MPSTXMEMSEL(x) (((x) >> S_MPSTXMEMSEL) & M_MPSTXMEMSEL)
+
+#define A_MPS_TX_SE_CNT_TP01 0x9418
+#define A_MPS_TX_SE_CNT_TP23 0x941c
+#define A_MPS_TX_SE_CNT_MAC01 0x9420
+#define A_MPS_TX_SE_CNT_MAC23 0x9424
+#define A_MPS_TX_SECNT_SPI_BUBBLE_ERR 0x9428
+
+#define S_BUBBLEERR    16
+#define M_BUBBLEERR    0xffU
+#define V_BUBBLEERR(x) ((x) << S_BUBBLEERR)
+#define G_BUBBLEERR(x) (((x) >> S_BUBBLEERR) & M_BUBBLEERR)
+
+#define S_SPI    8
+#define M_SPI    0xffU
+#define V_SPI(x) ((x) << S_SPI)
+#define G_SPI(x) (((x) >> S_SPI) & M_SPI)
+
+#define S_SECNT    0
+#define M_SECNT    0xffU
+#define V_SECNT(x) ((x) << S_SECNT)
+#define G_SECNT(x) (((x) >> S_SECNT) & M_SECNT)
+
+#define A_MPS_TX_SECNT_BUBBLE_CLR 0x942c
+
+#define S_BUBBLECLR    8
+#define M_BUBBLECLR    0xffU
+#define V_BUBBLECLR(x) ((x) << S_BUBBLECLR)
+#define G_BUBBLECLR(x) (((x) >> S_BUBBLECLR) & M_BUBBLECLR)
+
+#define A_MPS_TX_PORT_ERR 0x9430
+
+#define S_LPBKPT3    7
+#define V_LPBKPT3(x) ((x) << S_LPBKPT3)
+#define F_LPBKPT3    V_LPBKPT3(1U)
+
+#define S_LPBKPT2    6
+#define V_LPBKPT2(x) ((x) << S_LPBKPT2)
+#define F_LPBKPT2    V_LPBKPT2(1U)
+
+#define S_LPBKPT1    5
+#define V_LPBKPT1(x) ((x) << S_LPBKPT1)
+#define F_LPBKPT1    V_LPBKPT1(1U)
+
+#define S_LPBKPT0    4
+#define V_LPBKPT0(x) ((x) << S_LPBKPT0)
+#define F_LPBKPT0    V_LPBKPT0(1U)
+
+#define S_PT3    3
+#define V_PT3(x) ((x) << S_PT3)
+#define F_PT3    V_PT3(1U)
+
+#define S_PT2    2
+#define V_PT2(x) ((x) << S_PT2)
+#define F_PT2    V_PT2(1U)
+
+#define S_PT1    1
+#define V_PT1(x) ((x) << S_PT1)
+#define F_PT1    V_PT1(1U)
+
+#define S_PT0    0
+#define V_PT0(x) ((x) << S_PT0)
+#define F_PT0    V_PT0(1U)
+
+#define A_MPS_TX_LPBK_DROP_BP_CTL_CH0 0x9434
+
+#define S_BPEN    1
+#define V_BPEN(x) ((x) << S_BPEN)
+#define F_BPEN    V_BPEN(1U)
+
+#define S_DROPEN    0
+#define V_DROPEN(x) ((x) << S_DROPEN)
+#define F_DROPEN    V_DROPEN(1U)
+
+#define A_MPS_TX_LPBK_DROP_BP_CTL_CH1 0x9438
+#define A_MPS_TX_LPBK_DROP_BP_CTL_CH2 0x943c
+#define A_MPS_TX_LPBK_DROP_BP_CTL_CH3 0x9440
+#define A_MPS_TX_DEBUG_REG_TP2TX_10 0x9444
+
+#define S_SOPCH1    31
+#define V_SOPCH1(x) ((x) << S_SOPCH1)
+#define F_SOPCH1    V_SOPCH1(1U)
+
+#define S_EOPCH1    30
+#define V_EOPCH1(x) ((x) << S_EOPCH1)
+#define F_EOPCH1    V_EOPCH1(1U)
+
+#define S_SIZECH1    27
+#define M_SIZECH1    0x7U
+#define V_SIZECH1(x) ((x) << S_SIZECH1)
+#define G_SIZECH1(x) (((x) >> S_SIZECH1) & M_SIZECH1)
+
+#define S_ERRCH1    26
+#define V_ERRCH1(x) ((x) << S_ERRCH1)
+#define F_ERRCH1    V_ERRCH1(1U)
+
+#define S_FULLCH1    25
+#define V_FULLCH1(x) ((x) << S_FULLCH1)
+#define F_FULLCH1    V_FULLCH1(1U)
+
+#define S_VALIDCH1    24
+#define V_VALIDCH1(x) ((x) << S_VALIDCH1)
+#define F_VALIDCH1    V_VALIDCH1(1U)
+
+#define S_DATACH1    16
+#define M_DATACH1    0xffU
+#define V_DATACH1(x) ((x) << S_DATACH1)
+#define G_DATACH1(x) (((x) >> S_DATACH1) & M_DATACH1)
+
+#define S_SOPCH0    15
+#define V_SOPCH0(x) ((x) << S_SOPCH0)
+#define F_SOPCH0    V_SOPCH0(1U)
+
+#define S_EOPCH0    14
+#define V_EOPCH0(x) ((x) << S_EOPCH0)
+#define F_EOPCH0    V_EOPCH0(1U)
+
+#define S_SIZECH0    11
+#define M_SIZECH0    0x7U
+#define V_SIZECH0(x) ((x) << S_SIZECH0)
+#define G_SIZECH0(x) (((x) >> S_SIZECH0) & M_SIZECH0)
+
+#define S_ERRCH0    10
+#define V_ERRCH0(x) ((x) << S_ERRCH0)
+#define F_ERRCH0    V_ERRCH0(1U)
+
+#define S_FULLCH0    9
+#define V_FULLCH0(x) ((x) << S_FULLCH0)
+#define F_FULLCH0    V_FULLCH0(1U)
+
+#define S_VALIDCH0    8
+#define V_VALIDCH0(x) ((x) << S_VALIDCH0)
+#define F_VALIDCH0    V_VALIDCH0(1U)
+
+#define S_DATACH0    0
+#define M_DATACH0    0xffU
+#define V_DATACH0(x) ((x) << S_DATACH0)
+#define G_DATACH0(x) (((x) >> S_DATACH0) & M_DATACH0)
+
+#define A_MPS_TX_DEBUG_REG_TP2TX_32 0x9448
+
+#define S_SOPCH3    31
+#define V_SOPCH3(x) ((x) << S_SOPCH3)
+#define F_SOPCH3    V_SOPCH3(1U)
+
+#define S_EOPCH3    30
+#define V_EOPCH3(x) ((x) << S_EOPCH3)
+#define F_EOPCH3    V_EOPCH3(1U)
+
+#define S_SIZECH3    27
+#define M_SIZECH3    0x7U
+#define V_SIZECH3(x) ((x) << S_SIZECH3)
+#define G_SIZECH3(x) (((x) >> S_SIZECH3) & M_SIZECH3)
+
+#define S_ERRCH3    26
+#define V_ERRCH3(x) ((x) << S_ERRCH3)
+#define F_ERRCH3    V_ERRCH3(1U)
+
+#define S_FULLCH3    25
+#define V_FULLCH3(x) ((x) << S_FULLCH3)
+#define F_FULLCH3    V_FULLCH3(1U)
+
+#define S_VALIDCH3    24
+#define V_VALIDCH3(x) ((x) << S_VALIDCH3)
+#define F_VALIDCH3    V_VALIDCH3(1U)
+
+#define S_DATACH3    16
+#define M_DATACH3    0xffU
+#define V_DATACH3(x) ((x) << S_DATACH3)
+#define G_DATACH3(x) (((x) >> S_DATACH3) & M_DATACH3)
+
+#define S_SOPCH2    15
+#define V_SOPCH2(x) ((x) << S_SOPCH2)
+#define F_SOPCH2    V_SOPCH2(1U)
+
+#define S_EOPCH2    14
+#define V_EOPCH2(x) ((x) << S_EOPCH2)
+#define F_EOPCH2    V_EOPCH2(1U)
+
+#define S_SIZECH2    11
+#define M_SIZECH2    0x7U
+#define V_SIZECH2(x) ((x) << S_SIZECH2)
+#define G_SIZECH2(x) (((x) >> S_SIZECH2) & M_SIZECH2)
+
+#define S_ERRCH2    10
+#define V_ERRCH2(x) ((x) << S_ERRCH2)
+#define F_ERRCH2    V_ERRCH2(1U)
+
+#define S_FULLCH2    9
+#define V_FULLCH2(x) ((x) << S_FULLCH2)
+#define F_FULLCH2    V_FULLCH2(1U)
+
+#define S_VALIDCH2    8
+#define V_VALIDCH2(x) ((x) << S_VALIDCH2)
+#define F_VALIDCH2    V_VALIDCH2(1U)
+
+#define S_DATACH2    0
+#define M_DATACH2    0xffU
+#define V_DATACH2(x) ((x) << S_DATACH2)
+#define G_DATACH2(x) (((x) >> S_DATACH2) & M_DATACH2)
+
+#define A_MPS_TX_DEBUG_REG_TX2MAC_10 0x944c
+
+#define S_SOPPT1    31
+#define V_SOPPT1(x) ((x) << S_SOPPT1)
+#define F_SOPPT1    V_SOPPT1(1U)
+
+#define S_EOPPT1    30
+#define V_EOPPT1(x) ((x) << S_EOPPT1)
+#define F_EOPPT1    V_EOPPT1(1U)
+
+#define S_SIZEPT1    27
+#define M_SIZEPT1    0x7U
+#define V_SIZEPT1(x) ((x) << S_SIZEPT1)
+#define G_SIZEPT1(x) (((x) >> S_SIZEPT1) & M_SIZEPT1)
+
+#define S_ERRPT1    26
+#define V_ERRPT1(x) ((x) << S_ERRPT1)
+#define F_ERRPT1    V_ERRPT1(1U)
+
+#define S_FULLPT1    25
+#define V_FULLPT1(x) ((x) << S_FULLPT1)
+#define F_FULLPT1    V_FULLPT1(1U)
+
+#define S_VALIDPT1    24
+#define V_VALIDPT1(x) ((x) << S_VALIDPT1)
+#define F_VALIDPT1    V_VALIDPT1(1U)
+
+#define S_DATAPT1    16
+#define M_DATAPT1    0xffU
+#define V_DATAPT1(x) ((x) << S_DATAPT1)
+#define G_DATAPT1(x) (((x) >> S_DATAPT1) & M_DATAPT1)
+
+#define S_SOPPT0    15
+#define V_SOPPT0(x) ((x) << S_SOPPT0)
+#define F_SOPPT0    V_SOPPT0(1U)
+
+#define S_EOPPT0    14
+#define V_EOPPT0(x) ((x) << S_EOPPT0)
+#define F_EOPPT0    V_EOPPT0(1U)
+
+#define S_SIZEPT0    11
+#define M_SIZEPT0    0x7U
+#define V_SIZEPT0(x) ((x) << S_SIZEPT0)
+#define G_SIZEPT0(x) (((x) >> S_SIZEPT0) & M_SIZEPT0)
+
+#define S_ERRPT0    10
+#define V_ERRPT0(x) ((x) << S_ERRPT0)
+#define F_ERRPT0    V_ERRPT0(1U)
+
+#define S_FULLPT0    9
+#define V_FULLPT0(x) ((x) << S_FULLPT0)
+#define F_FULLPT0    V_FULLPT0(1U)
+
+#define S_VALIDPT0    8
+#define V_VALIDPT0(x) ((x) << S_VALIDPT0)
+#define F_VALIDPT0    V_VALIDPT0(1U)
+
+#define S_DATAPT0    0
+#define M_DATAPT0    0xffU
+#define V_DATAPT0(x) ((x) << S_DATAPT0)
+#define G_DATAPT0(x) (((x) >> S_DATAPT0) & M_DATAPT0)
+
+#define A_MPS_TX_DEBUG_REG_TX2MAC_32 0x9450
+
+#define S_SOPPT3    31
+#define V_SOPPT3(x) ((x) << S_SOPPT3)
+#define F_SOPPT3    V_SOPPT3(1U)
+
+#define S_EOPPT3    30
+#define V_EOPPT3(x) ((x) << S_EOPPT3)
+#define F_EOPPT3    V_EOPPT3(1U)
+
+#define S_SIZEPT3    27
+#define M_SIZEPT3    0x7U
+#define V_SIZEPT3(x) ((x) << S_SIZEPT3)
+#define G_SIZEPT3(x) (((x) >> S_SIZEPT3) & M_SIZEPT3)
+
+#define S_ERRPT3    26
+#define V_ERRPT3(x) ((x) << S_ERRPT3)
+#define F_ERRPT3    V_ERRPT3(1U)
+
+#define S_FULLPT3    25
+#define V_FULLPT3(x) ((x) << S_FULLPT3)
+#define F_FULLPT3    V_FULLPT3(1U)
+
+#define S_VALIDPT3    24
+#define V_VALIDPT3(x) ((x) << S_VALIDPT3)
+#define F_VALIDPT3    V_VALIDPT3(1U)
+
+#define S_DATAPT3    16
+#define M_DATAPT3    0xffU
+#define V_DATAPT3(x) ((x) << S_DATAPT3)
+#define G_DATAPT3(x) (((x) >> S_DATAPT3) & M_DATAPT3)
+
+#define S_SOPPT2    15
+#define V_SOPPT2(x) ((x) << S_SOPPT2)
+#define F_SOPPT2    V_SOPPT2(1U)
+
+#define S_EOPPT2    14
+#define V_EOPPT2(x) ((x) << S_EOPPT2)
+#define F_EOPPT2    V_EOPPT2(1U)
+
+#define S_SIZEPT2    11
+#define M_SIZEPT2    0x7U
+#define V_SIZEPT2(x) ((x) << S_SIZEPT2)
+#define G_SIZEPT2(x) (((x) >> S_SIZEPT2) & M_SIZEPT2)
+
+#define S_ERRPT2    10
+#define V_ERRPT2(x) ((x) << S_ERRPT2)
+#define F_ERRPT2    V_ERRPT2(1U)
+
+#define S_FULLPT2    9
+#define V_FULLPT2(x) ((x) << S_FULLPT2)
+#define F_FULLPT2    V_FULLPT2(1U)
+
+#define S_VALIDPT2    8
+#define V_VALIDPT2(x) ((x) << S_VALIDPT2)
+#define F_VALIDPT2    V_VALIDPT2(1U)
+
+#define S_DATAPT2    0
+#define M_DATAPT2    0xffU
+#define V_DATAPT2(x) ((x) << S_DATAPT2)
+#define G_DATAPT2(x) (((x) >> S_DATAPT2) & M_DATAPT2)
+
+#define A_MPS_TX_SGE_CH_PAUSE_IGNR 0x9454
+
+#define S_SGEPAUSEIGNR    0
+#define M_SGEPAUSEIGNR    0xfU
+#define V_SGEPAUSEIGNR(x) ((x) << S_SGEPAUSEIGNR)
+#define G_SGEPAUSEIGNR(x) (((x) >> S_SGEPAUSEIGNR) & M_SGEPAUSEIGNR)
+
+#define A_MPS_TX_DEBUG_SUBPART_SEL 0x9458
+
+#define S_SUBPRTH    11
+#define M_SUBPRTH    0x1fU
+#define V_SUBPRTH(x) ((x) << S_SUBPRTH)
+#define G_SUBPRTH(x) (((x) >> S_SUBPRTH) & M_SUBPRTH)
+
+#define S_PORTH    8
+#define M_PORTH    0x7U
+#define V_PORTH(x) ((x) << S_PORTH)
+#define G_PORTH(x) (((x) >> S_PORTH) & M_PORTH)
+
+#define S_SUBPRTL    3
+#define M_SUBPRTL    0x1fU
+#define V_SUBPRTL(x) ((x) << S_SUBPRTL)
+#define G_SUBPRTL(x) (((x) >> S_SUBPRTL) & M_SUBPRTL)
+
+#define S_PORTL    0
+#define M_PORTL    0x7U
+#define V_PORTL(x) ((x) << S_PORTL)
+#define G_PORTL(x) (((x) >> S_PORTL) & M_PORTL)
+
+#define A_MPS_STAT_CTL 0x9600
+
+#define S_COUNTVFINPF    1
+#define V_COUNTVFINPF(x) ((x) << S_COUNTVFINPF)
+#define F_COUNTVFINPF    V_COUNTVFINPF(1U)
+
+#define S_LPBKERRSTAT    0
+#define V_LPBKERRSTAT(x) ((x) << S_LPBKERRSTAT)
+#define F_LPBKERRSTAT    V_LPBKERRSTAT(1U)
+
+#define A_MPS_STAT_INT_ENABLE 0x9608
+
+#define S_PLREADSYNCERR    0
+#define V_PLREADSYNCERR(x) ((x) << S_PLREADSYNCERR)
+#define F_PLREADSYNCERR    V_PLREADSYNCERR(1U)
+
+#define A_MPS_STAT_INT_CAUSE 0x960c
+#define A_MPS_STAT_PERR_INT_ENABLE_SRAM 0x9610
+
+#define S_RXBG    20
+#define V_RXBG(x) ((x) << S_RXBG)
+#define F_RXBG    V_RXBG(1U)
+
+#define S_RXVF    18
+#define M_RXVF    0x3U
+#define V_RXVF(x) ((x) << S_RXVF)
+#define G_RXVF(x) (((x) >> S_RXVF) & M_RXVF)
+
+#define S_TXVF    16
+#define M_TXVF    0x3U
+#define V_TXVF(x) ((x) << S_TXVF)
+#define G_TXVF(x) (((x) >> S_TXVF) & M_TXVF)
+
+#define S_RXPF    13
+#define M_RXPF    0x7U
+#define V_RXPF(x) ((x) << S_RXPF)
+#define G_RXPF(x) (((x) >> S_RXPF) & M_RXPF)
+
+#define S_TXPF    11
+#define M_TXPF    0x3U
+#define V_TXPF(x) ((x) << S_TXPF)
+#define G_TXPF(x) (((x) >> S_TXPF) & M_TXPF)
+
+#define S_RXPORT    7
+#define M_RXPORT    0xfU
+#define V_RXPORT(x) ((x) << S_RXPORT)
+#define G_RXPORT(x) (((x) >> S_RXPORT) & M_RXPORT)
+
+#define S_LBPORT    4
+#define M_LBPORT    0x7U
+#define V_LBPORT(x) ((x) << S_LBPORT)
+#define G_LBPORT(x) (((x) >> S_LBPORT) & M_LBPORT)
+
+#define S_TXPORT    0
+#define M_TXPORT    0xfU
+#define V_TXPORT(x) ((x) << S_TXPORT)
+#define G_TXPORT(x) (((x) >> S_TXPORT) & M_TXPORT)
+
+#define A_MPS_STAT_PERR_INT_CAUSE_SRAM 0x9614
+#define A_MPS_STAT_PERR_ENABLE_SRAM 0x9618
+#define A_MPS_STAT_PERR_INT_ENABLE_TX_FIFO 0x961c
+
+#define S_TX    12
+#define M_TX    0xffU
+#define V_TX(x) ((x) << S_TX)
+#define G_TX(x) (((x) >> S_TX) & M_TX)
+
+#define S_TXPAUSEFIFO    8
+#define M_TXPAUSEFIFO    0xfU
+#define V_TXPAUSEFIFO(x) ((x) << S_TXPAUSEFIFO)
+#define G_TXPAUSEFIFO(x) (((x) >> S_TXPAUSEFIFO) & M_TXPAUSEFIFO)
+
+#define S_DROP    0
+#define M_DROP    0xffU
+#define V_DROP(x) ((x) << S_DROP)
+#define G_DROP(x) (((x) >> S_DROP) & M_DROP)
+
+#define A_MPS_STAT_PERR_INT_CAUSE_TX_FIFO 0x9620
+#define A_MPS_STAT_PERR_ENABLE_TX_FIFO 0x9624
+#define A_MPS_STAT_PERR_INT_ENABLE_RX_FIFO 0x9628
+
+#define S_PAUSEFIFO    20
+#define M_PAUSEFIFO    0xfU
+#define V_PAUSEFIFO(x) ((x) << S_PAUSEFIFO)
+#define G_PAUSEFIFO(x) (((x) >> S_PAUSEFIFO) & M_PAUSEFIFO)
+
+#define S_LPBK    16
+#define M_LPBK    0xfU
+#define V_LPBK(x) ((x) << S_LPBK)
+#define G_LPBK(x) (((x) >> S_LPBK) & M_LPBK)
+
+#define S_NQ    8
+#define M_NQ    0xffU
+#define V_NQ(x) ((x) << S_NQ)
+#define G_NQ(x) (((x) >> S_NQ) & M_NQ)
+
+#define S_PV    4
+#define M_PV    0xfU
+#define V_PV(x) ((x) << S_PV)
+#define G_PV(x) (((x) >> S_PV) & M_PV)
+
+#define S_MAC    0
+#define M_MAC    0xfU
+#define V_MAC(x) ((x) << S_MAC)
+#define G_MAC(x) (((x) >> S_MAC) & M_MAC)
+
+#define A_MPS_STAT_PERR_INT_CAUSE_RX_FIFO 0x962c
+#define A_MPS_STAT_PERR_ENABLE_RX_FIFO 0x9630
+#define A_MPS_STAT_PERR_INJECT 0x9634
+
+#define S_STATMEMSEL    1
+#define M_STATMEMSEL    0x7fU
+#define V_STATMEMSEL(x) ((x) << S_STATMEMSEL)
+#define G_STATMEMSEL(x) (((x) >> S_STATMEMSEL) & M_STATMEMSEL)
+
+#define A_MPS_STAT_DEBUG_SUB_SEL 0x9638
+#define A_MPS_STAT_RX_BG_0_MAC_DROP_FRAME_L 0x9640
+#define A_MPS_STAT_RX_BG_0_MAC_DROP_FRAME_H 0x9644
+#define A_MPS_STAT_RX_BG_1_MAC_DROP_FRAME_L 0x9648
+#define A_MPS_STAT_RX_BG_1_MAC_DROP_FRAME_H 0x964c
+#define A_MPS_STAT_RX_BG_2_MAC_DROP_FRAME_L 0x9650
+#define A_MPS_STAT_RX_BG_2_MAC_DROP_FRAME_H 0x9654
+#define A_MPS_STAT_RX_BG_3_MAC_DROP_FRAME_L 0x9658
+#define A_MPS_STAT_RX_BG_3_MAC_DROP_FRAME_H 0x965c
+#define A_MPS_STAT_RX_BG_0_LB_DROP_FRAME_L 0x9660
+#define A_MPS_STAT_RX_BG_0_LB_DROP_FRAME_H 0x9664
+#define A_MPS_STAT_RX_BG_1_LB_DROP_FRAME_L 0x9668
+#define A_MPS_STAT_RX_BG_1_LB_DROP_FRAME_H 0x966c
+#define A_MPS_STAT_RX_BG_2_LB_DROP_FRAME_L 0x9670
+#define A_MPS_STAT_RX_BG_2_LB_DROP_FRAME_H 0x9674
+#define A_MPS_STAT_RX_BG_3_LB_DROP_FRAME_L 0x9678
+#define A_MPS_STAT_RX_BG_3_LB_DROP_FRAME_H 0x967c
+#define A_MPS_STAT_RX_BG_0_MAC_TRUNC_FRAME_L 0x9680
+#define A_MPS_STAT_RX_BG_0_MAC_TRUNC_FRAME_H 0x9684
+#define A_MPS_STAT_RX_BG_1_MAC_TRUNC_FRAME_L 0x9688
+#define A_MPS_STAT_RX_BG_1_MAC_TRUNC_FRAME_H 0x968c
+#define A_MPS_STAT_RX_BG_2_MAC_TRUNC_FRAME_L 0x9690
+#define A_MPS_STAT_RX_BG_2_MAC_TRUNC_FRAME_H 0x9694
+#define A_MPS_STAT_RX_BG_3_MAC_TRUNC_FRAME_L 0x9698
+#define A_MPS_STAT_RX_BG_3_MAC_TRUNC_FRAME_H 0x969c
+#define A_MPS_STAT_RX_BG_0_LB_TRUNC_FRAME_L 0x96a0
+#define A_MPS_STAT_RX_BG_0_LB_TRUNC_FRAME_H 0x96a4
+#define A_MPS_STAT_RX_BG_1_LB_TRUNC_FRAME_L 0x96a8
+#define A_MPS_STAT_RX_BG_1_LB_TRUNC_FRAME_H 0x96ac
+#define A_MPS_STAT_RX_BG_2_LB_TRUNC_FRAME_L 0x96b0
+#define A_MPS_STAT_RX_BG_2_LB_TRUNC_FRAME_H 0x96b4
+#define A_MPS_STAT_RX_BG_3_LB_TRUNC_FRAME_L 0x96b8
+#define A_MPS_STAT_RX_BG_3_LB_TRUNC_FRAME_H 0x96bc
+#define A_MPS_TRC_CFG 0x9800
+
+#define S_TRCFIFOEMPTY    4
+#define V_TRCFIFOEMPTY(x) ((x) << S_TRCFIFOEMPTY)
+#define F_TRCFIFOEMPTY    V_TRCFIFOEMPTY(1U)
+
+#define S_TRCIGNOREDROPINPUT    3
+#define V_TRCIGNOREDROPINPUT(x) ((x) << S_TRCIGNOREDROPINPUT)
+#define F_TRCIGNOREDROPINPUT    V_TRCIGNOREDROPINPUT(1U)
+
+#define S_TRCKEEPDUPLICATES    2
+#define V_TRCKEEPDUPLICATES(x) ((x) << S_TRCKEEPDUPLICATES)
+#define F_TRCKEEPDUPLICATES    V_TRCKEEPDUPLICATES(1U)
+
+#define S_TRCEN    1
+#define V_TRCEN(x) ((x) << S_TRCEN)
+#define F_TRCEN    V_TRCEN(1U)
+
+#define S_TRCMULTIFILTER    0
+#define V_TRCMULTIFILTER(x) ((x) << S_TRCMULTIFILTER)
+#define F_TRCMULTIFILTER    V_TRCMULTIFILTER(1U)
+
+#define A_MPS_TRC_RSS_HASH 0x9804
+#define A_MPS_TRC_RSS_CONTROL 0x9808
+
+#define S_RSSCONTROL    16
+#define M_RSSCONTROL    0xffU
+#define V_RSSCONTROL(x) ((x) << S_RSSCONTROL)
+#define G_RSSCONTROL(x) (((x) >> S_RSSCONTROL) & M_RSSCONTROL)
+
+#define S_QUEUENUMBER    0
+#define M_QUEUENUMBER    0xffffU
+#define V_QUEUENUMBER(x) ((x) << S_QUEUENUMBER)
+#define G_QUEUENUMBER(x) (((x) >> S_QUEUENUMBER) & M_QUEUENUMBER)
+
+#define A_MPS_TRC_FILTER_MATCH_CTL_A 0x9810
+
+#define S_TFINVERTMATCH    24
+#define V_TFINVERTMATCH(x) ((x) << S_TFINVERTMATCH)
+#define F_TFINVERTMATCH    V_TFINVERTMATCH(1U)
+
+#define S_TFPKTTOOLARGE    23
+#define V_TFPKTTOOLARGE(x) ((x) << S_TFPKTTOOLARGE)
+#define F_TFPKTTOOLARGE    V_TFPKTTOOLARGE(1U)
+
+#define S_TFEN    22
+#define V_TFEN(x) ((x) << S_TFEN)
+#define F_TFEN    V_TFEN(1U)
+
+#define S_TFPORT    18
+#define M_TFPORT    0xfU
+#define V_TFPORT(x) ((x) << S_TFPORT)
+#define G_TFPORT(x) (((x) >> S_TFPORT) & M_TFPORT)
+
+#define S_TFDROP    17
+#define V_TFDROP(x) ((x) << S_TFDROP)
+#define F_TFDROP    V_TFDROP(1U)
+
+#define S_TFSOPEOPERR    16
+#define V_TFSOPEOPERR(x) ((x) << S_TFSOPEOPERR)
+#define F_TFSOPEOPERR    V_TFSOPEOPERR(1U)
+
+#define S_TFLENGTH    8
+#define M_TFLENGTH    0x1fU
+#define V_TFLENGTH(x) ((x) << S_TFLENGTH)
+#define G_TFLENGTH(x) (((x) >> S_TFLENGTH) & M_TFLENGTH)
+
+#define S_TFOFFSET    0
+#define M_TFOFFSET    0x1fU
+#define V_TFOFFSET(x) ((x) << S_TFOFFSET)
+#define G_TFOFFSET(x) (((x) >> S_TFOFFSET) & M_TFOFFSET)
+
+#define A_MPS_TRC_FILTER_MATCH_CTL_B 0x9820
+
+#define S_TFMINPKTSIZE    16
+#define M_TFMINPKTSIZE    0x1ffU
+#define V_TFMINPKTSIZE(x) ((x) << S_TFMINPKTSIZE)
+#define G_TFMINPKTSIZE(x) (((x) >> S_TFMINPKTSIZE) & M_TFMINPKTSIZE)
+
+#define S_TFCAPTUREMAX    0
+#define M_TFCAPTUREMAX    0x3fffU
+#define V_TFCAPTUREMAX(x) ((x) << S_TFCAPTUREMAX)
+#define G_TFCAPTUREMAX(x) (((x) >> S_TFCAPTUREMAX) & M_TFCAPTUREMAX)
+
+#define A_MPS_TRC_FILTER_RUNT_CTL 0x9830
+
+#define S_TFRUNTSIZE    0
+#define M_TFRUNTSIZE    0x3fU
+#define V_TFRUNTSIZE(x) ((x) << S_TFRUNTSIZE)
+#define G_TFRUNTSIZE(x) (((x) >> S_TFRUNTSIZE) & M_TFRUNTSIZE)
+
+#define A_MPS_TRC_FILTER_DROP 0x9840
+
+#define S_TFDROPINPCOUNT    16
+#define M_TFDROPINPCOUNT    0xffffU
+#define V_TFDROPINPCOUNT(x) ((x) << S_TFDROPINPCOUNT)
+#define G_TFDROPINPCOUNT(x) (((x) >> S_TFDROPINPCOUNT) & M_TFDROPINPCOUNT)
+
+#define S_TFDROPBUFFERCOUNT    0
+#define M_TFDROPBUFFERCOUNT    0xffffU
+#define V_TFDROPBUFFERCOUNT(x) ((x) << S_TFDROPBUFFERCOUNT)
+#define G_TFDROPBUFFERCOUNT(x) (((x) >> S_TFDROPBUFFERCOUNT) & M_TFDROPBUFFERCOUNT)
+
+#define A_MPS_TRC_PERR_INJECT 0x9850
+
+#define S_TRCMEMSEL    1
+#define M_TRCMEMSEL    0xfU
+#define V_TRCMEMSEL(x) ((x) << S_TRCMEMSEL)
+#define G_TRCMEMSEL(x) (((x) >> S_TRCMEMSEL) & M_TRCMEMSEL)
+
+#define A_MPS_TRC_PERR_ENABLE 0x9854
+
+#define S_MISCPERR    8
+#define V_MISCPERR(x) ((x) << S_MISCPERR)
+#define F_MISCPERR    V_MISCPERR(1U)
+
+#define S_PKTFIFO    4
+#define M_PKTFIFO    0xfU
+#define V_PKTFIFO(x) ((x) << S_PKTFIFO)
+#define G_PKTFIFO(x) (((x) >> S_PKTFIFO) & M_PKTFIFO)
+
+#define S_FILTMEM    0
+#define M_FILTMEM    0xfU
+#define V_FILTMEM(x) ((x) << S_FILTMEM)
+#define G_FILTMEM(x) (((x) >> S_FILTMEM) & M_FILTMEM)
+
+#define A_MPS_TRC_INT_ENABLE 0x9858
+
+#define S_TRCPLERRENB    9
+#define V_TRCPLERRENB(x) ((x) << S_TRCPLERRENB)
+#define F_TRCPLERRENB    V_TRCPLERRENB(1U)
+
+#define A_MPS_TRC_INT_CAUSE 0x985c
+#define A_MPS_TRC_TIMESTAMP_L 0x9860
+#define A_MPS_TRC_TIMESTAMP_H 0x9864
+#define A_MPS_TRC_FILTER0_MATCH 0x9c00
+#define A_MPS_TRC_FILTER0_DONT_CARE 0x9c80
+#define A_MPS_TRC_FILTER1_MATCH 0x9d00
+#define A_MPS_TRC_FILTER1_DONT_CARE 0x9d80
+#define A_MPS_TRC_FILTER2_MATCH 0x9e00
+#define A_MPS_TRC_FILTER2_DONT_CARE 0x9e80
+#define A_MPS_TRC_FILTER3_MATCH 0x9f00
+#define A_MPS_TRC_FILTER3_DONT_CARE 0x9f80
+#define A_MPS_CLS_CTL 0xd000
+
+#define S_MEMWRITEFAULT    4
+#define V_MEMWRITEFAULT(x) ((x) << S_MEMWRITEFAULT)
+#define F_MEMWRITEFAULT    V_MEMWRITEFAULT(1U)
+
+#define S_MEMWRITEWAITING    3
+#define V_MEMWRITEWAITING(x) ((x) << S_MEMWRITEWAITING)
+#define F_MEMWRITEWAITING    V_MEMWRITEWAITING(1U)
+
+#define S_CIMNOPROMISCUOUS    2
+#define V_CIMNOPROMISCUOUS(x) ((x) << S_CIMNOPROMISCUOUS)
+#define F_CIMNOPROMISCUOUS    V_CIMNOPROMISCUOUS(1U)
+
+#define S_HYPERVISORONLY    1
+#define V_HYPERVISORONLY(x) ((x) << S_HYPERVISORONLY)
+#define F_HYPERVISORONLY    V_HYPERVISORONLY(1U)
+
+#define S_VLANCLSEN    0
+#define V_VLANCLSEN(x) ((x) << S_VLANCLSEN)
+#define F_VLANCLSEN    V_VLANCLSEN(1U)
+
+#define A_MPS_CLS_ARB_WEIGHT 0xd004
+
+#define S_PLWEIGHT    16
+#define M_PLWEIGHT    0x1fU
+#define V_PLWEIGHT(x) ((x) << S_PLWEIGHT)
+#define G_PLWEIGHT(x) (((x) >> S_PLWEIGHT) & M_PLWEIGHT)
+
+#define S_CIMWEIGHT    8
+#define M_CIMWEIGHT    0x1fU
+#define V_CIMWEIGHT(x) ((x) << S_CIMWEIGHT)
+#define G_CIMWEIGHT(x) (((x) >> S_CIMWEIGHT) & M_CIMWEIGHT)
+
+#define S_LPBKWEIGHT    0
+#define M_LPBKWEIGHT    0x1fU
+#define V_LPBKWEIGHT(x) ((x) << S_LPBKWEIGHT)
+#define G_LPBKWEIGHT(x) (((x) >> S_LPBKWEIGHT) & M_LPBKWEIGHT)
+
+#define A_MPS_CLS_BMC_MAC_ADDR_L 0xd010
+#define A_MPS_CLS_BMC_MAC_ADDR_H 0xd014
+#define A_MPS_CLS_BMC_VLAN 0xd018
+#define A_MPS_CLS_PERR_INJECT 0xd01c
+
+#define S_CLS_MEMSEL    1
+#define M_CLS_MEMSEL    0x3U
+#define V_CLS_MEMSEL(x) ((x) << S_CLS_MEMSEL)
+#define G_CLS_MEMSEL(x) (((x) >> S_CLS_MEMSEL) & M_CLS_MEMSEL)
+
+#define A_MPS_CLS_PERR_ENABLE 0xd020
+
+#define S_HASHSRAM    2
+#define V_HASHSRAM(x) ((x) << S_HASHSRAM)
+#define F_HASHSRAM    V_HASHSRAM(1U)
+
+#define S_MATCHTCAM    1
+#define V_MATCHTCAM(x) ((x) << S_MATCHTCAM)
+#define F_MATCHTCAM    V_MATCHTCAM(1U)
+
+#define S_MATCHSRAM    0
+#define V_MATCHSRAM(x) ((x) << S_MATCHSRAM)
+#define F_MATCHSRAM    V_MATCHSRAM(1U)
+
+#define A_MPS_CLS_INT_ENABLE 0xd024
+
+#define S_PLERRENB    3
+#define V_PLERRENB(x) ((x) << S_PLERRENB)
+#define F_PLERRENB    V_PLERRENB(1U)
+
+#define A_MPS_CLS_INT_CAUSE 0xd028
+#define A_MPS_CLS_PL_TEST_DATA_L 0xd02c
+#define A_MPS_CLS_PL_TEST_DATA_H 0xd030
+#define A_MPS_CLS_PL_TEST_RES_DATA 0xd034
+
+#define S_CLS_PRIORITY    24
+#define M_CLS_PRIORITY    0x7U
+#define V_CLS_PRIORITY(x) ((x) << S_CLS_PRIORITY)
+#define G_CLS_PRIORITY(x) (((x) >> S_CLS_PRIORITY) & M_CLS_PRIORITY)
+
+#define S_CLS_REPLICATE    23
+#define V_CLS_REPLICATE(x) ((x) << S_CLS_REPLICATE)
+#define F_CLS_REPLICATE    V_CLS_REPLICATE(1U)
+
+#define S_CLS_INDEX    14
+#define M_CLS_INDEX    0x1ffU
+#define V_CLS_INDEX(x) ((x) << S_CLS_INDEX)
+#define G_CLS_INDEX(x) (((x) >> S_CLS_INDEX) & M_CLS_INDEX)
+
+#define S_CLS_VF    7
+#define M_CLS_VF    0x7fU
+#define V_CLS_VF(x) ((x) << S_CLS_VF)
+#define G_CLS_VF(x) (((x) >> S_CLS_VF) & M_CLS_VF)
+
+#define S_CLS_VF_VLD    6
+#define V_CLS_VF_VLD(x) ((x) << S_CLS_VF_VLD)
+#define F_CLS_VF_VLD    V_CLS_VF_VLD(1U)
+
+#define S_CLS_PF    3
+#define M_CLS_PF    0x7U
+#define V_CLS_PF(x) ((x) << S_CLS_PF)
+#define G_CLS_PF(x) (((x) >> S_CLS_PF) & M_CLS_PF)
+
+#define S_CLS_MATCH    0
+#define M_CLS_MATCH    0x7U
+#define V_CLS_MATCH(x) ((x) << S_CLS_MATCH)
+#define G_CLS_MATCH(x) (((x) >> S_CLS_MATCH) & M_CLS_MATCH)
+
+#define A_MPS_CLS_PL_TEST_CTL 0xd038
+
+#define S_PLTESTCTL    0
+#define V_PLTESTCTL(x) ((x) << S_PLTESTCTL)
+#define F_PLTESTCTL    V_PLTESTCTL(1U)
+
+#define A_MPS_CLS_PORT_BMC_CTL 0xd03c
+
+#define S_PRTBMCCTL    0
+#define V_PRTBMCCTL(x) ((x) << S_PRTBMCCTL)
+#define F_PRTBMCCTL    V_PRTBMCCTL(1U)
+
+#define A_MPS_CLS_VLAN_TABLE 0xdfc0
+
+#define S_VLAN_MASK    16
+#define M_VLAN_MASK    0xfffU
+#define V_VLAN_MASK(x) ((x) << S_VLAN_MASK)
+#define G_VLAN_MASK(x) (((x) >> S_VLAN_MASK) & M_VLAN_MASK)
+
+#define S_VLANPF    13
+#define M_VLANPF    0x7U
+#define V_VLANPF(x) ((x) << S_VLANPF)
+#define G_VLANPF(x) (((x) >> S_VLANPF) & M_VLANPF)
+
+#define S_VLAN_VALID    12
+#define V_VLAN_VALID(x) ((x) << S_VLAN_VALID)
+#define F_VLAN_VALID    V_VLAN_VALID(1U)
+
+#define A_MPS_CLS_SRAM_L 0xe000
+
+#define S_MULTILISTEN3    28
+#define V_MULTILISTEN3(x) ((x) << S_MULTILISTEN3)
+#define F_MULTILISTEN3    V_MULTILISTEN3(1U)
+
+#define S_MULTILISTEN2    27
+#define V_MULTILISTEN2(x) ((x) << S_MULTILISTEN2)
+#define F_MULTILISTEN2    V_MULTILISTEN2(1U)
+
+#define S_MULTILISTEN1    26
+#define V_MULTILISTEN1(x) ((x) << S_MULTILISTEN1)
+#define F_MULTILISTEN1    V_MULTILISTEN1(1U)
+
+#define S_MULTILISTEN0    25
+#define V_MULTILISTEN0(x) ((x) << S_MULTILISTEN0)
+#define F_MULTILISTEN0    V_MULTILISTEN0(1U)
+
+#define S_SRAM_PRIO3    22
+#define M_SRAM_PRIO3    0x7U
+#define V_SRAM_PRIO3(x) ((x) << S_SRAM_PRIO3)
+#define G_SRAM_PRIO3(x) (((x) >> S_SRAM_PRIO3) & M_SRAM_PRIO3)
+
+#define S_SRAM_PRIO2    19
+#define M_SRAM_PRIO2    0x7U
+#define V_SRAM_PRIO2(x) ((x) << S_SRAM_PRIO2)
+#define G_SRAM_PRIO2(x) (((x) >> S_SRAM_PRIO2) & M_SRAM_PRIO2)
+
+#define S_SRAM_PRIO1    16
+#define M_SRAM_PRIO1    0x7U
+#define V_SRAM_PRIO1(x) ((x) << S_SRAM_PRIO1)
+#define G_SRAM_PRIO1(x) (((x) >> S_SRAM_PRIO1) & M_SRAM_PRIO1)
+
+#define S_SRAM_PRIO0    13
+#define M_SRAM_PRIO0    0x7U
+#define V_SRAM_PRIO0(x) ((x) << S_SRAM_PRIO0)
+#define G_SRAM_PRIO0(x) (((x) >> S_SRAM_PRIO0) & M_SRAM_PRIO0)
+
+#define S_SRAM_VLD    12
+#define V_SRAM_VLD(x) ((x) << S_SRAM_VLD)
+#define F_SRAM_VLD    V_SRAM_VLD(1U)
+
+#define A_MPS_CLS_SRAM_H 0xe004
+
+#define S_MACPARITY1    9
+#define V_MACPARITY1(x) ((x) << S_MACPARITY1)
+#define F_MACPARITY1    V_MACPARITY1(1U)
+
+#define S_MACPARITY0    8
+#define V_MACPARITY0(x) ((x) << S_MACPARITY0)
+#define F_MACPARITY0    V_MACPARITY0(1U)
+
+#define S_MACPARITYMASKSIZE    4
+#define M_MACPARITYMASKSIZE    0xfU
+#define V_MACPARITYMASKSIZE(x) ((x) << S_MACPARITYMASKSIZE)
+#define G_MACPARITYMASKSIZE(x) (((x) >> S_MACPARITYMASKSIZE) & M_MACPARITYMASKSIZE)
+
+#define S_PORTMAP    0
+#define M_PORTMAP    0xfU
+#define V_PORTMAP(x) ((x) << S_PORTMAP)
+#define G_PORTMAP(x) (((x) >> S_PORTMAP) & M_PORTMAP)
+
+#define A_MPS_CLS_TCAM_Y_L 0xf000
+#define A_MPS_CLS_TCAM_Y_H 0xf004
+
+#define S_TCAMYH    0
+#define M_TCAMYH    0xffffU
+#define V_TCAMYH(x) ((x) << S_TCAMYH)
+#define G_TCAMYH(x) (((x) >> S_TCAMYH) & M_TCAMYH)
+
+#define A_MPS_CLS_TCAM_X_L 0xf008
+#define A_MPS_CLS_TCAM_X_H 0xf00c
+
+#define S_TCAMXH    0
+#define M_TCAMXH    0xffffU
+#define V_TCAMXH(x) ((x) << S_TCAMXH)
+#define G_TCAMXH(x) (((x) >> S_TCAMXH) & M_TCAMXH)
+
+#define A_MPS_RX_CTL 0x11000
+
+#define S_FILT_VLAN_SEL    17
+#define V_FILT_VLAN_SEL(x) ((x) << S_FILT_VLAN_SEL)
+#define F_FILT_VLAN_SEL    V_FILT_VLAN_SEL(1U)
+
+#define S_CBA_EN    16
+#define V_CBA_EN(x) ((x) << S_CBA_EN)
+#define F_CBA_EN    V_CBA_EN(1U)
+
+#define S_BLK_SNDR    12
+#define M_BLK_SNDR    0xfU
+#define V_BLK_SNDR(x) ((x) << S_BLK_SNDR)
+#define G_BLK_SNDR(x) (((x) >> S_BLK_SNDR) & M_BLK_SNDR)
+
+#define S_CMPRS    8
+#define M_CMPRS    0xfU
+#define V_CMPRS(x) ((x) << S_CMPRS)
+#define G_CMPRS(x) (((x) >> S_CMPRS) & M_CMPRS)
+
+#define S_SNF    0
+#define M_SNF    0xffU
+#define V_SNF(x) ((x) << S_SNF)
+#define G_SNF(x) (((x) >> S_SNF) & M_SNF)
+
+#define A_MPS_RX_PORT_MUX_CTL 0x11004
+
+#define S_CTL_P3    12
+#define M_CTL_P3    0xfU
+#define V_CTL_P3(x) ((x) << S_CTL_P3)
+#define G_CTL_P3(x) (((x) >> S_CTL_P3) & M_CTL_P3)
+
+#define S_CTL_P2    8
+#define M_CTL_P2    0xfU
+#define V_CTL_P2(x) ((x) << S_CTL_P2)
+#define G_CTL_P2(x) (((x) >> S_CTL_P2) & M_CTL_P2)
+
+#define S_CTL_P1    4
+#define M_CTL_P1    0xfU
+#define V_CTL_P1(x) ((x) << S_CTL_P1)
+#define G_CTL_P1(x) (((x) >> S_CTL_P1) & M_CTL_P1)
+
+#define S_CTL_P0    0
+#define M_CTL_P0    0xfU
+#define V_CTL_P0(x) ((x) << S_CTL_P0)
+#define G_CTL_P0(x) (((x) >> S_CTL_P0) & M_CTL_P0)
+
+#define A_MPS_RX_PG_FL 0x11008
+
+#define S_RST    16
+#define V_RST(x) ((x) << S_RST)
+#define F_RST    V_RST(1U)
+
+#define S_CNT    0
+#define M_CNT    0xffffU
+#define V_CNT(x) ((x) << S_CNT)
+#define G_CNT(x) (((x) >> S_CNT) & M_CNT)
+
+#define A_MPS_RX_PKT_FL 0x1100c
+#define A_MPS_RX_PG_RSV0 0x11010
+
+#define S_CLR_INTR    31
+#define V_CLR_INTR(x) ((x) << S_CLR_INTR)
+#define F_CLR_INTR    V_CLR_INTR(1U)
+
+#define S_SET_INTR    30
+#define V_SET_INTR(x) ((x) << S_SET_INTR)
+#define F_SET_INTR    V_SET_INTR(1U)
+
+#define S_USED    16
+#define M_USED    0x7ffU
+#define V_USED(x) ((x) << S_USED)
+#define G_USED(x) (((x) >> S_USED) & M_USED)
+
+#define S_ALLOC    0
+#define M_ALLOC    0x7ffU
+#define V_ALLOC(x) ((x) << S_ALLOC)
+#define G_ALLOC(x) (((x) >> S_ALLOC) & M_ALLOC)
+
+#define A_MPS_RX_PG_RSV1 0x11014
+#define A_MPS_RX_PG_RSV2 0x11018
+#define A_MPS_RX_PG_RSV3 0x1101c
+#define A_MPS_RX_PG_RSV4 0x11020
+#define A_MPS_RX_PG_RSV5 0x11024
+#define A_MPS_RX_PG_RSV6 0x11028
+#define A_MPS_RX_PG_RSV7 0x1102c
+#define A_MPS_RX_PG_SHR_BG0 0x11030
+
+#define S_EN    31
+#define V_EN(x) ((x) << S_EN)
+#define F_EN    V_EN(1U)
+
+#define S_SEL    30
+#define V_SEL(x) ((x) << S_SEL)
+#define F_SEL    V_SEL(1U)
+
+#define S_MAX    16
+#define M_MAX    0x7ffU
+#define V_MAX(x) ((x) << S_MAX)
+#define G_MAX(x) (((x) >> S_MAX) & M_MAX)
+
+#define S_BORW    0
+#define M_BORW    0x7ffU
+#define V_BORW(x) ((x) << S_BORW)
+#define G_BORW(x) (((x) >> S_BORW) & M_BORW)
+
+#define A_MPS_RX_PG_SHR_BG1 0x11034
+#define A_MPS_RX_PG_SHR_BG2 0x11038
+#define A_MPS_RX_PG_SHR_BG3 0x1103c
+#define A_MPS_RX_PG_SHR0 0x11040
+
+#define S_QUOTA    16
+#define M_QUOTA    0x7ffU
+#define V_QUOTA(x) ((x) << S_QUOTA)
+#define G_QUOTA(x) (((x) >> S_QUOTA) & M_QUOTA)
+
+#define S_SHR_USED    0
+#define M_SHR_USED    0x7ffU
+#define V_SHR_USED(x) ((x) << S_SHR_USED)
+#define G_SHR_USED(x) (((x) >> S_SHR_USED) & M_SHR_USED)
+
+#define A_MPS_RX_PG_SHR1 0x11044
+#define A_MPS_RX_PG_HYST_BG0 0x11048
+
+#define S_TH    0
+#define M_TH    0x7ffU
+#define V_TH(x) ((x) << S_TH)
+#define G_TH(x) (((x) >> S_TH) & M_TH)
+
+#define A_MPS_RX_PG_HYST_BG1 0x1104c
+#define A_MPS_RX_PG_HYST_BG2 0x11050
+#define A_MPS_RX_PG_HYST_BG3 0x11054
+#define A_MPS_RX_OCH_CTL 0x11058
+
+#define S_DROP_WT    27
+#define M_DROP_WT    0x1fU
+#define V_DROP_WT(x) ((x) << S_DROP_WT)
+#define G_DROP_WT(x) (((x) >> S_DROP_WT) & M_DROP_WT)
+
+#define S_TRUNC_WT    22
+#define M_TRUNC_WT    0x1fU
+#define V_TRUNC_WT(x) ((x) << S_TRUNC_WT)
+#define G_TRUNC_WT(x) (((x) >> S_TRUNC_WT) & M_TRUNC_WT)
+
+#define S_OCH_DRAIN    13
+#define M_OCH_DRAIN    0x1fU
+#define V_OCH_DRAIN(x) ((x) << S_OCH_DRAIN)
+#define G_OCH_DRAIN(x) (((x) >> S_OCH_DRAIN) & M_OCH_DRAIN)
+
+#define S_OCH_DROP    8
+#define M_OCH_DROP    0x1fU
+#define V_OCH_DROP(x) ((x) << S_OCH_DROP)
+#define G_OCH_DROP(x) (((x) >> S_OCH_DROP) & M_OCH_DROP)
+
+#define S_STOP    0
+#define M_STOP    0x1fU
+#define V_STOP(x) ((x) << S_STOP)
+#define G_STOP(x) (((x) >> S_STOP) & M_STOP)
+
+#define A_MPS_RX_LPBK_BP0 0x1105c
+
+#define S_THRESH    0
+#define M_THRESH    0x7ffU
+#define V_THRESH(x) ((x) << S_THRESH)
+#define G_THRESH(x) (((x) >> S_THRESH) & M_THRESH)
+
+#define A_MPS_RX_LPBK_BP1 0x11060
+#define A_MPS_RX_LPBK_BP2 0x11064
+#define A_MPS_RX_LPBK_BP3 0x11068
+#define A_MPS_RX_PORT_GAP 0x1106c
+
+#define S_GAP    0
+#define M_GAP    0xfffffU
+#define V_GAP(x) ((x) << S_GAP)
+#define G_GAP(x) (((x) >> S_GAP) & M_GAP)
+
+#define A_MPS_RX_CHMN_CNT 0x11070
+#define A_MPS_RX_PERR_INT_CAUSE 0x11074
+
+#define S_FF    23
+#define V_FF(x) ((x) << S_FF)
+#define F_FF    V_FF(1U)
+
+#define S_PGMO    22
+#define V_PGMO(x) ((x) << S_PGMO)
+#define F_PGMO    V_PGMO(1U)
+
+#define S_PGME    21
+#define V_PGME(x) ((x) << S_PGME)
+#define F_PGME    V_PGME(1U)
+
+#define S_CHMN    20
+#define V_CHMN(x) ((x) << S_CHMN)
+#define F_CHMN    V_CHMN(1U)
+
+#define S_RPLC    19
+#define V_RPLC(x) ((x) << S_RPLC)
+#define F_RPLC    V_RPLC(1U)
+
+#define S_ATRB    18
+#define V_ATRB(x) ((x) << S_ATRB)
+#define F_ATRB    V_ATRB(1U)
+
+#define S_PSMX    17
+#define V_PSMX(x) ((x) << S_PSMX)
+#define F_PSMX    V_PSMX(1U)
+
+#define S_PGLL    16
+#define V_PGLL(x) ((x) << S_PGLL)
+#define F_PGLL    V_PGLL(1U)
+
+#define S_PGFL    15
+#define V_PGFL(x) ((x) << S_PGFL)
+#define F_PGFL    V_PGFL(1U)
+
+#define S_PKTQ    14
+#define V_PKTQ(x) ((x) << S_PKTQ)
+#define F_PKTQ    V_PKTQ(1U)
+
+#define S_PKFL    13
+#define V_PKFL(x) ((x) << S_PKFL)
+#define F_PKFL    V_PKFL(1U)
+
+#define S_PPM3    12
+#define V_PPM3(x) ((x) << S_PPM3)
+#define F_PPM3    V_PPM3(1U)
+
+#define S_PPM2    11
+#define V_PPM2(x) ((x) << S_PPM2)
+#define F_PPM2    V_PPM2(1U)
+
+#define S_PPM1    10
+#define V_PPM1(x) ((x) << S_PPM1)
+#define F_PPM1    V_PPM1(1U)
+
+#define S_PPM0    9
+#define V_PPM0(x) ((x) << S_PPM0)
+#define F_PPM0    V_PPM0(1U)
+
+#define S_SPMX    8
+#define V_SPMX(x) ((x) << S_SPMX)
+#define F_SPMX    V_SPMX(1U)
+
+#define S_CDL3    7
+#define V_CDL3(x) ((x) << S_CDL3)
+#define F_CDL3    V_CDL3(1U)
+
+#define S_CDL2    6
+#define V_CDL2(x) ((x) << S_CDL2)
+#define F_CDL2    V_CDL2(1U)
+
+#define S_CDL1    5
+#define V_CDL1(x) ((x) << S_CDL1)
+#define F_CDL1    V_CDL1(1U)
+
+#define S_CDL0    4
+#define V_CDL0(x) ((x) << S_CDL0)
+#define F_CDL0    V_CDL0(1U)
+
+#define S_CDM3    3
+#define V_CDM3(x) ((x) << S_CDM3)
+#define F_CDM3    V_CDM3(1U)
+
+#define S_CDM2    2
+#define V_CDM2(x) ((x) << S_CDM2)
+#define F_CDM2    V_CDM2(1U)
+
+#define S_CDM1    1
+#define V_CDM1(x) ((x) << S_CDM1)
+#define F_CDM1    V_CDM1(1U)
+
+#define S_CDM0    0
+#define V_CDM0(x) ((x) << S_CDM0)
+#define F_CDM0    V_CDM0(1U)
+
+#define A_MPS_RX_PERR_INT_ENABLE 0x11078
+#define A_MPS_RX_PERR_ENABLE 0x1107c
+#define A_MPS_RX_PERR_INJECT 0x11080
+#define A_MPS_RX_FUNC_INT_CAUSE 0x11084
+
+#define S_INT_ERR_INT    8
+#define M_INT_ERR_INT    0x1fU
+#define V_INT_ERR_INT(x) ((x) << S_INT_ERR_INT)
+#define G_INT_ERR_INT(x) (((x) >> S_INT_ERR_INT) & M_INT_ERR_INT)
+
+#define S_PG_TH_INT7    7
+#define V_PG_TH_INT7(x) ((x) << S_PG_TH_INT7)
+#define F_PG_TH_INT7    V_PG_TH_INT7(1U)
+
+#define S_PG_TH_INT6    6
+#define V_PG_TH_INT6(x) ((x) << S_PG_TH_INT6)
+#define F_PG_TH_INT6    V_PG_TH_INT6(1U)
+
+#define S_PG_TH_INT5    5
+#define V_PG_TH_INT5(x) ((x) << S_PG_TH_INT5)
+#define F_PG_TH_INT5    V_PG_TH_INT5(1U)
+
+#define S_PG_TH_INT4    4
+#define V_PG_TH_INT4(x) ((x) << S_PG_TH_INT4)
+#define F_PG_TH_INT4    V_PG_TH_INT4(1U)
+
+#define S_PG_TH_INT3    3
+#define V_PG_TH_INT3(x) ((x) << S_PG_TH_INT3)
+#define F_PG_TH_INT3    V_PG_TH_INT3(1U)
+
+#define S_PG_TH_INT2    2
+#define V_PG_TH_INT2(x) ((x) << S_PG_TH_INT2)
+#define F_PG_TH_INT2    V_PG_TH_INT2(1U)
+
+#define S_PG_TH_INT1    1
+#define V_PG_TH_INT1(x) ((x) << S_PG_TH_INT1)
+#define F_PG_TH_INT1    V_PG_TH_INT1(1U)
+
+#define S_PG_TH_INT0    0
+#define V_PG_TH_INT0(x) ((x) << S_PG_TH_INT0)
+#define F_PG_TH_INT0    V_PG_TH_INT0(1U)
+
+#define A_MPS_RX_FUNC_INT_ENABLE 0x11088
+#define A_MPS_RX_PAUSE_GEN_TH_0 0x1108c
+
+#define S_TH_HIGH    16
+#define M_TH_HIGH    0xffffU
+#define V_TH_HIGH(x) ((x) << S_TH_HIGH)
+#define G_TH_HIGH(x) (((x) >> S_TH_HIGH) & M_TH_HIGH)
+
+#define S_TH_LOW    0
+#define M_TH_LOW    0xffffU
+#define V_TH_LOW(x) ((x) << S_TH_LOW)
+#define G_TH_LOW(x) (((x) >> S_TH_LOW) & M_TH_LOW)
+
+#define A_MPS_RX_PAUSE_GEN_TH_1 0x11090
+#define A_MPS_RX_PAUSE_GEN_TH_2 0x11094
+#define A_MPS_RX_PAUSE_GEN_TH_3 0x11098
+#define A_MPS_RX_PPP_ATRB 0x1109c
+
+#define S_ETYPE    16
+#define M_ETYPE    0xffffU
+#define V_ETYPE(x) ((x) << S_ETYPE)
+#define G_ETYPE(x) (((x) >> S_ETYPE) & M_ETYPE)
+
+#define S_OPCODE    0
+#define M_OPCODE    0xffffU
+#define V_OPCODE(x) ((x) << S_OPCODE)
+#define G_OPCODE(x) (((x) >> S_OPCODE) & M_OPCODE)
+
+#define A_MPS_RX_QFC0_ATRB 0x110a0
+
+#define S_DA    0
+#define M_DA    0xffffU
+#define V_DA(x) ((x) << S_DA)
+#define G_DA(x) (((x) >> S_DA) & M_DA)
+
+#define A_MPS_RX_QFC1_ATRB 0x110a4
+#define A_MPS_RX_PT_ARB0 0x110a8
+
+#define S_LPBK_WT    16
+#define M_LPBK_WT    0x3fffU
+#define V_LPBK_WT(x) ((x) << S_LPBK_WT)
+#define G_LPBK_WT(x) (((x) >> S_LPBK_WT) & M_LPBK_WT)
+
+#define S_MAC_WT    0
+#define M_MAC_WT    0x3fffU
+#define V_MAC_WT(x) ((x) << S_MAC_WT)
+#define G_MAC_WT(x) (((x) >> S_MAC_WT) & M_MAC_WT)
+
+#define A_MPS_RX_PT_ARB1 0x110ac
+#define A_MPS_RX_PT_ARB2 0x110b0
+#define A_MPS_RX_PT_ARB3 0x110b4
+#define A_MPS_RX_PT_ARB4 0x110b8
+#define A_MPS_PF_OUT_EN 0x110bc
+
+#define S_OUTEN    0
+#define M_OUTEN    0xffU
+#define V_OUTEN(x) ((x) << S_OUTEN)
+#define G_OUTEN(x) (((x) >> S_OUTEN) & M_OUTEN)
+
+#define A_MPS_BMC_MTU 0x110c0
+
+#define S_MTU    0
+#define M_MTU    0x3fffU
+#define V_MTU(x) ((x) << S_MTU)
+#define G_MTU(x) (((x) >> S_MTU) & M_MTU)
+
+#define A_MPS_BMC_PKT_CNT 0x110c4
+#define A_MPS_BMC_BYTE_CNT 0x110c8
+#define A_MPS_PFVF_ATRB_CTL 0x110cc
+
+#define S_RD_WRN    31
+#define V_RD_WRN(x) ((x) << S_RD_WRN)
+#define F_RD_WRN    V_RD_WRN(1U)
+
+#define S_PFVF    0
+#define M_PFVF    0xffU
+#define V_PFVF(x) ((x) << S_PFVF)
+#define G_PFVF(x) (((x) >> S_PFVF) & M_PFVF)
+
+#define A_MPS_PFVF_ATRB 0x110d0
+
+#define S_ATTR_PF    28
+#define M_ATTR_PF    0x7U
+#define V_ATTR_PF(x) ((x) << S_ATTR_PF)
+#define G_ATTR_PF(x) (((x) >> S_ATTR_PF) & M_ATTR_PF)
+
+#define S_OFF    18
+#define V_OFF(x) ((x) << S_OFF)
+#define F_OFF    V_OFF(1U)
+
+#define S_NV_DROP    17
+#define V_NV_DROP(x) ((x) << S_NV_DROP)
+#define F_NV_DROP    V_NV_DROP(1U)
+
+#define S_ATTR_MODE    16
+#define V_ATTR_MODE(x) ((x) << S_ATTR_MODE)
+#define F_ATTR_MODE    V_ATTR_MODE(1U)
+
+#define A_MPS_PFVF_ATRB_FLTR0 0x110d4
+
+#define S_VLAN_EN    16
+#define V_VLAN_EN(x) ((x) << S_VLAN_EN)
+#define F_VLAN_EN    V_VLAN_EN(1U)
+
+#define S_VLAN_ID    0
+#define M_VLAN_ID    0xfffU
+#define V_VLAN_ID(x) ((x) << S_VLAN_ID)
+#define G_VLAN_ID(x) (((x) >> S_VLAN_ID) & M_VLAN_ID)
+
+#define A_MPS_PFVF_ATRB_FLTR1 0x110d8
+#define A_MPS_PFVF_ATRB_FLTR2 0x110dc
+#define A_MPS_PFVF_ATRB_FLTR3 0x110e0
+#define A_MPS_PFVF_ATRB_FLTR4 0x110e4
+#define A_MPS_PFVF_ATRB_FLTR5 0x110e8
+#define A_MPS_PFVF_ATRB_FLTR6 0x110ec
+#define A_MPS_PFVF_ATRB_FLTR7 0x110f0
+#define A_MPS_PFVF_ATRB_FLTR8 0x110f4
+#define A_MPS_PFVF_ATRB_FLTR9 0x110f8
+#define A_MPS_PFVF_ATRB_FLTR10 0x110fc
+#define A_MPS_PFVF_ATRB_FLTR11 0x11100
+#define A_MPS_PFVF_ATRB_FLTR12 0x11104
+#define A_MPS_PFVF_ATRB_FLTR13 0x11108
+#define A_MPS_PFVF_ATRB_FLTR14 0x1110c
+#define A_MPS_PFVF_ATRB_FLTR15 0x11110
+#define A_MPS_RPLC_MAP_CTL 0x11114
+
+#define S_RPLC_MAP_ADDR    0
+#define M_RPLC_MAP_ADDR    0x3ffU
+#define V_RPLC_MAP_ADDR(x) ((x) << S_RPLC_MAP_ADDR)
+#define G_RPLC_MAP_ADDR(x) (((x) >> S_RPLC_MAP_ADDR) & M_RPLC_MAP_ADDR)
+
+#define A_MPS_PF_RPLCT_MAP 0x11118
+
+#define S_PF_EN    0
+#define M_PF_EN    0xffU
+#define V_PF_EN(x) ((x) << S_PF_EN)
+#define G_PF_EN(x) (((x) >> S_PF_EN) & M_PF_EN)
+
+#define A_MPS_VF_RPLCT_MAP0 0x1111c
+#define A_MPS_VF_RPLCT_MAP1 0x11120
+#define A_MPS_VF_RPLCT_MAP2 0x11124
+#define A_MPS_VF_RPLCT_MAP3 0x11128
+#define A_MPS_MEM_DBG_CTL 0x1112c
+
+#define S_PKD    17
+#define V_PKD(x) ((x) << S_PKD)
+#define F_PKD    V_PKD(1U)
+
+#define S_PGD    16
+#define V_PGD(x) ((x) << S_PGD)
+#define F_PGD    V_PGD(1U)
+
+#define A_MPS_PKD_MEM_DATA0 0x11130
+#define A_MPS_PKD_MEM_DATA1 0x11134
+#define A_MPS_PKD_MEM_DATA2 0x11138
+#define A_MPS_PGD_MEM_DATA 0x1113c
+#define A_MPS_RX_SE_CNT_ERR 0x11140
+
+#define S_RX_SE_ERRMAP    0
+#define M_RX_SE_ERRMAP    0xfffffU
+#define V_RX_SE_ERRMAP(x) ((x) << S_RX_SE_ERRMAP)
+#define G_RX_SE_ERRMAP(x) (((x) >> S_RX_SE_ERRMAP) & M_RX_SE_ERRMAP)
+
+#define A_MPS_RX_SE_CNT_CLR 0x11144
+#define A_MPS_RX_SE_CNT_IN0 0x11148
+
+#define S_SOP_CNT_PM    24
+#define M_SOP_CNT_PM    0xffU
+#define V_SOP_CNT_PM(x) ((x) << S_SOP_CNT_PM)
+#define G_SOP_CNT_PM(x) (((x) >> S_SOP_CNT_PM) & M_SOP_CNT_PM)
+
+#define S_EOP_CNT_PM    16
+#define M_EOP_CNT_PM    0xffU
+#define V_EOP_CNT_PM(x) ((x) << S_EOP_CNT_PM)
+#define G_EOP_CNT_PM(x) (((x) >> S_EOP_CNT_PM) & M_EOP_CNT_PM)
+
+#define S_SOP_CNT_IN    8
+#define M_SOP_CNT_IN    0xffU
+#define V_SOP_CNT_IN(x) ((x) << S_SOP_CNT_IN)
+#define G_SOP_CNT_IN(x) (((x) >> S_SOP_CNT_IN) & M_SOP_CNT_IN)
+
+#define S_EOP_CNT_IN    0
+#define M_EOP_CNT_IN    0xffU
+#define V_EOP_CNT_IN(x) ((x) << S_EOP_CNT_IN)
+#define G_EOP_CNT_IN(x) (((x) >> S_EOP_CNT_IN) & M_EOP_CNT_IN)
+
+#define A_MPS_RX_SE_CNT_IN1 0x1114c
+#define A_MPS_RX_SE_CNT_IN2 0x11150
+#define A_MPS_RX_SE_CNT_IN3 0x11154
+#define A_MPS_RX_SE_CNT_IN4 0x11158
+#define A_MPS_RX_SE_CNT_IN5 0x1115c
+#define A_MPS_RX_SE_CNT_IN6 0x11160
+#define A_MPS_RX_SE_CNT_IN7 0x11164
+#define A_MPS_RX_SE_CNT_OUT01 0x11168
+
+#define S_SOP_CNT_1    24
+#define M_SOP_CNT_1    0xffU
+#define V_SOP_CNT_1(x) ((x) << S_SOP_CNT_1)
+#define G_SOP_CNT_1(x) (((x) >> S_SOP_CNT_1) & M_SOP_CNT_1)
+
+#define S_EOP_CNT_1    16
+#define M_EOP_CNT_1    0xffU
+#define V_EOP_CNT_1(x) ((x) << S_EOP_CNT_1)
+#define G_EOP_CNT_1(x) (((x) >> S_EOP_CNT_1) & M_EOP_CNT_1)
+
+#define S_SOP_CNT_0    8
+#define M_SOP_CNT_0    0xffU
+#define V_SOP_CNT_0(x) ((x) << S_SOP_CNT_0)
+#define G_SOP_CNT_0(x) (((x) >> S_SOP_CNT_0) & M_SOP_CNT_0)
+
+#define S_EOP_CNT_0    0
+#define M_EOP_CNT_0    0xffU
+#define V_EOP_CNT_0(x) ((x) << S_EOP_CNT_0)
+#define G_EOP_CNT_0(x) (((x) >> S_EOP_CNT_0) & M_EOP_CNT_0)
+
+#define A_MPS_RX_SE_CNT_OUT23 0x1116c
+
+#define S_SOP_CNT_3    24
+#define M_SOP_CNT_3    0xffU
+#define V_SOP_CNT_3(x) ((x) << S_SOP_CNT_3)
+#define G_SOP_CNT_3(x) (((x) >> S_SOP_CNT_3) & M_SOP_CNT_3)
+
+#define S_EOP_CNT_3    16
+#define M_EOP_CNT_3    0xffU
+#define V_EOP_CNT_3(x) ((x) << S_EOP_CNT_3)
+#define G_EOP_CNT_3(x) (((x) >> S_EOP_CNT_3) & M_EOP_CNT_3)
+
+#define S_SOP_CNT_2    8
+#define M_SOP_CNT_2    0xffU
+#define V_SOP_CNT_2(x) ((x) << S_SOP_CNT_2)
+#define G_SOP_CNT_2(x) (((x) >> S_SOP_CNT_2) & M_SOP_CNT_2)
+
+#define S_EOP_CNT_2    0
+#define M_EOP_CNT_2    0xffU
+#define V_EOP_CNT_2(x) ((x) << S_EOP_CNT_2)
+#define G_EOP_CNT_2(x) (((x) >> S_EOP_CNT_2) & M_EOP_CNT_2)
+
+#define A_MPS_RX_SPI_ERR 0x11170
+
+#define S_LENERR    21
+#define M_LENERR    0xfU
+#define V_LENERR(x) ((x) << S_LENERR)
+#define G_LENERR(x) (((x) >> S_LENERR) & M_LENERR)
+
+#define S_SPIERR    0
+#define M_SPIERR    0x1fffffU
+#define V_SPIERR(x) ((x) << S_SPIERR)
+#define G_SPIERR(x) (((x) >> S_SPIERR) & M_SPIERR)
+
+#define A_MPS_RX_IN_BUS_STATE 0x11174
+
+#define S_ST3    24
+#define M_ST3    0xffU
+#define V_ST3(x) ((x) << S_ST3)
+#define G_ST3(x) (((x) >> S_ST3) & M_ST3)
+
+#define S_ST2    16
+#define M_ST2    0xffU
+#define V_ST2(x) ((x) << S_ST2)
+#define G_ST2(x) (((x) >> S_ST2) & M_ST2)
+
+#define S_ST1    8
+#define M_ST1    0xffU
+#define V_ST1(x) ((x) << S_ST1)
+#define G_ST1(x) (((x) >> S_ST1) & M_ST1)
+
+#define S_ST0    0
+#define M_ST0    0xffU
+#define V_ST0(x) ((x) << S_ST0)
+#define G_ST0(x) (((x) >> S_ST0) & M_ST0)
+
+#define A_MPS_RX_OUT_BUS_STATE 0x11178
+
+#define S_ST_NCSI    23
+#define M_ST_NCSI    0x1ffU
+#define V_ST_NCSI(x) ((x) << S_ST_NCSI)
+#define G_ST_NCSI(x) (((x) >> S_ST_NCSI) & M_ST_NCSI)
+
+#define S_ST_TP    0
+#define M_ST_TP    0x7fffffU
+#define V_ST_TP(x) ((x) << S_ST_TP)
+#define G_ST_TP(x) (((x) >> S_ST_TP) & M_ST_TP)
+
+#define A_MPS_RX_DBG_CTL 0x1117c
+
+#define S_OUT_DBG_CHNL    8
+#define M_OUT_DBG_CHNL    0x7U
+#define V_OUT_DBG_CHNL(x) ((x) << S_OUT_DBG_CHNL)
+#define G_OUT_DBG_CHNL(x) (((x) >> S_OUT_DBG_CHNL) & M_OUT_DBG_CHNL)
+
+#define S_DBG_PKD_QSEL    7
+#define V_DBG_PKD_QSEL(x) ((x) << S_DBG_PKD_QSEL)
+#define F_DBG_PKD_QSEL    V_DBG_PKD_QSEL(1U)
+
+#define S_DBG_CDS_INV    6
+#define V_DBG_CDS_INV(x) ((x) << S_DBG_CDS_INV)
+#define F_DBG_CDS_INV    V_DBG_CDS_INV(1U)
+
+#define S_IN_DBG_PORT    3
+#define M_IN_DBG_PORT    0x7U
+#define V_IN_DBG_PORT(x) ((x) << S_IN_DBG_PORT)
+#define G_IN_DBG_PORT(x) (((x) >> S_IN_DBG_PORT) & M_IN_DBG_PORT)
+
+#define S_IN_DBG_CHNL    0
+#define M_IN_DBG_CHNL    0x7U
+#define V_IN_DBG_CHNL(x) ((x) << S_IN_DBG_CHNL)
+#define G_IN_DBG_CHNL(x) (((x) >> S_IN_DBG_CHNL) & M_IN_DBG_CHNL)
+
+#define A_MPS_RX_CLS_DROP_CNT0 0x11180
+
+#define S_LPBK_CNT0    16
+#define M_LPBK_CNT0    0xffffU
+#define V_LPBK_CNT0(x) ((x) << S_LPBK_CNT0)
+#define G_LPBK_CNT0(x) (((x) >> S_LPBK_CNT0) & M_LPBK_CNT0)
+
+#define S_MAC_CNT0    0
+#define M_MAC_CNT0    0xffffU
+#define V_MAC_CNT0(x) ((x) << S_MAC_CNT0)
+#define G_MAC_CNT0(x) (((x) >> S_MAC_CNT0) & M_MAC_CNT0)
+
+#define A_MPS_RX_CLS_DROP_CNT1 0x11184
+
+#define S_LPBK_CNT1    16
+#define M_LPBK_CNT1    0xffffU
+#define V_LPBK_CNT1(x) ((x) << S_LPBK_CNT1)
+#define G_LPBK_CNT1(x) (((x) >> S_LPBK_CNT1) & M_LPBK_CNT1)
+
+#define S_MAC_CNT1    0
+#define M_MAC_CNT1    0xffffU
+#define V_MAC_CNT1(x) ((x) << S_MAC_CNT1)
+#define G_MAC_CNT1(x) (((x) >> S_MAC_CNT1) & M_MAC_CNT1)
+
+#define A_MPS_RX_CLS_DROP_CNT2 0x11188
+
+#define S_LPBK_CNT2    16
+#define M_LPBK_CNT2    0xffffU
+#define V_LPBK_CNT2(x) ((x) << S_LPBK_CNT2)
+#define G_LPBK_CNT2(x) (((x) >> S_LPBK_CNT2) & M_LPBK_CNT2)
+
+#define S_MAC_CNT2    0
+#define M_MAC_CNT2    0xffffU
+#define V_MAC_CNT2(x) ((x) << S_MAC_CNT2)
+#define G_MAC_CNT2(x) (((x) >> S_MAC_CNT2) & M_MAC_CNT2)
+
+#define A_MPS_RX_CLS_DROP_CNT3 0x1118c
+
+#define S_LPBK_CNT3    16
+#define M_LPBK_CNT3    0xffffU
+#define V_LPBK_CNT3(x) ((x) << S_LPBK_CNT3)
+#define G_LPBK_CNT3(x) (((x) >> S_LPBK_CNT3) & M_LPBK_CNT3)
+
+#define S_MAC_CNT3    0
+#define M_MAC_CNT3    0xffffU
+#define V_MAC_CNT3(x) ((x) << S_MAC_CNT3)
+#define G_MAC_CNT3(x) (((x) >> S_MAC_CNT3) & M_MAC_CNT3)
+
+#define A_MPS_RX_SPARE 0x11190
+
+/* registers for module CPL_SWITCH */
+#define CPL_SWITCH_BASE_ADDR 0x19040
+
+#define A_CPL_SWITCH_CNTRL 0x19040
+
+#define S_CPL_PKT_TID    8
+#define M_CPL_PKT_TID    0xffffffU
+#define V_CPL_PKT_TID(x) ((x) << S_CPL_PKT_TID)
+#define G_CPL_PKT_TID(x) (((x) >> S_CPL_PKT_TID) & M_CPL_PKT_TID)
+
+#define S_CIM_TRUNCATE_ENABLE    5
+#define V_CIM_TRUNCATE_ENABLE(x) ((x) << S_CIM_TRUNCATE_ENABLE)
+#define F_CIM_TRUNCATE_ENABLE    V_CIM_TRUNCATE_ENABLE(1U)
+
+#define S_CIM_TO_UP_FULL_SIZE    4
+#define V_CIM_TO_UP_FULL_SIZE(x) ((x) << S_CIM_TO_UP_FULL_SIZE)
+#define F_CIM_TO_UP_FULL_SIZE    V_CIM_TO_UP_FULL_SIZE(1U)
+
+#define S_CPU_NO_ENABLE    3
+#define V_CPU_NO_ENABLE(x) ((x) << S_CPU_NO_ENABLE)
+#define F_CPU_NO_ENABLE    V_CPU_NO_ENABLE(1U)
+
+#define S_SWITCH_TABLE_ENABLE    2
+#define V_SWITCH_TABLE_ENABLE(x) ((x) << S_SWITCH_TABLE_ENABLE)
+#define F_SWITCH_TABLE_ENABLE    V_SWITCH_TABLE_ENABLE(1U)
+
+#define S_SGE_ENABLE    1
+#define V_SGE_ENABLE(x) ((x) << S_SGE_ENABLE)
+#define F_SGE_ENABLE    V_SGE_ENABLE(1U)
+
+#define S_CIM_ENABLE    0
+#define V_CIM_ENABLE(x) ((x) << S_CIM_ENABLE)
+#define F_CIM_ENABLE    V_CIM_ENABLE(1U)
+
+#define A_CPL_SWITCH_TBL_IDX 0x19044
+
+#define S_SWITCH_TBL_IDX    0
+#define M_SWITCH_TBL_IDX    0xfU
+#define V_SWITCH_TBL_IDX(x) ((x) << S_SWITCH_TBL_IDX)
+#define G_SWITCH_TBL_IDX(x) (((x) >> S_SWITCH_TBL_IDX) & M_SWITCH_TBL_IDX)
+
+#define A_CPL_SWITCH_TBL_DATA 0x19048
+#define A_CPL_SWITCH_ZERO_ERROR 0x1904c
+
+#define S_ZERO_CMD_CH1    8
+#define M_ZERO_CMD_CH1    0xffU
+#define V_ZERO_CMD_CH1(x) ((x) << S_ZERO_CMD_CH1)
+#define G_ZERO_CMD_CH1(x) (((x) >> S_ZERO_CMD_CH1) & M_ZERO_CMD_CH1)
+
+#define S_ZERO_CMD_CH0    0
+#define M_ZERO_CMD_CH0    0xffU
+#define V_ZERO_CMD_CH0(x) ((x) << S_ZERO_CMD_CH0)
+#define G_ZERO_CMD_CH0(x) (((x) >> S_ZERO_CMD_CH0) & M_ZERO_CMD_CH0)
+
+#define A_CPL_INTR_ENABLE 0x19050
+
+#define S_CIM_OP_MAP_PERR    5
+#define V_CIM_OP_MAP_PERR(x) ((x) << S_CIM_OP_MAP_PERR)
+#define F_CIM_OP_MAP_PERR    V_CIM_OP_MAP_PERR(1U)
+
+#define S_CIM_OVFL_ERROR    4
+#define V_CIM_OVFL_ERROR(x) ((x) << S_CIM_OVFL_ERROR)
+#define F_CIM_OVFL_ERROR    V_CIM_OVFL_ERROR(1U)
+
+#define S_TP_FRAMING_ERROR    3
+#define V_TP_FRAMING_ERROR(x) ((x) << S_TP_FRAMING_ERROR)
+#define F_TP_FRAMING_ERROR    V_TP_FRAMING_ERROR(1U)
+
+#define S_SGE_FRAMING_ERROR    2
+#define V_SGE_FRAMING_ERROR(x) ((x) << S_SGE_FRAMING_ERROR)
+#define F_SGE_FRAMING_ERROR    V_SGE_FRAMING_ERROR(1U)
+
+#define S_CIM_FRAMING_ERROR    1
+#define V_CIM_FRAMING_ERROR(x) ((x) << S_CIM_FRAMING_ERROR)
+#define F_CIM_FRAMING_ERROR    V_CIM_FRAMING_ERROR(1U)
+
+#define S_ZERO_SWITCH_ERROR    0
+#define V_ZERO_SWITCH_ERROR(x) ((x) << S_ZERO_SWITCH_ERROR)
+#define F_ZERO_SWITCH_ERROR    V_ZERO_SWITCH_ERROR(1U)
+
+#define A_CPL_INTR_CAUSE 0x19054
+#define A_CPL_MAP_TBL_IDX 0x19058
+
+#define S_MAP_TBL_IDX    0
+#define M_MAP_TBL_IDX    0xffU
+#define V_MAP_TBL_IDX(x) ((x) << S_MAP_TBL_IDX)
+#define G_MAP_TBL_IDX(x) (((x) >> S_MAP_TBL_IDX) & M_MAP_TBL_IDX)
+
+#define A_CPL_MAP_TBL_DATA 0x1905c
+
+#define S_MAP_TBL_DATA    0
+#define M_MAP_TBL_DATA    0xffU
+#define V_MAP_TBL_DATA(x) ((x) << S_MAP_TBL_DATA)
+#define G_MAP_TBL_DATA(x) (((x) >> S_MAP_TBL_DATA) & M_MAP_TBL_DATA)
+
+/* registers for module SMB */
+#define SMB_BASE_ADDR 0x19060
+
+#define A_SMB_GLOBAL_TIME_CFG 0x19060
+
+#define S_MACROCNTCFG    8
+#define M_MACROCNTCFG    0x1fU
+#define V_MACROCNTCFG(x) ((x) << S_MACROCNTCFG)
+#define G_MACROCNTCFG(x) (((x) >> S_MACROCNTCFG) & M_MACROCNTCFG)
+
+#define S_MICROCNTCFG    0
+#define M_MICROCNTCFG    0xffU
+#define V_MICROCNTCFG(x) ((x) << S_MICROCNTCFG)
+#define G_MICROCNTCFG(x) (((x) >> S_MICROCNTCFG) & M_MICROCNTCFG)
+
+#define A_SMB_MST_TIMEOUT_CFG 0x19064
+
+#define S_MSTTIMEOUTCFG    0
+#define M_MSTTIMEOUTCFG    0xffffffU
+#define V_MSTTIMEOUTCFG(x) ((x) << S_MSTTIMEOUTCFG)
+#define G_MSTTIMEOUTCFG(x) (((x) >> S_MSTTIMEOUTCFG) & M_MSTTIMEOUTCFG)
+
+#define A_SMB_MST_CTL_CFG 0x19068
+
+#define S_MSTFIFODBG    31
+#define V_MSTFIFODBG(x) ((x) << S_MSTFIFODBG)
+#define F_MSTFIFODBG    V_MSTFIFODBG(1U)
+
+#define S_MSTFIFODBGCLR    30
+#define V_MSTFIFODBGCLR(x) ((x) << S_MSTFIFODBGCLR)
+#define F_MSTFIFODBGCLR    V_MSTFIFODBGCLR(1U)
+
+#define S_MSTRXBYTECFG    12
+#define M_MSTRXBYTECFG    0x3fU
+#define V_MSTRXBYTECFG(x) ((x) << S_MSTRXBYTECFG)
+#define G_MSTRXBYTECFG(x) (((x) >> S_MSTRXBYTECFG) & M_MSTRXBYTECFG)
+
+#define S_MSTTXBYTECFG    6
+#define M_MSTTXBYTECFG    0x3fU
+#define V_MSTTXBYTECFG(x) ((x) << S_MSTTXBYTECFG)
+#define G_MSTTXBYTECFG(x) (((x) >> S_MSTTXBYTECFG) & M_MSTTXBYTECFG)
+
+#define S_MSTRESET    1
+#define V_MSTRESET(x) ((x) << S_MSTRESET)
+#define F_MSTRESET    V_MSTRESET(1U)
+
+#define S_MSTCTLEN    0
+#define V_MSTCTLEN(x) ((x) << S_MSTCTLEN)
+#define F_MSTCTLEN    V_MSTCTLEN(1U)
+
+#define A_SMB_MST_CTL_STS 0x1906c
+
+#define S_MSTRXBYTECNT    12
+#define M_MSTRXBYTECNT    0x3fU
+#define V_MSTRXBYTECNT(x) ((x) << S_MSTRXBYTECNT)
+#define G_MSTRXBYTECNT(x) (((x) >> S_MSTRXBYTECNT) & M_MSTRXBYTECNT)
+
+#define S_MSTTXBYTECNT    6
+#define M_MSTTXBYTECNT    0x3fU
+#define V_MSTTXBYTECNT(x) ((x) << S_MSTTXBYTECNT)
+#define G_MSTTXBYTECNT(x) (((x) >> S_MSTTXBYTECNT) & M_MSTTXBYTECNT)
+
+#define S_MSTBUSYSTS    0
+#define V_MSTBUSYSTS(x) ((x) << S_MSTBUSYSTS)
+#define F_MSTBUSYSTS    V_MSTBUSYSTS(1U)
+
+#define A_SMB_MST_TX_FIFO_RDWR 0x19070
+#define A_SMB_MST_RX_FIFO_RDWR 0x19074
+#define A_SMB_SLV_TIMEOUT_CFG 0x19078
+
+#define S_SLVTIMEOUTCFG    0
+#define M_SLVTIMEOUTCFG    0xffffffU
+#define V_SLVTIMEOUTCFG(x) ((x) << S_SLVTIMEOUTCFG)
+#define G_SLVTIMEOUTCFG(x) (((x) >> S_SLVTIMEOUTCFG) & M_SLVTIMEOUTCFG)
+
+#define A_SMB_SLV_CTL_CFG 0x1907c
+
+#define S_SLVFIFODBG    31
+#define V_SLVFIFODBG(x) ((x) << S_SLVFIFODBG)
+#define F_SLVFIFODBG    V_SLVFIFODBG(1U)
+
+#define S_SLVFIFODBGCLR    30
+#define V_SLVFIFODBGCLR(x) ((x) << S_SLVFIFODBGCLR)
+#define F_SLVFIFODBGCLR    V_SLVFIFODBGCLR(1U)
+
+#define S_SLVCRCOUTBITINV    21
+#define V_SLVCRCOUTBITINV(x) ((x) << S_SLVCRCOUTBITINV)
+#define F_SLVCRCOUTBITINV    V_SLVCRCOUTBITINV(1U)
+
+#define S_SLVCRCOUTBITREV    20
+#define V_SLVCRCOUTBITREV(x) ((x) << S_SLVCRCOUTBITREV)
+#define F_SLVCRCOUTBITREV    V_SLVCRCOUTBITREV(1U)
+
+#define S_SLVCRCINBITREV    19
+#define V_SLVCRCINBITREV(x) ((x) << S_SLVCRCINBITREV)
+#define F_SLVCRCINBITREV    V_SLVCRCINBITREV(1U)
+
+#define S_SLVCRCPRESET    11
+#define M_SLVCRCPRESET    0xffU
+#define V_SLVCRCPRESET(x) ((x) << S_SLVCRCPRESET)
+#define G_SLVCRCPRESET(x) (((x) >> S_SLVCRCPRESET) & M_SLVCRCPRESET)
+
+#define S_SLVADDRCFG    4
+#define M_SLVADDRCFG    0x7fU
+#define V_SLVADDRCFG(x) ((x) << S_SLVADDRCFG)
+#define G_SLVADDRCFG(x) (((x) >> S_SLVADDRCFG) & M_SLVADDRCFG)
+
+#define S_SLVALRTSET    2
+#define V_SLVALRTSET(x) ((x) << S_SLVALRTSET)
+#define F_SLVALRTSET    V_SLVALRTSET(1U)
+
+#define S_SLVRESET    1
+#define V_SLVRESET(x) ((x) << S_SLVRESET)
+#define F_SLVRESET    V_SLVRESET(1U)
+
+#define S_SLVCTLEN    0
+#define V_SLVCTLEN(x) ((x) << S_SLVCTLEN)
+#define F_SLVCTLEN    V_SLVCTLEN(1U)
+
+#define A_SMB_SLV_CTL_STS 0x19080
+
+#define S_SLVFIFOTXCNT    12
+#define M_SLVFIFOTXCNT    0x3fU
+#define V_SLVFIFOTXCNT(x) ((x) << S_SLVFIFOTXCNT)
+#define G_SLVFIFOTXCNT(x) (((x) >> S_SLVFIFOTXCNT) & M_SLVFIFOTXCNT)
+
+#define S_SLVFIFOCNT    6
+#define M_SLVFIFOCNT    0x3fU
+#define V_SLVFIFOCNT(x) ((x) << S_SLVFIFOCNT)
+#define G_SLVFIFOCNT(x) (((x) >> S_SLVFIFOCNT) & M_SLVFIFOCNT)
+
+#define S_SLVALRTSTS    2
+#define V_SLVALRTSTS(x) ((x) << S_SLVALRTSTS)
+#define F_SLVALRTSTS    V_SLVALRTSTS(1U)
+
+#define S_SLVBUSYSTS    0
+#define V_SLVBUSYSTS(x) ((x) << S_SLVBUSYSTS)
+#define F_SLVBUSYSTS    V_SLVBUSYSTS(1U)
+
+#define A_SMB_SLV_FIFO_RDWR 0x19084
+#define A_SMB_INT_ENABLE 0x1908c
+
+#define S_MSTTXFIFOPAREN    21
+#define V_MSTTXFIFOPAREN(x) ((x) << S_MSTTXFIFOPAREN)
+#define F_MSTTXFIFOPAREN    V_MSTTXFIFOPAREN(1U)
+
+#define S_MSTRXFIFOPAREN    20
+#define V_MSTRXFIFOPAREN(x) ((x) << S_MSTRXFIFOPAREN)
+#define F_MSTRXFIFOPAREN    V_MSTRXFIFOPAREN(1U)
+
+#define S_SLVFIFOPAREN    19
+#define V_SLVFIFOPAREN(x) ((x) << S_SLVFIFOPAREN)
+#define F_SLVFIFOPAREN    V_SLVFIFOPAREN(1U)
+
+#define S_SLVUNEXPBUSSTOPEN    18
+#define V_SLVUNEXPBUSSTOPEN(x) ((x) << S_SLVUNEXPBUSSTOPEN)
+#define F_SLVUNEXPBUSSTOPEN    V_SLVUNEXPBUSSTOPEN(1U)
+
+#define S_SLVUNEXPBUSSTARTEN    17
+#define V_SLVUNEXPBUSSTARTEN(x) ((x) << S_SLVUNEXPBUSSTARTEN)
+#define F_SLVUNEXPBUSSTARTEN    V_SLVUNEXPBUSSTARTEN(1U)
+
+#define S_SLVCOMMANDCODEINVEN    16
+#define V_SLVCOMMANDCODEINVEN(x) ((x) << S_SLVCOMMANDCODEINVEN)
+#define F_SLVCOMMANDCODEINVEN    V_SLVCOMMANDCODEINVEN(1U)
+
+#define S_SLVBYTECNTERREN    15
+#define V_SLVBYTECNTERREN(x) ((x) << S_SLVBYTECNTERREN)
+#define F_SLVBYTECNTERREN    V_SLVBYTECNTERREN(1U)
+
+#define S_SLVUNEXPACKMSTEN    14
+#define V_SLVUNEXPACKMSTEN(x) ((x) << S_SLVUNEXPACKMSTEN)
+#define F_SLVUNEXPACKMSTEN    V_SLVUNEXPACKMSTEN(1U)
+
+#define S_SLVUNEXPNACKMSTEN    13
+#define V_SLVUNEXPNACKMSTEN(x) ((x) << S_SLVUNEXPNACKMSTEN)
+#define F_SLVUNEXPNACKMSTEN    V_SLVUNEXPNACKMSTEN(1U)
+
+#define S_SLVNOBUSSTOPEN    12
+#define V_SLVNOBUSSTOPEN(x) ((x) << S_SLVNOBUSSTOPEN)
+#define F_SLVNOBUSSTOPEN    V_SLVNOBUSSTOPEN(1U)
+
+#define S_SLVNOREPSTARTEN    11
+#define V_SLVNOREPSTARTEN(x) ((x) << S_SLVNOREPSTARTEN)
+#define F_SLVNOREPSTARTEN    V_SLVNOREPSTARTEN(1U)
+
+#define S_SLVRXADDRINTEN    10
+#define V_SLVRXADDRINTEN(x) ((x) << S_SLVRXADDRINTEN)
+#define F_SLVRXADDRINTEN    V_SLVRXADDRINTEN(1U)
+
+#define S_SLVRXPECERRINTEN    9
+#define V_SLVRXPECERRINTEN(x) ((x) << S_SLVRXPECERRINTEN)
+#define F_SLVRXPECERRINTEN    V_SLVRXPECERRINTEN(1U)
+
+#define S_SLVPREPTOARPINTEN    8
+#define V_SLVPREPTOARPINTEN(x) ((x) << S_SLVPREPTOARPINTEN)
+#define F_SLVPREPTOARPINTEN    V_SLVPREPTOARPINTEN(1U)
+
+#define S_SLVTIMEOUTINTEN    7
+#define V_SLVTIMEOUTINTEN(x) ((x) << S_SLVTIMEOUTINTEN)
+#define F_SLVTIMEOUTINTEN    V_SLVTIMEOUTINTEN(1U)
+
+#define S_SLVERRINTEN    6
+#define V_SLVERRINTEN(x) ((x) << S_SLVERRINTEN)
+#define F_SLVERRINTEN    V_SLVERRINTEN(1U)
+
+#define S_SLVDONEINTEN    5
+#define V_SLVDONEINTEN(x) ((x) << S_SLVDONEINTEN)
+#define F_SLVDONEINTEN    V_SLVDONEINTEN(1U)
+
+#define S_SLVRXRDYINTEN    4
+#define V_SLVRXRDYINTEN(x) ((x) << S_SLVRXRDYINTEN)
+#define F_SLVRXRDYINTEN    V_SLVRXRDYINTEN(1U)
+
+#define S_MSTTIMEOUTINTEN    3
+#define V_MSTTIMEOUTINTEN(x) ((x) << S_MSTTIMEOUTINTEN)
+#define F_MSTTIMEOUTINTEN    V_MSTTIMEOUTINTEN(1U)
+
+#define S_MSTNACKINTEN    2
+#define V_MSTNACKINTEN(x) ((x) << S_MSTNACKINTEN)
+#define F_MSTNACKINTEN    V_MSTNACKINTEN(1U)
+
+#define S_MSTLOSTARBINTEN    1
+#define V_MSTLOSTARBINTEN(x) ((x) << S_MSTLOSTARBINTEN)
+#define F_MSTLOSTARBINTEN    V_MSTLOSTARBINTEN(1U)
+
+#define S_MSTDONEINTEN    0
+#define V_MSTDONEINTEN(x) ((x) << S_MSTDONEINTEN)
+#define F_MSTDONEINTEN    V_MSTDONEINTEN(1U)
+
+#define A_SMB_INT_CAUSE 0x19090
+
+#define S_MSTTXFIFOPARINT    21
+#define V_MSTTXFIFOPARINT(x) ((x) << S_MSTTXFIFOPARINT)
+#define F_MSTTXFIFOPARINT    V_MSTTXFIFOPARINT(1U)
+
+#define S_MSTRXFIFOPARINT    20
+#define V_MSTRXFIFOPARINT(x) ((x) << S_MSTRXFIFOPARINT)
+#define F_MSTRXFIFOPARINT    V_MSTRXFIFOPARINT(1U)
+
+#define S_SLVFIFOPARINT    19
+#define V_SLVFIFOPARINT(x) ((x) << S_SLVFIFOPARINT)
+#define F_SLVFIFOPARINT    V_SLVFIFOPARINT(1U)
+
+#define S_SLVUNEXPBUSSTOPINT    18
+#define V_SLVUNEXPBUSSTOPINT(x) ((x) << S_SLVUNEXPBUSSTOPINT)
+#define F_SLVUNEXPBUSSTOPINT    V_SLVUNEXPBUSSTOPINT(1U)
+
+#define S_SLVUNEXPBUSSTARTINT    17
+#define V_SLVUNEXPBUSSTARTINT(x) ((x) << S_SLVUNEXPBUSSTARTINT)
+#define F_SLVUNEXPBUSSTARTINT    V_SLVUNEXPBUSSTARTINT(1U)
+
+#define S_SLVCOMMANDCODEINVINT    16
+#define V_SLVCOMMANDCODEINVINT(x) ((x) << S_SLVCOMMANDCODEINVINT)
+#define F_SLVCOMMANDCODEINVINT    V_SLVCOMMANDCODEINVINT(1U)
+
+#define S_SLVBYTECNTERRINT    15
+#define V_SLVBYTECNTERRINT(x) ((x) << S_SLVBYTECNTERRINT)
+#define F_SLVBYTECNTERRINT    V_SLVBYTECNTERRINT(1U)
+
+#define S_SLVUNEXPACKMSTINT    14
+#define V_SLVUNEXPACKMSTINT(x) ((x) << S_SLVUNEXPACKMSTINT)
+#define F_SLVUNEXPACKMSTINT    V_SLVUNEXPACKMSTINT(1U)
+
+#define S_SLVUNEXPNACKMSTINT    13
+#define V_SLVUNEXPNACKMSTINT(x) ((x) << S_SLVUNEXPNACKMSTINT)
+#define F_SLVUNEXPNACKMSTINT    V_SLVUNEXPNACKMSTINT(1U)
+
+#define S_SLVNOBUSSTOPINT    12
+#define V_SLVNOBUSSTOPINT(x) ((x) << S_SLVNOBUSSTOPINT)
+#define F_SLVNOBUSSTOPINT    V_SLVNOBUSSTOPINT(1U)
+
+#define S_SLVNOREPSTARTINT    11
+#define V_SLVNOREPSTARTINT(x) ((x) << S_SLVNOREPSTARTINT)
+#define F_SLVNOREPSTARTINT    V_SLVNOREPSTARTINT(1U)
+
+#define S_SLVRXADDRINT    10
+#define V_SLVRXADDRINT(x) ((x) << S_SLVRXADDRINT)
+#define F_SLVRXADDRINT    V_SLVRXADDRINT(1U)
+
+#define S_SLVRXPECERRINT    9
+#define V_SLVRXPECERRINT(x) ((x) << S_SLVRXPECERRINT)
+#define F_SLVRXPECERRINT    V_SLVRXPECERRINT(1U)
+
+#define S_SLVPREPTOARPINT    8
+#define V_SLVPREPTOARPINT(x) ((x) << S_SLVPREPTOARPINT)
+#define F_SLVPREPTOARPINT    V_SLVPREPTOARPINT(1U)
+
+#define S_SLVTIMEOUTINT    7
+#define V_SLVTIMEOUTINT(x) ((x) << S_SLVTIMEOUTINT)
+#define F_SLVTIMEOUTINT    V_SLVTIMEOUTINT(1U)
+
+#define S_SLVERRINT    6
+#define V_SLVERRINT(x) ((x) << S_SLVERRINT)
+#define F_SLVERRINT    V_SLVERRINT(1U)
+
+#define S_SLVDONEINT    5
+#define V_SLVDONEINT(x) ((x) << S_SLVDONEINT)
+#define F_SLVDONEINT    V_SLVDONEINT(1U)
+
+#define S_SLVRXRDYINT    4
+#define V_SLVRXRDYINT(x) ((x) << S_SLVRXRDYINT)
+#define F_SLVRXRDYINT    V_SLVRXRDYINT(1U)
+
+#define S_MSTTIMEOUTINT    3
+#define V_MSTTIMEOUTINT(x) ((x) << S_MSTTIMEOUTINT)
+#define F_MSTTIMEOUTINT    V_MSTTIMEOUTINT(1U)
+
+#define S_MSTNACKINT    2
+#define V_MSTNACKINT(x) ((x) << S_MSTNACKINT)
+#define F_MSTNACKINT    V_MSTNACKINT(1U)
+
+#define S_MSTLOSTARBINT    1
+#define V_MSTLOSTARBINT(x) ((x) << S_MSTLOSTARBINT)
+#define F_MSTLOSTARBINT    V_MSTLOSTARBINT(1U)
+
+#define S_MSTDONEINT    0
+#define V_MSTDONEINT(x) ((x) << S_MSTDONEINT)
+#define F_MSTDONEINT    V_MSTDONEINT(1U)
+
+#define A_SMB_DEBUG_DATA 0x19094
+
+#define S_DEBUGDATAH    16
+#define M_DEBUGDATAH    0xffffU
+#define V_DEBUGDATAH(x) ((x) << S_DEBUGDATAH)
+#define G_DEBUGDATAH(x) (((x) >> S_DEBUGDATAH) & M_DEBUGDATAH)
+
+#define S_DEBUGDATAL    0
+#define M_DEBUGDATAL    0xffffU
+#define V_DEBUGDATAL(x) ((x) << S_DEBUGDATAL)
+#define G_DEBUGDATAL(x) (((x) >> S_DEBUGDATAL) & M_DEBUGDATAL)
+
+#define A_SMB_PERR_EN 0x19098
+
+#define S_MSTTXFIFOPERREN    2
+#define V_MSTTXFIFOPERREN(x) ((x) << S_MSTTXFIFOPERREN)
+#define F_MSTTXFIFOPERREN    V_MSTTXFIFOPERREN(1U)
+
+#define S_MSTRXFIFOPERREN    1
+#define V_MSTRXFIFOPERREN(x) ((x) << S_MSTRXFIFOPERREN)
+#define F_MSTRXFIFOPERREN    V_MSTRXFIFOPERREN(1U)
+
+#define S_SLVFIFOPERREN    0
+#define V_SLVFIFOPERREN(x) ((x) << S_SLVFIFOPERREN)
+#define F_SLVFIFOPERREN    V_SLVFIFOPERREN(1U)
+
+#define A_SMB_PERR_INJ 0x1909c
+
+#define S_MSTTXINJDATAERR    3
+#define V_MSTTXINJDATAERR(x) ((x) << S_MSTTXINJDATAERR)
+#define F_MSTTXINJDATAERR    V_MSTTXINJDATAERR(1U)
+
+#define S_MSTRXINJDATAERR    2
+#define V_MSTRXINJDATAERR(x) ((x) << S_MSTRXINJDATAERR)
+#define F_MSTRXINJDATAERR    V_MSTRXINJDATAERR(1U)
+
+#define S_SLVINJDATAERR    1
+#define V_SLVINJDATAERR(x) ((x) << S_SLVINJDATAERR)
+#define F_SLVINJDATAERR    V_SLVINJDATAERR(1U)
+
+#define S_FIFOINJDATAERREN    0
+#define V_FIFOINJDATAERREN(x) ((x) << S_FIFOINJDATAERREN)
+#define F_FIFOINJDATAERREN    V_FIFOINJDATAERREN(1U)
+
+#define A_SMB_SLV_ARP_CTL 0x190a0
+
+#define S_ARPCOMMANDCODE    2
+#define M_ARPCOMMANDCODE    0xffU
+#define V_ARPCOMMANDCODE(x) ((x) << S_ARPCOMMANDCODE)
+#define G_ARPCOMMANDCODE(x) (((x) >> S_ARPCOMMANDCODE) & M_ARPCOMMANDCODE)
+
+#define S_ARPADDRRES    1
+#define V_ARPADDRRES(x) ((x) << S_ARPADDRRES)
+#define F_ARPADDRRES    V_ARPADDRRES(1U)
+
+#define S_ARPADDRVAL    0
+#define V_ARPADDRVAL(x) ((x) << S_ARPADDRVAL)
+#define F_ARPADDRVAL    V_ARPADDRVAL(1U)
+
+#define A_SMB_ARP_UDID0 0x190a4
+#define A_SMB_ARP_UDID1 0x190a8
+
+#define S_SUBSYSTEMVENDORID    16
+#define M_SUBSYSTEMVENDORID    0xffffU
+#define V_SUBSYSTEMVENDORID(x) ((x) << S_SUBSYSTEMVENDORID)
+#define G_SUBSYSTEMVENDORID(x) (((x) >> S_SUBSYSTEMVENDORID) & M_SUBSYSTEMVENDORID)
+
+#define S_SUBSYSTEMDEVICEID    0
+#define M_SUBSYSTEMDEVICEID    0xffffU
+#define V_SUBSYSTEMDEVICEID(x) ((x) << S_SUBSYSTEMDEVICEID)
+#define G_SUBSYSTEMDEVICEID(x) (((x) >> S_SUBSYSTEMDEVICEID) & M_SUBSYSTEMDEVICEID)
+
+#define A_SMB_ARP_UDID2 0x190ac
+
+#define S_DEVICEID    16
+#define M_DEVICEID    0xffffU
+#define V_DEVICEID(x) ((x) << S_DEVICEID)
+#define G_DEVICEID(x) (((x) >> S_DEVICEID) & M_DEVICEID)
+
+#define S_INTERFACE    0
+#define M_INTERFACE    0xffffU
+#define V_INTERFACE(x) ((x) << S_INTERFACE)
+#define G_INTERFACE(x) (((x) >> S_INTERFACE) & M_INTERFACE)
+
+#define A_SMB_ARP_UDID3 0x190b0
+
+#define S_DEVICECAP    24
+#define M_DEVICECAP    0xffU
+#define V_DEVICECAP(x) ((x) << S_DEVICECAP)
+#define G_DEVICECAP(x) (((x) >> S_DEVICECAP) & M_DEVICECAP)
+
+#define S_VERSIONID    16
+#define M_VERSIONID    0xffU
+#define V_VERSIONID(x) ((x) << S_VERSIONID)
+#define G_VERSIONID(x) (((x) >> S_VERSIONID) & M_VERSIONID)
+
+#define S_VENDORID    0
+#define M_VENDORID    0xffffU
+#define V_VENDORID(x) ((x) << S_VENDORID)
+#define G_VENDORID(x) (((x) >> S_VENDORID) & M_VENDORID)
+
+#define A_SMB_SLV_AUX_ADDR0 0x190b4
+
+#define S_AUXADDR0VAL    6
+#define V_AUXADDR0VAL(x) ((x) << S_AUXADDR0VAL)
+#define F_AUXADDR0VAL    V_AUXADDR0VAL(1U)
+
+#define S_AUXADDR0    0
+#define M_AUXADDR0    0x3fU
+#define V_AUXADDR0(x) ((x) << S_AUXADDR0)
+#define G_AUXADDR0(x) (((x) >> S_AUXADDR0) & M_AUXADDR0)
+
+#define A_SMB_SLV_AUX_ADDR1 0x190b8
+
+#define S_AUXADDR1VAL    6
+#define V_AUXADDR1VAL(x) ((x) << S_AUXADDR1VAL)
+#define F_AUXADDR1VAL    V_AUXADDR1VAL(1U)
+
+#define S_AUXADDR1    0
+#define M_AUXADDR1    0x3fU
+#define V_AUXADDR1(x) ((x) << S_AUXADDR1)
+#define G_AUXADDR1(x) (((x) >> S_AUXADDR1) & M_AUXADDR1)
+
+#define A_SMB_SLV_AUX_ADDR2 0x190bc
+
+#define S_AUXADDR2VAL    6
+#define V_AUXADDR2VAL(x) ((x) << S_AUXADDR2VAL)
+#define F_AUXADDR2VAL    V_AUXADDR2VAL(1U)
+
+#define S_AUXADDR2    0
+#define M_AUXADDR2    0x3fU
+#define V_AUXADDR2(x) ((x) << S_AUXADDR2)
+#define G_AUXADDR2(x) (((x) >> S_AUXADDR2) & M_AUXADDR2)
+
+#define A_SMB_SLV_AUX_ADDR3 0x190c0
+
+#define S_AUXADDR3VAL    6
+#define V_AUXADDR3VAL(x) ((x) << S_AUXADDR3VAL)
+#define F_AUXADDR3VAL    V_AUXADDR3VAL(1U)
+
+#define S_AUXADDR3    0
+#define M_AUXADDR3    0x3fU
+#define V_AUXADDR3(x) ((x) << S_AUXADDR3)
+#define G_AUXADDR3(x) (((x) >> S_AUXADDR3) & M_AUXADDR3)
+
+#define A_SMB_COMMAND_CODE0 0x190c4
+
+#define S_SMBUSCOMMANDCODE0    0
+#define M_SMBUSCOMMANDCODE0    0xffU
+#define V_SMBUSCOMMANDCODE0(x) ((x) << S_SMBUSCOMMANDCODE0)
+#define G_SMBUSCOMMANDCODE0(x) (((x) >> S_SMBUSCOMMANDCODE0) & M_SMBUSCOMMANDCODE0)
+
+#define A_SMB_COMMAND_CODE1 0x190c8
+
+#define S_SMBUSCOMMANDCODE1    0
+#define M_SMBUSCOMMANDCODE1    0xffU
+#define V_SMBUSCOMMANDCODE1(x) ((x) << S_SMBUSCOMMANDCODE1)
+#define G_SMBUSCOMMANDCODE1(x) (((x) >> S_SMBUSCOMMANDCODE1) & M_SMBUSCOMMANDCODE1)
+
+#define A_SMB_COMMAND_CODE2 0x190cc
+
+#define S_SMBUSCOMMANDCODE2    0
+#define M_SMBUSCOMMANDCODE2    0xffU
+#define V_SMBUSCOMMANDCODE2(x) ((x) << S_SMBUSCOMMANDCODE2)
+#define G_SMBUSCOMMANDCODE2(x) (((x) >> S_SMBUSCOMMANDCODE2) & M_SMBUSCOMMANDCODE2)
+
+#define A_SMB_COMMAND_CODE3 0x190d0
+
+#define S_SMBUSCOMMANDCODE3    0
+#define M_SMBUSCOMMANDCODE3    0xffU
+#define V_SMBUSCOMMANDCODE3(x) ((x) << S_SMBUSCOMMANDCODE3)
+#define G_SMBUSCOMMANDCODE3(x) (((x) >> S_SMBUSCOMMANDCODE3) & M_SMBUSCOMMANDCODE3)
+
+#define A_SMB_COMMAND_CODE4 0x190d4
+
+#define S_SMBUSCOMMANDCODE4    0
+#define M_SMBUSCOMMANDCODE4    0xffU
+#define V_SMBUSCOMMANDCODE4(x) ((x) << S_SMBUSCOMMANDCODE4)
+#define G_SMBUSCOMMANDCODE4(x) (((x) >> S_SMBUSCOMMANDCODE4) & M_SMBUSCOMMANDCODE4)
+
+#define A_SMB_COMMAND_CODE5 0x190d8
+
+#define S_SMBUSCOMMANDCODE5    0
+#define M_SMBUSCOMMANDCODE5    0xffU
+#define V_SMBUSCOMMANDCODE5(x) ((x) << S_SMBUSCOMMANDCODE5)
+#define G_SMBUSCOMMANDCODE5(x) (((x) >> S_SMBUSCOMMANDCODE5) & M_SMBUSCOMMANDCODE5)
+
+#define A_SMB_COMMAND_CODE6 0x190dc
+
+#define S_SMBUSCOMMANDCODE6    0
+#define M_SMBUSCOMMANDCODE6    0xffU
+#define V_SMBUSCOMMANDCODE6(x) ((x) << S_SMBUSCOMMANDCODE6)
+#define G_SMBUSCOMMANDCODE6(x) (((x) >> S_SMBUSCOMMANDCODE6) & M_SMBUSCOMMANDCODE6)
+
+#define A_SMB_COMMAND_CODE7 0x190e0
+
+#define S_SMBUSCOMMANDCODE7    0
+#define M_SMBUSCOMMANDCODE7    0xffU
+#define V_SMBUSCOMMANDCODE7(x) ((x) << S_SMBUSCOMMANDCODE7)
+#define G_SMBUSCOMMANDCODE7(x) (((x) >> S_SMBUSCOMMANDCODE7) & M_SMBUSCOMMANDCODE7)
+
+#define A_SMB_MICRO_CNT_CLK_CFG 0x190e4
+
+#define S_MACROCNTCLKCFG    8
+#define M_MACROCNTCLKCFG    0x1fU
+#define V_MACROCNTCLKCFG(x) ((x) << S_MACROCNTCLKCFG)
+#define G_MACROCNTCLKCFG(x) (((x) >> S_MACROCNTCLKCFG) & M_MACROCNTCLKCFG)
+
+#define S_MICROCNTCLKCFG    0
+#define M_MICROCNTCLKCFG    0xffU
+#define V_MICROCNTCLKCFG(x) ((x) << S_MICROCNTCLKCFG)
+#define G_MICROCNTCLKCFG(x) (((x) >> S_MICROCNTCLKCFG) & M_MICROCNTCLKCFG)
+
+/* registers for module I2CM */
+#define I2CM_BASE_ADDR 0x190f0
+
+#define A_I2CM_CFG 0x190f0
+
+#define S_I2C_CLKDIV    0
+#define M_I2C_CLKDIV    0xfffU
+#define V_I2C_CLKDIV(x) ((x) << S_I2C_CLKDIV)
+#define G_I2C_CLKDIV(x) (((x) >> S_I2C_CLKDIV) & M_I2C_CLKDIV)
+
+#define A_I2CM_DATA 0x190f4
+
+#define S_I2C_DATA    0
+#define M_I2C_DATA    0xffU
+#define V_I2C_DATA(x) ((x) << S_I2C_DATA)
+#define G_I2C_DATA(x) (((x) >> S_I2C_DATA) & M_I2C_DATA)
+
+#define A_I2CM_OP 0x190f8
+
+#define S_I2C_ACK    30
+#define V_I2C_ACK(x) ((x) << S_I2C_ACK)
+#define F_I2C_ACK    V_I2C_ACK(1U)
+
+#define S_I2C_CONT    1
+#define V_I2C_CONT(x) ((x) << S_I2C_CONT)
+#define F_I2C_CONT    V_I2C_CONT(1U)
+
+#define S_OP    0
+#define V_OP(x) ((x) << S_OP)
+#define F_OP    V_OP(1U)
+
+/* registers for module MI */
+#define MI_BASE_ADDR 0x19100
+
+#define A_MI_CFG 0x19100
+
+#define S_T4_ST    14
+#define V_T4_ST(x) ((x) << S_T4_ST)
+#define F_T4_ST    V_T4_ST(1U)
+
+#define S_CLKDIV    5
+#define M_CLKDIV    0xffU
+#define V_CLKDIV(x) ((x) << S_CLKDIV)
+#define G_CLKDIV(x) (((x) >> S_CLKDIV) & M_CLKDIV)
+
+#define S_ST    3
+#define M_ST    0x3U
+#define V_ST(x) ((x) << S_ST)
+#define G_ST(x) (((x) >> S_ST) & M_ST)
+
+#define S_PREEN    2
+#define V_PREEN(x) ((x) << S_PREEN)
+#define F_PREEN    V_PREEN(1U)
+
+#define S_MDIINV    1
+#define V_MDIINV(x) ((x) << S_MDIINV)
+#define F_MDIINV    V_MDIINV(1U)
+
+#define S_MDIO_1P2V_SEL    0
+#define V_MDIO_1P2V_SEL(x) ((x) << S_MDIO_1P2V_SEL)
+#define F_MDIO_1P2V_SEL    V_MDIO_1P2V_SEL(1U)
+
+#define A_MI_ADDR 0x19104
+
+#define S_PHYADDR    5
+#define M_PHYADDR    0x1fU
+#define V_PHYADDR(x) ((x) << S_PHYADDR)
+#define G_PHYADDR(x) (((x) >> S_PHYADDR) & M_PHYADDR)
+
+#define S_REGADDR    0
+#define M_REGADDR    0x1fU
+#define V_REGADDR(x) ((x) << S_REGADDR)
+#define G_REGADDR(x) (((x) >> S_REGADDR) & M_REGADDR)
+
+#define A_MI_DATA 0x19108
+
+#define S_MDIDATA    0
+#define M_MDIDATA    0xffffU
+#define V_MDIDATA(x) ((x) << S_MDIDATA)
+#define G_MDIDATA(x) (((x) >> S_MDIDATA) & M_MDIDATA)
+
+#define A_MI_OP 0x1910c
+
+#define S_INC    2
+#define V_INC(x) ((x) << S_INC)
+#define F_INC    V_INC(1U)
+
+#define S_MDIOP    0
+#define M_MDIOP    0x3U
+#define V_MDIOP(x) ((x) << S_MDIOP)
+#define G_MDIOP(x) (((x) >> S_MDIOP) & M_MDIOP)
+
+/* registers for module UART */
+#define UART_BASE_ADDR 0x19110
+
+#define A_UART_CONFIG 0x19110
+
+#define S_STOPBITS    22
+#define M_STOPBITS    0x3U
+#define V_STOPBITS(x) ((x) << S_STOPBITS)
+#define G_STOPBITS(x) (((x) >> S_STOPBITS) & M_STOPBITS)
+
+#define S_PARITY    20
+#define M_PARITY    0x3U
+#define V_PARITY(x) ((x) << S_PARITY)
+#define G_PARITY(x) (((x) >> S_PARITY) & M_PARITY)
+
+#define S_DATABITS    16
+#define M_DATABITS    0xfU
+#define V_DATABITS(x) ((x) << S_DATABITS)
+#define G_DATABITS(x) (((x) >> S_DATABITS) & M_DATABITS)
+
+#define S_UART_CLKDIV    0
+#define M_UART_CLKDIV    0xfffU
+#define V_UART_CLKDIV(x) ((x) << S_UART_CLKDIV)
+#define G_UART_CLKDIV(x) (((x) >> S_UART_CLKDIV) & M_UART_CLKDIV)
+
+/* registers for module PMU */
+#define PMU_BASE_ADDR 0x19120
+
+#define A_PMU_PART_CG_PWRMODE 0x19120
+
+#define S_TPPARTCGEN    14
+#define V_TPPARTCGEN(x) ((x) << S_TPPARTCGEN)
+#define F_TPPARTCGEN    V_TPPARTCGEN(1U)
+
+#define S_PDPPARTCGEN    13
+#define V_PDPPARTCGEN(x) ((x) << S_PDPPARTCGEN)
+#define F_PDPPARTCGEN    V_PDPPARTCGEN(1U)
+
+#define S_PCIEPARTCGEN    12
+#define V_PCIEPARTCGEN(x) ((x) << S_PCIEPARTCGEN)
+#define F_PCIEPARTCGEN    V_PCIEPARTCGEN(1U)
+
+#define S_EDC1PARTCGEN    11
+#define V_EDC1PARTCGEN(x) ((x) << S_EDC1PARTCGEN)
+#define F_EDC1PARTCGEN    V_EDC1PARTCGEN(1U)
+
+#define S_MCPARTCGEN    10
+#define V_MCPARTCGEN(x) ((x) << S_MCPARTCGEN)
+#define F_MCPARTCGEN    V_MCPARTCGEN(1U)
+
+#define S_EDC0PARTCGEN    9
+#define V_EDC0PARTCGEN(x) ((x) << S_EDC0PARTCGEN)
+#define F_EDC0PARTCGEN    V_EDC0PARTCGEN(1U)
+
+#define S_LEPARTCGEN    8
+#define V_LEPARTCGEN(x) ((x) << S_LEPARTCGEN)
+#define F_LEPARTCGEN    V_LEPARTCGEN(1U)
+
+#define S_INITPOWERMODE    0
+#define M_INITPOWERMODE    0x3U
+#define V_INITPOWERMODE(x) ((x) << S_INITPOWERMODE)
+#define G_INITPOWERMODE(x) (((x) >> S_INITPOWERMODE) & M_INITPOWERMODE)
+
+#define A_PMU_SLEEPMODE_WAKEUP 0x19124
+
+#define S_HWWAKEUPEN    5
+#define V_HWWAKEUPEN(x) ((x) << S_HWWAKEUPEN)
+#define F_HWWAKEUPEN    V_HWWAKEUPEN(1U)
+
+#define S_PORT3SLEEPMODE    4
+#define V_PORT3SLEEPMODE(x) ((x) << S_PORT3SLEEPMODE)
+#define F_PORT3SLEEPMODE    V_PORT3SLEEPMODE(1U)
+
+#define S_PORT2SLEEPMODE    3
+#define V_PORT2SLEEPMODE(x) ((x) << S_PORT2SLEEPMODE)
+#define F_PORT2SLEEPMODE    V_PORT2SLEEPMODE(1U)
+
+#define S_PORT1SLEEPMODE    2
+#define V_PORT1SLEEPMODE(x) ((x) << S_PORT1SLEEPMODE)
+#define F_PORT1SLEEPMODE    V_PORT1SLEEPMODE(1U)
+
+#define S_PORT0SLEEPMODE    1
+#define V_PORT0SLEEPMODE(x) ((x) << S_PORT0SLEEPMODE)
+#define F_PORT0SLEEPMODE    V_PORT0SLEEPMODE(1U)
+
+#define S_WAKEUP    0
+#define V_WAKEUP(x) ((x) << S_WAKEUP)
+#define F_WAKEUP    V_WAKEUP(1U)
+
+/* registers for module ULP_RX */
+#define ULP_RX_BASE_ADDR 0x19150
+
+#define A_ULP_RX_CTL 0x19150
+
+#define S_PCMD1THRESHOLD    24
+#define M_PCMD1THRESHOLD    0xffU
+#define V_PCMD1THRESHOLD(x) ((x) << S_PCMD1THRESHOLD)
+#define G_PCMD1THRESHOLD(x) (((x) >> S_PCMD1THRESHOLD) & M_PCMD1THRESHOLD)
+
+#define S_PCMD0THRESHOLD    16
+#define M_PCMD0THRESHOLD    0xffU
+#define V_PCMD0THRESHOLD(x) ((x) << S_PCMD0THRESHOLD)
+#define G_PCMD0THRESHOLD(x) (((x) >> S_PCMD0THRESHOLD) & M_PCMD0THRESHOLD)
+
+#define S_DISABLE_0B_STAG_ERR    14
+#define V_DISABLE_0B_STAG_ERR(x) ((x) << S_DISABLE_0B_STAG_ERR)
+#define F_DISABLE_0B_STAG_ERR    V_DISABLE_0B_STAG_ERR(1U)
+
+#define S_RDMA_0B_WR_OPCODE    10
+#define M_RDMA_0B_WR_OPCODE    0xfU
+#define V_RDMA_0B_WR_OPCODE(x) ((x) << S_RDMA_0B_WR_OPCODE)
+#define G_RDMA_0B_WR_OPCODE(x) (((x) >> S_RDMA_0B_WR_OPCODE) & M_RDMA_0B_WR_OPCODE)
+
+#define S_RDMA_0B_WR_PASS    9
+#define V_RDMA_0B_WR_PASS(x) ((x) << S_RDMA_0B_WR_PASS)
+#define F_RDMA_0B_WR_PASS    V_RDMA_0B_WR_PASS(1U)
+
+#define S_STAG_RQE    8
+#define V_STAG_RQE(x) ((x) << S_STAG_RQE)
+#define F_STAG_RQE    V_STAG_RQE(1U)
+
+#define S_RDMA_STATE_EN    7
+#define V_RDMA_STATE_EN(x) ((x) << S_RDMA_STATE_EN)
+#define F_RDMA_STATE_EN    V_RDMA_STATE_EN(1U)
+
+#define S_CRC1_EN    6
+#define V_CRC1_EN(x) ((x) << S_CRC1_EN)
+#define F_CRC1_EN    V_CRC1_EN(1U)
+
+#define S_RDMA_0B_WR_CQE    5
+#define V_RDMA_0B_WR_CQE(x) ((x) << S_RDMA_0B_WR_CQE)
+#define F_RDMA_0B_WR_CQE    V_RDMA_0B_WR_CQE(1U)
+
+#define S_PCIE_ATRB_EN    4
+#define V_PCIE_ATRB_EN(x) ((x) << S_PCIE_ATRB_EN)
+#define F_PCIE_ATRB_EN    V_PCIE_ATRB_EN(1U)
+
+#define S_RDMA_PERMISSIVE_MODE    3
+#define V_RDMA_PERMISSIVE_MODE(x) ((x) << S_RDMA_PERMISSIVE_MODE)
+#define F_RDMA_PERMISSIVE_MODE    V_RDMA_PERMISSIVE_MODE(1U)
+
+#define S_PAGEPODME    2
+#define V_PAGEPODME(x) ((x) << S_PAGEPODME)
+#define F_PAGEPODME    V_PAGEPODME(1U)
+
+#define S_ISCSITAGTCB    1
+#define V_ISCSITAGTCB(x) ((x) << S_ISCSITAGTCB)
+#define F_ISCSITAGTCB    V_ISCSITAGTCB(1U)
+
+#define S_TDDPTAGTCB    0
+#define V_TDDPTAGTCB(x) ((x) << S_TDDPTAGTCB)
+#define F_TDDPTAGTCB    V_TDDPTAGTCB(1U)
+
+#define A_ULP_RX_INT_ENABLE 0x19154
+
+#define S_ENABLE_CTX_1    24
+#define V_ENABLE_CTX_1(x) ((x) << S_ENABLE_CTX_1)
+#define F_ENABLE_CTX_1    V_ENABLE_CTX_1(1U)
+
+#define S_ENABLE_CTX_0    23
+#define V_ENABLE_CTX_0(x) ((x) << S_ENABLE_CTX_0)
+#define F_ENABLE_CTX_0    V_ENABLE_CTX_0(1U)
+
+#define S_ENABLE_FF    22
+#define V_ENABLE_FF(x) ((x) << S_ENABLE_FF)
+#define F_ENABLE_FF    V_ENABLE_FF(1U)
+
+#define S_ENABLE_APF_1    21
+#define V_ENABLE_APF_1(x) ((x) << S_ENABLE_APF_1)
+#define F_ENABLE_APF_1    V_ENABLE_APF_1(1U)
+
+#define S_ENABLE_APF_0    20
+#define V_ENABLE_APF_0(x) ((x) << S_ENABLE_APF_0)
+#define F_ENABLE_APF_0    V_ENABLE_APF_0(1U)
+
+#define S_ENABLE_AF_1    19
+#define V_ENABLE_AF_1(x) ((x) << S_ENABLE_AF_1)
+#define F_ENABLE_AF_1    V_ENABLE_AF_1(1U)
+
+#define S_ENABLE_AF_0    18
+#define V_ENABLE_AF_0(x) ((x) << S_ENABLE_AF_0)
+#define F_ENABLE_AF_0    V_ENABLE_AF_0(1U)
+
+#define S_ENABLE_PCMDF_1    17
+#define V_ENABLE_PCMDF_1(x) ((x) << S_ENABLE_PCMDF_1)
+#define F_ENABLE_PCMDF_1    V_ENABLE_PCMDF_1(1U)
+
+#define S_ENABLE_MPARC_1    16
+#define V_ENABLE_MPARC_1(x) ((x) << S_ENABLE_MPARC_1)
+#define F_ENABLE_MPARC_1    V_ENABLE_MPARC_1(1U)
+
+#define S_ENABLE_MPARF_1    15
+#define V_ENABLE_MPARF_1(x) ((x) << S_ENABLE_MPARF_1)
+#define F_ENABLE_MPARF_1    V_ENABLE_MPARF_1(1U)
+
+#define S_ENABLE_DDPCF_1    14
+#define V_ENABLE_DDPCF_1(x) ((x) << S_ENABLE_DDPCF_1)
+#define F_ENABLE_DDPCF_1    V_ENABLE_DDPCF_1(1U)
+
+#define S_ENABLE_TPTCF_1    13
+#define V_ENABLE_TPTCF_1(x) ((x) << S_ENABLE_TPTCF_1)
+#define F_ENABLE_TPTCF_1    V_ENABLE_TPTCF_1(1U)
+
+#define S_ENABLE_PCMDF_0    12
+#define V_ENABLE_PCMDF_0(x) ((x) << S_ENABLE_PCMDF_0)
+#define F_ENABLE_PCMDF_0    V_ENABLE_PCMDF_0(1U)
+
+#define S_ENABLE_MPARC_0    11
+#define V_ENABLE_MPARC_0(x) ((x) << S_ENABLE_MPARC_0)
+#define F_ENABLE_MPARC_0    V_ENABLE_MPARC_0(1U)
+
+#define S_ENABLE_MPARF_0    10
+#define V_ENABLE_MPARF_0(x) ((x) << S_ENABLE_MPARF_0)
+#define F_ENABLE_MPARF_0    V_ENABLE_MPARF_0(1U)
+
+#define S_ENABLE_DDPCF_0    9
+#define V_ENABLE_DDPCF_0(x) ((x) << S_ENABLE_DDPCF_0)
+#define F_ENABLE_DDPCF_0    V_ENABLE_DDPCF_0(1U)
+
+#define S_ENABLE_TPTCF_0    8
+#define V_ENABLE_TPTCF_0(x) ((x) << S_ENABLE_TPTCF_0)
+#define F_ENABLE_TPTCF_0    V_ENABLE_TPTCF_0(1U)
+
+#define S_ENABLE_DDPDF_1    7
+#define V_ENABLE_DDPDF_1(x) ((x) << S_ENABLE_DDPDF_1)
+#define F_ENABLE_DDPDF_1    V_ENABLE_DDPDF_1(1U)
+
+#define S_ENABLE_DDPMF_1    6
+#define V_ENABLE_DDPMF_1(x) ((x) << S_ENABLE_DDPMF_1)
+#define F_ENABLE_DDPMF_1    V_ENABLE_DDPMF_1(1U)
+
+#define S_ENABLE_MEMRF_1    5
+#define V_ENABLE_MEMRF_1(x) ((x) << S_ENABLE_MEMRF_1)
+#define F_ENABLE_MEMRF_1    V_ENABLE_MEMRF_1(1U)
+
+#define S_ENABLE_PRSDF_1    4
+#define V_ENABLE_PRSDF_1(x) ((x) << S_ENABLE_PRSDF_1)
+#define F_ENABLE_PRSDF_1    V_ENABLE_PRSDF_1(1U)
+
+#define S_ENABLE_DDPDF_0    3
+#define V_ENABLE_DDPDF_0(x) ((x) << S_ENABLE_DDPDF_0)
+#define F_ENABLE_DDPDF_0    V_ENABLE_DDPDF_0(1U)
+
+#define S_ENABLE_DDPMF_0    2
+#define V_ENABLE_DDPMF_0(x) ((x) << S_ENABLE_DDPMF_0)
+#define F_ENABLE_DDPMF_0    V_ENABLE_DDPMF_0(1U)
+
+#define S_ENABLE_MEMRF_0    1
+#define V_ENABLE_MEMRF_0(x) ((x) << S_ENABLE_MEMRF_0)
+#define F_ENABLE_MEMRF_0    V_ENABLE_MEMRF_0(1U)
+
+#define S_ENABLE_PRSDF_0    0
+#define V_ENABLE_PRSDF_0(x) ((x) << S_ENABLE_PRSDF_0)
+#define F_ENABLE_PRSDF_0    V_ENABLE_PRSDF_0(1U)
+
+#define A_ULP_RX_INT_CAUSE 0x19158
+
+#define S_CAUSE_CTX_1    24
+#define V_CAUSE_CTX_1(x) ((x) << S_CAUSE_CTX_1)
+#define F_CAUSE_CTX_1    V_CAUSE_CTX_1(1U)
+
+#define S_CAUSE_CTX_0    23
+#define V_CAUSE_CTX_0(x) ((x) << S_CAUSE_CTX_0)
+#define F_CAUSE_CTX_0    V_CAUSE_CTX_0(1U)
+
+#define S_CAUSE_FF    22
+#define V_CAUSE_FF(x) ((x) << S_CAUSE_FF)
+#define F_CAUSE_FF    V_CAUSE_FF(1U)
+
+#define S_CAUSE_APF_1    21
+#define V_CAUSE_APF_1(x) ((x) << S_CAUSE_APF_1)
+#define F_CAUSE_APF_1    V_CAUSE_APF_1(1U)
+
+#define S_CAUSE_APF_0    20
+#define V_CAUSE_APF_0(x) ((x) << S_CAUSE_APF_0)
+#define F_CAUSE_APF_0    V_CAUSE_APF_0(1U)
+
+#define S_CAUSE_AF_1    19
+#define V_CAUSE_AF_1(x) ((x) << S_CAUSE_AF_1)
+#define F_CAUSE_AF_1    V_CAUSE_AF_1(1U)
+
+#define S_CAUSE_AF_0    18
+#define V_CAUSE_AF_0(x) ((x) << S_CAUSE_AF_0)
+#define F_CAUSE_AF_0    V_CAUSE_AF_0(1U)
+
+#define S_CAUSE_PCMDF_1    17
+#define V_CAUSE_PCMDF_1(x) ((x) << S_CAUSE_PCMDF_1)
+#define F_CAUSE_PCMDF_1    V_CAUSE_PCMDF_1(1U)
+
+#define S_CAUSE_MPARC_1    16
+#define V_CAUSE_MPARC_1(x) ((x) << S_CAUSE_MPARC_1)
+#define F_CAUSE_MPARC_1    V_CAUSE_MPARC_1(1U)
+
+#define S_CAUSE_MPARF_1    15
+#define V_CAUSE_MPARF_1(x) ((x) << S_CAUSE_MPARF_1)
+#define F_CAUSE_MPARF_1    V_CAUSE_MPARF_1(1U)
+
+#define S_CAUSE_DDPCF_1    14
+#define V_CAUSE_DDPCF_1(x) ((x) << S_CAUSE_DDPCF_1)
+#define F_CAUSE_DDPCF_1    V_CAUSE_DDPCF_1(1U)
+
+#define S_CAUSE_TPTCF_1    13
+#define V_CAUSE_TPTCF_1(x) ((x) << S_CAUSE_TPTCF_1)
+#define F_CAUSE_TPTCF_1    V_CAUSE_TPTCF_1(1U)
+
+#define S_CAUSE_PCMDF_0    12
+#define V_CAUSE_PCMDF_0(x) ((x) << S_CAUSE_PCMDF_0)
+#define F_CAUSE_PCMDF_0    V_CAUSE_PCMDF_0(1U)
+
+#define S_CAUSE_MPARC_0    11
+#define V_CAUSE_MPARC_0(x) ((x) << S_CAUSE_MPARC_0)
+#define F_CAUSE_MPARC_0    V_CAUSE_MPARC_0(1U)
+
+#define S_CAUSE_MPARF_0    10
+#define V_CAUSE_MPARF_0(x) ((x) << S_CAUSE_MPARF_0)
+#define F_CAUSE_MPARF_0    V_CAUSE_MPARF_0(1U)
+
+#define S_CAUSE_DDPCF_0    9
+#define V_CAUSE_DDPCF_0(x) ((x) << S_CAUSE_DDPCF_0)
+#define F_CAUSE_DDPCF_0    V_CAUSE_DDPCF_0(1U)
+
+#define S_CAUSE_TPTCF_0    8
+#define V_CAUSE_TPTCF_0(x) ((x) << S_CAUSE_TPTCF_0)
+#define F_CAUSE_TPTCF_0    V_CAUSE_TPTCF_0(1U)
+
+#define S_CAUSE_DDPDF_1    7
+#define V_CAUSE_DDPDF_1(x) ((x) << S_CAUSE_DDPDF_1)
+#define F_CAUSE_DDPDF_1    V_CAUSE_DDPDF_1(1U)
+
+#define S_CAUSE_DDPMF_1    6
+#define V_CAUSE_DDPMF_1(x) ((x) << S_CAUSE_DDPMF_1)
+#define F_CAUSE_DDPMF_1    V_CAUSE_DDPMF_1(1U)
+
+#define S_CAUSE_MEMRF_1    5
+#define V_CAUSE_MEMRF_1(x) ((x) << S_CAUSE_MEMRF_1)
+#define F_CAUSE_MEMRF_1    V_CAUSE_MEMRF_1(1U)
+
+#define S_CAUSE_PRSDF_1    4
+#define V_CAUSE_PRSDF_1(x) ((x) << S_CAUSE_PRSDF_1)
+#define F_CAUSE_PRSDF_1    V_CAUSE_PRSDF_1(1U)
+
+#define S_CAUSE_DDPDF_0    3
+#define V_CAUSE_DDPDF_0(x) ((x) << S_CAUSE_DDPDF_0)
+#define F_CAUSE_DDPDF_0    V_CAUSE_DDPDF_0(1U)
+
+#define S_CAUSE_DDPMF_0    2
+#define V_CAUSE_DDPMF_0(x) ((x) << S_CAUSE_DDPMF_0)
+#define F_CAUSE_DDPMF_0    V_CAUSE_DDPMF_0(1U)
+
+#define S_CAUSE_MEMRF_0    1
+#define V_CAUSE_MEMRF_0(x) ((x) << S_CAUSE_MEMRF_0)
+#define F_CAUSE_MEMRF_0    V_CAUSE_MEMRF_0(1U)
+
+#define S_CAUSE_PRSDF_0    0
+#define V_CAUSE_PRSDF_0(x) ((x) << S_CAUSE_PRSDF_0)
+#define F_CAUSE_PRSDF_0    V_CAUSE_PRSDF_0(1U)
+
+#define A_ULP_RX_ISCSI_LLIMIT 0x1915c
+
+#define S_ISCSILLIMIT    6
+#define M_ISCSILLIMIT    0x3ffffffU
+#define V_ISCSILLIMIT(x) ((x) << S_ISCSILLIMIT)
+#define G_ISCSILLIMIT(x) (((x) >> S_ISCSILLIMIT) & M_ISCSILLIMIT)
+
+#define A_ULP_RX_ISCSI_ULIMIT 0x19160
+
+#define S_ISCSIULIMIT    6
+#define M_ISCSIULIMIT    0x3ffffffU
+#define V_ISCSIULIMIT(x) ((x) << S_ISCSIULIMIT)
+#define G_ISCSIULIMIT(x) (((x) >> S_ISCSIULIMIT) & M_ISCSIULIMIT)
+
+#define A_ULP_RX_ISCSI_TAGMASK 0x19164
+
+#define S_ISCSITAGMASK    6
+#define M_ISCSITAGMASK    0x3ffffffU
+#define V_ISCSITAGMASK(x) ((x) << S_ISCSITAGMASK)
+#define G_ISCSITAGMASK(x) (((x) >> S_ISCSITAGMASK) & M_ISCSITAGMASK)
+
+#define A_ULP_RX_ISCSI_PSZ 0x19168
+
+#define S_HPZ3    24
+#define M_HPZ3    0xfU
+#define V_HPZ3(x) ((x) << S_HPZ3)
+#define G_HPZ3(x) (((x) >> S_HPZ3) & M_HPZ3)
+
+#define S_HPZ2    16
+#define M_HPZ2    0xfU
+#define V_HPZ2(x) ((x) << S_HPZ2)
+#define G_HPZ2(x) (((x) >> S_HPZ2) & M_HPZ2)
+
+#define S_HPZ1    8
+#define M_HPZ1    0xfU
+#define V_HPZ1(x) ((x) << S_HPZ1)
+#define G_HPZ1(x) (((x) >> S_HPZ1) & M_HPZ1)
+
+#define S_HPZ0    0
+#define M_HPZ0    0xfU
+#define V_HPZ0(x) ((x) << S_HPZ0)
+#define G_HPZ0(x) (((x) >> S_HPZ0) & M_HPZ0)
+
+#define A_ULP_RX_TDDP_LLIMIT 0x1916c
+
+#define S_TDDPLLIMIT    6
+#define M_TDDPLLIMIT    0x3ffffffU
+#define V_TDDPLLIMIT(x) ((x) << S_TDDPLLIMIT)
+#define G_TDDPLLIMIT(x) (((x) >> S_TDDPLLIMIT) & M_TDDPLLIMIT)
+
+#define A_ULP_RX_TDDP_ULIMIT 0x19170
+
+#define S_TDDPULIMIT    6
+#define M_TDDPULIMIT    0x3ffffffU
+#define V_TDDPULIMIT(x) ((x) << S_TDDPULIMIT)
+#define G_TDDPULIMIT(x) (((x) >> S_TDDPULIMIT) & M_TDDPULIMIT)
+
+#define A_ULP_RX_TDDP_TAGMASK 0x19174
+
+#define S_TDDPTAGMASK    6
+#define M_TDDPTAGMASK    0x3ffffffU
+#define V_TDDPTAGMASK(x) ((x) << S_TDDPTAGMASK)
+#define G_TDDPTAGMASK(x) (((x) >> S_TDDPTAGMASK) & M_TDDPTAGMASK)
+
+#define A_ULP_RX_TDDP_PSZ 0x19178
+#define A_ULP_RX_STAG_LLIMIT 0x1917c
+#define A_ULP_RX_STAG_ULIMIT 0x19180
+#define A_ULP_RX_RQ_LLIMIT 0x19184
+#define A_ULP_RX_RQ_ULIMIT 0x19188
+#define A_ULP_RX_PBL_LLIMIT 0x1918c
+#define A_ULP_RX_PBL_ULIMIT 0x19190
+#define A_ULP_RX_CTX_BASE 0x19194
+#define A_ULP_RX_PERR_ENABLE 0x1919c
+#define A_ULP_RX_PERR_INJECT 0x191a0
+#define A_ULP_RX_RQUDP_LLIMIT 0x191a4
+#define A_ULP_RX_RQUDP_ULIMIT 0x191a8
+#define A_ULP_RX_CTX_ACC_CH0 0x191ac
+
+#define S_REQ    21
+#define V_REQ(x) ((x) << S_REQ)
+#define F_REQ    V_REQ(1U)
+
+#define S_WB    20
+#define V_WB(x) ((x) << S_WB)
+#define F_WB    V_WB(1U)
+
+#define S_ULPRX_TID    0
+#define M_ULPRX_TID    0xfffffU
+#define V_ULPRX_TID(x) ((x) << S_ULPRX_TID)
+#define G_ULPRX_TID(x) (((x) >> S_ULPRX_TID) & M_ULPRX_TID)
+
+#define A_ULP_RX_CTX_ACC_CH1 0x191b0
+#define A_ULP_RX_SE_CNT_ERR 0x191d0
+#define A_ULP_RX_SE_CNT_CLR 0x191d4
+
+#define S_CLRCHAN0    4
+#define M_CLRCHAN0    0xfU
+#define V_CLRCHAN0(x) ((x) << S_CLRCHAN0)
+#define G_CLRCHAN0(x) (((x) >> S_CLRCHAN0) & M_CLRCHAN0)
+
+#define S_CLRCHAN1    0
+#define M_CLRCHAN1    0xfU
+#define V_CLRCHAN1(x) ((x) << S_CLRCHAN1)
+#define G_CLRCHAN1(x) (((x) >> S_CLRCHAN1) & M_CLRCHAN1)
+
+#define A_ULP_RX_SE_CNT_CH0 0x191d8
+
+#define S_SOP_CNT_OUT0    28
+#define M_SOP_CNT_OUT0    0xfU
+#define V_SOP_CNT_OUT0(x) ((x) << S_SOP_CNT_OUT0)
+#define G_SOP_CNT_OUT0(x) (((x) >> S_SOP_CNT_OUT0) & M_SOP_CNT_OUT0)
+
+#define S_EOP_CNT_OUT0    24
+#define M_EOP_CNT_OUT0    0xfU
+#define V_EOP_CNT_OUT0(x) ((x) << S_EOP_CNT_OUT0)
+#define G_EOP_CNT_OUT0(x) (((x) >> S_EOP_CNT_OUT0) & M_EOP_CNT_OUT0)
+
+#define S_SOP_CNT_AL0    20
+#define M_SOP_CNT_AL0    0xfU
+#define V_SOP_CNT_AL0(x) ((x) << S_SOP_CNT_AL0)
+#define G_SOP_CNT_AL0(x) (((x) >> S_SOP_CNT_AL0) & M_SOP_CNT_AL0)
+
+#define S_EOP_CNT_AL0    16
+#define M_EOP_CNT_AL0    0xfU
+#define V_EOP_CNT_AL0(x) ((x) << S_EOP_CNT_AL0)
+#define G_EOP_CNT_AL0(x) (((x) >> S_EOP_CNT_AL0) & M_EOP_CNT_AL0)
+
+#define S_SOP_CNT_MR0    12
+#define M_SOP_CNT_MR0    0xfU
+#define V_SOP_CNT_MR0(x) ((x) << S_SOP_CNT_MR0)
+#define G_SOP_CNT_MR0(x) (((x) >> S_SOP_CNT_MR0) & M_SOP_CNT_MR0)
+
+#define S_EOP_CNT_MR0    8
+#define M_EOP_CNT_MR0    0xfU
+#define V_EOP_CNT_MR0(x) ((x) << S_EOP_CNT_MR0)
+#define G_EOP_CNT_MR0(x) (((x) >> S_EOP_CNT_MR0) & M_EOP_CNT_MR0)
+
+#define S_SOP_CNT_IN0    4
+#define M_SOP_CNT_IN0    0xfU
+#define V_SOP_CNT_IN0(x) ((x) << S_SOP_CNT_IN0)
+#define G_SOP_CNT_IN0(x) (((x) >> S_SOP_CNT_IN0) & M_SOP_CNT_IN0)
+
+#define S_EOP_CNT_IN0    0
+#define M_EOP_CNT_IN0    0xfU
+#define V_EOP_CNT_IN0(x) ((x) << S_EOP_CNT_IN0)
+#define G_EOP_CNT_IN0(x) (((x) >> S_EOP_CNT_IN0) & M_EOP_CNT_IN0)
+
+#define A_ULP_RX_SE_CNT_CH1 0x191dc
+
+#define S_SOP_CNT_OUT1    28
+#define M_SOP_CNT_OUT1    0xfU
+#define V_SOP_CNT_OUT1(x) ((x) << S_SOP_CNT_OUT1)
+#define G_SOP_CNT_OUT1(x) (((x) >> S_SOP_CNT_OUT1) & M_SOP_CNT_OUT1)
+
+#define S_EOP_CNT_OUT1    24
+#define M_EOP_CNT_OUT1    0xfU
+#define V_EOP_CNT_OUT1(x) ((x) << S_EOP_CNT_OUT1)
+#define G_EOP_CNT_OUT1(x) (((x) >> S_EOP_CNT_OUT1) & M_EOP_CNT_OUT1)
+
+#define S_SOP_CNT_AL1    20
+#define M_SOP_CNT_AL1    0xfU
+#define V_SOP_CNT_AL1(x) ((x) << S_SOP_CNT_AL1)
+#define G_SOP_CNT_AL1(x) (((x) >> S_SOP_CNT_AL1) & M_SOP_CNT_AL1)
+
+#define S_EOP_CNT_AL1    16
+#define M_EOP_CNT_AL1    0xfU
+#define V_EOP_CNT_AL1(x) ((x) << S_EOP_CNT_AL1)
+#define G_EOP_CNT_AL1(x) (((x) >> S_EOP_CNT_AL1) & M_EOP_CNT_AL1)
+
+#define S_SOP_CNT_MR1    12
+#define M_SOP_CNT_MR1    0xfU
+#define V_SOP_CNT_MR1(x) ((x) << S_SOP_CNT_MR1)
+#define G_SOP_CNT_MR1(x) (((x) >> S_SOP_CNT_MR1) & M_SOP_CNT_MR1)
+
+#define S_EOP_CNT_MR1    8
+#define M_EOP_CNT_MR1    0xfU
+#define V_EOP_CNT_MR1(x) ((x) << S_EOP_CNT_MR1)
+#define G_EOP_CNT_MR1(x) (((x) >> S_EOP_CNT_MR1) & M_EOP_CNT_MR1)
+
+#define S_SOP_CNT_IN1    4
+#define M_SOP_CNT_IN1    0xfU
+#define V_SOP_CNT_IN1(x) ((x) << S_SOP_CNT_IN1)
+#define G_SOP_CNT_IN1(x) (((x) >> S_SOP_CNT_IN1) & M_SOP_CNT_IN1)
+
+#define S_EOP_CNT_IN1    0
+#define M_EOP_CNT_IN1    0xfU
+#define V_EOP_CNT_IN1(x) ((x) << S_EOP_CNT_IN1)
+#define G_EOP_CNT_IN1(x) (((x) >> S_EOP_CNT_IN1) & M_EOP_CNT_IN1)
+
+#define A_ULP_RX_DBG_CTL 0x191e0
+
+#define S_EN_DBG_H    17
+#define V_EN_DBG_H(x) ((x) << S_EN_DBG_H)
+#define F_EN_DBG_H    V_EN_DBG_H(1U)
+
+#define S_EN_DBG_L    16
+#define V_EN_DBG_L(x) ((x) << S_EN_DBG_L)
+#define F_EN_DBG_L    V_EN_DBG_L(1U)
+
+#define S_SEL_H    8
+#define M_SEL_H    0xffU
+#define V_SEL_H(x) ((x) << S_SEL_H)
+#define G_SEL_H(x) (((x) >> S_SEL_H) & M_SEL_H)
+
+#define S_SEL_L    0
+#define M_SEL_L    0xffU
+#define V_SEL_L(x) ((x) << S_SEL_L)
+#define G_SEL_L(x) (((x) >> S_SEL_L) & M_SEL_L)
+
+#define A_ULP_RX_DBG_DATAH 0x191e4
+#define A_ULP_RX_DBG_DATAL 0x191e8
+#define A_ULP_RX_LA_CHNL 0x19238
+
+#define S_CHNL_SEL    0
+#define V_CHNL_SEL(x) ((x) << S_CHNL_SEL)
+#define F_CHNL_SEL    V_CHNL_SEL(1U)
+
+#define A_ULP_RX_LA_CTL 0x1923c
+
+#define S_TRC_SEL    0
+#define V_TRC_SEL(x) ((x) << S_TRC_SEL)
+#define F_TRC_SEL    V_TRC_SEL(1U)
+
+#define A_ULP_RX_LA_RDPTR 0x19240
+
+#define S_RD_PTR    0
+#define M_RD_PTR    0x1ffU
+#define V_RD_PTR(x) ((x) << S_RD_PTR)
+#define G_RD_PTR(x) (((x) >> S_RD_PTR) & M_RD_PTR)
+
+#define A_ULP_RX_LA_RDDATA 0x19244
+#define A_ULP_RX_LA_WRPTR 0x19248
+
+#define S_WR_PTR    0
+#define M_WR_PTR    0x1ffU
+#define V_WR_PTR(x) ((x) << S_WR_PTR)
+#define G_WR_PTR(x) (((x) >> S_WR_PTR) & M_WR_PTR)
+
+#define A_ULP_RX_LA_RESERVED 0x1924c
+
+/* registers for module SF */
+#define SF_BASE_ADDR 0x193f8
+
+#define A_SF_DATA 0x193f8
+#define A_SF_OP 0x193fc
+
+#define S_SF_LOCK    4
+#define V_SF_LOCK(x) ((x) << S_SF_LOCK)
+#define F_SF_LOCK    V_SF_LOCK(1U)
+
+#define S_CONT    3
+#define V_CONT(x) ((x) << S_CONT)
+#define F_CONT    V_CONT(1U)
+
+#define S_BYTECNT    1
+#define M_BYTECNT    0x3U
+#define V_BYTECNT(x) ((x) << S_BYTECNT)
+#define G_BYTECNT(x) (((x) >> S_BYTECNT) & M_BYTECNT)
+
+/* registers for module PL */
+#define PL_BASE_ADDR 0x19400
+
+#define A_PL_VF_WHOAMI 0x0
+
+#define S_PORTXMAP    24
+#define M_PORTXMAP    0x7U
+#define V_PORTXMAP(x) ((x) << S_PORTXMAP)
+#define G_PORTXMAP(x) (((x) >> S_PORTXMAP) & M_PORTXMAP)
+
+#define S_SOURCEBUS    16
+#define M_SOURCEBUS    0x3U
+#define V_SOURCEBUS(x) ((x) << S_SOURCEBUS)
+#define G_SOURCEBUS(x) (((x) >> S_SOURCEBUS) & M_SOURCEBUS)
+
+#define S_SOURCEPF    8
+#define M_SOURCEPF    0x7U
+#define V_SOURCEPF(x) ((x) << S_SOURCEPF)
+#define G_SOURCEPF(x) (((x) >> S_SOURCEPF) & M_SOURCEPF)
+
+#define S_ISVF    7
+#define V_ISVF(x) ((x) << S_ISVF)
+#define F_ISVF    V_ISVF(1U)
+
+#define S_VFID    0
+#define M_VFID    0x7fU
+#define V_VFID(x) ((x) << S_VFID)
+#define G_VFID(x) (((x) >> S_VFID) & M_VFID)
+
+#define A_PL_PF_INT_CAUSE 0x3c0
+
+#define S_PFSW    3
+#define V_PFSW(x) ((x) << S_PFSW)
+#define F_PFSW    V_PFSW(1U)
+
+#define S_PFSGE    2
+#define V_PFSGE(x) ((x) << S_PFSGE)
+#define F_PFSGE    V_PFSGE(1U)
+
+#define S_PFCIM    1
+#define V_PFCIM(x) ((x) << S_PFCIM)
+#define F_PFCIM    V_PFCIM(1U)
+
+#define S_PFMPS    0
+#define V_PFMPS(x) ((x) << S_PFMPS)
+#define F_PFMPS    V_PFMPS(1U)
+
+#define A_PL_PF_INT_ENABLE 0x3c4
+#define A_PL_PF_CTL 0x3c8
+
+#define S_SWINT    0
+#define V_SWINT(x) ((x) << S_SWINT)
+#define F_SWINT    V_SWINT(1U)
+
+#define A_PL_WHOAMI 0x19400
+#define A_PL_PERR_CAUSE 0x19404
+
+#define S_UART    28
+#define V_UART(x) ((x) << S_UART)
+#define F_UART    V_UART(1U)
+
+#define S_ULP_TX    27
+#define V_ULP_TX(x) ((x) << S_ULP_TX)
+#define F_ULP_TX    V_ULP_TX(1U)
+
+#define S_SGE    26
+#define V_SGE(x) ((x) << S_SGE)
+#define F_SGE    V_SGE(1U)
+
+#define S_HMA    25
+#define V_HMA(x) ((x) << S_HMA)
+#define F_HMA    V_HMA(1U)
+
+#define S_CPL_SWITCH    24
+#define V_CPL_SWITCH(x) ((x) << S_CPL_SWITCH)
+#define F_CPL_SWITCH    V_CPL_SWITCH(1U)
+
+#define S_ULP_RX    23
+#define V_ULP_RX(x) ((x) << S_ULP_RX)
+#define F_ULP_RX    V_ULP_RX(1U)
+
+#define S_PM_RX    22
+#define V_PM_RX(x) ((x) << S_PM_RX)
+#define F_PM_RX    V_PM_RX(1U)
+
+#define S_PM_TX    21
+#define V_PM_TX(x) ((x) << S_PM_TX)
+#define F_PM_TX    V_PM_TX(1U)
+
+#define S_MA    20
+#define V_MA(x) ((x) << S_MA)
+#define F_MA    V_MA(1U)
+
+#define S_TP    19
+#define V_TP(x) ((x) << S_TP)
+#define F_TP    V_TP(1U)
+
+#define S_LE    18
+#define V_LE(x) ((x) << S_LE)
+#define F_LE    V_LE(1U)
+
+#define S_EDC1    17
+#define V_EDC1(x) ((x) << S_EDC1)
+#define F_EDC1    V_EDC1(1U)
+
+#define S_EDC0    16
+#define V_EDC0(x) ((x) << S_EDC0)
+#define F_EDC0    V_EDC0(1U)
+
+#define S_MC    15
+#define V_MC(x) ((x) << S_MC)
+#define F_MC    V_MC(1U)
+
+#define S_PCIE    14
+#define V_PCIE(x) ((x) << S_PCIE)
+#define F_PCIE    V_PCIE(1U)
+
+#define S_PMU    13
+#define V_PMU(x) ((x) << S_PMU)
+#define F_PMU    V_PMU(1U)
+
+#define S_XGMAC_KR1    12
+#define V_XGMAC_KR1(x) ((x) << S_XGMAC_KR1)
+#define F_XGMAC_KR1    V_XGMAC_KR1(1U)
+
+#define S_XGMAC_KR0    11
+#define V_XGMAC_KR0(x) ((x) << S_XGMAC_KR0)
+#define F_XGMAC_KR0    V_XGMAC_KR0(1U)
+
+#define S_XGMAC1    10
+#define V_XGMAC1(x) ((x) << S_XGMAC1)
+#define F_XGMAC1    V_XGMAC1(1U)
+
+#define S_XGMAC0    9
+#define V_XGMAC0(x) ((x) << S_XGMAC0)
+#define F_XGMAC0    V_XGMAC0(1U)
+
+#define S_SMB    8
+#define V_SMB(x) ((x) << S_SMB)
+#define F_SMB    V_SMB(1U)
+
+#define S_SF    7
+#define V_SF(x) ((x) << S_SF)
+#define F_SF    V_SF(1U)
+
+#define S_PL    6
+#define V_PL(x) ((x) << S_PL)
+#define F_PL    V_PL(1U)
+
+#define S_NCSI    5
+#define V_NCSI(x) ((x) << S_NCSI)
+#define F_NCSI    V_NCSI(1U)
+
+#define S_MPS    4
+#define V_MPS(x) ((x) << S_MPS)
+#define F_MPS    V_MPS(1U)
+
+#define S_MI    3
+#define V_MI(x) ((x) << S_MI)
+#define F_MI    V_MI(1U)
+
+#define S_DBG    2
+#define V_DBG(x) ((x) << S_DBG)
+#define F_DBG    V_DBG(1U)
+
+#define S_I2CM    1
+#define V_I2CM(x) ((x) << S_I2CM)
+#define F_I2CM    V_I2CM(1U)
+
+#define S_CIM    0
+#define V_CIM(x) ((x) << S_CIM)
+#define F_CIM    V_CIM(1U)
+
+#define A_PL_PERR_ENABLE 0x19408
+#define A_PL_INT_CAUSE 0x1940c
+
+#define S_FLR    30
+#define V_FLR(x) ((x) << S_FLR)
+#define F_FLR    V_FLR(1U)
+
+#define S_SW_CIM    29
+#define V_SW_CIM(x) ((x) << S_SW_CIM)
+#define F_SW_CIM    V_SW_CIM(1U)
+
+#define A_PL_INT_ENABLE 0x19410
+#define A_PL_INT_MAP0 0x19414
+
+#define S_MAPNCSI    16
+#define M_MAPNCSI    0x1ffU
+#define V_MAPNCSI(x) ((x) << S_MAPNCSI)
+#define G_MAPNCSI(x) (((x) >> S_MAPNCSI) & M_MAPNCSI)
+
+#define S_MAPDEFAULT    0
+#define M_MAPDEFAULT    0x1ffU
+#define V_MAPDEFAULT(x) ((x) << S_MAPDEFAULT)
+#define G_MAPDEFAULT(x) (((x) >> S_MAPDEFAULT) & M_MAPDEFAULT)
+
+#define A_PL_INT_MAP1 0x19418
+
+#define S_MAPXGMAC1    16
+#define M_MAPXGMAC1    0x1ffU
+#define V_MAPXGMAC1(x) ((x) << S_MAPXGMAC1)
+#define G_MAPXGMAC1(x) (((x) >> S_MAPXGMAC1) & M_MAPXGMAC1)
+
+#define S_MAPXGMAC0    0
+#define M_MAPXGMAC0    0x1ffU
+#define V_MAPXGMAC0(x) ((x) << S_MAPXGMAC0)
+#define G_MAPXGMAC0(x) (((x) >> S_MAPXGMAC0) & M_MAPXGMAC0)
+
+#define A_PL_INT_MAP2 0x1941c
+
+#define S_MAPXGMAC_KR1    16
+#define M_MAPXGMAC_KR1    0x1ffU
+#define V_MAPXGMAC_KR1(x) ((x) << S_MAPXGMAC_KR1)
+#define G_MAPXGMAC_KR1(x) (((x) >> S_MAPXGMAC_KR1) & M_MAPXGMAC_KR1)
+
+#define S_MAPXGMAC_KR0    0
+#define M_MAPXGMAC_KR0    0x1ffU
+#define V_MAPXGMAC_KR0(x) ((x) << S_MAPXGMAC_KR0)
+#define G_MAPXGMAC_KR0(x) (((x) >> S_MAPXGMAC_KR0) & M_MAPXGMAC_KR0)
+
+#define A_PL_INT_MAP3 0x19420
+
+#define S_MAPMI    16
+#define M_MAPMI    0x1ffU
+#define V_MAPMI(x) ((x) << S_MAPMI)
+#define G_MAPMI(x) (((x) >> S_MAPMI) & M_MAPMI)
+
+#define S_MAPSMB    0
+#define M_MAPSMB    0x1ffU
+#define V_MAPSMB(x) ((x) << S_MAPSMB)
+#define G_MAPSMB(x) (((x) >> S_MAPSMB) & M_MAPSMB)
+
+#define A_PL_INT_MAP4 0x19424
+
+#define S_MAPDBG    16
+#define M_MAPDBG    0x1ffU
+#define V_MAPDBG(x) ((x) << S_MAPDBG)
+#define G_MAPDBG(x) (((x) >> S_MAPDBG) & M_MAPDBG)
+
+#define S_MAPI2CM    0
+#define M_MAPI2CM    0x1ffU
+#define V_MAPI2CM(x) ((x) << S_MAPI2CM)
+#define G_MAPI2CM(x) (((x) >> S_MAPI2CM) & M_MAPI2CM)
+
+#define A_PL_RST 0x19428
+
+#define S_FATALPERREN    3
+#define V_FATALPERREN(x) ((x) << S_FATALPERREN)
+#define F_FATALPERREN    V_FATALPERREN(1U)
+
+#define S_SWINTCIM    2
+#define V_SWINTCIM(x) ((x) << S_SWINTCIM)
+#define F_SWINTCIM    V_SWINTCIM(1U)
+
+#define S_PIORST    1
+#define V_PIORST(x) ((x) << S_PIORST)
+#define F_PIORST    V_PIORST(1U)
+
+#define S_PIORSTMODE    0
+#define V_PIORSTMODE(x) ((x) << S_PIORSTMODE)
+#define F_PIORSTMODE    V_PIORSTMODE(1U)
+
+#define A_PL_PL_PERR_INJECT 0x1942c
+
+#define S_PL_MEMSEL    1
+#define V_PL_MEMSEL(x) ((x) << S_PL_MEMSEL)
+#define F_PL_MEMSEL    V_PL_MEMSEL(1U)
+
+#define A_PL_PL_INT_CAUSE 0x19430
+
+#define S_PF_ENABLEERR    5
+#define V_PF_ENABLEERR(x) ((x) << S_PF_ENABLEERR)
+#define F_PF_ENABLEERR    V_PF_ENABLEERR(1U)
+
+#define S_FATALPERR    4
+#define V_FATALPERR(x) ((x) << S_FATALPERR)
+#define F_FATALPERR    V_FATALPERR(1U)
+
+#define S_INVALIDACCESS    3
+#define V_INVALIDACCESS(x) ((x) << S_INVALIDACCESS)
+#define F_INVALIDACCESS    V_INVALIDACCESS(1U)
+
+#define S_TIMEOUT    2
+#define V_TIMEOUT(x) ((x) << S_TIMEOUT)
+#define F_TIMEOUT    V_TIMEOUT(1U)
+
+#define S_PLERR    1
+#define V_PLERR(x) ((x) << S_PLERR)
+#define F_PLERR    V_PLERR(1U)
+
+#define S_PERRVFID    0
+#define V_PERRVFID(x) ((x) << S_PERRVFID)
+#define F_PERRVFID    V_PERRVFID(1U)
+
+#define A_PL_PL_INT_ENABLE 0x19434
+#define A_PL_PL_PERR_ENABLE 0x19438
+#define A_PL_REV 0x1943c
+
+#define S_REV    0
+#define M_REV    0xfU
+#define V_REV(x) ((x) << S_REV)
+#define G_REV(x) (((x) >> S_REV) & M_REV)
+
+#define A_PL_SEMAPHORE_CTL 0x1944c
+
+#define S_LOCKSTATUS    16
+#define M_LOCKSTATUS    0xffU
+#define V_LOCKSTATUS(x) ((x) << S_LOCKSTATUS)
+#define G_LOCKSTATUS(x) (((x) >> S_LOCKSTATUS) & M_LOCKSTATUS)
+
+#define S_OWNEROVERRIDE    8
+#define V_OWNEROVERRIDE(x) ((x) << S_OWNEROVERRIDE)
+#define F_OWNEROVERRIDE    V_OWNEROVERRIDE(1U)
+
+#define S_ENABLEPF    0
+#define M_ENABLEPF    0xffU
+#define V_ENABLEPF(x) ((x) << S_ENABLEPF)
+#define G_ENABLEPF(x) (((x) >> S_ENABLEPF) & M_ENABLEPF)
+
+#define A_PL_SEMAPHORE_LOCK 0x19450
+
+#define S_SEMLOCK    31
+#define V_SEMLOCK(x) ((x) << S_SEMLOCK)
+#define F_SEMLOCK    V_SEMLOCK(1U)
+
+#define S_SEMSRCBUS    3
+#define M_SEMSRCBUS    0x3U
+#define V_SEMSRCBUS(x) ((x) << S_SEMSRCBUS)
+#define G_SEMSRCBUS(x) (((x) >> S_SEMSRCBUS) & M_SEMSRCBUS)
+
+#define S_SEMSRCPF    0
+#define M_SEMSRCPF    0x7U
+#define V_SEMSRCPF(x) ((x) << S_SEMSRCPF)
+#define G_SEMSRCPF(x) (((x) >> S_SEMSRCPF) & M_SEMSRCPF)
+
+#define A_PL_PF_ENABLE 0x19470
+
+#define S_PF_ENABLE    0
+#define M_PF_ENABLE    0xffU
+#define V_PF_ENABLE(x) ((x) << S_PF_ENABLE)
+#define G_PF_ENABLE(x) (((x) >> S_PF_ENABLE) & M_PF_ENABLE)
+
+#define A_PL_PORTX_MAP 0x19474
+
+#define S_MAP7    28
+#define M_MAP7    0x7U
+#define V_MAP7(x) ((x) << S_MAP7)
+#define G_MAP7(x) (((x) >> S_MAP7) & M_MAP7)
+
+#define S_MAP6    24
+#define M_MAP6    0x7U
+#define V_MAP6(x) ((x) << S_MAP6)
+#define G_MAP6(x) (((x) >> S_MAP6) & M_MAP6)
+
+#define S_MAP5    20
+#define M_MAP5    0x7U
+#define V_MAP5(x) ((x) << S_MAP5)
+#define G_MAP5(x) (((x) >> S_MAP5) & M_MAP5)
+
+#define S_MAP4    16
+#define M_MAP4    0x7U
+#define V_MAP4(x) ((x) << S_MAP4)
+#define G_MAP4(x) (((x) >> S_MAP4) & M_MAP4)
+
+#define S_MAP3    12
+#define M_MAP3    0x7U
+#define V_MAP3(x) ((x) << S_MAP3)
+#define G_MAP3(x) (((x) >> S_MAP3) & M_MAP3)
+
+#define S_MAP2    8
+#define M_MAP2    0x7U
+#define V_MAP2(x) ((x) << S_MAP2)
+#define G_MAP2(x) (((x) >> S_MAP2) & M_MAP2)
+
+#define S_MAP1    4
+#define M_MAP1    0x7U
+#define V_MAP1(x) ((x) << S_MAP1)
+#define G_MAP1(x) (((x) >> S_MAP1) & M_MAP1)
+
+#define S_MAP0    0
+#define M_MAP0    0x7U
+#define V_MAP0(x) ((x) << S_MAP0)
+#define G_MAP0(x) (((x) >> S_MAP0) & M_MAP0)
+
+#define A_PL_VF_SLICE_L 0x19490
+
+#define S_LIMITADDR    16
+#define M_LIMITADDR    0x3ffU
+#define V_LIMITADDR(x) ((x) << S_LIMITADDR)
+#define G_LIMITADDR(x) (((x) >> S_LIMITADDR) & M_LIMITADDR)
+
+#define S_SLICEBASEADDR    0
+#define M_SLICEBASEADDR    0x3ffU
+#define V_SLICEBASEADDR(x) ((x) << S_SLICEBASEADDR)
+#define G_SLICEBASEADDR(x) (((x) >> S_SLICEBASEADDR) & M_SLICEBASEADDR)
+
+#define A_PL_VF_SLICE_H 0x19494
+
+#define S_MODINDX    16
+#define M_MODINDX    0x7U
+#define V_MODINDX(x) ((x) << S_MODINDX)
+#define G_MODINDX(x) (((x) >> S_MODINDX) & M_MODINDX)
+
+#define S_MODOFFSET    0
+#define M_MODOFFSET    0x3ffU
+#define V_MODOFFSET(x) ((x) << S_MODOFFSET)
+#define G_MODOFFSET(x) (((x) >> S_MODOFFSET) & M_MODOFFSET)
+
+#define A_PL_FLR_VF_STATUS 0x194d0
+#define A_PL_FLR_PF_STATUS 0x194e0
+
+#define S_FLR_PF    0
+#define M_FLR_PF    0xffU
+#define V_FLR_PF(x) ((x) << S_FLR_PF)
+#define G_FLR_PF(x) (((x) >> S_FLR_PF) & M_FLR_PF)
+
+#define A_PL_TIMEOUT_CTL 0x194f0
+
+#define S_PL_TIMEOUT    0
+#define M_PL_TIMEOUT    0xffffU
+#define V_PL_TIMEOUT(x) ((x) << S_PL_TIMEOUT)
+#define G_PL_TIMEOUT(x) (((x) >> S_PL_TIMEOUT) & M_PL_TIMEOUT)
+
+#define A_PL_TIMEOUT_STATUS0 0x194f4
+
+#define S_PL_TOADDR    2
+#define M_PL_TOADDR    0xfffffffU
+#define V_PL_TOADDR(x) ((x) << S_PL_TOADDR)
+#define G_PL_TOADDR(x) (((x) >> S_PL_TOADDR) & M_PL_TOADDR)
+
+#define A_PL_TIMEOUT_STATUS1 0x194f8
+
+#define S_PL_TOVALID    31
+#define V_PL_TOVALID(x) ((x) << S_PL_TOVALID)
+#define F_PL_TOVALID    V_PL_TOVALID(1U)
+
+#define S_WRITE    22
+#define V_WRITE(x) ((x) << S_WRITE)
+#define F_WRITE    V_WRITE(1U)
+
+#define S_PL_TOBUS    20
+#define M_PL_TOBUS    0x3U
+#define V_PL_TOBUS(x) ((x) << S_PL_TOBUS)
+#define G_PL_TOBUS(x) (((x) >> S_PL_TOBUS) & M_PL_TOBUS)
+
+#define S_RGN    19
+#define V_RGN(x) ((x) << S_RGN)
+#define F_RGN    V_RGN(1U)
+
+#define S_PL_TOPF    16
+#define M_PL_TOPF    0x7U
+#define V_PL_TOPF(x) ((x) << S_PL_TOPF)
+#define G_PL_TOPF(x) (((x) >> S_PL_TOPF) & M_PL_TOPF)
+
+#define S_PL_TORID    0
+#define M_PL_TORID    0xffffU
+#define V_PL_TORID(x) ((x) << S_PL_TORID)
+#define G_PL_TORID(x) (((x) >> S_PL_TORID) & M_PL_TORID)
+
+#define A_PL_VFID_MAP 0x19800
+
+#define S_VFID_VLD    7
+#define V_VFID_VLD(x) ((x) << S_VFID_VLD)
+#define F_VFID_VLD    V_VFID_VLD(1U)
+
+/* registers for module LE */
+#define LE_BASE_ADDR 0x19c00
+
+#define A_LE_BUF_CONFIG 0x19c00
+#define A_LE_DB_CONFIG 0x19c04
+
+#define S_TCAMCMDOVLAPEN    21
+#define V_TCAMCMDOVLAPEN(x) ((x) << S_TCAMCMDOVLAPEN)
+#define F_TCAMCMDOVLAPEN    V_TCAMCMDOVLAPEN(1U)
+
+#define S_HASHEN    20
+#define V_HASHEN(x) ((x) << S_HASHEN)
+#define F_HASHEN    V_HASHEN(1U)
+
+#define S_ASBOTHSRCHEN    18
+#define V_ASBOTHSRCHEN(x) ((x) << S_ASBOTHSRCHEN)
+#define F_ASBOTHSRCHEN    V_ASBOTHSRCHEN(1U)
+
+#define S_ASLIPCOMPEN    17
+#define V_ASLIPCOMPEN(x) ((x) << S_ASLIPCOMPEN)
+#define F_ASLIPCOMPEN    V_ASLIPCOMPEN(1U)
+
+#define S_BUILD    16
+#define V_BUILD(x) ((x) << S_BUILD)
+#define F_BUILD    V_BUILD(1U)
+
+#define S_FILTEREN    11
+#define V_FILTEREN(x) ((x) << S_FILTEREN)
+#define F_FILTEREN    V_FILTEREN(1U)
+
+#define S_SYNMODE    7
+#define M_SYNMODE    0x3U
+#define V_SYNMODE(x) ((x) << S_SYNMODE)
+#define G_SYNMODE(x) (((x) >> S_SYNMODE) & M_SYNMODE)
+
+#define S_LEBUSEN    5
+#define V_LEBUSEN(x) ((x) << S_LEBUSEN)
+#define F_LEBUSEN    V_LEBUSEN(1U)
+
+#define S_ELOOKDUMEN    4
+#define V_ELOOKDUMEN(x) ((x) << S_ELOOKDUMEN)
+#define F_ELOOKDUMEN    V_ELOOKDUMEN(1U)
+
+#define S_IPV4ONLYEN    3
+#define V_IPV4ONLYEN(x) ((x) << S_IPV4ONLYEN)
+#define F_IPV4ONLYEN    V_IPV4ONLYEN(1U)
+
+#define S_MOSTCMDOEN    2
+#define V_MOSTCMDOEN(x) ((x) << S_MOSTCMDOEN)
+#define F_MOSTCMDOEN    V_MOSTCMDOEN(1U)
+
+#define S_DELACTSYNOEN    1
+#define V_DELACTSYNOEN(x) ((x) << S_DELACTSYNOEN)
+#define F_DELACTSYNOEN    V_DELACTSYNOEN(1U)
+
+#define S_CMDOVERLAPDIS    0
+#define V_CMDOVERLAPDIS(x) ((x) << S_CMDOVERLAPDIS)
+#define F_CMDOVERLAPDIS    V_CMDOVERLAPDIS(1U)
+
+#define A_LE_MISC 0x19c08
+
+#define S_CMPUNVAIL    0
+#define M_CMPUNVAIL    0xfU
+#define V_CMPUNVAIL(x) ((x) << S_CMPUNVAIL)
+#define G_CMPUNVAIL(x) (((x) >> S_CMPUNVAIL) & M_CMPUNVAIL)
+
+#define A_LE_DB_ROUTING_TABLE_INDEX 0x19c10
+
+#define S_RTINDX    7
+#define M_RTINDX    0x3fU
+#define V_RTINDX(x) ((x) << S_RTINDX)
+#define G_RTINDX(x) (((x) >> S_RTINDX) & M_RTINDX)
+
+#define A_LE_DB_FILTER_TABLE_INDEX 0x19c14
+
+#define S_FTINDX    7
+#define M_FTINDX    0x3fU
+#define V_FTINDX(x) ((x) << S_FTINDX)
+#define G_FTINDX(x) (((x) >> S_FTINDX) & M_FTINDX)
+
+#define A_LE_DB_SERVER_INDEX 0x19c18
+
+#define S_SRINDX    7
+#define M_SRINDX    0x3fU
+#define V_SRINDX(x) ((x) << S_SRINDX)
+#define G_SRINDX(x) (((x) >> S_SRINDX) & M_SRINDX)
+
+#define A_LE_DB_CLIP_TABLE_INDEX 0x19c1c
+
+#define S_CLIPTINDX    7
+#define M_CLIPTINDX    0x3fU
+#define V_CLIPTINDX(x) ((x) << S_CLIPTINDX)
+#define G_CLIPTINDX(x) (((x) >> S_CLIPTINDX) & M_CLIPTINDX)
+
+#define A_LE_DB_ACT_CNT_IPV4 0x19c20
+
+#define S_ACTCNTIPV4    0
+#define M_ACTCNTIPV4    0xfffffU
+#define V_ACTCNTIPV4(x) ((x) << S_ACTCNTIPV4)
+#define G_ACTCNTIPV4(x) (((x) >> S_ACTCNTIPV4) & M_ACTCNTIPV4)
+
+#define A_LE_DB_ACT_CNT_IPV6 0x19c24
+
+#define S_ACTCNTIPV6    0
+#define M_ACTCNTIPV6    0xfffffU
+#define V_ACTCNTIPV6(x) ((x) << S_ACTCNTIPV6)
+#define G_ACTCNTIPV6(x) (((x) >> S_ACTCNTIPV6) & M_ACTCNTIPV6)
+
+#define A_LE_DB_HASH_CONFIG 0x19c28
+
+#define S_HASHTIDSIZE    16
+#define M_HASHTIDSIZE    0x3fU
+#define V_HASHTIDSIZE(x) ((x) << S_HASHTIDSIZE)
+#define G_HASHTIDSIZE(x) (((x) >> S_HASHTIDSIZE) & M_HASHTIDSIZE)
+
+#define S_HASHSIZE    0
+#define M_HASHSIZE    0x3fU
+#define V_HASHSIZE(x) ((x) << S_HASHSIZE)
+#define G_HASHSIZE(x) (((x) >> S_HASHSIZE) & M_HASHSIZE)
+
+#define A_LE_DB_HASH_TABLE_BASE 0x19c2c
+#define A_LE_DB_HASH_TID_BASE 0x19c30
+#define A_LE_DB_SIZE 0x19c34
+#define A_LE_DB_INT_ENABLE 0x19c38
+
+#define S_MSGSEL    27
+#define M_MSGSEL    0x1fU
+#define V_MSGSEL(x) ((x) << S_MSGSEL)
+#define G_MSGSEL(x) (((x) >> S_MSGSEL) & M_MSGSEL)
+
+#define S_REQQPARERR    16
+#define V_REQQPARERR(x) ((x) << S_REQQPARERR)
+#define F_REQQPARERR    V_REQQPARERR(1U)
+
+#define S_UNKNOWNCMD    15
+#define V_UNKNOWNCMD(x) ((x) << S_UNKNOWNCMD)
+#define F_UNKNOWNCMD    V_UNKNOWNCMD(1U)
+
+#define S_DROPFILTERHIT    13
+#define V_DROPFILTERHIT(x) ((x) << S_DROPFILTERHIT)
+#define F_DROPFILTERHIT    V_DROPFILTERHIT(1U)
+
+#define S_FILTERHIT    12
+#define V_FILTERHIT(x) ((x) << S_FILTERHIT)
+#define F_FILTERHIT    V_FILTERHIT(1U)
+
+#define S_SYNCOOKIEOFF    11
+#define V_SYNCOOKIEOFF(x) ((x) << S_SYNCOOKIEOFF)
+#define F_SYNCOOKIEOFF    V_SYNCOOKIEOFF(1U)
+
+#define S_SYNCOOKIEBAD    10
+#define V_SYNCOOKIEBAD(x) ((x) << S_SYNCOOKIEBAD)
+#define F_SYNCOOKIEBAD    V_SYNCOOKIEBAD(1U)
+
+#define S_SYNCOOKIE    9
+#define V_SYNCOOKIE(x) ((x) << S_SYNCOOKIE)
+#define F_SYNCOOKIE    V_SYNCOOKIE(1U)
+
+#define S_NFASRCHFAIL    8
+#define V_NFASRCHFAIL(x) ((x) << S_NFASRCHFAIL)
+#define F_NFASRCHFAIL    V_NFASRCHFAIL(1U)
+
+#define S_ACTRGNFULL    7
+#define V_ACTRGNFULL(x) ((x) << S_ACTRGNFULL)
+#define F_ACTRGNFULL    V_ACTRGNFULL(1U)
+
+#define S_PARITYERR    6
+#define V_PARITYERR(x) ((x) << S_PARITYERR)
+#define F_PARITYERR    V_PARITYERR(1U)
+
+#define S_LIPMISS    5
+#define V_LIPMISS(x) ((x) << S_LIPMISS)
+#define F_LIPMISS    V_LIPMISS(1U)
+
+#define S_LIP0    4
+#define V_LIP0(x) ((x) << S_LIP0)
+#define F_LIP0    V_LIP0(1U)
+
+#define S_MISS    3
+#define V_MISS(x) ((x) << S_MISS)
+#define F_MISS    V_MISS(1U)
+
+#define S_ROUTINGHIT    2
+#define V_ROUTINGHIT(x) ((x) << S_ROUTINGHIT)
+#define F_ROUTINGHIT    V_ROUTINGHIT(1U)
+
+#define S_ACTIVEHIT    1
+#define V_ACTIVEHIT(x) ((x) << S_ACTIVEHIT)
+#define F_ACTIVEHIT    V_ACTIVEHIT(1U)
+
+#define S_SERVERHIT    0
+#define V_SERVERHIT(x) ((x) << S_SERVERHIT)
+#define F_SERVERHIT    V_SERVERHIT(1U)
+
+#define A_LE_DB_INT_CAUSE 0x19c3c
+#define A_LE_DB_INT_TID 0x19c40
+
+#define S_INTTID    0
+#define M_INTTID    0xfffffU
+#define V_INTTID(x) ((x) << S_INTTID)
+#define G_INTTID(x) (((x) >> S_INTTID) & M_INTTID)
+
+#define A_LE_DB_INT_PTID 0x19c44
+
+#define S_INTPTID    0
+#define M_INTPTID    0xfffffU
+#define V_INTPTID(x) ((x) << S_INTPTID)
+#define G_INTPTID(x) (((x) >> S_INTPTID) & M_INTPTID)
+
+#define A_LE_DB_INT_INDEX 0x19c48
+
+#define S_INTINDEX    0
+#define M_INTINDEX    0xfffffU
+#define V_INTINDEX(x) ((x) << S_INTINDEX)
+#define G_INTINDEX(x) (((x) >> S_INTINDEX) & M_INTINDEX)
+
+#define A_LE_DB_INT_CMD 0x19c4c
+
+#define S_INTCMD    0
+#define M_INTCMD    0xfU
+#define V_INTCMD(x) ((x) << S_INTCMD)
+#define G_INTCMD(x) (((x) >> S_INTCMD) & M_INTCMD)
+
+#define A_LE_DB_MASK_IPV4 0x19c50
+#define A_LE_DB_MASK_IPV6 0x19ca0
+#define A_LE_DB_REQ_RSP_CNT 0x19ce4
+#define A_LE_DB_DBGI_CONFIG 0x19cf0
+
+#define S_DBGICMDPERR    31
+#define V_DBGICMDPERR(x) ((x) << S_DBGICMDPERR)
+#define F_DBGICMDPERR    V_DBGICMDPERR(1U)
+
+#define S_DBGICMDRANGE    22
+#define M_DBGICMDRANGE    0x7U
+#define V_DBGICMDRANGE(x) ((x) << S_DBGICMDRANGE)
+#define G_DBGICMDRANGE(x) (((x) >> S_DBGICMDRANGE) & M_DBGICMDRANGE)
+
+#define S_DBGICMDMSKTYPE    21
+#define V_DBGICMDMSKTYPE(x) ((x) << S_DBGICMDMSKTYPE)
+#define F_DBGICMDMSKTYPE    V_DBGICMDMSKTYPE(1U)
+
+#define S_DBGICMDSEARCH    20
+#define V_DBGICMDSEARCH(x) ((x) << S_DBGICMDSEARCH)
+#define F_DBGICMDSEARCH    V_DBGICMDSEARCH(1U)
+
+#define S_DBGICMDREAD    19
+#define V_DBGICMDREAD(x) ((x) << S_DBGICMDREAD)
+#define F_DBGICMDREAD    V_DBGICMDREAD(1U)
+
+#define S_DBGICMDLEARN    18
+#define V_DBGICMDLEARN(x) ((x) << S_DBGICMDLEARN)
+#define F_DBGICMDLEARN    V_DBGICMDLEARN(1U)
+
+#define S_DBGICMDERASE    17
+#define V_DBGICMDERASE(x) ((x) << S_DBGICMDERASE)
+#define F_DBGICMDERASE    V_DBGICMDERASE(1U)
+
+#define S_DBGICMDIPV6    16
+#define V_DBGICMDIPV6(x) ((x) << S_DBGICMDIPV6)
+#define F_DBGICMDIPV6    V_DBGICMDIPV6(1U)
+
+#define S_DBGICMDTYPE    13
+#define M_DBGICMDTYPE    0x7U
+#define V_DBGICMDTYPE(x) ((x) << S_DBGICMDTYPE)
+#define G_DBGICMDTYPE(x) (((x) >> S_DBGICMDTYPE) & M_DBGICMDTYPE)
+
+#define S_DBGICMDACKERR    12
+#define V_DBGICMDACKERR(x) ((x) << S_DBGICMDACKERR)
+#define F_DBGICMDACKERR    V_DBGICMDACKERR(1U)
+
+#define S_DBGICMDBUSY    3
+#define V_DBGICMDBUSY(x) ((x) << S_DBGICMDBUSY)
+#define F_DBGICMDBUSY    V_DBGICMDBUSY(1U)
+
+#define S_DBGICMDSTRT    2
+#define V_DBGICMDSTRT(x) ((x) << S_DBGICMDSTRT)
+#define F_DBGICMDSTRT    V_DBGICMDSTRT(1U)
+
+#define S_DBGICMDMODE    0
+#define M_DBGICMDMODE    0x3U
+#define V_DBGICMDMODE(x) ((x) << S_DBGICMDMODE)
+#define G_DBGICMDMODE(x) (((x) >> S_DBGICMDMODE) & M_DBGICMDMODE)
+
+#define A_LE_DB_DBGI_REQ_TCAM_CMD 0x19cf4
+
+#define S_DBGICMD    20
+#define M_DBGICMD    0xfU
+#define V_DBGICMD(x) ((x) << S_DBGICMD)
+#define G_DBGICMD(x) (((x) >> S_DBGICMD) & M_DBGICMD)
+
+#define S_DBGITINDEX    0
+#define M_DBGITINDEX    0xfffffU
+#define V_DBGITINDEX(x) ((x) << S_DBGITINDEX)
+#define G_DBGITINDEX(x) (((x) >> S_DBGITINDEX) & M_DBGITINDEX)
+
+#define A_LE_PERR_ENABLE 0x19cf8
+
+#define S_REQQUEUE    1
+#define V_REQQUEUE(x) ((x) << S_REQQUEUE)
+#define F_REQQUEUE    V_REQQUEUE(1U)
+
+#define S_TCAM    0
+#define V_TCAM(x) ((x) << S_TCAM)
+#define F_TCAM    V_TCAM(1U)
+
+#define A_LE_SPARE 0x19cfc
+#define A_LE_DB_DBGI_REQ_DATA 0x19d00
+#define A_LE_DB_DBGI_REQ_MASK 0x19d50
+#define A_LE_DB_DBGI_RSP_STATUS 0x19d94
+
+#define S_DBGIRSPINDEX    12
+#define M_DBGIRSPINDEX    0xfffffU
+#define V_DBGIRSPINDEX(x) ((x) << S_DBGIRSPINDEX)
+#define G_DBGIRSPINDEX(x) (((x) >> S_DBGIRSPINDEX) & M_DBGIRSPINDEX)
+
+#define S_DBGIRSPMSG    8
+#define M_DBGIRSPMSG    0xfU
+#define V_DBGIRSPMSG(x) ((x) << S_DBGIRSPMSG)
+#define G_DBGIRSPMSG(x) (((x) >> S_DBGIRSPMSG) & M_DBGIRSPMSG)
+
+#define S_DBGIRSPMSGVLD    7
+#define V_DBGIRSPMSGVLD(x) ((x) << S_DBGIRSPMSGVLD)
+#define F_DBGIRSPMSGVLD    V_DBGIRSPMSGVLD(1U)
+
+#define S_DBGIRSPMHIT    2
+#define V_DBGIRSPMHIT(x) ((x) << S_DBGIRSPMHIT)
+#define F_DBGIRSPMHIT    V_DBGIRSPMHIT(1U)
+
+#define S_DBGIRSPHIT    1
+#define V_DBGIRSPHIT(x) ((x) << S_DBGIRSPHIT)
+#define F_DBGIRSPHIT    V_DBGIRSPHIT(1U)
+
+#define S_DBGIRSPVALID    0
+#define V_DBGIRSPVALID(x) ((x) << S_DBGIRSPVALID)
+#define F_DBGIRSPVALID    V_DBGIRSPVALID(1U)
+
+#define A_LE_DB_DBGI_RSP_DATA 0x19da0
+#define A_LE_DB_DBGI_RSP_LAST_CMD 0x19de4
+
+#define S_LASTCMDB    16
+#define M_LASTCMDB    0x7ffU
+#define V_LASTCMDB(x) ((x) << S_LASTCMDB)
+#define G_LASTCMDB(x) (((x) >> S_LASTCMDB) & M_LASTCMDB)
+
+#define S_LASTCMDA    0
+#define M_LASTCMDA    0x7ffU
+#define V_LASTCMDA(x) ((x) << S_LASTCMDA)
+#define G_LASTCMDA(x) (((x) >> S_LASTCMDA) & M_LASTCMDA)
+
+#define A_LE_DB_DROP_FILTER_ENTRY 0x19de8
+
+#define S_DROPFILTEREN    31
+#define V_DROPFILTEREN(x) ((x) << S_DROPFILTEREN)
+#define F_DROPFILTEREN    V_DROPFILTEREN(1U)
+
+#define S_DROPFILTERCLEAR    17
+#define V_DROPFILTERCLEAR(x) ((x) << S_DROPFILTERCLEAR)
+#define F_DROPFILTERCLEAR    V_DROPFILTERCLEAR(1U)
+
+#define S_DROPFILTERSET    16
+#define V_DROPFILTERSET(x) ((x) << S_DROPFILTERSET)
+#define F_DROPFILTERSET    V_DROPFILTERSET(1U)
+
+#define S_DROPFILTERFIDX    0
+#define M_DROPFILTERFIDX    0x1fffU
+#define V_DROPFILTERFIDX(x) ((x) << S_DROPFILTERFIDX)
+#define G_DROPFILTERFIDX(x) (((x) >> S_DROPFILTERFIDX) & M_DROPFILTERFIDX)
+
+#define A_LE_DB_PTID_SVRBASE 0x19df0
+
+#define S_SVRBASE_ADDR    2
+#define M_SVRBASE_ADDR    0x3ffffU
+#define V_SVRBASE_ADDR(x) ((x) << S_SVRBASE_ADDR)
+#define G_SVRBASE_ADDR(x) (((x) >> S_SVRBASE_ADDR) & M_SVRBASE_ADDR)
+
+#define A_LE_DB_FTID_FLTRBASE 0x19df4
+
+#define S_FLTRBASE_ADDR    2
+#define M_FLTRBASE_ADDR    0x3ffffU
+#define V_FLTRBASE_ADDR(x) ((x) << S_FLTRBASE_ADDR)
+#define G_FLTRBASE_ADDR(x) (((x) >> S_FLTRBASE_ADDR) & M_FLTRBASE_ADDR)
+
+#define A_LE_DB_TID_HASHBASE 0x19df8
+
+#define S_HASHBASE_ADDR    2
+#define M_HASHBASE_ADDR    0xfffffU
+#define V_HASHBASE_ADDR(x) ((x) << S_HASHBASE_ADDR)
+#define G_HASHBASE_ADDR(x) (((x) >> S_HASHBASE_ADDR) & M_HASHBASE_ADDR)
+
+#define A_LE_PERR_INJECT 0x19dfc
+
+#define S_LEMEMSEL    1
+#define M_LEMEMSEL    0x7U
+#define V_LEMEMSEL(x) ((x) << S_LEMEMSEL)
+#define G_LEMEMSEL(x) (((x) >> S_LEMEMSEL) & M_LEMEMSEL)
+
+#define A_LE_DB_ACTIVE_MASK_IPV4 0x19e00
+#define A_LE_DB_ACTIVE_MASK_IPV6 0x19e50
+#define A_LE_HASH_MASK_GEN_IPV4 0x19ea0
+#define A_LE_HASH_MASK_GEN_IPV6 0x19eb0
+#define A_LE_HASH_MASK_CMP_IPV4 0x19ee0
+#define A_LE_HASH_MASK_CMP_IPV6 0x19ef0
+#define A_LE_DEBUG_LA_CONFIG 0x19f20
+#define A_LE_REQ_DEBUG_LA_DATA 0x19f24
+#define A_LE_REQ_DEBUG_LA_WRPTR 0x19f28
+#define A_LE_RSP_DEBUG_LA_DATA 0x19f2c
+#define A_LE_RSP_DEBUG_LA_WRPTR 0x19f30
+
+/* registers for module NCSI */
+#define NCSI_BASE_ADDR 0x1a000
+
+#define A_NCSI_PORT_CFGREG 0x1a000
+
+#define S_WIREEN    28
+#define M_WIREEN    0xfU
+#define V_WIREEN(x) ((x) << S_WIREEN)
+#define G_WIREEN(x) (((x) >> S_WIREEN) & M_WIREEN)
+
+#define S_STRP_CRC    24
+#define M_STRP_CRC    0xfU
+#define V_STRP_CRC(x) ((x) << S_STRP_CRC)
+#define G_STRP_CRC(x) (((x) >> S_STRP_CRC) & M_STRP_CRC)
+
+#define S_RX_HALT    22
+#define V_RX_HALT(x) ((x) << S_RX_HALT)
+#define F_RX_HALT    V_RX_HALT(1U)
+
+#define S_FLUSH_RX_FIFO    21
+#define V_FLUSH_RX_FIFO(x) ((x) << S_FLUSH_RX_FIFO)
+#define F_FLUSH_RX_FIFO    V_FLUSH_RX_FIFO(1U)
+
+#define S_HW_ARB_EN    20
+#define V_HW_ARB_EN(x) ((x) << S_HW_ARB_EN)
+#define F_HW_ARB_EN    V_HW_ARB_EN(1U)
+
+#define S_SOFT_PKG_SEL    19
+#define V_SOFT_PKG_SEL(x) ((x) << S_SOFT_PKG_SEL)
+#define F_SOFT_PKG_SEL    V_SOFT_PKG_SEL(1U)
+
+#define S_ERR_DISCARD_EN    18
+#define V_ERR_DISCARD_EN(x) ((x) << S_ERR_DISCARD_EN)
+#define F_ERR_DISCARD_EN    V_ERR_DISCARD_EN(1U)
+
+#define S_MAX_PKT_SIZE    4
+#define M_MAX_PKT_SIZE    0x3fffU
+#define V_MAX_PKT_SIZE(x) ((x) << S_MAX_PKT_SIZE)
+#define G_MAX_PKT_SIZE(x) (((x) >> S_MAX_PKT_SIZE) & M_MAX_PKT_SIZE)
+
+#define S_RX_BYTE_SWAP    3
+#define V_RX_BYTE_SWAP(x) ((x) << S_RX_BYTE_SWAP)
+#define F_RX_BYTE_SWAP    V_RX_BYTE_SWAP(1U)
+
+#define S_TX_BYTE_SWAP    2
+#define V_TX_BYTE_SWAP(x) ((x) << S_TX_BYTE_SWAP)
+#define F_TX_BYTE_SWAP    V_TX_BYTE_SWAP(1U)
+
+#define A_NCSI_RST_CTRL 0x1a004
+
+#define S_MAC_REF_RST    2
+#define V_MAC_REF_RST(x) ((x) << S_MAC_REF_RST)
+#define F_MAC_REF_RST    V_MAC_REF_RST(1U)
+
+#define S_MAC_RX_RST    1
+#define V_MAC_RX_RST(x) ((x) << S_MAC_RX_RST)
+#define F_MAC_RX_RST    V_MAC_RX_RST(1U)
+
+#define S_MAC_TX_RST    0
+#define V_MAC_TX_RST(x) ((x) << S_MAC_TX_RST)
+#define F_MAC_TX_RST    V_MAC_TX_RST(1U)
+
+#define A_NCSI_CH0_SADDR_LOW 0x1a010
+#define A_NCSI_CH0_SADDR_HIGH 0x1a014
+
+#define S_CHO_SADDR_EN    31
+#define V_CHO_SADDR_EN(x) ((x) << S_CHO_SADDR_EN)
+#define F_CHO_SADDR_EN    V_CHO_SADDR_EN(1U)
+
+#define S_CH0_SADDR_HIGH    0
+#define M_CH0_SADDR_HIGH    0xffffU
+#define V_CH0_SADDR_HIGH(x) ((x) << S_CH0_SADDR_HIGH)
+#define G_CH0_SADDR_HIGH(x) (((x) >> S_CH0_SADDR_HIGH) & M_CH0_SADDR_HIGH)
+
+#define A_NCSI_CH1_SADDR_LOW 0x1a018
+#define A_NCSI_CH1_SADDR_HIGH 0x1a01c
+
+#define S_CH1_SADDR_EN    31
+#define V_CH1_SADDR_EN(x) ((x) << S_CH1_SADDR_EN)
+#define F_CH1_SADDR_EN    V_CH1_SADDR_EN(1U)
+
+#define S_CH1_SADDR_HIGH    0
+#define M_CH1_SADDR_HIGH    0xffffU
+#define V_CH1_SADDR_HIGH(x) ((x) << S_CH1_SADDR_HIGH)
+#define G_CH1_SADDR_HIGH(x) (((x) >> S_CH1_SADDR_HIGH) & M_CH1_SADDR_HIGH)
+
+#define A_NCSI_CH2_SADDR_LOW 0x1a020
+#define A_NCSI_CH2_SADDR_HIGH 0x1a024
+
+#define S_CH2_SADDR_EN    31
+#define V_CH2_SADDR_EN(x) ((x) << S_CH2_SADDR_EN)
+#define F_CH2_SADDR_EN    V_CH2_SADDR_EN(1U)
+
+#define S_CH2_SADDR_HIGH    0
+#define M_CH2_SADDR_HIGH    0xffffU
+#define V_CH2_SADDR_HIGH(x) ((x) << S_CH2_SADDR_HIGH)
+#define G_CH2_SADDR_HIGH(x) (((x) >> S_CH2_SADDR_HIGH) & M_CH2_SADDR_HIGH)
+
+#define A_NCSI_CH3_SADDR_LOW 0x1a028
+#define A_NCSI_CH3_SADDR_HIGH 0x1a02c
+
+#define S_CH3_SADDR_EN    31
+#define V_CH3_SADDR_EN(x) ((x) << S_CH3_SADDR_EN)
+#define F_CH3_SADDR_EN    V_CH3_SADDR_EN(1U)
+
+#define S_CH3_SADDR_HIGH    0
+#define M_CH3_SADDR_HIGH    0xffffU
+#define V_CH3_SADDR_HIGH(x) ((x) << S_CH3_SADDR_HIGH)
+#define G_CH3_SADDR_HIGH(x) (((x) >> S_CH3_SADDR_HIGH) & M_CH3_SADDR_HIGH)
+
+#define A_NCSI_WORK_REQHDR_0 0x1a030
+#define A_NCSI_WORK_REQHDR_1 0x1a034
+#define A_NCSI_WORK_REQHDR_2 0x1a038
+#define A_NCSI_WORK_REQHDR_3 0x1a03c
+#define A_NCSI_MPS_HDR_LO 0x1a040
+#define A_NCSI_MPS_HDR_HI 0x1a044
+#define A_NCSI_CTL 0x1a048
+
+#define S_STRIP_OVLAN    3
+#define V_STRIP_OVLAN(x) ((x) << S_STRIP_OVLAN)
+#define F_STRIP_OVLAN    V_STRIP_OVLAN(1U)
+
+#define S_BMC_DROP_NON_BC    2
+#define V_BMC_DROP_NON_BC(x) ((x) << S_BMC_DROP_NON_BC)
+#define F_BMC_DROP_NON_BC    V_BMC_DROP_NON_BC(1U)
+
+#define S_BMC_RX_FWD_ALL    1
+#define V_BMC_RX_FWD_ALL(x) ((x) << S_BMC_RX_FWD_ALL)
+#define F_BMC_RX_FWD_ALL    V_BMC_RX_FWD_ALL(1U)
+
+#define S_FWD_BMC    0
+#define V_FWD_BMC(x) ((x) << S_FWD_BMC)
+#define F_FWD_BMC    V_FWD_BMC(1U)
+
+#define A_NCSI_NCSI_ETYPE 0x1a04c
+
+#define S_NCSI_ETHERTYPE    0
+#define M_NCSI_ETHERTYPE    0xffffU
+#define V_NCSI_ETHERTYPE(x) ((x) << S_NCSI_ETHERTYPE)
+#define G_NCSI_ETHERTYPE(x) (((x) >> S_NCSI_ETHERTYPE) & M_NCSI_ETHERTYPE)
+
+#define A_NCSI_RX_FIFO_CNT 0x1a050
+
+#define S_NCSI_RXFIFO_CNT    0
+#define M_NCSI_RXFIFO_CNT    0x7ffU
+#define V_NCSI_RXFIFO_CNT(x) ((x) << S_NCSI_RXFIFO_CNT)
+#define G_NCSI_RXFIFO_CNT(x) (((x) >> S_NCSI_RXFIFO_CNT) & M_NCSI_RXFIFO_CNT)
+
+#define A_NCSI_RX_ERR_CNT 0x1a054
+#define A_NCSI_RX_OF_CNT 0x1a058
+#define A_NCSI_RX_MS_CNT 0x1a05c
+#define A_NCSI_RX_IE_CNT 0x1a060
+#define A_NCSI_MPS_DEMUX_CNT 0x1a064
+
+#define S_MPS2CIM_CNT    16
+#define M_MPS2CIM_CNT    0x1ffU
+#define V_MPS2CIM_CNT(x) ((x) << S_MPS2CIM_CNT)
+#define G_MPS2CIM_CNT(x) (((x) >> S_MPS2CIM_CNT) & M_MPS2CIM_CNT)
+
+#define S_MPS2BMC_CNT    0
+#define M_MPS2BMC_CNT    0x1ffU
+#define V_MPS2BMC_CNT(x) ((x) << S_MPS2BMC_CNT)
+#define G_MPS2BMC_CNT(x) (((x) >> S_MPS2BMC_CNT) & M_MPS2BMC_CNT)
+
+#define A_NCSI_CIM_DEMUX_CNT 0x1a068
+
+#define S_CIM2MPS_CNT    16
+#define M_CIM2MPS_CNT    0x1ffU
+#define V_CIM2MPS_CNT(x) ((x) << S_CIM2MPS_CNT)
+#define G_CIM2MPS_CNT(x) (((x) >> S_CIM2MPS_CNT) & M_CIM2MPS_CNT)
+
+#define S_CIM2BMC_CNT    0
+#define M_CIM2BMC_CNT    0x1ffU
+#define V_CIM2BMC_CNT(x) ((x) << S_CIM2BMC_CNT)
+#define G_CIM2BMC_CNT(x) (((x) >> S_CIM2BMC_CNT) & M_CIM2BMC_CNT)
+
+#define A_NCSI_TX_FIFO_CNT 0x1a06c
+
+#define S_TX_FIFO_CNT    0
+#define M_TX_FIFO_CNT    0x3ffU
+#define V_TX_FIFO_CNT(x) ((x) << S_TX_FIFO_CNT)
+#define G_TX_FIFO_CNT(x) (((x) >> S_TX_FIFO_CNT) & M_TX_FIFO_CNT)
+
+#define A_NCSI_SE_CNT_CTL 0x1a0b0
+
+#define S_SE_CNT_CLR    0
+#define M_SE_CNT_CLR    0xfU
+#define V_SE_CNT_CLR(x) ((x) << S_SE_CNT_CLR)
+#define G_SE_CNT_CLR(x) (((x) >> S_SE_CNT_CLR) & M_SE_CNT_CLR)
+
+#define A_NCSI_SE_CNT_MPS 0x1a0b4
+
+#define S_NC2MPS_SOP_CNT    24
+#define M_NC2MPS_SOP_CNT    0xffU
+#define V_NC2MPS_SOP_CNT(x) ((x) << S_NC2MPS_SOP_CNT)
+#define G_NC2MPS_SOP_CNT(x) (((x) >> S_NC2MPS_SOP_CNT) & M_NC2MPS_SOP_CNT)
+
+#define S_NC2MPS_EOP_CNT    16
+#define M_NC2MPS_EOP_CNT    0x3fU
+#define V_NC2MPS_EOP_CNT(x) ((x) << S_NC2MPS_EOP_CNT)
+#define G_NC2MPS_EOP_CNT(x) (((x) >> S_NC2MPS_EOP_CNT) & M_NC2MPS_EOP_CNT)
+
+#define S_MPS2NC_SOP_CNT    8
+#define M_MPS2NC_SOP_CNT    0xffU
+#define V_MPS2NC_SOP_CNT(x) ((x) << S_MPS2NC_SOP_CNT)
+#define G_MPS2NC_SOP_CNT(x) (((x) >> S_MPS2NC_SOP_CNT) & M_MPS2NC_SOP_CNT)
+
+#define S_MPS2NC_EOP_CNT    0
+#define M_MPS2NC_EOP_CNT    0xffU
+#define V_MPS2NC_EOP_CNT(x) ((x) << S_MPS2NC_EOP_CNT)
+#define G_MPS2NC_EOP_CNT(x) (((x) >> S_MPS2NC_EOP_CNT) & M_MPS2NC_EOP_CNT)
+
+#define A_NCSI_SE_CNT_CIM 0x1a0b8
+
+#define S_NC2CIM_SOP_CNT    24
+#define M_NC2CIM_SOP_CNT    0xffU
+#define V_NC2CIM_SOP_CNT(x) ((x) << S_NC2CIM_SOP_CNT)
+#define G_NC2CIM_SOP_CNT(x) (((x) >> S_NC2CIM_SOP_CNT) & M_NC2CIM_SOP_CNT)
+
+#define S_NC2CIM_EOP_CNT    16
+#define M_NC2CIM_EOP_CNT    0x3fU
+#define V_NC2CIM_EOP_CNT(x) ((x) << S_NC2CIM_EOP_CNT)
+#define G_NC2CIM_EOP_CNT(x) (((x) >> S_NC2CIM_EOP_CNT) & M_NC2CIM_EOP_CNT)
+
+#define S_CIM2NC_SOP_CNT    8
+#define M_CIM2NC_SOP_CNT    0xffU
+#define V_CIM2NC_SOP_CNT(x) ((x) << S_CIM2NC_SOP_CNT)
+#define G_CIM2NC_SOP_CNT(x) (((x) >> S_CIM2NC_SOP_CNT) & M_CIM2NC_SOP_CNT)
+
+#define S_CIM2NC_EOP_CNT    0
+#define M_CIM2NC_EOP_CNT    0xffU
+#define V_CIM2NC_EOP_CNT(x) ((x) << S_CIM2NC_EOP_CNT)
+#define G_CIM2NC_EOP_CNT(x) (((x) >> S_CIM2NC_EOP_CNT) & M_CIM2NC_EOP_CNT)
+
+#define A_NCSI_BUS_DEBUG 0x1a0bc
+
+#define S_SOP_CNT_ERR    12
+#define M_SOP_CNT_ERR    0xfU
+#define V_SOP_CNT_ERR(x) ((x) << S_SOP_CNT_ERR)
+#define G_SOP_CNT_ERR(x) (((x) >> S_SOP_CNT_ERR) & M_SOP_CNT_ERR)
+
+#define S_BUS_STATE_MPS_OUT    6
+#define M_BUS_STATE_MPS_OUT    0x3U
+#define V_BUS_STATE_MPS_OUT(x) ((x) << S_BUS_STATE_MPS_OUT)
+#define G_BUS_STATE_MPS_OUT(x) (((x) >> S_BUS_STATE_MPS_OUT) & M_BUS_STATE_MPS_OUT)
+
+#define S_BUS_STATE_MPS_IN    4
+#define M_BUS_STATE_MPS_IN    0x3U
+#define V_BUS_STATE_MPS_IN(x) ((x) << S_BUS_STATE_MPS_IN)
+#define G_BUS_STATE_MPS_IN(x) (((x) >> S_BUS_STATE_MPS_IN) & M_BUS_STATE_MPS_IN)
+
+#define S_BUS_STATE_CIM_OUT    2
+#define M_BUS_STATE_CIM_OUT    0x3U
+#define V_BUS_STATE_CIM_OUT(x) ((x) << S_BUS_STATE_CIM_OUT)
+#define G_BUS_STATE_CIM_OUT(x) (((x) >> S_BUS_STATE_CIM_OUT) & M_BUS_STATE_CIM_OUT)
+
+#define S_BUS_STATE_CIM_IN    0
+#define M_BUS_STATE_CIM_IN    0x3U
+#define V_BUS_STATE_CIM_IN(x) ((x) << S_BUS_STATE_CIM_IN)
+#define G_BUS_STATE_CIM_IN(x) (((x) >> S_BUS_STATE_CIM_IN) & M_BUS_STATE_CIM_IN)
+
+#define A_NCSI_LA_RDPTR 0x1a0c0
+#define A_NCSI_LA_RDDATA 0x1a0c4
+#define A_NCSI_LA_WRPTR 0x1a0c8
+#define A_NCSI_LA_RESERVED 0x1a0cc
+#define A_NCSI_LA_CTL 0x1a0d0
+#define A_NCSI_INT_ENABLE 0x1a0d4
+
+#define S_CIM_DM_PRTY_ERR    8
+#define V_CIM_DM_PRTY_ERR(x) ((x) << S_CIM_DM_PRTY_ERR)
+#define F_CIM_DM_PRTY_ERR    V_CIM_DM_PRTY_ERR(1U)
+
+#define S_MPS_DM_PRTY_ERR    7
+#define V_MPS_DM_PRTY_ERR(x) ((x) << S_MPS_DM_PRTY_ERR)
+#define F_MPS_DM_PRTY_ERR    V_MPS_DM_PRTY_ERR(1U)
+
+#define S_TOKEN    6
+#define V_TOKEN(x) ((x) << S_TOKEN)
+#define F_TOKEN    V_TOKEN(1U)
+
+#define S_ARB_DONE    5
+#define V_ARB_DONE(x) ((x) << S_ARB_DONE)
+#define F_ARB_DONE    V_ARB_DONE(1U)
+
+#define S_ARB_STARTED    4
+#define V_ARB_STARTED(x) ((x) << S_ARB_STARTED)
+#define F_ARB_STARTED    V_ARB_STARTED(1U)
+
+#define S_WOL    3
+#define V_WOL(x) ((x) << S_WOL)
+#define F_WOL    V_WOL(1U)
+
+#define S_MACINT    2
+#define V_MACINT(x) ((x) << S_MACINT)
+#define F_MACINT    V_MACINT(1U)
+
+#define S_TXFIFO_PRTY_ERR    1
+#define V_TXFIFO_PRTY_ERR(x) ((x) << S_TXFIFO_PRTY_ERR)
+#define F_TXFIFO_PRTY_ERR    V_TXFIFO_PRTY_ERR(1U)
+
+#define S_RXFIFO_PRTY_ERR    0
+#define V_RXFIFO_PRTY_ERR(x) ((x) << S_RXFIFO_PRTY_ERR)
+#define F_RXFIFO_PRTY_ERR    V_RXFIFO_PRTY_ERR(1U)
+
+#define A_NCSI_INT_CAUSE 0x1a0d8
+#define A_NCSI_STATUS 0x1a0dc
+
+#define S_MASTER    1
+#define V_MASTER(x) ((x) << S_MASTER)
+#define F_MASTER    V_MASTER(1U)
+
+#define S_ARB_STATUS    0
+#define V_ARB_STATUS(x) ((x) << S_ARB_STATUS)
+#define F_ARB_STATUS    V_ARB_STATUS(1U)
+
+#define A_NCSI_PAUSE_CTRL 0x1a0e0
+
+#define S_FORCEPAUSE    0
+#define V_FORCEPAUSE(x) ((x) << S_FORCEPAUSE)
+#define F_FORCEPAUSE    V_FORCEPAUSE(1U)
+
+#define A_NCSI_PAUSE_TIMEOUT 0x1a0e4
+#define A_NCSI_PAUSE_WM 0x1a0ec
+
+#define S_PAUSEHWM    16
+#define M_PAUSEHWM    0x7ffU
+#define V_PAUSEHWM(x) ((x) << S_PAUSEHWM)
+#define G_PAUSEHWM(x) (((x) >> S_PAUSEHWM) & M_PAUSEHWM)
+
+#define S_PAUSELWM    0
+#define M_PAUSELWM    0x7ffU
+#define V_PAUSELWM(x) ((x) << S_PAUSELWM)
+#define G_PAUSELWM(x) (((x) >> S_PAUSELWM) & M_PAUSELWM)
+
+#define A_NCSI_DEBUG 0x1a0f0
+
+#define S_DEBUGSEL    0
+#define M_DEBUGSEL    0x3fU
+#define V_DEBUGSEL(x) ((x) << S_DEBUGSEL)
+#define G_DEBUGSEL(x) (((x) >> S_DEBUGSEL) & M_DEBUGSEL)
+
+#define A_NCSI_PERR_INJECT 0x1a0f4
+
+#define S_MCSIMELSEL    1
+#define V_MCSIMELSEL(x) ((x) << S_MCSIMELSEL)
+#define F_MCSIMELSEL    V_MCSIMELSEL(1U)
+
+#define A_NCSI_MACB_NETWORK_CTRL 0x1a100
+
+#define S_TXSNDZEROPAUSE    12
+#define V_TXSNDZEROPAUSE(x) ((x) << S_TXSNDZEROPAUSE)
+#define F_TXSNDZEROPAUSE    V_TXSNDZEROPAUSE(1U)
+
+#define S_TXSNDPAUSE    11
+#define V_TXSNDPAUSE(x) ((x) << S_TXSNDPAUSE)
+#define F_TXSNDPAUSE    V_TXSNDPAUSE(1U)
+
+#define S_TXSTOP    10
+#define V_TXSTOP(x) ((x) << S_TXSTOP)
+#define F_TXSTOP    V_TXSTOP(1U)
+
+#define S_TXSTART    9
+#define V_TXSTART(x) ((x) << S_TXSTART)
+#define F_TXSTART    V_TXSTART(1U)
+
+#define S_BACKPRESS    8
+#define V_BACKPRESS(x) ((x) << S_BACKPRESS)
+#define F_BACKPRESS    V_BACKPRESS(1U)
+
+#define S_STATWREN    7
+#define V_STATWREN(x) ((x) << S_STATWREN)
+#define F_STATWREN    V_STATWREN(1U)
+
+#define S_INCRSTAT    6
+#define V_INCRSTAT(x) ((x) << S_INCRSTAT)
+#define F_INCRSTAT    V_INCRSTAT(1U)
+
+#define S_CLEARSTAT    5
+#define V_CLEARSTAT(x) ((x) << S_CLEARSTAT)
+#define F_CLEARSTAT    V_CLEARSTAT(1U)
+
+#define S_ENMGMTPORT    4
+#define V_ENMGMTPORT(x) ((x) << S_ENMGMTPORT)
+#define F_ENMGMTPORT    V_ENMGMTPORT(1U)
+
+#define S_NCSITXEN    3
+#define V_NCSITXEN(x) ((x) << S_NCSITXEN)
+#define F_NCSITXEN    V_NCSITXEN(1U)
+
+#define S_NCSIRXEN    2
+#define V_NCSIRXEN(x) ((x) << S_NCSIRXEN)
+#define F_NCSIRXEN    V_NCSIRXEN(1U)
+
+#define S_LOOPLOCAL    1
+#define V_LOOPLOCAL(x) ((x) << S_LOOPLOCAL)
+#define F_LOOPLOCAL    V_LOOPLOCAL(1U)
+
+#define S_LOOPPHY    0
+#define V_LOOPPHY(x) ((x) << S_LOOPPHY)
+#define F_LOOPPHY    V_LOOPPHY(1U)
+
+#define A_NCSI_MACB_NETWORK_CFG 0x1a104
+
+#define S_PCLKDIV128    22
+#define V_PCLKDIV128(x) ((x) << S_PCLKDIV128)
+#define F_PCLKDIV128    V_PCLKDIV128(1U)
+
+#define S_COPYPAUSE    21
+#define V_COPYPAUSE(x) ((x) << S_COPYPAUSE)
+#define F_COPYPAUSE    V_COPYPAUSE(1U)
+
+#define S_NONSTDPREOK    20
+#define V_NONSTDPREOK(x) ((x) << S_NONSTDPREOK)
+#define F_NONSTDPREOK    V_NONSTDPREOK(1U)
+
+#define S_NOFCS    19
+#define V_NOFCS(x) ((x) << S_NOFCS)
+#define F_NOFCS    V_NOFCS(1U)
+
+#define S_RXENHALFDUP    18
+#define V_RXENHALFDUP(x) ((x) << S_RXENHALFDUP)
+#define F_RXENHALFDUP    V_RXENHALFDUP(1U)
+
+#define S_NOCOPYFCS    17
+#define V_NOCOPYFCS(x) ((x) << S_NOCOPYFCS)
+#define F_NOCOPYFCS    V_NOCOPYFCS(1U)
+
+#define S_LENCHKEN    16
+#define V_LENCHKEN(x) ((x) << S_LENCHKEN)
+#define F_LENCHKEN    V_LENCHKEN(1U)
+
+#define S_RXBUFOFFSET    14
+#define M_RXBUFOFFSET    0x3U
+#define V_RXBUFOFFSET(x) ((x) << S_RXBUFOFFSET)
+#define G_RXBUFOFFSET(x) (((x) >> S_RXBUFOFFSET) & M_RXBUFOFFSET)
+
+#define S_PAUSEEN    13
+#define V_PAUSEEN(x) ((x) << S_PAUSEEN)
+#define F_PAUSEEN    V_PAUSEEN(1U)
+
+#define S_RETRYTEST    12
+#define V_RETRYTEST(x) ((x) << S_RETRYTEST)
+#define F_RETRYTEST    V_RETRYTEST(1U)
+
+#define S_PCLKDIV    10
+#define M_PCLKDIV    0x3U
+#define V_PCLKDIV(x) ((x) << S_PCLKDIV)
+#define G_PCLKDIV(x) (((x) >> S_PCLKDIV) & M_PCLKDIV)
+
+#define S_EXTCLASS    9
+#define V_EXTCLASS(x) ((x) << S_EXTCLASS)
+#define F_EXTCLASS    V_EXTCLASS(1U)
+
+#define S_EN1536FRAME    8
+#define V_EN1536FRAME(x) ((x) << S_EN1536FRAME)
+#define F_EN1536FRAME    V_EN1536FRAME(1U)
+
+#define S_UCASTHASHEN    7
+#define V_UCASTHASHEN(x) ((x) << S_UCASTHASHEN)
+#define F_UCASTHASHEN    V_UCASTHASHEN(1U)
+
+#define S_MCASTHASHEN    6
+#define V_MCASTHASHEN(x) ((x) << S_MCASTHASHEN)
+#define F_MCASTHASHEN    V_MCASTHASHEN(1U)
+
+#define S_RXBCASTDIS    5
+#define V_RXBCASTDIS(x) ((x) << S_RXBCASTDIS)
+#define F_RXBCASTDIS    V_RXBCASTDIS(1U)
+
+#define S_NCSICOPYALLFRAMES    4
+#define V_NCSICOPYALLFRAMES(x) ((x) << S_NCSICOPYALLFRAMES)
+#define F_NCSICOPYALLFRAMES    V_NCSICOPYALLFRAMES(1U)
+
+#define S_JUMBOEN    3
+#define V_JUMBOEN(x) ((x) << S_JUMBOEN)
+#define F_JUMBOEN    V_JUMBOEN(1U)
+
+#define S_SEREN    2
+#define V_SEREN(x) ((x) << S_SEREN)
+#define F_SEREN    V_SEREN(1U)
+
+#define S_FULLDUPLEX    1
+#define V_FULLDUPLEX(x) ((x) << S_FULLDUPLEX)
+#define F_FULLDUPLEX    V_FULLDUPLEX(1U)
+
+#define S_SPEED    0
+#define V_SPEED(x) ((x) << S_SPEED)
+#define F_SPEED    V_SPEED(1U)
+
+#define A_NCSI_MACB_NETWORK_STATUS 0x1a108
+
+#define S_PHYMGMTSTATUS    2
+#define V_PHYMGMTSTATUS(x) ((x) << S_PHYMGMTSTATUS)
+#define F_PHYMGMTSTATUS    V_PHYMGMTSTATUS(1U)
+
+#define S_MDISTATUS    1
+#define V_MDISTATUS(x) ((x) << S_MDISTATUS)
+#define F_MDISTATUS    V_MDISTATUS(1U)
+
+#define S_LINKSTATUS    0
+#define V_LINKSTATUS(x) ((x) << S_LINKSTATUS)
+#define F_LINKSTATUS    V_LINKSTATUS(1U)
+
+#define A_NCSI_MACB_TX_STATUS 0x1a114
+
+#define S_UNDERRUNERR    6
+#define V_UNDERRUNERR(x) ((x) << S_UNDERRUNERR)
+#define F_UNDERRUNERR    V_UNDERRUNERR(1U)
+
+#define S_TXCOMPLETE    5
+#define V_TXCOMPLETE(x) ((x) << S_TXCOMPLETE)
+#define F_TXCOMPLETE    V_TXCOMPLETE(1U)
+
+#define S_BUFFEREXHAUSTED    4
+#define V_BUFFEREXHAUSTED(x) ((x) << S_BUFFEREXHAUSTED)
+#define F_BUFFEREXHAUSTED    V_BUFFEREXHAUSTED(1U)
+
+#define S_TXPROGRESS    3
+#define V_TXPROGRESS(x) ((x) << S_TXPROGRESS)
+#define F_TXPROGRESS    V_TXPROGRESS(1U)
+
+#define S_RETRYLIMIT    2
+#define V_RETRYLIMIT(x) ((x) << S_RETRYLIMIT)
+#define F_RETRYLIMIT    V_RETRYLIMIT(1U)
+
+#define S_COLEVENT    1
+#define V_COLEVENT(x) ((x) << S_COLEVENT)
+#define F_COLEVENT    V_COLEVENT(1U)
+
+#define S_USEDBITREAD    0
+#define V_USEDBITREAD(x) ((x) << S_USEDBITREAD)
+#define F_USEDBITREAD    V_USEDBITREAD(1U)
+
+#define A_NCSI_MACB_RX_BUF_QPTR 0x1a118
+
+#define S_RXBUFQPTR    2
+#define M_RXBUFQPTR    0x3fffffffU
+#define V_RXBUFQPTR(x) ((x) << S_RXBUFQPTR)
+#define G_RXBUFQPTR(x) (((x) >> S_RXBUFQPTR) & M_RXBUFQPTR)
+
+#define A_NCSI_MACB_TX_BUF_QPTR 0x1a11c
+
+#define S_TXBUFQPTR    2
+#define M_TXBUFQPTR    0x3fffffffU
+#define V_TXBUFQPTR(x) ((x) << S_TXBUFQPTR)
+#define G_TXBUFQPTR(x) (((x) >> S_TXBUFQPTR) & M_TXBUFQPTR)
+
+#define A_NCSI_MACB_RX_STATUS 0x1a120
+
+#define S_RXOVERRUNERR    2
+#define V_RXOVERRUNERR(x) ((x) << S_RXOVERRUNERR)
+#define F_RXOVERRUNERR    V_RXOVERRUNERR(1U)
+
+#define S_MACB_FRAMERCVD    1
+#define V_MACB_FRAMERCVD(x) ((x) << S_MACB_FRAMERCVD)
+#define F_MACB_FRAMERCVD    V_MACB_FRAMERCVD(1U)
+
+#define S_NORXBUF    0
+#define V_NORXBUF(x) ((x) << S_NORXBUF)
+#define F_NORXBUF    V_NORXBUF(1U)
+
+#define A_NCSI_MACB_INT_STATUS 0x1a124
+
+#define S_PAUSETIMEZERO    13
+#define V_PAUSETIMEZERO(x) ((x) << S_PAUSETIMEZERO)
+#define F_PAUSETIMEZERO    V_PAUSETIMEZERO(1U)
+
+#define S_PAUSERCVD    12
+#define V_PAUSERCVD(x) ((x) << S_PAUSERCVD)
+#define F_PAUSERCVD    V_PAUSERCVD(1U)
+
+#define S_HRESPNOTOK    11
+#define V_HRESPNOTOK(x) ((x) << S_HRESPNOTOK)
+#define F_HRESPNOTOK    V_HRESPNOTOK(1U)
+
+#define S_RXOVERRUN    10
+#define V_RXOVERRUN(x) ((x) << S_RXOVERRUN)
+#define F_RXOVERRUN    V_RXOVERRUN(1U)
+
+#define S_LINKCHANGE    9
+#define V_LINKCHANGE(x) ((x) << S_LINKCHANGE)
+#define F_LINKCHANGE    V_LINKCHANGE(1U)
+
+#define S_INT_TXCOMPLETE    7
+#define V_INT_TXCOMPLETE(x) ((x) << S_INT_TXCOMPLETE)
+#define F_INT_TXCOMPLETE    V_INT_TXCOMPLETE(1U)
+
+#define S_TXBUFERR    6
+#define V_TXBUFERR(x) ((x) << S_TXBUFERR)
+#define F_TXBUFERR    V_TXBUFERR(1U)
+
+#define S_RETRYLIMITERR    5
+#define V_RETRYLIMITERR(x) ((x) << S_RETRYLIMITERR)
+#define F_RETRYLIMITERR    V_RETRYLIMITERR(1U)
+
+#define S_TXBUFUNDERRUN    4
+#define V_TXBUFUNDERRUN(x) ((x) << S_TXBUFUNDERRUN)
+#define F_TXBUFUNDERRUN    V_TXBUFUNDERRUN(1U)
+
+#define S_TXUSEDBITREAD    3
+#define V_TXUSEDBITREAD(x) ((x) << S_TXUSEDBITREAD)
+#define F_TXUSEDBITREAD    V_TXUSEDBITREAD(1U)
+
+#define S_RXUSEDBITREAD    2
+#define V_RXUSEDBITREAD(x) ((x) << S_RXUSEDBITREAD)
+#define F_RXUSEDBITREAD    V_RXUSEDBITREAD(1U)
+
+#define S_RXCOMPLETE    1
+#define V_RXCOMPLETE(x) ((x) << S_RXCOMPLETE)
+#define F_RXCOMPLETE    V_RXCOMPLETE(1U)
+
+#define S_MGMTFRAMESENT    0
+#define V_MGMTFRAMESENT(x) ((x) << S_MGMTFRAMESENT)
+#define F_MGMTFRAMESENT    V_MGMTFRAMESENT(1U)
+
+#define A_NCSI_MACB_INT_EN 0x1a128
+#define A_NCSI_MACB_INT_DIS 0x1a12c
+#define A_NCSI_MACB_INT_MASK 0x1a130
+#define A_NCSI_MACB_PAUSE_TIME 0x1a138
+
+#define S_PAUSETIME    0
+#define M_PAUSETIME    0xffffU
+#define V_PAUSETIME(x) ((x) << S_PAUSETIME)
+#define G_PAUSETIME(x) (((x) >> S_PAUSETIME) & M_PAUSETIME)
+
+#define A_NCSI_MACB_PAUSE_FRAMES_RCVD 0x1a13c
+
+#define S_PAUSEFRRCVD    0
+#define M_PAUSEFRRCVD    0xffffU
+#define V_PAUSEFRRCVD(x) ((x) << S_PAUSEFRRCVD)
+#define G_PAUSEFRRCVD(x) (((x) >> S_PAUSEFRRCVD) & M_PAUSEFRRCVD)
+
+#define A_NCSI_MACB_TX_FRAMES_OK 0x1a140
+
+#define S_TXFRAMESOK    0
+#define M_TXFRAMESOK    0xffffffU
+#define V_TXFRAMESOK(x) ((x) << S_TXFRAMESOK)
+#define G_TXFRAMESOK(x) (((x) >> S_TXFRAMESOK) & M_TXFRAMESOK)
+
+#define A_NCSI_MACB_SINGLE_COL_FRAMES 0x1a144
+
+#define S_SINGLECOLTXFRAMES    0
+#define M_SINGLECOLTXFRAMES    0xffffU
+#define V_SINGLECOLTXFRAMES(x) ((x) << S_SINGLECOLTXFRAMES)
+#define G_SINGLECOLTXFRAMES(x) (((x) >> S_SINGLECOLTXFRAMES) & M_SINGLECOLTXFRAMES)
+
+#define A_NCSI_MACB_MUL_COL_FRAMES 0x1a148
+
+#define S_MULCOLTXFRAMES    0
+#define M_MULCOLTXFRAMES    0xffffU
+#define V_MULCOLTXFRAMES(x) ((x) << S_MULCOLTXFRAMES)
+#define G_MULCOLTXFRAMES(x) (((x) >> S_MULCOLTXFRAMES) & M_MULCOLTXFRAMES)
+
+#define A_NCSI_MACB_RX_FRAMES_OK 0x1a14c
+
+#define S_RXFRAMESOK    0
+#define M_RXFRAMESOK    0xffffffU
+#define V_RXFRAMESOK(x) ((x) << S_RXFRAMESOK)
+#define G_RXFRAMESOK(x) (((x) >> S_RXFRAMESOK) & M_RXFRAMESOK)
+
+#define A_NCSI_MACB_FCS_ERR 0x1a150
+
+#define S_RXFCSERR    0
+#define M_RXFCSERR    0xffU
+#define V_RXFCSERR(x) ((x) << S_RXFCSERR)
+#define G_RXFCSERR(x) (((x) >> S_RXFCSERR) & M_RXFCSERR)
+
+#define A_NCSI_MACB_ALIGN_ERR 0x1a154
+
+#define S_RXALIGNERR    0
+#define M_RXALIGNERR    0xffU
+#define V_RXALIGNERR(x) ((x) << S_RXALIGNERR)
+#define G_RXALIGNERR(x) (((x) >> S_RXALIGNERR) & M_RXALIGNERR)
+
+#define A_NCSI_MACB_DEF_TX_FRAMES 0x1a158
+
+#define S_TXDEFERREDFRAMES    0
+#define M_TXDEFERREDFRAMES    0xffffU
+#define V_TXDEFERREDFRAMES(x) ((x) << S_TXDEFERREDFRAMES)
+#define G_TXDEFERREDFRAMES(x) (((x) >> S_TXDEFERREDFRAMES) & M_TXDEFERREDFRAMES)
+
+#define A_NCSI_MACB_LATE_COL 0x1a15c
+
+#define S_LATECOLLISIONS    0
+#define M_LATECOLLISIONS    0xffffU
+#define V_LATECOLLISIONS(x) ((x) << S_LATECOLLISIONS)
+#define G_LATECOLLISIONS(x) (((x) >> S_LATECOLLISIONS) & M_LATECOLLISIONS)
+
+#define A_NCSI_MACB_EXCESSIVE_COL 0x1a160
+
+#define S_EXCESSIVECOLLISIONS    0
+#define M_EXCESSIVECOLLISIONS    0xffU
+#define V_EXCESSIVECOLLISIONS(x) ((x) << S_EXCESSIVECOLLISIONS)
+#define G_EXCESSIVECOLLISIONS(x) (((x) >> S_EXCESSIVECOLLISIONS) & M_EXCESSIVECOLLISIONS)
+
+#define A_NCSI_MACB_TX_UNDERRUN_ERR 0x1a164
+
+#define S_TXUNDERRUNERR    0
+#define M_TXUNDERRUNERR    0xffU
+#define V_TXUNDERRUNERR(x) ((x) << S_TXUNDERRUNERR)
+#define G_TXUNDERRUNERR(x) (((x) >> S_TXUNDERRUNERR) & M_TXUNDERRUNERR)
+
+#define A_NCSI_MACB_CARRIER_SENSE_ERR 0x1a168
+
+#define S_CARRIERSENSEERRS    0
+#define M_CARRIERSENSEERRS    0xffU
+#define V_CARRIERSENSEERRS(x) ((x) << S_CARRIERSENSEERRS)
+#define G_CARRIERSENSEERRS(x) (((x) >> S_CARRIERSENSEERRS) & M_CARRIERSENSEERRS)
+
+#define A_NCSI_MACB_RX_RESOURCE_ERR 0x1a16c
+
+#define S_RXRESOURCEERR    0
+#define M_RXRESOURCEERR    0xffffU
+#define V_RXRESOURCEERR(x) ((x) << S_RXRESOURCEERR)
+#define G_RXRESOURCEERR(x) (((x) >> S_RXRESOURCEERR) & M_RXRESOURCEERR)
+
+#define A_NCSI_MACB_RX_OVERRUN_ERR 0x1a170
+
+#define S_RXOVERRUNERRCNT    0
+#define M_RXOVERRUNERRCNT    0xffU
+#define V_RXOVERRUNERRCNT(x) ((x) << S_RXOVERRUNERRCNT)
+#define G_RXOVERRUNERRCNT(x) (((x) >> S_RXOVERRUNERRCNT) & M_RXOVERRUNERRCNT)
+
+#define A_NCSI_MACB_RX_SYMBOL_ERR 0x1a174
+
+#define S_RXSYMBOLERR    0
+#define M_RXSYMBOLERR    0xffU
+#define V_RXSYMBOLERR(x) ((x) << S_RXSYMBOLERR)
+#define G_RXSYMBOLERR(x) (((x) >> S_RXSYMBOLERR) & M_RXSYMBOLERR)
+
+#define A_NCSI_MACB_RX_OVERSIZE_FRAME 0x1a178
+
+#define S_RXOVERSIZEERR    0
+#define M_RXOVERSIZEERR    0xffU
+#define V_RXOVERSIZEERR(x) ((x) << S_RXOVERSIZEERR)
+#define G_RXOVERSIZEERR(x) (((x) >> S_RXOVERSIZEERR) & M_RXOVERSIZEERR)
+
+#define A_NCSI_MACB_RX_JABBER_ERR 0x1a17c
+
+#define S_RXJABBERERR    0
+#define M_RXJABBERERR    0xffU
+#define V_RXJABBERERR(x) ((x) << S_RXJABBERERR)
+#define G_RXJABBERERR(x) (((x) >> S_RXJABBERERR) & M_RXJABBERERR)
+
+#define A_NCSI_MACB_RX_UNDERSIZE_FRAME 0x1a180
+
+#define S_RXUNDERSIZEFR    0
+#define M_RXUNDERSIZEFR    0xffU
+#define V_RXUNDERSIZEFR(x) ((x) << S_RXUNDERSIZEFR)
+#define G_RXUNDERSIZEFR(x) (((x) >> S_RXUNDERSIZEFR) & M_RXUNDERSIZEFR)
+
+#define A_NCSI_MACB_SQE_TEST_ERR 0x1a184
+
+#define S_SQETESTERR    0
+#define M_SQETESTERR    0xffU
+#define V_SQETESTERR(x) ((x) << S_SQETESTERR)
+#define G_SQETESTERR(x) (((x) >> S_SQETESTERR) & M_SQETESTERR)
+
+#define A_NCSI_MACB_LENGTH_ERR 0x1a188
+
+#define S_LENGTHERR    0
+#define M_LENGTHERR    0xffU
+#define V_LENGTHERR(x) ((x) << S_LENGTHERR)
+#define G_LENGTHERR(x) (((x) >> S_LENGTHERR) & M_LENGTHERR)
+
+#define A_NCSI_MACB_TX_PAUSE_FRAMES 0x1a18c
+
+#define S_TXPAUSEFRAMES    0
+#define M_TXPAUSEFRAMES    0xffffU
+#define V_TXPAUSEFRAMES(x) ((x) << S_TXPAUSEFRAMES)
+#define G_TXPAUSEFRAMES(x) (((x) >> S_TXPAUSEFRAMES) & M_TXPAUSEFRAMES)
+
+#define A_NCSI_MACB_HASH_LOW 0x1a190
+#define A_NCSI_MACB_HASH_HIGH 0x1a194
+#define A_NCSI_MACB_SPECIFIC_1_LOW 0x1a198
+#define A_NCSI_MACB_SPECIFIC_1_HIGH 0x1a19c
+
+#define S_MATCHHIGH    0
+#define M_MATCHHIGH    0xffffU
+#define V_MATCHHIGH(x) ((x) << S_MATCHHIGH)
+#define G_MATCHHIGH(x) (((x) >> S_MATCHHIGH) & M_MATCHHIGH)
+
+#define A_NCSI_MACB_SPECIFIC_2_LOW 0x1a1a0
+#define A_NCSI_MACB_SPECIFIC_2_HIGH 0x1a1a4
+#define A_NCSI_MACB_SPECIFIC_3_LOW 0x1a1a8
+#define A_NCSI_MACB_SPECIFIC_3_HIGH 0x1a1ac
+#define A_NCSI_MACB_SPECIFIC_4_LOW 0x1a1b0
+#define A_NCSI_MACB_SPECIFIC_4_HIGH 0x1a1b4
+#define A_NCSI_MACB_TYPE_ID 0x1a1b8
+
+#define S_TYPEID    0
+#define M_TYPEID    0xffffU
+#define V_TYPEID(x) ((x) << S_TYPEID)
+#define G_TYPEID(x) (((x) >> S_TYPEID) & M_TYPEID)
+
+#define A_NCSI_MACB_TX_PAUSE_QUANTUM 0x1a1bc
+
+#define S_TXPAUSEQUANTUM    0
+#define M_TXPAUSEQUANTUM    0xffffU
+#define V_TXPAUSEQUANTUM(x) ((x) << S_TXPAUSEQUANTUM)
+#define G_TXPAUSEQUANTUM(x) (((x) >> S_TXPAUSEQUANTUM) & M_TXPAUSEQUANTUM)
+
+#define A_NCSI_MACB_USER_IO 0x1a1c0
+
+#define S_USERPROGINPUT    16
+#define M_USERPROGINPUT    0xffffU
+#define V_USERPROGINPUT(x) ((x) << S_USERPROGINPUT)
+#define G_USERPROGINPUT(x) (((x) >> S_USERPROGINPUT) & M_USERPROGINPUT)
+
+#define S_USERPROGOUTPUT    0
+#define M_USERPROGOUTPUT    0xffffU
+#define V_USERPROGOUTPUT(x) ((x) << S_USERPROGOUTPUT)
+#define G_USERPROGOUTPUT(x) (((x) >> S_USERPROGOUTPUT) & M_USERPROGOUTPUT)
+
+#define A_NCSI_MACB_WOL_CFG 0x1a1c4
+
+#define S_MCHASHEN    19
+#define V_MCHASHEN(x) ((x) << S_MCHASHEN)
+#define F_MCHASHEN    V_MCHASHEN(1U)
+
+#define S_SPECIFIC1EN    18
+#define V_SPECIFIC1EN(x) ((x) << S_SPECIFIC1EN)
+#define F_SPECIFIC1EN    V_SPECIFIC1EN(1U)
+
+#define S_ARPEN    17
+#define V_ARPEN(x) ((x) << S_ARPEN)
+#define F_ARPEN    V_ARPEN(1U)
+
+#define S_MAGICPKTEN    16
+#define V_MAGICPKTEN(x) ((x) << S_MAGICPKTEN)
+#define F_MAGICPKTEN    V_MAGICPKTEN(1U)
+
+#define S_ARPIPADDR    0
+#define M_ARPIPADDR    0xffffU
+#define V_ARPIPADDR(x) ((x) << S_ARPIPADDR)
+#define G_ARPIPADDR(x) (((x) >> S_ARPIPADDR) & M_ARPIPADDR)
+
+#define A_NCSI_MACB_REV_STATUS 0x1a1fc
+
+#define S_PARTREF    16
+#define M_PARTREF    0xffffU
+#define V_PARTREF(x) ((x) << S_PARTREF)
+#define G_PARTREF(x) (((x) >> S_PARTREF) & M_PARTREF)
+
+#define S_DESREV    0
+#define M_DESREV    0xffffU
+#define V_DESREV(x) ((x) << S_DESREV)
+#define G_DESREV(x) (((x) >> S_DESREV) & M_DESREV)
+
+/* registers for module XGMAC */
+#define XGMAC_BASE_ADDR 0x0
+
+#define A_XGMAC_PORT_CFG 0x1000
+
+#define S_XGMII_CLK_SEL    29
+#define M_XGMII_CLK_SEL    0x7U
+#define V_XGMII_CLK_SEL(x) ((x) << S_XGMII_CLK_SEL)
+#define G_XGMII_CLK_SEL(x) (((x) >> S_XGMII_CLK_SEL) & M_XGMII_CLK_SEL)
+
+#define S_SINKTX    27
+#define V_SINKTX(x) ((x) << S_SINKTX)
+#define F_SINKTX    V_SINKTX(1U)
+
+#define S_SINKTXONLINKDOWN    26
+#define V_SINKTXONLINKDOWN(x) ((x) << S_SINKTXONLINKDOWN)
+#define F_SINKTXONLINKDOWN    V_SINKTXONLINKDOWN(1U)
+
+#define S_XG2G_SPEED_MODE    25
+#define V_XG2G_SPEED_MODE(x) ((x) << S_XG2G_SPEED_MODE)
+#define F_XG2G_SPEED_MODE    V_XG2G_SPEED_MODE(1U)
+
+#define S_LOOPNOFWD    24
+#define V_LOOPNOFWD(x) ((x) << S_LOOPNOFWD)
+#define F_LOOPNOFWD    V_LOOPNOFWD(1U)
+
+#define S_XGM_TX_PAUSE_SIZE    23
+#define V_XGM_TX_PAUSE_SIZE(x) ((x) << S_XGM_TX_PAUSE_SIZE)
+#define F_XGM_TX_PAUSE_SIZE    V_XGM_TX_PAUSE_SIZE(1U)
+
+#define S_XGM_TX_PAUSE_FRAME    22
+#define V_XGM_TX_PAUSE_FRAME(x) ((x) << S_XGM_TX_PAUSE_FRAME)
+#define F_XGM_TX_PAUSE_FRAME    V_XGM_TX_PAUSE_FRAME(1U)
+
+#define S_XGM_TX_DISABLE_PRE    21
+#define V_XGM_TX_DISABLE_PRE(x) ((x) << S_XGM_TX_DISABLE_PRE)
+#define F_XGM_TX_DISABLE_PRE    V_XGM_TX_DISABLE_PRE(1U)
+
+#define S_XGM_TX_DISABLE_CRC    20
+#define V_XGM_TX_DISABLE_CRC(x) ((x) << S_XGM_TX_DISABLE_CRC)
+#define F_XGM_TX_DISABLE_CRC    V_XGM_TX_DISABLE_CRC(1U)
+
+#define S_SMUX_RX_LOOP    19
+#define V_SMUX_RX_LOOP(x) ((x) << S_SMUX_RX_LOOP)
+#define F_SMUX_RX_LOOP    V_SMUX_RX_LOOP(1U)
+
+#define S_RX_LANE_SWAP    18
+#define V_RX_LANE_SWAP(x) ((x) << S_RX_LANE_SWAP)
+#define F_RX_LANE_SWAP    V_RX_LANE_SWAP(1U)
+
+#define S_TX_LANE_SWAP    17
+#define V_TX_LANE_SWAP(x) ((x) << S_TX_LANE_SWAP)
+#define F_TX_LANE_SWAP    V_TX_LANE_SWAP(1U)
+
+#define S_SIGNAL_DET    14
+#define V_SIGNAL_DET(x) ((x) << S_SIGNAL_DET)
+#define F_SIGNAL_DET    V_SIGNAL_DET(1U)
+
+#define S_PMUX_RX_LOOP    13
+#define V_PMUX_RX_LOOP(x) ((x) << S_PMUX_RX_LOOP)
+#define F_PMUX_RX_LOOP    V_PMUX_RX_LOOP(1U)
+
+#define S_PMUX_TX_LOOP    12
+#define V_PMUX_TX_LOOP(x) ((x) << S_PMUX_TX_LOOP)
+#define F_PMUX_TX_LOOP    V_PMUX_TX_LOOP(1U)
+
+#define S_XGM_RX_SEL    10
+#define M_XGM_RX_SEL    0x3U
+#define V_XGM_RX_SEL(x) ((x) << S_XGM_RX_SEL)
+#define G_XGM_RX_SEL(x) (((x) >> S_XGM_RX_SEL) & M_XGM_RX_SEL)
+
+#define S_PCS_TX_SEL    8
+#define M_PCS_TX_SEL    0x3U
+#define V_PCS_TX_SEL(x) ((x) << S_PCS_TX_SEL)
+#define G_PCS_TX_SEL(x) (((x) >> S_PCS_TX_SEL) & M_PCS_TX_SEL)
+
+#define S_XAUI20_REM_PRE    5
+#define V_XAUI20_REM_PRE(x) ((x) << S_XAUI20_REM_PRE)
+#define F_XAUI20_REM_PRE    V_XAUI20_REM_PRE(1U)
+
+#define S_XAUI20_XGMII_SEL    4
+#define V_XAUI20_XGMII_SEL(x) ((x) << S_XAUI20_XGMII_SEL)
+#define F_XAUI20_XGMII_SEL    V_XAUI20_XGMII_SEL(1U)
+
+#define S_PORT_SEL    0
+#define V_PORT_SEL(x) ((x) << S_PORT_SEL)
+#define F_PORT_SEL    V_PORT_SEL(1U)
+
+#define A_XGMAC_PORT_RESET_CTRL 0x1004
+
+#define S_AUXEXT_RESET    10
+#define V_AUXEXT_RESET(x) ((x) << S_AUXEXT_RESET)
+#define F_AUXEXT_RESET    V_AUXEXT_RESET(1U)
+
+#define S_TXFIFO_RESET    9
+#define V_TXFIFO_RESET(x) ((x) << S_TXFIFO_RESET)
+#define F_TXFIFO_RESET    V_TXFIFO_RESET(1U)
+
+#define S_RXFIFO_RESET    8
+#define V_RXFIFO_RESET(x) ((x) << S_RXFIFO_RESET)
+#define F_RXFIFO_RESET    V_RXFIFO_RESET(1U)
+
+#define S_BEAN_RESET    7
+#define V_BEAN_RESET(x) ((x) << S_BEAN_RESET)
+#define F_BEAN_RESET    V_BEAN_RESET(1U)
+
+#define S_XAUI_RESET    6
+#define V_XAUI_RESET(x) ((x) << S_XAUI_RESET)
+#define F_XAUI_RESET    V_XAUI_RESET(1U)
+
+#define S_AE_RESET    5
+#define V_AE_RESET(x) ((x) << S_AE_RESET)
+#define F_AE_RESET    V_AE_RESET(1U)
+
+#define S_XGM_RESET    4
+#define V_XGM_RESET(x) ((x) << S_XGM_RESET)
+#define F_XGM_RESET    V_XGM_RESET(1U)
+
+#define S_XG2G_RESET    3
+#define V_XG2G_RESET(x) ((x) << S_XG2G_RESET)
+#define F_XG2G_RESET    V_XG2G_RESET(1U)
+
+#define S_WOL_RESET    2
+#define V_WOL_RESET(x) ((x) << S_WOL_RESET)
+#define F_WOL_RESET    V_WOL_RESET(1U)
+
+#define S_XFI_PCS_RESET    1
+#define V_XFI_PCS_RESET(x) ((x) << S_XFI_PCS_RESET)
+#define F_XFI_PCS_RESET    V_XFI_PCS_RESET(1U)
+
+#define S_HSS_RESET    0
+#define V_HSS_RESET(x) ((x) << S_HSS_RESET)
+#define F_HSS_RESET    V_HSS_RESET(1U)
+
+#define A_XGMAC_PORT_LED_CFG 0x1008
+
+#define S_LED1_CFG    5
+#define M_LED1_CFG    0x7U
+#define V_LED1_CFG(x) ((x) << S_LED1_CFG)
+#define G_LED1_CFG(x) (((x) >> S_LED1_CFG) & M_LED1_CFG)
+
+#define S_LED1_POLARITY_INV    4
+#define V_LED1_POLARITY_INV(x) ((x) << S_LED1_POLARITY_INV)
+#define F_LED1_POLARITY_INV    V_LED1_POLARITY_INV(1U)
+
+#define S_LED0_CFG    1
+#define M_LED0_CFG    0x7U
+#define V_LED0_CFG(x) ((x) << S_LED0_CFG)
+#define G_LED0_CFG(x) (((x) >> S_LED0_CFG) & M_LED0_CFG)
+
+#define S_LED0_POLARITY_INV    0
+#define V_LED0_POLARITY_INV(x) ((x) << S_LED0_POLARITY_INV)
+#define F_LED0_POLARITY_INV    V_LED0_POLARITY_INV(1U)
+
+#define A_XGMAC_PORT_LED_COUNTHI 0x100c
+
+#define S_LED_COUNT_HI    0
+#define M_LED_COUNT_HI    0x1ffffffU
+#define V_LED_COUNT_HI(x) ((x) << S_LED_COUNT_HI)
+#define G_LED_COUNT_HI(x) (((x) >> S_LED_COUNT_HI) & M_LED_COUNT_HI)
+
+#define A_XGMAC_PORT_LED_COUNTLO 0x1010
+
+#define S_LED_COUNT_LO    0
+#define M_LED_COUNT_LO    0x1ffffffU
+#define V_LED_COUNT_LO(x) ((x) << S_LED_COUNT_LO)
+#define G_LED_COUNT_LO(x) (((x) >> S_LED_COUNT_LO) & M_LED_COUNT_LO)
+
+#define A_XGMAC_PORT_DEBUG_CFG 0x1014
+
+#define S_TESTCLK_SEL    0
+#define M_TESTCLK_SEL    0xfU
+#define V_TESTCLK_SEL(x) ((x) << S_TESTCLK_SEL)
+#define G_TESTCLK_SEL(x) (((x) >> S_TESTCLK_SEL) & M_TESTCLK_SEL)
+
+#define A_XGMAC_PORT_CFG2 0x1018
+
+#define S_RX_POLARITY_INV    28
+#define M_RX_POLARITY_INV    0xfU
+#define V_RX_POLARITY_INV(x) ((x) << S_RX_POLARITY_INV)
+#define G_RX_POLARITY_INV(x) (((x) >> S_RX_POLARITY_INV) & M_RX_POLARITY_INV)
+
+#define S_TX_POLARITY_INV    24
+#define M_TX_POLARITY_INV    0xfU
+#define V_TX_POLARITY_INV(x) ((x) << S_TX_POLARITY_INV)
+#define G_TX_POLARITY_INV(x) (((x) >> S_TX_POLARITY_INV) & M_TX_POLARITY_INV)
+
+#define S_INSTANCENUM    22
+#define M_INSTANCENUM    0x3U
+#define V_INSTANCENUM(x) ((x) << S_INSTANCENUM)
+#define G_INSTANCENUM(x) (((x) >> S_INSTANCENUM) & M_INSTANCENUM)
+
+#define S_STOPONPERR    21
+#define V_STOPONPERR(x) ((x) << S_STOPONPERR)
+#define F_STOPONPERR    V_STOPONPERR(1U)
+
+#define S_MACTXEN    20
+#define V_MACTXEN(x) ((x) << S_MACTXEN)
+#define F_MACTXEN    V_MACTXEN(1U)
+
+#define S_MACRXEN    19
+#define V_MACRXEN(x) ((x) << S_MACRXEN)
+#define F_MACRXEN    V_MACRXEN(1U)
+
+#define S_PATEN    18
+#define V_PATEN(x) ((x) << S_PATEN)
+#define F_PATEN    V_PATEN(1U)
+
+#define S_MAGICEN    17
+#define V_MAGICEN(x) ((x) << S_MAGICEN)
+#define F_MAGICEN    V_MAGICEN(1U)
+
+#define S_TX_IPG    4
+#define M_TX_IPG    0x1fffU
+#define V_TX_IPG(x) ((x) << S_TX_IPG)
+#define G_TX_IPG(x) (((x) >> S_TX_IPG) & M_TX_IPG)
+
+#define S_AEC_PMA_TX_READY    1
+#define V_AEC_PMA_TX_READY(x) ((x) << S_AEC_PMA_TX_READY)
+#define F_AEC_PMA_TX_READY    V_AEC_PMA_TX_READY(1U)
+
+#define S_AEC_PMA_RX_READY    0
+#define V_AEC_PMA_RX_READY(x) ((x) << S_AEC_PMA_RX_READY)
+#define F_AEC_PMA_RX_READY    V_AEC_PMA_RX_READY(1U)
+
+#define A_XGMAC_PORT_PKT_COUNT 0x101c
+
+#define S_TX_SOP_COUNT    24
+#define M_TX_SOP_COUNT    0xffU
+#define V_TX_SOP_COUNT(x) ((x) << S_TX_SOP_COUNT)
+#define G_TX_SOP_COUNT(x) (((x) >> S_TX_SOP_COUNT) & M_TX_SOP_COUNT)
+
+#define S_TX_EOP_COUNT    16
+#define M_TX_EOP_COUNT    0xffU
+#define V_TX_EOP_COUNT(x) ((x) << S_TX_EOP_COUNT)
+#define G_TX_EOP_COUNT(x) (((x) >> S_TX_EOP_COUNT) & M_TX_EOP_COUNT)
+
+#define S_RX_SOP_COUNT    8
+#define M_RX_SOP_COUNT    0xffU
+#define V_RX_SOP_COUNT(x) ((x) << S_RX_SOP_COUNT)
+#define G_RX_SOP_COUNT(x) (((x) >> S_RX_SOP_COUNT) & M_RX_SOP_COUNT)
+
+#define S_RX_EOP_COUNT    0
+#define M_RX_EOP_COUNT    0xffU
+#define V_RX_EOP_COUNT(x) ((x) << S_RX_EOP_COUNT)
+#define G_RX_EOP_COUNT(x) (((x) >> S_RX_EOP_COUNT) & M_RX_EOP_COUNT)
+
+#define A_XGMAC_PORT_PERR_INJECT 0x1020
+
+#define S_XGMMEMSEL    1
+#define V_XGMMEMSEL(x) ((x) << S_XGMMEMSEL)
+#define F_XGMMEMSEL    V_XGMMEMSEL(1U)
+
+#define A_XGMAC_PORT_MAGIC_MACID_LO 0x1024
+#define A_XGMAC_PORT_MAGIC_MACID_HI 0x1028
+
+#define S_MAC_WOL_DA    0
+#define M_MAC_WOL_DA    0xffffU
+#define V_MAC_WOL_DA(x) ((x) << S_MAC_WOL_DA)
+#define G_MAC_WOL_DA(x) (((x) >> S_MAC_WOL_DA) & M_MAC_WOL_DA)
+
+#define A_XGMAC_PORT_BUILD_REVISION 0x102c
+#define A_XGMAC_PORT_XGMII_SE_COUNT 0x1030
+
+#define S_TXSOP    24
+#define M_TXSOP    0xffU
+#define V_TXSOP(x) ((x) << S_TXSOP)
+#define G_TXSOP(x) (((x) >> S_TXSOP) & M_TXSOP)
+
+#define S_TXEOP    16
+#define M_TXEOP    0xffU
+#define V_TXEOP(x) ((x) << S_TXEOP)
+#define G_TXEOP(x) (((x) >> S_TXEOP) & M_TXEOP)
+
+#define S_RXSOP    8
+#define M_RXSOP    0xffU
+#define V_RXSOP(x) ((x) << S_RXSOP)
+#define G_RXSOP(x) (((x) >> S_RXSOP) & M_RXSOP)
+
+#define A_XGMAC_PORT_LINK_STATUS 0x1034
+
+#define S_REMFLT    3
+#define V_REMFLT(x) ((x) << S_REMFLT)
+#define F_REMFLT    V_REMFLT(1U)
+
+#define S_LOCFLT    2
+#define V_LOCFLT(x) ((x) << S_LOCFLT)
+#define F_LOCFLT    V_LOCFLT(1U)
+
+#define S_LINKUP    1
+#define V_LINKUP(x) ((x) << S_LINKUP)
+#define F_LINKUP    V_LINKUP(1U)
+
+#define S_LINKDN    0
+#define V_LINKDN(x) ((x) << S_LINKDN)
+#define F_LINKDN    V_LINKDN(1U)
+
+#define A_XGMAC_PORT_CHECKIN 0x1038
+
+#define S_PREAMBLE    1
+#define V_PREAMBLE(x) ((x) << S_PREAMBLE)
+#define F_PREAMBLE    V_PREAMBLE(1U)
+
+#define S_CHECKIN    0
+#define V_CHECKIN(x) ((x) << S_CHECKIN)
+#define F_CHECKIN    V_CHECKIN(1U)
+
+#define A_XGMAC_PORT_FAULT_TEST 0x103c
+
+#define S_FLTTYPE    1
+#define V_FLTTYPE(x) ((x) << S_FLTTYPE)
+#define F_FLTTYPE    V_FLTTYPE(1U)
+
+#define S_FLTCTRL    0
+#define V_FLTCTRL(x) ((x) << S_FLTCTRL)
+#define F_FLTCTRL    V_FLTCTRL(1U)
+
+#define A_XGMAC_PORT_SPARE 0x1040
+#define A_XGMAC_PORT_HSS_SIGDET_STATUS 0x1044
+
+#define S_SIGNALDETECT    0
+#define M_SIGNALDETECT    0xfU
+#define V_SIGNALDETECT(x) ((x) << S_SIGNALDETECT)
+#define G_SIGNALDETECT(x) (((x) >> S_SIGNALDETECT) & M_SIGNALDETECT)
+
+#define A_XGMAC_PORT_EXT_LOS_STATUS 0x1048
+#define A_XGMAC_PORT_EXT_LOS_CTRL 0x104c
+
+#define S_CTRL    0
+#define M_CTRL    0xfU
+#define V_CTRL(x) ((x) << S_CTRL)
+#define G_CTRL(x) (((x) >> S_CTRL) & M_CTRL)
+
+#define A_XGMAC_PORT_FPGA_PAUSE_CTL 0x1050
+
+#define S_CTL    31
+#define V_CTL(x) ((x) << S_CTL)
+#define F_CTL    V_CTL(1U)
+
+#define S_HWM    13
+#define M_HWM    0x1fffU
+#define V_HWM(x) ((x) << S_HWM)
+#define G_HWM(x) (((x) >> S_HWM) & M_HWM)
+
+#define S_LWM    0
+#define M_LWM    0x1fffU
+#define V_LWM(x) ((x) << S_LWM)
+#define G_LWM(x) (((x) >> S_LWM) & M_LWM)
+
+#define A_XGMAC_PORT_FPGA_ERRPKT_CNT 0x1054
+#define A_XGMAC_PORT_LA_TX_0 0x1058
+#define A_XGMAC_PORT_LA_RX_0 0x105c
+#define A_XGMAC_PORT_FPGA_LA_CTL 0x1060
+
+#define S_RXRST    5
+#define V_RXRST(x) ((x) << S_RXRST)
+#define F_RXRST    V_RXRST(1U)
+
+#define S_TXRST    4
+#define V_TXRST(x) ((x) << S_TXRST)
+#define F_TXRST    V_TXRST(1U)
+
+#define S_XGMII    3
+#define V_XGMII(x) ((x) << S_XGMII)
+#define F_XGMII    V_XGMII(1U)
+
+#define S_LAPAUSE    2
+#define V_LAPAUSE(x) ((x) << S_LAPAUSE)
+#define F_LAPAUSE    V_LAPAUSE(1U)
+
+#define S_STOPERR    1
+#define V_STOPERR(x) ((x) << S_STOPERR)
+#define F_STOPERR    V_STOPERR(1U)
+
+#define S_LASTOP    0
+#define V_LASTOP(x) ((x) << S_LASTOP)
+#define F_LASTOP    V_LASTOP(1U)
+
+#define A_XGMAC_PORT_EPIO_DATA0 0x10c0
+#define A_XGMAC_PORT_EPIO_DATA1 0x10c4
+#define A_XGMAC_PORT_EPIO_DATA2 0x10c8
+#define A_XGMAC_PORT_EPIO_DATA3 0x10cc
+#define A_XGMAC_PORT_EPIO_OP 0x10d0
+
+#define S_EPIOWR    8
+#define V_EPIOWR(x) ((x) << S_EPIOWR)
+#define F_EPIOWR    V_EPIOWR(1U)
+
+#define S_ADDRESS    0
+#define M_ADDRESS    0xffU
+#define V_ADDRESS(x) ((x) << S_ADDRESS)
+#define G_ADDRESS(x) (((x) >> S_ADDRESS) & M_ADDRESS)
+
+#define A_XGMAC_PORT_WOL_STATUS 0x10d4
+
+#define S_MAGICDETECTED    31
+#define V_MAGICDETECTED(x) ((x) << S_MAGICDETECTED)
+#define F_MAGICDETECTED    V_MAGICDETECTED(1U)
+
+#define S_PATDETECTED    30
+#define V_PATDETECTED(x) ((x) << S_PATDETECTED)
+#define F_PATDETECTED    V_PATDETECTED(1U)
+
+#define S_CLEARMAGIC    4
+#define V_CLEARMAGIC(x) ((x) << S_CLEARMAGIC)
+#define F_CLEARMAGIC    V_CLEARMAGIC(1U)
+
+#define S_CLEARMATCH    3
+#define V_CLEARMATCH(x) ((x) << S_CLEARMATCH)
+#define F_CLEARMATCH    V_CLEARMATCH(1U)
+
+#define S_MATCHEDFILTER    0
+#define M_MATCHEDFILTER    0x7U
+#define V_MATCHEDFILTER(x) ((x) << S_MATCHEDFILTER)
+#define G_MATCHEDFILTER(x) (((x) >> S_MATCHEDFILTER) & M_MATCHEDFILTER)
+
+#define A_XGMAC_PORT_INT_EN 0x10d8
+
+#define S_EXT_LOS    28
+#define V_EXT_LOS(x) ((x) << S_EXT_LOS)
+#define F_EXT_LOS    V_EXT_LOS(1U)
+
+#define S_INCMPTBL_LINK    27
+#define V_INCMPTBL_LINK(x) ((x) << S_INCMPTBL_LINK)
+#define F_INCMPTBL_LINK    V_INCMPTBL_LINK(1U)
+
+#define S_PATDETWAKE    26
+#define V_PATDETWAKE(x) ((x) << S_PATDETWAKE)
+#define F_PATDETWAKE    V_PATDETWAKE(1U)
+
+#define S_MAGICWAKE    25
+#define V_MAGICWAKE(x) ((x) << S_MAGICWAKE)
+#define F_MAGICWAKE    V_MAGICWAKE(1U)
+
+#define S_SIGDETCHG    24
+#define V_SIGDETCHG(x) ((x) << S_SIGDETCHG)
+#define F_SIGDETCHG    V_SIGDETCHG(1U)
+
+#define S_PCSR_FEC_CORR    23
+#define V_PCSR_FEC_CORR(x) ((x) << S_PCSR_FEC_CORR)
+#define F_PCSR_FEC_CORR    V_PCSR_FEC_CORR(1U)
+
+#define S_AE_TRAIN_LOCAL    22
+#define V_AE_TRAIN_LOCAL(x) ((x) << S_AE_TRAIN_LOCAL)
+#define F_AE_TRAIN_LOCAL    V_AE_TRAIN_LOCAL(1U)
+
+#define S_HSSPLL_LOCK    21
+#define V_HSSPLL_LOCK(x) ((x) << S_HSSPLL_LOCK)
+#define F_HSSPLL_LOCK    V_HSSPLL_LOCK(1U)
+
+#define S_HSSPRT_READY    20
+#define V_HSSPRT_READY(x) ((x) << S_HSSPRT_READY)
+#define F_HSSPRT_READY    V_HSSPRT_READY(1U)
+
+#define S_AUTONEG_DONE    19
+#define V_AUTONEG_DONE(x) ((x) << S_AUTONEG_DONE)
+#define F_AUTONEG_DONE    V_AUTONEG_DONE(1U)
+
+#define S_PCSR_HI_BER    18
+#define V_PCSR_HI_BER(x) ((x) << S_PCSR_HI_BER)
+#define F_PCSR_HI_BER    V_PCSR_HI_BER(1U)
+
+#define S_PCSR_FEC_ERROR    17
+#define V_PCSR_FEC_ERROR(x) ((x) << S_PCSR_FEC_ERROR)
+#define F_PCSR_FEC_ERROR    V_PCSR_FEC_ERROR(1U)
+
+#define S_PCSR_LINK_FAIL    16
+#define V_PCSR_LINK_FAIL(x) ((x) << S_PCSR_LINK_FAIL)
+#define F_PCSR_LINK_FAIL    V_PCSR_LINK_FAIL(1U)
+
+#define S_XAUI_DEC_ERROR    15
+#define V_XAUI_DEC_ERROR(x) ((x) << S_XAUI_DEC_ERROR)
+#define F_XAUI_DEC_ERROR    V_XAUI_DEC_ERROR(1U)
+
+#define S_XAUI_LINK_FAIL    14
+#define V_XAUI_LINK_FAIL(x) ((x) << S_XAUI_LINK_FAIL)
+#define F_XAUI_LINK_FAIL    V_XAUI_LINK_FAIL(1U)
+
+#define S_PCS_CTC_ERROR    13
+#define V_PCS_CTC_ERROR(x) ((x) << S_PCS_CTC_ERROR)
+#define F_PCS_CTC_ERROR    V_PCS_CTC_ERROR(1U)
+
+#define S_PCS_LINK_GOOD    12
+#define V_PCS_LINK_GOOD(x) ((x) << S_PCS_LINK_GOOD)
+#define F_PCS_LINK_GOOD    V_PCS_LINK_GOOD(1U)
+
+#define S_PCS_LINK_FAIL    11
+#define V_PCS_LINK_FAIL(x) ((x) << S_PCS_LINK_FAIL)
+#define F_PCS_LINK_FAIL    V_PCS_LINK_FAIL(1U)
+
+#define S_RXFIFOOVERFLOW    10
+#define V_RXFIFOOVERFLOW(x) ((x) << S_RXFIFOOVERFLOW)
+#define F_RXFIFOOVERFLOW    V_RXFIFOOVERFLOW(1U)
+
+#define S_HSSPRBSERR    9
+#define V_HSSPRBSERR(x) ((x) << S_HSSPRBSERR)
+#define F_HSSPRBSERR    V_HSSPRBSERR(1U)
+
+#define S_HSSEYEQUAL    8
+#define V_HSSEYEQUAL(x) ((x) << S_HSSEYEQUAL)
+#define F_HSSEYEQUAL    V_HSSEYEQUAL(1U)
+
+#define S_REMOTEFAULT    7
+#define V_REMOTEFAULT(x) ((x) << S_REMOTEFAULT)
+#define F_REMOTEFAULT    V_REMOTEFAULT(1U)
+
+#define S_LOCALFAULT    6
+#define V_LOCALFAULT(x) ((x) << S_LOCALFAULT)
+#define F_LOCALFAULT    V_LOCALFAULT(1U)
+
+#define S_MAC_LINK_DOWN    5
+#define V_MAC_LINK_DOWN(x) ((x) << S_MAC_LINK_DOWN)
+#define F_MAC_LINK_DOWN    V_MAC_LINK_DOWN(1U)
+
+#define S_MAC_LINK_UP    4
+#define V_MAC_LINK_UP(x) ((x) << S_MAC_LINK_UP)
+#define F_MAC_LINK_UP    V_MAC_LINK_UP(1U)
+
+#define S_BEAN_INT    3
+#define V_BEAN_INT(x) ((x) << S_BEAN_INT)
+#define F_BEAN_INT    V_BEAN_INT(1U)
+
+#define S_XGM_INT    2
+#define V_XGM_INT(x) ((x) << S_XGM_INT)
+#define F_XGM_INT    V_XGM_INT(1U)
+
+#define A_XGMAC_PORT_INT_CAUSE 0x10dc
+#define A_XGMAC_PORT_HSS_CFG0 0x10e0
+
+#define S_TXDTS    31
+#define V_TXDTS(x) ((x) << S_TXDTS)
+#define F_TXDTS    V_TXDTS(1U)
+
+#define S_TXCTS    30
+#define V_TXCTS(x) ((x) << S_TXCTS)
+#define F_TXCTS    V_TXCTS(1U)
+
+#define S_TXBTS    29
+#define V_TXBTS(x) ((x) << S_TXBTS)
+#define F_TXBTS    V_TXBTS(1U)
+
+#define S_TXATS    28
+#define V_TXATS(x) ((x) << S_TXATS)
+#define F_TXATS    V_TXATS(1U)
+
+#define S_TXDOBS    27
+#define V_TXDOBS(x) ((x) << S_TXDOBS)
+#define F_TXDOBS    V_TXDOBS(1U)
+
+#define S_TXCOBS    26
+#define V_TXCOBS(x) ((x) << S_TXCOBS)
+#define F_TXCOBS    V_TXCOBS(1U)
+
+#define S_TXBOBS    25
+#define V_TXBOBS(x) ((x) << S_TXBOBS)
+#define F_TXBOBS    V_TXBOBS(1U)
+
+#define S_TXAOBS    24
+#define V_TXAOBS(x) ((x) << S_TXAOBS)
+#define F_TXAOBS    V_TXAOBS(1U)
+
+#define S_HSSREFCLKSEL    20
+#define V_HSSREFCLKSEL(x) ((x) << S_HSSREFCLKSEL)
+#define F_HSSREFCLKSEL    V_HSSREFCLKSEL(1U)
+
+#define S_HSSAVDHI    17
+#define V_HSSAVDHI(x) ((x) << S_HSSAVDHI)
+#define F_HSSAVDHI    V_HSSAVDHI(1U)
+
+#define S_HSSRXTS    16
+#define V_HSSRXTS(x) ((x) << S_HSSRXTS)
+#define F_HSSRXTS    V_HSSRXTS(1U)
+
+#define S_HSSTXACMODE    15
+#define V_HSSTXACMODE(x) ((x) << S_HSSTXACMODE)
+#define F_HSSTXACMODE    V_HSSTXACMODE(1U)
+
+#define S_HSSRXACMODE    14
+#define V_HSSRXACMODE(x) ((x) << S_HSSRXACMODE)
+#define F_HSSRXACMODE    V_HSSRXACMODE(1U)
+
+#define S_HSSRESYNC    13
+#define V_HSSRESYNC(x) ((x) << S_HSSRESYNC)
+#define F_HSSRESYNC    V_HSSRESYNC(1U)
+
+#define S_HSSRECCAL    12
+#define V_HSSRECCAL(x) ((x) << S_HSSRECCAL)
+#define F_HSSRECCAL    V_HSSRECCAL(1U)
+
+#define S_HSSPDWNPLL    11
+#define V_HSSPDWNPLL(x) ((x) << S_HSSPDWNPLL)
+#define F_HSSPDWNPLL    V_HSSPDWNPLL(1U)
+
+#define S_HSSDIVSEL    9
+#define M_HSSDIVSEL    0x3U
+#define V_HSSDIVSEL(x) ((x) << S_HSSDIVSEL)
+#define G_HSSDIVSEL(x) (((x) >> S_HSSDIVSEL) & M_HSSDIVSEL)
+
+#define S_HSSREFDIV    8
+#define V_HSSREFDIV(x) ((x) << S_HSSREFDIV)
+#define F_HSSREFDIV    V_HSSREFDIV(1U)
+
+#define S_HSSPLLBYP    7
+#define V_HSSPLLBYP(x) ((x) << S_HSSPLLBYP)
+#define F_HSSPLLBYP    V_HSSPLLBYP(1U)
+
+#define S_HSSLOFREQPLL    6
+#define V_HSSLOFREQPLL(x) ((x) << S_HSSLOFREQPLL)
+#define F_HSSLOFREQPLL    V_HSSLOFREQPLL(1U)
+
+#define S_HSSLOFREQ2PLL    5
+#define V_HSSLOFREQ2PLL(x) ((x) << S_HSSLOFREQ2PLL)
+#define F_HSSLOFREQ2PLL    V_HSSLOFREQ2PLL(1U)
+
+#define S_HSSEXTC16SEL    4
+#define V_HSSEXTC16SEL(x) ((x) << S_HSSEXTC16SEL)
+#define F_HSSEXTC16SEL    V_HSSEXTC16SEL(1U)
+
+#define S_HSSRSTCONFIG    1
+#define M_HSSRSTCONFIG    0x7U
+#define V_HSSRSTCONFIG(x) ((x) << S_HSSRSTCONFIG)
+#define G_HSSRSTCONFIG(x) (((x) >> S_HSSRSTCONFIG) & M_HSSRSTCONFIG)
+
+#define S_HSSPRBSEN    0
+#define V_HSSPRBSEN(x) ((x) << S_HSSPRBSEN)
+#define F_HSSPRBSEN    V_HSSPRBSEN(1U)
+
+#define A_XGMAC_PORT_HSS_CFG1 0x10e4
+
+#define S_RXDPRBSRST    28
+#define V_RXDPRBSRST(x) ((x) << S_RXDPRBSRST)
+#define F_RXDPRBSRST    V_RXDPRBSRST(1U)
+
+#define S_RXDPRBSEN    27
+#define V_RXDPRBSEN(x) ((x) << S_RXDPRBSEN)
+#define F_RXDPRBSEN    V_RXDPRBSEN(1U)
+
+#define S_RXDPRBSFRCERR    26
+#define V_RXDPRBSFRCERR(x) ((x) << S_RXDPRBSFRCERR)
+#define F_RXDPRBSFRCERR    V_RXDPRBSFRCERR(1U)
+
+#define S_TXDPRBSRST    25
+#define V_TXDPRBSRST(x) ((x) << S_TXDPRBSRST)
+#define F_TXDPRBSRST    V_TXDPRBSRST(1U)
+
+#define S_TXDPRBSEN    24
+#define V_TXDPRBSEN(x) ((x) << S_TXDPRBSEN)
+#define F_TXDPRBSEN    V_TXDPRBSEN(1U)
+
+#define S_RXCPRBSRST    20
+#define V_RXCPRBSRST(x) ((x) << S_RXCPRBSRST)
+#define F_RXCPRBSRST    V_RXCPRBSRST(1U)
+
+#define S_RXCPRBSEN    19
+#define V_RXCPRBSEN(x) ((x) << S_RXCPRBSEN)
+#define F_RXCPRBSEN    V_RXCPRBSEN(1U)
+
+#define S_RXCPRBSFRCERR    18
+#define V_RXCPRBSFRCERR(x) ((x) << S_RXCPRBSFRCERR)
+#define F_RXCPRBSFRCERR    V_RXCPRBSFRCERR(1U)
+
+#define S_TXCPRBSRST    17
+#define V_TXCPRBSRST(x) ((x) << S_TXCPRBSRST)
+#define F_TXCPRBSRST    V_TXCPRBSRST(1U)
+
+#define S_TXCPRBSEN    16
+#define V_TXCPRBSEN(x) ((x) << S_TXCPRBSEN)
+#define F_TXCPRBSEN    V_TXCPRBSEN(1U)
+
+#define S_RXBPRBSRST    12
+#define V_RXBPRBSRST(x) ((x) << S_RXBPRBSRST)
+#define F_RXBPRBSRST    V_RXBPRBSRST(1U)
+
+#define S_RXBPRBSEN    11
+#define V_RXBPRBSEN(x) ((x) << S_RXBPRBSEN)
+#define F_RXBPRBSEN    V_RXBPRBSEN(1U)
+
+#define S_RXBPRBSFRCERR    10
+#define V_RXBPRBSFRCERR(x) ((x) << S_RXBPRBSFRCERR)
+#define F_RXBPRBSFRCERR    V_RXBPRBSFRCERR(1U)
+
+#define S_TXBPRBSRST    9
+#define V_TXBPRBSRST(x) ((x) << S_TXBPRBSRST)
+#define F_TXBPRBSRST    V_TXBPRBSRST(1U)
+
+#define S_TXBPRBSEN    8
+#define V_TXBPRBSEN(x) ((x) << S_TXBPRBSEN)
+#define F_TXBPRBSEN    V_TXBPRBSEN(1U)
+
+#define S_RXAPRBSRST    4
+#define V_RXAPRBSRST(x) ((x) << S_RXAPRBSRST)
+#define F_RXAPRBSRST    V_RXAPRBSRST(1U)
+
+#define S_RXAPRBSEN    3
+#define V_RXAPRBSEN(x) ((x) << S_RXAPRBSEN)
+#define F_RXAPRBSEN    V_RXAPRBSEN(1U)
+
+#define S_RXAPRBSFRCERR    2
+#define V_RXAPRBSFRCERR(x) ((x) << S_RXAPRBSFRCERR)
+#define F_RXAPRBSFRCERR    V_RXAPRBSFRCERR(1U)
+
+#define S_TXAPRBSRST    1
+#define V_TXAPRBSRST(x) ((x) << S_TXAPRBSRST)
+#define F_TXAPRBSRST    V_TXAPRBSRST(1U)
+
+#define S_TXAPRBSEN    0
+#define V_TXAPRBSEN(x) ((x) << S_TXAPRBSEN)
+#define F_TXAPRBSEN    V_TXAPRBSEN(1U)
+
+#define A_XGMAC_PORT_HSS_CFG2 0x10e8
+
+#define S_RXDDATASYNC    23
+#define V_RXDDATASYNC(x) ((x) << S_RXDDATASYNC)
+#define F_RXDDATASYNC    V_RXDDATASYNC(1U)
+
+#define S_RXCDATASYNC    22
+#define V_RXCDATASYNC(x) ((x) << S_RXCDATASYNC)
+#define F_RXCDATASYNC    V_RXCDATASYNC(1U)
+
+#define S_RXBDATASYNC    21
+#define V_RXBDATASYNC(x) ((x) << S_RXBDATASYNC)
+#define F_RXBDATASYNC    V_RXBDATASYNC(1U)
+
+#define S_RXADATASYNC    20
+#define V_RXADATASYNC(x) ((x) << S_RXADATASYNC)
+#define F_RXADATASYNC    V_RXADATASYNC(1U)
+
+#define S_RXDEARLYIN    19
+#define V_RXDEARLYIN(x) ((x) << S_RXDEARLYIN)
+#define F_RXDEARLYIN    V_RXDEARLYIN(1U)
+
+#define S_RXDLATEIN    18
+#define V_RXDLATEIN(x) ((x) << S_RXDLATEIN)
+#define F_RXDLATEIN    V_RXDLATEIN(1U)
+
+#define S_RXDPHSLOCK    17
+#define V_RXDPHSLOCK(x) ((x) << S_RXDPHSLOCK)
+#define F_RXDPHSLOCK    V_RXDPHSLOCK(1U)
+
+#define S_RXDPHSDNIN    16
+#define V_RXDPHSDNIN(x) ((x) << S_RXDPHSDNIN)
+#define F_RXDPHSDNIN    V_RXDPHSDNIN(1U)
+
+#define S_RXDPHSUPIN    15
+#define V_RXDPHSUPIN(x) ((x) << S_RXDPHSUPIN)
+#define F_RXDPHSUPIN    V_RXDPHSUPIN(1U)
+
+#define S_RXCEARLYIN    14
+#define V_RXCEARLYIN(x) ((x) << S_RXCEARLYIN)
+#define F_RXCEARLYIN    V_RXCEARLYIN(1U)
+
+#define S_RXCLATEIN    13
+#define V_RXCLATEIN(x) ((x) << S_RXCLATEIN)
+#define F_RXCLATEIN    V_RXCLATEIN(1U)
+
+#define S_RXCPHSLOCK    12
+#define V_RXCPHSLOCK(x) ((x) << S_RXCPHSLOCK)
+#define F_RXCPHSLOCK    V_RXCPHSLOCK(1U)
+
+#define S_RXCPHSDNIN    11
+#define V_RXCPHSDNIN(x) ((x) << S_RXCPHSDNIN)
+#define F_RXCPHSDNIN    V_RXCPHSDNIN(1U)
+
+#define S_RXCPHSUPIN    10
+#define V_RXCPHSUPIN(x) ((x) << S_RXCPHSUPIN)
+#define F_RXCPHSUPIN    V_RXCPHSUPIN(1U)
+
+#define S_RXBEARLYIN    9
+#define V_RXBEARLYIN(x) ((x) << S_RXBEARLYIN)
+#define F_RXBEARLYIN    V_RXBEARLYIN(1U)
+
+#define S_RXBLATEIN    8
+#define V_RXBLATEIN(x) ((x) << S_RXBLATEIN)
+#define F_RXBLATEIN    V_RXBLATEIN(1U)
+
+#define S_RXBPHSLOCK    7
+#define V_RXBPHSLOCK(x) ((x) << S_RXBPHSLOCK)
+#define F_RXBPHSLOCK    V_RXBPHSLOCK(1U)
+
+#define S_RXBPHSDNIN    6
+#define V_RXBPHSDNIN(x) ((x) << S_RXBPHSDNIN)
+#define F_RXBPHSDNIN    V_RXBPHSDNIN(1U)
+
+#define S_RXBPHSUPIN    5
+#define V_RXBPHSUPIN(x) ((x) << S_RXBPHSUPIN)
+#define F_RXBPHSUPIN    V_RXBPHSUPIN(1U)
+
+#define S_RXAEARLYIN    4
+#define V_RXAEARLYIN(x) ((x) << S_RXAEARLYIN)
+#define F_RXAEARLYIN    V_RXAEARLYIN(1U)
+
+#define S_RXALATEIN    3
+#define V_RXALATEIN(x) ((x) << S_RXALATEIN)
+#define F_RXALATEIN    V_RXALATEIN(1U)
+
+#define S_RXAPHSLOCK    2
+#define V_RXAPHSLOCK(x) ((x) << S_RXAPHSLOCK)
+#define F_RXAPHSLOCK    V_RXAPHSLOCK(1U)
+
+#define S_RXAPHSDNIN    1
+#define V_RXAPHSDNIN(x) ((x) << S_RXAPHSDNIN)
+#define F_RXAPHSDNIN    V_RXAPHSDNIN(1U)
+
+#define S_RXAPHSUPIN    0
+#define V_RXAPHSUPIN(x) ((x) << S_RXAPHSUPIN)
+#define F_RXAPHSUPIN    V_RXAPHSUPIN(1U)
+
+#define A_XGMAC_PORT_HSS_STATUS 0x10ec
+
+#define S_RXDPRBSSYNC    15
+#define V_RXDPRBSSYNC(x) ((x) << S_RXDPRBSSYNC)
+#define F_RXDPRBSSYNC    V_RXDPRBSSYNC(1U)
+
+#define S_RXCPRBSSYNC    14
+#define V_RXCPRBSSYNC(x) ((x) << S_RXCPRBSSYNC)
+#define F_RXCPRBSSYNC    V_RXCPRBSSYNC(1U)
+
+#define S_RXBPRBSSYNC    13
+#define V_RXBPRBSSYNC(x) ((x) << S_RXBPRBSSYNC)
+#define F_RXBPRBSSYNC    V_RXBPRBSSYNC(1U)
+
+#define S_RXAPRBSSYNC    12
+#define V_RXAPRBSSYNC(x) ((x) << S_RXAPRBSSYNC)
+#define F_RXAPRBSSYNC    V_RXAPRBSSYNC(1U)
+
+#define S_RXDPRBSERR    11
+#define V_RXDPRBSERR(x) ((x) << S_RXDPRBSERR)
+#define F_RXDPRBSERR    V_RXDPRBSERR(1U)
+
+#define S_RXCPRBSERR    10
+#define V_RXCPRBSERR(x) ((x) << S_RXCPRBSERR)
+#define F_RXCPRBSERR    V_RXCPRBSERR(1U)
+
+#define S_RXBPRBSERR    9
+#define V_RXBPRBSERR(x) ((x) << S_RXBPRBSERR)
+#define F_RXBPRBSERR    V_RXBPRBSERR(1U)
+
+#define S_RXAPRBSERR    8
+#define V_RXAPRBSERR(x) ((x) << S_RXAPRBSERR)
+#define F_RXAPRBSERR    V_RXAPRBSERR(1U)
+
+#define S_RXDSIGDET    7
+#define V_RXDSIGDET(x) ((x) << S_RXDSIGDET)
+#define F_RXDSIGDET    V_RXDSIGDET(1U)
+
+#define S_RXCSIGDET    6
+#define V_RXCSIGDET(x) ((x) << S_RXCSIGDET)
+#define F_RXCSIGDET    V_RXCSIGDET(1U)
+
+#define S_RXBSIGDET    5
+#define V_RXBSIGDET(x) ((x) << S_RXBSIGDET)
+#define F_RXBSIGDET    V_RXBSIGDET(1U)
+
+#define S_RXASIGDET    4
+#define V_RXASIGDET(x) ((x) << S_RXASIGDET)
+#define F_RXASIGDET    V_RXASIGDET(1U)
+
+#define S_HSSPLLLOCK    1
+#define V_HSSPLLLOCK(x) ((x) << S_HSSPLLLOCK)
+#define F_HSSPLLLOCK    V_HSSPLLLOCK(1U)
+
+#define S_HSSPRTREADY    0
+#define V_HSSPRTREADY(x) ((x) << S_HSSPRTREADY)
+#define F_HSSPRTREADY    V_HSSPRTREADY(1U)
+
+#define A_XGMAC_PORT_XGM_TX_CTRL 0x1200
+
+#define S_SENDPAUSE    2
+#define V_SENDPAUSE(x) ((x) << S_SENDPAUSE)
+#define F_SENDPAUSE    V_SENDPAUSE(1U)
+
+#define S_SENDZEROPAUSE    1
+#define V_SENDZEROPAUSE(x) ((x) << S_SENDZEROPAUSE)
+#define F_SENDZEROPAUSE    V_SENDZEROPAUSE(1U)
+
+#define S_XGM_TXEN    0
+#define V_XGM_TXEN(x) ((x) << S_XGM_TXEN)
+#define F_XGM_TXEN    V_XGM_TXEN(1U)
+
+#define A_XGMAC_PORT_XGM_TX_CFG 0x1204
+
+#define S_CRCCAL    8
+#define M_CRCCAL    0x3U
+#define V_CRCCAL(x) ((x) << S_CRCCAL)
+#define G_CRCCAL(x) (((x) >> S_CRCCAL) & M_CRCCAL)
+
+#define S_DISDEFIDLECNT    7
+#define V_DISDEFIDLECNT(x) ((x) << S_DISDEFIDLECNT)
+#define F_DISDEFIDLECNT    V_DISDEFIDLECNT(1U)
+
+#define S_DECAVGTXIPG    6
+#define V_DECAVGTXIPG(x) ((x) << S_DECAVGTXIPG)
+#define F_DECAVGTXIPG    V_DECAVGTXIPG(1U)
+
+#define S_UNIDIRTXEN    5
+#define V_UNIDIRTXEN(x) ((x) << S_UNIDIRTXEN)
+#define F_UNIDIRTXEN    V_UNIDIRTXEN(1U)
+
+#define S_CFGCLKSPEED    2
+#define M_CFGCLKSPEED    0x7U
+#define V_CFGCLKSPEED(x) ((x) << S_CFGCLKSPEED)
+#define G_CFGCLKSPEED(x) (((x) >> S_CFGCLKSPEED) & M_CFGCLKSPEED)
+
+#define S_STRETCHMODE    1
+#define V_STRETCHMODE(x) ((x) << S_STRETCHMODE)
+#define F_STRETCHMODE    V_STRETCHMODE(1U)
+
+#define S_TXPAUSEEN    0
+#define V_TXPAUSEEN(x) ((x) << S_TXPAUSEEN)
+#define F_TXPAUSEEN    V_TXPAUSEEN(1U)
+
+#define A_XGMAC_PORT_XGM_TX_PAUSE_QUANTA 0x1208
+
+#define S_TXPAUSEQUANTA    0
+#define M_TXPAUSEQUANTA    0xffffU
+#define V_TXPAUSEQUANTA(x) ((x) << S_TXPAUSEQUANTA)
+#define G_TXPAUSEQUANTA(x) (((x) >> S_TXPAUSEQUANTA) & M_TXPAUSEQUANTA)
+
+#define A_XGMAC_PORT_XGM_RX_CTRL 0x120c
+#define A_XGMAC_PORT_XGM_RX_CFG 0x1210
+
+#define S_RXCRCCAL    16
+#define M_RXCRCCAL    0x3U
+#define V_RXCRCCAL(x) ((x) << S_RXCRCCAL)
+#define G_RXCRCCAL(x) (((x) >> S_RXCRCCAL) & M_RXCRCCAL)
+
+#define S_STATLOCALFAULT    15
+#define V_STATLOCALFAULT(x) ((x) << S_STATLOCALFAULT)
+#define F_STATLOCALFAULT    V_STATLOCALFAULT(1U)
+
+#define S_STATREMOTEFAULT    14
+#define V_STATREMOTEFAULT(x) ((x) << S_STATREMOTEFAULT)
+#define F_STATREMOTEFAULT    V_STATREMOTEFAULT(1U)
+
+#define S_LENERRFRAMEDIS    13
+#define V_LENERRFRAMEDIS(x) ((x) << S_LENERRFRAMEDIS)
+#define F_LENERRFRAMEDIS    V_LENERRFRAMEDIS(1U)
+
+#define S_CON802_3PREAMBLE    12
+#define V_CON802_3PREAMBLE(x) ((x) << S_CON802_3PREAMBLE)
+#define F_CON802_3PREAMBLE    V_CON802_3PREAMBLE(1U)
+
+#define S_ENNON802_3PREAMBLE    11
+#define V_ENNON802_3PREAMBLE(x) ((x) << S_ENNON802_3PREAMBLE)
+#define F_ENNON802_3PREAMBLE    V_ENNON802_3PREAMBLE(1U)
+
+#define S_COPYPREAMBLE    10
+#define V_COPYPREAMBLE(x) ((x) << S_COPYPREAMBLE)
+#define F_COPYPREAMBLE    V_COPYPREAMBLE(1U)
+
+#define S_DISPAUSEFRAMES    9
+#define V_DISPAUSEFRAMES(x) ((x) << S_DISPAUSEFRAMES)
+#define F_DISPAUSEFRAMES    V_DISPAUSEFRAMES(1U)
+
+#define S_EN1536BFRAMES    8
+#define V_EN1536BFRAMES(x) ((x) << S_EN1536BFRAMES)
+#define F_EN1536BFRAMES    V_EN1536BFRAMES(1U)
+
+#define S_ENJUMBO    7
+#define V_ENJUMBO(x) ((x) << S_ENJUMBO)
+#define F_ENJUMBO    V_ENJUMBO(1U)
+
+#define S_RMFCS    6
+#define V_RMFCS(x) ((x) << S_RMFCS)
+#define F_RMFCS    V_RMFCS(1U)
+
+#define S_DISNONVLAN    5
+#define V_DISNONVLAN(x) ((x) << S_DISNONVLAN)
+#define F_DISNONVLAN    V_DISNONVLAN(1U)
+
+#define S_ENEXTMATCH    4
+#define V_ENEXTMATCH(x) ((x) << S_ENEXTMATCH)
+#define F_ENEXTMATCH    V_ENEXTMATCH(1U)
+
+#define S_ENHASHUCAST    3
+#define V_ENHASHUCAST(x) ((x) << S_ENHASHUCAST)
+#define F_ENHASHUCAST    V_ENHASHUCAST(1U)
+
+#define S_ENHASHMCAST    2
+#define V_ENHASHMCAST(x) ((x) << S_ENHASHMCAST)
+#define F_ENHASHMCAST    V_ENHASHMCAST(1U)
+
+#define S_DISBCAST    1
+#define V_DISBCAST(x) ((x) << S_DISBCAST)
+#define F_DISBCAST    V_DISBCAST(1U)
+
+#define S_COPYALLFRAMES    0
+#define V_COPYALLFRAMES(x) ((x) << S_COPYALLFRAMES)
+#define F_COPYALLFRAMES    V_COPYALLFRAMES(1U)
+
+#define A_XGMAC_PORT_XGM_RX_HASH_LOW 0x1214
+#define A_XGMAC_PORT_XGM_RX_HASH_HIGH 0x1218
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_1 0x121c
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_1 0x1220
+
+#define S_ADDRESS_HIGH    0
+#define M_ADDRESS_HIGH    0xffffU
+#define V_ADDRESS_HIGH(x) ((x) << S_ADDRESS_HIGH)
+#define G_ADDRESS_HIGH(x) (((x) >> S_ADDRESS_HIGH) & M_ADDRESS_HIGH)
+
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_2 0x1224
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_2 0x1228
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_3 0x122c
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_3 0x1230
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_4 0x1234
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_4 0x1238
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_5 0x123c
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_5 0x1240
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_6 0x1244
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_6 0x1248
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_7 0x124c
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_7 0x1250
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_LOW_8 0x1254
+#define A_XGMAC_PORT_XGM_RX_EXACT_MATCH_HIGH_8 0x1258
+#define A_XGMAC_PORT_XGM_RX_TYPE_MATCH_1 0x125c
+
+#define S_ENTYPEMATCH    31
+#define V_ENTYPEMATCH(x) ((x) << S_ENTYPEMATCH)
+#define F_ENTYPEMATCH    V_ENTYPEMATCH(1U)
+
+#define S_TYPE    0
+#define M_TYPE    0xffffU
+#define V_TYPE(x) ((x) << S_TYPE)
+#define G_TYPE(x) (((x) >> S_TYPE) & M_TYPE)
+
+#define A_XGMAC_PORT_XGM_RX_TYPE_MATCH_2 0x1260
+#define A_XGMAC_PORT_XGM_RX_TYPE_MATCH_3 0x1264
+#define A_XGMAC_PORT_XGM_RX_TYPE_MATCH_4 0x1268
+#define A_XGMAC_PORT_XGM_INT_STATUS 0x126c
+
+#define S_XGMIIEXTINT    10
+#define V_XGMIIEXTINT(x) ((x) << S_XGMIIEXTINT)
+#define F_XGMIIEXTINT    V_XGMIIEXTINT(1U)
+
+#define S_LINKFAULTCHANGE    9
+#define V_LINKFAULTCHANGE(x) ((x) << S_LINKFAULTCHANGE)
+#define F_LINKFAULTCHANGE    V_LINKFAULTCHANGE(1U)
+
+#define S_PHYFRAMECOMPLETE    8
+#define V_PHYFRAMECOMPLETE(x) ((x) << S_PHYFRAMECOMPLETE)
+#define F_PHYFRAMECOMPLETE    V_PHYFRAMECOMPLETE(1U)
+
+#define S_PAUSEFRAMETXMT    7
+#define V_PAUSEFRAMETXMT(x) ((x) << S_PAUSEFRAMETXMT)
+#define F_PAUSEFRAMETXMT    V_PAUSEFRAMETXMT(1U)
+
+#define S_PAUSECNTRTIMEOUT    6
+#define V_PAUSECNTRTIMEOUT(x) ((x) << S_PAUSECNTRTIMEOUT)
+#define F_PAUSECNTRTIMEOUT    V_PAUSECNTRTIMEOUT(1U)
+
+#define S_NON0PAUSERCVD    5
+#define V_NON0PAUSERCVD(x) ((x) << S_NON0PAUSERCVD)
+#define F_NON0PAUSERCVD    V_NON0PAUSERCVD(1U)
+
+#define S_STATOFLOW    4
+#define V_STATOFLOW(x) ((x) << S_STATOFLOW)
+#define F_STATOFLOW    V_STATOFLOW(1U)
+
+#define S_TXERRFIFO    3
+#define V_TXERRFIFO(x) ((x) << S_TXERRFIFO)
+#define F_TXERRFIFO    V_TXERRFIFO(1U)
+
+#define S_TXUFLOW    2
+#define V_TXUFLOW(x) ((x) << S_TXUFLOW)
+#define F_TXUFLOW    V_TXUFLOW(1U)
+
+#define S_FRAMETXMT    1
+#define V_FRAMETXMT(x) ((x) << S_FRAMETXMT)
+#define F_FRAMETXMT    V_FRAMETXMT(1U)
+
+#define S_FRAMERCVD    0
+#define V_FRAMERCVD(x) ((x) << S_FRAMERCVD)
+#define F_FRAMERCVD    V_FRAMERCVD(1U)
+
+#define A_XGMAC_PORT_XGM_INT_MASK 0x1270
+#define A_XGMAC_PORT_XGM_INT_EN 0x1274
+#define A_XGMAC_PORT_XGM_INT_DISABLE 0x1278
+#define A_XGMAC_PORT_XGM_TX_PAUSE_TIMER 0x127c
+
+#define S_CURPAUSETIMER    0
+#define M_CURPAUSETIMER    0xffffU
+#define V_CURPAUSETIMER(x) ((x) << S_CURPAUSETIMER)
+#define G_CURPAUSETIMER(x) (((x) >> S_CURPAUSETIMER) & M_CURPAUSETIMER)
+
+#define A_XGMAC_PORT_XGM_STAT_CTRL 0x1280
+
+#define S_READSNPSHOT    4
+#define V_READSNPSHOT(x) ((x) << S_READSNPSHOT)
+#define F_READSNPSHOT    V_READSNPSHOT(1U)
+
+#define S_TAKESNPSHOT    3
+#define V_TAKESNPSHOT(x) ((x) << S_TAKESNPSHOT)
+#define F_TAKESNPSHOT    V_TAKESNPSHOT(1U)
+
+#define S_CLRSTATS    2
+#define V_CLRSTATS(x) ((x) << S_CLRSTATS)
+#define F_CLRSTATS    V_CLRSTATS(1U)
+
+#define S_INCRSTATS    1
+#define V_INCRSTATS(x) ((x) << S_INCRSTATS)
+#define F_INCRSTATS    V_INCRSTATS(1U)
+
+#define S_ENTESTMODEWR    0
+#define V_ENTESTMODEWR(x) ((x) << S_ENTESTMODEWR)
+#define F_ENTESTMODEWR    V_ENTESTMODEWR(1U)
+
+#define A_XGMAC_PORT_XGM_MDIO_CTRL 0x1284
+
+#define S_FRAMETYPE    30
+#define M_FRAMETYPE    0x3U
+#define V_FRAMETYPE(x) ((x) << S_FRAMETYPE)
+#define G_FRAMETYPE(x) (((x) >> S_FRAMETYPE) & M_FRAMETYPE)
+
+#define S_OPERATION    28
+#define M_OPERATION    0x3U
+#define V_OPERATION(x) ((x) << S_OPERATION)
+#define G_OPERATION(x) (((x) >> S_OPERATION) & M_OPERATION)
+
+#define S_PORTADDR    23
+#define M_PORTADDR    0x1fU
+#define V_PORTADDR(x) ((x) << S_PORTADDR)
+#define G_PORTADDR(x) (((x) >> S_PORTADDR) & M_PORTADDR)
+
+#define S_DEVADDR    18
+#define M_DEVADDR    0x1fU
+#define V_DEVADDR(x) ((x) << S_DEVADDR)
+#define G_DEVADDR(x) (((x) >> S_DEVADDR) & M_DEVADDR)
+
+#define S_RESRV    16
+#define M_RESRV    0x3U
+#define V_RESRV(x) ((x) << S_RESRV)
+#define G_RESRV(x) (((x) >> S_RESRV) & M_RESRV)
+
+#define S_DATA    0
+#define M_DATA    0xffffU
+#define V_DATA(x) ((x) << S_DATA)
+#define G_DATA(x) (((x) >> S_DATA) & M_DATA)
+
+#define A_XGMAC_PORT_XGM_MODULE_ID 0x12fc
+
+#define S_MODULEID    16
+#define M_MODULEID    0xffffU
+#define V_MODULEID(x) ((x) << S_MODULEID)
+#define G_MODULEID(x) (((x) >> S_MODULEID) & M_MODULEID)
+
+#define S_MODULEREV    0
+#define M_MODULEREV    0xffffU
+#define V_MODULEREV(x) ((x) << S_MODULEREV)
+#define G_MODULEREV(x) (((x) >> S_MODULEREV) & M_MODULEREV)
+
+#define A_XGMAC_PORT_XGM_STAT_TX_BYTE_LOW 0x1300
+#define A_XGMAC_PORT_XGM_STAT_TX_BYTE_HIGH 0x1304
+
+#define S_TXBYTES_HIGH    0
+#define M_TXBYTES_HIGH    0x1fffU
+#define V_TXBYTES_HIGH(x) ((x) << S_TXBYTES_HIGH)
+#define G_TXBYTES_HIGH(x) (((x) >> S_TXBYTES_HIGH) & M_TXBYTES_HIGH)
+
+#define A_XGMAC_PORT_XGM_STAT_TX_FRAME_LOW 0x1308
+#define A_XGMAC_PORT_XGM_STAT_TX_FRAME_HIGH 0x130c
+
+#define S_TXFRAMES_HIGH    0
+#define M_TXFRAMES_HIGH    0xfU
+#define V_TXFRAMES_HIGH(x) ((x) << S_TXFRAMES_HIGH)
+#define G_TXFRAMES_HIGH(x) (((x) >> S_TXFRAMES_HIGH) & M_TXFRAMES_HIGH)
+
+#define A_XGMAC_PORT_XGM_STAT_TX_BCAST 0x1310
+#define A_XGMAC_PORT_XGM_STAT_TX_MCAST 0x1314
+#define A_XGMAC_PORT_XGM_STAT_TX_PAUSE 0x1318
+#define A_XGMAC_PORT_XGM_STAT_TX_64B_FRAMES 0x131c
+#define A_XGMAC_PORT_XGM_STAT_TX_65_127B_FRAMES 0x1320
+#define A_XGMAC_PORT_XGM_STAT_TX_128_255B_FRAMES 0x1324
+#define A_XGMAC_PORT_XGM_STAT_TX_256_511B_FRAMES 0x1328
+#define A_XGMAC_PORT_XGM_STAT_TX_512_1023B_FRAMES 0x132c
+#define A_XGMAC_PORT_XGM_STAT_TX_1024_1518B_FRAMES 0x1330
+#define A_XGMAC_PORT_XGM_STAT_TX_1519_MAXB_FRAMES 0x1334
+#define A_XGMAC_PORT_XGM_STAT_TX_ERR_FRAMES 0x1338
+#define A_XGMAC_PORT_XGM_STAT_RX_BYTES_LOW 0x133c
+#define A_XGMAC_PORT_XGM_STAT_RX_BYTES_HIGH 0x1340
+
+#define S_RXBYTES_HIGH    0
+#define M_RXBYTES_HIGH    0x1fffU
+#define V_RXBYTES_HIGH(x) ((x) << S_RXBYTES_HIGH)
+#define G_RXBYTES_HIGH(x) (((x) >> S_RXBYTES_HIGH) & M_RXBYTES_HIGH)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_FRAMES_LOW 0x1344
+#define A_XGMAC_PORT_XGM_STAT_RX_FRAMES_HIGH 0x1348
+
+#define S_RXFRAMES_HIGH    0
+#define M_RXFRAMES_HIGH    0xfU
+#define V_RXFRAMES_HIGH(x) ((x) << S_RXFRAMES_HIGH)
+#define G_RXFRAMES_HIGH(x) (((x) >> S_RXFRAMES_HIGH) & M_RXFRAMES_HIGH)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_BCAST_FRAMES 0x134c
+#define A_XGMAC_PORT_XGM_STAT_RX_MCAST_FRAMES 0x1350
+#define A_XGMAC_PORT_XGM_STAT_RX_PAUSE_FRAMES 0x1354
+
+#define S_RXPAUSEFRAMES    0
+#define M_RXPAUSEFRAMES    0xffffU
+#define V_RXPAUSEFRAMES(x) ((x) << S_RXPAUSEFRAMES)
+#define G_RXPAUSEFRAMES(x) (((x) >> S_RXPAUSEFRAMES) & M_RXPAUSEFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_64B_FRAMES 0x1358
+#define A_XGMAC_PORT_XGM_STAT_RX_65_127B_FRAMES 0x135c
+#define A_XGMAC_PORT_XGM_STAT_RX_128_255B_FRAMES 0x1360
+#define A_XGMAC_PORT_XGM_STAT_RX_256_511B_FRAMES 0x1364
+#define A_XGMAC_PORT_XGM_STAT_RX_512_1023B_FRAMES 0x1368
+#define A_XGMAC_PORT_XGM_STAT_RX_1024_1518B_FRAMES 0x136c
+#define A_XGMAC_PORT_XGM_STAT_RX_1519_MAXB_FRAMES 0x1370
+#define A_XGMAC_PORT_XGM_STAT_RX_SHORT_FRAMES 0x1374
+
+#define S_RXSHORTFRAMES    0
+#define M_RXSHORTFRAMES    0xffffU
+#define V_RXSHORTFRAMES(x) ((x) << S_RXSHORTFRAMES)
+#define G_RXSHORTFRAMES(x) (((x) >> S_RXSHORTFRAMES) & M_RXSHORTFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_OVERSIZE_FRAMES 0x1378
+
+#define S_RXOVERSIZEFRAMES    0
+#define M_RXOVERSIZEFRAMES    0xffffU
+#define V_RXOVERSIZEFRAMES(x) ((x) << S_RXOVERSIZEFRAMES)
+#define G_RXOVERSIZEFRAMES(x) (((x) >> S_RXOVERSIZEFRAMES) & M_RXOVERSIZEFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_JABBER_FRAMES 0x137c
+
+#define S_RXJABBERFRAMES    0
+#define M_RXJABBERFRAMES    0xffffU
+#define V_RXJABBERFRAMES(x) ((x) << S_RXJABBERFRAMES)
+#define G_RXJABBERFRAMES(x) (((x) >> S_RXJABBERFRAMES) & M_RXJABBERFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_CRC_ERR_FRAMES 0x1380
+
+#define S_RXCRCERRFRAMES    0
+#define M_RXCRCERRFRAMES    0xffffU
+#define V_RXCRCERRFRAMES(x) ((x) << S_RXCRCERRFRAMES)
+#define G_RXCRCERRFRAMES(x) (((x) >> S_RXCRCERRFRAMES) & M_RXCRCERRFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_LENGTH_ERR_FRAMES 0x1384
+
+#define S_RXLENGTHERRFRAMES    0
+#define M_RXLENGTHERRFRAMES    0xffffU
+#define V_RXLENGTHERRFRAMES(x) ((x) << S_RXLENGTHERRFRAMES)
+#define G_RXLENGTHERRFRAMES(x) (((x) >> S_RXLENGTHERRFRAMES) & M_RXLENGTHERRFRAMES)
+
+#define A_XGMAC_PORT_XGM_STAT_RX_SYM_CODE_ERR_FRAMES 0x1388
+
+#define S_RXSYMCODEERRFRAMES    0
+#define M_RXSYMCODEERRFRAMES    0xffffU
+#define V_RXSYMCODEERRFRAMES(x) ((x) << S_RXSYMCODEERRFRAMES)
+#define G_RXSYMCODEERRFRAMES(x) (((x) >> S_RXSYMCODEERRFRAMES) & M_RXSYMCODEERRFRAMES)
+
+#define A_XGMAC_PORT_XAUI_CTRL 0x1400
+
+#define S_POLARITY_INV_RX    8
+#define M_POLARITY_INV_RX    0xfU
+#define V_POLARITY_INV_RX(x) ((x) << S_POLARITY_INV_RX)
+#define G_POLARITY_INV_RX(x) (((x) >> S_POLARITY_INV_RX) & M_POLARITY_INV_RX)
+
+#define S_POLARITY_INV_TX    4
+#define M_POLARITY_INV_TX    0xfU
+#define V_POLARITY_INV_TX(x) ((x) << S_POLARITY_INV_TX)
+#define G_POLARITY_INV_TX(x) (((x) >> S_POLARITY_INV_TX) & M_POLARITY_INV_TX)
+
+#define S_TEST_SEL    2
+#define M_TEST_SEL    0x3U
+#define V_TEST_SEL(x) ((x) << S_TEST_SEL)
+#define G_TEST_SEL(x) (((x) >> S_TEST_SEL) & M_TEST_SEL)
+
+#define S_TEST_EN    0
+#define V_TEST_EN(x) ((x) << S_TEST_EN)
+#define F_TEST_EN    V_TEST_EN(1U)
+
+#define A_XGMAC_PORT_XAUI_STATUS 0x1404
+
+#define S_DECODE_ERROR    12
+#define M_DECODE_ERROR    0xffU
+#define V_DECODE_ERROR(x) ((x) << S_DECODE_ERROR)
+#define G_DECODE_ERROR(x) (((x) >> S_DECODE_ERROR) & M_DECODE_ERROR)
+
+#define S_LANE3_CTC_STATUS    11
+#define V_LANE3_CTC_STATUS(x) ((x) << S_LANE3_CTC_STATUS)
+#define F_LANE3_CTC_STATUS    V_LANE3_CTC_STATUS(1U)
+
+#define S_LANE2_CTC_STATUS    10
+#define V_LANE2_CTC_STATUS(x) ((x) << S_LANE2_CTC_STATUS)
+#define F_LANE2_CTC_STATUS    V_LANE2_CTC_STATUS(1U)
+
+#define S_LANE1_CTC_STATUS    9
+#define V_LANE1_CTC_STATUS(x) ((x) << S_LANE1_CTC_STATUS)
+#define F_LANE1_CTC_STATUS    V_LANE1_CTC_STATUS(1U)
+
+#define S_LANE0_CTC_STATUS    8
+#define V_LANE0_CTC_STATUS(x) ((x) << S_LANE0_CTC_STATUS)
+#define F_LANE0_CTC_STATUS    V_LANE0_CTC_STATUS(1U)
+
+#define S_ALIGN_STATUS    4
+#define V_ALIGN_STATUS(x) ((x) << S_ALIGN_STATUS)
+#define F_ALIGN_STATUS    V_ALIGN_STATUS(1U)
+
+#define S_LANE3_SYNC_STATUS    3
+#define V_LANE3_SYNC_STATUS(x) ((x) << S_LANE3_SYNC_STATUS)
+#define F_LANE3_SYNC_STATUS    V_LANE3_SYNC_STATUS(1U)
+
+#define S_LANE2_SYNC_STATUS    2
+#define V_LANE2_SYNC_STATUS(x) ((x) << S_LANE2_SYNC_STATUS)
+#define F_LANE2_SYNC_STATUS    V_LANE2_SYNC_STATUS(1U)
+
+#define S_LANE1_SYNC_STATUS    1
+#define V_LANE1_SYNC_STATUS(x) ((x) << S_LANE1_SYNC_STATUS)
+#define F_LANE1_SYNC_STATUS    V_LANE1_SYNC_STATUS(1U)
+
+#define S_LANE0_SYNC_STATUS    0
+#define V_LANE0_SYNC_STATUS(x) ((x) << S_LANE0_SYNC_STATUS)
+#define F_LANE0_SYNC_STATUS    V_LANE0_SYNC_STATUS(1U)
+
+#define A_XGMAC_PORT_PCSR_CTRL 0x1500
+
+#define S_RX_CLK_SPEED    7
+#define V_RX_CLK_SPEED(x) ((x) << S_RX_CLK_SPEED)
+#define F_RX_CLK_SPEED    V_RX_CLK_SPEED(1U)
+
+#define S_SCRBYPASS    6
+#define V_SCRBYPASS(x) ((x) << S_SCRBYPASS)
+#define F_SCRBYPASS    V_SCRBYPASS(1U)
+
+#define S_FECERRINDEN    5
+#define V_FECERRINDEN(x) ((x) << S_FECERRINDEN)
+#define F_FECERRINDEN    V_FECERRINDEN(1U)
+
+#define S_FECEN    4
+#define V_FECEN(x) ((x) << S_FECEN)
+#define F_FECEN    V_FECEN(1U)
+
+#define S_TESTSEL    2
+#define M_TESTSEL    0x3U
+#define V_TESTSEL(x) ((x) << S_TESTSEL)
+#define G_TESTSEL(x) (((x) >> S_TESTSEL) & M_TESTSEL)
+
+#define S_SCRLOOPEN    1
+#define V_SCRLOOPEN(x) ((x) << S_SCRLOOPEN)
+#define F_SCRLOOPEN    V_SCRLOOPEN(1U)
+
+#define S_XGMIILOOPEN    0
+#define V_XGMIILOOPEN(x) ((x) << S_XGMIILOOPEN)
+#define F_XGMIILOOPEN    V_XGMIILOOPEN(1U)
+
+#define A_XGMAC_PORT_PCSR_TXTEST_CTRL 0x1510
+
+#define S_TX_PRBS9_EN    4
+#define V_TX_PRBS9_EN(x) ((x) << S_TX_PRBS9_EN)
+#define F_TX_PRBS9_EN    V_TX_PRBS9_EN(1U)
+
+#define S_TX_PRBS31_EN    3
+#define V_TX_PRBS31_EN(x) ((x) << S_TX_PRBS31_EN)
+#define F_TX_PRBS31_EN    V_TX_PRBS31_EN(1U)
+
+#define S_TX_TST_DAT_SEL    2
+#define V_TX_TST_DAT_SEL(x) ((x) << S_TX_TST_DAT_SEL)
+#define F_TX_TST_DAT_SEL    V_TX_TST_DAT_SEL(1U)
+
+#define S_TX_TST_SEL    1
+#define V_TX_TST_SEL(x) ((x) << S_TX_TST_SEL)
+#define F_TX_TST_SEL    V_TX_TST_SEL(1U)
+
+#define S_TX_TST_EN    0
+#define V_TX_TST_EN(x) ((x) << S_TX_TST_EN)
+#define F_TX_TST_EN    V_TX_TST_EN(1U)
+
+#define A_XGMAC_PORT_PCSR_TXTEST_SEEDA_LOWER 0x1514
+#define A_XGMAC_PORT_PCSR_TXTEST_SEEDA_UPPER 0x1518
+
+#define S_SEEDA_UPPER    0
+#define M_SEEDA_UPPER    0x3ffffffU
+#define V_SEEDA_UPPER(x) ((x) << S_SEEDA_UPPER)
+#define G_SEEDA_UPPER(x) (((x) >> S_SEEDA_UPPER) & M_SEEDA_UPPER)
+
+#define A_XGMAC_PORT_PCSR_TXTEST_SEEDB_LOWER 0x152c
+#define A_XGMAC_PORT_PCSR_TXTEST_SEEDB_UPPER 0x1530
+
+#define S_SEEDB_UPPER    0
+#define M_SEEDB_UPPER    0x3ffffffU
+#define V_SEEDB_UPPER(x) ((x) << S_SEEDB_UPPER)
+#define G_SEEDB_UPPER(x) (((x) >> S_SEEDB_UPPER) & M_SEEDB_UPPER)
+
+#define A_XGMAC_PORT_PCSR_RXTEST_CTRL 0x153c
+
+#define S_TPTER_CNT_RST    7
+#define V_TPTER_CNT_RST(x) ((x) << S_TPTER_CNT_RST)
+#define F_TPTER_CNT_RST    V_TPTER_CNT_RST(1U)
+
+#define S_TEST_CNT_125US    6
+#define V_TEST_CNT_125US(x) ((x) << S_TEST_CNT_125US)
+#define F_TEST_CNT_125US    V_TEST_CNT_125US(1U)
+
+#define S_TEST_CNT_PRE    5
+#define V_TEST_CNT_PRE(x) ((x) << S_TEST_CNT_PRE)
+#define F_TEST_CNT_PRE    V_TEST_CNT_PRE(1U)
+
+#define S_BER_CNT_RST    4
+#define V_BER_CNT_RST(x) ((x) << S_BER_CNT_RST)
+#define F_BER_CNT_RST    V_BER_CNT_RST(1U)
+
+#define S_ERR_BLK_CNT_RST    3
+#define V_ERR_BLK_CNT_RST(x) ((x) << S_ERR_BLK_CNT_RST)
+#define F_ERR_BLK_CNT_RST    V_ERR_BLK_CNT_RST(1U)
+
+#define S_RX_PRBS31_EN    2
+#define V_RX_PRBS31_EN(x) ((x) << S_RX_PRBS31_EN)
+#define F_RX_PRBS31_EN    V_RX_PRBS31_EN(1U)
+
+#define S_RX_TST_DAT_SEL    1
+#define V_RX_TST_DAT_SEL(x) ((x) << S_RX_TST_DAT_SEL)
+#define F_RX_TST_DAT_SEL    V_RX_TST_DAT_SEL(1U)
+
+#define S_RX_TST_EN    0
+#define V_RX_TST_EN(x) ((x) << S_RX_TST_EN)
+#define F_RX_TST_EN    V_RX_TST_EN(1U)
+
+#define A_XGMAC_PORT_PCSR_STATUS 0x1550
+
+#define S_ERR_BLK_CNT    16
+#define M_ERR_BLK_CNT    0xffU
+#define V_ERR_BLK_CNT(x) ((x) << S_ERR_BLK_CNT)
+#define G_ERR_BLK_CNT(x) (((x) >> S_ERR_BLK_CNT) & M_ERR_BLK_CNT)
+
+#define S_BER_COUNT    8
+#define M_BER_COUNT    0x3fU
+#define V_BER_COUNT(x) ((x) << S_BER_COUNT)
+#define G_BER_COUNT(x) (((x) >> S_BER_COUNT) & M_BER_COUNT)
+
+#define S_HI_BER    2
+#define V_HI_BER(x) ((x) << S_HI_BER)
+#define F_HI_BER    V_HI_BER(1U)
+
+#define S_RX_FAULT    1
+#define V_RX_FAULT(x) ((x) << S_RX_FAULT)
+#define F_RX_FAULT    V_RX_FAULT(1U)
+
+#define S_TX_FAULT    0
+#define V_TX_FAULT(x) ((x) << S_TX_FAULT)
+#define F_TX_FAULT    V_TX_FAULT(1U)
+
+#define A_XGMAC_PORT_PCSR_TEST_STATUS 0x1554
+
+#define S_TPT_ERR_CNT    0
+#define M_TPT_ERR_CNT    0xffffU
+#define V_TPT_ERR_CNT(x) ((x) << S_TPT_ERR_CNT)
+#define G_TPT_ERR_CNT(x) (((x) >> S_TPT_ERR_CNT) & M_TPT_ERR_CNT)
+
+#define A_XGMAC_PORT_AN_CONTROL 0x1600
+
+#define S_SOFT_RESET    15
+#define V_SOFT_RESET(x) ((x) << S_SOFT_RESET)
+#define F_SOFT_RESET    V_SOFT_RESET(1U)
+
+#define S_AN_ENABLE    12
+#define V_AN_ENABLE(x) ((x) << S_AN_ENABLE)
+#define F_AN_ENABLE    V_AN_ENABLE(1U)
+
+#define S_RESTART_AN    9
+#define V_RESTART_AN(x) ((x) << S_RESTART_AN)
+#define F_RESTART_AN    V_RESTART_AN(1U)
+
+#define A_XGMAC_PORT_AN_STATUS 0x1604
+
+#define S_NONCER_MATCH    31
+#define V_NONCER_MATCH(x) ((x) << S_NONCER_MATCH)
+#define F_NONCER_MATCH    V_NONCER_MATCH(1U)
+
+#define S_PARALLEL_DET_FAULT    9
+#define V_PARALLEL_DET_FAULT(x) ((x) << S_PARALLEL_DET_FAULT)
+#define F_PARALLEL_DET_FAULT    V_PARALLEL_DET_FAULT(1U)
+
+#define S_PAGE_RECEIVED    6
+#define V_PAGE_RECEIVED(x) ((x) << S_PAGE_RECEIVED)
+#define F_PAGE_RECEIVED    V_PAGE_RECEIVED(1U)
+
+#define S_AN_COMPLETE    5
+#define V_AN_COMPLETE(x) ((x) << S_AN_COMPLETE)
+#define F_AN_COMPLETE    V_AN_COMPLETE(1U)
+
+#define S_STAT_REMFAULT    4
+#define V_STAT_REMFAULT(x) ((x) << S_STAT_REMFAULT)
+#define F_STAT_REMFAULT    V_STAT_REMFAULT(1U)
+
+#define S_AN_ABILITY    3
+#define V_AN_ABILITY(x) ((x) << S_AN_ABILITY)
+#define F_AN_ABILITY    V_AN_ABILITY(1U)
+
+#define S_LINK_STATUS    2
+#define V_LINK_STATUS(x) ((x) << S_LINK_STATUS)
+#define F_LINK_STATUS    V_LINK_STATUS(1U)
+
+#define S_PARTNER_AN_ABILITY    0
+#define V_PARTNER_AN_ABILITY(x) ((x) << S_PARTNER_AN_ABILITY)
+#define F_PARTNER_AN_ABILITY    V_PARTNER_AN_ABILITY(1U)
+
+#define A_XGMAC_PORT_AN_ADVERTISEMENT 0x1608
+
+#define S_FEC_ENABLE    31
+#define V_FEC_ENABLE(x) ((x) << S_FEC_ENABLE)
+#define F_FEC_ENABLE    V_FEC_ENABLE(1U)
+
+#define S_FEC_ABILITY    30
+#define V_FEC_ABILITY(x) ((x) << S_FEC_ABILITY)
+#define F_FEC_ABILITY    V_FEC_ABILITY(1U)
+
+#define S_10GBASE_KR_CAPABLE    23
+#define V_10GBASE_KR_CAPABLE(x) ((x) << S_10GBASE_KR_CAPABLE)
+#define F_10GBASE_KR_CAPABLE    V_10GBASE_KR_CAPABLE(1U)
+
+#define S_10GBASE_KX4_CAPABLE    22
+#define V_10GBASE_KX4_CAPABLE(x) ((x) << S_10GBASE_KX4_CAPABLE)
+#define F_10GBASE_KX4_CAPABLE    V_10GBASE_KX4_CAPABLE(1U)
+
+#define S_1000BASE_KX_CAPABLE    21
+#define V_1000BASE_KX_CAPABLE(x) ((x) << S_1000BASE_KX_CAPABLE)
+#define F_1000BASE_KX_CAPABLE    V_1000BASE_KX_CAPABLE(1U)
+
+#define S_TRANSMITTED_NONCE    16
+#define M_TRANSMITTED_NONCE    0x1fU
+#define V_TRANSMITTED_NONCE(x) ((x) << S_TRANSMITTED_NONCE)
+#define G_TRANSMITTED_NONCE(x) (((x) >> S_TRANSMITTED_NONCE) & M_TRANSMITTED_NONCE)
+
+#define S_NP    15
+#define V_NP(x) ((x) << S_NP)
+#define F_NP    V_NP(1U)
+
+#define S_ACK    14
+#define V_ACK(x) ((x) << S_ACK)
+#define F_ACK    V_ACK(1U)
+
+#define S_REMOTE_FAULT    13
+#define V_REMOTE_FAULT(x) ((x) << S_REMOTE_FAULT)
+#define F_REMOTE_FAULT    V_REMOTE_FAULT(1U)
+
+#define S_ASM_DIR    11
+#define V_ASM_DIR(x) ((x) << S_ASM_DIR)
+#define F_ASM_DIR    V_ASM_DIR(1U)
+
+#define S_PAUSE    10
+#define V_PAUSE(x) ((x) << S_PAUSE)
+#define F_PAUSE    V_PAUSE(1U)
+
+#define S_ECHOED_NONCE    5
+#define M_ECHOED_NONCE    0x1fU
+#define V_ECHOED_NONCE(x) ((x) << S_ECHOED_NONCE)
+#define G_ECHOED_NONCE(x) (((x) >> S_ECHOED_NONCE) & M_ECHOED_NONCE)
+
+#define A_XGMAC_PORT_AN_LINK_PARTNER_ABILITY 0x160c
+
+#define S_SELECTOR_FIELD    0
+#define M_SELECTOR_FIELD    0x1fU
+#define V_SELECTOR_FIELD(x) ((x) << S_SELECTOR_FIELD)
+#define G_SELECTOR_FIELD(x) (((x) >> S_SELECTOR_FIELD) & M_SELECTOR_FIELD)
+
+#define A_XGMAC_PORT_AN_NP_LOWER_TRANSMIT 0x1610
+
+#define S_NP_INFO    16
+#define M_NP_INFO    0xffffU
+#define V_NP_INFO(x) ((x) << S_NP_INFO)
+#define G_NP_INFO(x) (((x) >> S_NP_INFO) & M_NP_INFO)
+
+#define S_NP_INDICATION    15
+#define V_NP_INDICATION(x) ((x) << S_NP_INDICATION)
+#define F_NP_INDICATION    V_NP_INDICATION(1U)
+
+#define S_MESSAGE_PAGE    13
+#define V_MESSAGE_PAGE(x) ((x) << S_MESSAGE_PAGE)
+#define F_MESSAGE_PAGE    V_MESSAGE_PAGE(1U)
+
+#define S_ACK_2    12
+#define V_ACK_2(x) ((x) << S_ACK_2)
+#define F_ACK_2    V_ACK_2(1U)
+
+#define S_TOGGLE    11
+#define V_TOGGLE(x) ((x) << S_TOGGLE)
+#define F_TOGGLE    V_TOGGLE(1U)
+
+#define A_XGMAC_PORT_AN_NP_UPPER_TRANSMIT 0x1614
+
+#define S_NP_INFO_HI    0
+#define M_NP_INFO_HI    0xffffU
+#define V_NP_INFO_HI(x) ((x) << S_NP_INFO_HI)
+#define G_NP_INFO_HI(x) (((x) >> S_NP_INFO_HI) & M_NP_INFO_HI)
+
+#define A_XGMAC_PORT_AN_LP_NP_LOWER 0x1618
+#define A_XGMAC_PORT_AN_LP_NP_UPPER 0x161c
+#define A_XGMAC_PORT_AN_BACKPLANE_ETHERNET_STATUS 0x1624
+
+#define S_TX_PAUSE_OKAY    6
+#define V_TX_PAUSE_OKAY(x) ((x) << S_TX_PAUSE_OKAY)
+#define F_TX_PAUSE_OKAY    V_TX_PAUSE_OKAY(1U)
+
+#define S_RX_PAUSE_OKAY    5
+#define V_RX_PAUSE_OKAY(x) ((x) << S_RX_PAUSE_OKAY)
+#define F_RX_PAUSE_OKAY    V_RX_PAUSE_OKAY(1U)
+
+#define S_10GBASE_KR_FEC_NEG    4
+#define V_10GBASE_KR_FEC_NEG(x) ((x) << S_10GBASE_KR_FEC_NEG)
+#define F_10GBASE_KR_FEC_NEG    V_10GBASE_KR_FEC_NEG(1U)
+
+#define S_10GBASE_KR_NEG    3
+#define V_10GBASE_KR_NEG(x) ((x) << S_10GBASE_KR_NEG)
+#define F_10GBASE_KR_NEG    V_10GBASE_KR_NEG(1U)
+
+#define S_10GBASE_KX4_NEG    2
+#define V_10GBASE_KX4_NEG(x) ((x) << S_10GBASE_KX4_NEG)
+#define F_10GBASE_KX4_NEG    V_10GBASE_KX4_NEG(1U)
+
+#define S_1000BASE_KX_NEG    1
+#define V_1000BASE_KX_NEG(x) ((x) << S_1000BASE_KX_NEG)
+#define F_1000BASE_KX_NEG    V_1000BASE_KX_NEG(1U)
+
+#define S_BP_AN_ABILITY    0
+#define V_BP_AN_ABILITY(x) ((x) << S_BP_AN_ABILITY)
+#define F_BP_AN_ABILITY    V_BP_AN_ABILITY(1U)
+
+#define A_XGMAC_PORT_AN_TX_NONCE_CONTROL 0x1628
+
+#define S_BYPASS_LFSR    15
+#define V_BYPASS_LFSR(x) ((x) << S_BYPASS_LFSR)
+#define F_BYPASS_LFSR    V_BYPASS_LFSR(1U)
+
+#define S_LFSR_INIT    0
+#define M_LFSR_INIT    0x7fffU
+#define V_LFSR_INIT(x) ((x) << S_LFSR_INIT)
+#define G_LFSR_INIT(x) (((x) >> S_LFSR_INIT) & M_LFSR_INIT)
+
+#define A_XGMAC_PORT_AN_INTERRUPT_STATUS 0x162c
+
+#define S_NP_FROM_LP    3
+#define V_NP_FROM_LP(x) ((x) << S_NP_FROM_LP)
+#define F_NP_FROM_LP    V_NP_FROM_LP(1U)
+
+#define S_PARALLELDETFAULTINT    2
+#define V_PARALLELDETFAULTINT(x) ((x) << S_PARALLELDETFAULTINT)
+#define F_PARALLELDETFAULTINT    V_PARALLELDETFAULTINT(1U)
+
+#define S_BP_FROM_LP    1
+#define V_BP_FROM_LP(x) ((x) << S_BP_FROM_LP)
+#define F_BP_FROM_LP    V_BP_FROM_LP(1U)
+
+#define S_PCS_AN_COMPLETE    0
+#define V_PCS_AN_COMPLETE(x) ((x) << S_PCS_AN_COMPLETE)
+#define F_PCS_AN_COMPLETE    V_PCS_AN_COMPLETE(1U)
+
+#define A_XGMAC_PORT_AN_GENERIC_TIMER_TIMEOUT 0x1630
+
+#define S_GENERIC_TIMEOUT    0
+#define M_GENERIC_TIMEOUT    0x7fffffU
+#define V_GENERIC_TIMEOUT(x) ((x) << S_GENERIC_TIMEOUT)
+#define G_GENERIC_TIMEOUT(x) (((x) >> S_GENERIC_TIMEOUT) & M_GENERIC_TIMEOUT)
+
+#define A_XGMAC_PORT_AN_BREAK_LINK_TIMEOUT 0x1634
+
+#define S_BREAK_LINK_TIMEOUT    0
+#define M_BREAK_LINK_TIMEOUT    0xffffffU
+#define V_BREAK_LINK_TIMEOUT(x) ((x) << S_BREAK_LINK_TIMEOUT)
+#define G_BREAK_LINK_TIMEOUT(x) (((x) >> S_BREAK_LINK_TIMEOUT) & M_BREAK_LINK_TIMEOUT)
+
+#define A_XGMAC_PORT_AN_MODULE_ID 0x163c
+
+#define S_MODULE_ID    16
+#define M_MODULE_ID    0xffffU
+#define V_MODULE_ID(x) ((x) << S_MODULE_ID)
+#define G_MODULE_ID(x) (((x) >> S_MODULE_ID) & M_MODULE_ID)
+
+#define S_MODULE_REVISION    0
+#define M_MODULE_REVISION    0xffffU
+#define V_MODULE_REVISION(x) ((x) << S_MODULE_REVISION)
+#define G_MODULE_REVISION(x) (((x) >> S_MODULE_REVISION) & M_MODULE_REVISION)
+
+#define A_XGMAC_PORT_AE_RX_COEF_REQ 0x1700
+
+#define S_RXREQ_CPRE    13
+#define V_RXREQ_CPRE(x) ((x) << S_RXREQ_CPRE)
+#define F_RXREQ_CPRE    V_RXREQ_CPRE(1U)
+
+#define S_RXREQ_CINIT    12
+#define V_RXREQ_CINIT(x) ((x) << S_RXREQ_CINIT)
+#define F_RXREQ_CINIT    V_RXREQ_CINIT(1U)
+
+#define S_RXREQ_C0    4
+#define M_RXREQ_C0    0x3U
+#define V_RXREQ_C0(x) ((x) << S_RXREQ_C0)
+#define G_RXREQ_C0(x) (((x) >> S_RXREQ_C0) & M_RXREQ_C0)
+
+#define S_RXREQ_C1    2
+#define M_RXREQ_C1    0x3U
+#define V_RXREQ_C1(x) ((x) << S_RXREQ_C1)
+#define G_RXREQ_C1(x) (((x) >> S_RXREQ_C1) & M_RXREQ_C1)
+
+#define S_RXREQ_C2    0
+#define M_RXREQ_C2    0x3U
+#define V_RXREQ_C2(x) ((x) << S_RXREQ_C2)
+#define G_RXREQ_C2(x) (((x) >> S_RXREQ_C2) & M_RXREQ_C2)
+
+#define A_XGMAC_PORT_AE_RX_COEF_STAT 0x1704
+
+#define S_RXSTAT_RDY    15
+#define V_RXSTAT_RDY(x) ((x) << S_RXSTAT_RDY)
+#define F_RXSTAT_RDY    V_RXSTAT_RDY(1U)
+
+#define S_RXSTAT_C0    4
+#define M_RXSTAT_C0    0x3U
+#define V_RXSTAT_C0(x) ((x) << S_RXSTAT_C0)
+#define G_RXSTAT_C0(x) (((x) >> S_RXSTAT_C0) & M_RXSTAT_C0)
+
+#define S_RXSTAT_C1    2
+#define M_RXSTAT_C1    0x3U
+#define V_RXSTAT_C1(x) ((x) << S_RXSTAT_C1)
+#define G_RXSTAT_C1(x) (((x) >> S_RXSTAT_C1) & M_RXSTAT_C1)
+
+#define S_RXSTAT_C2    0
+#define M_RXSTAT_C2    0x3U
+#define V_RXSTAT_C2(x) ((x) << S_RXSTAT_C2)
+#define G_RXSTAT_C2(x) (((x) >> S_RXSTAT_C2) & M_RXSTAT_C2)
+
+#define A_XGMAC_PORT_AE_TX_COEF_REQ 0x1708
+
+#define S_TXREQ_CPRE    13
+#define V_TXREQ_CPRE(x) ((x) << S_TXREQ_CPRE)
+#define F_TXREQ_CPRE    V_TXREQ_CPRE(1U)
+
+#define S_TXREQ_CINIT    12
+#define V_TXREQ_CINIT(x) ((x) << S_TXREQ_CINIT)
+#define F_TXREQ_CINIT    V_TXREQ_CINIT(1U)
+
+#define S_TXREQ_C0    4
+#define M_TXREQ_C0    0x3U
+#define V_TXREQ_C0(x) ((x) << S_TXREQ_C0)
+#define G_TXREQ_C0(x) (((x) >> S_TXREQ_C0) & M_TXREQ_C0)
+
+#define S_TXREQ_C1    2
+#define M_TXREQ_C1    0x3U
+#define V_TXREQ_C1(x) ((x) << S_TXREQ_C1)
+#define G_TXREQ_C1(x) (((x) >> S_TXREQ_C1) & M_TXREQ_C1)
+
+#define S_TXREQ_C2    0
+#define M_TXREQ_C2    0x3U
+#define V_TXREQ_C2(x) ((x) << S_TXREQ_C2)
+#define G_TXREQ_C2(x) (((x) >> S_TXREQ_C2) & M_TXREQ_C2)
+
+#define A_XGMAC_PORT_AE_TX_COEF_STAT 0x170c
+
+#define S_TXSTAT_RDY    15
+#define V_TXSTAT_RDY(x) ((x) << S_TXSTAT_RDY)
+#define F_TXSTAT_RDY    V_TXSTAT_RDY(1U)
+
+#define S_TXSTAT_C0    4
+#define M_TXSTAT_C0    0x3U
+#define V_TXSTAT_C0(x) ((x) << S_TXSTAT_C0)
+#define G_TXSTAT_C0(x) (((x) >> S_TXSTAT_C0) & M_TXSTAT_C0)
+
+#define S_TXSTAT_C1    2
+#define M_TXSTAT_C1    0x3U
+#define V_TXSTAT_C1(x) ((x) << S_TXSTAT_C1)
+#define G_TXSTAT_C1(x) (((x) >> S_TXSTAT_C1) & M_TXSTAT_C1)
+
+#define S_TXSTAT_C2    0
+#define M_TXSTAT_C2    0x3U
+#define V_TXSTAT_C2(x) ((x) << S_TXSTAT_C2)
+#define G_TXSTAT_C2(x) (((x) >> S_TXSTAT_C2) & M_TXSTAT_C2)
+
+#define A_XGMAC_PORT_AE_REG_MODE 0x1710
+
+#define S_MAN_DEC    4
+#define M_MAN_DEC    0x3U
+#define V_MAN_DEC(x) ((x) << S_MAN_DEC)
+#define G_MAN_DEC(x) (((x) >> S_MAN_DEC) & M_MAN_DEC)
+
+#define S_MANUAL_RDY    3
+#define V_MANUAL_RDY(x) ((x) << S_MANUAL_RDY)
+#define F_MANUAL_RDY    V_MANUAL_RDY(1U)
+
+#define S_MWT_DISABLE    2
+#define V_MWT_DISABLE(x) ((x) << S_MWT_DISABLE)
+#define F_MWT_DISABLE    V_MWT_DISABLE(1U)
+
+#define S_MDIO_OVR    1
+#define V_MDIO_OVR(x) ((x) << S_MDIO_OVR)
+#define F_MDIO_OVR    V_MDIO_OVR(1U)
+
+#define S_STICKY_MODE    0
+#define V_STICKY_MODE(x) ((x) << S_STICKY_MODE)
+#define F_STICKY_MODE    V_STICKY_MODE(1U)
+
+#define A_XGMAC_PORT_AE_PRBS_CTL 0x1714
+
+#define S_PRBS_CHK_ERRCNT    8
+#define M_PRBS_CHK_ERRCNT    0xffU
+#define V_PRBS_CHK_ERRCNT(x) ((x) << S_PRBS_CHK_ERRCNT)
+#define G_PRBS_CHK_ERRCNT(x) (((x) >> S_PRBS_CHK_ERRCNT) & M_PRBS_CHK_ERRCNT)
+
+#define S_PRBS_SYNCCNT    5
+#define M_PRBS_SYNCCNT    0x7U
+#define V_PRBS_SYNCCNT(x) ((x) << S_PRBS_SYNCCNT)
+#define G_PRBS_SYNCCNT(x) (((x) >> S_PRBS_SYNCCNT) & M_PRBS_SYNCCNT)
+
+#define S_PRBS_CHK_SYNC    4
+#define V_PRBS_CHK_SYNC(x) ((x) << S_PRBS_CHK_SYNC)
+#define F_PRBS_CHK_SYNC    V_PRBS_CHK_SYNC(1U)
+
+#define S_PRBS_CHK_RST    3
+#define V_PRBS_CHK_RST(x) ((x) << S_PRBS_CHK_RST)
+#define F_PRBS_CHK_RST    V_PRBS_CHK_RST(1U)
+
+#define S_PRBS_CHK_OFF    2
+#define V_PRBS_CHK_OFF(x) ((x) << S_PRBS_CHK_OFF)
+#define F_PRBS_CHK_OFF    V_PRBS_CHK_OFF(1U)
+
+#define S_PRBS_GEN_FRCERR    1
+#define V_PRBS_GEN_FRCERR(x) ((x) << S_PRBS_GEN_FRCERR)
+#define F_PRBS_GEN_FRCERR    V_PRBS_GEN_FRCERR(1U)
+
+#define S_PRBS_GEN_OFF    0
+#define V_PRBS_GEN_OFF(x) ((x) << S_PRBS_GEN_OFF)
+#define F_PRBS_GEN_OFF    V_PRBS_GEN_OFF(1U)
+
+#define A_XGMAC_PORT_AE_FSM_CTL 0x1718
+
+#define S_FSM_TR_LCL    14
+#define V_FSM_TR_LCL(x) ((x) << S_FSM_TR_LCL)
+#define F_FSM_TR_LCL    V_FSM_TR_LCL(1U)
+
+#define S_FSM_GDMRK    11
+#define M_FSM_GDMRK    0x7U
+#define V_FSM_GDMRK(x) ((x) << S_FSM_GDMRK)
+#define G_FSM_GDMRK(x) (((x) >> S_FSM_GDMRK) & M_FSM_GDMRK)
+
+#define S_FSM_BADMRK    8
+#define M_FSM_BADMRK    0x7U
+#define V_FSM_BADMRK(x) ((x) << S_FSM_BADMRK)
+#define G_FSM_BADMRK(x) (((x) >> S_FSM_BADMRK) & M_FSM_BADMRK)
+
+#define S_FSM_TR_FAIL    7
+#define V_FSM_TR_FAIL(x) ((x) << S_FSM_TR_FAIL)
+#define F_FSM_TR_FAIL    V_FSM_TR_FAIL(1U)
+
+#define S_FSM_TR_ACT    6
+#define V_FSM_TR_ACT(x) ((x) << S_FSM_TR_ACT)
+#define F_FSM_TR_ACT    V_FSM_TR_ACT(1U)
+
+#define S_FSM_FRM_LCK    5
+#define V_FSM_FRM_LCK(x) ((x) << S_FSM_FRM_LCK)
+#define F_FSM_FRM_LCK    V_FSM_FRM_LCK(1U)
+
+#define S_FSM_TR_COMP    4
+#define V_FSM_TR_COMP(x) ((x) << S_FSM_TR_COMP)
+#define F_FSM_TR_COMP    V_FSM_TR_COMP(1U)
+
+#define S_MC_RX_RDY    3
+#define V_MC_RX_RDY(x) ((x) << S_MC_RX_RDY)
+#define F_MC_RX_RDY    V_MC_RX_RDY(1U)
+
+#define S_FSM_CU_DIS    2
+#define V_FSM_CU_DIS(x) ((x) << S_FSM_CU_DIS)
+#define F_FSM_CU_DIS    V_FSM_CU_DIS(1U)
+
+#define S_FSM_TR_RST    1
+#define V_FSM_TR_RST(x) ((x) << S_FSM_TR_RST)
+#define F_FSM_TR_RST    V_FSM_TR_RST(1U)
+
+#define S_FSM_TR_EN    0
+#define V_FSM_TR_EN(x) ((x) << S_FSM_TR_EN)
+#define F_FSM_TR_EN    V_FSM_TR_EN(1U)
+
+#define A_XGMAC_PORT_AE_FSM_STATE 0x171c
+
+#define S_CC2FSM_STATE    13
+#define M_CC2FSM_STATE    0x7U
+#define V_CC2FSM_STATE(x) ((x) << S_CC2FSM_STATE)
+#define G_CC2FSM_STATE(x) (((x) >> S_CC2FSM_STATE) & M_CC2FSM_STATE)
+
+#define S_CC1FSM_STATE    10
+#define M_CC1FSM_STATE    0x7U
+#define V_CC1FSM_STATE(x) ((x) << S_CC1FSM_STATE)
+#define G_CC1FSM_STATE(x) (((x) >> S_CC1FSM_STATE) & M_CC1FSM_STATE)
+
+#define S_CC0FSM_STATE    7
+#define M_CC0FSM_STATE    0x7U
+#define V_CC0FSM_STATE(x) ((x) << S_CC0FSM_STATE)
+#define G_CC0FSM_STATE(x) (((x) >> S_CC0FSM_STATE) & M_CC0FSM_STATE)
+
+#define S_FLFSM_STATE    4
+#define M_FLFSM_STATE    0x7U
+#define V_FLFSM_STATE(x) ((x) << S_FLFSM_STATE)
+#define G_FLFSM_STATE(x) (((x) >> S_FLFSM_STATE) & M_FLFSM_STATE)
+
+#define S_TFSM_STATE    0
+#define M_TFSM_STATE    0x7U
+#define V_TFSM_STATE(x) ((x) << S_TFSM_STATE)
+#define G_TFSM_STATE(x) (((x) >> S_TFSM_STATE) & M_TFSM_STATE)
+
+#define A_XGMAC_PORT_AE_TX_DIS 0x1780
+
+#define S_PMD_TX_DIS    0
+#define V_PMD_TX_DIS(x) ((x) << S_PMD_TX_DIS)
+#define F_PMD_TX_DIS    V_PMD_TX_DIS(1U)
+
+#define A_XGMAC_PORT_AE_KR_CTRL 0x1784
+
+#define S_TRAINING_ENABLE    1
+#define V_TRAINING_ENABLE(x) ((x) << S_TRAINING_ENABLE)
+#define F_TRAINING_ENABLE    V_TRAINING_ENABLE(1U)
+
+#define S_RESTART_TRAINING    0
+#define V_RESTART_TRAINING(x) ((x) << S_RESTART_TRAINING)
+#define F_RESTART_TRAINING    V_RESTART_TRAINING(1U)
+
+#define A_XGMAC_PORT_AE_RX_SIGDET 0x1788
+
+#define S_PMD_SIGDET    0
+#define V_PMD_SIGDET(x) ((x) << S_PMD_SIGDET)
+#define F_PMD_SIGDET    V_PMD_SIGDET(1U)
+
+#define A_XGMAC_PORT_AE_KR_STATUS 0x178c
+
+#define S_TRAINING_FAILURE    3
+#define V_TRAINING_FAILURE(x) ((x) << S_TRAINING_FAILURE)
+#define F_TRAINING_FAILURE    V_TRAINING_FAILURE(1U)
+
+#define S_TRAINING    2
+#define V_TRAINING(x) ((x) << S_TRAINING)
+#define F_TRAINING    V_TRAINING(1U)
+
+#define S_FRAME_LOCK    1
+#define V_FRAME_LOCK(x) ((x) << S_FRAME_LOCK)
+#define F_FRAME_LOCK    V_FRAME_LOCK(1U)
+
+#define S_RX_TRAINED    0
+#define V_RX_TRAINED(x) ((x) << S_RX_TRAINED)
+#define F_RX_TRAINED    V_RX_TRAINED(1U)
+
+#define A_XGMAC_PORT_HSS_TXA_MODE_CFG 0x1800
+
+#define S_BWSEL    2
+#define M_BWSEL    0x3U
+#define V_BWSEL(x) ((x) << S_BWSEL)
+#define G_BWSEL(x) (((x) >> S_BWSEL) & M_BWSEL)
+
+#define S_RTSEL    0
+#define M_RTSEL    0x3U
+#define V_RTSEL(x) ((x) << S_RTSEL)
+#define G_RTSEL(x) (((x) >> S_RTSEL) & M_RTSEL)
+
+#define A_XGMAC_PORT_HSS_TXA_TEST_CTRL 0x1804
+
+#define S_TWDP    5
+#define V_TWDP(x) ((x) << S_TWDP)
+#define F_TWDP    V_TWDP(1U)
+
+#define S_TPGRST    4
+#define V_TPGRST(x) ((x) << S_TPGRST)
+#define F_TPGRST    V_TPGRST(1U)
+
+#define S_TPGEN    3
+#define V_TPGEN(x) ((x) << S_TPGEN)
+#define F_TPGEN    V_TPGEN(1U)
+
+#define S_TPSEL    0
+#define M_TPSEL    0x7U
+#define V_TPSEL(x) ((x) << S_TPSEL)
+#define G_TPSEL(x) (((x) >> S_TPSEL) & M_TPSEL)
+
+#define A_XGMAC_PORT_HSS_TXA_COEFF_CTRL 0x1808
+
+#define S_AEINVPOL    6
+#define V_AEINVPOL(x) ((x) << S_AEINVPOL)
+#define F_AEINVPOL    V_AEINVPOL(1U)
+
+#define S_AESOURCE    5
+#define V_AESOURCE(x) ((x) << S_AESOURCE)
+#define F_AESOURCE    V_AESOURCE(1U)
+
+#define S_EQMODE    4
+#define V_EQMODE(x) ((x) << S_EQMODE)
+#define F_EQMODE    V_EQMODE(1U)
+
+#define S_OCOEF    3
+#define V_OCOEF(x) ((x) << S_OCOEF)
+#define F_OCOEF    V_OCOEF(1U)
+
+#define S_COEFRST    2
+#define V_COEFRST(x) ((x) << S_COEFRST)
+#define F_COEFRST    V_COEFRST(1U)
+
+#define S_SPEN    1
+#define V_SPEN(x) ((x) << S_SPEN)
+#define F_SPEN    V_SPEN(1U)
+
+#define S_ALOAD    0
+#define V_ALOAD(x) ((x) << S_ALOAD)
+#define F_ALOAD    V_ALOAD(1U)
+
+#define A_XGMAC_PORT_HSS_TXA_DRIVER_MODE 0x180c
+
+#define S_DRVOFFT    5
+#define V_DRVOFFT(x) ((x) << S_DRVOFFT)
+#define F_DRVOFFT    V_DRVOFFT(1U)
+
+#define S_SLEW    2
+#define M_SLEW    0x7U
+#define V_SLEW(x) ((x) << S_SLEW)
+#define G_SLEW(x) (((x) >> S_SLEW) & M_SLEW)
+
+#define S_FFE    0
+#define M_FFE    0x3U
+#define V_FFE(x) ((x) << S_FFE)
+#define G_FFE(x) (((x) >> S_FFE) & M_FFE)
+
+#define A_XGMAC_PORT_HSS_TXA_DRIVER_OVR_CTRL 0x1810
+
+#define S_VLINC    7
+#define V_VLINC(x) ((x) << S_VLINC)
+#define F_VLINC    V_VLINC(1U)
+
+#define S_VLDEC    6
+#define V_VLDEC(x) ((x) << S_VLDEC)
+#define F_VLDEC    V_VLDEC(1U)
+
+#define S_LOPWR    5
+#define V_LOPWR(x) ((x) << S_LOPWR)
+#define F_LOPWR    V_LOPWR(1U)
+
+#define S_TDMEN    4
+#define V_TDMEN(x) ((x) << S_TDMEN)
+#define F_TDMEN    V_TDMEN(1U)
+
+#define S_DCCEN    3
+#define V_DCCEN(x) ((x) << S_DCCEN)
+#define F_DCCEN    V_DCCEN(1U)
+
+#define S_VHSEL    2
+#define V_VHSEL(x) ((x) << S_VHSEL)
+#define F_VHSEL    V_VHSEL(1U)
+
+#define S_IDAC    0
+#define M_IDAC    0x3U
+#define V_IDAC(x) ((x) << S_IDAC)
+#define G_IDAC(x) (((x) >> S_IDAC) & M_IDAC)
+
+#define A_XGMAC_PORT_HSS_TXA_TDM_BIASGEN_STANDBY_TIMER 0x1814
+
+#define S_STBY    0
+#define M_STBY    0xffffU
+#define V_STBY(x) ((x) << S_STBY)
+#define G_STBY(x) (((x) >> S_STBY) & M_STBY)
+
+#define A_XGMAC_PORT_HSS_TXA_TDM_BIASGEN_PWRON_TIMER 0x1818
+
+#define S_PON    0
+#define M_PON    0xffffU
+#define V_PON(x) ((x) << S_PON)
+#define G_PON(x) (((x) >> S_PON) & M_PON)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP0_COEFF 0x1820
+
+#define S_NXTT0    0
+#define M_NXTT0    0xfU
+#define V_NXTT0(x) ((x) << S_NXTT0)
+#define G_NXTT0(x) (((x) >> S_NXTT0) & M_NXTT0)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP1_COEFF 0x1824
+
+#define S_NXTT1    0
+#define M_NXTT1    0x3fU
+#define V_NXTT1(x) ((x) << S_NXTT1)
+#define G_NXTT1(x) (((x) >> S_NXTT1) & M_NXTT1)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP2_COEFF 0x1828
+
+#define S_NXTT2    0
+#define M_NXTT2    0x1fU
+#define V_NXTT2(x) ((x) << S_NXTT2)
+#define G_NXTT2(x) (((x) >> S_NXTT2) & M_NXTT2)
+
+#define A_XGMAC_PORT_HSS_TXA_PWR 0x1830
+
+#define S_TXPWR    0
+#define M_TXPWR    0x7fU
+#define V_TXPWR(x) ((x) << S_TXPWR)
+#define G_TXPWR(x) (((x) >> S_TXPWR) & M_TXPWR)
+
+#define A_XGMAC_PORT_HSS_TXA_POLARITY 0x1834
+
+#define S_TXPOL    4
+#define M_TXPOL    0x7U
+#define V_TXPOL(x) ((x) << S_TXPOL)
+#define G_TXPOL(x) (((x) >> S_TXPOL) & M_TXPOL)
+
+#define S_NTXPOL    0
+#define M_NTXPOL    0x7U
+#define V_NTXPOL(x) ((x) << S_NTXPOL)
+#define G_NTXPOL(x) (((x) >> S_NTXPOL) & M_NTXPOL)
+
+#define A_XGMAC_PORT_HSS_TXA_8023AP_AE_CMD 0x1838
+
+#define S_CXPRESET    13
+#define V_CXPRESET(x) ((x) << S_CXPRESET)
+#define F_CXPRESET    V_CXPRESET(1U)
+
+#define S_CXINIT    12
+#define V_CXINIT(x) ((x) << S_CXINIT)
+#define F_CXINIT    V_CXINIT(1U)
+
+#define S_C2UPDT    4
+#define M_C2UPDT    0x3U
+#define V_C2UPDT(x) ((x) << S_C2UPDT)
+#define G_C2UPDT(x) (((x) >> S_C2UPDT) & M_C2UPDT)
+
+#define S_C1UPDT    2
+#define M_C1UPDT    0x3U
+#define V_C1UPDT(x) ((x) << S_C1UPDT)
+#define G_C1UPDT(x) (((x) >> S_C1UPDT) & M_C1UPDT)
+
+#define S_C0UPDT    0
+#define M_C0UPDT    0x3U
+#define V_C0UPDT(x) ((x) << S_C0UPDT)
+#define G_C0UPDT(x) (((x) >> S_C0UPDT) & M_C0UPDT)
+
+#define A_XGMAC_PORT_HSS_TXA_8023AP_AE_STATUS 0x183c
+
+#define S_C2STAT    4
+#define M_C2STAT    0x3U
+#define V_C2STAT(x) ((x) << S_C2STAT)
+#define G_C2STAT(x) (((x) >> S_C2STAT) & M_C2STAT)
+
+#define S_C1STAT    2
+#define M_C1STAT    0x3U
+#define V_C1STAT(x) ((x) << S_C1STAT)
+#define G_C1STAT(x) (((x) >> S_C1STAT) & M_C1STAT)
+
+#define S_C0STAT    0
+#define M_C0STAT    0x3U
+#define V_C0STAT(x) ((x) << S_C0STAT)
+#define G_C0STAT(x) (((x) >> S_C0STAT) & M_C0STAT)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP0_IDAC_OVR 0x1840
+
+#define S_NIDAC0    0
+#define M_NIDAC0    0x1fU
+#define V_NIDAC0(x) ((x) << S_NIDAC0)
+#define G_NIDAC0(x) (((x) >> S_NIDAC0) & M_NIDAC0)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP1_IDAC_OVR 0x1844
+
+#define S_NIDAC1    0
+#define M_NIDAC1    0x7fU
+#define V_NIDAC1(x) ((x) << S_NIDAC1)
+#define G_NIDAC1(x) (((x) >> S_NIDAC1) & M_NIDAC1)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP2_IDAC_OVR 0x1848
+
+#define S_NIDAC2    0
+#define M_NIDAC2    0x3fU
+#define V_NIDAC2(x) ((x) << S_NIDAC2)
+#define G_NIDAC2(x) (((x) >> S_NIDAC2) & M_NIDAC2)
+
+#define A_XGMAC_PORT_HSS_TXA_PWR_DAC_OVR 0x1850
+
+#define S_OPEN    7
+#define V_OPEN(x) ((x) << S_OPEN)
+#define F_OPEN    V_OPEN(1U)
+
+#define S_OPVAL    0
+#define M_OPVAL    0x1fU
+#define V_OPVAL(x) ((x) << S_OPVAL)
+#define G_OPVAL(x) (((x) >> S_OPVAL) & M_OPVAL)
+
+#define A_XGMAC_PORT_HSS_TXA_PWR_DAC 0x1854
+
+#define S_PDAC    0
+#define M_PDAC    0x1fU
+#define V_PDAC(x) ((x) << S_PDAC)
+#define G_PDAC(x) (((x) >> S_PDAC) & M_PDAC)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP0_IDAC_APP 0x1860
+
+#define S_AIDAC0    0
+#define M_AIDAC0    0x1fU
+#define V_AIDAC0(x) ((x) << S_AIDAC0)
+#define G_AIDAC0(x) (((x) >> S_AIDAC0) & M_AIDAC0)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP1_IDAC_APP 0x1864
+
+#define S_AIDAC1    0
+#define M_AIDAC1    0x1fU
+#define V_AIDAC1(x) ((x) << S_AIDAC1)
+#define G_AIDAC1(x) (((x) >> S_AIDAC1) & M_AIDAC1)
+
+#define A_XGMAC_PORT_HSS_TXA_TAP2_IDAC_APP 0x1868
+
+#define S_TXA_AIDAC2    0
+#define M_TXA_AIDAC2    0x1fU
+#define V_TXA_AIDAC2(x) ((x) << S_TXA_AIDAC2)
+#define G_TXA_AIDAC2(x) (((x) >> S_TXA_AIDAC2) & M_TXA_AIDAC2)
+
+#define A_XGMAC_PORT_HSS_TXA_SEG_DIS_APP 0x1870
+
+#define S_CURSD    0
+#define M_CURSD    0x7fU
+#define V_CURSD(x) ((x) << S_CURSD)
+#define G_CURSD(x) (((x) >> S_CURSD) & M_CURSD)
+
+#define A_XGMAC_PORT_HSS_TXA_EXT_ADDR_DATA 0x1878
+
+#define S_XDATA    0
+#define M_XDATA    0xffffU
+#define V_XDATA(x) ((x) << S_XDATA)
+#define G_XDATA(x) (((x) >> S_XDATA) & M_XDATA)
+
+#define A_XGMAC_PORT_HSS_TXA_EXT_ADDR 0x187c
+
+#define S_EXTADDR    1
+#define M_EXTADDR    0x1fU
+#define V_EXTADDR(x) ((x) << S_EXTADDR)
+#define G_EXTADDR(x) (((x) >> S_EXTADDR) & M_EXTADDR)
+
+#define S_XWR    0
+#define V_XWR(x) ((x) << S_XWR)
+#define F_XWR    V_XWR(1U)
+
+#define A_XGMAC_PORT_HSS_TXB_MODE_CFG 0x1880
+#define A_XGMAC_PORT_HSS_TXB_TEST_CTRL 0x1884
+#define A_XGMAC_PORT_HSS_TXB_COEFF_CTRL 0x1888
+#define A_XGMAC_PORT_HSS_TXB_DRIVER_MODE 0x188c
+#define A_XGMAC_PORT_HSS_TXB_DRIVER_OVR_CTRL 0x1890
+#define A_XGMAC_PORT_HSS_TXB_TDM_BIASGEN_STANDBY_TIMER 0x1894
+#define A_XGMAC_PORT_HSS_TXB_TDM_BIASGEN_PWRON_TIMER 0x1898
+#define A_XGMAC_PORT_HSS_TXB_TAP0_COEFF 0x18a0
+#define A_XGMAC_PORT_HSS_TXB_TAP1_COEFF 0x18a4
+#define A_XGMAC_PORT_HSS_TXB_TAP2_COEFF 0x18a8
+#define A_XGMAC_PORT_HSS_TXB_PWR 0x18b0
+#define A_XGMAC_PORT_HSS_TXB_POLARITY 0x18b4
+#define A_XGMAC_PORT_HSS_TXB_8023AP_AE_CMD 0x18b8
+#define A_XGMAC_PORT_HSS_TXB_8023AP_AE_STATUS 0x18bc
+#define A_XGMAC_PORT_HSS_TXB_TAP0_IDAC_OVR 0x18c0
+#define A_XGMAC_PORT_HSS_TXB_TAP1_IDAC_OVR 0x18c4
+#define A_XGMAC_PORT_HSS_TXB_TAP2_IDAC_OVR 0x18c8
+#define A_XGMAC_PORT_HSS_TXB_PWR_DAC_OVR 0x18d0
+#define A_XGMAC_PORT_HSS_TXB_PWR_DAC 0x18d4
+#define A_XGMAC_PORT_HSS_TXB_TAP0_IDAC_APP 0x18e0
+#define A_XGMAC_PORT_HSS_TXB_TAP1_IDAC_APP 0x18e4
+#define A_XGMAC_PORT_HSS_TXB_TAP2_IDAC_APP 0x18e8
+
+#define S_AIDAC2    0
+#define M_AIDAC2    0x3fU
+#define V_AIDAC2(x) ((x) << S_AIDAC2)
+#define G_AIDAC2(x) (((x) >> S_AIDAC2) & M_AIDAC2)
+
+#define A_XGMAC_PORT_HSS_TXB_SEG_DIS_APP 0x18f0
+#define A_XGMAC_PORT_HSS_TXB_EXT_ADDR_DATA 0x18f8
+#define A_XGMAC_PORT_HSS_TXB_EXT_ADDR 0x18fc
+
+#define S_XADDR    2
+#define M_XADDR    0xfU
+#define V_XADDR(x) ((x) << S_XADDR)
+#define G_XADDR(x) (((x) >> S_XADDR) & M_XADDR)
+
+#define A_XGMAC_PORT_HSS_RXA_CFG_MODE 0x1900
+
+#define S_BW810    8
+#define V_BW810(x) ((x) << S_BW810)
+#define F_BW810    V_BW810(1U)
+
+#define S_AUXCLK    7
+#define V_AUXCLK(x) ((x) << S_AUXCLK)
+#define F_AUXCLK    V_AUXCLK(1U)
+
+#define S_DMSEL    4
+#define M_DMSEL    0x7U
+#define V_DMSEL(x) ((x) << S_DMSEL)
+#define G_DMSEL(x) (((x) >> S_DMSEL) & M_DMSEL)
+
+#define A_XGMAC_PORT_HSS_RXA_TEST_CTRL 0x1904
+
+#define S_RCLKEN    15
+#define V_RCLKEN(x) ((x) << S_RCLKEN)
+#define F_RCLKEN    V_RCLKEN(1U)
+
+#define S_RRATE    13
+#define M_RRATE    0x3U
+#define V_RRATE(x) ((x) << S_RRATE)
+#define G_RRATE(x) (((x) >> S_RRATE) & M_RRATE)
+
+#define S_LBFRCERROR    10
+#define V_LBFRCERROR(x) ((x) << S_LBFRCERROR)
+#define F_LBFRCERROR    V_LBFRCERROR(1U)
+
+#define S_LBERROR    9
+#define V_LBERROR(x) ((x) << S_LBERROR)
+#define F_LBERROR    V_LBERROR(1U)
+
+#define S_LBSYNC    8
+#define V_LBSYNC(x) ((x) << S_LBSYNC)
+#define F_LBSYNC    V_LBSYNC(1U)
+
+#define S_FDWRAPCLK    7
+#define V_FDWRAPCLK(x) ((x) << S_FDWRAPCLK)
+#define F_FDWRAPCLK    V_FDWRAPCLK(1U)
+
+#define S_FDWRAP    6
+#define V_FDWRAP(x) ((x) << S_FDWRAP)
+#define F_FDWRAP    V_FDWRAP(1U)
+
+#define S_PRST    4
+#define V_PRST(x) ((x) << S_PRST)
+#define F_PRST    V_PRST(1U)
+
+#define S_PCHKEN    3
+#define V_PCHKEN(x) ((x) << S_PCHKEN)
+#define F_PCHKEN    V_PCHKEN(1U)
+
+#define S_PRBSSEL    0
+#define M_PRBSSEL    0x7U
+#define V_PRBSSEL(x) ((x) << S_PRBSSEL)
+#define G_PRBSSEL(x) (((x) >> S_PRBSSEL) & M_PRBSSEL)
+
+#define A_XGMAC_PORT_HSS_RXA_PH_ROTATOR_CTRL 0x1908
+
+#define S_FTHROT    12
+#define M_FTHROT    0xfU
+#define V_FTHROT(x) ((x) << S_FTHROT)
+#define G_FTHROT(x) (((x) >> S_FTHROT) & M_FTHROT)
+
+#define S_RTHROT    11
+#define V_RTHROT(x) ((x) << S_RTHROT)
+#define F_RTHROT    V_RTHROT(1U)
+
+#define S_FILTCTL    7
+#define M_FILTCTL    0xfU
+#define V_FILTCTL(x) ((x) << S_FILTCTL)
+#define G_FILTCTL(x) (((x) >> S_FILTCTL) & M_FILTCTL)
+
+#define S_RSRVO    5
+#define M_RSRVO    0x3U
+#define V_RSRVO(x) ((x) << S_RSRVO)
+#define G_RSRVO(x) (((x) >> S_RSRVO) & M_RSRVO)
+
+#define S_EXTEL    4
+#define V_EXTEL(x) ((x) << S_EXTEL)
+#define F_EXTEL    V_EXTEL(1U)
+
+#define S_RSTONSTUCK    3
+#define V_RSTONSTUCK(x) ((x) << S_RSTONSTUCK)
+#define F_RSTONSTUCK    V_RSTONSTUCK(1U)
+
+#define S_FREEZEFW    2
+#define V_FREEZEFW(x) ((x) << S_FREEZEFW)
+#define F_FREEZEFW    V_FREEZEFW(1U)
+
+#define S_RESETFW    1
+#define V_RESETFW(x) ((x) << S_RESETFW)
+#define F_RESETFW    V_RESETFW(1U)
+
+#define S_SSCENABLE    0
+#define V_SSCENABLE(x) ((x) << S_SSCENABLE)
+#define F_SSCENABLE    V_SSCENABLE(1U)
+
+#define A_XGMAC_PORT_HSS_RXA_PH_ROTATOR_OFFSET_CTRL 0x190c
+
+#define S_RSNP    11
+#define V_RSNP(x) ((x) << S_RSNP)
+#define F_RSNP    V_RSNP(1U)
+
+#define S_TSOEN    10
+#define V_TSOEN(x) ((x) << S_TSOEN)
+#define F_TSOEN    V_TSOEN(1U)
+
+#define S_OFFEN    9
+#define V_OFFEN(x) ((x) << S_OFFEN)
+#define F_OFFEN    V_OFFEN(1U)
+
+#define S_TMSCAL    7
+#define M_TMSCAL    0x3U
+#define V_TMSCAL(x) ((x) << S_TMSCAL)
+#define G_TMSCAL(x) (((x) >> S_TMSCAL) & M_TMSCAL)
+
+#define S_APADJ    6
+#define V_APADJ(x) ((x) << S_APADJ)
+#define F_APADJ    V_APADJ(1U)
+
+#define S_RSEL    5
+#define V_RSEL(x) ((x) << S_RSEL)
+#define F_RSEL    V_RSEL(1U)
+
+#define S_PHOFFS    0
+#define M_PHOFFS    0x1fU
+#define V_PHOFFS(x) ((x) << S_PHOFFS)
+#define G_PHOFFS(x) (((x) >> S_PHOFFS) & M_PHOFFS)
+
+#define A_XGMAC_PORT_HSS_RXA_PH_ROTATOR_POSITION1 0x1910
+
+#define S_ROT0A    8
+#define M_ROT0A    0x3fU
+#define V_ROT0A(x) ((x) << S_ROT0A)
+#define G_ROT0A(x) (((x) >> S_ROT0A) & M_ROT0A)
+
+#define S_RTSEL_SNAPSHOT    0
+#define M_RTSEL_SNAPSHOT    0x3fU
+#define V_RTSEL_SNAPSHOT(x) ((x) << S_RTSEL_SNAPSHOT)
+#define G_RTSEL_SNAPSHOT(x) (((x) >> S_RTSEL_SNAPSHOT) & M_RTSEL_SNAPSHOT)
+
+#define A_XGMAC_PORT_HSS_RXA_PH_ROTATOR_POSITION2 0x1914
+
+#define S_ROT90    0
+#define M_ROT90    0x3fU
+#define V_ROT90(x) ((x) << S_ROT90)
+#define G_ROT90(x) (((x) >> S_ROT90) & M_ROT90)
+
+#define A_XGMAC_PORT_HSS_RXA_PH_ROTATOR_STATIC_PH_OFFSET 0x1918
+
+#define S_RCALER    15
+#define V_RCALER(x) ((x) << S_RCALER)
+#define F_RCALER    V_RCALER(1U)
+
+#define S_RAOOFF    10
+#define M_RAOOFF    0x1fU
+#define V_RAOOFF(x) ((x) << S_RAOOFF)
+#define G_RAOOFF(x) (((x) >> S_RAOOFF) & M_RAOOFF)
+
+#define S_RAEOFF    5
+#define M_RAEOFF    0x1fU
+#define V_RAEOFF(x) ((x) << S_RAEOFF)
+#define G_RAEOFF(x) (((x) >> S_RAEOFF) & M_RAEOFF)
+
+#define S_RDOFF    0
+#define M_RDOFF    0x1fU
+#define V_RDOFF(x) ((x) << S_RDOFF)
+#define G_RDOFF(x) (((x) >> S_RDOFF) & M_RDOFF)
+
+#define A_XGMAC_PORT_HSS_RXA_SIGDET_CTRL 0x191c
+
+#define S_SIGNSD    13
+#define M_SIGNSD    0x3U
+#define V_SIGNSD(x) ((x) << S_SIGNSD)
+#define G_SIGNSD(x) (((x) >> S_SIGNSD) & M_SIGNSD)
+
+#define S_DACSD    8
+#define M_DACSD    0x1fU
+#define V_DACSD(x) ((x) << S_DACSD)
+#define G_DACSD(x) (((x) >> S_DACSD) & M_DACSD)
+
+#define S_SDPDN    6
+#define V_SDPDN(x) ((x) << S_SDPDN)
+#define F_SDPDN    V_SDPDN(1U)
+
+#define S_SIGDET    5
+#define V_SIGDET(x) ((x) << S_SIGDET)
+#define F_SIGDET    V_SIGDET(1U)
+
+#define S_SDLVL    0
+#define M_SDLVL    0x1fU
+#define V_SDLVL(x) ((x) << S_SDLVL)
+#define G_SDLVL(x) (((x) >> S_SDLVL) & M_SDLVL)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_CTRL 0x1920
+
+#define S_REQCMP    15
+#define V_REQCMP(x) ((x) << S_REQCMP)
+#define F_REQCMP    V_REQCMP(1U)
+
+#define S_DFEREQ    14
+#define V_DFEREQ(x) ((x) << S_DFEREQ)
+#define F_DFEREQ    V_DFEREQ(1U)
+
+#define S_SPCEN    13
+#define V_SPCEN(x) ((x) << S_SPCEN)
+#define F_SPCEN    V_SPCEN(1U)
+
+#define S_GATEEN    12
+#define V_GATEEN(x) ((x) << S_GATEEN)
+#define F_GATEEN    V_GATEEN(1U)
+
+#define S_SPIFMT    9
+#define M_SPIFMT    0x7U
+#define V_SPIFMT(x) ((x) << S_SPIFMT)
+#define G_SPIFMT(x) (((x) >> S_SPIFMT) & M_SPIFMT)
+
+#define S_DFEPWR    6
+#define M_DFEPWR    0x7U
+#define V_DFEPWR(x) ((x) << S_DFEPWR)
+#define G_DFEPWR(x) (((x) >> S_DFEPWR) & M_DFEPWR)
+
+#define S_STNDBY    5
+#define V_STNDBY(x) ((x) << S_STNDBY)
+#define F_STNDBY    V_STNDBY(1U)
+
+#define S_FRCH    4
+#define V_FRCH(x) ((x) << S_FRCH)
+#define F_FRCH    V_FRCH(1U)
+
+#define S_NONRND    3
+#define V_NONRND(x) ((x) << S_NONRND)
+#define F_NONRND    V_NONRND(1U)
+
+#define S_NONRNF    2
+#define V_NONRNF(x) ((x) << S_NONRNF)
+#define F_NONRNF    V_NONRNF(1U)
+
+#define S_FSTLCK    1
+#define V_FSTLCK(x) ((x) << S_FSTLCK)
+#define F_FSTLCK    V_FSTLCK(1U)
+
+#define S_DFERST    0
+#define V_DFERST(x) ((x) << S_DFERST)
+#define F_DFERST    V_DFERST(1U)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_DATA_EDGE_SAMPLE 0x1924
+
+#define S_ESAMP    8
+#define M_ESAMP    0xffU
+#define V_ESAMP(x) ((x) << S_ESAMP)
+#define G_ESAMP(x) (((x) >> S_ESAMP) & M_ESAMP)
+
+#define S_DSAMP    0
+#define M_DSAMP    0xffU
+#define V_DSAMP(x) ((x) << S_DSAMP)
+#define G_DSAMP(x) (((x) >> S_DSAMP) & M_DSAMP)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_AMP_SAMPLE 0x1928
+
+#define S_SMODE    8
+#define M_SMODE    0xfU
+#define V_SMODE(x) ((x) << S_SMODE)
+#define G_SMODE(x) (((x) >> S_SMODE) & M_SMODE)
+
+#define S_ADCORR    7
+#define V_ADCORR(x) ((x) << S_ADCORR)
+#define F_ADCORR    V_ADCORR(1U)
+
+#define S_TRAINEN    6
+#define V_TRAINEN(x) ((x) << S_TRAINEN)
+#define F_TRAINEN    V_TRAINEN(1U)
+
+#define S_ASAMPQ    3
+#define M_ASAMPQ    0x7U
+#define V_ASAMPQ(x) ((x) << S_ASAMPQ)
+#define G_ASAMPQ(x) (((x) >> S_ASAMPQ) & M_ASAMPQ)
+
+#define S_ASAMP    0
+#define M_ASAMP    0x7U
+#define V_ASAMP(x) ((x) << S_ASAMP)
+#define G_ASAMP(x) (((x) >> S_ASAMP) & M_ASAMP)
+
+#define A_XGMAC_PORT_HSS_RXA_VGA_CTRL1 0x192c
+
+#define S_POLE    12
+#define M_POLE    0x3U
+#define V_POLE(x) ((x) << S_POLE)
+#define G_POLE(x) (((x) >> S_POLE) & M_POLE)
+
+#define S_PEAK    8
+#define M_PEAK    0x7U
+#define V_PEAK(x) ((x) << S_PEAK)
+#define G_PEAK(x) (((x) >> S_PEAK) & M_PEAK)
+
+#define S_VOFFSN    6
+#define M_VOFFSN    0x3U
+#define V_VOFFSN(x) ((x) << S_VOFFSN)
+#define G_VOFFSN(x) (((x) >> S_VOFFSN) & M_VOFFSN)
+
+#define S_VOFFA    0
+#define M_VOFFA    0x3fU
+#define V_VOFFA(x) ((x) << S_VOFFA)
+#define G_VOFFA(x) (((x) >> S_VOFFA) & M_VOFFA)
+
+#define A_XGMAC_PORT_HSS_RXA_VGA_CTRL2 0x1930
+
+#define S_SHORTV    10
+#define V_SHORTV(x) ((x) << S_SHORTV)
+#define F_SHORTV    V_SHORTV(1U)
+
+#define S_VGAIN    0
+#define M_VGAIN    0xfU
+#define V_VGAIN(x) ((x) << S_VGAIN)
+#define G_VGAIN(x) (((x) >> S_VGAIN) & M_VGAIN)
+
+#define A_XGMAC_PORT_HSS_RXA_VGA_CTRL3 0x1934
+
+#define S_HBND1    10
+#define V_HBND1(x) ((x) << S_HBND1)
+#define F_HBND1    V_HBND1(1U)
+
+#define S_HBND0    9
+#define V_HBND0(x) ((x) << S_HBND0)
+#define F_HBND0    V_HBND0(1U)
+
+#define S_VLCKD    8
+#define V_VLCKD(x) ((x) << S_VLCKD)
+#define F_VLCKD    V_VLCKD(1U)
+
+#define S_VLCKDF    7
+#define V_VLCKDF(x) ((x) << S_VLCKDF)
+#define F_VLCKDF    V_VLCKDF(1U)
+
+#define S_AMAXT    0
+#define M_AMAXT    0x7fU
+#define V_AMAXT(x) ((x) << S_AMAXT)
+#define G_AMAXT(x) (((x) >> S_AMAXT) & M_AMAXT)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_D00_D01_OFFSET 0x1938
+
+#define S_D01SN    13
+#define M_D01SN    0x3U
+#define V_D01SN(x) ((x) << S_D01SN)
+#define G_D01SN(x) (((x) >> S_D01SN) & M_D01SN)
+
+#define S_D01AMP    8
+#define M_D01AMP    0x1fU
+#define V_D01AMP(x) ((x) << S_D01AMP)
+#define G_D01AMP(x) (((x) >> S_D01AMP) & M_D01AMP)
+
+#define S_D00SN    5
+#define M_D00SN    0x3U
+#define V_D00SN(x) ((x) << S_D00SN)
+#define G_D00SN(x) (((x) >> S_D00SN) & M_D00SN)
+
+#define S_D00AMP    0
+#define M_D00AMP    0x1fU
+#define V_D00AMP(x) ((x) << S_D00AMP)
+#define G_D00AMP(x) (((x) >> S_D00AMP) & M_D00AMP)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_D10_D11_OFFSET 0x193c
+
+#define S_D11SN    13
+#define M_D11SN    0x3U
+#define V_D11SN(x) ((x) << S_D11SN)
+#define G_D11SN(x) (((x) >> S_D11SN) & M_D11SN)
+
+#define S_D11AMP    8
+#define M_D11AMP    0x1fU
+#define V_D11AMP(x) ((x) << S_D11AMP)
+#define G_D11AMP(x) (((x) >> S_D11AMP) & M_D11AMP)
+
+#define S_D10SN    5
+#define M_D10SN    0x3U
+#define V_D10SN(x) ((x) << S_D10SN)
+#define G_D10SN(x) (((x) >> S_D10SN) & M_D10SN)
+
+#define S_D10AMP    0
+#define M_D10AMP    0x1fU
+#define V_D10AMP(x) ((x) << S_D10AMP)
+#define G_D10AMP(x) (((x) >> S_D10AMP) & M_D10AMP)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_E0_E1_OFFSET 0x1940
+
+#define S_E1SN    13
+#define M_E1SN    0x3U
+#define V_E1SN(x) ((x) << S_E1SN)
+#define G_E1SN(x) (((x) >> S_E1SN) & M_E1SN)
+
+#define S_E1AMP    8
+#define M_E1AMP    0x1fU
+#define V_E1AMP(x) ((x) << S_E1AMP)
+#define G_E1AMP(x) (((x) >> S_E1AMP) & M_E1AMP)
+
+#define S_E0SN    5
+#define M_E0SN    0x3U
+#define V_E0SN(x) ((x) << S_E0SN)
+#define G_E0SN(x) (((x) >> S_E0SN) & M_E0SN)
+
+#define S_E0AMP    0
+#define M_E0AMP    0x1fU
+#define V_E0AMP(x) ((x) << S_E0AMP)
+#define G_E0AMP(x) (((x) >> S_E0AMP) & M_E0AMP)
+
+#define A_XGMAC_PORT_HSS_RXA_DACA_OFFSET 0x1944
+
+#define S_AOFFO    8
+#define M_AOFFO    0x3fU
+#define V_AOFFO(x) ((x) << S_AOFFO)
+#define G_AOFFO(x) (((x) >> S_AOFFO) & M_AOFFO)
+
+#define S_AOFFE    0
+#define M_AOFFE    0x3fU
+#define V_AOFFE(x) ((x) << S_AOFFE)
+#define G_AOFFE(x) (((x) >> S_AOFFE) & M_AOFFE)
+
+#define A_XGMAC_PORT_HSS_RXA_DACAP_DAC_AN_OFFSET 0x1948
+
+#define S_DACAN    8
+#define M_DACAN    0xffU
+#define V_DACAN(x) ((x) << S_DACAN)
+#define G_DACAN(x) (((x) >> S_DACAN) & M_DACAN)
+
+#define S_DACAP    0
+#define M_DACAP    0xffU
+#define V_DACAP(x) ((x) << S_DACAP)
+#define G_DACAP(x) (((x) >> S_DACAP) & M_DACAP)
+
+#define A_XGMAC_PORT_HSS_RXA_DACA_MIN 0x194c
+
+#define S_DACAZ    8
+#define M_DACAZ    0xffU
+#define V_DACAZ(x) ((x) << S_DACAZ)
+#define G_DACAZ(x) (((x) >> S_DACAZ) & M_DACAZ)
+
+#define S_DACAM    0
+#define M_DACAM    0xffU
+#define V_DACAM(x) ((x) << S_DACAM)
+#define G_DACAM(x) (((x) >> S_DACAM) & M_DACAM)
+
+#define A_XGMAC_PORT_HSS_RXA_ADAC_CTRL 0x1950
+
+#define S_ADSN    7
+#define M_ADSN    0x3U
+#define V_ADSN(x) ((x) << S_ADSN)
+#define G_ADSN(x) (((x) >> S_ADSN) & M_ADSN)
+
+#define S_ADMAG    0
+#define M_ADMAG    0x7fU
+#define V_ADMAG(x) ((x) << S_ADMAG)
+#define G_ADMAG(x) (((x) >> S_ADMAG) & M_ADMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DIGITAL_EYE_CTRL 0x1954
+
+#define S_BLKAZ    15
+#define V_BLKAZ(x) ((x) << S_BLKAZ)
+#define F_BLKAZ    V_BLKAZ(1U)
+
+#define S_WIDTH    10
+#define M_WIDTH    0x1fU
+#define V_WIDTH(x) ((x) << S_WIDTH)
+#define G_WIDTH(x) (((x) >> S_WIDTH) & M_WIDTH)
+
+#define S_MINWIDTH    5
+#define M_MINWIDTH    0x1fU
+#define V_MINWIDTH(x) ((x) << S_MINWIDTH)
+#define G_MINWIDTH(x) (((x) >> S_MINWIDTH) & M_MINWIDTH)
+
+#define S_MINAMP    0
+#define M_MINAMP    0x1fU
+#define V_MINAMP(x) ((x) << S_MINAMP)
+#define G_MINAMP(x) (((x) >> S_MINAMP) & M_MINAMP)
+
+#define A_XGMAC_PORT_HSS_RXA_DIGITAL_EYE_METRICS 0x1958
+
+#define S_EMBRDY    10
+#define V_EMBRDY(x) ((x) << S_EMBRDY)
+#define F_EMBRDY    V_EMBRDY(1U)
+
+#define S_EMBUMP    7
+#define V_EMBUMP(x) ((x) << S_EMBUMP)
+#define F_EMBUMP    V_EMBUMP(1U)
+
+#define S_EMMD    5
+#define M_EMMD    0x3U
+#define V_EMMD(x) ((x) << S_EMMD)
+#define G_EMMD(x) (((x) >> S_EMMD) & M_EMMD)
+
+#define S_EMPAT    1
+#define V_EMPAT(x) ((x) << S_EMPAT)
+#define F_EMPAT    V_EMPAT(1U)
+
+#define S_EMEN    0
+#define V_EMEN(x) ((x) << S_EMEN)
+#define F_EMEN    V_EMEN(1U)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_H1 0x195c
+
+#define S_H1OSN    14
+#define M_H1OSN    0x3U
+#define V_H1OSN(x) ((x) << S_H1OSN)
+#define G_H1OSN(x) (((x) >> S_H1OSN) & M_H1OSN)
+
+#define S_H1OMAG    8
+#define M_H1OMAG    0x3fU
+#define V_H1OMAG(x) ((x) << S_H1OMAG)
+#define G_H1OMAG(x) (((x) >> S_H1OMAG) & M_H1OMAG)
+
+#define S_H1ESN    6
+#define M_H1ESN    0x3U
+#define V_H1ESN(x) ((x) << S_H1ESN)
+#define G_H1ESN(x) (((x) >> S_H1ESN) & M_H1ESN)
+
+#define S_H1EMAG    0
+#define M_H1EMAG    0x3fU
+#define V_H1EMAG(x) ((x) << S_H1EMAG)
+#define G_H1EMAG(x) (((x) >> S_H1EMAG) & M_H1EMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_H2 0x1960
+
+#define S_H2OSN    13
+#define M_H2OSN    0x3U
+#define V_H2OSN(x) ((x) << S_H2OSN)
+#define G_H2OSN(x) (((x) >> S_H2OSN) & M_H2OSN)
+
+#define S_H2OMAG    8
+#define M_H2OMAG    0x1fU
+#define V_H2OMAG(x) ((x) << S_H2OMAG)
+#define G_H2OMAG(x) (((x) >> S_H2OMAG) & M_H2OMAG)
+
+#define S_H2ESN    5
+#define M_H2ESN    0x3U
+#define V_H2ESN(x) ((x) << S_H2ESN)
+#define G_H2ESN(x) (((x) >> S_H2ESN) & M_H2ESN)
+
+#define S_H2EMAG    0
+#define M_H2EMAG    0x1fU
+#define V_H2EMAG(x) ((x) << S_H2EMAG)
+#define G_H2EMAG(x) (((x) >> S_H2EMAG) & M_H2EMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_H3 0x1964
+
+#define S_H3OSN    12
+#define M_H3OSN    0x3U
+#define V_H3OSN(x) ((x) << S_H3OSN)
+#define G_H3OSN(x) (((x) >> S_H3OSN) & M_H3OSN)
+
+#define S_H3OMAG    8
+#define M_H3OMAG    0xfU
+#define V_H3OMAG(x) ((x) << S_H3OMAG)
+#define G_H3OMAG(x) (((x) >> S_H3OMAG) & M_H3OMAG)
+
+#define S_H3ESN    4
+#define M_H3ESN    0x3U
+#define V_H3ESN(x) ((x) << S_H3ESN)
+#define G_H3ESN(x) (((x) >> S_H3ESN) & M_H3ESN)
+
+#define S_H3EMAG    0
+#define M_H3EMAG    0xfU
+#define V_H3EMAG(x) ((x) << S_H3EMAG)
+#define G_H3EMAG(x) (((x) >> S_H3EMAG) & M_H3EMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_H4 0x1968
+
+#define S_H4OSN    12
+#define M_H4OSN    0x3U
+#define V_H4OSN(x) ((x) << S_H4OSN)
+#define G_H4OSN(x) (((x) >> S_H4OSN) & M_H4OSN)
+
+#define S_H4OMAG    8
+#define M_H4OMAG    0xfU
+#define V_H4OMAG(x) ((x) << S_H4OMAG)
+#define G_H4OMAG(x) (((x) >> S_H4OMAG) & M_H4OMAG)
+
+#define S_H4ESN    4
+#define M_H4ESN    0x3U
+#define V_H4ESN(x) ((x) << S_H4ESN)
+#define G_H4ESN(x) (((x) >> S_H4ESN) & M_H4ESN)
+
+#define S_H4EMAG    0
+#define M_H4EMAG    0xfU
+#define V_H4EMAG(x) ((x) << S_H4EMAG)
+#define G_H4EMAG(x) (((x) >> S_H4EMAG) & M_H4EMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_H5 0x196c
+
+#define S_H5OSN    12
+#define M_H5OSN    0x3U
+#define V_H5OSN(x) ((x) << S_H5OSN)
+#define G_H5OSN(x) (((x) >> S_H5OSN) & M_H5OSN)
+
+#define S_H5OMAG    8
+#define M_H5OMAG    0xfU
+#define V_H5OMAG(x) ((x) << S_H5OMAG)
+#define G_H5OMAG(x) (((x) >> S_H5OMAG) & M_H5OMAG)
+
+#define S_H5ESN    4
+#define M_H5ESN    0x3U
+#define V_H5ESN(x) ((x) << S_H5ESN)
+#define G_H5ESN(x) (((x) >> S_H5ESN) & M_H5ESN)
+
+#define S_H5EMAG    0
+#define M_H5EMAG    0xfU
+#define V_H5EMAG(x) ((x) << S_H5EMAG)
+#define G_H5EMAG(x) (((x) >> S_H5EMAG) & M_H5EMAG)
+
+#define A_XGMAC_PORT_HSS_RXA_DAC_DPC 0x1970
+
+#define S_DPCCVG    13
+#define V_DPCCVG(x) ((x) << S_DPCCVG)
+#define F_DPCCVG    V_DPCCVG(1U)
+
+#define S_DACCVG    12
+#define V_DACCVG(x) ((x) << S_DACCVG)
+#define F_DACCVG    V_DACCVG(1U)
+
+#define S_DPCTGT    9
+#define M_DPCTGT    0x7U
+#define V_DPCTGT(x) ((x) << S_DPCTGT)
+#define G_DPCTGT(x) (((x) >> S_DPCTGT) & M_DPCTGT)
+
+#define S_BLKH1T    8
+#define V_BLKH1T(x) ((x) << S_BLKH1T)
+#define F_BLKH1T    V_BLKH1T(1U)
+
+#define S_BLKOAE    7
+#define V_BLKOAE(x) ((x) << S_BLKOAE)
+#define F_BLKOAE    V_BLKOAE(1U)
+
+#define S_H1TGT    4
+#define M_H1TGT    0x7U
+#define V_H1TGT(x) ((x) << S_H1TGT)
+#define G_H1TGT(x) (((x) >> S_H1TGT) & M_H1TGT)
+
+#define S_OAE    0
+#define M_OAE    0xfU
+#define V_OAE(x) ((x) << S_OAE)
+#define G_OAE(x) (((x) >> S_OAE) & M_OAE)
+
+#define A_XGMAC_PORT_HSS_RXA_DDC 0x1974
+
+#define S_OLS    11
+#define M_OLS    0x1fU
+#define V_OLS(x) ((x) << S_OLS)
+#define G_OLS(x) (((x) >> S_OLS) & M_OLS)
+
+#define S_OES    6
+#define M_OES    0x1fU
+#define V_OES(x) ((x) << S_OES)
+#define G_OES(x) (((x) >> S_OES) & M_OES)
+
+#define S_BLKODEC    5
+#define V_BLKODEC(x) ((x) << S_BLKODEC)
+#define F_BLKODEC    V_BLKODEC(1U)
+
+#define S_ODEC    0
+#define M_ODEC    0x1fU
+#define V_ODEC(x) ((x) << S_ODEC)
+#define G_ODEC(x) (((x) >> S_ODEC) & M_ODEC)
+
+#define A_XGMAC_PORT_HSS_RXA_INTERNAL_STATUS 0x1978
+
+#define S_BER6    15
+#define V_BER6(x) ((x) << S_BER6)
+#define F_BER6    V_BER6(1U)
+
+#define S_BER6VAL    14
+#define V_BER6VAL(x) ((x) << S_BER6VAL)
+#define F_BER6VAL    V_BER6VAL(1U)
+
+#define S_BER3VAL    13
+#define V_BER3VAL(x) ((x) << S_BER3VAL)
+#define F_BER3VAL    V_BER3VAL(1U)
+
+#define S_DPCCMP    9
+#define V_DPCCMP(x) ((x) << S_DPCCMP)
+#define F_DPCCMP    V_DPCCMP(1U)
+
+#define S_DACCMP    8
+#define V_DACCMP(x) ((x) << S_DACCMP)
+#define F_DACCMP    V_DACCMP(1U)
+
+#define S_DDCCMP    7
+#define V_DDCCMP(x) ((x) << S_DDCCMP)
+#define F_DDCCMP    V_DDCCMP(1U)
+
+#define S_AERRFLG    6
+#define V_AERRFLG(x) ((x) << S_AERRFLG)
+#define F_AERRFLG    V_AERRFLG(1U)
+
+#define S_WERRFLG    5
+#define V_WERRFLG(x) ((x) << S_WERRFLG)
+#define F_WERRFLG    V_WERRFLG(1U)
+
+#define S_TRCMP    4
+#define V_TRCMP(x) ((x) << S_TRCMP)
+#define F_TRCMP    V_TRCMP(1U)
+
+#define S_VLCKF    3
+#define V_VLCKF(x) ((x) << S_VLCKF)
+#define F_VLCKF    V_VLCKF(1U)
+
+#define S_ROCADJ    2
+#define V_ROCADJ(x) ((x) << S_ROCADJ)
+#define F_ROCADJ    V_ROCADJ(1U)
+
+#define S_ROCCMP    1
+#define V_ROCCMP(x) ((x) << S_ROCCMP)
+#define F_ROCCMP    V_ROCCMP(1U)
+
+#define S_OCCMP    0
+#define V_OCCMP(x) ((x) << S_OCCMP)
+#define F_OCCMP    V_OCCMP(1U)
+
+#define A_XGMAC_PORT_HSS_RXA_DFE_FUNC_CTRL 0x197c
+
+#define S_FDPC    15
+#define V_FDPC(x) ((x) << S_FDPC)
+#define F_FDPC    V_FDPC(1U)
+
+#define S_FDAC    14
+#define V_FDAC(x) ((x) << S_FDAC)
+#define F_FDAC    V_FDAC(1U)
+
+#define S_FDDC    13
+#define V_FDDC(x) ((x) << S_FDDC)
+#define F_FDDC    V_FDDC(1U)
+
+#define S_FNRND    12
+#define V_FNRND(x) ((x) << S_FNRND)
+#define F_FNRND    V_FNRND(1U)
+
+#define S_FVGAIN    11
+#define V_FVGAIN(x) ((x) << S_FVGAIN)
+#define F_FVGAIN    V_FVGAIN(1U)
+
+#define S_FVOFF    10
+#define V_FVOFF(x) ((x) << S_FVOFF)
+#define F_FVOFF    V_FVOFF(1U)
+
+#define S_FSDET    9
+#define V_FSDET(x) ((x) << S_FSDET)
+#define F_FSDET    V_FSDET(1U)
+
+#define S_FBER6    8
+#define V_FBER6(x) ((x) << S_FBER6)
+#define F_FBER6    V_FBER6(1U)
+
+#define S_FROTO    7
+#define V_FROTO(x) ((x) << S_FROTO)
+#define F_FROTO    V_FROTO(1U)
+
+#define S_FH4H5    6
+#define V_FH4H5(x) ((x) << S_FH4H5)
+#define F_FH4H5    V_FH4H5(1U)
+
+#define S_FH2H3    5
+#define V_FH2H3(x) ((x) << S_FH2H3)
+#define F_FH2H3    V_FH2H3(1U)
+
+#define S_FH1    4
+#define V_FH1(x) ((x) << S_FH1)
+#define F_FH1    V_FH1(1U)
+
+#define S_FH1SN    3
+#define V_FH1SN(x) ((x) << S_FH1SN)
+#define F_FH1SN    V_FH1SN(1U)
+
+#define S_FNRDF    2
+#define V_FNRDF(x) ((x) << S_FNRDF)
+#define F_FNRDF    V_FNRDF(1U)
+
+#define S_FADAC    0
+#define V_FADAC(x) ((x) << S_FADAC)
+#define F_FADAC    V_FADAC(1U)
+
+#define A_XGMAC_PORT_HSS_RXB_CFG_MODE 0x1980
+#define A_XGMAC_PORT_HSS_RXB_TEST_CTRL 0x1984
+#define A_XGMAC_PORT_HSS_RXB_PH_ROTATOR_CTRL 0x1988
+#define A_XGMAC_PORT_HSS_RXB_PH_ROTATOR_OFFSET_CTRL 0x198c
+#define A_XGMAC_PORT_HSS_RXB_PH_ROTATOR_POSITION1 0x1990
+#define A_XGMAC_PORT_HSS_RXB_PH_ROTATOR_POSITION2 0x1994
+#define A_XGMAC_PORT_HSS_RXB_PH_ROTATOR_STATIC_PH_OFFSET 0x1998
+#define A_XGMAC_PORT_HSS_RXB_SIGDET_CTRL 0x199c
+#define A_XGMAC_PORT_HSS_RXB_DFE_CTRL 0x19a0
+#define A_XGMAC_PORT_HSS_RXB_DFE_DATA_EDGE_SAMPLE 0x19a4
+#define A_XGMAC_PORT_HSS_RXB_DFE_AMP_SAMPLE 0x19a8
+#define A_XGMAC_PORT_HSS_RXB_VGA_CTRL1 0x19ac
+#define A_XGMAC_PORT_HSS_RXB_VGA_CTRL2 0x19b0
+#define A_XGMAC_PORT_HSS_RXB_VGA_CTRL3 0x19b4
+#define A_XGMAC_PORT_HSS_RXB_DFE_D00_D01_OFFSET 0x19b8
+#define A_XGMAC_PORT_HSS_RXB_DFE_D10_D11_OFFSET 0x19bc
+#define A_XGMAC_PORT_HSS_RXB_DFE_E0_E1_OFFSET 0x19c0
+#define A_XGMAC_PORT_HSS_RXB_DACA_OFFSET 0x19c4
+#define A_XGMAC_PORT_HSS_RXB_DACAP_DAC_AN_OFFSET 0x19c8
+#define A_XGMAC_PORT_HSS_RXB_DACA_MIN 0x19cc
+#define A_XGMAC_PORT_HSS_RXB_ADAC_CTRL 0x19d0
+#define A_XGMAC_PORT_HSS_RXB_DIGITAL_EYE_CTRL 0x19d4
+#define A_XGMAC_PORT_HSS_RXB_DIGITAL_EYE_METRICS 0x19d8
+#define A_XGMAC_PORT_HSS_RXB_DFE_H1 0x19dc
+#define A_XGMAC_PORT_HSS_RXB_DFE_H2 0x19e0
+#define A_XGMAC_PORT_HSS_RXB_DFE_H3 0x19e4
+#define A_XGMAC_PORT_HSS_RXB_DFE_H4 0x19e8
+#define A_XGMAC_PORT_HSS_RXB_DFE_H5 0x19ec
+#define A_XGMAC_PORT_HSS_RXB_DAC_DPC 0x19f0
+#define A_XGMAC_PORT_HSS_RXB_DDC 0x19f4
+#define A_XGMAC_PORT_HSS_RXB_INTERNAL_STATUS 0x19f8
+#define A_XGMAC_PORT_HSS_RXB_DFE_FUNC_CTRL 0x19fc
+#define A_XGMAC_PORT_HSS_TXC_MODE_CFG 0x1a00
+#define A_XGMAC_PORT_HSS_TXC_TEST_CTRL 0x1a04
+#define A_XGMAC_PORT_HSS_TXC_COEFF_CTRL 0x1a08
+#define A_XGMAC_PORT_HSS_TXC_DRIVER_MODE 0x1a0c
+#define A_XGMAC_PORT_HSS_TXC_DRIVER_OVR_CTRL 0x1a10
+#define A_XGMAC_PORT_HSS_TXC_TDM_BIASGEN_STANDBY_TIMER 0x1a14
+#define A_XGMAC_PORT_HSS_TXC_TDM_BIASGEN_PWRON_TIMER 0x1a18
+#define A_XGMAC_PORT_HSS_TXC_TAP0_COEFF 0x1a20
+#define A_XGMAC_PORT_HSS_TXC_TAP1_COEFF 0x1a24
+#define A_XGMAC_PORT_HSS_TXC_TAP2_COEFF 0x1a28
+#define A_XGMAC_PORT_HSS_TXC_PWR 0x1a30
+#define A_XGMAC_PORT_HSS_TXC_POLARITY 0x1a34
+#define A_XGMAC_PORT_HSS_TXC_8023AP_AE_CMD 0x1a38
+#define A_XGMAC_PORT_HSS_TXC_8023AP_AE_STATUS 0x1a3c
+#define A_XGMAC_PORT_HSS_TXC_TAP0_IDAC_OVR 0x1a40
+#define A_XGMAC_PORT_HSS_TXC_TAP1_IDAC_OVR 0x1a44
+#define A_XGMAC_PORT_HSS_TXC_TAP2_IDAC_OVR 0x1a48
+#define A_XGMAC_PORT_HSS_TXC_PWR_DAC_OVR 0x1a50
+#define A_XGMAC_PORT_HSS_TXC_PWR_DAC 0x1a54
+#define A_XGMAC_PORT_HSS_TXC_TAP0_IDAC_APP 0x1a60
+#define A_XGMAC_PORT_HSS_TXC_TAP1_IDAC_APP 0x1a64
+#define A_XGMAC_PORT_HSS_TXC_TAP2_IDAC_APP 0x1a68
+#define A_XGMAC_PORT_HSS_TXC_SEG_DIS_APP 0x1a70
+#define A_XGMAC_PORT_HSS_TXC_EXT_ADDR_DATA 0x1a78
+#define A_XGMAC_PORT_HSS_TXC_EXT_ADDR 0x1a7c
+#define A_XGMAC_PORT_HSS_TXD_MODE_CFG 0x1a80
+#define A_XGMAC_PORT_HSS_TXD_TEST_CTRL 0x1a84
+#define A_XGMAC_PORT_HSS_TXD_COEFF_CTRL 0x1a88
+#define A_XGMAC_PORT_HSS_TXD_DRIVER_MODE 0x1a8c
+#define A_XGMAC_PORT_HSS_TXD_DRIVER_OVR_CTRL 0x1a90
+#define A_XGMAC_PORT_HSS_TXD_TDM_BIASGEN_STANDBY_TIMER 0x1a94
+#define A_XGMAC_PORT_HSS_TXD_TDM_BIASGEN_PWRON_TIMER 0x1a98
+#define A_XGMAC_PORT_HSS_TXD_TAP0_COEFF 0x1aa0
+#define A_XGMAC_PORT_HSS_TXD_TAP1_COEFF 0x1aa4
+#define A_XGMAC_PORT_HSS_TXD_TAP2_COEFF 0x1aa8
+#define A_XGMAC_PORT_HSS_TXD_PWR 0x1ab0
+#define A_XGMAC_PORT_HSS_TXD_POLARITY 0x1ab4
+#define A_XGMAC_PORT_HSS_TXD_8023AP_AE_CMD 0x1ab8
+#define A_XGMAC_PORT_HSS_TXD_8023AP_AE_STATUS 0x1abc
+#define A_XGMAC_PORT_HSS_TXD_TAP0_IDAC_OVR 0x1ac0
+#define A_XGMAC_PORT_HSS_TXD_TAP1_IDAC_OVR 0x1ac4
+#define A_XGMAC_PORT_HSS_TXD_TAP2_IDAC_OVR 0x1ac8
+#define A_XGMAC_PORT_HSS_TXD_PWR_DAC_OVR 0x1ad0
+#define A_XGMAC_PORT_HSS_TXD_PWR_DAC 0x1ad4
+#define A_XGMAC_PORT_HSS_TXD_TAP0_IDAC_APP 0x1ae0
+#define A_XGMAC_PORT_HSS_TXD_TAP1_IDAC_APP 0x1ae4
+#define A_XGMAC_PORT_HSS_TXD_TAP2_IDAC_APP 0x1ae8
+#define A_XGMAC_PORT_HSS_TXD_SEG_DIS_APP 0x1af0
+#define A_XGMAC_PORT_HSS_TXD_EXT_ADDR_DATA 0x1af8
+#define A_XGMAC_PORT_HSS_TXD_EXT_ADDR 0x1afc
+#define A_XGMAC_PORT_HSS_RXC_CFG_MODE 0x1b00
+#define A_XGMAC_PORT_HSS_RXC_TEST_CTRL 0x1b04
+#define A_XGMAC_PORT_HSS_RXC_PH_ROTATOR_CTRL 0x1b08
+#define A_XGMAC_PORT_HSS_RXC_PH_ROTATOR_OFFSET_CTRL 0x1b0c
+#define A_XGMAC_PORT_HSS_RXC_PH_ROTATOR_POSITION1 0x1b10
+#define A_XGMAC_PORT_HSS_RXC_PH_ROTATOR_POSITION2 0x1b14
+#define A_XGMAC_PORT_HSS_RXC_PH_ROTATOR_STATIC_PH_OFFSET 0x1b18
+#define A_XGMAC_PORT_HSS_RXC_SIGDET_CTRL 0x1b1c
+#define A_XGMAC_PORT_HSS_RXC_DFE_CTRL 0x1b20
+#define A_XGMAC_PORT_HSS_RXC_DFE_DATA_EDGE_SAMPLE 0x1b24
+#define A_XGMAC_PORT_HSS_RXC_DFE_AMP_SAMPLE 0x1b28
+#define A_XGMAC_PORT_HSS_RXC_VGA_CTRL1 0x1b2c
+#define A_XGMAC_PORT_HSS_RXC_VGA_CTRL2 0x1b30
+#define A_XGMAC_PORT_HSS_RXC_VGA_CTRL3 0x1b34
+#define A_XGMAC_PORT_HSS_RXC_DFE_D00_D01_OFFSET 0x1b38
+#define A_XGMAC_PORT_HSS_RXC_DFE_D10_D11_OFFSET 0x1b3c
+#define A_XGMAC_PORT_HSS_RXC_DFE_E0_E1_OFFSET 0x1b40
+#define A_XGMAC_PORT_HSS_RXC_DACA_OFFSET 0x1b44
+#define A_XGMAC_PORT_HSS_RXC_DACAP_DAC_AN_OFFSET 0x1b48
+#define A_XGMAC_PORT_HSS_RXC_DACA_MIN 0x1b4c
+#define A_XGMAC_PORT_HSS_RXC_ADAC_CTRL 0x1b50
+#define A_XGMAC_PORT_HSS_RXC_DIGITAL_EYE_CTRL 0x1b54
+#define A_XGMAC_PORT_HSS_RXC_DIGITAL_EYE_METRICS 0x1b58
+#define A_XGMAC_PORT_HSS_RXC_DFE_H1 0x1b5c
+#define A_XGMAC_PORT_HSS_RXC_DFE_H2 0x1b60
+#define A_XGMAC_PORT_HSS_RXC_DFE_H3 0x1b64
+#define A_XGMAC_PORT_HSS_RXC_DFE_H4 0x1b68
+#define A_XGMAC_PORT_HSS_RXC_DFE_H5 0x1b6c
+#define A_XGMAC_PORT_HSS_RXC_DAC_DPC 0x1b70
+#define A_XGMAC_PORT_HSS_RXC_DDC 0x1b74
+#define A_XGMAC_PORT_HSS_RXC_INTERNAL_STATUS 0x1b78
+#define A_XGMAC_PORT_HSS_RXC_DFE_FUNC_CTRL 0x1b7c
+#define A_XGMAC_PORT_HSS_RXD_CFG_MODE 0x1b80
+#define A_XGMAC_PORT_HSS_RXD_TEST_CTRL 0x1b84
+#define A_XGMAC_PORT_HSS_RXD_PH_ROTATOR_CTRL 0x1b88
+#define A_XGMAC_PORT_HSS_RXD_PH_ROTATOR_OFFSET_CTRL 0x1b8c
+#define A_XGMAC_PORT_HSS_RXD_PH_ROTATOR_POSITION1 0x1b90
+#define A_XGMAC_PORT_HSS_RXD_PH_ROTATOR_POSITION2 0x1b94
+#define A_XGMAC_PORT_HSS_RXD_PH_ROTATOR_STATIC_PH_OFFSET 0x1b98
+#define A_XGMAC_PORT_HSS_RXD_SIGDET_CTRL 0x1b9c
+#define A_XGMAC_PORT_HSS_RXD_DFE_CTRL 0x1ba0
+#define A_XGMAC_PORT_HSS_RXD_DFE_DATA_EDGE_SAMPLE 0x1ba4
+#define A_XGMAC_PORT_HSS_RXD_DFE_AMP_SAMPLE 0x1ba8
+#define A_XGMAC_PORT_HSS_RXD_VGA_CTRL1 0x1bac
+#define A_XGMAC_PORT_HSS_RXD_VGA_CTRL2 0x1bb0
+#define A_XGMAC_PORT_HSS_RXD_VGA_CTRL3 0x1bb4
+#define A_XGMAC_PORT_HSS_RXD_DFE_D00_D01_OFFSET 0x1bb8
+#define A_XGMAC_PORT_HSS_RXD_DFE_D10_D11_OFFSET 0x1bbc
+#define A_XGMAC_PORT_HSS_RXD_DFE_E0_E1_OFFSET 0x1bc0
+#define A_XGMAC_PORT_HSS_RXD_DACA_OFFSET 0x1bc4
+#define A_XGMAC_PORT_HSS_RXD_DACAP_DAC_AN_OFFSET 0x1bc8
+#define A_XGMAC_PORT_HSS_RXD_DACA_MIN 0x1bcc
+#define A_XGMAC_PORT_HSS_RXD_ADAC_CTRL 0x1bd0
+#define A_XGMAC_PORT_HSS_RXD_DIGITAL_EYE_CTRL 0x1bd4
+#define A_XGMAC_PORT_HSS_RXD_DIGITAL_EYE_METRICS 0x1bd8
+#define A_XGMAC_PORT_HSS_RXD_DFE_H1 0x1bdc
+#define A_XGMAC_PORT_HSS_RXD_DFE_H2 0x1be0
+#define A_XGMAC_PORT_HSS_RXD_DFE_H3 0x1be4
+#define A_XGMAC_PORT_HSS_RXD_DFE_H4 0x1be8
+#define A_XGMAC_PORT_HSS_RXD_DFE_H5 0x1bec
+#define A_XGMAC_PORT_HSS_RXD_DAC_DPC 0x1bf0
+#define A_XGMAC_PORT_HSS_RXD_DDC 0x1bf4
+#define A_XGMAC_PORT_HSS_RXD_INTERNAL_STATUS 0x1bf8
+#define A_XGMAC_PORT_HSS_RXD_DFE_FUNC_CTRL 0x1bfc
+#define A_XGMAC_PORT_HSS_VCO_COARSE_CALIBRATION_0 0x1c00
+
+#define S_BSELO    0
+#define M_BSELO    0xfU
+#define V_BSELO(x) ((x) << S_BSELO)
+#define G_BSELO(x) (((x) >> S_BSELO) & M_BSELO)
+
+#define A_XGMAC_PORT_HSS_VCO_COARSE_CALIBRATION_1 0x1c04
+
+#define S_LDET    4
+#define V_LDET(x) ((x) << S_LDET)
+#define F_LDET    V_LDET(1U)
+
+#define S_CCERR    3
+#define V_CCERR(x) ((x) << S_CCERR)
+#define F_CCERR    V_CCERR(1U)
+
+#define S_CCCMP    2
+#define V_CCCMP(x) ((x) << S_CCCMP)
+#define F_CCCMP    V_CCCMP(1U)
+
+#define A_XGMAC_PORT_HSS_VCO_COARSE_CALIBRATION_2 0x1c08
+
+#define S_BSELI    0
+#define M_BSELI    0xfU
+#define V_BSELI(x) ((x) << S_BSELI)
+#define G_BSELI(x) (((x) >> S_BSELI) & M_BSELI)
+
+#define A_XGMAC_PORT_HSS_VCO_COARSE_CALIBRATION_3 0x1c0c
+
+#define S_VISEL    4
+#define V_VISEL(x) ((x) << S_VISEL)
+#define F_VISEL    V_VISEL(1U)
+
+#define S_FMIN    3
+#define V_FMIN(x) ((x) << S_FMIN)
+#define F_FMIN    V_FMIN(1U)
+
+#define S_FMAX    2
+#define V_FMAX(x) ((x) << S_FMAX)
+#define F_FMAX    V_FMAX(1U)
+
+#define S_CVHOLD    1
+#define V_CVHOLD(x) ((x) << S_CVHOLD)
+#define F_CVHOLD    V_CVHOLD(1U)
+
+#define S_TCDIS    0
+#define V_TCDIS(x) ((x) << S_TCDIS)
+#define F_TCDIS    V_TCDIS(1U)
+
+#define A_XGMAC_PORT_HSS_VCO_COARSE_CALIBRATION_4 0x1c10
+
+#define S_CMETH    2
+#define V_CMETH(x) ((x) << S_CMETH)
+#define F_CMETH    V_CMETH(1U)
+
+#define S_RECAL    1
+#define V_RECAL(x) ((x) << S_RECAL)
+#define F_RECAL    V_RECAL(1U)
+
+#define S_CCLD    0
+#define V_CCLD(x) ((x) << S_CCLD)
+#define F_CCLD    V_CCLD(1U)
+
+#define A_XGMAC_PORT_HSS_ANALOG_TEST_MUX 0x1c14
+
+#define S_ATST    0
+#define M_ATST    0x1fU
+#define V_ATST(x) ((x) << S_ATST)
+#define G_ATST(x) (((x) >> S_ATST) & M_ATST)
+
+#define A_XGMAC_PORT_HSS_PORT_EN_0 0x1c18
+
+#define S_RXDEN    7
+#define V_RXDEN(x) ((x) << S_RXDEN)
+#define F_RXDEN    V_RXDEN(1U)
+
+#define S_RXCEN    6
+#define V_RXCEN(x) ((x) << S_RXCEN)
+#define F_RXCEN    V_RXCEN(1U)
+
+#define S_TXDEN    5
+#define V_TXDEN(x) ((x) << S_TXDEN)
+#define F_TXDEN    V_TXDEN(1U)
+
+#define S_TXCEN    4
+#define V_TXCEN(x) ((x) << S_TXCEN)
+#define F_TXCEN    V_TXCEN(1U)
+
+#define S_RXBEN    3
+#define V_RXBEN(x) ((x) << S_RXBEN)
+#define F_RXBEN    V_RXBEN(1U)
+
+#define S_RXAEN    2
+#define V_RXAEN(x) ((x) << S_RXAEN)
+#define F_RXAEN    V_RXAEN(1U)
+
+#define S_TXBEN    1
+#define V_TXBEN(x) ((x) << S_TXBEN)
+#define F_TXBEN    V_TXBEN(1U)
+
+#define S_TXAEN    0
+#define V_TXAEN(x) ((x) << S_TXAEN)
+#define F_TXAEN    V_TXAEN(1U)
+
+#define A_XGMAC_PORT_HSS_PORT_RESET_0 0x1c20
+
+#define S_RXDRST    7
+#define V_RXDRST(x) ((x) << S_RXDRST)
+#define F_RXDRST    V_RXDRST(1U)
+
+#define S_RXCRST    6
+#define V_RXCRST(x) ((x) << S_RXCRST)
+#define F_RXCRST    V_RXCRST(1U)
+
+#define S_TXDRST    5
+#define V_TXDRST(x) ((x) << S_TXDRST)
+#define F_TXDRST    V_TXDRST(1U)
+
+#define S_TXCRST    4
+#define V_TXCRST(x) ((x) << S_TXCRST)
+#define F_TXCRST    V_TXCRST(1U)
+
+#define S_RXBRST    3
+#define V_RXBRST(x) ((x) << S_RXBRST)
+#define F_RXBRST    V_RXBRST(1U)
+
+#define S_RXARST    2
+#define V_RXARST(x) ((x) << S_RXARST)
+#define F_RXARST    V_RXARST(1U)
+
+#define S_TXBRST    1
+#define V_TXBRST(x) ((x) << S_TXBRST)
+#define F_TXBRST    V_TXBRST(1U)
+
+#define S_TXARST    0
+#define V_TXARST(x) ((x) << S_TXARST)
+#define F_TXARST    V_TXARST(1U)
+
+#define A_XGMAC_PORT_HSS_CHARGE_PUMP_CTRL 0x1c28
+
+#define S_ENCPIS    2
+#define V_ENCPIS(x) ((x) << S_ENCPIS)
+#define F_ENCPIS    V_ENCPIS(1U)
+
+#define S_CPISEL    0
+#define M_CPISEL    0x3U
+#define V_CPISEL(x) ((x) << S_CPISEL)
+#define G_CPISEL(x) (((x) >> S_CPISEL) & M_CPISEL)
+
+#define A_XGMAC_PORT_HSS_BAND_GAP_CTRL 0x1c2c
+
+#define S_BGCTL    0
+#define M_BGCTL    0x1fU
+#define V_BGCTL(x) ((x) << S_BGCTL)
+#define G_BGCTL(x) (((x) >> S_BGCTL) & M_BGCTL)
+
+#define A_XGMAC_PORT_HSS_LOFREQ_OVR 0x1c30
+
+#define S_LFREQ2    3
+#define V_LFREQ2(x) ((x) << S_LFREQ2)
+#define F_LFREQ2    V_LFREQ2(1U)
+
+#define S_LFREQ1    2
+#define V_LFREQ1(x) ((x) << S_LFREQ1)
+#define F_LFREQ1    V_LFREQ1(1U)
+
+#define S_LFREQO    1
+#define V_LFREQO(x) ((x) << S_LFREQO)
+#define F_LFREQO    V_LFREQO(1U)
+
+#define S_LFSEL    0
+#define V_LFSEL(x) ((x) << S_LFSEL)
+#define F_LFSEL    V_LFSEL(1U)
+
+#define A_XGMAC_PORT_HSS_VOLTAGE_BOOST_CTRL 0x1c38
+
+#define S_PFVAL    2
+#define V_PFVAL(x) ((x) << S_PFVAL)
+#define F_PFVAL    V_PFVAL(1U)
+
+#define S_PFEN    1
+#define V_PFEN(x) ((x) << S_PFEN)
+#define F_PFEN    V_PFEN(1U)
+
+#define S_VBADJ    0
+#define V_VBADJ(x) ((x) << S_VBADJ)
+#define F_VBADJ    V_VBADJ(1U)
+
+#define A_XGMAC_PORT_HSS_TX_MODE_CFG 0x1c80
+#define A_XGMAC_PORT_HSS_TXTEST_CTRL 0x1c84
+#define A_XGMAC_PORT_HSS_TX_COEFF_CTRL 0x1c88
+#define A_XGMAC_PORT_HSS_TX_DRIVER_MODE 0x1c8c
+#define A_XGMAC_PORT_HSS_TX_DRIVER_OVR_CTRL 0x1c90
+#define A_XGMAC_PORT_HSS_TX_TDM_BIASGEN_STANDBY_TIMER 0x1c94
+#define A_XGMAC_PORT_HSS_TX_TDM_BIASGEN_PWRON_TIMER 0x1c98
+#define A_XGMAC_PORT_HSS_TX_TAP0_COEFF 0x1ca0
+#define A_XGMAC_PORT_HSS_TX_TAP1_COEFF 0x1ca4
+#define A_XGMAC_PORT_HSS_TX_TAP2_COEFF 0x1ca8
+#define A_XGMAC_PORT_HSS_TX_PWR 0x1cb0
+#define A_XGMAC_PORT_HSS_TX_POLARITY 0x1cb4
+#define A_XGMAC_PORT_HSS_TX_8023AP_AE_CMD 0x1cb8
+#define A_XGMAC_PORT_HSS_TX_8023AP_AE_STATUS 0x1cbc
+#define A_XGMAC_PORT_HSS_TX_TAP0_IDAC_OVR 0x1cc0
+#define A_XGMAC_PORT_HSS_TX_TAP1_IDAC_OVR 0x1cc4
+#define A_XGMAC_PORT_HSS_TX_TAP2_IDAC_OVR 0x1cc8
+#define A_XGMAC_PORT_HSS_TX_PWR_DAC_OVR 0x1cd0
+#define A_XGMAC_PORT_HSS_TX_PWR_DAC 0x1cd4
+#define A_XGMAC_PORT_HSS_TX_TAP0_IDAC_APP 0x1ce0
+#define A_XGMAC_PORT_HSS_TX_TAP1_IDAC_APP 0x1ce4
+#define A_XGMAC_PORT_HSS_TX_TAP2_IDAC_APP 0x1ce8
+#define A_XGMAC_PORT_HSS_TX_SEG_DIS_APP 0x1cf0
+#define A_XGMAC_PORT_HSS_TX_EXT_ADDR_DATA 0x1cf8
+#define A_XGMAC_PORT_HSS_TX_EXT_ADDR 0x1cfc
+#define A_XGMAC_PORT_HSS_RX_CFG_MODE 0x1d00
+#define A_XGMAC_PORT_HSS_RXTEST_CTRL 0x1d04
+#define A_XGMAC_PORT_HSS_RX_PH_ROTATOR_CTRL 0x1d08
+#define A_XGMAC_PORT_HSS_RX_PH_ROTATOR_OFFSET_CTRL 0x1d0c
+#define A_XGMAC_PORT_HSS_RX_PH_ROTATOR_POSITION1 0x1d10
+#define A_XGMAC_PORT_HSS_RX_PH_ROTATOR_POSITION2 0x1d14
+#define A_XGMAC_PORT_HSS_RX_PH_ROTATOR_STATIC_PH_OFFSET 0x1d18
+#define A_XGMAC_PORT_HSS_RX_SIGDET_CTRL 0x1d1c
+#define A_XGMAC_PORT_HSS_RX_DFE_CTRL 0x1d20
+#define A_XGMAC_PORT_HSS_RX_DFE_DATA_EDGE_SAMPLE 0x1d24
+#define A_XGMAC_PORT_HSS_RX_DFE_AMP_SAMPLE 0x1d28
+#define A_XGMAC_PORT_HSS_RX_VGA_CTRL1 0x1d2c
+#define A_XGMAC_PORT_HSS_RX_VGA_CTRL2 0x1d30
+#define A_XGMAC_PORT_HSS_RX_VGA_CTRL3 0x1d34
+#define A_XGMAC_PORT_HSS_RX_DFE_D00_D01_OFFSET 0x1d38
+#define A_XGMAC_PORT_HSS_RX_DFE_D10_D11_OFFSET 0x1d3c
+#define A_XGMAC_PORT_HSS_RX_DFE_E0_E1_OFFSET 0x1d40
+#define A_XGMAC_PORT_HSS_RX_DACA_OFFSET 0x1d44
+#define A_XGMAC_PORT_HSS_RX_DACAP_DAC_AN_OFFSET 0x1d48
+#define A_XGMAC_PORT_HSS_RX_DACA_MIN 0x1d4c
+#define A_XGMAC_PORT_HSS_RX_ADAC_CTRL 0x1d50
+#define A_XGMAC_PORT_HSS_RX_DIGITAL_EYE_CTRL 0x1d54
+#define A_XGMAC_PORT_HSS_RX_DIGITAL_EYE_METRICS 0x1d58
+#define A_XGMAC_PORT_HSS_RX_DFE_H1 0x1d5c
+#define A_XGMAC_PORT_HSS_RX_DFE_H2 0x1d60
+#define A_XGMAC_PORT_HSS_RX_DFE_H3 0x1d64
+#define A_XGMAC_PORT_HSS_RX_DFE_H4 0x1d68
+#define A_XGMAC_PORT_HSS_RX_DFE_H5 0x1d6c
+#define A_XGMAC_PORT_HSS_RX_DAC_DPC 0x1d70
+#define A_XGMAC_PORT_HSS_RX_DDC 0x1d74
+#define A_XGMAC_PORT_HSS_RX_INTERNAL_STATUS 0x1d78
+#define A_XGMAC_PORT_HSS_RX_DFE_FUNC_CTRL 0x1d7c
+#define A_XGMAC_PORT_HSS_TXRX_CFG_MODE 0x1e00
+#define A_XGMAC_PORT_HSS_TXRXTEST_CTRL 0x1e04
+
+/* registers for module UP */
+#define UP_BASE_ADDR 0x0
+
+#define A_UP_IBQ_CONFIG 0x0
+
+#define S_IBQGEN2    2
+#define M_IBQGEN2    0x3fffffffU
+#define V_IBQGEN2(x) ((x) << S_IBQGEN2)
+#define G_IBQGEN2(x) (((x) >> S_IBQGEN2) & M_IBQGEN2)
+
+#define S_IBQBUSY    1
+#define V_IBQBUSY(x) ((x) << S_IBQBUSY)
+#define F_IBQBUSY    V_IBQBUSY(1U)
+
+#define S_IBQEN    0
+#define V_IBQEN(x) ((x) << S_IBQEN)
+#define F_IBQEN    V_IBQEN(1U)
+
+#define A_UP_OBQ_CONFIG 0x4
+
+#define S_OBQGEN2    2
+#define M_OBQGEN2    0x3fffffffU
+#define V_OBQGEN2(x) ((x) << S_OBQGEN2)
+#define G_OBQGEN2(x) (((x) >> S_OBQGEN2) & M_OBQGEN2)
+
+#define S_OBQBUSY    1
+#define V_OBQBUSY(x) ((x) << S_OBQBUSY)
+#define F_OBQBUSY    V_OBQBUSY(1U)
+
+#define S_OBQEN    0
+#define V_OBQEN(x) ((x) << S_OBQEN)
+#define F_OBQEN    V_OBQEN(1U)
+
+#define A_UP_IBQ_GEN 0x8
+
+#define S_IBQGEN0    22
+#define M_IBQGEN0    0x3ffU
+#define V_IBQGEN0(x) ((x) << S_IBQGEN0)
+#define G_IBQGEN0(x) (((x) >> S_IBQGEN0) & M_IBQGEN0)
+
+#define S_IBQTSCHCHNLRDY    18
+#define M_IBQTSCHCHNLRDY    0xfU
+#define V_IBQTSCHCHNLRDY(x) ((x) << S_IBQTSCHCHNLRDY)
+#define G_IBQTSCHCHNLRDY(x) (((x) >> S_IBQTSCHCHNLRDY) & M_IBQTSCHCHNLRDY)
+
+#define S_IBQMBVFSTATUS    17
+#define V_IBQMBVFSTATUS(x) ((x) << S_IBQMBVFSTATUS)
+#define F_IBQMBVFSTATUS    V_IBQMBVFSTATUS(1U)
+
+#define S_IBQMBSTATUS    16
+#define V_IBQMBSTATUS(x) ((x) << S_IBQMBSTATUS)
+#define F_IBQMBSTATUS    V_IBQMBSTATUS(1U)
+
+#define S_IBQGEN1    6
+#define M_IBQGEN1    0x3ffU
+#define V_IBQGEN1(x) ((x) << S_IBQGEN1)
+#define G_IBQGEN1(x) (((x) >> S_IBQGEN1) & M_IBQGEN1)
+
+#define S_IBQEMPTY    0
+#define M_IBQEMPTY    0x3fU
+#define V_IBQEMPTY(x) ((x) << S_IBQEMPTY)
+#define G_IBQEMPTY(x) (((x) >> S_IBQEMPTY) & M_IBQEMPTY)
+
+#define A_UP_OBQ_GEN 0xc
+
+#define S_OBQGEN    6
+#define M_OBQGEN    0x3ffffffU
+#define V_OBQGEN(x) ((x) << S_OBQGEN)
+#define G_OBQGEN(x) (((x) >> S_OBQGEN) & M_OBQGEN)
+
+#define S_OBQFULL    0
+#define M_OBQFULL    0x3fU
+#define V_OBQFULL(x) ((x) << S_OBQFULL)
+#define G_OBQFULL(x) (((x) >> S_OBQFULL) & M_OBQFULL)
+
+#define A_UP_IBQ_0_RDADDR 0x10
+
+#define S_QUEID    13
+#define M_QUEID    0x7ffffU
+#define V_QUEID(x) ((x) << S_QUEID)
+#define G_QUEID(x) (((x) >> S_QUEID) & M_QUEID)
+
+#define S_IBQRDADDR    0
+#define M_IBQRDADDR    0x1fffU
+#define V_IBQRDADDR(x) ((x) << S_IBQRDADDR)
+#define G_IBQRDADDR(x) (((x) >> S_IBQRDADDR) & M_IBQRDADDR)
+
+#define A_UP_IBQ_0_WRADDR 0x14
+
+#define S_IBQWRADDR    0
+#define M_IBQWRADDR    0x1fffU
+#define V_IBQWRADDR(x) ((x) << S_IBQWRADDR)
+#define G_IBQWRADDR(x) (((x) >> S_IBQWRADDR) & M_IBQWRADDR)
+
+#define A_UP_IBQ_0_STATUS 0x18
+
+#define S_QUEERRFRAME    31
+#define V_QUEERRFRAME(x) ((x) << S_QUEERRFRAME)
+#define F_QUEERRFRAME    V_QUEERRFRAME(1U)
+
+#define S_QUEREMFLITS    0
+#define M_QUEREMFLITS    0x7ffU
+#define V_QUEREMFLITS(x) ((x) << S_QUEREMFLITS)
+#define G_QUEREMFLITS(x) (((x) >> S_QUEREMFLITS) & M_QUEREMFLITS)
+
+#define A_UP_IBQ_0_PKTCNT 0x1c
+
+#define S_QUEEOPCNT    16
+#define M_QUEEOPCNT    0xfffU
+#define V_QUEEOPCNT(x) ((x) << S_QUEEOPCNT)
+#define G_QUEEOPCNT(x) (((x) >> S_QUEEOPCNT) & M_QUEEOPCNT)
+
+#define S_QUESOPCNT    0
+#define M_QUESOPCNT    0xfffU
+#define V_QUESOPCNT(x) ((x) << S_QUESOPCNT)
+#define G_QUESOPCNT(x) (((x) >> S_QUESOPCNT) & M_QUESOPCNT)
+
+#define A_UP_IBQ_1_RDADDR 0x20
+#define A_UP_IBQ_1_WRADDR 0x24
+#define A_UP_IBQ_1_STATUS 0x28
+#define A_UP_IBQ_1_PKTCNT 0x2c
+#define A_UP_IBQ_2_RDADDR 0x30
+#define A_UP_IBQ_2_WRADDR 0x34
+#define A_UP_IBQ_2_STATUS 0x38
+#define A_UP_IBQ_2_PKTCNT 0x3c
+#define A_UP_IBQ_3_RDADDR 0x40
+#define A_UP_IBQ_3_WRADDR 0x44
+#define A_UP_IBQ_3_STATUS 0x48
+#define A_UP_IBQ_3_PKTCNT 0x4c
+#define A_UP_IBQ_4_RDADDR 0x50
+#define A_UP_IBQ_4_WRADDR 0x54
+#define A_UP_IBQ_4_STATUS 0x58
+#define A_UP_IBQ_4_PKTCNT 0x5c
+#define A_UP_IBQ_5_RDADDR 0x60
+#define A_UP_IBQ_5_WRADDR 0x64
+#define A_UP_IBQ_5_STATUS 0x68
+#define A_UP_IBQ_5_PKTCNT 0x6c
+#define A_UP_OBQ_0_RDADDR 0x70
+
+#define S_OBQID    15
+#define M_OBQID    0x1ffffU
+#define V_OBQID(x) ((x) << S_OBQID)
+#define G_OBQID(x) (((x) >> S_OBQID) & M_OBQID)
+
+#define S_QUERDADDR    0
+#define M_QUERDADDR    0x7fffU
+#define V_QUERDADDR(x) ((x) << S_QUERDADDR)
+#define G_QUERDADDR(x) (((x) >> S_QUERDADDR) & M_QUERDADDR)
+
+#define A_UP_OBQ_0_WRADDR 0x74
+
+#define S_QUEWRADDR    0
+#define M_QUEWRADDR    0x7fffU
+#define V_QUEWRADDR(x) ((x) << S_QUEWRADDR)
+#define G_QUEWRADDR(x) (((x) >> S_QUEWRADDR) & M_QUEWRADDR)
+
+#define A_UP_OBQ_0_STATUS 0x78
+#define A_UP_OBQ_0_PKTCNT 0x7c
+#define A_UP_OBQ_1_RDADDR 0x80
+#define A_UP_OBQ_1_WRADDR 0x84
+#define A_UP_OBQ_1_STATUS 0x88
+#define A_UP_OBQ_1_PKTCNT 0x8c
+#define A_UP_OBQ_2_RDADDR 0x90
+#define A_UP_OBQ_2_WRADDR 0x94
+#define A_UP_OBQ_2_STATUS 0x98
+#define A_UP_OBQ_2_PKTCNT 0x9c
+#define A_UP_OBQ_3_RDADDR 0xa0
+#define A_UP_OBQ_3_WRADDR 0xa4
+#define A_UP_OBQ_3_STATUS 0xa8
+#define A_UP_OBQ_3_PKTCNT 0xac
+#define A_UP_OBQ_4_RDADDR 0xb0
+#define A_UP_OBQ_4_WRADDR 0xb4
+#define A_UP_OBQ_4_STATUS 0xb8
+#define A_UP_OBQ_4_PKTCNT 0xbc
+#define A_UP_OBQ_5_RDADDR 0xc0
+#define A_UP_OBQ_5_WRADDR 0xc4
+#define A_UP_OBQ_5_STATUS 0xc8
+#define A_UP_OBQ_5_PKTCNT 0xcc
+#define A_UP_IBQ_0_CONFIG 0xd0
+
+#define S_QUESIZE    26
+#define M_QUESIZE    0x3fU
+#define V_QUESIZE(x) ((x) << S_QUESIZE)
+#define G_QUESIZE(x) (((x) >> S_QUESIZE) & M_QUESIZE)
+
+#define S_QUEBASE    8
+#define M_QUEBASE    0x3fU
+#define V_QUEBASE(x) ((x) << S_QUEBASE)
+#define G_QUEBASE(x) (((x) >> S_QUEBASE) & M_QUEBASE)
+
+#define S_QUEDBG8BEN    7
+#define V_QUEDBG8BEN(x) ((x) << S_QUEDBG8BEN)
+#define F_QUEDBG8BEN    V_QUEDBG8BEN(1U)
+
+#define S_QUEBAREADDR    0
+#define V_QUEBAREADDR(x) ((x) << S_QUEBAREADDR)
+#define F_QUEBAREADDR    V_QUEBAREADDR(1U)
+
+#define A_UP_IBQ_0_REALADDR 0xd4
+
+#define S_QUERDADDRWRAP    31
+#define V_QUERDADDRWRAP(x) ((x) << S_QUERDADDRWRAP)
+#define F_QUERDADDRWRAP    V_QUERDADDRWRAP(1U)
+
+#define S_QUEWRADDRWRAP    30
+#define V_QUEWRADDRWRAP(x) ((x) << S_QUEWRADDRWRAP)
+#define F_QUEWRADDRWRAP    V_QUEWRADDRWRAP(1U)
+
+#define S_QUEMEMADDR    3
+#define M_QUEMEMADDR    0x7ffU
+#define V_QUEMEMADDR(x) ((x) << S_QUEMEMADDR)
+#define G_QUEMEMADDR(x) (((x) >> S_QUEMEMADDR) & M_QUEMEMADDR)
+
+#define A_UP_IBQ_1_CONFIG 0xd8
+#define A_UP_IBQ_1_REALADDR 0xdc
+#define A_UP_IBQ_2_CONFIG 0xe0
+#define A_UP_IBQ_2_REALADDR 0xe4
+#define A_UP_IBQ_3_CONFIG 0xe8
+#define A_UP_IBQ_3_REALADDR 0xec
+#define A_UP_IBQ_4_CONFIG 0xf0
+#define A_UP_IBQ_4_REALADDR 0xf4
+#define A_UP_IBQ_5_CONFIG 0xf8
+#define A_UP_IBQ_5_REALADDR 0xfc
+#define A_UP_OBQ_0_CONFIG 0x100
+#define A_UP_OBQ_0_REALADDR 0x104
+#define A_UP_OBQ_1_CONFIG 0x108
+#define A_UP_OBQ_1_REALADDR 0x10c
+#define A_UP_OBQ_2_CONFIG 0x110
+#define A_UP_OBQ_2_REALADDR 0x114
+#define A_UP_OBQ_3_CONFIG 0x118
+#define A_UP_OBQ_3_REALADDR 0x11c
+#define A_UP_OBQ_4_CONFIG 0x120
+#define A_UP_OBQ_4_REALADDR 0x124
+#define A_UP_OBQ_5_CONFIG 0x128
+#define A_UP_OBQ_5_REALADDR 0x12c
+#define A_UP_MAILBOX_STATUS 0x130
+
+#define S_MBGEN0    20
+#define M_MBGEN0    0xfffU
+#define V_MBGEN0(x) ((x) << S_MBGEN0)
+#define G_MBGEN0(x) (((x) >> S_MBGEN0) & M_MBGEN0)
+
+#define S_GENTIMERTRIGGER    16
+#define M_GENTIMERTRIGGER    0xfU
+#define V_GENTIMERTRIGGER(x) ((x) << S_GENTIMERTRIGGER)
+#define G_GENTIMERTRIGGER(x) (((x) >> S_GENTIMERTRIGGER) & M_GENTIMERTRIGGER)
+
+#define S_MBGEN1    8
+#define M_MBGEN1    0xffU
+#define V_MBGEN1(x) ((x) << S_MBGEN1)
+#define G_MBGEN1(x) (((x) >> S_MBGEN1) & M_MBGEN1)
+
+#define S_MBPFINT    0
+#define M_MBPFINT    0xffU
+#define V_MBPFINT(x) ((x) << S_MBPFINT)
+#define G_MBPFINT(x) (((x) >> S_MBPFINT) & M_MBPFINT)
+
+#define A_UP_UP_DBG_LA_CFG 0x140
+
+#define S_UPDBGLACAPTBUB    31
+#define V_UPDBGLACAPTBUB(x) ((x) << S_UPDBGLACAPTBUB)
+#define F_UPDBGLACAPTBUB    V_UPDBGLACAPTBUB(1U)
+
+#define S_UPDBGLACAPTPCONLY    30
+#define V_UPDBGLACAPTPCONLY(x) ((x) << S_UPDBGLACAPTPCONLY)
+#define F_UPDBGLACAPTPCONLY    V_UPDBGLACAPTPCONLY(1U)
+
+#define S_UPDBGLAMASKSTOP    29
+#define V_UPDBGLAMASKSTOP(x) ((x) << S_UPDBGLAMASKSTOP)
+#define F_UPDBGLAMASKSTOP    V_UPDBGLAMASKSTOP(1U)
+
+#define S_UPDBGLAMASKTRIG    28
+#define V_UPDBGLAMASKTRIG(x) ((x) << S_UPDBGLAMASKTRIG)
+#define F_UPDBGLAMASKTRIG    V_UPDBGLAMASKTRIG(1U)
+
+#define S_UPDBGLAWRPTR    16
+#define M_UPDBGLAWRPTR    0xfffU
+#define V_UPDBGLAWRPTR(x) ((x) << S_UPDBGLAWRPTR)
+#define G_UPDBGLAWRPTR(x) (((x) >> S_UPDBGLAWRPTR) & M_UPDBGLAWRPTR)
+
+#define S_UPDBGLARDPTR    2
+#define M_UPDBGLARDPTR    0xfffU
+#define V_UPDBGLARDPTR(x) ((x) << S_UPDBGLARDPTR)
+#define G_UPDBGLARDPTR(x) (((x) >> S_UPDBGLARDPTR) & M_UPDBGLARDPTR)
+
+#define S_UPDBGLARDEN    1
+#define V_UPDBGLARDEN(x) ((x) << S_UPDBGLARDEN)
+#define F_UPDBGLARDEN    V_UPDBGLARDEN(1U)
+
+#define S_UPDBGLAEN    0
+#define V_UPDBGLAEN(x) ((x) << S_UPDBGLAEN)
+#define F_UPDBGLAEN    V_UPDBGLAEN(1U)
+
+#define A_UP_UP_DBG_LA_DATA 0x144
+#define A_UP_PIO_MST_CONFIG 0x148
+
+#define S_FLSRC    24
+#define M_FLSRC    0x7U
+#define V_FLSRC(x) ((x) << S_FLSRC)
+#define G_FLSRC(x) (((x) >> S_FLSRC) & M_FLSRC)
+
+#define S_SEPROT    23
+#define V_SEPROT(x) ((x) << S_SEPROT)
+#define F_SEPROT    V_SEPROT(1U)
+
+#define S_SESRC    20
+#define M_SESRC    0x7U
+#define V_SESRC(x) ((x) << S_SESRC)
+#define G_SESRC(x) (((x) >> S_SESRC) & M_SESRC)
+
+#define S_UPRGN    19
+#define V_UPRGN(x) ((x) << S_UPRGN)
+#define F_UPRGN    V_UPRGN(1U)
+
+#define S_UPPF    16
+#define M_UPPF    0x7U
+#define V_UPPF(x) ((x) << S_UPPF)
+#define G_UPPF(x) (((x) >> S_UPPF) & M_UPPF)
+
+#define S_UPRID    0
+#define M_UPRID    0xffffU
+#define V_UPRID(x) ((x) << S_UPRID)
+#define G_UPRID(x) (((x) >> S_UPRID) & M_UPRID)
+
+#define A_UP_UP_SELF_CONTROL 0x14c
+
+#define S_UPSELFRESET    0
+#define V_UPSELFRESET(x) ((x) << S_UPSELFRESET)
+#define F_UPSELFRESET    V_UPSELFRESET(1U)
+
+#define A_UP_MAILBOX_PF0_CTL 0x180
+#define A_UP_MAILBOX_PF1_CTL 0x190
+#define A_UP_MAILBOX_PF2_CTL 0x1a0
+#define A_UP_MAILBOX_PF3_CTL 0x1b0
+#define A_UP_MAILBOX_PF4_CTL 0x1c0
+#define A_UP_MAILBOX_PF5_CTL 0x1d0
+#define A_UP_MAILBOX_PF6_CTL 0x1e0
+#define A_UP_MAILBOX_PF7_CTL 0x1f0
+#define A_UP_TSCH_CHNLN_CLASS_RDY 0x200
+#define A_UP_TSCH_CHNLN_CLASS_WATCH_RDY 0x204
+
+#define S_TSCHWRRLIMIT    16
+#define M_TSCHWRRLIMIT    0xffffU
+#define V_TSCHWRRLIMIT(x) ((x) << S_TSCHWRRLIMIT)
+#define G_TSCHWRRLIMIT(x) (((x) >> S_TSCHWRRLIMIT) & M_TSCHWRRLIMIT)
+
+#define S_TSCHCHNLCWRDY    0
+#define M_TSCHCHNLCWRDY    0xffffU
+#define V_TSCHCHNLCWRDY(x) ((x) << S_TSCHCHNLCWRDY)
+#define G_TSCHCHNLCWRDY(x) (((x) >> S_TSCHCHNLCWRDY) & M_TSCHCHNLCWRDY)
+
+#define A_UP_TSCH_CHNLN_CLASS_WATCH_LIST 0x208
+
+#define S_TSCHWRRRELOAD    16
+#define M_TSCHWRRRELOAD    0xffffU
+#define V_TSCHWRRRELOAD(x) ((x) << S_TSCHWRRRELOAD)
+#define G_TSCHWRRRELOAD(x) (((x) >> S_TSCHWRRRELOAD) & M_TSCHWRRRELOAD)
+
+#define S_TSCHCHNLCWATCH    0
+#define M_TSCHCHNLCWATCH    0xffffU
+#define V_TSCHCHNLCWATCH(x) ((x) << S_TSCHCHNLCWATCH)
+#define G_TSCHCHNLCWATCH(x) (((x) >> S_TSCHCHNLCWATCH) & M_TSCHCHNLCWATCH)
+
+#define A_UP_TSCH_CHNLN_CLASS_TAKE 0x20c
+
+#define S_TSCHCHNLCNUM    24
+#define M_TSCHCHNLCNUM    0x1fU
+#define V_TSCHCHNLCNUM(x) ((x) << S_TSCHCHNLCNUM)
+#define G_TSCHCHNLCNUM(x) (((x) >> S_TSCHCHNLCNUM) & M_TSCHCHNLCNUM)
+
+#define S_TSCHCHNLCCNT    0
+#define M_TSCHCHNLCCNT    0xffffffU
+#define V_TSCHCHNLCCNT(x) ((x) << S_TSCHCHNLCCNT)
+#define G_TSCHCHNLCCNT(x) (((x) >> S_TSCHCHNLCCNT) & M_TSCHCHNLCCNT)
+
+#define A_UP_UPLADBGPCCHKDATA_0 0x240
+#define A_UP_UPLADBGPCCHKMASK_0 0x244
+#define A_UP_UPLADBGPCCHKDATA_1 0x250
+#define A_UP_UPLADBGPCCHKMASK_1 0x254
+#define A_UP_UPLADBGPCCHKDATA_2 0x260
+#define A_UP_UPLADBGPCCHKMASK_2 0x264
+#define A_UP_UPLADBGPCCHKDATA_3 0x270
+#define A_UP_UPLADBGPCCHKMASK_3 0x274
+
+/* registers for module CIM_CTL */
+#define CIM_CTL_BASE_ADDR 0x0
+
+#define A_CIM_CTL_CONFIG 0x0
+
+#define S_AUTOPREFLOC    17
+#define M_AUTOPREFLOC    0x1fU
+#define V_AUTOPREFLOC(x) ((x) << S_AUTOPREFLOC)
+#define G_AUTOPREFLOC(x) (((x) >> S_AUTOPREFLOC) & M_AUTOPREFLOC)
+
+#define S_AUTOPREFEN    16
+#define V_AUTOPREFEN(x) ((x) << S_AUTOPREFEN)
+#define F_AUTOPREFEN    V_AUTOPREFEN(1U)
+
+#define S_DISMATIMEOUT    15
+#define V_DISMATIMEOUT(x) ((x) << S_DISMATIMEOUT)
+#define F_DISMATIMEOUT    V_DISMATIMEOUT(1U)
+
+#define S_PIFMULTICMD    8
+#define V_PIFMULTICMD(x) ((x) << S_PIFMULTICMD)
+#define F_PIFMULTICMD    V_PIFMULTICMD(1U)
+
+#define S_UPSELFRESETTOUT    7
+#define V_UPSELFRESETTOUT(x) ((x) << S_UPSELFRESETTOUT)
+#define F_UPSELFRESETTOUT    V_UPSELFRESETTOUT(1U)
+
+#define S_PLSWAPDISWR    6
+#define V_PLSWAPDISWR(x) ((x) << S_PLSWAPDISWR)
+#define F_PLSWAPDISWR    V_PLSWAPDISWR(1U)
+
+#define S_PLSWAPDISRD    5
+#define V_PLSWAPDISRD(x) ((x) << S_PLSWAPDISRD)
+#define F_PLSWAPDISRD    V_PLSWAPDISRD(1U)
+
+#define S_PREFEN    0
+#define V_PREFEN(x) ((x) << S_PREFEN)
+#define F_PREFEN    V_PREFEN(1U)
+
+#define A_CIM_CTL_PREFADDR 0x4
+#define A_CIM_CTL_ALLOCADDR 0x8
+#define A_CIM_CTL_INVLDTADDR 0xc
+#define A_CIM_CTL_STATIC_PREFADDR0 0x10
+#define A_CIM_CTL_STATIC_PREFADDR1 0x14
+#define A_CIM_CTL_STATIC_PREFADDR2 0x18
+#define A_CIM_CTL_STATIC_PREFADDR3 0x1c
+#define A_CIM_CTL_STATIC_PREFADDR4 0x20
+#define A_CIM_CTL_STATIC_PREFADDR5 0x24
+#define A_CIM_CTL_STATIC_PREFADDR6 0x28
+#define A_CIM_CTL_STATIC_PREFADDR7 0x2c
+#define A_CIM_CTL_STATIC_PREFADDR8 0x30
+#define A_CIM_CTL_STATIC_PREFADDR9 0x34
+#define A_CIM_CTL_STATIC_PREFADDR10 0x38
+#define A_CIM_CTL_STATIC_PREFADDR11 0x3c
+#define A_CIM_CTL_STATIC_PREFADDR12 0x40
+#define A_CIM_CTL_STATIC_PREFADDR13 0x44
+#define A_CIM_CTL_STATIC_PREFADDR14 0x48
+#define A_CIM_CTL_STATIC_PREFADDR15 0x4c
+#define A_CIM_CTL_STATIC_ALLOCADDR0 0x50
+#define A_CIM_CTL_STATIC_ALLOCADDR1 0x54
+#define A_CIM_CTL_STATIC_ALLOCADDR2 0x58
+#define A_CIM_CTL_STATIC_ALLOCADDR3 0x5c
+#define A_CIM_CTL_STATIC_ALLOCADDR4 0x60
+#define A_CIM_CTL_STATIC_ALLOCADDR5 0x64
+#define A_CIM_CTL_STATIC_ALLOCADDR6 0x68
+#define A_CIM_CTL_STATIC_ALLOCADDR7 0x6c
+#define A_CIM_CTL_STATIC_ALLOCADDR8 0x70
+#define A_CIM_CTL_STATIC_ALLOCADDR9 0x74
+#define A_CIM_CTL_STATIC_ALLOCADDR10 0x78
+#define A_CIM_CTL_STATIC_ALLOCADDR11 0x7c
+#define A_CIM_CTL_STATIC_ALLOCADDR12 0x80
+#define A_CIM_CTL_STATIC_ALLOCADDR13 0x84
+#define A_CIM_CTL_STATIC_ALLOCADDR14 0x88
+#define A_CIM_CTL_STATIC_ALLOCADDR15 0x8c
+#define A_CIM_CTL_FIFO_CNT 0x90
+
+#define S_CTLFIFOCNT    0
+#define M_CTLFIFOCNT    0xfU
+#define V_CTLFIFOCNT(x) ((x) << S_CTLFIFOCNT)
+#define G_CTLFIFOCNT(x) (((x) >> S_CTLFIFOCNT) & M_CTLFIFOCNT)
+
+#define A_CIM_CTL_GLB_TIMER 0x94
+#define A_CIM_CTL_TIMER0 0x98
+#define A_CIM_CTL_TIMER1 0x9c
+#define A_CIM_CTL_GEN0 0xa0
+#define A_CIM_CTL_GEN1 0xa4
+#define A_CIM_CTL_GEN2 0xa8
+#define A_CIM_CTL_GEN3 0xac
+#define A_CIM_CTL_GLB_TIMER_TICK 0xb0
+#define A_CIM_CTL_GEN_TIMER0_CTL 0xb4
+
+#define S_GENTIMERRUN    7
+#define V_GENTIMERRUN(x) ((x) << S_GENTIMERRUN)
+#define F_GENTIMERRUN    V_GENTIMERRUN(1U)
+
+#define S_GENTIMERTRIG    6
+#define V_GENTIMERTRIG(x) ((x) << S_GENTIMERTRIG)
+#define F_GENTIMERTRIG    V_GENTIMERTRIG(1U)
+
+#define S_GENTIMERACT    4
+#define M_GENTIMERACT    0x3U
+#define V_GENTIMERACT(x) ((x) << S_GENTIMERACT)
+#define G_GENTIMERACT(x) (((x) >> S_GENTIMERACT) & M_GENTIMERACT)
+
+#define S_GENTIMERCFG    2
+#define M_GENTIMERCFG    0x3U
+#define V_GENTIMERCFG(x) ((x) << S_GENTIMERCFG)
+#define G_GENTIMERCFG(x) (((x) >> S_GENTIMERCFG) & M_GENTIMERCFG)
+
+#define S_GENTIMERSTOP    1
+#define V_GENTIMERSTOP(x) ((x) << S_GENTIMERSTOP)
+#define F_GENTIMERSTOP    V_GENTIMERSTOP(1U)
+
+#define S_GENTIMERSTRT    0
+#define V_GENTIMERSTRT(x) ((x) << S_GENTIMERSTRT)
+#define F_GENTIMERSTRT    V_GENTIMERSTRT(1U)
+
+#define A_CIM_CTL_GEN_TIMER0 0xb8
+#define A_CIM_CTL_GEN_TIMER1_CTL 0xbc
+#define A_CIM_CTL_GEN_TIMER1 0xc0
+#define A_CIM_CTL_GEN_TIMER2_CTL 0xc4
+#define A_CIM_CTL_GEN_TIMER2 0xc8
+#define A_CIM_CTL_GEN_TIMER3_CTL 0xcc
+#define A_CIM_CTL_GEN_TIMER3 0xd0
+#define A_CIM_CTL_MAILBOX_VF_STATUS 0xe0
+#define A_CIM_CTL_MAILBOX_VFN_CTL 0x100
+#define A_CIM_CTL_TSCH_CHNLN_CTL 0x900
+
+#define S_TSCHNLEN    31
+#define V_TSCHNLEN(x) ((x) << S_TSCHNLEN)
+#define F_TSCHNLEN    V_TSCHNLEN(1U)
+
+#define S_TSCHNRESET    30
+#define V_TSCHNRESET(x) ((x) << S_TSCHNRESET)
+#define F_TSCHNRESET    V_TSCHNRESET(1U)
+
+#define A_CIM_CTL_TSCH_CHNLN_TICK 0x904
+
+#define S_TSCHNLTICK    0
+#define M_TSCHNLTICK    0xffffU
+#define V_TSCHNLTICK(x) ((x) << S_TSCHNLTICK)
+#define G_TSCHNLTICK(x) (((x) >> S_TSCHNLTICK) & M_TSCHNLTICK)
+
+#define A_CIM_CTL_TSCH_CHNLN_CLASS_ENABLE_A 0x908
+
+#define S_TSC15WRREN    31
+#define V_TSC15WRREN(x) ((x) << S_TSC15WRREN)
+#define F_TSC15WRREN    V_TSC15WRREN(1U)
+
+#define S_TSC15RATEEN    30
+#define V_TSC15RATEEN(x) ((x) << S_TSC15RATEEN)
+#define F_TSC15RATEEN    V_TSC15RATEEN(1U)
+
+#define S_TSC14WRREN    29
+#define V_TSC14WRREN(x) ((x) << S_TSC14WRREN)
+#define F_TSC14WRREN    V_TSC14WRREN(1U)
+
+#define S_TSC14RATEEN    28
+#define V_TSC14RATEEN(x) ((x) << S_TSC14RATEEN)
+#define F_TSC14RATEEN    V_TSC14RATEEN(1U)
+
+#define S_TSC13WRREN    27
+#define V_TSC13WRREN(x) ((x) << S_TSC13WRREN)
+#define F_TSC13WRREN    V_TSC13WRREN(1U)
+
+#define S_TSC13RATEEN    26
+#define V_TSC13RATEEN(x) ((x) << S_TSC13RATEEN)
+#define F_TSC13RATEEN    V_TSC13RATEEN(1U)
+
+#define S_TSC12WRREN    25
+#define V_TSC12WRREN(x) ((x) << S_TSC12WRREN)
+#define F_TSC12WRREN    V_TSC12WRREN(1U)
+
+#define S_TSC12RATEEN    24
+#define V_TSC12RATEEN(x) ((x) << S_TSC12RATEEN)
+#define F_TSC12RATEEN    V_TSC12RATEEN(1U)
+
+#define S_TSC11WRREN    23
+#define V_TSC11WRREN(x) ((x) << S_TSC11WRREN)
+#define F_TSC11WRREN    V_TSC11WRREN(1U)
+
+#define S_TSC11RATEEN    22
+#define V_TSC11RATEEN(x) ((x) << S_TSC11RATEEN)
+#define F_TSC11RATEEN    V_TSC11RATEEN(1U)
+
+#define S_TSC10WRREN    21
+#define V_TSC10WRREN(x) ((x) << S_TSC10WRREN)
+#define F_TSC10WRREN    V_TSC10WRREN(1U)
+
+#define S_TSC10RATEEN    20
+#define V_TSC10RATEEN(x) ((x) << S_TSC10RATEEN)
+#define F_TSC10RATEEN    V_TSC10RATEEN(1U)
+
+#define S_TSC9WRREN    19
+#define V_TSC9WRREN(x) ((x) << S_TSC9WRREN)
+#define F_TSC9WRREN    V_TSC9WRREN(1U)
+
+#define S_TSC9RATEEN    18
+#define V_TSC9RATEEN(x) ((x) << S_TSC9RATEEN)
+#define F_TSC9RATEEN    V_TSC9RATEEN(1U)
+
+#define S_TSC8WRREN    17
+#define V_TSC8WRREN(x) ((x) << S_TSC8WRREN)
+#define F_TSC8WRREN    V_TSC8WRREN(1U)
+
+#define S_TSC8RATEEN    16
+#define V_TSC8RATEEN(x) ((x) << S_TSC8RATEEN)
+#define F_TSC8RATEEN    V_TSC8RATEEN(1U)
+
+#define S_TSC7WRREN    15
+#define V_TSC7WRREN(x) ((x) << S_TSC7WRREN)
+#define F_TSC7WRREN    V_TSC7WRREN(1U)
+
+#define S_TSC7RATEEN    14
+#define V_TSC7RATEEN(x) ((x) << S_TSC7RATEEN)
+#define F_TSC7RATEEN    V_TSC7RATEEN(1U)
+
+#define S_TSC6WRREN    13
+#define V_TSC6WRREN(x) ((x) << S_TSC6WRREN)
+#define F_TSC6WRREN    V_TSC6WRREN(1U)
+
+#define S_TSC6RATEEN    12
+#define V_TSC6RATEEN(x) ((x) << S_TSC6RATEEN)
+#define F_TSC6RATEEN    V_TSC6RATEEN(1U)
+
+#define S_TSC5WRREN    11
+#define V_TSC5WRREN(x) ((x) << S_TSC5WRREN)
+#define F_TSC5WRREN    V_TSC5WRREN(1U)
+
+#define S_TSC5RATEEN    10
+#define V_TSC5RATEEN(x) ((x) << S_TSC5RATEEN)
+#define F_TSC5RATEEN    V_TSC5RATEEN(1U)
+
+#define S_TSC4WRREN    9
+#define V_TSC4WRREN(x) ((x) << S_TSC4WRREN)
+#define F_TSC4WRREN    V_TSC4WRREN(1U)
+
+#define S_TSC4RATEEN    8
+#define V_TSC4RATEEN(x) ((x) << S_TSC4RATEEN)
+#define F_TSC4RATEEN    V_TSC4RATEEN(1U)
+
+#define S_TSC3WRREN    7
+#define V_TSC3WRREN(x) ((x) << S_TSC3WRREN)
+#define F_TSC3WRREN    V_TSC3WRREN(1U)
+
+#define S_TSC3RATEEN    6
+#define V_TSC3RATEEN(x) ((x) << S_TSC3RATEEN)
+#define F_TSC3RATEEN    V_TSC3RATEEN(1U)
+
+#define S_TSC2WRREN    5
+#define V_TSC2WRREN(x) ((x) << S_TSC2WRREN)
+#define F_TSC2WRREN    V_TSC2WRREN(1U)
+
+#define S_TSC2RATEEN    4
+#define V_TSC2RATEEN(x) ((x) << S_TSC2RATEEN)
+#define F_TSC2RATEEN    V_TSC2RATEEN(1U)
+
+#define S_TSC1WRREN    3
+#define V_TSC1WRREN(x) ((x) << S_TSC1WRREN)
+#define F_TSC1WRREN    V_TSC1WRREN(1U)
+
+#define S_TSC1RATEEN    2
+#define V_TSC1RATEEN(x) ((x) << S_TSC1RATEEN)
+#define F_TSC1RATEEN    V_TSC1RATEEN(1U)
+
+#define S_TSC0WRREN    1
+#define V_TSC0WRREN(x) ((x) << S_TSC0WRREN)
+#define F_TSC0WRREN    V_TSC0WRREN(1U)
+
+#define S_TSC0RATEEN    0
+#define V_TSC0RATEEN(x) ((x) << S_TSC0RATEEN)
+#define F_TSC0RATEEN    V_TSC0RATEEN(1U)
+
+#define A_CIM_CTL_TSCH_MIN_MAX_EN 0x90c
+
+#define S_MIN_MAX_EN    0
+#define V_MIN_MAX_EN(x) ((x) << S_MIN_MAX_EN)
+#define F_MIN_MAX_EN    V_MIN_MAX_EN(1U)
+
+#define A_CIM_CTL_TSCH_CHNLN_RATE_LIMITER 0x910
+
+#define S_TSCHNLRATENEG    31
+#define V_TSCHNLRATENEG(x) ((x) << S_TSCHNLRATENEG)
+#define F_TSCHNLRATENEG    V_TSCHNLRATENEG(1U)
+
+#define S_TSCHNLRATEL    0
+#define M_TSCHNLRATEL    0x7fffffffU
+#define V_TSCHNLRATEL(x) ((x) << S_TSCHNLRATEL)
+#define G_TSCHNLRATEL(x) (((x) >> S_TSCHNLRATEL) & M_TSCHNLRATEL)
+
+#define A_CIM_CTL_TSCH_CHNLN_RATE_PROPERTIES 0x914
+
+#define S_TSCHNLRMAX    16
+#define M_TSCHNLRMAX    0xffffU
+#define V_TSCHNLRMAX(x) ((x) << S_TSCHNLRMAX)
+#define G_TSCHNLRMAX(x) (((x) >> S_TSCHNLRMAX) & M_TSCHNLRMAX)
+
+#define S_TSCHNLRINCR    0
+#define M_TSCHNLRINCR    0xffffU
+#define V_TSCHNLRINCR(x) ((x) << S_TSCHNLRINCR)
+#define G_TSCHNLRINCR(x) (((x) >> S_TSCHNLRINCR) & M_TSCHNLRINCR)
+
+#define A_CIM_CTL_TSCH_CHNLN_WRR 0x918
+#define A_CIM_CTL_TSCH_CHNLN_WEIGHT 0x91c
+
+#define S_TSCHNLWEIGHT    0
+#define M_TSCHNLWEIGHT    0x3fffffU
+#define V_TSCHNLWEIGHT(x) ((x) << S_TSCHNLWEIGHT)
+#define G_TSCHNLWEIGHT(x) (((x) >> S_TSCHNLWEIGHT) & M_TSCHNLWEIGHT)
+
+#define A_CIM_CTL_TSCH_CHNLN_CLASSM_RATE_PROPERTIES 0x924
+
+#define S_TSCCLRMAX    16
+#define M_TSCCLRMAX    0xffffU
+#define V_TSCCLRMAX(x) ((x) << S_TSCCLRMAX)
+#define G_TSCCLRMAX(x) (((x) >> S_TSCCLRMAX) & M_TSCCLRMAX)
+
+#define S_TSCCLRINCR    0
+#define M_TSCCLRINCR    0xffffU
+#define V_TSCCLRINCR(x) ((x) << S_TSCCLRINCR)
+#define G_TSCCLRINCR(x) (((x) >> S_TSCCLRINCR) & M_TSCCLRINCR)
+
+#define A_CIM_CTL_TSCH_CHNLN_CLASSM_WRR 0x928
+
+#define S_TSCCLWRRNEG    31
+#define V_TSCCLWRRNEG(x) ((x) << S_TSCCLWRRNEG)
+#define F_TSCCLWRRNEG    V_TSCCLWRRNEG(1U)
+
+#define S_TSCCLWRR    0
+#define M_TSCCLWRR    0x3ffffffU
+#define V_TSCCLWRR(x) ((x) << S_TSCCLWRR)
+#define G_TSCCLWRR(x) (((x) >> S_TSCCLWRR) & M_TSCCLWRR)
+
+#define A_CIM_CTL_TSCH_CHNLN_CLASSM_WEIGHT 0x92c
+
+#define S_TSCCLWEIGHT    0
+#define M_TSCCLWEIGHT    0xffffU
+#define V_TSCCLWEIGHT(x) ((x) << S_TSCCLWEIGHT)
+#define G_TSCCLWEIGHT(x) (((x) >> S_TSCCLWEIGHT) & M_TSCCLWEIGHT)
diff --git a/drivers/net/cxgb4/t4_regs_values.h b/drivers/net/cxgb4/t4_regs_values.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_regs_values.h
@@ -0,0 +1,174 @@
+/*
+ * This file is part of the Chelsio T4 support code.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __T4_REGS_VALUES_H__
+#define __T4_REGS_VALUES_H__
+
+/*
+ * This file contains definitions for various T4 register value hardware
+ * constants.  The types of values encoded here are predominantly those for
+ * register fields which control "modal" behavior.  For the most part, we do
+ * not include definitions for register fields which are simple numeric
+ * metrics, etc.
+ *
+ * These new "modal values" use a naming convention which matches the
+ * currently existing macros in t4_reg.h.  For register field FOO which would
+ * have S_FOO, M_FOO, V_FOO() and G_FOO() macros, we introduce X_FOO_{MODE}
+ * definitions.  These can be used as V_FOO(X_FOO_MODE) or as (G_FOO(x) ==
+ * X_FOO_MODE).
+ *
+ * Note that this should all be part of t4_regs.h but the toolset used to
+ * generate that file doesn't [yet] have the capability of collecting these
+ * constants.
+ */
+
+/*
+ * SGE definitions.
+ * ================
+ */
+
+/*
+ * SGE register field values.
+ */
+
+/* CONTROL register */
+#define X_FLSPLITMODE_FLSPLITMIN	0
+#define X_FLSPLITMODE_ETHHDR		1
+#define X_FLSPLITMODE_IPHDR		2
+#define X_FLSPLITMODE_TCPHDR		3
+
+#define X_DCASYSTYPE_FSB		0
+#define X_DCASYSTYPE_CSI		1
+
+#define X_EGSTATPAGESIZE_64B		0
+#define X_EGSTATPAGESIZE_128B		1
+
+#define X_RXPKTCPLMODE_DATA		0
+#define X_RXPKTCPLMODE_SPLIT		1
+
+#define X_INGPCIEBOUNDARY_SHIFT		5
+#define X_INGPCIEBOUNDARY_32B		0
+#define X_INGPCIEBOUNDARY_64B		1
+#define X_INGPCIEBOUNDARY_128B		2
+#define X_INGPCIEBOUNDARY_256B		3
+#define X_INGPCIEBOUNDARY_512B		4
+#define X_INGPCIEBOUNDARY_1024B		5
+#define X_INGPCIEBOUNDARY_2048B		6
+#define X_INGPCIEBOUNDARY_4096B		7
+
+#define X_INGPADBOUNDARY_SHIFT		5
+#define X_INGPADBOUNDARY_32B		0
+#define X_INGPADBOUNDARY_64B		1
+#define X_INGPADBOUNDARY_128B		2
+#define X_INGPADBOUNDARY_256B		3
+#define X_INGPADBOUNDARY_512B		4
+#define X_INGPADBOUNDARY_1024B		5
+#define X_INGPADBOUNDARY_2048B		6
+#define X_INGPADBOUNDARY_4096B		7
+
+#define X_EGRPCIEBOUNDARY_SHIFT		5
+#define X_EGRPCIEBOUNDARY_32B		0
+#define X_EGRPCIEBOUNDARY_64B		1
+#define X_EGRPCIEBOUNDARY_128B		2
+#define X_EGRPCIEBOUNDARY_256B		3
+#define X_EGRPCIEBOUNDARY_512B		4
+#define X_EGRPCIEBOUNDARY_1024B		5
+#define X_EGRPCIEBOUNDARY_2048B		6
+#define X_EGRPCIEBOUNDARY_4096B		7
+
+/* GTS register */
+#define SGE_TIMERREGS			6
+#define X_TIMERREG_COUNTER0		0
+#define X_TIMERREG_COUNTER1		1
+#define X_TIMERREG_COUNTER2		2
+#define X_TIMERREG_COUNTER3		3
+#define X_TIMERREG_COUNTER4		4
+#define X_TIMERREG_COUNTER5		5
+#define X_TIMERREG_RESTART_COUNTER	6
+#define X_TIMERREG_UPDATE_CIDX		7
+
+/*
+ * Egress Context field values
+ */
+#define EC_WR_UNITS			16
+
+#define X_FETCHBURSTMIN_SHIFT		4
+#define X_FETCHBURSTMIN_16B		0
+#define X_FETCHBURSTMIN_32B		1
+#define X_FETCHBURSTMIN_64B		2
+#define X_FETCHBURSTMIN_128B		3
+
+#define X_FETCHBURSTMAX_SHIFT		6
+#define X_FETCHBURSTMAX_64B		0
+#define X_FETCHBURSTMAX_128B		1
+#define X_FETCHBURSTMAX_256B		2
+#define X_FETCHBURSTMAX_512B		3
+
+#define X_HOSTFCMODE_NONE		0
+#define X_HOSTFCMODE_INGRESS_QUEUE	1
+#define X_HOSTFCMODE_STATUS_PAGE	2
+#define X_HOSTFCMODE_BOTH		3
+
+#define X_HOSTFCOWNER_UP		0
+#define X_HOSTFCOWNER_SGE		1
+
+#define X_CIDXFLUSHTHRESH_1		0
+#define X_CIDXFLUSHTHRESH_2		1
+#define X_CIDXFLUSHTHRESH_4		2
+#define X_CIDXFLUSHTHRESH_8		3
+#define X_CIDXFLUSHTHRESH_16		4
+#define X_CIDXFLUSHTHRESH_32		5
+#define X_CIDXFLUSHTHRESH_64		6
+#define X_CIDXFLUSHTHRESH_128		7
+
+#define X_IDXSIZE_UNIT			64
+
+#define X_BASEADDRESS_ALIGN		512
+
+/*
+ * Ingress Context field values
+ */
+#define X_UPDATESCHEDULING_TIMER	0
+#define X_UPDATESCHEDULING_COUNTER_OPTTIMER	1
+
+#define X_UPDATEDELIVERY_NONE		0
+#define X_UPDATEDELIVERY_INTERRUPT	1
+#define X_UPDATEDELIVERY_STATUS_PAGE	2
+#define X_UPDATEDELIVERY_BOTH		3
+
+#define X_INTERRUPTDESTINATION_PCIE	0
+#define X_INTERRUPTDESTINATION_IQ	1
+
+#define X_QUEUEENTRYSIZE_16B		0
+#define X_QUEUEENTRYSIZE_32B		1
+#define X_QUEUEENTRYSIZE_64B		2
+#define X_QUEUEENTRYSIZE_128B		3
+
+#define IC_SIZE_UNIT			16
+#define IC_BASEADDRESS_ALIGN		512
+
+#define X_RSPD_TYPE_FLBUF		0
+#define X_RSPD_TYPE_CPL			1
+#define X_RSPD_TYPE_INTR		2
+
+/*
+ * CIM definitions.
+ * ================
+ */
+
+/*
+ * CIM register field values.
+ */
+#define X_MBOWNER_NONE			0
+#define X_MBOWNER_FW			1
+#define X_MBOWNER_PL			2
+
+#endif /* __T4_REGS_VALUES_H__ */
diff --git a/drivers/net/cxgb4/t4_tcb.h b/drivers/net/cxgb4/t4_tcb.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4_tcb.h
@@ -0,0 +1,724 @@
+/* This file is automatically generated --- changes will be lost */
+
+#ifndef _T4_TCB_DEFS_H
+#define _T4_TCB_DEFS_H
+
+/* 3:0 */
+#define W_TCB_ULP_TYPE    0
+#define S_TCB_ULP_TYPE    0
+#define M_TCB_ULP_TYPE    0xfULL
+#define V_TCB_ULP_TYPE(x) ((x) << S_TCB_ULP_TYPE)
+
+/* 11:4 */
+#define W_TCB_ULP_RAW    0
+#define S_TCB_ULP_RAW    4
+#define M_TCB_ULP_RAW    0xffULL
+#define V_TCB_ULP_RAW(x) ((x) << S_TCB_ULP_RAW)
+
+/* 23:12 */
+#define W_TCB_L2T_IX    0
+#define S_TCB_L2T_IX    12
+#define M_TCB_L2T_IX    0xfffULL
+#define V_TCB_L2T_IX(x) ((x) << S_TCB_L2T_IX)
+
+/* 31:24 */
+#define W_TCB_SMAC_SEL    0
+#define S_TCB_SMAC_SEL    24
+#define M_TCB_SMAC_SEL    0xffULL
+#define V_TCB_SMAC_SEL(x) ((x) << S_TCB_SMAC_SEL)
+
+/* 95:32 */
+#define W_TCB_T_FLAGS    1
+#define S_TCB_T_FLAGS    0
+#define M_TCB_T_FLAGS    0xffffffffffffffffULL
+#define V_TCB_T_FLAGS(x) ((__u64)(x) << S_TCB_T_FLAGS)
+
+/* 105:96 */
+#define W_TCB_RSS_INFO    3
+#define S_TCB_RSS_INFO    0
+#define M_TCB_RSS_INFO    0x3ffULL
+#define V_TCB_RSS_INFO(x) ((x) << S_TCB_RSS_INFO)
+
+/* 111:106 */
+#define W_TCB_TOS    3
+#define S_TCB_TOS    10
+#define M_TCB_TOS    0x3fULL
+#define V_TCB_TOS(x) ((x) << S_TCB_TOS)
+
+/* 115:112 */
+#define W_TCB_T_STATE    3
+#define S_TCB_T_STATE    16
+#define M_TCB_T_STATE    0xfULL
+#define V_TCB_T_STATE(x) ((x) << S_TCB_T_STATE)
+
+/* 119:116 */
+#define W_TCB_MAX_RT    3
+#define S_TCB_MAX_RT    20
+#define M_TCB_MAX_RT    0xfULL
+#define V_TCB_MAX_RT(x) ((x) << S_TCB_MAX_RT)
+
+/* 123:120 */
+#define W_TCB_T_MAXSEG    3
+#define S_TCB_T_MAXSEG    24
+#define M_TCB_T_MAXSEG    0xfULL
+#define V_TCB_T_MAXSEG(x) ((x) << S_TCB_T_MAXSEG)
+
+/* 127:124 */
+#define W_TCB_SND_SCALE    3
+#define S_TCB_SND_SCALE    28
+#define M_TCB_SND_SCALE    0xfULL
+#define V_TCB_SND_SCALE(x) ((x) << S_TCB_SND_SCALE)
+
+/* 131:128 */
+#define W_TCB_RCV_SCALE    4
+#define S_TCB_RCV_SCALE    0
+#define M_TCB_RCV_SCALE    0xfULL
+#define V_TCB_RCV_SCALE(x) ((x) << S_TCB_RCV_SCALE)
+
+/* 135:132 */
+#define W_TCB_T_RXTSHIFT    4
+#define S_TCB_T_RXTSHIFT    4
+#define M_TCB_T_RXTSHIFT    0xfULL
+#define V_TCB_T_RXTSHIFT(x) ((x) << S_TCB_T_RXTSHIFT)
+
+/* 139:136 */
+#define W_TCB_T_DUPACKS    4
+#define S_TCB_T_DUPACKS    8
+#define M_TCB_T_DUPACKS    0xfULL
+#define V_TCB_T_DUPACKS(x) ((x) << S_TCB_T_DUPACKS)
+
+/* 143:140 */
+#define W_TCB_TIMESTAMP_OFFSET    4
+#define S_TCB_TIMESTAMP_OFFSET    12
+#define M_TCB_TIMESTAMP_OFFSET    0xfULL
+#define V_TCB_TIMESTAMP_OFFSET(x) ((x) << S_TCB_TIMESTAMP_OFFSET)
+
+/* 159:144 */
+#define W_TCB_RCV_ADV    4
+#define S_TCB_RCV_ADV    16
+#define M_TCB_RCV_ADV    0xffffULL
+#define V_TCB_RCV_ADV(x) ((x) << S_TCB_RCV_ADV)
+
+/* 191:160 */
+#define W_TCB_TIMESTAMP    5
+#define S_TCB_TIMESTAMP    0
+#define M_TCB_TIMESTAMP    0xffffffffULL
+#define V_TCB_TIMESTAMP(x) ((x) << S_TCB_TIMESTAMP)
+
+/* 223:192 */
+#define W_TCB_T_RTT_TS_RECENT_AGE    6
+#define S_TCB_T_RTT_TS_RECENT_AGE    0
+#define M_TCB_T_RTT_TS_RECENT_AGE    0xffffffffULL
+#define V_TCB_T_RTT_TS_RECENT_AGE(x) ((x) << S_TCB_T_RTT_TS_RECENT_AGE)
+
+/* 255:224 */
+#define W_TCB_T_RTSEQ_RECENT    7
+#define S_TCB_T_RTSEQ_RECENT    0
+#define M_TCB_T_RTSEQ_RECENT    0xffffffffULL
+#define V_TCB_T_RTSEQ_RECENT(x) ((x) << S_TCB_T_RTSEQ_RECENT)
+
+/* 271:256 */
+#define W_TCB_T_SRTT    8
+#define S_TCB_T_SRTT    0
+#define M_TCB_T_SRTT    0xffffULL
+#define V_TCB_T_SRTT(x) ((x) << S_TCB_T_SRTT)
+
+/* 287:272 */
+#define W_TCB_T_RTTVAR    8
+#define S_TCB_T_RTTVAR    16
+#define M_TCB_T_RTTVAR    0xffffULL
+#define V_TCB_T_RTTVAR(x) ((x) << S_TCB_T_RTTVAR)
+
+/* 319:288 */
+#define W_TCB_TX_MAX    9
+#define S_TCB_TX_MAX    0
+#define M_TCB_TX_MAX    0xffffffffULL
+#define V_TCB_TX_MAX(x) ((x) << S_TCB_TX_MAX)
+
+/* 347:320 */
+#define W_TCB_SND_UNA_RAW    10
+#define S_TCB_SND_UNA_RAW    0
+#define M_TCB_SND_UNA_RAW    0xfffffffULL
+#define V_TCB_SND_UNA_RAW(x) ((x) << S_TCB_SND_UNA_RAW)
+
+/* 375:348 */
+#define W_TCB_SND_NXT_RAW    10
+#define S_TCB_SND_NXT_RAW    28
+#define M_TCB_SND_NXT_RAW    0xfffffffULL
+#define V_TCB_SND_NXT_RAW(x) ((__u64)(x) << S_TCB_SND_NXT_RAW)
+
+/* 403:376 */
+#define W_TCB_SND_MAX_RAW    11
+#define S_TCB_SND_MAX_RAW    24
+#define M_TCB_SND_MAX_RAW    0xfffffffULL
+#define V_TCB_SND_MAX_RAW(x) ((__u64)(x) << S_TCB_SND_MAX_RAW)
+
+/* 431:404 */
+#define W_TCB_SND_REC_RAW    12
+#define S_TCB_SND_REC_RAW    20
+#define M_TCB_SND_REC_RAW    0xfffffffULL
+#define V_TCB_SND_REC_RAW(x) ((__u64)(x) << S_TCB_SND_REC_RAW)
+
+/* 459:432 */
+#define W_TCB_SND_CWND    13
+#define S_TCB_SND_CWND    16
+#define M_TCB_SND_CWND    0xfffffffULL
+#define V_TCB_SND_CWND(x) ((__u64)(x) << S_TCB_SND_CWND)
+
+/* 487:460 */
+#define W_TCB_SND_SSTHRESH    14
+#define S_TCB_SND_SSTHRESH    12
+#define M_TCB_SND_SSTHRESH    0xfffffffULL
+#define V_TCB_SND_SSTHRESH(x) ((__u64)(x) << S_TCB_SND_SSTHRESH)
+
+/* 504:488 */
+#define W_TCB_TX_HDR_PTR_RAW    15
+#define S_TCB_TX_HDR_PTR_RAW    8
+#define M_TCB_TX_HDR_PTR_RAW    0x1ffffULL
+#define V_TCB_TX_HDR_PTR_RAW(x) ((x) << S_TCB_TX_HDR_PTR_RAW)
+
+/* 521:505 */
+#define W_TCB_TX_LAST_PTR_RAW    15
+#define S_TCB_TX_LAST_PTR_RAW    25
+#define M_TCB_TX_LAST_PTR_RAW    0x1ffffULL
+#define V_TCB_TX_LAST_PTR_RAW(x) ((__u64)(x) << S_TCB_TX_LAST_PTR_RAW)
+
+/* 553:522 */
+#define W_TCB_RCV_NXT    16
+#define S_TCB_RCV_NXT    10
+#define M_TCB_RCV_NXT    0xffffffffULL
+#define V_TCB_RCV_NXT(x) ((__u64)(x) << S_TCB_RCV_NXT)
+
+/* 581:554 */
+#define W_TCB_RCV_WND    17
+#define S_TCB_RCV_WND    10
+#define M_TCB_RCV_WND    0xfffffffULL
+#define V_TCB_RCV_WND(x) ((__u64)(x) << S_TCB_RCV_WND)
+
+/* 609:582 */
+#define W_TCB_RX_HDR_OFFSET    18
+#define S_TCB_RX_HDR_OFFSET    6
+#define M_TCB_RX_HDR_OFFSET    0xfffffffULL
+#define V_TCB_RX_HDR_OFFSET(x) ((__u64)(x) << S_TCB_RX_HDR_OFFSET)
+
+/* 637:610 */
+#define W_TCB_TS_LAST_ACK_SENT_RAW    19
+#define S_TCB_TS_LAST_ACK_SENT_RAW    2
+#define M_TCB_TS_LAST_ACK_SENT_RAW    0xfffffffULL
+#define V_TCB_TS_LAST_ACK_SENT_RAW(x) ((x) << S_TCB_TS_LAST_ACK_SENT_RAW)
+
+/* 665:638 */
+#define W_TCB_RX_FRAG0_START_IDX_RAW    19
+#define S_TCB_RX_FRAG0_START_IDX_RAW    30
+#define M_TCB_RX_FRAG0_START_IDX_RAW    0xfffffffULL
+#define V_TCB_RX_FRAG0_START_IDX_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG0_START_IDX_RAW)
+
+/* 693:666 */
+#define W_TCB_RX_FRAG1_START_IDX_OFFSET    20
+#define S_TCB_RX_FRAG1_START_IDX_OFFSET    26
+#define M_TCB_RX_FRAG1_START_IDX_OFFSET    0xfffffffULL
+#define V_TCB_RX_FRAG1_START_IDX_OFFSET(x) ((__u64)(x) << S_TCB_RX_FRAG1_START_IDX_OFFSET)
+
+/* 721:694 */
+#define W_TCB_RX_FRAG0_LEN    21
+#define S_TCB_RX_FRAG0_LEN    22
+#define M_TCB_RX_FRAG0_LEN    0xfffffffULL
+#define V_TCB_RX_FRAG0_LEN(x) ((__u64)(x) << S_TCB_RX_FRAG0_LEN)
+
+/* 749:722 */
+#define W_TCB_RX_FRAG1_LEN    22
+#define S_TCB_RX_FRAG1_LEN    18
+#define M_TCB_RX_FRAG1_LEN    0xfffffffULL
+#define V_TCB_RX_FRAG1_LEN(x) ((__u64)(x) << S_TCB_RX_FRAG1_LEN)
+
+/* 765:750 */
+#define W_TCB_PDU_LEN    23
+#define S_TCB_PDU_LEN    14
+#define M_TCB_PDU_LEN    0xffffULL
+#define V_TCB_PDU_LEN(x) ((x) << S_TCB_PDU_LEN)
+
+/* 782:766 */
+#define W_TCB_RX_PTR_RAW    23
+#define S_TCB_RX_PTR_RAW    30
+#define M_TCB_RX_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_PTR_RAW(x) ((__u64)(x) << S_TCB_RX_PTR_RAW)
+
+/* 799:783 */
+#define W_TCB_RX_FRAG1_PTR_RAW    24
+#define S_TCB_RX_FRAG1_PTR_RAW    15
+#define M_TCB_RX_FRAG1_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG1_PTR_RAW(x) ((x) << S_TCB_RX_FRAG1_PTR_RAW)
+
+/* 831:800 */
+#define W_TCB_MAIN_SLUSH    25
+#define S_TCB_MAIN_SLUSH    0
+#define M_TCB_MAIN_SLUSH    0xffffffffULL
+#define V_TCB_MAIN_SLUSH(x) ((x) << S_TCB_MAIN_SLUSH)
+
+/* 846:832 */
+#define W_TCB_AUX1_SLUSH0    26
+#define S_TCB_AUX1_SLUSH0    0
+#define M_TCB_AUX1_SLUSH0    0x7fffULL
+#define V_TCB_AUX1_SLUSH0(x) ((x) << S_TCB_AUX1_SLUSH0)
+
+/* 874:847 */
+#define W_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    26
+#define S_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    15
+#define M_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    0xfffffffULL
+#define V_TCB_RX_FRAG2_START_IDX_OFFSET_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG2_START_IDX_OFFSET_RAW)
+
+/* 891:875 */
+#define W_TCB_RX_FRAG2_PTR_RAW    27
+#define S_TCB_RX_FRAG2_PTR_RAW    11
+#define M_TCB_RX_FRAG2_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG2_PTR_RAW(x) ((x) << S_TCB_RX_FRAG2_PTR_RAW)
+
+/* 919:892 */
+#define W_TCB_RX_FRAG2_LEN_RAW    27
+#define S_TCB_RX_FRAG2_LEN_RAW    28
+#define M_TCB_RX_FRAG2_LEN_RAW    0xfffffffULL
+#define V_TCB_RX_FRAG2_LEN_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG2_LEN_RAW)
+
+/* 936:920 */
+#define W_TCB_RX_FRAG3_PTR_RAW    28
+#define S_TCB_RX_FRAG3_PTR_RAW    24
+#define M_TCB_RX_FRAG3_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG3_PTR_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG3_PTR_RAW)
+
+/* 964:937 */
+#define W_TCB_RX_FRAG3_LEN_RAW    29
+#define S_TCB_RX_FRAG3_LEN_RAW    9
+#define M_TCB_RX_FRAG3_LEN_RAW    0xfffffffULL
+#define V_TCB_RX_FRAG3_LEN_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG3_LEN_RAW)
+
+/* 992:965 */
+#define W_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    30
+#define S_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    5
+#define M_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    0xfffffffULL
+#define V_TCB_RX_FRAG3_START_IDX_OFFSET_RAW(x) ((__u64)(x) << S_TCB_RX_FRAG3_START_IDX_OFFSET_RAW)
+
+/* 1000:993 */
+#define W_TCB_PDU_HDR_LEN    31
+#define S_TCB_PDU_HDR_LEN    1
+#define M_TCB_PDU_HDR_LEN    0xffULL
+#define V_TCB_PDU_HDR_LEN(x) ((x) << S_TCB_PDU_HDR_LEN)
+
+/* 1023:1001 */
+#define W_TCB_AUX1_SLUSH1    31
+#define S_TCB_AUX1_SLUSH1    9
+#define M_TCB_AUX1_SLUSH1    0x7fffffULL
+#define V_TCB_AUX1_SLUSH1(x) ((x) << S_TCB_AUX1_SLUSH1)
+
+/* 840:832 */
+#define W_TCB_IRS_ULP    26
+#define S_TCB_IRS_ULP    0
+#define M_TCB_IRS_ULP    0x1ffULL
+#define V_TCB_IRS_ULP(x) ((x) << S_TCB_IRS_ULP)
+
+/* 849:841 */
+#define W_TCB_ISS_ULP    26
+#define S_TCB_ISS_ULP    9
+#define M_TCB_ISS_ULP    0x1ffULL
+#define V_TCB_ISS_ULP(x) ((x) << S_TCB_ISS_ULP)
+
+/* 863:850 */
+#define W_TCB_TX_PDU_LEN    26
+#define S_TCB_TX_PDU_LEN    18
+#define M_TCB_TX_PDU_LEN    0x3fffULL
+#define V_TCB_TX_PDU_LEN(x) ((x) << S_TCB_TX_PDU_LEN)
+
+/* 879:864 */
+#define W_TCB_CQ_IDX_SQ    27
+#define S_TCB_CQ_IDX_SQ    0
+#define M_TCB_CQ_IDX_SQ    0xffffULL
+#define V_TCB_CQ_IDX_SQ(x) ((x) << S_TCB_CQ_IDX_SQ)
+
+/* 895:880 */
+#define W_TCB_CQ_IDX_RQ    27
+#define S_TCB_CQ_IDX_RQ    16
+#define M_TCB_CQ_IDX_RQ    0xffffULL
+#define V_TCB_CQ_IDX_RQ(x) ((x) << S_TCB_CQ_IDX_RQ)
+
+/* 911:896 */
+#define W_TCB_QP_ID    28
+#define S_TCB_QP_ID    0
+#define M_TCB_QP_ID    0xffffULL
+#define V_TCB_QP_ID(x) ((x) << S_TCB_QP_ID)
+
+/* 927:912 */
+#define W_TCB_PD_ID    28
+#define S_TCB_PD_ID    16
+#define M_TCB_PD_ID    0xffffULL
+#define V_TCB_PD_ID(x) ((x) << S_TCB_PD_ID)
+
+/* 959:928 */
+#define W_TCB_STAG    29
+#define S_TCB_STAG    0
+#define M_TCB_STAG    0xffffffffULL
+#define V_TCB_STAG(x) ((x) << S_TCB_STAG)
+
+/* 985:960 */
+#define W_TCB_RQ_START    30
+#define S_TCB_RQ_START    0
+#define M_TCB_RQ_START    0x3ffffffULL
+#define V_TCB_RQ_START(x) ((x) << S_TCB_RQ_START)
+
+/* 998:986 */
+#define W_TCB_RQ_MSN    30
+#define S_TCB_RQ_MSN    26
+#define M_TCB_RQ_MSN    0x1fffULL
+#define V_TCB_RQ_MSN(x) ((__u64)(x) << S_TCB_RQ_MSN)
+
+/* 1002:999 */
+#define W_TCB_RQ_MAX_OFFSET    31
+#define S_TCB_RQ_MAX_OFFSET    7
+#define M_TCB_RQ_MAX_OFFSET    0xfULL
+#define V_TCB_RQ_MAX_OFFSET(x) ((x) << S_TCB_RQ_MAX_OFFSET)
+
+/* 1015:1003 */
+#define W_TCB_RQ_WRITE_PTR    31
+#define S_TCB_RQ_WRITE_PTR    11
+#define M_TCB_RQ_WRITE_PTR    0x1fffULL
+#define V_TCB_RQ_WRITE_PTR(x) ((x) << S_TCB_RQ_WRITE_PTR)
+
+/* 1019:1016 */
+#define W_TCB_RDMAP_OPCODE    31
+#define S_TCB_RDMAP_OPCODE    24
+#define M_TCB_RDMAP_OPCODE    0xfULL
+#define V_TCB_RDMAP_OPCODE(x) ((x) << S_TCB_RDMAP_OPCODE)
+
+/* 1020:1020 */
+#define W_TCB_ORD_L_BIT_VLD    31
+#define S_TCB_ORD_L_BIT_VLD    28
+#define M_TCB_ORD_L_BIT_VLD    0x1ULL
+#define V_TCB_ORD_L_BIT_VLD(x) ((x) << S_TCB_ORD_L_BIT_VLD)
+
+/* 1021:1021 */
+#define W_TCB_TX_FLUSH    31
+#define S_TCB_TX_FLUSH    29
+#define M_TCB_TX_FLUSH    0x1ULL
+#define V_TCB_TX_FLUSH(x) ((x) << S_TCB_TX_FLUSH)
+
+/* 1022:1022 */
+#define W_TCB_TX_OOS_RXMT    31
+#define S_TCB_TX_OOS_RXMT    30
+#define M_TCB_TX_OOS_RXMT    0x1ULL
+#define V_TCB_TX_OOS_RXMT(x) ((x) << S_TCB_TX_OOS_RXMT)
+
+/* 1023:1023 */
+#define W_TCB_TX_OOS_TXMT    31
+#define S_TCB_TX_OOS_TXMT    31
+#define M_TCB_TX_OOS_TXMT    0x1ULL
+#define V_TCB_TX_OOS_TXMT(x) ((x) << S_TCB_TX_OOS_TXMT)
+
+/* 855:832 */
+#define W_TCB_RX_DDP_BUF0_OFFSET    26
+#define S_TCB_RX_DDP_BUF0_OFFSET    0
+#define M_TCB_RX_DDP_BUF0_OFFSET    0xffffffULL
+#define V_TCB_RX_DDP_BUF0_OFFSET(x) ((x) << S_TCB_RX_DDP_BUF0_OFFSET)
+
+/* 879:856 */
+#define W_TCB_RX_DDP_BUF0_LEN    26
+#define S_TCB_RX_DDP_BUF0_LEN    24
+#define M_TCB_RX_DDP_BUF0_LEN    0xffffffULL
+#define V_TCB_RX_DDP_BUF0_LEN(x) ((__u64)(x) << S_TCB_RX_DDP_BUF0_LEN)
+
+/* 903:880 */
+#define W_TCB_RX_DDP_FLAGS    27
+#define S_TCB_RX_DDP_FLAGS    16
+#define M_TCB_RX_DDP_FLAGS    0xffffffULL
+#define V_TCB_RX_DDP_FLAGS(x) ((__u64)(x) << S_TCB_RX_DDP_FLAGS)
+
+/* 927:904 */
+#define W_TCB_RX_DDP_BUF1_OFFSET    28
+#define S_TCB_RX_DDP_BUF1_OFFSET    8
+#define M_TCB_RX_DDP_BUF1_OFFSET    0xffffffULL
+#define V_TCB_RX_DDP_BUF1_OFFSET(x) ((x) << S_TCB_RX_DDP_BUF1_OFFSET)
+
+/* 951:928 */
+#define W_TCB_RX_DDP_BUF1_LEN    29
+#define S_TCB_RX_DDP_BUF1_LEN    0
+#define M_TCB_RX_DDP_BUF1_LEN    0xffffffULL
+#define V_TCB_RX_DDP_BUF1_LEN(x) ((x) << S_TCB_RX_DDP_BUF1_LEN)
+
+/* 959:952 */
+#define W_TCB_AUX3_SLUSH    29
+#define S_TCB_AUX3_SLUSH    24
+#define M_TCB_AUX3_SLUSH    0xffULL
+#define V_TCB_AUX3_SLUSH(x) ((x) << S_TCB_AUX3_SLUSH)
+
+/* 991:960 */
+#define W_TCB_RX_DDP_BUF0_TAG    30
+#define S_TCB_RX_DDP_BUF0_TAG    0
+#define M_TCB_RX_DDP_BUF0_TAG    0xffffffffULL
+#define V_TCB_RX_DDP_BUF0_TAG(x) ((x) << S_TCB_RX_DDP_BUF0_TAG)
+
+/* 1023:992 */
+#define W_TCB_RX_DDP_BUF1_TAG    31
+#define S_TCB_RX_DDP_BUF1_TAG    0
+#define M_TCB_RX_DDP_BUF1_TAG    0xffffffffULL
+#define V_TCB_RX_DDP_BUF1_TAG(x) ((x) << S_TCB_RX_DDP_BUF1_TAG)
+
+#define S_TF_MIGRATING    0
+#define V_TF_MIGRATING(x) ((x) << S_TF_MIGRATING)
+
+#define S_TF_NON_OFFLOAD    1
+#define V_TF_NON_OFFLOAD(x) ((x) << S_TF_NON_OFFLOAD)
+
+#define S_TF_LOCK_TID    2
+#define V_TF_LOCK_TID(x) ((x) << S_TF_LOCK_TID)
+
+#define S_TF_KEEPALIVE    3
+#define V_TF_KEEPALIVE(x) ((x) << S_TF_KEEPALIVE)
+
+#define S_TF_DACK    4
+#define V_TF_DACK(x) ((x) << S_TF_DACK)
+
+#define S_TF_DACK_MSS    5
+#define V_TF_DACK_MSS(x) ((x) << S_TF_DACK_MSS)
+
+#define S_TF_DACK_NOT_ACKED    6
+#define V_TF_DACK_NOT_ACKED(x) ((x) << S_TF_DACK_NOT_ACKED)
+
+#define S_TF_NAGLE    7
+#define V_TF_NAGLE(x) ((x) << S_TF_NAGLE)
+
+#define S_TF_SSWS_DISABLED    8
+#define V_TF_SSWS_DISABLED(x) ((x) << S_TF_SSWS_DISABLED)
+
+#define S_TF_RX_FLOW_CONTROL_DDP    9
+#define V_TF_RX_FLOW_CONTROL_DDP(x) ((x) << S_TF_RX_FLOW_CONTROL_DDP)
+
+#define S_TF_RX_FLOW_CONTROL_DISABLE    10
+#define V_TF_RX_FLOW_CONTROL_DISABLE(x) ((x) << S_TF_RX_FLOW_CONTROL_DISABLE)
+
+#define S_TF_RX_CHANNEL    11
+#define V_TF_RX_CHANNEL(x) ((x) << S_TF_RX_CHANNEL)
+
+#define S_TF_TX_CHANNEL0    12
+#define V_TF_TX_CHANNEL0(x) ((x) << S_TF_TX_CHANNEL0)
+
+#define S_TF_TX_CHANNEL1    13
+#define V_TF_TX_CHANNEL1(x) ((x) << S_TF_TX_CHANNEL1)
+
+#define S_TF_TX_QUIESCE    14
+#define V_TF_TX_QUIESCE(x) ((x) << S_TF_TX_QUIESCE)
+
+#define S_TF_RX_QUIESCE    15
+#define V_TF_RX_QUIESCE(x) ((x) << S_TF_RX_QUIESCE)
+
+#define S_TF_TX_PACE_AUTO    16
+#define V_TF_TX_PACE_AUTO(x) ((x) << S_TF_TX_PACE_AUTO)
+
+#define S_TF_MASK_HASH    16
+#define V_TF_MASK_HASH(x) ((x) << S_TF_MASK_HASH)
+
+#define S_TF_TX_PACE_FIXED    17
+#define V_TF_TX_PACE_FIXED(x) ((x) << S_TF_TX_PACE_FIXED)
+
+#define S_TF_DIRECT_STEER_HASH    17
+#define V_TF_DIRECT_STEER_HASH(x) ((x) << S_TF_DIRECT_STEER_HASH)
+
+#define S_TF_TX_QUEUE    18
+#define M_TF_TX_QUEUE    0x7ULL
+#define V_TF_TX_QUEUE(x) ((x) << S_TF_TX_QUEUE)
+
+#define S_TF_TURBO    21
+#define V_TF_TURBO(x) ((x) << S_TF_TURBO)
+
+#define S_TF_REPORT_TID    21
+#define V_TF_REPORT_TID(x) ((x) << S_TF_REPORT_TID)
+
+#define S_TF_CCTRL_SEL0    22
+#define V_TF_CCTRL_SEL0(x) ((x) << S_TF_CCTRL_SEL0)
+
+#define S_TF_DROP    22
+#define V_TF_DROP(x) ((x) << S_TF_DROP)
+
+#define S_TF_CCTRL_SEL1    23
+#define V_TF_CCTRL_SEL1(x) ((x) << S_TF_CCTRL_SEL1)
+
+#define S_TF_DIRECT_STEER    23
+#define V_TF_DIRECT_STEER(x) ((x) << S_TF_DIRECT_STEER)
+
+#define S_TF_CORE_FIN    24
+#define V_TF_CORE_FIN(x) ((x) << S_TF_CORE_FIN)
+
+#define S_TF_CORE_URG    25
+#define V_TF_CORE_URG(x) ((x) << S_TF_CORE_URG)
+
+#define S_TF_CORE_MORE    26
+#define V_TF_CORE_MORE(x) ((x) << S_TF_CORE_MORE)
+
+#define S_TF_CORE_PUSH    27
+#define V_TF_CORE_PUSH(x) ((x) << S_TF_CORE_PUSH)
+
+#define S_TF_CORE_FLUSH    28
+#define V_TF_CORE_FLUSH(x) ((x) << S_TF_CORE_FLUSH)
+
+#define S_TF_RCV_COALESCE_ENABLE    29
+#define V_TF_RCV_COALESCE_ENABLE(x) ((x) << S_TF_RCV_COALESCE_ENABLE)
+
+#define S_TF_RCV_COALESCE_PUSH    30
+#define V_TF_RCV_COALESCE_PUSH(x) ((x) << S_TF_RCV_COALESCE_PUSH)
+
+#define S_TF_RCV_COALESCE_LAST_PSH    31
+#define V_TF_RCV_COALESCE_LAST_PSH(x) ((x) << S_TF_RCV_COALESCE_LAST_PSH)
+
+#define S_TF_RCV_COALESCE_HEARTBEAT    32
+#define V_TF_RCV_COALESCE_HEARTBEAT(x) ((__u64)(x) << S_TF_RCV_COALESCE_HEARTBEAT)
+
+#define S_TF_INIT    33
+#define V_TF_INIT(x) ((__u64)(x) << S_TF_INIT)
+
+#define S_TF_ACTIVE_OPEN    34
+#define V_TF_ACTIVE_OPEN(x) ((__u64)(x) << S_TF_ACTIVE_OPEN)
+
+#define S_TF_ASK_MODE    35
+#define V_TF_ASK_MODE(x) ((__u64)(x) << S_TF_ASK_MODE)
+
+#define S_TF_MOD_SCHD_REASON0    36
+#define V_TF_MOD_SCHD_REASON0(x) ((__u64)(x) << S_TF_MOD_SCHD_REASON0)
+
+#define S_TF_MOD_SCHD_REASON1    37
+#define V_TF_MOD_SCHD_REASON1(x) ((__u64)(x) << S_TF_MOD_SCHD_REASON1)
+
+#define S_TF_MOD_SCHD_REASON2    38
+#define V_TF_MOD_SCHD_REASON2(x) ((__u64)(x) << S_TF_MOD_SCHD_REASON2)
+
+#define S_TF_MOD_SCHD_TX    39
+#define V_TF_MOD_SCHD_TX(x) ((__u64)(x) << S_TF_MOD_SCHD_TX)
+
+#define S_TF_MOD_SCHD_RX    40
+#define V_TF_MOD_SCHD_RX(x) ((__u64)(x) << S_TF_MOD_SCHD_RX)
+
+#define S_TF_TIMER    41
+#define V_TF_TIMER(x) ((__u64)(x) << S_TF_TIMER)
+
+#define S_TF_DACK_TIMER    42
+#define V_TF_DACK_TIMER(x) ((__u64)(x) << S_TF_DACK_TIMER)
+
+#define S_TF_PEER_FIN    43
+#define V_TF_PEER_FIN(x) ((__u64)(x) << S_TF_PEER_FIN)
+
+#define S_TF_TX_COMPACT    44
+#define V_TF_TX_COMPACT(x) ((__u64)(x) << S_TF_TX_COMPACT)
+
+#define S_TF_RX_COMPACT    45
+#define V_TF_RX_COMPACT(x) ((__u64)(x) << S_TF_RX_COMPACT)
+
+#define S_TF_RDMA_ERROR    46
+#define V_TF_RDMA_ERROR(x) ((__u64)(x) << S_TF_RDMA_ERROR)
+
+#define S_TF_RDMA_FLM_ERROR    47
+#define V_TF_RDMA_FLM_ERROR(x) ((__u64)(x) << S_TF_RDMA_FLM_ERROR)
+
+#define S_TF_TX_PDU_OUT    48
+#define V_TF_TX_PDU_OUT(x) ((__u64)(x) << S_TF_TX_PDU_OUT)
+
+#define S_TF_RX_PDU_OUT    49
+#define V_TF_RX_PDU_OUT(x) ((__u64)(x) << S_TF_RX_PDU_OUT)
+
+#define S_TF_DUPACK_COUNT_ODD    50
+#define V_TF_DUPACK_COUNT_ODD(x) ((__u64)(x) << S_TF_DUPACK_COUNT_ODD)
+
+#define S_TF_FAST_RECOVERY    51
+#define V_TF_FAST_RECOVERY(x) ((__u64)(x) << S_TF_FAST_RECOVERY)
+
+#define S_TF_RECV_SCALE    52
+#define V_TF_RECV_SCALE(x) ((__u64)(x) << S_TF_RECV_SCALE)
+
+#define S_TF_RECV_TSTMP    53
+#define V_TF_RECV_TSTMP(x) ((__u64)(x) << S_TF_RECV_TSTMP)
+
+#define S_TF_RECV_SACK    54
+#define V_TF_RECV_SACK(x) ((__u64)(x) << S_TF_RECV_SACK)
+
+#define S_TF_PEND_CTL0    55
+#define V_TF_PEND_CTL0(x) ((__u64)(x) << S_TF_PEND_CTL0)
+
+#define S_TF_PEND_CTL1    56
+#define V_TF_PEND_CTL1(x) ((__u64)(x) << S_TF_PEND_CTL1)
+
+#define S_TF_PEND_CTL2    57
+#define V_TF_PEND_CTL2(x) ((__u64)(x) << S_TF_PEND_CTL2)
+
+#define S_TF_IP_VERSION    58
+#define V_TF_IP_VERSION(x) ((__u64)(x) << S_TF_IP_VERSION)
+
+#define S_TF_CCTRL_ECN    59
+#define V_TF_CCTRL_ECN(x) ((__u64)(x) << S_TF_CCTRL_ECN)
+
+#define S_TF_LPBK    59
+#define V_TF_LPBK(x) ((__u64)(x) << S_TF_LPBK)
+
+#define S_TF_CCTRL_ECE    60
+#define V_TF_CCTRL_ECE(x) ((__u64)(x) << S_TF_CCTRL_ECE)
+
+#define S_TF_REWRITE_DMAC    60
+#define V_TF_REWRITE_DMAC(x) ((__u64)(x) << S_TF_REWRITE_DMAC)
+
+#define S_TF_CCTRL_CWR    61
+#define V_TF_CCTRL_CWR(x) ((__u64)(x) << S_TF_CCTRL_CWR)
+
+#define S_TF_REWRITE_SMAC    61
+#define V_TF_REWRITE_SMAC(x) ((__u64)(x) << S_TF_REWRITE_SMAC)
+
+#define S_TF_CCTRL_RFR    62
+#define V_TF_CCTRL_RFR(x) ((__u64)(x) << S_TF_CCTRL_RFR)
+
+#define S_TF_DDP_INDICATE_OUT    16
+#define V_TF_DDP_INDICATE_OUT(x) ((x) << S_TF_DDP_INDICATE_OUT)
+
+#define S_TF_DDP_ACTIVE_BUF    17
+#define V_TF_DDP_ACTIVE_BUF(x) ((x) << S_TF_DDP_ACTIVE_BUF)
+
+#define S_TF_DDP_OFF    18
+#define V_TF_DDP_OFF(x) ((x) << S_TF_DDP_OFF)
+
+#define S_TF_DDP_WAIT_FRAG    19
+#define V_TF_DDP_WAIT_FRAG(x) ((x) << S_TF_DDP_WAIT_FRAG)
+
+#define S_TF_DDP_BUF_INF    20
+#define V_TF_DDP_BUF_INF(x) ((x) << S_TF_DDP_BUF_INF)
+
+#define S_TF_DDP_RX2TX    21
+#define V_TF_DDP_RX2TX(x) ((x) << S_TF_DDP_RX2TX)
+
+#define S_TF_DDP_BUF0_VALID    24
+#define V_TF_DDP_BUF0_VALID(x) ((x) << S_TF_DDP_BUF0_VALID)
+
+#define S_TF_DDP_BUF0_INDICATE    25
+#define V_TF_DDP_BUF0_INDICATE(x) ((x) << S_TF_DDP_BUF0_INDICATE)
+
+#define S_TF_DDP_BUF0_FLUSH    26
+#define V_TF_DDP_BUF0_FLUSH(x) ((x) << S_TF_DDP_BUF0_FLUSH)
+
+#define S_TF_DDP_PSHF_ENABLE_0    27
+#define V_TF_DDP_PSHF_ENABLE_0(x) ((x) << S_TF_DDP_PSHF_ENABLE_0)
+
+#define S_TF_DDP_PUSH_DISABLE_0    28
+#define V_TF_DDP_PUSH_DISABLE_0(x) ((x) << S_TF_DDP_PUSH_DISABLE_0)
+
+#define S_TF_DDP_PSH_NO_INVALIDATE0    29
+#define V_TF_DDP_PSH_NO_INVALIDATE0(x) ((x) << S_TF_DDP_PSH_NO_INVALIDATE0)
+
+#define S_TF_DDP_BUF1_VALID    32
+#define V_TF_DDP_BUF1_VALID(x) ((__u64)(x) << S_TF_DDP_BUF1_VALID)
+
+#define S_TF_DDP_BUF1_INDICATE    33
+#define V_TF_DDP_BUF1_INDICATE(x) ((__u64)(x) << S_TF_DDP_BUF1_INDICATE)
+
+#define S_TF_DDP_BUF1_FLUSH    34
+#define V_TF_DDP_BUF1_FLUSH(x) ((__u64)(x) << S_TF_DDP_BUF1_FLUSH)
+
+#define S_TF_DDP_PSHF_ENABLE_1    35
+#define V_TF_DDP_PSHF_ENABLE_1(x) ((__u64)(x) << S_TF_DDP_PSHF_ENABLE_1)
+
+#define S_TF_DDP_PUSH_DISABLE_1    36
+#define V_TF_DDP_PUSH_DISABLE_1(x) ((__u64)(x) << S_TF_DDP_PUSH_DISABLE_1)
+
+#define S_TF_DDP_PSH_NO_INVALIDATE1    37
+#define V_TF_DDP_PSH_NO_INVALIDATE1(x) ((__u64)(x) << S_TF_DDP_PSH_NO_INVALIDATE1)
+
+#endif /* _T4_TCB_DEFS_H */
diff --git a/drivers/net/cxgb4/t4fw_interface.h b/drivers/net/cxgb4/t4fw_interface.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4fw_interface.h
@@ -0,0 +1,7322 @@
+/*
+ * Chelsio Terminator 4 (T4) Firmware interface header file.
+ *
+ * Copyright (C) 2009-2012 Chelsio Communications.  All rights reserved.
+ *
+ * Written by felix marti (felix@chelsio.com)
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+#ifndef _T4FW_INTERFACE_H_
+#define _T4FW_INTERFACE_H_
+
+/******************************************************************************
+ *   R E T U R N   V A L U E S
+ ********************************/
+
+enum fw_retval{
+	FW_SUCCESS		= 0,	/* completed sucessfully */
+	FW_EPERM		= 1,	/* operation not permitted */
+	FW_ENOENT		= 2,	/* no such file or directory */
+	FW_EIO			= 5,	/* input/output error; hw bad */
+	FW_ENOEXEC		= 8,	/* exec format error; inv microcode */
+	FW_EAGAIN		= 11,	/* try again */
+	FW_ENOMEM		= 12,	/* out of memory */
+	FW_EFAULT		= 14,	/* bad address; fw bad */
+	FW_EBUSY		= 16,	/* resource busy */
+	FW_EEXIST		= 17,	/* file exists */
+	FW_EINVAL		= 22,	/* invalid argument */
+	FW_ENOSPC		= 28,	/* no space left on device */
+	FW_ENOSYS		= 38,	/* functionality not implemented */
+	FW_EPROTO		= 71,	/* protocol error */
+	FW_EADDRINUSE		= 98,	/* address already in use */
+	FW_EADDRNOTAVAIL	= 99,	/* cannot assigned requested address */
+	FW_ENETDOWN		= 100,	/* network is down */
+	FW_ENETUNREACH		= 101,	/* network is unreachable */
+	FW_ENOBUFS		= 105,	/* no buffer space available */
+	FW_ETIMEDOUT		= 110,	/* timeout */
+	FW_EINPROGRESS		= 115,	/* fw internal */
+	FW_SCSI_ABORT_REQUESTED	= 128,	/* */
+	FW_SCSI_ABORT_TIMEDOUT	= 129,	/* */
+	FW_SCSI_ABORTED		= 130,	/* */
+	FW_SCSI_CLOSE_REQUESTED	= 131,	/* */
+	FW_ERR_LINK_DOWN	= 132,	/* */
+	FW_RDEV_NOT_READY	= 133,	/* */
+	FW_ERR_RDEV_LOST	= 134,	/* */
+	FW_ERR_RDEV_LOGO	= 135,	/* */
+	FW_FCOE_NO_XCHG		= 136,	/* */
+	FW_SCSI_RSP_ERR		= 137,	/* */
+	FW_ERR_RDEV_IMPL_LOGO	= 138,	/* */
+	FW_SCSI_UNDER_FLOW_ERR  = 139,	/* */
+	FW_SCSI_OVER_FLOW_ERR   = 140,	/* */
+	FW_SCSI_DDP_ERR		= 141,	/* DDP error*/
+	FW_SCSI_TASK_ERR	= 142,	/* No SCSI tasks available */
+};
+
+/******************************************************************************
+ *   W O R K   R E Q U E S T s
+ ********************************/
+
+enum fw_wr_opcodes {
+	FW_FILTER_WR		= 0x02,
+	FW_ULPTX_WR		= 0x04,
+	FW_TP_WR		= 0x05,
+	FW_ETH_TX_PKT_WR	= 0x08,
+	FW_ETH_TX_PKTS_WR	= 0x09,
+	FW_ETH_TX_UO_WR		= 0x1c,
+	FW_EQ_FLUSH_WR		= 0x1b,
+	FW_OFLD_CONNECTION_WR	= 0x2f,
+	FW_FLOWC_WR		= 0x0a,
+	FW_OFLD_TX_DATA_WR	= 0x0b,
+	FW_CMD_WR		= 0x10,
+	FW_ETH_TX_PKT_VM_WR	= 0x11,
+	FW_RI_RES_WR		= 0x0c,
+	FW_RI_RDMA_WRITE_WR	= 0x14,
+	FW_RI_SEND_WR		= 0x15,
+	FW_RI_RDMA_READ_WR	= 0x16,
+	FW_RI_RECV_WR		= 0x17,
+	FW_RI_BIND_MW_WR	= 0x18,
+	FW_RI_FR_NSMR_WR	= 0x19,
+	FW_RI_INV_LSTAG_WR	= 0x1a,
+	FW_RI_WR		= 0x0d,
+	FW_CHNET_IFCONF_WR	= 0x6b,
+	FW_RDEV_WR		= 0x38,
+	FW_FOISCSI_NODE_WR	= 0x60,
+	FW_FOISCSI_CTRL_WR	= 0x6a,
+	FW_FOISCSI_CHAP_WR	= 0x6c,
+	FW_FCOE_ELS_CT_WR	= 0x30,
+	FW_SCSI_WRITE_WR	= 0x31,
+	FW_SCSI_READ_WR		= 0x32,
+	FW_SCSI_CMD_WR		= 0x33,
+	FW_SCSI_ABRT_CLS_WR	= 0x34,
+	FW_SCSI_TGT_ACC_WR	= 0x35,
+	FW_SCSI_TGT_XMIT_WR	= 0x36,
+	FW_SCSI_TGT_RSP_WR	= 0x37,
+	FW_LASTC2E_WR		= 0x70
+};
+
+/*
+ * Generic work request header flit0
+ */
+struct fw_wr_hdr {
+	__be32 hi;
+	__be32 lo;
+};
+
+/*	work request opcode (hi)
+ */
+#define S_FW_WR_OP		24
+#define M_FW_WR_OP		0xff
+#define V_FW_WR_OP(x)		((x) << S_FW_WR_OP)
+#define G_FW_WR_OP(x)		(((x) >> S_FW_WR_OP) & M_FW_WR_OP)
+
+/*	atomic flag (hi) - firmware encapsulates CPLs in CPL_BARRIER
+ */
+#define S_FW_WR_ATOMIC		23
+#define M_FW_WR_ATOMIC		0x1
+#define V_FW_WR_ATOMIC(x)	((x) << S_FW_WR_ATOMIC)
+#define G_FW_WR_ATOMIC(x)	\
+    (((x) >> S_FW_WR_ATOMIC) & M_FW_WR_ATOMIC)
+#define F_FW_WR_ATOMIC		V_FW_WR_ATOMIC(1U)
+
+/*	flush flag (hi) - firmware flushes flushable work request buffered
+ *			      in the flow context.
+ */
+#define S_FW_WR_FLUSH     22
+#define M_FW_WR_FLUSH     0x1
+#define V_FW_WR_FLUSH(x)  ((x) << S_FW_WR_FLUSH)
+#define G_FW_WR_FLUSH(x)  \
+    (((x) >> S_FW_WR_FLUSH) & M_FW_WR_FLUSH)
+#define F_FW_WR_FLUSH     V_FW_WR_FLUSH(1U)
+
+/*	completion flag (hi) - firmware generates a cpl_fw6_ack
+ */
+#define S_FW_WR_COMPL     21
+#define M_FW_WR_COMPL     0x1
+#define V_FW_WR_COMPL(x)  ((x) << S_FW_WR_COMPL)
+#define G_FW_WR_COMPL(x)  \
+    (((x) >> S_FW_WR_COMPL) & M_FW_WR_COMPL)
+#define F_FW_WR_COMPL     V_FW_WR_COMPL(1U)
+
+
+/*	work request immediate data lengh (hi)
+ */
+#define S_FW_WR_IMMDLEN	0
+#define M_FW_WR_IMMDLEN	0xff
+#define V_FW_WR_IMMDLEN(x)	((x) << S_FW_WR_IMMDLEN)
+#define G_FW_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_WR_IMMDLEN) & M_FW_WR_IMMDLEN)
+
+/*	egress queue status update to associated ingress queue entry (lo)
+ */
+#define S_FW_WR_EQUIQ		31
+#define M_FW_WR_EQUIQ		0x1
+#define V_FW_WR_EQUIQ(x)	((x) << S_FW_WR_EQUIQ)
+#define G_FW_WR_EQUIQ(x)	(((x) >> S_FW_WR_EQUIQ) & M_FW_WR_EQUIQ)
+#define F_FW_WR_EQUIQ		V_FW_WR_EQUIQ(1U)
+
+/*	egress queue status update to egress queue status entry (lo)
+ */
+#define S_FW_WR_EQUEQ		30
+#define M_FW_WR_EQUEQ		0x1
+#define V_FW_WR_EQUEQ(x)	((x) << S_FW_WR_EQUEQ)
+#define G_FW_WR_EQUEQ(x)	(((x) >> S_FW_WR_EQUEQ) & M_FW_WR_EQUEQ)
+#define F_FW_WR_EQUEQ		V_FW_WR_EQUEQ(1U)
+
+/*	flow context identifier (lo)
+ */
+#define S_FW_WR_FLOWID		8
+#define M_FW_WR_FLOWID		0xfffff
+#define V_FW_WR_FLOWID(x)	((x) << S_FW_WR_FLOWID)
+#define G_FW_WR_FLOWID(x)	(((x) >> S_FW_WR_FLOWID) & M_FW_WR_FLOWID)
+
+/*	length in units of 16-bytes (lo)
+ */
+#define S_FW_WR_LEN16		0
+#define M_FW_WR_LEN16		0xff
+#define V_FW_WR_LEN16(x)	((x) << S_FW_WR_LEN16)
+#define G_FW_WR_LEN16(x)	(((x) >> S_FW_WR_LEN16) & M_FW_WR_LEN16)
+
+/* valid filter configurations for compressed tuple
+ * Encodings: TPL - Compressed TUPLE for filter in addition to 4-tuple
+ * FR - FRAGMENT, FC - FCoE, MT - MPS MATCH TYPE, M - MPS MATCH,
+ * E - Ethertype, P - Port, PR - Protocol, T - TOS, IV - Inner VLAN,
+ * OV - Outer VLAN/VNIC_ID,
+*/
+#define HW_TPL_FR_MT_M_E_P_FC		0x3C3
+#define HW_TPL_FR_MT_M_PR_T_FC		0x3B3
+#define HW_TPL_FR_MT_M_IV_P_FC		0x38B
+#define HW_TPL_FR_MT_M_OV_P_FC		0x387
+#define HW_TPL_FR_MT_E_PR_T		0x370
+#define HW_TPL_FR_MT_E_PR_P_FC		0X363
+#define HW_TPL_FR_MT_E_T_P_FC		0X353
+#define HW_TPL_FR_MT_PR_IV_P_FC		0X32B
+#define HW_TPL_FR_MT_PR_OV_P_FC		0X327
+#define HW_TPL_FR_MT_T_IV_P_FC		0X31B
+#define HW_TPL_FR_MT_T_OV_P_FC		0X317
+#define HW_TPL_FR_M_E_PR_FC		0X2E1
+#define HW_TPL_FR_M_E_T_FC		0X2D1
+#define HW_TPL_FR_M_PR_IV_FC		0X2A9
+#define HW_TPL_FR_M_PR_OV_FC		0X2A5
+#define HW_TPL_FR_M_T_IV_FC		0X299
+#define HW_TPL_FR_M_T_OV_FC		0X295
+#define HW_TPL_FR_E_PR_T_P		0X272
+#define HW_TPL_FR_E_PR_T_FC		0X271
+#define HW_TPL_FR_E_IV_FC		0X249
+#define HW_TPL_FR_E_OV_FC		0X245
+#define HW_TPL_FR_PR_T_IV_FC		0X239
+#define HW_TPL_FR_PR_T_OV_FC		0X235
+#define HW_TPL_FR_IV_OV_FC		0X20D
+#define HW_TPL_MT_M_E_PR		0X1E0
+#define HW_TPL_MT_M_E_T			0X1D0
+#define HW_TPL_MT_E_PR_T_FC		0X171
+#define HW_TPL_MT_E_IV			0X148
+#define HW_TPL_MT_E_OV			0X144
+#define HW_TPL_MT_PR_T_IV		0X138
+#define HW_TPL_MT_PR_T_OV		0X134
+#define HW_TPL_M_E_PR_P			0X0E2
+#define HW_TPL_M_E_T_P			0X0D2
+#define HW_TPL_E_PR_T_P_FC		0X073
+#define HW_TPL_E_IV_P			0X04A
+#define HW_TPL_E_OV_P			0X046
+#define HW_TPL_PR_T_IV_P		0X03A
+#define HW_TPL_PR_T_OV_P		0X036
+
+/* filter wr reply code in cookie in CPL_SET_TCB_RPL */
+enum fw_filter_wr_cookie {
+	FW_FILTER_WR_SUCCESS,
+	FW_FILTER_WR_FLT_ADDED,
+	FW_FILTER_WR_FLT_DELETED,
+	FW_FILTER_WR_SMT_TBL_FULL,
+	FW_FILTER_WR_EINVAL,
+};
+
+struct fw_filter_wr {
+	__be32 op_pkd;
+	__be32 len16_pkd;
+	__be64 r3;
+	__be32 tid_to_iq;
+	__be32 del_filter_to_l2tix;
+	__be16 ethtype;
+	__be16 ethtypem;
+	__u8   frag_to_ovlan_vldm;
+	__u8   smac_sel;
+	__be16 rx_chan_rx_rpl_iq;
+	__be32 maci_to_matchtypem;
+	__u8   ptcl;
+	__u8   ptclm;
+	__u8   ttyp;
+	__u8   ttypm;
+	__be16 ivlan;
+	__be16 ivlanm;
+	__be16 ovlan;
+	__be16 ovlanm;
+	__u8   lip[16];
+	__u8   lipm[16];
+	__u8   fip[16];
+	__u8   fipm[16];
+	__be16 lp;
+	__be16 lpm;
+	__be16 fp;
+	__be16 fpm;
+	__be16 r7;
+	__u8   sma[6];
+};
+
+#define S_FW_FILTER_WR_TID	12
+#define M_FW_FILTER_WR_TID	0xfffff
+#define V_FW_FILTER_WR_TID(x)	((x) << S_FW_FILTER_WR_TID)
+#define G_FW_FILTER_WR_TID(x)	\
+    (((x) >> S_FW_FILTER_WR_TID) & M_FW_FILTER_WR_TID)
+
+#define S_FW_FILTER_WR_RQTYPE		11
+#define M_FW_FILTER_WR_RQTYPE		0x1
+#define V_FW_FILTER_WR_RQTYPE(x)	((x) << S_FW_FILTER_WR_RQTYPE)
+#define G_FW_FILTER_WR_RQTYPE(x)	\
+    (((x) >> S_FW_FILTER_WR_RQTYPE) & M_FW_FILTER_WR_RQTYPE)
+#define F_FW_FILTER_WR_RQTYPE	V_FW_FILTER_WR_RQTYPE(1U)
+
+#define S_FW_FILTER_WR_NOREPLY		10
+#define M_FW_FILTER_WR_NOREPLY		0x1
+#define V_FW_FILTER_WR_NOREPLY(x)	((x) << S_FW_FILTER_WR_NOREPLY)
+#define G_FW_FILTER_WR_NOREPLY(x)	\
+    (((x) >> S_FW_FILTER_WR_NOREPLY) & M_FW_FILTER_WR_NOREPLY)
+#define F_FW_FILTER_WR_NOREPLY	V_FW_FILTER_WR_NOREPLY(1U)
+
+#define S_FW_FILTER_WR_IQ	0
+#define M_FW_FILTER_WR_IQ	0x3ff
+#define V_FW_FILTER_WR_IQ(x)	((x) << S_FW_FILTER_WR_IQ)
+#define G_FW_FILTER_WR_IQ(x)	\
+    (((x) >> S_FW_FILTER_WR_IQ) & M_FW_FILTER_WR_IQ)
+
+#define S_FW_FILTER_WR_DEL_FILTER	31
+#define M_FW_FILTER_WR_DEL_FILTER	0x1
+#define V_FW_FILTER_WR_DEL_FILTER(x)	((x) << S_FW_FILTER_WR_DEL_FILTER)
+#define G_FW_FILTER_WR_DEL_FILTER(x)	\
+    (((x) >> S_FW_FILTER_WR_DEL_FILTER) & M_FW_FILTER_WR_DEL_FILTER)
+#define F_FW_FILTER_WR_DEL_FILTER	V_FW_FILTER_WR_DEL_FILTER(1U)
+
+#define S_FW_FILTER_WR_RPTTID		25
+#define M_FW_FILTER_WR_RPTTID		0x1
+#define V_FW_FILTER_WR_RPTTID(x)	((x) << S_FW_FILTER_WR_RPTTID)
+#define G_FW_FILTER_WR_RPTTID(x)	\
+    (((x) >> S_FW_FILTER_WR_RPTTID) & M_FW_FILTER_WR_RPTTID)
+#define F_FW_FILTER_WR_RPTTID	V_FW_FILTER_WR_RPTTID(1U)
+
+#define S_FW_FILTER_WR_DROP	24
+#define M_FW_FILTER_WR_DROP	0x1
+#define V_FW_FILTER_WR_DROP(x)	((x) << S_FW_FILTER_WR_DROP)
+#define G_FW_FILTER_WR_DROP(x)	\
+    (((x) >> S_FW_FILTER_WR_DROP) & M_FW_FILTER_WR_DROP)
+#define F_FW_FILTER_WR_DROP	V_FW_FILTER_WR_DROP(1U)
+
+#define S_FW_FILTER_WR_DIRSTEER		23
+#define M_FW_FILTER_WR_DIRSTEER		0x1
+#define V_FW_FILTER_WR_DIRSTEER(x)	((x) << S_FW_FILTER_WR_DIRSTEER)
+#define G_FW_FILTER_WR_DIRSTEER(x)	\
+    (((x) >> S_FW_FILTER_WR_DIRSTEER) & M_FW_FILTER_WR_DIRSTEER)
+#define F_FW_FILTER_WR_DIRSTEER	V_FW_FILTER_WR_DIRSTEER(1U)
+
+#define S_FW_FILTER_WR_MASKHASH		22
+#define M_FW_FILTER_WR_MASKHASH		0x1
+#define V_FW_FILTER_WR_MASKHASH(x)	((x) << S_FW_FILTER_WR_MASKHASH)
+#define G_FW_FILTER_WR_MASKHASH(x)	\
+    (((x) >> S_FW_FILTER_WR_MASKHASH) & M_FW_FILTER_WR_MASKHASH)
+#define F_FW_FILTER_WR_MASKHASH	V_FW_FILTER_WR_MASKHASH(1U)
+
+#define S_FW_FILTER_WR_DIRSTEERHASH	21
+#define M_FW_FILTER_WR_DIRSTEERHASH	0x1
+#define V_FW_FILTER_WR_DIRSTEERHASH(x)	((x) << S_FW_FILTER_WR_DIRSTEERHASH)
+#define G_FW_FILTER_WR_DIRSTEERHASH(x)	\
+    (((x) >> S_FW_FILTER_WR_DIRSTEERHASH) & M_FW_FILTER_WR_DIRSTEERHASH)
+#define F_FW_FILTER_WR_DIRSTEERHASH	V_FW_FILTER_WR_DIRSTEERHASH(1U)
+
+#define S_FW_FILTER_WR_LPBK	20
+#define M_FW_FILTER_WR_LPBK	0x1
+#define V_FW_FILTER_WR_LPBK(x)	((x) << S_FW_FILTER_WR_LPBK)
+#define G_FW_FILTER_WR_LPBK(x)	\
+    (((x) >> S_FW_FILTER_WR_LPBK) & M_FW_FILTER_WR_LPBK)
+#define F_FW_FILTER_WR_LPBK	V_FW_FILTER_WR_LPBK(1U)
+
+#define S_FW_FILTER_WR_DMAC	19
+#define M_FW_FILTER_WR_DMAC	0x1
+#define V_FW_FILTER_WR_DMAC(x)	((x) << S_FW_FILTER_WR_DMAC)
+#define G_FW_FILTER_WR_DMAC(x)	\
+    (((x) >> S_FW_FILTER_WR_DMAC) & M_FW_FILTER_WR_DMAC)
+#define F_FW_FILTER_WR_DMAC	V_FW_FILTER_WR_DMAC(1U)
+
+#define S_FW_FILTER_WR_SMAC	18
+#define M_FW_FILTER_WR_SMAC	0x1
+#define V_FW_FILTER_WR_SMAC(x)	((x) << S_FW_FILTER_WR_SMAC)
+#define G_FW_FILTER_WR_SMAC(x)	\
+    (((x) >> S_FW_FILTER_WR_SMAC) & M_FW_FILTER_WR_SMAC)
+#define F_FW_FILTER_WR_SMAC	V_FW_FILTER_WR_SMAC(1U)
+
+#define S_FW_FILTER_WR_INSVLAN		17
+#define M_FW_FILTER_WR_INSVLAN		0x1
+#define V_FW_FILTER_WR_INSVLAN(x)	((x) << S_FW_FILTER_WR_INSVLAN)
+#define G_FW_FILTER_WR_INSVLAN(x)	\
+    (((x) >> S_FW_FILTER_WR_INSVLAN) & M_FW_FILTER_WR_INSVLAN)
+#define F_FW_FILTER_WR_INSVLAN	V_FW_FILTER_WR_INSVLAN(1U)
+
+#define S_FW_FILTER_WR_RMVLAN		16
+#define M_FW_FILTER_WR_RMVLAN		0x1
+#define V_FW_FILTER_WR_RMVLAN(x)	((x) << S_FW_FILTER_WR_RMVLAN)
+#define G_FW_FILTER_WR_RMVLAN(x)	\
+    (((x) >> S_FW_FILTER_WR_RMVLAN) & M_FW_FILTER_WR_RMVLAN)
+#define F_FW_FILTER_WR_RMVLAN	V_FW_FILTER_WR_RMVLAN(1U)
+
+#define S_FW_FILTER_WR_HITCNTS		15
+#define M_FW_FILTER_WR_HITCNTS		0x1
+#define V_FW_FILTER_WR_HITCNTS(x)	((x) << S_FW_FILTER_WR_HITCNTS)
+#define G_FW_FILTER_WR_HITCNTS(x)	\
+    (((x) >> S_FW_FILTER_WR_HITCNTS) & M_FW_FILTER_WR_HITCNTS)
+#define F_FW_FILTER_WR_HITCNTS	V_FW_FILTER_WR_HITCNTS(1U)
+
+#define S_FW_FILTER_WR_TXCHAN		13
+#define M_FW_FILTER_WR_TXCHAN		0x3
+#define V_FW_FILTER_WR_TXCHAN(x)	((x) << S_FW_FILTER_WR_TXCHAN)
+#define G_FW_FILTER_WR_TXCHAN(x)	\
+    (((x) >> S_FW_FILTER_WR_TXCHAN) & M_FW_FILTER_WR_TXCHAN)
+
+#define S_FW_FILTER_WR_PRIO	12
+#define M_FW_FILTER_WR_PRIO	0x1
+#define V_FW_FILTER_WR_PRIO(x)	((x) << S_FW_FILTER_WR_PRIO)
+#define G_FW_FILTER_WR_PRIO(x)	\
+    (((x) >> S_FW_FILTER_WR_PRIO) & M_FW_FILTER_WR_PRIO)
+#define F_FW_FILTER_WR_PRIO	V_FW_FILTER_WR_PRIO(1U)
+
+#define S_FW_FILTER_WR_L2TIX	0
+#define M_FW_FILTER_WR_L2TIX	0xfff
+#define V_FW_FILTER_WR_L2TIX(x)	((x) << S_FW_FILTER_WR_L2TIX)
+#define G_FW_FILTER_WR_L2TIX(x)	\
+    (((x) >> S_FW_FILTER_WR_L2TIX) & M_FW_FILTER_WR_L2TIX)
+
+#define S_FW_FILTER_WR_FRAG	7
+#define M_FW_FILTER_WR_FRAG	0x1
+#define V_FW_FILTER_WR_FRAG(x)	((x) << S_FW_FILTER_WR_FRAG)
+#define G_FW_FILTER_WR_FRAG(x)	\
+    (((x) >> S_FW_FILTER_WR_FRAG) & M_FW_FILTER_WR_FRAG)
+#define F_FW_FILTER_WR_FRAG	V_FW_FILTER_WR_FRAG(1U)
+
+#define S_FW_FILTER_WR_FRAGM	6
+#define M_FW_FILTER_WR_FRAGM	0x1
+#define V_FW_FILTER_WR_FRAGM(x)	((x) << S_FW_FILTER_WR_FRAGM)
+#define G_FW_FILTER_WR_FRAGM(x)	\
+    (((x) >> S_FW_FILTER_WR_FRAGM) & M_FW_FILTER_WR_FRAGM)
+#define F_FW_FILTER_WR_FRAGM	V_FW_FILTER_WR_FRAGM(1U)
+
+#define S_FW_FILTER_WR_IVLAN_VLD	5
+#define M_FW_FILTER_WR_IVLAN_VLD	0x1
+#define V_FW_FILTER_WR_IVLAN_VLD(x)	((x) << S_FW_FILTER_WR_IVLAN_VLD)
+#define G_FW_FILTER_WR_IVLAN_VLD(x)	\
+    (((x) >> S_FW_FILTER_WR_IVLAN_VLD) & M_FW_FILTER_WR_IVLAN_VLD)
+#define F_FW_FILTER_WR_IVLAN_VLD	V_FW_FILTER_WR_IVLAN_VLD(1U)
+
+#define S_FW_FILTER_WR_OVLAN_VLD	4
+#define M_FW_FILTER_WR_OVLAN_VLD	0x1
+#define V_FW_FILTER_WR_OVLAN_VLD(x)	((x) << S_FW_FILTER_WR_OVLAN_VLD)
+#define G_FW_FILTER_WR_OVLAN_VLD(x)	\
+    (((x) >> S_FW_FILTER_WR_OVLAN_VLD) & M_FW_FILTER_WR_OVLAN_VLD)
+#define F_FW_FILTER_WR_OVLAN_VLD	V_FW_FILTER_WR_OVLAN_VLD(1U)
+
+#define S_FW_FILTER_WR_IVLAN_VLDM	3
+#define M_FW_FILTER_WR_IVLAN_VLDM	0x1
+#define V_FW_FILTER_WR_IVLAN_VLDM(x)	((x) << S_FW_FILTER_WR_IVLAN_VLDM)
+#define G_FW_FILTER_WR_IVLAN_VLDM(x)	\
+    (((x) >> S_FW_FILTER_WR_IVLAN_VLDM) & M_FW_FILTER_WR_IVLAN_VLDM)
+#define F_FW_FILTER_WR_IVLAN_VLDM	V_FW_FILTER_WR_IVLAN_VLDM(1U)
+
+#define S_FW_FILTER_WR_OVLAN_VLDM	2
+#define M_FW_FILTER_WR_OVLAN_VLDM	0x1
+#define V_FW_FILTER_WR_OVLAN_VLDM(x)	((x) << S_FW_FILTER_WR_OVLAN_VLDM)
+#define G_FW_FILTER_WR_OVLAN_VLDM(x)	\
+    (((x) >> S_FW_FILTER_WR_OVLAN_VLDM) & M_FW_FILTER_WR_OVLAN_VLDM)
+#define F_FW_FILTER_WR_OVLAN_VLDM	V_FW_FILTER_WR_OVLAN_VLDM(1U)
+
+#define S_FW_FILTER_WR_RX_CHAN		15
+#define M_FW_FILTER_WR_RX_CHAN		0x1
+#define V_FW_FILTER_WR_RX_CHAN(x)	((x) << S_FW_FILTER_WR_RX_CHAN)
+#define G_FW_FILTER_WR_RX_CHAN(x)	\
+    (((x) >> S_FW_FILTER_WR_RX_CHAN) & M_FW_FILTER_WR_RX_CHAN)
+#define F_FW_FILTER_WR_RX_CHAN	V_FW_FILTER_WR_RX_CHAN(1U)
+
+#define S_FW_FILTER_WR_RX_RPL_IQ	0
+#define M_FW_FILTER_WR_RX_RPL_IQ	0x3ff
+#define V_FW_FILTER_WR_RX_RPL_IQ(x)	((x) << S_FW_FILTER_WR_RX_RPL_IQ)
+#define G_FW_FILTER_WR_RX_RPL_IQ(x)	\
+    (((x) >> S_FW_FILTER_WR_RX_RPL_IQ) & M_FW_FILTER_WR_RX_RPL_IQ)
+
+#define S_FW_FILTER_WR_MACI	23
+#define M_FW_FILTER_WR_MACI	0x1ff
+#define V_FW_FILTER_WR_MACI(x)	((x) << S_FW_FILTER_WR_MACI)
+#define G_FW_FILTER_WR_MACI(x)	\
+    (((x) >> S_FW_FILTER_WR_MACI) & M_FW_FILTER_WR_MACI)
+
+#define S_FW_FILTER_WR_MACIM	14
+#define M_FW_FILTER_WR_MACIM	0x1ff
+#define V_FW_FILTER_WR_MACIM(x)	((x) << S_FW_FILTER_WR_MACIM)
+#define G_FW_FILTER_WR_MACIM(x)	\
+    (((x) >> S_FW_FILTER_WR_MACIM) & M_FW_FILTER_WR_MACIM)
+
+#define S_FW_FILTER_WR_FCOE	13
+#define M_FW_FILTER_WR_FCOE	0x1
+#define V_FW_FILTER_WR_FCOE(x)	((x) << S_FW_FILTER_WR_FCOE)
+#define G_FW_FILTER_WR_FCOE(x)	\
+    (((x) >> S_FW_FILTER_WR_FCOE) & M_FW_FILTER_WR_FCOE)
+#define F_FW_FILTER_WR_FCOE	V_FW_FILTER_WR_FCOE(1U)
+
+#define S_FW_FILTER_WR_FCOEM	12
+#define M_FW_FILTER_WR_FCOEM	0x1
+#define V_FW_FILTER_WR_FCOEM(x)	((x) << S_FW_FILTER_WR_FCOEM)
+#define G_FW_FILTER_WR_FCOEM(x)	\
+    (((x) >> S_FW_FILTER_WR_FCOEM) & M_FW_FILTER_WR_FCOEM)
+#define F_FW_FILTER_WR_FCOEM	V_FW_FILTER_WR_FCOEM(1U)
+
+#define S_FW_FILTER_WR_PORT	9
+#define M_FW_FILTER_WR_PORT	0x7
+#define V_FW_FILTER_WR_PORT(x)	((x) << S_FW_FILTER_WR_PORT)
+#define G_FW_FILTER_WR_PORT(x)	\
+    (((x) >> S_FW_FILTER_WR_PORT) & M_FW_FILTER_WR_PORT)
+
+#define S_FW_FILTER_WR_PORTM	6
+#define M_FW_FILTER_WR_PORTM	0x7
+#define V_FW_FILTER_WR_PORTM(x)	((x) << S_FW_FILTER_WR_PORTM)
+#define G_FW_FILTER_WR_PORTM(x)	\
+    (((x) >> S_FW_FILTER_WR_PORTM) & M_FW_FILTER_WR_PORTM)
+
+#define S_FW_FILTER_WR_MATCHTYPE	3
+#define M_FW_FILTER_WR_MATCHTYPE	0x7
+#define V_FW_FILTER_WR_MATCHTYPE(x)	((x) << S_FW_FILTER_WR_MATCHTYPE)
+#define G_FW_FILTER_WR_MATCHTYPE(x)	\
+    (((x) >> S_FW_FILTER_WR_MATCHTYPE) & M_FW_FILTER_WR_MATCHTYPE)
+
+#define S_FW_FILTER_WR_MATCHTYPEM	0
+#define M_FW_FILTER_WR_MATCHTYPEM	0x7
+#define V_FW_FILTER_WR_MATCHTYPEM(x)	((x) << S_FW_FILTER_WR_MATCHTYPEM)
+#define G_FW_FILTER_WR_MATCHTYPEM(x)	\
+    (((x) >> S_FW_FILTER_WR_MATCHTYPEM) & M_FW_FILTER_WR_MATCHTYPEM)
+
+struct fw_ulptx_wr {
+	__be32 op_to_compl;
+	__be32 flowid_len16;
+	__u64  cookie;
+};
+
+struct fw_tp_wr {
+	__be32 op_to_immdlen;
+	__be32 flowid_len16;
+	__u64  cookie;
+};
+
+struct fw_eth_tx_pkt_wr {
+	__be32 op_immdlen;
+	__be32 equiq_to_len16;
+	__be64 r3;
+};
+
+#define S_FW_ETH_TX_PKT_WR_IMMDLEN	0
+#define M_FW_ETH_TX_PKT_WR_IMMDLEN	0x1ff
+#define V_FW_ETH_TX_PKT_WR_IMMDLEN(x)	((x) << S_FW_ETH_TX_PKT_WR_IMMDLEN)
+#define G_FW_ETH_TX_PKT_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_ETH_TX_PKT_WR_IMMDLEN) & M_FW_ETH_TX_PKT_WR_IMMDLEN)
+
+struct fw_eth_tx_pkts_wr {
+	__be32 op_pkd;
+	__be32 equiq_to_len16;
+	__be32 r3;
+	__be16 plen;
+	__u8   npkt;
+	__u8   type;
+};
+
+struct fw_eth_tx_uo_wr {
+	__be32 op_immdlen;
+	__be32 equiq_to_len16;
+	__be64 r3;
+	__be16 ethlen;
+	__be16 iplen;
+	__be16 udplen;
+	__be16 mss;
+	__be32 length;
+	__be32 r4;
+};
+
+struct fw_eq_flush_wr {
+	__u8   opcode;
+	__u8   r1[3];
+	__be32 equiq_to_len16;
+	__be64 r3;
+};
+
+struct fw_ofld_connection_wr {
+	__be32 op_compl;
+	__be32 len16_pkd;
+	__u64  cookie;
+	__be64 r2;
+	__be64 r3;
+	struct fw_ofld_connection_le {
+		__be32 version_cpl;
+		__be32 filter;
+		__be32 r1;
+		__be16 lport;
+		__be16 pport;
+		union fw_ofld_connection_leip {
+			struct fw_ofld_connection_le_ipv4 {
+				__be32 pip;
+				__be32 lip;
+				__be64 r0;
+				__be64 r1;
+				__be64 r2;
+			} ipv4;
+			struct fw_ofld_connection_le_ipv6 {
+				__be64 pip_hi;
+				__be64 pip_lo;
+				__be64 lip_hi;
+				__be64 lip_lo;
+			} ipv6;
+		} u;
+	} le;
+	struct fw_ofld_connection_tcb {
+		__be32 t_state_to_astid;
+		__be16 cplrxdataack_cplpassacceptrpl;
+		__be16 rcv_adv;
+		__be32 rcv_nxt;
+		__be32 tx_max;
+		__be64 opt0;
+		__be32 opt2;
+		__be32 r1;
+		__be64 r2;
+		__be64 r3;
+	} tcb;
+};
+
+#define S_FW_OFLD_CONNECTION_WR_VERSION		31
+#define M_FW_OFLD_CONNECTION_WR_VERSION		0x1
+#define V_FW_OFLD_CONNECTION_WR_VERSION(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_VERSION)
+#define G_FW_OFLD_CONNECTION_WR_VERSION(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_VERSION) & \
+     M_FW_OFLD_CONNECTION_WR_VERSION)
+#define F_FW_OFLD_CONNECTION_WR_VERSION	V_FW_OFLD_CONNECTION_WR_VERSION(1U)
+
+#define S_FW_OFLD_CONNECTION_WR_CPL	30
+#define M_FW_OFLD_CONNECTION_WR_CPL	0x1
+#define V_FW_OFLD_CONNECTION_WR_CPL(x)	((x) << S_FW_OFLD_CONNECTION_WR_CPL)
+#define G_FW_OFLD_CONNECTION_WR_CPL(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_CPL) & M_FW_OFLD_CONNECTION_WR_CPL)
+#define F_FW_OFLD_CONNECTION_WR_CPL	V_FW_OFLD_CONNECTION_WR_CPL(1U)
+
+#define S_FW_OFLD_CONNECTION_WR_T_STATE		28
+#define M_FW_OFLD_CONNECTION_WR_T_STATE		0xf
+#define V_FW_OFLD_CONNECTION_WR_T_STATE(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_T_STATE)
+#define G_FW_OFLD_CONNECTION_WR_T_STATE(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_T_STATE) & \
+     M_FW_OFLD_CONNECTION_WR_T_STATE)
+
+#define S_FW_OFLD_CONNECTION_WR_RCV_SCALE	24
+#define M_FW_OFLD_CONNECTION_WR_RCV_SCALE	0xf
+#define V_FW_OFLD_CONNECTION_WR_RCV_SCALE(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_RCV_SCALE)
+#define G_FW_OFLD_CONNECTION_WR_RCV_SCALE(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_RCV_SCALE) & \
+     M_FW_OFLD_CONNECTION_WR_RCV_SCALE)
+
+#define S_FW_OFLD_CONNECTION_WR_ASTID		0
+#define M_FW_OFLD_CONNECTION_WR_ASTID		0xffffff
+#define V_FW_OFLD_CONNECTION_WR_ASTID(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_ASTID)
+#define G_FW_OFLD_CONNECTION_WR_ASTID(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_ASTID) & M_FW_OFLD_CONNECTION_WR_ASTID)
+
+#define S_FW_OFLD_CONNECTION_WR_CPLRXDATAACK	15
+#define M_FW_OFLD_CONNECTION_WR_CPLRXDATAACK	0x1
+#define V_FW_OFLD_CONNECTION_WR_CPLRXDATAACK(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_CPLRXDATAACK)
+#define G_FW_OFLD_CONNECTION_WR_CPLRXDATAACK(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_CPLRXDATAACK) & \
+     M_FW_OFLD_CONNECTION_WR_CPLRXDATAACK)
+#define F_FW_OFLD_CONNECTION_WR_CPLRXDATAACK	\
+    V_FW_OFLD_CONNECTION_WR_CPLRXDATAACK(1U)
+
+#define S_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL	14
+#define M_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL	0x1
+#define V_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL(x)	\
+    ((x) << S_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL)
+#define G_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL(x)	\
+    (((x) >> S_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL) & \
+     M_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL)
+#define F_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL	\
+    V_FW_OFLD_CONNECTION_WR_CPLPASSACCEPTRPL(1U)
+
+enum fw_flowc_mnem_tcpstate {
+	FW_FLOWC_MNEM_TCPSTATE_CLOSED	= 0, /* illegal */
+	FW_FLOWC_MNEM_TCPSTATE_LISTEN	= 1, /* illegal */
+	FW_FLOWC_MNEM_TCPSTATE_SYNSENT	= 2, /* illegal */
+	FW_FLOWC_MNEM_TCPSTATE_SYNRECEIVED = 3, /* illegal */
+	FW_FLOWC_MNEM_TCPSTATE_ESTABLISHED = 4, /* default */
+	FW_FLOWC_MNEM_TCPSTATE_CLOSEWAIT = 5, /* got peer close already */
+	FW_FLOWC_MNEM_TCPSTATE_FINWAIT1	= 6, /* haven't gotten ACK for FIN and
+					      * will resend FIN - equiv ESTAB
+					      */
+	FW_FLOWC_MNEM_TCPSTATE_CLOSING	= 7, /* haven't gotten ACK for FIN and
+					      * will resend FIN but have
+					      * received FIN
+					      */
+	FW_FLOWC_MNEM_TCPSTATE_LASTACK	= 8, /* haven't gotten ACK for FIN and
+					      * will resend FIN but have
+					      * received FIN
+					      */
+	FW_FLOWC_MNEM_TCPSTATE_FINWAIT2	= 9, /* sent FIN and got FIN + ACK,
+					      * waiting for FIN
+					      */
+	FW_FLOWC_MNEM_TCPSTATE_TIMEWAIT	= 10, /* not expected */
+};
+
+enum fw_flowc_mnem {
+	FW_FLOWC_MNEM_PFNVFN,		/* PFN [15:8] VFN [7:0] */
+	FW_FLOWC_MNEM_CH,
+	FW_FLOWC_MNEM_PORT,
+	FW_FLOWC_MNEM_IQID,
+	FW_FLOWC_MNEM_SNDNXT,
+	FW_FLOWC_MNEM_RCVNXT,
+	FW_FLOWC_MNEM_SNDBUF,
+	FW_FLOWC_MNEM_MSS,
+	FW_FLOWC_MNEM_TXDATAPLEN_MAX,
+	FW_FLOWC_MNEM_TCPSTATE,
+};
+
+struct fw_flowc_mnemval {
+	__u8   mnemonic;
+	__u8   r4[3];
+	__be32 val;
+};
+
+struct fw_flowc_wr {
+	__be32 op_to_nparams;
+	__be32 flowid_len16;
+#ifndef C99_NOT_SUPPORTED
+	struct fw_flowc_mnemval mnemval[0];
+#endif
+};
+
+#define S_FW_FLOWC_WR_NPARAMS		0
+#define M_FW_FLOWC_WR_NPARAMS		0xff
+#define V_FW_FLOWC_WR_NPARAMS(x)	((x) << S_FW_FLOWC_WR_NPARAMS)
+#define G_FW_FLOWC_WR_NPARAMS(x)	\
+    (((x) >> S_FW_FLOWC_WR_NPARAMS) & M_FW_FLOWC_WR_NPARAMS)
+
+struct fw_ofld_tx_data_wr {
+	__be32 op_to_immdlen;
+	__be32 flowid_len16;
+	__be32 plen;
+	__be32 tunnel_to_proxy;
+};
+
+#define S_FW_OFLD_TX_DATA_WR_TUNNEL	19
+#define M_FW_OFLD_TX_DATA_WR_TUNNEL	0x1
+#define V_FW_OFLD_TX_DATA_WR_TUNNEL(x)	((x) << S_FW_OFLD_TX_DATA_WR_TUNNEL)
+#define G_FW_OFLD_TX_DATA_WR_TUNNEL(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_TUNNEL) & M_FW_OFLD_TX_DATA_WR_TUNNEL)
+#define F_FW_OFLD_TX_DATA_WR_TUNNEL	V_FW_OFLD_TX_DATA_WR_TUNNEL(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_SAVE	18
+#define M_FW_OFLD_TX_DATA_WR_SAVE	0x1
+#define V_FW_OFLD_TX_DATA_WR_SAVE(x)	((x) << S_FW_OFLD_TX_DATA_WR_SAVE)
+#define G_FW_OFLD_TX_DATA_WR_SAVE(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_SAVE) & M_FW_OFLD_TX_DATA_WR_SAVE)
+#define F_FW_OFLD_TX_DATA_WR_SAVE	V_FW_OFLD_TX_DATA_WR_SAVE(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_FLUSH	17
+#define M_FW_OFLD_TX_DATA_WR_FLUSH	0x1
+#define V_FW_OFLD_TX_DATA_WR_FLUSH(x)	((x) << S_FW_OFLD_TX_DATA_WR_FLUSH)
+#define G_FW_OFLD_TX_DATA_WR_FLUSH(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_FLUSH) & M_FW_OFLD_TX_DATA_WR_FLUSH)
+#define F_FW_OFLD_TX_DATA_WR_FLUSH	V_FW_OFLD_TX_DATA_WR_FLUSH(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_URGENT	16
+#define M_FW_OFLD_TX_DATA_WR_URGENT	0x1
+#define V_FW_OFLD_TX_DATA_WR_URGENT(x)	((x) << S_FW_OFLD_TX_DATA_WR_URGENT)
+#define G_FW_OFLD_TX_DATA_WR_URGENT(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_URGENT) & M_FW_OFLD_TX_DATA_WR_URGENT)
+#define F_FW_OFLD_TX_DATA_WR_URGENT	V_FW_OFLD_TX_DATA_WR_URGENT(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_MORE	15
+#define M_FW_OFLD_TX_DATA_WR_MORE	0x1
+#define V_FW_OFLD_TX_DATA_WR_MORE(x)	((x) << S_FW_OFLD_TX_DATA_WR_MORE)
+#define G_FW_OFLD_TX_DATA_WR_MORE(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_MORE) & M_FW_OFLD_TX_DATA_WR_MORE)
+#define F_FW_OFLD_TX_DATA_WR_MORE	V_FW_OFLD_TX_DATA_WR_MORE(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_SHOVE	14
+#define M_FW_OFLD_TX_DATA_WR_SHOVE	0x1
+#define V_FW_OFLD_TX_DATA_WR_SHOVE(x)	((x) << S_FW_OFLD_TX_DATA_WR_SHOVE)
+#define G_FW_OFLD_TX_DATA_WR_SHOVE(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_SHOVE) & M_FW_OFLD_TX_DATA_WR_SHOVE)
+#define F_FW_OFLD_TX_DATA_WR_SHOVE	V_FW_OFLD_TX_DATA_WR_SHOVE(1U)
+
+#define S_FW_OFLD_TX_DATA_WR_ULPMODE	10
+#define M_FW_OFLD_TX_DATA_WR_ULPMODE	0xf
+#define V_FW_OFLD_TX_DATA_WR_ULPMODE(x)	((x) << S_FW_OFLD_TX_DATA_WR_ULPMODE)
+#define G_FW_OFLD_TX_DATA_WR_ULPMODE(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_ULPMODE) & M_FW_OFLD_TX_DATA_WR_ULPMODE)
+
+#define S_FW_OFLD_TX_DATA_WR_ULPSUBMODE		6
+#define M_FW_OFLD_TX_DATA_WR_ULPSUBMODE		0xf
+#define V_FW_OFLD_TX_DATA_WR_ULPSUBMODE(x)	\
+    ((x) << S_FW_OFLD_TX_DATA_WR_ULPSUBMODE)
+#define G_FW_OFLD_TX_DATA_WR_ULPSUBMODE(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_ULPSUBMODE) & \
+     M_FW_OFLD_TX_DATA_WR_ULPSUBMODE)
+
+#define S_FW_OFLD_TX_DATA_WR_PROXY	5
+#define M_FW_OFLD_TX_DATA_WR_PROXY	0x1
+#define V_FW_OFLD_TX_DATA_WR_PROXY(x)	((x) << S_FW_OFLD_TX_DATA_WR_PROXY)
+#define G_FW_OFLD_TX_DATA_WR_PROXY(x)	\
+    (((x) >> S_FW_OFLD_TX_DATA_WR_PROXY) & M_FW_OFLD_TX_DATA_WR_PROXY)
+#define F_FW_OFLD_TX_DATA_WR_PROXY	V_FW_OFLD_TX_DATA_WR_PROXY(1U)
+
+struct fw_cmd_wr {
+	__be32 op_dma;
+	__be32 len16_pkd;
+	__be64 cookie_daddr;
+};
+
+#define S_FW_CMD_WR_DMA		17
+#define M_FW_CMD_WR_DMA		0x1
+#define V_FW_CMD_WR_DMA(x)	((x) << S_FW_CMD_WR_DMA)
+#define G_FW_CMD_WR_DMA(x)	(((x) >> S_FW_CMD_WR_DMA) & M_FW_CMD_WR_DMA)
+#define F_FW_CMD_WR_DMA	V_FW_CMD_WR_DMA(1U)
+
+struct fw_eth_tx_pkt_vm_wr {
+	__be32 op_immdlen;
+	__be32 equiq_to_len16;
+	__be32 r3[2];
+	__u8   ethmacdst[6];
+	__u8   ethmacsrc[6];
+	__be16 ethtype;
+	__be16 vlantci;
+};
+
+/******************************************************************************
+ *   R I   W O R K   R E Q U E S T s
+ **************************************/
+
+enum fw_ri_wr_opcode {
+	FW_RI_RDMA_WRITE		= 0x0,               /* IETF RDMAP v1.0 ... */
+	FW_RI_READ_REQ			= 0x1,
+	FW_RI_READ_RESP			= 0x2,
+	FW_RI_SEND			= 0x3,
+	FW_RI_SEND_WITH_INV		= 0x4,
+	FW_RI_SEND_WITH_SE		= 0x5,
+	FW_RI_SEND_WITH_SE_INV		= 0x6,
+	FW_RI_TERMINATE			= 0x7,
+	FW_RI_RDMA_INIT			= 0x8,                /* CHELSIO RI specific ... */
+	FW_RI_BIND_MW			= 0x9,
+	FW_RI_FAST_REGISTER		= 0xa,
+	FW_RI_LOCAL_INV			= 0xb,
+	FW_RI_QP_MODIFY			= 0xc,
+	FW_RI_BYPASS			= 0xd,
+	FW_RI_RECEIVE			= 0xe,
+
+	FW_RI_SGE_EC_CR_RETURN		= 0xf
+};
+
+enum fw_ri_wr_flags {
+	FW_RI_COMPLETION_FLAG		= 0x01,
+	FW_RI_NOTIFICATION_FLAG		= 0x02,
+	FW_RI_SOLICITED_EVENT_FLAG	= 0x04,
+	FW_RI_READ_FENCE_FLAG		= 0x08,
+	FW_RI_LOCAL_FENCE_FLAG		= 0x10,
+	FW_RI_RDMA_READ_INVALIDATE	= 0x20
+};
+
+enum fw_ri_mpa_attrs {
+	FW_RI_MPA_RX_MARKER_ENABLE	= 0x01,
+	FW_RI_MPA_TX_MARKER_ENABLE	= 0x02,
+	FW_RI_MPA_CRC_ENABLE		= 0x04,
+	FW_RI_MPA_IETF_ENABLE		= 0x08
+};
+
+enum fw_ri_qp_caps {
+	FW_RI_QP_RDMA_READ_ENABLE	= 0x01,
+	FW_RI_QP_RDMA_WRITE_ENABLE	= 0x02,
+	FW_RI_QP_BIND_ENABLE		= 0x04,
+	FW_RI_QP_FAST_REGISTER_ENABLE	= 0x08,
+	FW_RI_QP_STAG0_ENABLE		= 0x10,
+	FW_RI_QP_RDMA_READ_REQ_0B_ENABLE= 0x80,
+};
+
+enum fw_ri_addr_type {
+	FW_RI_ZERO_BASED_TO		= 0x00,
+	FW_RI_VA_BASED_TO		= 0x01
+};
+
+enum fw_ri_mem_perms {
+	FW_RI_MEM_ACCESS_REM_WRITE	= 0x01,
+	FW_RI_MEM_ACCESS_REM_READ	= 0x02,
+	FW_RI_MEM_ACCESS_REM		= 0x03,
+	FW_RI_MEM_ACCESS_LOCAL_WRITE	= 0x04,
+	FW_RI_MEM_ACCESS_LOCAL_READ	= 0x08,
+	FW_RI_MEM_ACCESS_LOCAL		= 0x0C
+};
+
+enum fw_ri_stag_type {
+	FW_RI_STAG_NSMR			= 0x00,
+	FW_RI_STAG_SMR			= 0x01,
+	FW_RI_STAG_MW			= 0x02,
+	FW_RI_STAG_MW_RELAXED		= 0x03
+};
+
+enum fw_ri_data_op {
+	FW_RI_DATA_IMMD			= 0x81,
+	FW_RI_DATA_DSGL			= 0x82,
+	FW_RI_DATA_ISGL			= 0x83
+};
+
+enum fw_ri_sgl_depth {
+	FW_RI_SGL_DEPTH_MAX_SQ		= 16,
+	FW_RI_SGL_DEPTH_MAX_RQ		= 4
+};
+
+enum fw_ri_cqe_err {
+	FW_RI_CQE_ERR_SUCCESS		= 0x00,	/* success, no error detected */
+	FW_RI_CQE_ERR_STAG		= 0x01, /* STAG invalid */
+	FW_RI_CQE_ERR_PDID		= 0x02, /* PDID mismatch */
+	FW_RI_CQE_ERR_QPID		= 0x03, /* QPID mismatch */
+	FW_RI_CQE_ERR_ACCESS		= 0x04, /* Invalid access right */
+	FW_RI_CQE_ERR_WRAP		= 0x05, /* Wrap error */
+	FW_RI_CQE_ERR_BOUND		= 0x06, /* base and bounds violation */
+	FW_RI_CQE_ERR_INVALIDATE_SHARED_MR = 0x07, /* attempt to invalidate a SMR */
+	FW_RI_CQE_ERR_INVALIDATE_MR_WITH_MW_BOUND = 0x08, /* attempt to invalidate a MR w MW */
+	FW_RI_CQE_ERR_ECC		= 0x09,	/* ECC error detected */
+	FW_RI_CQE_ERR_ECC_PSTAG		= 0x0A, /* ECC error detected when reading the PSTAG for a MW Invalidate */
+	FW_RI_CQE_ERR_PBL_ADDR_BOUND	= 0x0B, // pbl address out of bound : software error
+	FW_RI_CQE_ERR_CRC		= 0x10,	/* CRC error */
+	FW_RI_CQE_ERR_MARKER		= 0x11,	/* Marker error */
+	FW_RI_CQE_ERR_PDU_LEN_ERR	= 0x12,	/* invalid PDU length */
+	FW_RI_CQE_ERR_OUT_OF_RQE	= 0x13,	/* out of RQE */
+	FW_RI_CQE_ERR_DDP_VERSION	= 0x14,	/* wrong DDP version */
+	FW_RI_CQE_ERR_RDMA_VERSION	= 0x15,	/* wrong RDMA version */
+	FW_RI_CQE_ERR_OPCODE		= 0x16,	/* invalid rdma opcode */
+	FW_RI_CQE_ERR_DDP_QUEUE_NUM	= 0x17,	/* invalid ddp queue number */
+	FW_RI_CQE_ERR_MSN		= 0x18, /* MSN error */
+	FW_RI_CQE_ERR_TBIT		= 0x19, /* tag bit not set correctly */
+	FW_RI_CQE_ERR_MO		= 0x1A, /* MO not zero for TERMINATE or READ_REQ */
+	FW_RI_CQE_ERR_MSN_GAP		= 0x1B, /* */
+	FW_RI_CQE_ERR_MSN_RANGE		= 0x1C, /* */
+	FW_RI_CQE_ERR_IRD_OVERFLOW	= 0x1D, /* */
+	FW_RI_CQE_ERR_RQE_ADDR_BOUND	= 0x1E, /*  RQE address out of bound : software error */
+	FW_RI_CQE_ERR_INTERNAL_ERR	= 0x1F  /* internel error (opcode mismatch) */
+
+};
+
+struct fw_ri_dsge_pair {
+	__be32	len[2];
+	__be64	addr[2];
+};
+
+struct fw_ri_dsgl {
+	__u8	op;
+	__u8	r1;
+	__be16	nsge;
+	__be32	len0;
+	__be64	addr0;
+#ifndef C99_NOT_SUPPORTED
+	struct fw_ri_dsge_pair sge[0];
+#endif
+};
+
+struct fw_ri_sge {
+	__be32 stag;
+	__be32 len;
+	__be64 to;
+};
+
+struct fw_ri_isgl {
+	__u8	op;
+	__u8	r1;
+	__be16	nsge;
+	__be32	r2;
+#ifndef C99_NOT_SUPPORTED
+	struct fw_ri_sge sge[0];
+#endif
+};
+
+struct fw_ri_immd {
+	__u8	op;
+	__u8	r1;
+	__be16	r2;
+	__be32	immdlen;
+#ifndef C99_NOT_SUPPORTED
+	__u8	data[0];
+#endif
+};
+
+struct fw_ri_tpte {
+	__be32 valid_to_pdid;
+	__be32 locread_to_qpid;
+	__be32 nosnoop_pbladdr;
+	__be32 len_lo;
+	__be32 va_hi;
+	__be32 va_lo_fbo;
+	__be32 dca_mwbcnt_pstag;
+	__be32 len_hi;
+};
+
+#define S_FW_RI_TPTE_VALID		31
+#define M_FW_RI_TPTE_VALID		0x1
+#define V_FW_RI_TPTE_VALID(x)		((x) << S_FW_RI_TPTE_VALID)
+#define G_FW_RI_TPTE_VALID(x)		\
+    (((x) >> S_FW_RI_TPTE_VALID) & M_FW_RI_TPTE_VALID)
+#define F_FW_RI_TPTE_VALID		V_FW_RI_TPTE_VALID(1U)
+
+#define S_FW_RI_TPTE_STAGKEY		23
+#define M_FW_RI_TPTE_STAGKEY		0xff
+#define V_FW_RI_TPTE_STAGKEY(x)		((x) << S_FW_RI_TPTE_STAGKEY)
+#define G_FW_RI_TPTE_STAGKEY(x)		\
+    (((x) >> S_FW_RI_TPTE_STAGKEY) & M_FW_RI_TPTE_STAGKEY)
+
+#define S_FW_RI_TPTE_STAGSTATE		22
+#define M_FW_RI_TPTE_STAGSTATE		0x1
+#define V_FW_RI_TPTE_STAGSTATE(x)	((x) << S_FW_RI_TPTE_STAGSTATE)
+#define G_FW_RI_TPTE_STAGSTATE(x)	\
+    (((x) >> S_FW_RI_TPTE_STAGSTATE) & M_FW_RI_TPTE_STAGSTATE)
+#define F_FW_RI_TPTE_STAGSTATE		V_FW_RI_TPTE_STAGSTATE(1U)
+
+#define S_FW_RI_TPTE_STAGTYPE		20
+#define M_FW_RI_TPTE_STAGTYPE		0x3
+#define V_FW_RI_TPTE_STAGTYPE(x)	((x) << S_FW_RI_TPTE_STAGTYPE)
+#define G_FW_RI_TPTE_STAGTYPE(x)	\
+    (((x) >> S_FW_RI_TPTE_STAGTYPE) & M_FW_RI_TPTE_STAGTYPE)
+
+#define S_FW_RI_TPTE_PDID		0
+#define M_FW_RI_TPTE_PDID		0xfffff
+#define V_FW_RI_TPTE_PDID(x)		((x) << S_FW_RI_TPTE_PDID)
+#define G_FW_RI_TPTE_PDID(x)		\
+    (((x) >> S_FW_RI_TPTE_PDID) & M_FW_RI_TPTE_PDID)
+
+#define S_FW_RI_TPTE_PERM		28
+#define M_FW_RI_TPTE_PERM		0xf
+#define V_FW_RI_TPTE_PERM(x)		((x) << S_FW_RI_TPTE_PERM)
+#define G_FW_RI_TPTE_PERM(x)		\
+    (((x) >> S_FW_RI_TPTE_PERM) & M_FW_RI_TPTE_PERM)
+
+#define S_FW_RI_TPTE_REMINVDIS		27
+#define M_FW_RI_TPTE_REMINVDIS		0x1
+#define V_FW_RI_TPTE_REMINVDIS(x)	((x) << S_FW_RI_TPTE_REMINVDIS)
+#define G_FW_RI_TPTE_REMINVDIS(x)	\
+    (((x) >> S_FW_RI_TPTE_REMINVDIS) & M_FW_RI_TPTE_REMINVDIS)
+#define F_FW_RI_TPTE_REMINVDIS		V_FW_RI_TPTE_REMINVDIS(1U)
+
+#define S_FW_RI_TPTE_ADDRTYPE		26
+#define M_FW_RI_TPTE_ADDRTYPE		1
+#define V_FW_RI_TPTE_ADDRTYPE(x)	((x) << S_FW_RI_TPTE_ADDRTYPE)
+#define G_FW_RI_TPTE_ADDRTYPE(x)	\
+    (((x) >> S_FW_RI_TPTE_ADDRTYPE) & M_FW_RI_TPTE_ADDRTYPE)
+#define F_FW_RI_TPTE_ADDRTYPE		V_FW_RI_TPTE_ADDRTYPE(1U)
+
+#define S_FW_RI_TPTE_MWBINDEN		25
+#define M_FW_RI_TPTE_MWBINDEN		0x1
+#define V_FW_RI_TPTE_MWBINDEN(x)	((x) << S_FW_RI_TPTE_MWBINDEN)
+#define G_FW_RI_TPTE_MWBINDEN(x)	\
+    (((x) >> S_FW_RI_TPTE_MWBINDEN) & M_FW_RI_TPTE_MWBINDEN)
+#define F_FW_RI_TPTE_MWBINDEN		V_FW_RI_TPTE_MWBINDEN(1U)
+
+#define S_FW_RI_TPTE_PS			20
+#define M_FW_RI_TPTE_PS			0x1f
+#define V_FW_RI_TPTE_PS(x)		((x) << S_FW_RI_TPTE_PS)
+#define G_FW_RI_TPTE_PS(x)		\
+    (((x) >> S_FW_RI_TPTE_PS) & M_FW_RI_TPTE_PS)
+
+#define S_FW_RI_TPTE_QPID		0
+#define M_FW_RI_TPTE_QPID		0xfffff
+#define V_FW_RI_TPTE_QPID(x)		((x) << S_FW_RI_TPTE_QPID)
+#define G_FW_RI_TPTE_QPID(x)		\
+    (((x) >> S_FW_RI_TPTE_QPID) & M_FW_RI_TPTE_QPID)
+
+#define S_FW_RI_TPTE_NOSNOOP		31
+#define M_FW_RI_TPTE_NOSNOOP		0x1
+#define V_FW_RI_TPTE_NOSNOOP(x)		((x) << S_FW_RI_TPTE_NOSNOOP)
+#define G_FW_RI_TPTE_NOSNOOP(x)		\
+    (((x) >> S_FW_RI_TPTE_NOSNOOP) & M_FW_RI_TPTE_NOSNOOP)
+#define F_FW_RI_TPTE_NOSNOOP		V_FW_RI_TPTE_NOSNOOP(1U)
+
+#define S_FW_RI_TPTE_PBLADDR		0
+#define M_FW_RI_TPTE_PBLADDR		0x1fffffff
+#define V_FW_RI_TPTE_PBLADDR(x)		((x) << S_FW_RI_TPTE_PBLADDR)
+#define G_FW_RI_TPTE_PBLADDR(x)		\
+    (((x) >> S_FW_RI_TPTE_PBLADDR) & M_FW_RI_TPTE_PBLADDR)
+
+#define S_FW_RI_TPTE_DCA		24
+#define M_FW_RI_TPTE_DCA		0x1f
+#define V_FW_RI_TPTE_DCA(x)		((x) << S_FW_RI_TPTE_DCA)
+#define G_FW_RI_TPTE_DCA(x)		\
+    (((x) >> S_FW_RI_TPTE_DCA) & M_FW_RI_TPTE_DCA)
+
+#define S_FW_RI_TPTE_MWBCNT_PSTAG	0
+#define M_FW_RI_TPTE_MWBCNT_PSTAG	0xffffff
+#define V_FW_RI_TPTE_MWBCNT_PSTAT(x)	\
+    ((x) << S_FW_RI_TPTE_MWBCNT_PSTAG)
+#define G_FW_RI_TPTE_MWBCNT_PSTAG(x)	\
+    (((x) >> S_FW_RI_TPTE_MWBCNT_PSTAG) & M_FW_RI_TPTE_MWBCNT_PSTAG)
+
+enum fw_ri_cqe_rxtx {
+	FW_RI_CQE_RXTX_RX = 0x0,
+	FW_RI_CQE_RXTX_TX = 0x1,
+};
+
+struct fw_ri_cqe {
+	union fw_ri_rxtx {
+		struct fw_ri_scqe {
+		__be32	qpid_n_stat_rxtx_type;
+		__be32	plen;
+		__be32	reserved;
+		__be32	wrid;
+		} scqe;
+		struct fw_ri_rcqe {
+		__be32	qpid_n_stat_rxtx_type;
+		__be32	plen;
+		__be32	stag;
+		__be32	msn;
+		} rcqe;
+	} u;
+};
+
+#define S_FW_RI_CQE_QPID      12
+#define M_FW_RI_CQE_QPID      0xfffff
+#define V_FW_RI_CQE_QPID(x)   ((x) << S_FW_RI_CQE_QPID)
+#define G_FW_RI_CQE_QPID(x)   \
+    (((x) >> S_FW_RI_CQE_QPID) &  M_FW_RI_CQE_QPID)
+
+#define S_FW_RI_CQE_NOTIFY    10
+#define M_FW_RI_CQE_NOTIFY    0x1
+#define V_FW_RI_CQE_NOTIFY(x) ((x) << S_FW_RI_CQE_NOTIFY)
+#define G_FW_RI_CQE_NOTIFY(x) \
+    (((x) >> S_FW_RI_CQE_NOTIFY) &  M_FW_RI_CQE_NOTIFY)
+
+#define S_FW_RI_CQE_STATUS    5
+#define M_FW_RI_CQE_STATUS    0x1f
+#define V_FW_RI_CQE_STATUS(x) ((x) << S_FW_RI_CQE_STATUS)
+#define G_FW_RI_CQE_STATUS(x) \
+    (((x) >> S_FW_RI_CQE_STATUS) &  M_FW_RI_CQE_STATUS)
+
+
+#define S_FW_RI_CQE_RXTX      4
+#define M_FW_RI_CQE_RXTX      0x1
+#define V_FW_RI_CQE_RXTX(x)   ((x) << S_FW_RI_CQE_RXTX)
+#define G_FW_RI_CQE_RXTX(x)   \
+    (((x) >> S_FW_RI_CQE_RXTX) &  M_FW_RI_CQE_RXTX)
+
+#define S_FW_RI_CQE_TYPE      0
+#define M_FW_RI_CQE_TYPE      0xf
+#define V_FW_RI_CQE_TYPE(x)   ((x) << S_FW_RI_CQE_TYPE)
+#define G_FW_RI_CQE_TYPE(x)   \
+    (((x) >> S_FW_RI_CQE_TYPE) &  M_FW_RI_CQE_TYPE)
+
+enum fw_ri_res_type {
+	FW_RI_RES_TYPE_SQ,
+	FW_RI_RES_TYPE_RQ,
+	FW_RI_RES_TYPE_CQ,
+};
+
+enum fw_ri_res_op {
+	FW_RI_RES_OP_WRITE,
+	FW_RI_RES_OP_RESET,
+};
+
+struct fw_ri_res {
+	union fw_ri_restype {
+		struct fw_ri_res_sqrq {
+			__u8   restype;
+			__u8   op;
+			__be16 r3;
+			__be32 eqid;
+			__be32 r4[2];
+			__be32 fetchszm_to_iqid;
+			__be32 dcaen_to_eqsize;
+			__be64 eqaddr;
+		} sqrq;
+		struct fw_ri_res_cq {
+			__u8   restype;
+			__u8   op;
+			__be16 r3;
+			__be32 iqid;
+			__be32 r4[2];
+			__be32 iqandst_to_iqandstindex;
+			__be16 iqdroprss_to_iqesize;
+			__be16 iqsize;
+			__be64 iqaddr;
+			__be32 iqns_iqro;
+			__be32 r6_lo;
+			__be64 r7;
+		} cq;
+	} u;
+};
+
+struct fw_ri_res_wr {
+	__be32 op_nres;
+	__be32 len16_pkd;
+	__u64  cookie;
+#ifndef C99_NOT_SUPPORTED
+	struct fw_ri_res res[0];
+#endif
+};
+
+#define S_FW_RI_RES_WR_NRES	0
+#define M_FW_RI_RES_WR_NRES	0xff
+#define V_FW_RI_RES_WR_NRES(x)	((x) << S_FW_RI_RES_WR_NRES)
+#define G_FW_RI_RES_WR_NRES(x)	\
+    (((x) >> S_FW_RI_RES_WR_NRES) & M_FW_RI_RES_WR_NRES)
+
+#define S_FW_RI_RES_WR_FETCHSZM		26
+#define M_FW_RI_RES_WR_FETCHSZM		0x1
+#define V_FW_RI_RES_WR_FETCHSZM(x)	((x) << S_FW_RI_RES_WR_FETCHSZM)
+#define G_FW_RI_RES_WR_FETCHSZM(x)	\
+    (((x) >> S_FW_RI_RES_WR_FETCHSZM) & M_FW_RI_RES_WR_FETCHSZM)
+#define F_FW_RI_RES_WR_FETCHSZM	V_FW_RI_RES_WR_FETCHSZM(1U)
+
+#define S_FW_RI_RES_WR_STATUSPGNS	25
+#define M_FW_RI_RES_WR_STATUSPGNS	0x1
+#define V_FW_RI_RES_WR_STATUSPGNS(x)	((x) << S_FW_RI_RES_WR_STATUSPGNS)
+#define G_FW_RI_RES_WR_STATUSPGNS(x)	\
+    (((x) >> S_FW_RI_RES_WR_STATUSPGNS) & M_FW_RI_RES_WR_STATUSPGNS)
+#define F_FW_RI_RES_WR_STATUSPGNS	V_FW_RI_RES_WR_STATUSPGNS(1U)
+
+#define S_FW_RI_RES_WR_STATUSPGRO	24
+#define M_FW_RI_RES_WR_STATUSPGRO	0x1
+#define V_FW_RI_RES_WR_STATUSPGRO(x)	((x) << S_FW_RI_RES_WR_STATUSPGRO)
+#define G_FW_RI_RES_WR_STATUSPGRO(x)	\
+    (((x) >> S_FW_RI_RES_WR_STATUSPGRO) & M_FW_RI_RES_WR_STATUSPGRO)
+#define F_FW_RI_RES_WR_STATUSPGRO	V_FW_RI_RES_WR_STATUSPGRO(1U)
+
+#define S_FW_RI_RES_WR_FETCHNS		23
+#define M_FW_RI_RES_WR_FETCHNS		0x1
+#define V_FW_RI_RES_WR_FETCHNS(x)	((x) << S_FW_RI_RES_WR_FETCHNS)
+#define G_FW_RI_RES_WR_FETCHNS(x)	\
+    (((x) >> S_FW_RI_RES_WR_FETCHNS) & M_FW_RI_RES_WR_FETCHNS)
+#define F_FW_RI_RES_WR_FETCHNS	V_FW_RI_RES_WR_FETCHNS(1U)
+
+#define S_FW_RI_RES_WR_FETCHRO		22
+#define M_FW_RI_RES_WR_FETCHRO		0x1
+#define V_FW_RI_RES_WR_FETCHRO(x)	((x) << S_FW_RI_RES_WR_FETCHRO)
+#define G_FW_RI_RES_WR_FETCHRO(x)	\
+    (((x) >> S_FW_RI_RES_WR_FETCHRO) & M_FW_RI_RES_WR_FETCHRO)
+#define F_FW_RI_RES_WR_FETCHRO	V_FW_RI_RES_WR_FETCHRO(1U)
+
+#define S_FW_RI_RES_WR_HOSTFCMODE	20
+#define M_FW_RI_RES_WR_HOSTFCMODE	0x3
+#define V_FW_RI_RES_WR_HOSTFCMODE(x)	((x) << S_FW_RI_RES_WR_HOSTFCMODE)
+#define G_FW_RI_RES_WR_HOSTFCMODE(x)	\
+    (((x) >> S_FW_RI_RES_WR_HOSTFCMODE) & M_FW_RI_RES_WR_HOSTFCMODE)
+
+#define S_FW_RI_RES_WR_CPRIO	19
+#define M_FW_RI_RES_WR_CPRIO	0x1
+#define V_FW_RI_RES_WR_CPRIO(x)	((x) << S_FW_RI_RES_WR_CPRIO)
+#define G_FW_RI_RES_WR_CPRIO(x)	\
+    (((x) >> S_FW_RI_RES_WR_CPRIO) & M_FW_RI_RES_WR_CPRIO)
+#define F_FW_RI_RES_WR_CPRIO	V_FW_RI_RES_WR_CPRIO(1U)
+
+#define S_FW_RI_RES_WR_ONCHIP		18
+#define M_FW_RI_RES_WR_ONCHIP		0x1
+#define V_FW_RI_RES_WR_ONCHIP(x)	((x) << S_FW_RI_RES_WR_ONCHIP)
+#define G_FW_RI_RES_WR_ONCHIP(x)	\
+    (((x) >> S_FW_RI_RES_WR_ONCHIP) & M_FW_RI_RES_WR_ONCHIP)
+#define F_FW_RI_RES_WR_ONCHIP	V_FW_RI_RES_WR_ONCHIP(1U)
+
+#define S_FW_RI_RES_WR_PCIECHN		16
+#define M_FW_RI_RES_WR_PCIECHN		0x3
+#define V_FW_RI_RES_WR_PCIECHN(x)	((x) << S_FW_RI_RES_WR_PCIECHN)
+#define G_FW_RI_RES_WR_PCIECHN(x)	\
+    (((x) >> S_FW_RI_RES_WR_PCIECHN) & M_FW_RI_RES_WR_PCIECHN)
+
+#define S_FW_RI_RES_WR_IQID	0
+#define M_FW_RI_RES_WR_IQID	0xffff
+#define V_FW_RI_RES_WR_IQID(x)	((x) << S_FW_RI_RES_WR_IQID)
+#define G_FW_RI_RES_WR_IQID(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQID) & M_FW_RI_RES_WR_IQID)
+
+#define S_FW_RI_RES_WR_DCAEN	31
+#define M_FW_RI_RES_WR_DCAEN	0x1
+#define V_FW_RI_RES_WR_DCAEN(x)	((x) << S_FW_RI_RES_WR_DCAEN)
+#define G_FW_RI_RES_WR_DCAEN(x)	\
+    (((x) >> S_FW_RI_RES_WR_DCAEN) & M_FW_RI_RES_WR_DCAEN)
+#define F_FW_RI_RES_WR_DCAEN	V_FW_RI_RES_WR_DCAEN(1U)
+
+#define S_FW_RI_RES_WR_DCACPU		26
+#define M_FW_RI_RES_WR_DCACPU		0x1f
+#define V_FW_RI_RES_WR_DCACPU(x)	((x) << S_FW_RI_RES_WR_DCACPU)
+#define G_FW_RI_RES_WR_DCACPU(x)	\
+    (((x) >> S_FW_RI_RES_WR_DCACPU) & M_FW_RI_RES_WR_DCACPU)
+
+#define S_FW_RI_RES_WR_FBMIN	23
+#define M_FW_RI_RES_WR_FBMIN	0x7
+#define V_FW_RI_RES_WR_FBMIN(x)	((x) << S_FW_RI_RES_WR_FBMIN)
+#define G_FW_RI_RES_WR_FBMIN(x)	\
+    (((x) >> S_FW_RI_RES_WR_FBMIN) & M_FW_RI_RES_WR_FBMIN)
+
+#define S_FW_RI_RES_WR_FBMAX	20
+#define M_FW_RI_RES_WR_FBMAX	0x7
+#define V_FW_RI_RES_WR_FBMAX(x)	((x) << S_FW_RI_RES_WR_FBMAX)
+#define G_FW_RI_RES_WR_FBMAX(x)	\
+    (((x) >> S_FW_RI_RES_WR_FBMAX) & M_FW_RI_RES_WR_FBMAX)
+
+#define S_FW_RI_RES_WR_CIDXFTHRESHO	19
+#define M_FW_RI_RES_WR_CIDXFTHRESHO	0x1
+#define V_FW_RI_RES_WR_CIDXFTHRESHO(x)	((x) << S_FW_RI_RES_WR_CIDXFTHRESHO)
+#define G_FW_RI_RES_WR_CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_RI_RES_WR_CIDXFTHRESHO) & M_FW_RI_RES_WR_CIDXFTHRESHO)
+#define F_FW_RI_RES_WR_CIDXFTHRESHO	V_FW_RI_RES_WR_CIDXFTHRESHO(1U)
+
+#define S_FW_RI_RES_WR_CIDXFTHRESH	16
+#define M_FW_RI_RES_WR_CIDXFTHRESH	0x7
+#define V_FW_RI_RES_WR_CIDXFTHRESH(x)	((x) << S_FW_RI_RES_WR_CIDXFTHRESH)
+#define G_FW_RI_RES_WR_CIDXFTHRESH(x)	\
+    (((x) >> S_FW_RI_RES_WR_CIDXFTHRESH) & M_FW_RI_RES_WR_CIDXFTHRESH)
+
+#define S_FW_RI_RES_WR_EQSIZE		0
+#define M_FW_RI_RES_WR_EQSIZE		0xffff
+#define V_FW_RI_RES_WR_EQSIZE(x)	((x) << S_FW_RI_RES_WR_EQSIZE)
+#define G_FW_RI_RES_WR_EQSIZE(x)	\
+    (((x) >> S_FW_RI_RES_WR_EQSIZE) & M_FW_RI_RES_WR_EQSIZE)
+
+#define S_FW_RI_RES_WR_IQANDST		15
+#define M_FW_RI_RES_WR_IQANDST		0x1
+#define V_FW_RI_RES_WR_IQANDST(x)	((x) << S_FW_RI_RES_WR_IQANDST)
+#define G_FW_RI_RES_WR_IQANDST(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQANDST) & M_FW_RI_RES_WR_IQANDST)
+#define F_FW_RI_RES_WR_IQANDST	V_FW_RI_RES_WR_IQANDST(1U)
+
+#define S_FW_RI_RES_WR_IQANUS		14
+#define M_FW_RI_RES_WR_IQANUS		0x1
+#define V_FW_RI_RES_WR_IQANUS(x)	((x) << S_FW_RI_RES_WR_IQANUS)
+#define G_FW_RI_RES_WR_IQANUS(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQANUS) & M_FW_RI_RES_WR_IQANUS)
+#define F_FW_RI_RES_WR_IQANUS	V_FW_RI_RES_WR_IQANUS(1U)
+
+#define S_FW_RI_RES_WR_IQANUD		12
+#define M_FW_RI_RES_WR_IQANUD		0x3
+#define V_FW_RI_RES_WR_IQANUD(x)	((x) << S_FW_RI_RES_WR_IQANUD)
+#define G_FW_RI_RES_WR_IQANUD(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQANUD) & M_FW_RI_RES_WR_IQANUD)
+
+#define S_FW_RI_RES_WR_IQANDSTINDEX	0
+#define M_FW_RI_RES_WR_IQANDSTINDEX	0xfff
+#define V_FW_RI_RES_WR_IQANDSTINDEX(x)	((x) << S_FW_RI_RES_WR_IQANDSTINDEX)
+#define G_FW_RI_RES_WR_IQANDSTINDEX(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQANDSTINDEX) & M_FW_RI_RES_WR_IQANDSTINDEX)
+
+#define S_FW_RI_RES_WR_IQDROPRSS	15
+#define M_FW_RI_RES_WR_IQDROPRSS	0x1
+#define V_FW_RI_RES_WR_IQDROPRSS(x)	((x) << S_FW_RI_RES_WR_IQDROPRSS)
+#define G_FW_RI_RES_WR_IQDROPRSS(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQDROPRSS) & M_FW_RI_RES_WR_IQDROPRSS)
+#define F_FW_RI_RES_WR_IQDROPRSS	V_FW_RI_RES_WR_IQDROPRSS(1U)
+
+#define S_FW_RI_RES_WR_IQGTSMODE	14
+#define M_FW_RI_RES_WR_IQGTSMODE	0x1
+#define V_FW_RI_RES_WR_IQGTSMODE(x)	((x) << S_FW_RI_RES_WR_IQGTSMODE)
+#define G_FW_RI_RES_WR_IQGTSMODE(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQGTSMODE) & M_FW_RI_RES_WR_IQGTSMODE)
+#define F_FW_RI_RES_WR_IQGTSMODE	V_FW_RI_RES_WR_IQGTSMODE(1U)
+
+#define S_FW_RI_RES_WR_IQPCIECH		12
+#define M_FW_RI_RES_WR_IQPCIECH		0x3
+#define V_FW_RI_RES_WR_IQPCIECH(x)	((x) << S_FW_RI_RES_WR_IQPCIECH)
+#define G_FW_RI_RES_WR_IQPCIECH(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQPCIECH) & M_FW_RI_RES_WR_IQPCIECH)
+
+#define S_FW_RI_RES_WR_IQDCAEN		11
+#define M_FW_RI_RES_WR_IQDCAEN		0x1
+#define V_FW_RI_RES_WR_IQDCAEN(x)	((x) << S_FW_RI_RES_WR_IQDCAEN)
+#define G_FW_RI_RES_WR_IQDCAEN(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQDCAEN) & M_FW_RI_RES_WR_IQDCAEN)
+#define F_FW_RI_RES_WR_IQDCAEN	V_FW_RI_RES_WR_IQDCAEN(1U)
+
+#define S_FW_RI_RES_WR_IQDCACPU		6
+#define M_FW_RI_RES_WR_IQDCACPU		0x1f
+#define V_FW_RI_RES_WR_IQDCACPU(x)	((x) << S_FW_RI_RES_WR_IQDCACPU)
+#define G_FW_RI_RES_WR_IQDCACPU(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQDCACPU) & M_FW_RI_RES_WR_IQDCACPU)
+
+#define S_FW_RI_RES_WR_IQINTCNTTHRESH		4
+#define M_FW_RI_RES_WR_IQINTCNTTHRESH		0x3
+#define V_FW_RI_RES_WR_IQINTCNTTHRESH(x)	\
+    ((x) << S_FW_RI_RES_WR_IQINTCNTTHRESH)
+#define G_FW_RI_RES_WR_IQINTCNTTHRESH(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQINTCNTTHRESH) & M_FW_RI_RES_WR_IQINTCNTTHRESH)
+
+#define S_FW_RI_RES_WR_IQO	3
+#define M_FW_RI_RES_WR_IQO	0x1
+#define V_FW_RI_RES_WR_IQO(x)	((x) << S_FW_RI_RES_WR_IQO)
+#define G_FW_RI_RES_WR_IQO(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQO) & M_FW_RI_RES_WR_IQO)
+#define F_FW_RI_RES_WR_IQO	V_FW_RI_RES_WR_IQO(1U)
+
+#define S_FW_RI_RES_WR_IQCPRIO		2
+#define M_FW_RI_RES_WR_IQCPRIO		0x1
+#define V_FW_RI_RES_WR_IQCPRIO(x)	((x) << S_FW_RI_RES_WR_IQCPRIO)
+#define G_FW_RI_RES_WR_IQCPRIO(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQCPRIO) & M_FW_RI_RES_WR_IQCPRIO)
+#define F_FW_RI_RES_WR_IQCPRIO	V_FW_RI_RES_WR_IQCPRIO(1U)
+
+#define S_FW_RI_RES_WR_IQESIZE		0
+#define M_FW_RI_RES_WR_IQESIZE		0x3
+#define V_FW_RI_RES_WR_IQESIZE(x)	((x) << S_FW_RI_RES_WR_IQESIZE)
+#define G_FW_RI_RES_WR_IQESIZE(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQESIZE) & M_FW_RI_RES_WR_IQESIZE)
+
+#define S_FW_RI_RES_WR_IQNS	31
+#define M_FW_RI_RES_WR_IQNS	0x1
+#define V_FW_RI_RES_WR_IQNS(x)	((x) << S_FW_RI_RES_WR_IQNS)
+#define G_FW_RI_RES_WR_IQNS(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQNS) & M_FW_RI_RES_WR_IQNS)
+#define F_FW_RI_RES_WR_IQNS	V_FW_RI_RES_WR_IQNS(1U)
+
+#define S_FW_RI_RES_WR_IQRO	30
+#define M_FW_RI_RES_WR_IQRO	0x1
+#define V_FW_RI_RES_WR_IQRO(x)	((x) << S_FW_RI_RES_WR_IQRO)
+#define G_FW_RI_RES_WR_IQRO(x)	\
+    (((x) >> S_FW_RI_RES_WR_IQRO) & M_FW_RI_RES_WR_IQRO)
+#define F_FW_RI_RES_WR_IQRO	V_FW_RI_RES_WR_IQRO(1U)
+
+struct fw_ri_rdma_write_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__be64 r2;
+	__be32 plen;
+	__be32 stag_sink;
+	__be64 to_sink;
+#ifndef C99_NOT_SUPPORTED
+	union {
+		struct fw_ri_immd immd_src[0];
+		struct fw_ri_isgl isgl_src[0];
+	} u;
+#endif
+};
+
+struct fw_ri_send_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__be32 sendop_pkd;
+	__be32 stag_inv;
+	__be32 plen;
+	__be32 r3;
+	__be64 r4;
+#ifndef C99_NOT_SUPPORTED
+	union {
+		struct fw_ri_immd immd_src[0];
+		struct fw_ri_isgl isgl_src[0];
+	} u;
+#endif
+};
+
+#define S_FW_RI_SEND_WR_SENDOP		0
+#define M_FW_RI_SEND_WR_SENDOP		0xf
+#define V_FW_RI_SEND_WR_SENDOP(x)	((x) << S_FW_RI_SEND_WR_SENDOP)
+#define G_FW_RI_SEND_WR_SENDOP(x)	\
+    (((x) >> S_FW_RI_SEND_WR_SENDOP) & M_FW_RI_SEND_WR_SENDOP)
+
+struct fw_ri_rdma_read_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__be64 r2;
+	__be32 stag_sink;
+	__be32 to_sink_hi;
+	__be32 to_sink_lo;
+	__be32 plen;
+	__be32 stag_src;
+	__be32 to_src_hi;
+	__be32 to_src_lo;
+	__be32 r5;
+};
+
+struct fw_ri_recv_wr {
+	__u8   opcode;
+	__u8   r1;
+	__u16  wrid;
+	__u8   r2[3];
+	__u8   len16;
+	struct fw_ri_isgl isgl;
+};
+
+struct fw_ri_bind_mw_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__u8   qpbinde_to_dcacpu;
+	__u8   pgsz_shift;
+	__u8   addr_type;
+	__u8   mem_perms;
+	__be32 stag_mr;
+	__be32 stag_mw;
+	__be32 r3;
+	__be64 len_mw;
+	__be64 va_fbo;
+	__be64 r4;
+};
+
+#define S_FW_RI_BIND_MW_WR_QPBINDE	6
+#define M_FW_RI_BIND_MW_WR_QPBINDE	0x1
+#define V_FW_RI_BIND_MW_WR_QPBINDE(x)	((x) << S_FW_RI_BIND_MW_WR_QPBINDE)
+#define G_FW_RI_BIND_MW_WR_QPBINDE(x)	\
+    (((x) >> S_FW_RI_BIND_MW_WR_QPBINDE) & M_FW_RI_BIND_MW_WR_QPBINDE)
+#define F_FW_RI_BIND_MW_WR_QPBINDE	V_FW_RI_BIND_MW_WR_QPBINDE(1U)
+
+#define S_FW_RI_BIND_MW_WR_NS		5
+#define M_FW_RI_BIND_MW_WR_NS		0x1
+#define V_FW_RI_BIND_MW_WR_NS(x)	((x) << S_FW_RI_BIND_MW_WR_NS)
+#define G_FW_RI_BIND_MW_WR_NS(x)	\
+    (((x) >> S_FW_RI_BIND_MW_WR_NS) & M_FW_RI_BIND_MW_WR_NS)
+#define F_FW_RI_BIND_MW_WR_NS	V_FW_RI_BIND_MW_WR_NS(1U)
+
+#define S_FW_RI_BIND_MW_WR_DCACPU	0
+#define M_FW_RI_BIND_MW_WR_DCACPU	0x1f
+#define V_FW_RI_BIND_MW_WR_DCACPU(x)	((x) << S_FW_RI_BIND_MW_WR_DCACPU)
+#define G_FW_RI_BIND_MW_WR_DCACPU(x)	\
+    (((x) >> S_FW_RI_BIND_MW_WR_DCACPU) & M_FW_RI_BIND_MW_WR_DCACPU)
+
+struct fw_ri_fr_nsmr_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__u8   qpbinde_to_dcacpu;
+	__u8   pgsz_shift;
+	__u8   addr_type;
+	__u8   mem_perms;
+	__be32 stag;
+	__be32 len_hi;
+	__be32 len_lo;
+	__be32 va_hi;
+	__be32 va_lo_fbo;
+};
+
+#define S_FW_RI_FR_NSMR_WR_QPBINDE	6
+#define M_FW_RI_FR_NSMR_WR_QPBINDE	0x1
+#define V_FW_RI_FR_NSMR_WR_QPBINDE(x)	((x) << S_FW_RI_FR_NSMR_WR_QPBINDE)
+#define G_FW_RI_FR_NSMR_WR_QPBINDE(x)	\
+    (((x) >> S_FW_RI_FR_NSMR_WR_QPBINDE) & M_FW_RI_FR_NSMR_WR_QPBINDE)
+#define F_FW_RI_FR_NSMR_WR_QPBINDE	V_FW_RI_FR_NSMR_WR_QPBINDE(1U)
+
+#define S_FW_RI_FR_NSMR_WR_NS		5
+#define M_FW_RI_FR_NSMR_WR_NS		0x1
+#define V_FW_RI_FR_NSMR_WR_NS(x)	((x) << S_FW_RI_FR_NSMR_WR_NS)
+#define G_FW_RI_FR_NSMR_WR_NS(x)	\
+    (((x) >> S_FW_RI_FR_NSMR_WR_NS) & M_FW_RI_FR_NSMR_WR_NS)
+#define F_FW_RI_FR_NSMR_WR_NS	V_FW_RI_FR_NSMR_WR_NS(1U)
+
+#define S_FW_RI_FR_NSMR_WR_DCACPU	0
+#define M_FW_RI_FR_NSMR_WR_DCACPU	0x1f
+#define V_FW_RI_FR_NSMR_WR_DCACPU(x)	((x) << S_FW_RI_FR_NSMR_WR_DCACPU)
+#define G_FW_RI_FR_NSMR_WR_DCACPU(x)	\
+    (((x) >> S_FW_RI_FR_NSMR_WR_DCACPU) & M_FW_RI_FR_NSMR_WR_DCACPU)
+
+struct fw_ri_inv_lstag_wr {
+	__u8   opcode;
+	__u8   flags;
+	__u16  wrid;
+	__u8   r1[3];
+	__u8   len16;
+	__be32 r2;
+	__be32 stag_inv;
+};
+
+enum fw_ri_type {
+	FW_RI_TYPE_INIT,
+	FW_RI_TYPE_FINI,
+	FW_RI_TYPE_TERMINATE
+};
+
+enum fw_ri_init_p2ptype {
+	FW_RI_INIT_P2PTYPE_RDMA_WRITE		= FW_RI_RDMA_WRITE,
+	FW_RI_INIT_P2PTYPE_READ_REQ		= FW_RI_READ_REQ,
+	FW_RI_INIT_P2PTYPE_SEND			= FW_RI_SEND,
+	FW_RI_INIT_P2PTYPE_SEND_WITH_INV	= FW_RI_SEND_WITH_INV,
+	FW_RI_INIT_P2PTYPE_SEND_WITH_SE		= FW_RI_SEND_WITH_SE,
+	FW_RI_INIT_P2PTYPE_SEND_WITH_SE_INV	= FW_RI_SEND_WITH_SE_INV,
+	FW_RI_INIT_P2PTYPE_DISABLED		= 0xf,
+};
+
+struct fw_ri_wr {
+	__be32 op_compl;
+	__be32 flowid_len16;
+	__u64  cookie;
+	union fw_ri {
+		struct fw_ri_init {
+			__u8   type;
+			__u8   mpareqbit_p2ptype;
+			__u8   r4[2];
+			__u8   mpa_attrs;
+			__u8   qp_caps;
+			__be16 nrqe;
+			__be32 pdid;
+			__be32 qpid;
+			__be32 sq_eqid;
+			__be32 rq_eqid;
+			__be32 scqid;
+			__be32 rcqid;
+			__be32 ord_max;
+			__be32 ird_max;
+			__be32 iss;
+			__be32 irs;
+			__be32 hwrqsize;
+			__be32 hwrqaddr;
+			__be64 r5;
+			union fw_ri_init_p2p {
+				struct fw_ri_rdma_write_wr write;
+				struct fw_ri_rdma_read_wr read;
+				struct fw_ri_send_wr send;
+			} u;
+		} init;
+		struct fw_ri_fini {
+			__u8   type;
+			__u8   r3[7];
+			__be64 r4;
+		} fini;
+		struct fw_ri_terminate {
+			__u8   type;
+			__u8   r3[3];
+			__be32 immdlen;
+			__u8   termmsg[40];
+		} terminate;
+	} u;
+};
+
+#define S_FW_RI_WR_MPAREQBIT	7
+#define M_FW_RI_WR_MPAREQBIT	0x1
+#define V_FW_RI_WR_MPAREQBIT(x)	((x) << S_FW_RI_WR_MPAREQBIT)
+#define G_FW_RI_WR_MPAREQBIT(x)	\
+    (((x) >> S_FW_RI_WR_MPAREQBIT) & M_FW_RI_WR_MPAREQBIT)
+#define F_FW_RI_WR_MPAREQBIT	V_FW_RI_WR_MPAREQBIT(1U)
+
+#define S_FW_RI_WR_0BRRBIT	6
+#define M_FW_RI_WR_0BRRBIT	0x1
+#define V_FW_RI_WR_0BRRBIT(x)	((x) << S_FW_RI_WR_0BRRBIT)
+#define G_FW_RI_WR_0BRRBIT(x)	\
+    (((x) >> S_FW_RI_WR_0BRRBIT) & M_FW_RI_WR_0BRRBIT)
+#define F_FW_RI_WR_0BRRBIT	V_FW_RI_WR_0BRRBIT(1U)
+
+#define S_FW_RI_WR_P2PTYPE	0
+#define M_FW_RI_WR_P2PTYPE	0xf
+#define V_FW_RI_WR_P2PTYPE(x)	((x) << S_FW_RI_WR_P2PTYPE)
+#define G_FW_RI_WR_P2PTYPE(x)	\
+    (((x) >> S_FW_RI_WR_P2PTYPE) & M_FW_RI_WR_P2PTYPE)
+
+/******************************************************************************
+ *  F O i S C S I   W O R K R E Q U E S T s
+ *********************************************/
+
+#define	FW_FOISCSI_NAME_MAX_LEN		224
+#define	FW_FOISCSI_ALIAS_MAX_LEN	224
+#define FW_FOISCSI_MAX_CHAP_NAME_LEN	64
+#define	FW_FOISCSI_INIT_NODE_MAX	8
+
+enum fw_chnet_ifconf_wr_subop {
+	FW_CHNET_IFCONF_WR_SUBOP_NONE = 0,
+	
+	FW_CHNET_IFCONF_WR_SUBOP_IPV4_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_IPV4_GET,
+	
+	FW_CHNET_IFCONF_WR_SUBOP_VLAN_IPV4_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_VLAN_IPV4_GET,
+
+	FW_CHNET_IFCONF_WR_SUBOP_IPV6_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_IPV6_GET,
+
+	FW_CHNET_IFCONF_WR_SUBOP_VLAN_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_VLAN_GET,
+
+	FW_CHNET_IFCONF_WR_SUBOP_MTU_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_MTU_GET,
+
+	FW_CHNET_IFCONF_WR_SUBOP_DHCP_SET,
+	FW_CHNET_IFCONF_WR_SUBOP_DHCP_GET,
+
+	FW_CHNET_IFCONF_WR_SUBOP_MAX,
+};
+
+struct fw_chnet_ifconf_wr {
+	__be32 op_compl;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be32 if_flowid;
+	__u8   idx;
+	__u8   subop;
+	__u8   retval;
+	__u8   r2;
+	__be64 r3;
+	struct fw_chnet_ifconf_params {
+		__be32 r0;
+		__be16 vlanid;
+		__be16 mtu;
+		union fw_chnet_ifconf_addr_type {
+			struct fw_chnet_ifconf_ipv4 {
+				__be32 addr;
+				__be32 mask;
+				__be32 router;
+				__be32 r0;
+				__be64 r1;
+			} ipv4;
+			struct fw_chnet_ifconf_ipv6 {
+				__be64 linklocal_lo;
+				__be64 linklocal_hi;
+				__be64 router_hi;
+				__be64 router_lo;
+				__be64 aconf_hi;
+				__be64 aconf_lo;
+				__be64 linklocal_aconf_hi;
+				__be64 linklocal_aconf_lo;
+				__be64 router_aconf_hi;
+				__be64 router_aconf_lo;
+				__be64 r0;
+			} ipv6;
+		} in_attr;
+	} param;
+};
+
+enum fw_foiscsi_session_type {
+	FW_FOISCSI_SESSION_TYPE_DISCOVERY = 0,
+	FW_FOISCSI_SESSION_TYPE_NORMAL,
+};
+
+enum fw_foiscsi_auth_policy {
+	FW_FOISCSI_AUTH_POLICY_ONEWAY = 0,
+	FW_FOISCSI_AUTH_POLICY_MUTUAL,
+};
+
+enum fw_foiscsi_auth_method {
+	FW_FOISCSI_AUTH_METHOD_NONE = 0,
+	FW_FOISCSI_AUTH_METHOD_CHAP,
+	FW_FOISCSI_AUTH_METHOD_CHAP_FST,
+	FW_FOISCSI_AUTH_METHOD_CHAP_SEC,
+};
+
+enum fw_foiscsi_digest_type {
+	FW_FOISCSI_DIGEST_TYPE_NONE = 0,
+	FW_FOISCSI_DIGEST_TYPE_CRC32,
+	FW_FOISCSI_DIGEST_TYPE_CRC32_FST,
+	FW_FOISCSI_DIGEST_TYPE_CRC32_SEC,
+};
+
+enum fw_foiscsi_wr_subop {
+	FW_FOISCSI_WR_SUBOP_ADD = 1,
+	FW_FOISCSI_WR_SUBOP_DEL = 2,
+	FW_FOISCSI_WR_SUBOP_MOD = 4,
+};
+
+enum fw_foiscsi_ctrl_state {
+	FW_FOISCSI_CTRL_STATE_FREE = 0,
+	FW_FOISCSI_CTRL_STATE_ONLINE = 1,
+	FW_FOISCSI_CTRL_STATE_FAILED,
+	FW_FOISCSI_CTRL_STATE_IN_RECOVERY,
+	FW_FOISCSI_CTRL_STATE_REDIRECT,
+};
+
+struct fw_rdev_wr {
+	__be32 op_to_immdlen;
+	__be32 alloc_to_len16;
+	__be64 cookie;
+	__u8   protocol;
+	__u8   event_cause;
+	__u8   cur_state;
+	__u8   prev_state;
+	__be32 flags_to_assoc_flowid;
+	union rdev_entry {
+		struct fcoe_rdev_entry {
+			__be32 flowid;
+			__u8   protocol;
+			__u8   event_cause;
+			__u8   flags;
+			__u8   rjt_reason;
+			__u8   cur_login_st;
+			__u8   prev_login_st;
+			__be16 rcv_fr_sz;
+			__u8   rd_xfer_rdy_to_rport_type;
+			__u8   vft_to_qos;
+			__u8   org_proc_assoc_to_acc_rsp_code;
+			__u8   enh_disc_to_tgt;
+			__u8   wwnn[8];
+			__u8   wwpn[8];
+			__be16 iqid;
+			__u8   fc_oui[3];
+			__u8   r_id[3];
+		} fcoe_rdev;
+		struct iscsi_rdev_entry {
+			__be32 flowid;
+			__u8   protocol;
+			__u8   event_cause;
+			__u8   flags;
+			__u8   r3;
+			__be16 iscsi_opts;
+			__be16 tcp_opts;
+			__be16 ip_opts;
+			__be16 max_rcv_len;
+			__be16 max_snd_len;
+			__be16 first_brst_len;
+			__be16 max_brst_len;
+			__be16 r4;
+			__be16 def_time2wait;
+			__be16 def_time2ret;
+			__be16 nop_out_intrvl;
+			__be16 non_scsi_to;
+			__be16 isid;
+			__be16 tsid;
+			__be16 port;
+			__be16 tpgt;
+			__u8   r5[6];
+			__be16 iqid;
+		} iscsi_rdev;
+	} u;
+};
+
+#define S_FW_RDEV_WR_IMMDLEN	0
+#define M_FW_RDEV_WR_IMMDLEN	0xff
+#define V_FW_RDEV_WR_IMMDLEN(x)	((x) << S_FW_RDEV_WR_IMMDLEN)
+#define G_FW_RDEV_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_RDEV_WR_IMMDLEN) & M_FW_RDEV_WR_IMMDLEN)
+
+#define S_FW_RDEV_WR_ALLOC	31
+#define M_FW_RDEV_WR_ALLOC	0x1
+#define V_FW_RDEV_WR_ALLOC(x)	((x) << S_FW_RDEV_WR_ALLOC)
+#define G_FW_RDEV_WR_ALLOC(x)	\
+    (((x) >> S_FW_RDEV_WR_ALLOC) & M_FW_RDEV_WR_ALLOC)
+#define F_FW_RDEV_WR_ALLOC	V_FW_RDEV_WR_ALLOC(1U)
+
+#define S_FW_RDEV_WR_FREE	30
+#define M_FW_RDEV_WR_FREE	0x1
+#define V_FW_RDEV_WR_FREE(x)	((x) << S_FW_RDEV_WR_FREE)
+#define G_FW_RDEV_WR_FREE(x)	\
+    (((x) >> S_FW_RDEV_WR_FREE) & M_FW_RDEV_WR_FREE)
+#define F_FW_RDEV_WR_FREE	V_FW_RDEV_WR_FREE(1U)
+
+#define S_FW_RDEV_WR_MODIFY	29
+#define M_FW_RDEV_WR_MODIFY	0x1
+#define V_FW_RDEV_WR_MODIFY(x)	((x) << S_FW_RDEV_WR_MODIFY)
+#define G_FW_RDEV_WR_MODIFY(x)	\
+    (((x) >> S_FW_RDEV_WR_MODIFY) & M_FW_RDEV_WR_MODIFY)
+#define F_FW_RDEV_WR_MODIFY	V_FW_RDEV_WR_MODIFY(1U)
+
+#define S_FW_RDEV_WR_FLOWID	8
+#define M_FW_RDEV_WR_FLOWID	0xfffff
+#define V_FW_RDEV_WR_FLOWID(x)	((x) << S_FW_RDEV_WR_FLOWID)
+#define G_FW_RDEV_WR_FLOWID(x)	\
+    (((x) >> S_FW_RDEV_WR_FLOWID) & M_FW_RDEV_WR_FLOWID)
+
+#define S_FW_RDEV_WR_LEN16	0
+#define M_FW_RDEV_WR_LEN16	0xff
+#define V_FW_RDEV_WR_LEN16(x)	((x) << S_FW_RDEV_WR_LEN16)
+#define G_FW_RDEV_WR_LEN16(x)	\
+    (((x) >> S_FW_RDEV_WR_LEN16) & M_FW_RDEV_WR_LEN16)
+
+#define S_FW_RDEV_WR_FLAGS	24
+#define M_FW_RDEV_WR_FLAGS	0xff
+#define V_FW_RDEV_WR_FLAGS(x)	((x) << S_FW_RDEV_WR_FLAGS)
+#define G_FW_RDEV_WR_FLAGS(x)	\
+    (((x) >> S_FW_RDEV_WR_FLAGS) & M_FW_RDEV_WR_FLAGS)
+
+#define S_FW_RDEV_WR_GET_NEXT		20
+#define M_FW_RDEV_WR_GET_NEXT		0xf
+#define V_FW_RDEV_WR_GET_NEXT(x)	((x) << S_FW_RDEV_WR_GET_NEXT)
+#define G_FW_RDEV_WR_GET_NEXT(x)	\
+    (((x) >> S_FW_RDEV_WR_GET_NEXT) & M_FW_RDEV_WR_GET_NEXT)
+
+#define S_FW_RDEV_WR_ASSOC_FLOWID	0
+#define M_FW_RDEV_WR_ASSOC_FLOWID	0xfffff
+#define V_FW_RDEV_WR_ASSOC_FLOWID(x)	((x) << S_FW_RDEV_WR_ASSOC_FLOWID)
+#define G_FW_RDEV_WR_ASSOC_FLOWID(x)	\
+    (((x) >> S_FW_RDEV_WR_ASSOC_FLOWID) & M_FW_RDEV_WR_ASSOC_FLOWID)
+
+#define S_FW_RDEV_WR_RJT	7
+#define M_FW_RDEV_WR_RJT	0x1
+#define V_FW_RDEV_WR_RJT(x)	((x) << S_FW_RDEV_WR_RJT)
+#define G_FW_RDEV_WR_RJT(x)	(((x) >> S_FW_RDEV_WR_RJT) & M_FW_RDEV_WR_RJT)
+#define F_FW_RDEV_WR_RJT	V_FW_RDEV_WR_RJT(1U)
+
+#define S_FW_RDEV_WR_REASON	0
+#define M_FW_RDEV_WR_REASON	0x7f
+#define V_FW_RDEV_WR_REASON(x)	((x) << S_FW_RDEV_WR_REASON)
+#define G_FW_RDEV_WR_REASON(x)	\
+    (((x) >> S_FW_RDEV_WR_REASON) & M_FW_RDEV_WR_REASON)
+
+#define S_FW_RDEV_WR_RD_XFER_RDY	7
+#define M_FW_RDEV_WR_RD_XFER_RDY	0x1
+#define V_FW_RDEV_WR_RD_XFER_RDY(x)	((x) << S_FW_RDEV_WR_RD_XFER_RDY)
+#define G_FW_RDEV_WR_RD_XFER_RDY(x)	\
+    (((x) >> S_FW_RDEV_WR_RD_XFER_RDY) & M_FW_RDEV_WR_RD_XFER_RDY)
+#define F_FW_RDEV_WR_RD_XFER_RDY	V_FW_RDEV_WR_RD_XFER_RDY(1U)
+
+#define S_FW_RDEV_WR_WR_XFER_RDY	6
+#define M_FW_RDEV_WR_WR_XFER_RDY	0x1
+#define V_FW_RDEV_WR_WR_XFER_RDY(x)	((x) << S_FW_RDEV_WR_WR_XFER_RDY)
+#define G_FW_RDEV_WR_WR_XFER_RDY(x)	\
+    (((x) >> S_FW_RDEV_WR_WR_XFER_RDY) & M_FW_RDEV_WR_WR_XFER_RDY)
+#define F_FW_RDEV_WR_WR_XFER_RDY	V_FW_RDEV_WR_WR_XFER_RDY(1U)
+
+#define S_FW_RDEV_WR_FC_SP	5
+#define M_FW_RDEV_WR_FC_SP	0x1
+#define V_FW_RDEV_WR_FC_SP(x)	((x) << S_FW_RDEV_WR_FC_SP)
+#define G_FW_RDEV_WR_FC_SP(x)	\
+    (((x) >> S_FW_RDEV_WR_FC_SP) & M_FW_RDEV_WR_FC_SP)
+#define F_FW_RDEV_WR_FC_SP	V_FW_RDEV_WR_FC_SP(1U)
+
+#define S_FW_RDEV_WR_RPORT_TYPE		0
+#define M_FW_RDEV_WR_RPORT_TYPE		0x1f
+#define V_FW_RDEV_WR_RPORT_TYPE(x)	((x) << S_FW_RDEV_WR_RPORT_TYPE)
+#define G_FW_RDEV_WR_RPORT_TYPE(x)	\
+    (((x) >> S_FW_RDEV_WR_RPORT_TYPE) & M_FW_RDEV_WR_RPORT_TYPE)
+
+#define S_FW_RDEV_WR_VFT	7
+#define M_FW_RDEV_WR_VFT	0x1
+#define V_FW_RDEV_WR_VFT(x)	((x) << S_FW_RDEV_WR_VFT)
+#define G_FW_RDEV_WR_VFT(x)	(((x) >> S_FW_RDEV_WR_VFT) & M_FW_RDEV_WR_VFT)
+#define F_FW_RDEV_WR_VFT	V_FW_RDEV_WR_VFT(1U)
+
+#define S_FW_RDEV_WR_NPIV	6
+#define M_FW_RDEV_WR_NPIV	0x1
+#define V_FW_RDEV_WR_NPIV(x)	((x) << S_FW_RDEV_WR_NPIV)
+#define G_FW_RDEV_WR_NPIV(x)	\
+    (((x) >> S_FW_RDEV_WR_NPIV) & M_FW_RDEV_WR_NPIV)
+#define F_FW_RDEV_WR_NPIV	V_FW_RDEV_WR_NPIV(1U)
+
+#define S_FW_RDEV_WR_CLASS	4
+#define M_FW_RDEV_WR_CLASS	0x3
+#define V_FW_RDEV_WR_CLASS(x)	((x) << S_FW_RDEV_WR_CLASS)
+#define G_FW_RDEV_WR_CLASS(x)	\
+    (((x) >> S_FW_RDEV_WR_CLASS) & M_FW_RDEV_WR_CLASS)
+
+#define S_FW_RDEV_WR_SEQ_DEL	3
+#define M_FW_RDEV_WR_SEQ_DEL	0x1
+#define V_FW_RDEV_WR_SEQ_DEL(x)	((x) << S_FW_RDEV_WR_SEQ_DEL)
+#define G_FW_RDEV_WR_SEQ_DEL(x)	\
+    (((x) >> S_FW_RDEV_WR_SEQ_DEL) & M_FW_RDEV_WR_SEQ_DEL)
+#define F_FW_RDEV_WR_SEQ_DEL	V_FW_RDEV_WR_SEQ_DEL(1U)
+
+#define S_FW_RDEV_WR_PRIO_PREEMP	2
+#define M_FW_RDEV_WR_PRIO_PREEMP	0x1
+#define V_FW_RDEV_WR_PRIO_PREEMP(x)	((x) << S_FW_RDEV_WR_PRIO_PREEMP)
+#define G_FW_RDEV_WR_PRIO_PREEMP(x)	\
+    (((x) >> S_FW_RDEV_WR_PRIO_PREEMP) & M_FW_RDEV_WR_PRIO_PREEMP)
+#define F_FW_RDEV_WR_PRIO_PREEMP	V_FW_RDEV_WR_PRIO_PREEMP(1U)
+
+#define S_FW_RDEV_WR_PREF	1
+#define M_FW_RDEV_WR_PREF	0x1
+#define V_FW_RDEV_WR_PREF(x)	((x) << S_FW_RDEV_WR_PREF)
+#define G_FW_RDEV_WR_PREF(x)	\
+    (((x) >> S_FW_RDEV_WR_PREF) & M_FW_RDEV_WR_PREF)
+#define F_FW_RDEV_WR_PREF	V_FW_RDEV_WR_PREF(1U)
+
+#define S_FW_RDEV_WR_QOS	0
+#define M_FW_RDEV_WR_QOS	0x1
+#define V_FW_RDEV_WR_QOS(x)	((x) << S_FW_RDEV_WR_QOS)
+#define G_FW_RDEV_WR_QOS(x)	(((x) >> S_FW_RDEV_WR_QOS) & M_FW_RDEV_WR_QOS)
+#define F_FW_RDEV_WR_QOS	V_FW_RDEV_WR_QOS(1U)
+
+#define S_FW_RDEV_WR_ORG_PROC_ASSOC	7
+#define M_FW_RDEV_WR_ORG_PROC_ASSOC	0x1
+#define V_FW_RDEV_WR_ORG_PROC_ASSOC(x)	((x) << S_FW_RDEV_WR_ORG_PROC_ASSOC)
+#define G_FW_RDEV_WR_ORG_PROC_ASSOC(x)	\
+    (((x) >> S_FW_RDEV_WR_ORG_PROC_ASSOC) & M_FW_RDEV_WR_ORG_PROC_ASSOC)
+#define F_FW_RDEV_WR_ORG_PROC_ASSOC	V_FW_RDEV_WR_ORG_PROC_ASSOC(1U)
+
+#define S_FW_RDEV_WR_RSP_PROC_ASSOC	6
+#define M_FW_RDEV_WR_RSP_PROC_ASSOC	0x1
+#define V_FW_RDEV_WR_RSP_PROC_ASSOC(x)	((x) << S_FW_RDEV_WR_RSP_PROC_ASSOC)
+#define G_FW_RDEV_WR_RSP_PROC_ASSOC(x)	\
+    (((x) >> S_FW_RDEV_WR_RSP_PROC_ASSOC) & M_FW_RDEV_WR_RSP_PROC_ASSOC)
+#define F_FW_RDEV_WR_RSP_PROC_ASSOC	V_FW_RDEV_WR_RSP_PROC_ASSOC(1U)
+
+#define S_FW_RDEV_WR_IMAGE_PAIR		5
+#define M_FW_RDEV_WR_IMAGE_PAIR		0x1
+#define V_FW_RDEV_WR_IMAGE_PAIR(x)	((x) << S_FW_RDEV_WR_IMAGE_PAIR)
+#define G_FW_RDEV_WR_IMAGE_PAIR(x)	\
+    (((x) >> S_FW_RDEV_WR_IMAGE_PAIR) & M_FW_RDEV_WR_IMAGE_PAIR)
+#define F_FW_RDEV_WR_IMAGE_PAIR	V_FW_RDEV_WR_IMAGE_PAIR(1U)
+
+#define S_FW_RDEV_WR_ACC_RSP_CODE	0
+#define M_FW_RDEV_WR_ACC_RSP_CODE	0x1f
+#define V_FW_RDEV_WR_ACC_RSP_CODE(x)	((x) << S_FW_RDEV_WR_ACC_RSP_CODE)
+#define G_FW_RDEV_WR_ACC_RSP_CODE(x)	\
+    (((x) >> S_FW_RDEV_WR_ACC_RSP_CODE) & M_FW_RDEV_WR_ACC_RSP_CODE)
+
+#define S_FW_RDEV_WR_ENH_DISC		7
+#define M_FW_RDEV_WR_ENH_DISC		0x1
+#define V_FW_RDEV_WR_ENH_DISC(x)	((x) << S_FW_RDEV_WR_ENH_DISC)
+#define G_FW_RDEV_WR_ENH_DISC(x)	\
+    (((x) >> S_FW_RDEV_WR_ENH_DISC) & M_FW_RDEV_WR_ENH_DISC)
+#define F_FW_RDEV_WR_ENH_DISC	V_FW_RDEV_WR_ENH_DISC(1U)
+
+#define S_FW_RDEV_WR_REC	6
+#define M_FW_RDEV_WR_REC	0x1
+#define V_FW_RDEV_WR_REC(x)	((x) << S_FW_RDEV_WR_REC)
+#define G_FW_RDEV_WR_REC(x)	(((x) >> S_FW_RDEV_WR_REC) & M_FW_RDEV_WR_REC)
+#define F_FW_RDEV_WR_REC	V_FW_RDEV_WR_REC(1U)
+
+#define S_FW_RDEV_WR_TASK_RETRY_ID	5
+#define M_FW_RDEV_WR_TASK_RETRY_ID	0x1
+#define V_FW_RDEV_WR_TASK_RETRY_ID(x)	((x) << S_FW_RDEV_WR_TASK_RETRY_ID)
+#define G_FW_RDEV_WR_TASK_RETRY_ID(x)	\
+    (((x) >> S_FW_RDEV_WR_TASK_RETRY_ID) & M_FW_RDEV_WR_TASK_RETRY_ID)
+#define F_FW_RDEV_WR_TASK_RETRY_ID	V_FW_RDEV_WR_TASK_RETRY_ID(1U)
+
+#define S_FW_RDEV_WR_RETRY	4
+#define M_FW_RDEV_WR_RETRY	0x1
+#define V_FW_RDEV_WR_RETRY(x)	((x) << S_FW_RDEV_WR_RETRY)
+#define G_FW_RDEV_WR_RETRY(x)	\
+    (((x) >> S_FW_RDEV_WR_RETRY) & M_FW_RDEV_WR_RETRY)
+#define F_FW_RDEV_WR_RETRY	V_FW_RDEV_WR_RETRY(1U)
+
+#define S_FW_RDEV_WR_CONF_CMPL		3
+#define M_FW_RDEV_WR_CONF_CMPL		0x1
+#define V_FW_RDEV_WR_CONF_CMPL(x)	((x) << S_FW_RDEV_WR_CONF_CMPL)
+#define G_FW_RDEV_WR_CONF_CMPL(x)	\
+    (((x) >> S_FW_RDEV_WR_CONF_CMPL) & M_FW_RDEV_WR_CONF_CMPL)
+#define F_FW_RDEV_WR_CONF_CMPL	V_FW_RDEV_WR_CONF_CMPL(1U)
+
+#define S_FW_RDEV_WR_DATA_OVLY		2
+#define M_FW_RDEV_WR_DATA_OVLY		0x1
+#define V_FW_RDEV_WR_DATA_OVLY(x)	((x) << S_FW_RDEV_WR_DATA_OVLY)
+#define G_FW_RDEV_WR_DATA_OVLY(x)	\
+    (((x) >> S_FW_RDEV_WR_DATA_OVLY) & M_FW_RDEV_WR_DATA_OVLY)
+#define F_FW_RDEV_WR_DATA_OVLY	V_FW_RDEV_WR_DATA_OVLY(1U)
+
+#define S_FW_RDEV_WR_INI	1
+#define M_FW_RDEV_WR_INI	0x1
+#define V_FW_RDEV_WR_INI(x)	((x) << S_FW_RDEV_WR_INI)
+#define G_FW_RDEV_WR_INI(x)	(((x) >> S_FW_RDEV_WR_INI) & M_FW_RDEV_WR_INI)
+#define F_FW_RDEV_WR_INI	V_FW_RDEV_WR_INI(1U)
+
+#define S_FW_RDEV_WR_TGT	0
+#define M_FW_RDEV_WR_TGT	0x1
+#define V_FW_RDEV_WR_TGT(x)	((x) << S_FW_RDEV_WR_TGT)
+#define G_FW_RDEV_WR_TGT(x)	(((x) >> S_FW_RDEV_WR_TGT) & M_FW_RDEV_WR_TGT)
+#define F_FW_RDEV_WR_TGT	V_FW_RDEV_WR_TGT(1U)
+
+struct fw_foiscsi_node_wr {
+	__be32 op_to_immdlen;
+	__be32 flowid_len16;
+	__u64  cookie;
+	__u8   subop;
+	__u8   status;
+	__u8   alias_len;
+	__u8   iqn_len;
+	__be32 node_flowid;
+	__be16 nodeid;
+	__be16 login_retry;
+	__be16 retry_timeout;
+	__be16 r3;
+	__u8   iqn[224];
+	__u8   alias[224];
+};
+
+#define S_FW_FOISCSI_NODE_WR_IMMDLEN	0
+#define M_FW_FOISCSI_NODE_WR_IMMDLEN	0xffff
+#define V_FW_FOISCSI_NODE_WR_IMMDLEN(x)	((x) << S_FW_FOISCSI_NODE_WR_IMMDLEN)
+#define G_FW_FOISCSI_NODE_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_FOISCSI_NODE_WR_IMMDLEN) & M_FW_FOISCSI_NODE_WR_IMMDLEN)
+
+struct fw_foiscsi_ctrl_wr {
+	__be32 op_compl;
+	__be32 flowid_len16;
+	__u64  cookie;
+	__u8   subop;
+	__u8   status;
+	__u8   ctrl_state;
+	__u8   io_state;
+	__be32 node_id;
+	__be32 ctrl_id;
+	__be32 io_id;
+	struct fw_foiscsi_sess_attr {
+		__be32 sess_type_to_erl;
+		__be16 max_conn;
+		__be16 max_r2t;
+		__be16 time2wait;
+		__be16 time2retain;
+		__be32 max_burst;
+		__be32 first_burst;
+		__be32 r1;
+	} sess_attr;
+	struct fw_foiscsi_conn_attr {
+		__be32 hdigest_to_auth_policy;
+		__be32 max_rcv_dsl;
+		__be32 ping_tmo;
+		__be16 dst_port;
+		__be16 src_port;
+		union fw_foiscsi_conn_attr_addr {
+			struct fw_foiscsi_conn_attr_ipv6 {
+				__be64 dst_addr[2];
+				__be64 src_addr[2];
+			} ipv6_addr;
+			struct fw_foiscsi_conn_attr_ipv4 {
+				__be32 dst_addr;
+				__be32 src_addr;
+			} ipv4_addr;
+		} u;
+	} conn_attr;
+	__u8   tgt_name_len;
+	__u8   r3[7];
+	__u8   tgt_name[224];
+};
+
+#define S_FW_FOISCSI_CTRL_WR_SESS_TYPE		30
+#define M_FW_FOISCSI_CTRL_WR_SESS_TYPE		0x3
+#define V_FW_FOISCSI_CTRL_WR_SESS_TYPE(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_SESS_TYPE)
+#define G_FW_FOISCSI_CTRL_WR_SESS_TYPE(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_SESS_TYPE) & M_FW_FOISCSI_CTRL_WR_SESS_TYPE)
+
+#define S_FW_FOISCSI_CTRL_WR_SEQ_INORDER	29
+#define M_FW_FOISCSI_CTRL_WR_SEQ_INORDER	0x1
+#define V_FW_FOISCSI_CTRL_WR_SEQ_INORDER(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_SEQ_INORDER)
+#define G_FW_FOISCSI_CTRL_WR_SEQ_INORDER(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_SEQ_INORDER) & \
+     M_FW_FOISCSI_CTRL_WR_SEQ_INORDER)
+#define F_FW_FOISCSI_CTRL_WR_SEQ_INORDER	\
+    V_FW_FOISCSI_CTRL_WR_SEQ_INORDER(1U)
+
+#define S_FW_FOISCSI_CTRL_WR_PDU_INORDER	28
+#define M_FW_FOISCSI_CTRL_WR_PDU_INORDER	0x1
+#define V_FW_FOISCSI_CTRL_WR_PDU_INORDER(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_PDU_INORDER)
+#define G_FW_FOISCSI_CTRL_WR_PDU_INORDER(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_PDU_INORDER) & \
+     M_FW_FOISCSI_CTRL_WR_PDU_INORDER)
+#define F_FW_FOISCSI_CTRL_WR_PDU_INORDER	\
+    V_FW_FOISCSI_CTRL_WR_PDU_INORDER(1U)
+
+#define S_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN	27
+#define M_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN	0x1
+#define V_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN)
+#define G_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN) & \
+     M_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN)
+#define F_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN	\
+    V_FW_FOISCSI_CTRL_WR_IMMD_DATA_EN(1U)
+
+#define S_FW_FOISCSI_CTRL_WR_INIT_R2T_EN	26
+#define M_FW_FOISCSI_CTRL_WR_INIT_R2T_EN	0x1
+#define V_FW_FOISCSI_CTRL_WR_INIT_R2T_EN(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_INIT_R2T_EN)
+#define G_FW_FOISCSI_CTRL_WR_INIT_R2T_EN(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_INIT_R2T_EN) & \
+     M_FW_FOISCSI_CTRL_WR_INIT_R2T_EN)
+#define F_FW_FOISCSI_CTRL_WR_INIT_R2T_EN	\
+    V_FW_FOISCSI_CTRL_WR_INIT_R2T_EN(1U)
+
+#define S_FW_FOISCSI_CTRL_WR_ERL	24
+#define M_FW_FOISCSI_CTRL_WR_ERL	0x3
+#define V_FW_FOISCSI_CTRL_WR_ERL(x)	((x) << S_FW_FOISCSI_CTRL_WR_ERL)
+#define G_FW_FOISCSI_CTRL_WR_ERL(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_ERL) & M_FW_FOISCSI_CTRL_WR_ERL)
+
+#define S_FW_FOISCSI_CTRL_WR_HDIGEST	30
+#define M_FW_FOISCSI_CTRL_WR_HDIGEST	0x3
+#define V_FW_FOISCSI_CTRL_WR_HDIGEST(x)	((x) << S_FW_FOISCSI_CTRL_WR_HDIGEST)
+#define G_FW_FOISCSI_CTRL_WR_HDIGEST(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_HDIGEST) & M_FW_FOISCSI_CTRL_WR_HDIGEST)
+
+#define S_FW_FOISCSI_CTRL_WR_DDIGEST	28
+#define M_FW_FOISCSI_CTRL_WR_DDIGEST	0x3
+#define V_FW_FOISCSI_CTRL_WR_DDIGEST(x)	((x) << S_FW_FOISCSI_CTRL_WR_DDIGEST)
+#define G_FW_FOISCSI_CTRL_WR_DDIGEST(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_DDIGEST) & M_FW_FOISCSI_CTRL_WR_DDIGEST)
+
+#define S_FW_FOISCSI_CTRL_WR_AUTH_METHOD	25
+#define M_FW_FOISCSI_CTRL_WR_AUTH_METHOD	0x7
+#define V_FW_FOISCSI_CTRL_WR_AUTH_METHOD(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_AUTH_METHOD)
+#define G_FW_FOISCSI_CTRL_WR_AUTH_METHOD(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_AUTH_METHOD) & \
+     M_FW_FOISCSI_CTRL_WR_AUTH_METHOD)
+
+#define S_FW_FOISCSI_CTRL_WR_AUTH_POLICY	23
+#define M_FW_FOISCSI_CTRL_WR_AUTH_POLICY	0x3
+#define V_FW_FOISCSI_CTRL_WR_AUTH_POLICY(x)	\
+    ((x) << S_FW_FOISCSI_CTRL_WR_AUTH_POLICY)
+#define G_FW_FOISCSI_CTRL_WR_AUTH_POLICY(x)	\
+    (((x) >> S_FW_FOISCSI_CTRL_WR_AUTH_POLICY) & \
+     M_FW_FOISCSI_CTRL_WR_AUTH_POLICY)
+
+struct fw_foiscsi_chap_wr {
+	__be32 op_compl;
+	__be32 flowid_len16;
+	__u64  cookie;
+	__u8   status;
+	__u8   id_len;
+	__u8   sec_len;
+	__u8   tgt_id_len;
+	__u8   tgt_sec_len;
+	__be16 node_id;
+	__u8   r2;
+	__u8   chap_id[64];
+	__u8   chap_sec[16];
+	__u8   tgt_id[64];
+	__u8   tgt_sec[16];
+};
+
+/******************************************************************************
+ *  F O F C O E   W O R K R E Q U E S T s
+ *******************************************/
+
+struct fw_fcoe_els_ct_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   tmo_val;
+	__u8   els_ct_type;
+	__u8   ctl_pri;
+	__u8   cp_en_class;
+	__be16 xfer_cnt;
+	__u8   fl_to_sp;
+	__u8   l_id[3];
+	__u8   r5;
+	__u8   r_id[3];
+	__be64 rsp_dmaaddr;
+	__be32 rsp_dmalen;
+	__be32 r6;
+};
+
+#define S_FW_FCOE_ELS_CT_WR_OPCODE	24
+#define M_FW_FCOE_ELS_CT_WR_OPCODE	0xff
+#define V_FW_FCOE_ELS_CT_WR_OPCODE(x)	((x) << S_FW_FCOE_ELS_CT_WR_OPCODE)
+#define G_FW_FCOE_ELS_CT_WR_OPCODE(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_OPCODE) & M_FW_FCOE_ELS_CT_WR_OPCODE)
+
+#define S_FW_FCOE_ELS_CT_WR_IMMDLEN	0
+#define M_FW_FCOE_ELS_CT_WR_IMMDLEN	0xff
+#define V_FW_FCOE_ELS_CT_WR_IMMDLEN(x)	((x) << S_FW_FCOE_ELS_CT_WR_IMMDLEN)
+#define G_FW_FCOE_ELS_CT_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_IMMDLEN) & M_FW_FCOE_ELS_CT_WR_IMMDLEN)
+
+#define S_FW_FCOE_ELS_CT_WR_FLOWID	8
+#define M_FW_FCOE_ELS_CT_WR_FLOWID	0xfffff
+#define V_FW_FCOE_ELS_CT_WR_FLOWID(x)	((x) << S_FW_FCOE_ELS_CT_WR_FLOWID)
+#define G_FW_FCOE_ELS_CT_WR_FLOWID(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_FLOWID) & M_FW_FCOE_ELS_CT_WR_FLOWID)
+
+#define S_FW_FCOE_ELS_CT_WR_LEN16	0
+#define M_FW_FCOE_ELS_CT_WR_LEN16	0xff
+#define V_FW_FCOE_ELS_CT_WR_LEN16(x)	((x) << S_FW_FCOE_ELS_CT_WR_LEN16)
+#define G_FW_FCOE_ELS_CT_WR_LEN16(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_LEN16) & M_FW_FCOE_ELS_CT_WR_LEN16)
+
+#define S_FW_FCOE_ELS_CT_WR_CP_EN	6
+#define M_FW_FCOE_ELS_CT_WR_CP_EN	0x3
+#define V_FW_FCOE_ELS_CT_WR_CP_EN(x)	((x) << S_FW_FCOE_ELS_CT_WR_CP_EN)
+#define G_FW_FCOE_ELS_CT_WR_CP_EN(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_CP_EN) & M_FW_FCOE_ELS_CT_WR_CP_EN)
+
+#define S_FW_FCOE_ELS_CT_WR_CLASS	4
+#define M_FW_FCOE_ELS_CT_WR_CLASS	0x3
+#define V_FW_FCOE_ELS_CT_WR_CLASS(x)	((x) << S_FW_FCOE_ELS_CT_WR_CLASS)
+#define G_FW_FCOE_ELS_CT_WR_CLASS(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_CLASS) & M_FW_FCOE_ELS_CT_WR_CLASS)
+
+#define S_FW_FCOE_ELS_CT_WR_FL		2
+#define M_FW_FCOE_ELS_CT_WR_FL		0x1
+#define V_FW_FCOE_ELS_CT_WR_FL(x)	((x) << S_FW_FCOE_ELS_CT_WR_FL)
+#define G_FW_FCOE_ELS_CT_WR_FL(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_FL) & M_FW_FCOE_ELS_CT_WR_FL)
+#define F_FW_FCOE_ELS_CT_WR_FL	V_FW_FCOE_ELS_CT_WR_FL(1U)
+
+#define S_FW_FCOE_ELS_CT_WR_NPIV	1
+#define M_FW_FCOE_ELS_CT_WR_NPIV	0x1
+#define V_FW_FCOE_ELS_CT_WR_NPIV(x)	((x) << S_FW_FCOE_ELS_CT_WR_NPIV)
+#define G_FW_FCOE_ELS_CT_WR_NPIV(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_NPIV) & M_FW_FCOE_ELS_CT_WR_NPIV)
+#define F_FW_FCOE_ELS_CT_WR_NPIV	V_FW_FCOE_ELS_CT_WR_NPIV(1U)
+
+#define S_FW_FCOE_ELS_CT_WR_SP		0
+#define M_FW_FCOE_ELS_CT_WR_SP		0x1
+#define V_FW_FCOE_ELS_CT_WR_SP(x)	((x) << S_FW_FCOE_ELS_CT_WR_SP)
+#define G_FW_FCOE_ELS_CT_WR_SP(x)	\
+    (((x) >> S_FW_FCOE_ELS_CT_WR_SP) & M_FW_FCOE_ELS_CT_WR_SP)
+#define F_FW_FCOE_ELS_CT_WR_SP	V_FW_FCOE_ELS_CT_WR_SP(1U)
+
+/******************************************************************************
+ *  S C S I   W O R K R E Q U E S T s   (FOiSCSI and FCOE unified data path)
+ *****************************************************************************/
+
+struct fw_scsi_write_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   tmo_val;
+	__u8   use_xfer_cnt;
+	union fw_scsi_write_priv {
+		struct fcoe_write_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r3_lo[2];
+		} fcoe;
+		struct iscsi_write_priv {
+			__u8   r3[4];
+		} iscsi;
+	} u;
+	__be32 xfer_cnt;
+	__be32 ini_xfer_cnt;
+	__be64 rsp_dmaaddr;
+	__be32 rsp_dmalen;
+	__be32 r4;
+};
+
+#define S_FW_SCSI_WRITE_WR_OPCODE	24
+#define M_FW_SCSI_WRITE_WR_OPCODE	0xff
+#define V_FW_SCSI_WRITE_WR_OPCODE(x)	((x) << S_FW_SCSI_WRITE_WR_OPCODE)
+#define G_FW_SCSI_WRITE_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_OPCODE) & M_FW_SCSI_WRITE_WR_OPCODE)
+
+#define S_FW_SCSI_WRITE_WR_IMMDLEN	0
+#define M_FW_SCSI_WRITE_WR_IMMDLEN	0xff
+#define V_FW_SCSI_WRITE_WR_IMMDLEN(x)	((x) << S_FW_SCSI_WRITE_WR_IMMDLEN)
+#define G_FW_SCSI_WRITE_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_IMMDLEN) & M_FW_SCSI_WRITE_WR_IMMDLEN)
+
+#define S_FW_SCSI_WRITE_WR_FLOWID	8
+#define M_FW_SCSI_WRITE_WR_FLOWID	0xfffff
+#define V_FW_SCSI_WRITE_WR_FLOWID(x)	((x) << S_FW_SCSI_WRITE_WR_FLOWID)
+#define G_FW_SCSI_WRITE_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_FLOWID) & M_FW_SCSI_WRITE_WR_FLOWID)
+
+#define S_FW_SCSI_WRITE_WR_LEN16	0
+#define M_FW_SCSI_WRITE_WR_LEN16	0xff
+#define V_FW_SCSI_WRITE_WR_LEN16(x)	((x) << S_FW_SCSI_WRITE_WR_LEN16)
+#define G_FW_SCSI_WRITE_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_LEN16) & M_FW_SCSI_WRITE_WR_LEN16)
+
+#define S_FW_SCSI_WRITE_WR_CP_EN	6
+#define M_FW_SCSI_WRITE_WR_CP_EN	0x3
+#define V_FW_SCSI_WRITE_WR_CP_EN(x)	((x) << S_FW_SCSI_WRITE_WR_CP_EN)
+#define G_FW_SCSI_WRITE_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_CP_EN) & M_FW_SCSI_WRITE_WR_CP_EN)
+
+#define S_FW_SCSI_WRITE_WR_CLASS	4
+#define M_FW_SCSI_WRITE_WR_CLASS	0x3
+#define V_FW_SCSI_WRITE_WR_CLASS(x)	((x) << S_FW_SCSI_WRITE_WR_CLASS)
+#define G_FW_SCSI_WRITE_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_WRITE_WR_CLASS) & M_FW_SCSI_WRITE_WR_CLASS)
+
+struct fw_scsi_read_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   tmo_val;
+	__u8   use_xfer_cnt;
+	union fw_scsi_read_priv {
+		struct fcoe_read_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r3_lo[2];
+		} fcoe;
+		struct iscsi_read_priv {
+			__u8   r3[4];
+		} iscsi;
+	} u;
+	__be32 xfer_cnt;
+	__be32 ini_xfer_cnt;
+	__be64 rsp_dmaaddr;
+	__be32 rsp_dmalen;
+	__be32 r4;
+};
+
+#define S_FW_SCSI_READ_WR_OPCODE	24
+#define M_FW_SCSI_READ_WR_OPCODE	0xff
+#define V_FW_SCSI_READ_WR_OPCODE(x)	((x) << S_FW_SCSI_READ_WR_OPCODE)
+#define G_FW_SCSI_READ_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_OPCODE) & M_FW_SCSI_READ_WR_OPCODE)
+
+#define S_FW_SCSI_READ_WR_IMMDLEN	0
+#define M_FW_SCSI_READ_WR_IMMDLEN	0xff
+#define V_FW_SCSI_READ_WR_IMMDLEN(x)	((x) << S_FW_SCSI_READ_WR_IMMDLEN)
+#define G_FW_SCSI_READ_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_IMMDLEN) & M_FW_SCSI_READ_WR_IMMDLEN)
+
+#define S_FW_SCSI_READ_WR_FLOWID	8
+#define M_FW_SCSI_READ_WR_FLOWID	0xfffff
+#define V_FW_SCSI_READ_WR_FLOWID(x)	((x) << S_FW_SCSI_READ_WR_FLOWID)
+#define G_FW_SCSI_READ_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_FLOWID) & M_FW_SCSI_READ_WR_FLOWID)
+
+#define S_FW_SCSI_READ_WR_LEN16		0
+#define M_FW_SCSI_READ_WR_LEN16		0xff
+#define V_FW_SCSI_READ_WR_LEN16(x)	((x) << S_FW_SCSI_READ_WR_LEN16)
+#define G_FW_SCSI_READ_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_LEN16) & M_FW_SCSI_READ_WR_LEN16)
+
+#define S_FW_SCSI_READ_WR_CP_EN		6
+#define M_FW_SCSI_READ_WR_CP_EN		0x3
+#define V_FW_SCSI_READ_WR_CP_EN(x)	((x) << S_FW_SCSI_READ_WR_CP_EN)
+#define G_FW_SCSI_READ_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_CP_EN) & M_FW_SCSI_READ_WR_CP_EN)
+
+#define S_FW_SCSI_READ_WR_CLASS		4
+#define M_FW_SCSI_READ_WR_CLASS		0x3
+#define V_FW_SCSI_READ_WR_CLASS(x)	((x) << S_FW_SCSI_READ_WR_CLASS)
+#define G_FW_SCSI_READ_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_READ_WR_CLASS) & M_FW_SCSI_READ_WR_CLASS)
+
+struct fw_scsi_cmd_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   tmo_val;
+	__u8   r3;
+	union fw_scsi_cmd_priv {
+		struct fcoe_cmd_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r4_lo[2];
+		} fcoe;
+		struct iscsi_cmd_priv {
+			__u8   r4[4];
+		} iscsi;
+	} u;
+	__u8   r5[8];
+	__be64 rsp_dmaaddr;
+	__be32 rsp_dmalen;
+	__be32 r6;
+};
+
+#define S_FW_SCSI_CMD_WR_OPCODE		24
+#define M_FW_SCSI_CMD_WR_OPCODE		0xff
+#define V_FW_SCSI_CMD_WR_OPCODE(x)	((x) << S_FW_SCSI_CMD_WR_OPCODE)
+#define G_FW_SCSI_CMD_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_OPCODE) & M_FW_SCSI_CMD_WR_OPCODE)
+
+#define S_FW_SCSI_CMD_WR_IMMDLEN	0
+#define M_FW_SCSI_CMD_WR_IMMDLEN	0xff
+#define V_FW_SCSI_CMD_WR_IMMDLEN(x)	((x) << S_FW_SCSI_CMD_WR_IMMDLEN)
+#define G_FW_SCSI_CMD_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_IMMDLEN) & M_FW_SCSI_CMD_WR_IMMDLEN)
+
+#define S_FW_SCSI_CMD_WR_FLOWID		8
+#define M_FW_SCSI_CMD_WR_FLOWID		0xfffff
+#define V_FW_SCSI_CMD_WR_FLOWID(x)	((x) << S_FW_SCSI_CMD_WR_FLOWID)
+#define G_FW_SCSI_CMD_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_FLOWID) & M_FW_SCSI_CMD_WR_FLOWID)
+
+#define S_FW_SCSI_CMD_WR_LEN16		0
+#define M_FW_SCSI_CMD_WR_LEN16		0xff
+#define V_FW_SCSI_CMD_WR_LEN16(x)	((x) << S_FW_SCSI_CMD_WR_LEN16)
+#define G_FW_SCSI_CMD_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_LEN16) & M_FW_SCSI_CMD_WR_LEN16)
+
+#define S_FW_SCSI_CMD_WR_CP_EN		6
+#define M_FW_SCSI_CMD_WR_CP_EN		0x3
+#define V_FW_SCSI_CMD_WR_CP_EN(x)	((x) << S_FW_SCSI_CMD_WR_CP_EN)
+#define G_FW_SCSI_CMD_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_CP_EN) & M_FW_SCSI_CMD_WR_CP_EN)
+
+#define S_FW_SCSI_CMD_WR_CLASS		4
+#define M_FW_SCSI_CMD_WR_CLASS		0x3
+#define V_FW_SCSI_CMD_WR_CLASS(x)	((x) << S_FW_SCSI_CMD_WR_CLASS)
+#define G_FW_SCSI_CMD_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_CMD_WR_CLASS) & M_FW_SCSI_CMD_WR_CLASS)
+
+struct fw_scsi_abrt_cls_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   tmo_val;
+	__u8   sub_opcode_to_chk_all_io;
+	__u8   r3[4];
+	__be64 t_cookie;
+};
+
+#define S_FW_SCSI_ABRT_CLS_WR_OPCODE	24
+#define M_FW_SCSI_ABRT_CLS_WR_OPCODE	0xff
+#define V_FW_SCSI_ABRT_CLS_WR_OPCODE(x)	((x) << S_FW_SCSI_ABRT_CLS_WR_OPCODE)
+#define G_FW_SCSI_ABRT_CLS_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_OPCODE) & M_FW_SCSI_ABRT_CLS_WR_OPCODE)
+
+#define S_FW_SCSI_ABRT_CLS_WR_IMMDLEN		0
+#define M_FW_SCSI_ABRT_CLS_WR_IMMDLEN		0xff
+#define V_FW_SCSI_ABRT_CLS_WR_IMMDLEN(x)	\
+    ((x) << S_FW_SCSI_ABRT_CLS_WR_IMMDLEN)
+#define G_FW_SCSI_ABRT_CLS_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_IMMDLEN) & M_FW_SCSI_ABRT_CLS_WR_IMMDLEN)
+
+#define S_FW_SCSI_ABRT_CLS_WR_FLOWID	8
+#define M_FW_SCSI_ABRT_CLS_WR_FLOWID	0xfffff
+#define V_FW_SCSI_ABRT_CLS_WR_FLOWID(x)	((x) << S_FW_SCSI_ABRT_CLS_WR_FLOWID)
+#define G_FW_SCSI_ABRT_CLS_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_FLOWID) & M_FW_SCSI_ABRT_CLS_WR_FLOWID)
+
+#define S_FW_SCSI_ABRT_CLS_WR_LEN16	0
+#define M_FW_SCSI_ABRT_CLS_WR_LEN16	0xff
+#define V_FW_SCSI_ABRT_CLS_WR_LEN16(x)	((x) << S_FW_SCSI_ABRT_CLS_WR_LEN16)
+#define G_FW_SCSI_ABRT_CLS_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_LEN16) & M_FW_SCSI_ABRT_CLS_WR_LEN16)
+
+#define S_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE	2
+#define M_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE	0x3f
+#define V_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE(x)	\
+    ((x) << S_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE)
+#define G_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE) & \
+     M_FW_SCSI_ABRT_CLS_WR_SUB_OPCODE)
+
+#define S_FW_SCSI_ABRT_CLS_WR_UNSOL	1
+#define M_FW_SCSI_ABRT_CLS_WR_UNSOL	0x1
+#define V_FW_SCSI_ABRT_CLS_WR_UNSOL(x)	((x) << S_FW_SCSI_ABRT_CLS_WR_UNSOL)
+#define G_FW_SCSI_ABRT_CLS_WR_UNSOL(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_UNSOL) & M_FW_SCSI_ABRT_CLS_WR_UNSOL)
+#define F_FW_SCSI_ABRT_CLS_WR_UNSOL	V_FW_SCSI_ABRT_CLS_WR_UNSOL(1U)
+
+#define S_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO	0
+#define M_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO	0x1
+#define V_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO(x)	\
+    ((x) << S_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO)
+#define G_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO(x)	\
+    (((x) >> S_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO) & \
+     M_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO)
+#define F_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO	\
+    V_FW_SCSI_ABRT_CLS_WR_CHK_ALL_IO(1U)
+
+struct fw_scsi_tgt_acc_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   r3;
+	__u8   use_burst_len;
+	union fw_scsi_tgt_acc_priv {
+		struct fcoe_tgt_acc_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r4_lo[2];
+		} fcoe;
+		struct iscsi_tgt_acc_priv {
+			__u8   r4[4];
+		} iscsi;
+	} u;
+	__be32 burst_len;
+	__be32 rel_off;
+	__be64 r5;
+	__be32 r6;
+	__be32 tot_xfer_len;
+};
+
+#define S_FW_SCSI_TGT_ACC_WR_OPCODE	24
+#define M_FW_SCSI_TGT_ACC_WR_OPCODE	0xff
+#define V_FW_SCSI_TGT_ACC_WR_OPCODE(x)	((x) << S_FW_SCSI_TGT_ACC_WR_OPCODE)
+#define G_FW_SCSI_TGT_ACC_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_OPCODE) & M_FW_SCSI_TGT_ACC_WR_OPCODE)
+
+#define S_FW_SCSI_TGT_ACC_WR_IMMDLEN	0
+#define M_FW_SCSI_TGT_ACC_WR_IMMDLEN	0xff
+#define V_FW_SCSI_TGT_ACC_WR_IMMDLEN(x)	((x) << S_FW_SCSI_TGT_ACC_WR_IMMDLEN)
+#define G_FW_SCSI_TGT_ACC_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_IMMDLEN) & M_FW_SCSI_TGT_ACC_WR_IMMDLEN)
+
+#define S_FW_SCSI_TGT_ACC_WR_FLOWID	8
+#define M_FW_SCSI_TGT_ACC_WR_FLOWID	0xfffff
+#define V_FW_SCSI_TGT_ACC_WR_FLOWID(x)	((x) << S_FW_SCSI_TGT_ACC_WR_FLOWID)
+#define G_FW_SCSI_TGT_ACC_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_FLOWID) & M_FW_SCSI_TGT_ACC_WR_FLOWID)
+
+#define S_FW_SCSI_TGT_ACC_WR_LEN16	0
+#define M_FW_SCSI_TGT_ACC_WR_LEN16	0xff
+#define V_FW_SCSI_TGT_ACC_WR_LEN16(x)	((x) << S_FW_SCSI_TGT_ACC_WR_LEN16)
+#define G_FW_SCSI_TGT_ACC_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_LEN16) & M_FW_SCSI_TGT_ACC_WR_LEN16)
+
+#define S_FW_SCSI_TGT_ACC_WR_CP_EN	6
+#define M_FW_SCSI_TGT_ACC_WR_CP_EN	0x3
+#define V_FW_SCSI_TGT_ACC_WR_CP_EN(x)	((x) << S_FW_SCSI_TGT_ACC_WR_CP_EN)
+#define G_FW_SCSI_TGT_ACC_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_CP_EN) & M_FW_SCSI_TGT_ACC_WR_CP_EN)
+
+#define S_FW_SCSI_TGT_ACC_WR_CLASS	4
+#define M_FW_SCSI_TGT_ACC_WR_CLASS	0x3
+#define V_FW_SCSI_TGT_ACC_WR_CLASS(x)	((x) << S_FW_SCSI_TGT_ACC_WR_CLASS)
+#define G_FW_SCSI_TGT_ACC_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_TGT_ACC_WR_CLASS) & M_FW_SCSI_TGT_ACC_WR_CLASS)
+
+struct fw_scsi_tgt_xmit_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   auto_rsp;
+	__u8   use_xfer_cnt;
+	union fw_scsi_tgt_xmit_priv {
+		struct fcoe_tgt_xmit_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r3_lo[2];
+		} fcoe;
+		struct iscsi_tgt_xmit_priv {
+			__u8   r3[4];
+		} iscsi;
+	} u;
+	__be32 xfer_cnt;
+	__be32 r4;
+	__be64 r5;
+	__be32 r6;
+	__be32 tot_xfer_len;
+};
+
+#define S_FW_SCSI_TGT_XMIT_WR_OPCODE	24
+#define M_FW_SCSI_TGT_XMIT_WR_OPCODE	0xff
+#define V_FW_SCSI_TGT_XMIT_WR_OPCODE(x)	((x) << S_FW_SCSI_TGT_XMIT_WR_OPCODE)
+#define G_FW_SCSI_TGT_XMIT_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_OPCODE) & M_FW_SCSI_TGT_XMIT_WR_OPCODE)
+
+#define S_FW_SCSI_TGT_XMIT_WR_IMMDLEN		0
+#define M_FW_SCSI_TGT_XMIT_WR_IMMDLEN		0xff
+#define V_FW_SCSI_TGT_XMIT_WR_IMMDLEN(x)	\
+    ((x) << S_FW_SCSI_TGT_XMIT_WR_IMMDLEN)
+#define G_FW_SCSI_TGT_XMIT_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_IMMDLEN) & M_FW_SCSI_TGT_XMIT_WR_IMMDLEN)
+
+#define S_FW_SCSI_TGT_XMIT_WR_FLOWID	8
+#define M_FW_SCSI_TGT_XMIT_WR_FLOWID	0xfffff
+#define V_FW_SCSI_TGT_XMIT_WR_FLOWID(x)	((x) << S_FW_SCSI_TGT_XMIT_WR_FLOWID)
+#define G_FW_SCSI_TGT_XMIT_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_FLOWID) & M_FW_SCSI_TGT_XMIT_WR_FLOWID)
+
+#define S_FW_SCSI_TGT_XMIT_WR_LEN16	0
+#define M_FW_SCSI_TGT_XMIT_WR_LEN16	0xff
+#define V_FW_SCSI_TGT_XMIT_WR_LEN16(x)	((x) << S_FW_SCSI_TGT_XMIT_WR_LEN16)
+#define G_FW_SCSI_TGT_XMIT_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_LEN16) & M_FW_SCSI_TGT_XMIT_WR_LEN16)
+
+#define S_FW_SCSI_TGT_XMIT_WR_CP_EN	6
+#define M_FW_SCSI_TGT_XMIT_WR_CP_EN	0x3
+#define V_FW_SCSI_TGT_XMIT_WR_CP_EN(x)	((x) << S_FW_SCSI_TGT_XMIT_WR_CP_EN)
+#define G_FW_SCSI_TGT_XMIT_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_CP_EN) & M_FW_SCSI_TGT_XMIT_WR_CP_EN)
+
+#define S_FW_SCSI_TGT_XMIT_WR_CLASS	4
+#define M_FW_SCSI_TGT_XMIT_WR_CLASS	0x3
+#define V_FW_SCSI_TGT_XMIT_WR_CLASS(x)	((x) << S_FW_SCSI_TGT_XMIT_WR_CLASS)
+#define G_FW_SCSI_TGT_XMIT_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_TGT_XMIT_WR_CLASS) & M_FW_SCSI_TGT_XMIT_WR_CLASS)
+
+struct fw_scsi_tgt_rsp_wr {
+	__be32 op_immdlen;
+	__be32 flowid_len16;
+	__be64 cookie;
+	__be16 iqid;
+	__u8   r3[2];
+	union fw_scsi_tgt_rsp_priv {
+		struct fcoe_tgt_rsp_priv {
+			__u8   ctl_pri;
+			__u8   cp_en_class;
+			__u8   r4_lo[2];
+		} fcoe;
+		struct iscsi_tgt_rsp_priv {
+			__u8   r4[4];
+		} iscsi;
+	} u;
+	__u8   r5[8];
+};
+
+#define S_FW_SCSI_TGT_RSP_WR_OPCODE	24
+#define M_FW_SCSI_TGT_RSP_WR_OPCODE	0xff
+#define V_FW_SCSI_TGT_RSP_WR_OPCODE(x)	((x) << S_FW_SCSI_TGT_RSP_WR_OPCODE)
+#define G_FW_SCSI_TGT_RSP_WR_OPCODE(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_OPCODE) & M_FW_SCSI_TGT_RSP_WR_OPCODE)
+
+#define S_FW_SCSI_TGT_RSP_WR_IMMDLEN	0
+#define M_FW_SCSI_TGT_RSP_WR_IMMDLEN	0xff
+#define V_FW_SCSI_TGT_RSP_WR_IMMDLEN(x)	((x) << S_FW_SCSI_TGT_RSP_WR_IMMDLEN)
+#define G_FW_SCSI_TGT_RSP_WR_IMMDLEN(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_IMMDLEN) & M_FW_SCSI_TGT_RSP_WR_IMMDLEN)
+
+#define S_FW_SCSI_TGT_RSP_WR_FLOWID	8
+#define M_FW_SCSI_TGT_RSP_WR_FLOWID	0xfffff
+#define V_FW_SCSI_TGT_RSP_WR_FLOWID(x)	((x) << S_FW_SCSI_TGT_RSP_WR_FLOWID)
+#define G_FW_SCSI_TGT_RSP_WR_FLOWID(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_FLOWID) & M_FW_SCSI_TGT_RSP_WR_FLOWID)
+
+#define S_FW_SCSI_TGT_RSP_WR_LEN16	0
+#define M_FW_SCSI_TGT_RSP_WR_LEN16	0xff
+#define V_FW_SCSI_TGT_RSP_WR_LEN16(x)	((x) << S_FW_SCSI_TGT_RSP_WR_LEN16)
+#define G_FW_SCSI_TGT_RSP_WR_LEN16(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_LEN16) & M_FW_SCSI_TGT_RSP_WR_LEN16)
+
+#define S_FW_SCSI_TGT_RSP_WR_CP_EN	6
+#define M_FW_SCSI_TGT_RSP_WR_CP_EN	0x3
+#define V_FW_SCSI_TGT_RSP_WR_CP_EN(x)	((x) << S_FW_SCSI_TGT_RSP_WR_CP_EN)
+#define G_FW_SCSI_TGT_RSP_WR_CP_EN(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_CP_EN) & M_FW_SCSI_TGT_RSP_WR_CP_EN)
+
+#define S_FW_SCSI_TGT_RSP_WR_CLASS	4
+#define M_FW_SCSI_TGT_RSP_WR_CLASS	0x3
+#define V_FW_SCSI_TGT_RSP_WR_CLASS(x)	((x) << S_FW_SCSI_TGT_RSP_WR_CLASS)
+#define G_FW_SCSI_TGT_RSP_WR_CLASS(x)	\
+    (((x) >> S_FW_SCSI_TGT_RSP_WR_CLASS) & M_FW_SCSI_TGT_RSP_WR_CLASS)
+
+/******************************************************************************
+ *  C O M M A N D s
+ *********************/
+
+/*
+ * The maximum length of time, in miliseconds, that we expect any firmware
+ * command to take to execute and return a reply to the host.  The RESET
+ * and INITIALIZE commands can take a fair amount of time to execute but
+ * most execute in far less time than this maximum.  This constant is used
+ * by host software to determine how long to wait for a firmware command
+ * reply before declaring the firmware as dead/unreachable ...
+ */
+#define FW_CMD_MAX_TIMEOUT	10000
+
+/*
+ * If a host driver does a HELLO and discovers that there's already a MASTER
+ * selected, we may have to wait for that MASTER to finish issuing RESET,
+ * configuration and INITIALIZE commands.  Also, there's a possibility that
+ * our own HELLO may get lost if it happens right as the MASTER is issuign a
+ * RESET command, so we need to be willing to make a few retries of our HELLO.
+ */
+#define FW_CMD_HELLO_TIMEOUT	(3 * FW_CMD_MAX_TIMEOUT)
+#define FW_CMD_HELLO_RETRIES	3
+
+enum fw_cmd_opcodes {
+	FW_LDST_CMD                    = 0x01,
+	FW_RESET_CMD                   = 0x03,
+	FW_HELLO_CMD                   = 0x04,
+	FW_BYE_CMD                     = 0x05,
+	FW_INITIALIZE_CMD              = 0x06,
+	FW_CAPS_CONFIG_CMD             = 0x07,
+	FW_PARAMS_CMD                  = 0x08,
+	FW_PFVF_CMD                    = 0x09,
+	FW_IQ_CMD                      = 0x10,
+	FW_EQ_MNGT_CMD                 = 0x11,
+	FW_EQ_ETH_CMD                  = 0x12,
+	FW_EQ_CTRL_CMD                 = 0x13,
+	FW_EQ_OFLD_CMD                 = 0x21,
+	FW_VI_CMD                      = 0x14,
+	FW_VI_MAC_CMD                  = 0x15,
+	FW_VI_RXMODE_CMD               = 0x16,
+	FW_VI_ENABLE_CMD               = 0x17,
+	FW_VI_STATS_CMD                = 0x1a,
+	FW_ACL_MAC_CMD                 = 0x18,
+	FW_ACL_VLAN_CMD                = 0x19,
+	FW_PORT_CMD                    = 0x1b,
+	FW_PORT_STATS_CMD              = 0x1c,
+	FW_PORT_LB_STATS_CMD           = 0x1d,
+	FW_PORT_TRACE_CMD              = 0x1e,
+	FW_PORT_TRACE_MMAP_CMD         = 0x1f,
+	FW_RSS_IND_TBL_CMD             = 0x20,
+	FW_RSS_GLB_CONFIG_CMD          = 0x22,
+	FW_RSS_VI_CONFIG_CMD           = 0x23,
+	FW_SCHED_CMD                   = 0x24,
+	FW_DEVLOG_CMD                  = 0x25,
+	FW_WATCHDOG_CMD                = 0x27,
+	FW_CLIP_CMD                    = 0x28,
+	FW_CHNET_IFACE_CMD             = 0x26,
+	FW_FCOE_RES_INFO_CMD           = 0x31,
+	FW_FCOE_LINK_CMD               = 0x32,
+	FW_FCOE_VNP_CMD                = 0x33,
+	FW_FCOE_SPARAMS_CMD            = 0x35,
+	FW_FCOE_STATS_CMD              = 0x37,
+	FW_FCOE_FCF_CMD                = 0x38,
+	FW_LASTC2E_CMD                 = 0x40,
+	FW_ERROR_CMD                   = 0x80,
+	FW_DEBUG_CMD                   = 0x81,
+};
+
+enum fw_cmd_cap {
+	FW_CMD_CAP_PF                  = 0x01,
+	FW_CMD_CAP_DMAQ                = 0x02,
+	FW_CMD_CAP_PORT                = 0x04,
+	FW_CMD_CAP_PORTPROMISC         = 0x08,
+	FW_CMD_CAP_PORTSTATS           = 0x10,
+	FW_CMD_CAP_VF                  = 0x80,
+};
+
+/*
+ * Generic command header flit0
+ */
+struct fw_cmd_hdr {
+	__be32 hi;
+	__be32 lo;
+};
+
+#define S_FW_CMD_OP		24
+#define M_FW_CMD_OP		0xff
+#define V_FW_CMD_OP(x)		((x) << S_FW_CMD_OP)
+#define G_FW_CMD_OP(x)		(((x) >> S_FW_CMD_OP) & M_FW_CMD_OP)
+
+#define S_FW_CMD_REQUEST	23
+#define M_FW_CMD_REQUEST	0x1
+#define V_FW_CMD_REQUEST(x)	((x) << S_FW_CMD_REQUEST)
+#define G_FW_CMD_REQUEST(x)	(((x) >> S_FW_CMD_REQUEST) & M_FW_CMD_REQUEST)
+#define F_FW_CMD_REQUEST	V_FW_CMD_REQUEST(1U)
+
+#define S_FW_CMD_READ		22
+#define M_FW_CMD_READ		0x1
+#define V_FW_CMD_READ(x)	((x) << S_FW_CMD_READ)
+#define G_FW_CMD_READ(x)	(((x) >> S_FW_CMD_READ) & M_FW_CMD_READ)
+#define F_FW_CMD_READ		V_FW_CMD_READ(1U)
+
+#define S_FW_CMD_WRITE		21
+#define M_FW_CMD_WRITE		0x1
+#define V_FW_CMD_WRITE(x)	((x) << S_FW_CMD_WRITE)
+#define G_FW_CMD_WRITE(x)	(((x) >> S_FW_CMD_WRITE) & M_FW_CMD_WRITE)
+#define F_FW_CMD_WRITE		V_FW_CMD_WRITE(1U)
+
+#define S_FW_CMD_EXEC		20
+#define M_FW_CMD_EXEC		0x1
+#define V_FW_CMD_EXEC(x)	((x) << S_FW_CMD_EXEC)
+#define G_FW_CMD_EXEC(x)	(((x) >> S_FW_CMD_EXEC) & M_FW_CMD_EXEC)
+#define F_FW_CMD_EXEC		V_FW_CMD_EXEC(1U)
+
+#define S_FW_CMD_RAMASK		20
+#define M_FW_CMD_RAMASK		0xf
+#define V_FW_CMD_RAMASK(x)	((x) << S_FW_CMD_RAMASK)
+#define G_FW_CMD_RAMASK(x)	(((x) >> S_FW_CMD_RAMASK) & M_FW_CMD_RAMASK)
+
+#define S_FW_CMD_RETVAL		8
+#define M_FW_CMD_RETVAL		0xff
+#define V_FW_CMD_RETVAL(x)	((x) << S_FW_CMD_RETVAL)
+#define G_FW_CMD_RETVAL(x)	(((x) >> S_FW_CMD_RETVAL) & M_FW_CMD_RETVAL)
+
+#define S_FW_CMD_LEN16		0
+#define M_FW_CMD_LEN16		0xff
+#define V_FW_CMD_LEN16(x)	((x) << S_FW_CMD_LEN16)
+#define G_FW_CMD_LEN16(x)	(((x) >> S_FW_CMD_LEN16) & M_FW_CMD_LEN16)
+
+#define FW_LEN16(fw_struct) V_FW_CMD_LEN16(sizeof(fw_struct) / 16)
+
+/*
+ *	address spaces
+ */
+enum fw_ldst_addrspc {
+	FW_LDST_ADDRSPC_FIRMWARE  = 0x0001,
+	FW_LDST_ADDRSPC_SGE_EGRC  = 0x0008,
+	FW_LDST_ADDRSPC_SGE_INGC  = 0x0009,
+	FW_LDST_ADDRSPC_SGE_FLMC  = 0x000a,
+	FW_LDST_ADDRSPC_SGE_CONMC = 0x000b,
+	FW_LDST_ADDRSPC_TP_PIO    = 0x0010,
+	FW_LDST_ADDRSPC_TP_TM_PIO = 0x0011,
+	FW_LDST_ADDRSPC_TP_MIB    = 0x0012,
+	FW_LDST_ADDRSPC_MDIO      = 0x0018,
+	FW_LDST_ADDRSPC_MPS       = 0x0020,
+	FW_LDST_ADDRSPC_FUNC      = 0x0028,
+	FW_LDST_ADDRSPC_FUNC_PCIE = 0x0029,
+	FW_LDST_ADDRSPC_FUNC_I2C  = 0x002A,
+	FW_LDST_ADDRSPC_LE	  = 0x0030,
+};
+
+/*
+ *	MDIO VSC8634 register access control field
+ */
+enum fw_ldst_mdio_vsc8634_aid {
+	FW_LDST_MDIO_VS_STANDARD,
+	FW_LDST_MDIO_VS_EXTENDED,
+	FW_LDST_MDIO_VS_GPIO
+};
+
+enum fw_ldst_mps_fid {
+	FW_LDST_MPS_ATRB,
+	FW_LDST_MPS_RPLC
+};
+
+enum fw_ldst_func_access_ctl {
+	FW_LDST_FUNC_ACC_CTL_VIID,
+	FW_LDST_FUNC_ACC_CTL_FID
+};
+
+enum fw_ldst_func_mod_index {
+	FW_LDST_FUNC_MPS
+};
+
+struct fw_ldst_cmd {
+	__be32 op_to_addrspace;
+	__be32 cycles_to_len16;
+	union fw_ldst {
+		struct fw_ldst_addrval {
+			__be32 addr;
+			__be32 val;
+		} addrval;
+		struct fw_ldst_idctxt {
+			__be32 physid;
+			__be32 msg_ctxtflush;
+			__be32 ctxt_data7;
+			__be32 ctxt_data6;
+			__be32 ctxt_data5;
+			__be32 ctxt_data4;
+			__be32 ctxt_data3;
+			__be32 ctxt_data2;
+			__be32 ctxt_data1;
+			__be32 ctxt_data0;
+		} idctxt;
+		struct fw_ldst_mdio {
+			__be16 paddr_mmd;
+			__be16 raddr;
+			__be16 vctl;
+			__be16 rval;
+		} mdio;
+		struct fw_ldst_mps {
+			__be16 fid_ctl;
+			__be16 rplcpf_pkd;
+			__be32 rplc127_96;
+			__be32 rplc95_64;
+			__be32 rplc63_32;
+			__be32 rplc31_0;
+			__be32 atrb;
+			__be16 vlan[16];
+		} mps;
+		struct fw_ldst_func {
+			__u8   access_ctl;
+			__u8   mod_index;
+			__be16 ctl_id;
+			__be32 offset;
+			__be64 data0;
+			__be64 data1;
+		} func;
+		struct fw_ldst_pcie {
+			__u8   ctrl_to_fn;
+			__u8   bnum;
+			__u8   r;
+			__u8   ext_r;
+			__u8   select_naccess;
+			__u8   pcie_fn;
+			__be16 nset_pkd;
+			__be32 data[12];
+		} pcie;
+		struct fw_ldst_i2c {
+			__u8   pid_pkd;
+			__u8   base;
+			__u8   boffset;
+			__u8   data;
+			__be32 r9;
+		} i2c;
+		struct fw_ldst_le {
+			__be16	region;
+			__be16	nval;
+			__u32	val[12];
+		} le;
+	} u;
+};
+
+#define S_FW_LDST_CMD_ADDRSPACE		0
+#define M_FW_LDST_CMD_ADDRSPACE		0xff
+#define V_FW_LDST_CMD_ADDRSPACE(x)	((x) << S_FW_LDST_CMD_ADDRSPACE)
+#define G_FW_LDST_CMD_ADDRSPACE(x)	\
+    (((x) >> S_FW_LDST_CMD_ADDRSPACE) & M_FW_LDST_CMD_ADDRSPACE)
+
+#define S_FW_LDST_CMD_CYCLES	16
+#define M_FW_LDST_CMD_CYCLES	0xffff
+#define V_FW_LDST_CMD_CYCLES(x)	((x) << S_FW_LDST_CMD_CYCLES)
+#define G_FW_LDST_CMD_CYCLES(x)	\
+    (((x) >> S_FW_LDST_CMD_CYCLES) & M_FW_LDST_CMD_CYCLES)
+
+#define S_FW_LDST_CMD_MSG	31
+#define M_FW_LDST_CMD_MSG	0x1
+#define V_FW_LDST_CMD_MSG(x)	((x) << S_FW_LDST_CMD_MSG)
+#define G_FW_LDST_CMD_MSG(x)	\
+    (((x) >> S_FW_LDST_CMD_MSG) & M_FW_LDST_CMD_MSG)
+#define F_FW_LDST_CMD_MSG	V_FW_LDST_CMD_MSG(1U)
+
+#define S_FW_LDST_CMD_CTXTFLUSH		30
+#define M_FW_LDST_CMD_CTXTFLUSH		0x1
+#define V_FW_LDST_CMD_CTXTFLUSH(x)	((x) << S_FW_LDST_CMD_CTXTFLUSH)
+#define G_FW_LDST_CMD_CTXTFLUSH(x)	\
+    (((x) >> S_FW_LDST_CMD_CTXTFLUSH) & M_FW_LDST_CMD_CTXTFLUSH)
+#define F_FW_LDST_CMD_CTXTFLUSH	V_FW_LDST_CMD_CTXTFLUSH(1U)
+
+#define S_FW_LDST_CMD_PADDR	8
+#define M_FW_LDST_CMD_PADDR	0x1f
+#define V_FW_LDST_CMD_PADDR(x)	((x) << S_FW_LDST_CMD_PADDR)
+#define G_FW_LDST_CMD_PADDR(x)	\
+    (((x) >> S_FW_LDST_CMD_PADDR) & M_FW_LDST_CMD_PADDR)
+
+#define S_FW_LDST_CMD_MMD	0
+#define M_FW_LDST_CMD_MMD	0x1f
+#define V_FW_LDST_CMD_MMD(x)	((x) << S_FW_LDST_CMD_MMD)
+#define G_FW_LDST_CMD_MMD(x)	\
+    (((x) >> S_FW_LDST_CMD_MMD) & M_FW_LDST_CMD_MMD)
+
+#define S_FW_LDST_CMD_FID	15
+#define M_FW_LDST_CMD_FID	0x1
+#define V_FW_LDST_CMD_FID(x)	((x) << S_FW_LDST_CMD_FID)
+#define G_FW_LDST_CMD_FID(x)	\
+    (((x) >> S_FW_LDST_CMD_FID) & M_FW_LDST_CMD_FID)
+#define F_FW_LDST_CMD_FID	V_FW_LDST_CMD_FID(1U)
+
+#define S_FW_LDST_CMD_CTL	0
+#define M_FW_LDST_CMD_CTL	0x7fff
+#define V_FW_LDST_CMD_CTL(x)	((x) << S_FW_LDST_CMD_CTL)
+#define G_FW_LDST_CMD_CTL(x)	\
+    (((x) >> S_FW_LDST_CMD_CTL) & M_FW_LDST_CMD_CTL)
+
+#define S_FW_LDST_CMD_RPLCPF	0
+#define M_FW_LDST_CMD_RPLCPF	0xff
+#define V_FW_LDST_CMD_RPLCPF(x)	((x) << S_FW_LDST_CMD_RPLCPF)
+#define G_FW_LDST_CMD_RPLCPF(x)	\
+    (((x) >> S_FW_LDST_CMD_RPLCPF) & M_FW_LDST_CMD_RPLCPF)
+
+#define S_FW_LDST_CMD_CTRL	7
+#define M_FW_LDST_CMD_CTRL	0x1
+#define V_FW_LDST_CMD_CTRL(x)	((x) << S_FW_LDST_CMD_CTRL)
+#define G_FW_LDST_CMD_CTRL(x)	\
+    (((x) >> S_FW_LDST_CMD_CTRL) & M_FW_LDST_CMD_CTRL)
+#define F_FW_LDST_CMD_CTRL	V_FW_LDST_CMD_CTRL(1U)
+
+#define S_FW_LDST_CMD_LC	4
+#define M_FW_LDST_CMD_LC	0x1
+#define V_FW_LDST_CMD_LC(x)	((x) << S_FW_LDST_CMD_LC)
+#define G_FW_LDST_CMD_LC(x)	(((x) >> S_FW_LDST_CMD_LC) & M_FW_LDST_CMD_LC)
+#define F_FW_LDST_CMD_LC	V_FW_LDST_CMD_LC(1U)
+
+#define S_FW_LDST_CMD_AI	3
+#define M_FW_LDST_CMD_AI	0x1
+#define V_FW_LDST_CMD_AI(x)	((x) << S_FW_LDST_CMD_AI)
+#define G_FW_LDST_CMD_AI(x)	(((x) >> S_FW_LDST_CMD_AI) & M_FW_LDST_CMD_AI)
+#define F_FW_LDST_CMD_AI	V_FW_LDST_CMD_AI(1U)
+
+#define S_FW_LDST_CMD_FN	0
+#define M_FW_LDST_CMD_FN	0x7
+#define V_FW_LDST_CMD_FN(x)	((x) << S_FW_LDST_CMD_FN)
+#define G_FW_LDST_CMD_FN(x)	(((x) >> S_FW_LDST_CMD_FN) & M_FW_LDST_CMD_FN)
+
+#define S_FW_LDST_CMD_SELECT	4
+#define M_FW_LDST_CMD_SELECT	0xf
+#define V_FW_LDST_CMD_SELECT(x)	((x) << S_FW_LDST_CMD_SELECT)
+#define G_FW_LDST_CMD_SELECT(x)	\
+    (((x) >> S_FW_LDST_CMD_SELECT) & M_FW_LDST_CMD_SELECT)
+
+#define S_FW_LDST_CMD_NACCESS		0
+#define M_FW_LDST_CMD_NACCESS		0xf
+#define V_FW_LDST_CMD_NACCESS(x)	((x) << S_FW_LDST_CMD_NACCESS)
+#define G_FW_LDST_CMD_NACCESS(x)	\
+    (((x) >> S_FW_LDST_CMD_NACCESS) & M_FW_LDST_CMD_NACCESS)
+
+#define S_FW_LDST_CMD_NSET	14
+#define M_FW_LDST_CMD_NSET	0x3
+#define V_FW_LDST_CMD_NSET(x)	((x) << S_FW_LDST_CMD_NSET)
+#define G_FW_LDST_CMD_NSET(x)	\
+    (((x) >> S_FW_LDST_CMD_NSET) & M_FW_LDST_CMD_NSET)
+
+#define S_FW_LDST_CMD_PID	6
+#define M_FW_LDST_CMD_PID	0x3
+#define V_FW_LDST_CMD_PID(x)	((x) << S_FW_LDST_CMD_PID)
+#define G_FW_LDST_CMD_PID(x)	\
+    (((x) >> S_FW_LDST_CMD_PID) & M_FW_LDST_CMD_PID)
+
+struct fw_reset_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__be32 val;
+	__be32 halt_pkd;
+};
+
+#define S_FW_RESET_CMD_HALT	31
+#define M_FW_RESET_CMD_HALT	0x1
+#define V_FW_RESET_CMD_HALT(x)	((x) << S_FW_RESET_CMD_HALT)
+#define G_FW_RESET_CMD_HALT(x)	\
+    (((x) >> S_FW_RESET_CMD_HALT) & M_FW_RESET_CMD_HALT)
+#define F_FW_RESET_CMD_HALT	V_FW_RESET_CMD_HALT(1U)
+
+enum {
+	FW_HELLO_CMD_STAGE_OS		= 0,
+	FW_HELLO_CMD_STAGE_PREOS0	= 1,
+	FW_HELLO_CMD_STAGE_PREOS1	= 2,
+	FW_HELLO_CMD_STAGE_POSTOS	= 3,
+};
+
+struct fw_hello_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__be32 err_to_clearinit;
+	__be32 fwrev;
+};
+
+#define S_FW_HELLO_CMD_ERR	31
+#define M_FW_HELLO_CMD_ERR	0x1
+#define V_FW_HELLO_CMD_ERR(x)	((x) << S_FW_HELLO_CMD_ERR)
+#define G_FW_HELLO_CMD_ERR(x)	\
+    (((x) >> S_FW_HELLO_CMD_ERR) & M_FW_HELLO_CMD_ERR)
+#define F_FW_HELLO_CMD_ERR	V_FW_HELLO_CMD_ERR(1U)
+
+#define S_FW_HELLO_CMD_INIT	30
+#define M_FW_HELLO_CMD_INIT	0x1
+#define V_FW_HELLO_CMD_INIT(x)	((x) << S_FW_HELLO_CMD_INIT)
+#define G_FW_HELLO_CMD_INIT(x)	\
+    (((x) >> S_FW_HELLO_CMD_INIT) & M_FW_HELLO_CMD_INIT)
+#define F_FW_HELLO_CMD_INIT	V_FW_HELLO_CMD_INIT(1U)
+
+#define S_FW_HELLO_CMD_MASTERDIS	29
+#define M_FW_HELLO_CMD_MASTERDIS	0x1
+#define V_FW_HELLO_CMD_MASTERDIS(x)	((x) << S_FW_HELLO_CMD_MASTERDIS)
+#define G_FW_HELLO_CMD_MASTERDIS(x)	\
+    (((x) >> S_FW_HELLO_CMD_MASTERDIS) & M_FW_HELLO_CMD_MASTERDIS)
+#define F_FW_HELLO_CMD_MASTERDIS	V_FW_HELLO_CMD_MASTERDIS(1U)
+
+#define S_FW_HELLO_CMD_MASTERFORCE	28
+#define M_FW_HELLO_CMD_MASTERFORCE	0x1
+#define V_FW_HELLO_CMD_MASTERFORCE(x)	((x) << S_FW_HELLO_CMD_MASTERFORCE)
+#define G_FW_HELLO_CMD_MASTERFORCE(x)	\
+    (((x) >> S_FW_HELLO_CMD_MASTERFORCE) & M_FW_HELLO_CMD_MASTERFORCE)
+#define F_FW_HELLO_CMD_MASTERFORCE	V_FW_HELLO_CMD_MASTERFORCE(1U)
+
+#define S_FW_HELLO_CMD_MBMASTER		24
+#define M_FW_HELLO_CMD_MBMASTER		0xf
+#define V_FW_HELLO_CMD_MBMASTER(x)	((x) << S_FW_HELLO_CMD_MBMASTER)
+#define G_FW_HELLO_CMD_MBMASTER(x)	\
+    (((x) >> S_FW_HELLO_CMD_MBMASTER) & M_FW_HELLO_CMD_MBMASTER)
+
+#define S_FW_HELLO_CMD_MBASYNCNOTINT	23
+#define M_FW_HELLO_CMD_MBASYNCNOTINT	0x1
+#define V_FW_HELLO_CMD_MBASYNCNOTINT(x)	((x) << S_FW_HELLO_CMD_MBASYNCNOTINT)
+#define G_FW_HELLO_CMD_MBASYNCNOTINT(x)	\
+    (((x) >> S_FW_HELLO_CMD_MBASYNCNOTINT) & M_FW_HELLO_CMD_MBASYNCNOTINT)
+#define F_FW_HELLO_CMD_MBASYNCNOTINT	V_FW_HELLO_CMD_MBASYNCNOTINT(1U)
+
+#define S_FW_HELLO_CMD_MBASYNCNOT	20
+#define M_FW_HELLO_CMD_MBASYNCNOT	0x7
+#define V_FW_HELLO_CMD_MBASYNCNOT(x)	((x) << S_FW_HELLO_CMD_MBASYNCNOT)
+#define G_FW_HELLO_CMD_MBASYNCNOT(x)	\
+    (((x) >> S_FW_HELLO_CMD_MBASYNCNOT) & M_FW_HELLO_CMD_MBASYNCNOT)
+
+#define S_FW_HELLO_CMD_STAGE	17
+#define M_FW_HELLO_CMD_STAGE	0x7
+#define V_FW_HELLO_CMD_STAGE(x)	((x) << S_FW_HELLO_CMD_STAGE)
+#define G_FW_HELLO_CMD_STAGE(x)	\
+    (((x) >> S_FW_HELLO_CMD_STAGE) & M_FW_HELLO_CMD_STAGE)
+
+#define S_FW_HELLO_CMD_CLEARINIT	16
+#define M_FW_HELLO_CMD_CLEARINIT	0x1
+#define V_FW_HELLO_CMD_CLEARINIT(x)	((x) << S_FW_HELLO_CMD_CLEARINIT)
+#define G_FW_HELLO_CMD_CLEARINIT(x)	\
+    (((x) >> S_FW_HELLO_CMD_CLEARINIT) & M_FW_HELLO_CMD_CLEARINIT)
+#define F_FW_HELLO_CMD_CLEARINIT	V_FW_HELLO_CMD_CLEARINIT(1U)
+
+struct fw_bye_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__be64 r3;
+};
+
+struct fw_initialize_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__be64 r3;
+};
+
+enum fw_caps_config_hm {
+	FW_CAPS_CONFIG_HM_PCIE		= 0x00000001,
+	FW_CAPS_CONFIG_HM_PL		= 0x00000002,
+	FW_CAPS_CONFIG_HM_SGE		= 0x00000004,
+	FW_CAPS_CONFIG_HM_CIM		= 0x00000008,
+	FW_CAPS_CONFIG_HM_ULPTX		= 0x00000010,
+	FW_CAPS_CONFIG_HM_TP		= 0x00000020,
+	FW_CAPS_CONFIG_HM_ULPRX		= 0x00000040,
+	FW_CAPS_CONFIG_HM_PMRX		= 0x00000080,
+	FW_CAPS_CONFIG_HM_PMTX		= 0x00000100,
+	FW_CAPS_CONFIG_HM_MC		= 0x00000200,
+	FW_CAPS_CONFIG_HM_LE		= 0x00000400,
+	FW_CAPS_CONFIG_HM_MPS		= 0x00000800,
+	FW_CAPS_CONFIG_HM_XGMAC		= 0x00001000,
+	FW_CAPS_CONFIG_HM_CPLSWITCH	= 0x00002000,
+	FW_CAPS_CONFIG_HM_T4DBG		= 0x00004000,
+	FW_CAPS_CONFIG_HM_MI		= 0x00008000,
+	FW_CAPS_CONFIG_HM_I2CM		= 0x00010000,
+	FW_CAPS_CONFIG_HM_NCSI		= 0x00020000,
+	FW_CAPS_CONFIG_HM_SMB		= 0x00040000,
+	FW_CAPS_CONFIG_HM_MA		= 0x00080000,
+	FW_CAPS_CONFIG_HM_EDRAM		= 0x00100000,
+	FW_CAPS_CONFIG_HM_PMU		= 0x00200000,
+	FW_CAPS_CONFIG_HM_UART		= 0x00400000,
+	FW_CAPS_CONFIG_HM_SF		= 0x00800000,
+};
+
+/*
+ * The VF Register Map.
+ *
+ * The Scatter Gather Engine (SGE), Multiport Support module (MPS), PIO Local
+ * bus module (PL) and CPU Interface Module (CIM) components are mapped via
+ * the Slice to Module Map Table (see below) in the Physical Function Register
+ * Map.  The Mail Box Data (MBDATA) range is mapped via the PCI-E Mailbox Base
+ * and Offset registers in the PF Register Map.  The MBDATA base address is
+ * quite constrained as it determines the Mailbox Data addresses for both PFs
+ * and VFs, and therefore must fit in both the VF and PF Register Maps without
+ * overlapping other registers.
+ */
+#define FW_T4VF_SGE_BASE_ADDR      0x0000
+#define FW_T4VF_MPS_BASE_ADDR      0x0100
+#define FW_T4VF_PL_BASE_ADDR       0x0200
+#define FW_T4VF_MBDATA_BASE_ADDR   0x0240
+#define FW_T4VF_CIM_BASE_ADDR      0x0300
+
+#define FW_T4VF_REGMAP_START       0x0000
+#define FW_T4VF_REGMAP_SIZE        0x0400
+
+enum fw_caps_config_nbm {
+	FW_CAPS_CONFIG_NBM_IPMI		= 0x00000001,
+	FW_CAPS_CONFIG_NBM_NCSI		= 0x00000002,
+};
+
+enum fw_caps_config_link {
+	FW_CAPS_CONFIG_LINK_PPP		= 0x00000001,
+	FW_CAPS_CONFIG_LINK_QFC		= 0x00000002,
+	FW_CAPS_CONFIG_LINK_DCBX	= 0x00000004,
+};
+
+enum fw_caps_config_switch {
+	FW_CAPS_CONFIG_SWITCH_INGRESS	= 0x00000001,
+	FW_CAPS_CONFIG_SWITCH_EGRESS	= 0x00000002,
+};
+
+enum fw_caps_config_nic {
+	FW_CAPS_CONFIG_NIC		= 0x00000001,
+	FW_CAPS_CONFIG_NIC_VM		= 0x00000002,
+	FW_CAPS_CONFIG_NIC_IDS		= 0x00000004,
+	FW_CAPS_CONFIG_NIC_UM		= 0x00000008,
+	FW_CAPS_CONFIG_NIC_UM_ISGL	= 0x00000010,
+};
+
+enum fw_caps_config_toe {
+	FW_CAPS_CONFIG_TOE		= 0x00000001,
+};
+
+enum fw_caps_config_rdma {
+	FW_CAPS_CONFIG_RDMA_RDDP	= 0x00000001,
+	FW_CAPS_CONFIG_RDMA_RDMAC	= 0x00000002,
+};
+
+enum fw_caps_config_iscsi {
+	FW_CAPS_CONFIG_ISCSI_INITIATOR_PDU = 0x00000001,
+	FW_CAPS_CONFIG_ISCSI_TARGET_PDU = 0x00000002,
+	FW_CAPS_CONFIG_ISCSI_INITIATOR_CNXOFLD = 0x00000004,
+	FW_CAPS_CONFIG_ISCSI_TARGET_CNXOFLD = 0x00000008,
+	FW_CAPS_CONFIG_ISCSI_INITIATOR_SSNOFLD = 0x00000010,
+	FW_CAPS_CONFIG_ISCSI_TARGET_SSNOFLD = 0x00000020,
+
+};
+
+enum fw_caps_config_fcoe {
+	FW_CAPS_CONFIG_FCOE_INITIATOR	= 0x00000001,
+	FW_CAPS_CONFIG_FCOE_TARGET	= 0x00000002,
+	FW_CAPS_CONFIG_FCOE_CTRL_OFLD   = 0x00000004,
+};
+
+enum fw_memtype_cf {
+	FW_MEMTYPE_CF_EDC0		= 0x0,
+	FW_MEMTYPE_CF_EDC1		= 0x1,
+	FW_MEMTYPE_CF_EXTMEM		= 0x2,
+	FW_MEMTYPE_CF_FLASH		= 0x4,
+	FW_MEMTYPE_CF_INTERNAL		= 0x5,
+};
+
+struct fw_caps_config_cmd {
+	__be32 op_to_write;
+	__be32 cfvalid_to_len16;
+	__be32 r2;
+	__be32 hwmbitmap;
+	__be16 nbmcaps;
+	__be16 linkcaps;
+	__be16 switchcaps;
+	__be16 r3;
+	__be16 niccaps;
+	__be16 toecaps;
+	__be16 rdmacaps;
+	__be16 r4;
+	__be16 iscsicaps;
+	__be16 fcoecaps;
+	__be32 cfcsum;
+	__be32 finiver;
+	__be32 finicsum;
+};
+
+#define S_FW_CAPS_CONFIG_CMD_CFVALID	27
+#define M_FW_CAPS_CONFIG_CMD_CFVALID	0x1
+#define V_FW_CAPS_CONFIG_CMD_CFVALID(x)	((x) << S_FW_CAPS_CONFIG_CMD_CFVALID)
+#define G_FW_CAPS_CONFIG_CMD_CFVALID(x)	\
+    (((x) >> S_FW_CAPS_CONFIG_CMD_CFVALID) & M_FW_CAPS_CONFIG_CMD_CFVALID)
+#define F_FW_CAPS_CONFIG_CMD_CFVALID	V_FW_CAPS_CONFIG_CMD_CFVALID(1U)
+
+#define S_FW_CAPS_CONFIG_CMD_MEMTYPE_CF		24
+#define M_FW_CAPS_CONFIG_CMD_MEMTYPE_CF		0x7
+#define V_FW_CAPS_CONFIG_CMD_MEMTYPE_CF(x)	\
+    ((x) << S_FW_CAPS_CONFIG_CMD_MEMTYPE_CF)
+#define G_FW_CAPS_CONFIG_CMD_MEMTYPE_CF(x)	\
+    (((x) >> S_FW_CAPS_CONFIG_CMD_MEMTYPE_CF) & \
+     M_FW_CAPS_CONFIG_CMD_MEMTYPE_CF)
+
+#define S_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF	16
+#define M_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF	0xff
+#define V_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF(x)	\
+    ((x) << S_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF)
+#define G_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF(x)	\
+    (((x) >> S_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF) & \
+     M_FW_CAPS_CONFIG_CMD_MEMADDR64K_CF)
+
+/*
+ * params command mnemonics
+ */
+enum fw_params_mnem {
+	FW_PARAMS_MNEM_DEV		= 1,	/* device params */
+	FW_PARAMS_MNEM_PFVF		= 2,	/* function params */
+	FW_PARAMS_MNEM_REG		= 3,	/* limited register access */
+	FW_PARAMS_MNEM_DMAQ		= 4,	/* dma queue params */
+	FW_PARAMS_MNEM_LAST
+};
+
+/*
+ * device parameters
+ */
+enum fw_params_param_dev {
+	FW_PARAMS_PARAM_DEV_CCLK	= 0x00, /* chip core clock in khz */
+	FW_PARAMS_PARAM_DEV_PORTVEC	= 0x01, /* the port vector */
+	FW_PARAMS_PARAM_DEV_NTID	= 0x02, /* reads the number of TIDs
+						 * allocated by the device's
+						 * Lookup Engine
+						 */
+	FW_PARAMS_PARAM_DEV_FLOWC_BUFFIFO_SZ = 0x03,
+	FW_PARAMS_PARAM_DEV_INTFVER_NIC	= 0x04,
+	FW_PARAMS_PARAM_DEV_INTFVER_VNIC = 0x05,
+	FW_PARAMS_PARAM_DEV_INTFVER_OFLD = 0x06,
+	FW_PARAMS_PARAM_DEV_INTFVER_RI	= 0x07,
+	FW_PARAMS_PARAM_DEV_INTFVER_ISCSIPDU = 0x08,
+	FW_PARAMS_PARAM_DEV_INTFVER_ISCSI = 0x09,
+	FW_PARAMS_PARAM_DEV_INTFVER_FCOE = 0x0A,
+	FW_PARAMS_PARAM_DEV_FWREV = 0x0B,
+	FW_PARAMS_PARAM_DEV_TPREV = 0x0C,
+	FW_PARAMS_PARAM_DEV_CF = 0x0D,
+	FW_PARAMS_PARAM_DEV_BYPASS = 0x0E,
+};
+
+/*
+ * physical and virtual function parameters
+ */
+enum fw_params_param_pfvf {
+	FW_PARAMS_PARAM_PFVF_RWXCAPS	= 0x00,
+	FW_PARAMS_PARAM_PFVF_ROUTE_START = 0x01,
+	FW_PARAMS_PARAM_PFVF_ROUTE_END = 0x02,
+	FW_PARAMS_PARAM_PFVF_CLIP_START = 0x03,
+	FW_PARAMS_PARAM_PFVF_CLIP_END = 0x04,
+	FW_PARAMS_PARAM_PFVF_FILTER_START = 0x05,
+	FW_PARAMS_PARAM_PFVF_FILTER_END = 0x06,
+	FW_PARAMS_PARAM_PFVF_SERVER_START = 0x07,
+	FW_PARAMS_PARAM_PFVF_SERVER_END = 0x08,
+	FW_PARAMS_PARAM_PFVF_TDDP_START = 0x09,
+	FW_PARAMS_PARAM_PFVF_TDDP_END = 0x0A,
+	FW_PARAMS_PARAM_PFVF_ISCSI_START = 0x0B,
+	FW_PARAMS_PARAM_PFVF_ISCSI_END = 0x0C,
+	FW_PARAMS_PARAM_PFVF_STAG_START = 0x0D,
+	FW_PARAMS_PARAM_PFVF_STAG_END = 0x0E,
+	FW_PARAMS_PARAM_PFVF_RQ_START = 0x1F,
+	FW_PARAMS_PARAM_PFVF_RQ_END	= 0x10,
+	FW_PARAMS_PARAM_PFVF_PBL_START = 0x11,
+	FW_PARAMS_PARAM_PFVF_PBL_END	= 0x12,
+	FW_PARAMS_PARAM_PFVF_L2T_START = 0x13,
+	FW_PARAMS_PARAM_PFVF_L2T_END = 0x14,
+	FW_PARAMS_PARAM_PFVF_SQRQ_START = 0x15,
+	FW_PARAMS_PARAM_PFVF_SQRQ_END	= 0x16,
+	FW_PARAMS_PARAM_PFVF_CQ_START	= 0x17,
+	FW_PARAMS_PARAM_PFVF_CQ_END	= 0x18,
+	FW_PARAMS_PARAM_PFVF_SCHEDCLASS_ETH = 0x20,
+	FW_PARAMS_PARAM_PFVF_VIID	= 0x24,
+	FW_PARAMS_PARAM_PFVF_CPMASK	= 0x25,
+	FW_PARAMS_PARAM_PFVF_OCQ_START	= 0x26,
+	FW_PARAMS_PARAM_PFVF_OCQ_END	= 0x27,
+	FW_PARAMS_PARAM_PFVF_CONM_MAP   = 0x28,
+	FW_PARAMS_PARAM_PFVF_IQFLINT_START = 0x29,
+	FW_PARAMS_PARAM_PFVF_IQFLINT_END = 0x2A,
+	FW_PARAMS_PARAM_PFVF_EQ_START	= 0x2B,
+	FW_PARAMS_PARAM_PFVF_EQ_END	= 0x2C,
+	FW_PARAMS_PARAM_PFVF_ACTIVE_FILTER_START = 0x2D,
+	FW_PARAMS_PARAM_PFVF_ACTIVE_FILTER_END = 0x2E
+};
+
+/*
+ * dma queue parameters
+ */
+enum fw_params_param_dmaq {
+	FW_PARAMS_PARAM_DMAQ_IQ_DCAEN_DCACPU = 0x00,
+	FW_PARAMS_PARAM_DMAQ_IQ_INTCNTTHRESH = 0x01,
+	FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_MNGT = 0x10,
+	FW_PARAMS_PARAM_DMAQ_EQ_CMPLIQID_CTRL = 0x11,
+	FW_PARAMS_PARAM_DMAQ_EQ_SCHEDCLASS_ETH = 0x12,
+};
+
+/*
+ * dev bypass parameters; actions and modes
+ */
+enum fw_params_param_dev_bypass {
+
+	/* actions
+	 */
+	FW_PARAMS_PARAM_DEV_BYPASS_PFAIL = 0x00,
+	FW_PARAMS_PARAM_DEV_BYPASS_CURRENT = 0x01,
+
+	/* modes
+	 */
+	FW_PARAMS_PARAM_DEV_BYPASS_NORMAL = 0x00,
+	FW_PARAMS_PARAM_DEV_BYPASS_DROP	= 0x1,
+	FW_PARAMS_PARAM_DEV_BYPASS_BYPASS = 0x2,
+};
+
+#define S_FW_PARAMS_MNEM	24
+#define M_FW_PARAMS_MNEM	0xff
+#define V_FW_PARAMS_MNEM(x)	((x) << S_FW_PARAMS_MNEM)
+#define G_FW_PARAMS_MNEM(x)	\
+    (((x) >> S_FW_PARAMS_MNEM) & M_FW_PARAMS_MNEM)
+
+#define S_FW_PARAMS_PARAM_X	16
+#define M_FW_PARAMS_PARAM_X	0xff
+#define V_FW_PARAMS_PARAM_X(x) ((x) << S_FW_PARAMS_PARAM_X)
+#define G_FW_PARAMS_PARAM_X(x) \
+    (((x) >> S_FW_PARAMS_PARAM_X) & M_FW_PARAMS_PARAM_X)
+
+#define S_FW_PARAMS_PARAM_Y	8
+#define M_FW_PARAMS_PARAM_Y	0xff
+#define V_FW_PARAMS_PARAM_Y(x) ((x) << S_FW_PARAMS_PARAM_Y)
+#define G_FW_PARAMS_PARAM_Y(x) \
+    (((x) >> S_FW_PARAMS_PARAM_Y) & M_FW_PARAMS_PARAM_Y)
+
+#define S_FW_PARAMS_PARAM_Z	0
+#define M_FW_PARAMS_PARAM_Z	0xff
+#define V_FW_PARAMS_PARAM_Z(x) ((x) << S_FW_PARAMS_PARAM_Z)
+#define G_FW_PARAMS_PARAM_Z(x) \
+    (((x) >> S_FW_PARAMS_PARAM_Z) & M_FW_PARAMS_PARAM_Z)
+
+#define S_FW_PARAMS_PARAM_XYZ	0
+#define M_FW_PARAMS_PARAM_XYZ	0xffffff
+#define V_FW_PARAMS_PARAM_XYZ(x) ((x) << S_FW_PARAMS_PARAM_XYZ)
+#define G_FW_PARAMS_PARAM_XYZ(x) \
+    (((x) >> S_FW_PARAMS_PARAM_XYZ) & M_FW_PARAMS_PARAM_XYZ)
+
+#define S_FW_PARAMS_PARAM_YZ	0
+#define M_FW_PARAMS_PARAM_YZ	0xffff
+#define V_FW_PARAMS_PARAM_YZ(x) ((x) << S_FW_PARAMS_PARAM_YZ)
+#define G_FW_PARAMS_PARAM_YZ(x) \
+    (((x) >> S_FW_PARAMS_PARAM_YZ) & M_FW_PARAMS_PARAM_YZ)
+
+struct fw_params_cmd {
+	__be32 op_to_vfn;
+	__be32 retval_len16;
+	struct fw_params_param {
+		__be32 mnem;
+		__be32 val;
+	} param[7];
+};
+
+#define S_FW_PARAMS_CMD_PFN	8
+#define M_FW_PARAMS_CMD_PFN	0x7
+#define V_FW_PARAMS_CMD_PFN(x)	((x) << S_FW_PARAMS_CMD_PFN)
+#define G_FW_PARAMS_CMD_PFN(x)	\
+    (((x) >> S_FW_PARAMS_CMD_PFN) & M_FW_PARAMS_CMD_PFN)
+
+#define S_FW_PARAMS_CMD_VFN	0
+#define M_FW_PARAMS_CMD_VFN	0xff
+#define V_FW_PARAMS_CMD_VFN(x)	((x) << S_FW_PARAMS_CMD_VFN)
+#define G_FW_PARAMS_CMD_VFN(x)	\
+    (((x) >> S_FW_PARAMS_CMD_VFN) & M_FW_PARAMS_CMD_VFN)
+
+struct fw_pfvf_cmd {
+	__be32 op_to_vfn;
+	__be32 retval_len16;
+	__be32 niqflint_niq;
+	__be32 type_to_neq;
+	__be32 tc_to_nexactf;
+	__be32 r_caps_to_nethctrl;
+	__be16 nricq;
+	__be16 nriqp;
+	__be32 r4;
+};
+
+#define S_FW_PFVF_CMD_PFN	8
+#define M_FW_PFVF_CMD_PFN	0x7
+#define V_FW_PFVF_CMD_PFN(x)	((x) << S_FW_PFVF_CMD_PFN)
+#define G_FW_PFVF_CMD_PFN(x)	\
+    (((x) >> S_FW_PFVF_CMD_PFN) & M_FW_PFVF_CMD_PFN)
+
+#define S_FW_PFVF_CMD_VFN	0
+#define M_FW_PFVF_CMD_VFN	0xff
+#define V_FW_PFVF_CMD_VFN(x)	((x) << S_FW_PFVF_CMD_VFN)
+#define G_FW_PFVF_CMD_VFN(x)	\
+    (((x) >> S_FW_PFVF_CMD_VFN) & M_FW_PFVF_CMD_VFN)
+
+#define S_FW_PFVF_CMD_NIQFLINT		20
+#define M_FW_PFVF_CMD_NIQFLINT		0xfff
+#define V_FW_PFVF_CMD_NIQFLINT(x)	((x) << S_FW_PFVF_CMD_NIQFLINT)
+#define G_FW_PFVF_CMD_NIQFLINT(x)	\
+    (((x) >> S_FW_PFVF_CMD_NIQFLINT) & M_FW_PFVF_CMD_NIQFLINT)
+
+#define S_FW_PFVF_CMD_NIQ	0
+#define M_FW_PFVF_CMD_NIQ	0xfffff
+#define V_FW_PFVF_CMD_NIQ(x)	((x) << S_FW_PFVF_CMD_NIQ)
+#define G_FW_PFVF_CMD_NIQ(x)	\
+    (((x) >> S_FW_PFVF_CMD_NIQ) & M_FW_PFVF_CMD_NIQ)
+
+#define S_FW_PFVF_CMD_TYPE	31
+#define M_FW_PFVF_CMD_TYPE	0x1
+#define V_FW_PFVF_CMD_TYPE(x)	((x) << S_FW_PFVF_CMD_TYPE)
+#define G_FW_PFVF_CMD_TYPE(x)	\
+    (((x) >> S_FW_PFVF_CMD_TYPE) & M_FW_PFVF_CMD_TYPE)
+#define F_FW_PFVF_CMD_TYPE	V_FW_PFVF_CMD_TYPE(1U)
+
+#define S_FW_PFVF_CMD_CMASK	24
+#define M_FW_PFVF_CMD_CMASK	0xf
+#define V_FW_PFVF_CMD_CMASK(x)	((x) << S_FW_PFVF_CMD_CMASK)
+#define G_FW_PFVF_CMD_CMASK(x)	\
+    (((x) >> S_FW_PFVF_CMD_CMASK) & M_FW_PFVF_CMD_CMASK)
+
+#define S_FW_PFVF_CMD_PMASK	20
+#define M_FW_PFVF_CMD_PMASK	0xf
+#define V_FW_PFVF_CMD_PMASK(x)	((x) << S_FW_PFVF_CMD_PMASK)
+#define G_FW_PFVF_CMD_PMASK(x)	\
+    (((x) >> S_FW_PFVF_CMD_PMASK) & M_FW_PFVF_CMD_PMASK)
+
+#define S_FW_PFVF_CMD_NEQ	0
+#define M_FW_PFVF_CMD_NEQ	0xfffff
+#define V_FW_PFVF_CMD_NEQ(x)	((x) << S_FW_PFVF_CMD_NEQ)
+#define G_FW_PFVF_CMD_NEQ(x)	\
+    (((x) >> S_FW_PFVF_CMD_NEQ) & M_FW_PFVF_CMD_NEQ)
+
+#define S_FW_PFVF_CMD_TC	24
+#define M_FW_PFVF_CMD_TC	0xff
+#define V_FW_PFVF_CMD_TC(x)	((x) << S_FW_PFVF_CMD_TC)
+#define G_FW_PFVF_CMD_TC(x)	(((x) >> S_FW_PFVF_CMD_TC) & M_FW_PFVF_CMD_TC)
+
+#define S_FW_PFVF_CMD_NVI	16
+#define M_FW_PFVF_CMD_NVI	0xff
+#define V_FW_PFVF_CMD_NVI(x)	((x) << S_FW_PFVF_CMD_NVI)
+#define G_FW_PFVF_CMD_NVI(x)	\
+    (((x) >> S_FW_PFVF_CMD_NVI) & M_FW_PFVF_CMD_NVI)
+
+#define S_FW_PFVF_CMD_NEXACTF		0
+#define M_FW_PFVF_CMD_NEXACTF		0xffff
+#define V_FW_PFVF_CMD_NEXACTF(x)	((x) << S_FW_PFVF_CMD_NEXACTF)
+#define G_FW_PFVF_CMD_NEXACTF(x)	\
+    (((x) >> S_FW_PFVF_CMD_NEXACTF) & M_FW_PFVF_CMD_NEXACTF)
+
+#define S_FW_PFVF_CMD_R_CAPS	24
+#define M_FW_PFVF_CMD_R_CAPS	0xff
+#define V_FW_PFVF_CMD_R_CAPS(x)	((x) << S_FW_PFVF_CMD_R_CAPS)
+#define G_FW_PFVF_CMD_R_CAPS(x)	\
+    (((x) >> S_FW_PFVF_CMD_R_CAPS) & M_FW_PFVF_CMD_R_CAPS)
+
+#define S_FW_PFVF_CMD_WX_CAPS		16
+#define M_FW_PFVF_CMD_WX_CAPS		0xff
+#define V_FW_PFVF_CMD_WX_CAPS(x)	((x) << S_FW_PFVF_CMD_WX_CAPS)
+#define G_FW_PFVF_CMD_WX_CAPS(x)	\
+    (((x) >> S_FW_PFVF_CMD_WX_CAPS) & M_FW_PFVF_CMD_WX_CAPS)
+
+#define S_FW_PFVF_CMD_NETHCTRL		0
+#define M_FW_PFVF_CMD_NETHCTRL		0xffff
+#define V_FW_PFVF_CMD_NETHCTRL(x)	((x) << S_FW_PFVF_CMD_NETHCTRL)
+#define G_FW_PFVF_CMD_NETHCTRL(x)	\
+    (((x) >> S_FW_PFVF_CMD_NETHCTRL) & M_FW_PFVF_CMD_NETHCTRL)
+
+/*
+ *	ingress queue type; the first 1K ingress queues can have associated 0,
+ *	1 or 2 free lists and an interrupt, all other ingress queues lack these
+ *	capabilities
+ */
+enum fw_iq_type {
+	FW_IQ_TYPE_FL_INT_CAP,
+	FW_IQ_TYPE_NO_FL_INT_CAP
+};
+
+struct fw_iq_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be16 physiqid;
+	__be16 iqid;
+	__be16 fl0id;
+	__be16 fl1id;
+	__be32 type_to_iqandstindex;
+	__be16 iqdroprss_to_iqesize;
+	__be16 iqsize;
+	__be64 iqaddr;
+	__be32 iqns_to_fl0congen;
+	__be16 fl0dcaen_to_fl0cidxfthresh;
+	__be16 fl0size;
+	__be64 fl0addr;
+	__be32 fl1cngchmap_to_fl1congen;
+	__be16 fl1dcaen_to_fl1cidxfthresh;
+	__be16 fl1size;
+	__be64 fl1addr;
+};
+
+#define S_FW_IQ_CMD_PFN		8
+#define M_FW_IQ_CMD_PFN		0x7
+#define V_FW_IQ_CMD_PFN(x)	((x) << S_FW_IQ_CMD_PFN)
+#define G_FW_IQ_CMD_PFN(x)	(((x) >> S_FW_IQ_CMD_PFN) & M_FW_IQ_CMD_PFN)
+
+#define S_FW_IQ_CMD_VFN		0
+#define M_FW_IQ_CMD_VFN		0xff
+#define V_FW_IQ_CMD_VFN(x)	((x) << S_FW_IQ_CMD_VFN)
+#define G_FW_IQ_CMD_VFN(x)	(((x) >> S_FW_IQ_CMD_VFN) & M_FW_IQ_CMD_VFN)
+
+#define S_FW_IQ_CMD_ALLOC	31
+#define M_FW_IQ_CMD_ALLOC	0x1
+#define V_FW_IQ_CMD_ALLOC(x)	((x) << S_FW_IQ_CMD_ALLOC)
+#define G_FW_IQ_CMD_ALLOC(x)	\
+    (((x) >> S_FW_IQ_CMD_ALLOC) & M_FW_IQ_CMD_ALLOC)
+#define F_FW_IQ_CMD_ALLOC	V_FW_IQ_CMD_ALLOC(1U)
+
+#define S_FW_IQ_CMD_FREE	30
+#define M_FW_IQ_CMD_FREE	0x1
+#define V_FW_IQ_CMD_FREE(x)	((x) << S_FW_IQ_CMD_FREE)
+#define G_FW_IQ_CMD_FREE(x)	(((x) >> S_FW_IQ_CMD_FREE) & M_FW_IQ_CMD_FREE)
+#define F_FW_IQ_CMD_FREE	V_FW_IQ_CMD_FREE(1U)
+
+#define S_FW_IQ_CMD_MODIFY	29
+#define M_FW_IQ_CMD_MODIFY	0x1
+#define V_FW_IQ_CMD_MODIFY(x)	((x) << S_FW_IQ_CMD_MODIFY)
+#define G_FW_IQ_CMD_MODIFY(x)	\
+    (((x) >> S_FW_IQ_CMD_MODIFY) & M_FW_IQ_CMD_MODIFY)
+#define F_FW_IQ_CMD_MODIFY	V_FW_IQ_CMD_MODIFY(1U)
+
+#define S_FW_IQ_CMD_IQSTART	28
+#define M_FW_IQ_CMD_IQSTART	0x1
+#define V_FW_IQ_CMD_IQSTART(x)	((x) << S_FW_IQ_CMD_IQSTART)
+#define G_FW_IQ_CMD_IQSTART(x)	\
+    (((x) >> S_FW_IQ_CMD_IQSTART) & M_FW_IQ_CMD_IQSTART)
+#define F_FW_IQ_CMD_IQSTART	V_FW_IQ_CMD_IQSTART(1U)
+
+#define S_FW_IQ_CMD_IQSTOP	27
+#define M_FW_IQ_CMD_IQSTOP	0x1
+#define V_FW_IQ_CMD_IQSTOP(x)	((x) << S_FW_IQ_CMD_IQSTOP)
+#define G_FW_IQ_CMD_IQSTOP(x)	\
+    (((x) >> S_FW_IQ_CMD_IQSTOP) & M_FW_IQ_CMD_IQSTOP)
+#define F_FW_IQ_CMD_IQSTOP	V_FW_IQ_CMD_IQSTOP(1U)
+
+#define S_FW_IQ_CMD_TYPE	29
+#define M_FW_IQ_CMD_TYPE	0x7
+#define V_FW_IQ_CMD_TYPE(x)	((x) << S_FW_IQ_CMD_TYPE)
+#define G_FW_IQ_CMD_TYPE(x)	(((x) >> S_FW_IQ_CMD_TYPE) & M_FW_IQ_CMD_TYPE)
+
+#define S_FW_IQ_CMD_IQASYNCH	28
+#define M_FW_IQ_CMD_IQASYNCH	0x1
+#define V_FW_IQ_CMD_IQASYNCH(x)	((x) << S_FW_IQ_CMD_IQASYNCH)
+#define G_FW_IQ_CMD_IQASYNCH(x)	\
+    (((x) >> S_FW_IQ_CMD_IQASYNCH) & M_FW_IQ_CMD_IQASYNCH)
+#define F_FW_IQ_CMD_IQASYNCH	V_FW_IQ_CMD_IQASYNCH(1U)
+
+#define S_FW_IQ_CMD_VIID	16
+#define M_FW_IQ_CMD_VIID	0xfff
+#define V_FW_IQ_CMD_VIID(x)	((x) << S_FW_IQ_CMD_VIID)
+#define G_FW_IQ_CMD_VIID(x)	(((x) >> S_FW_IQ_CMD_VIID) & M_FW_IQ_CMD_VIID)
+
+#define S_FW_IQ_CMD_IQANDST	15
+#define M_FW_IQ_CMD_IQANDST	0x1
+#define V_FW_IQ_CMD_IQANDST(x)	((x) << S_FW_IQ_CMD_IQANDST)
+#define G_FW_IQ_CMD_IQANDST(x)	\
+    (((x) >> S_FW_IQ_CMD_IQANDST) & M_FW_IQ_CMD_IQANDST)
+#define F_FW_IQ_CMD_IQANDST	V_FW_IQ_CMD_IQANDST(1U)
+
+#define S_FW_IQ_CMD_IQANUS	14
+#define M_FW_IQ_CMD_IQANUS	0x1
+#define V_FW_IQ_CMD_IQANUS(x)	((x) << S_FW_IQ_CMD_IQANUS)
+#define G_FW_IQ_CMD_IQANUS(x)	\
+    (((x) >> S_FW_IQ_CMD_IQANUS) & M_FW_IQ_CMD_IQANUS)
+#define F_FW_IQ_CMD_IQANUS	V_FW_IQ_CMD_IQANUS(1U)
+
+#define S_FW_IQ_CMD_IQANUD	12
+#define M_FW_IQ_CMD_IQANUD	0x3
+#define V_FW_IQ_CMD_IQANUD(x)	((x) << S_FW_IQ_CMD_IQANUD)
+#define G_FW_IQ_CMD_IQANUD(x)	\
+    (((x) >> S_FW_IQ_CMD_IQANUD) & M_FW_IQ_CMD_IQANUD)
+
+#define S_FW_IQ_CMD_IQANDSTINDEX	0
+#define M_FW_IQ_CMD_IQANDSTINDEX	0xfff
+#define V_FW_IQ_CMD_IQANDSTINDEX(x)	((x) << S_FW_IQ_CMD_IQANDSTINDEX)
+#define G_FW_IQ_CMD_IQANDSTINDEX(x)	\
+    (((x) >> S_FW_IQ_CMD_IQANDSTINDEX) & M_FW_IQ_CMD_IQANDSTINDEX)
+
+#define S_FW_IQ_CMD_IQDROPRSS		15
+#define M_FW_IQ_CMD_IQDROPRSS		0x1
+#define V_FW_IQ_CMD_IQDROPRSS(x)	((x) << S_FW_IQ_CMD_IQDROPRSS)
+#define G_FW_IQ_CMD_IQDROPRSS(x)	\
+    (((x) >> S_FW_IQ_CMD_IQDROPRSS) & M_FW_IQ_CMD_IQDROPRSS)
+#define F_FW_IQ_CMD_IQDROPRSS	V_FW_IQ_CMD_IQDROPRSS(1U)
+
+#define S_FW_IQ_CMD_IQGTSMODE		14
+#define M_FW_IQ_CMD_IQGTSMODE		0x1
+#define V_FW_IQ_CMD_IQGTSMODE(x)	((x) << S_FW_IQ_CMD_IQGTSMODE)
+#define G_FW_IQ_CMD_IQGTSMODE(x)	\
+    (((x) >> S_FW_IQ_CMD_IQGTSMODE) & M_FW_IQ_CMD_IQGTSMODE)
+#define F_FW_IQ_CMD_IQGTSMODE	V_FW_IQ_CMD_IQGTSMODE(1U)
+
+#define S_FW_IQ_CMD_IQPCIECH	12
+#define M_FW_IQ_CMD_IQPCIECH	0x3
+#define V_FW_IQ_CMD_IQPCIECH(x)	((x) << S_FW_IQ_CMD_IQPCIECH)
+#define G_FW_IQ_CMD_IQPCIECH(x)	\
+    (((x) >> S_FW_IQ_CMD_IQPCIECH) & M_FW_IQ_CMD_IQPCIECH)
+
+#define S_FW_IQ_CMD_IQDCAEN	11
+#define M_FW_IQ_CMD_IQDCAEN	0x1
+#define V_FW_IQ_CMD_IQDCAEN(x)	((x) << S_FW_IQ_CMD_IQDCAEN)
+#define G_FW_IQ_CMD_IQDCAEN(x)	\
+    (((x) >> S_FW_IQ_CMD_IQDCAEN) & M_FW_IQ_CMD_IQDCAEN)
+#define F_FW_IQ_CMD_IQDCAEN	V_FW_IQ_CMD_IQDCAEN(1U)
+
+#define S_FW_IQ_CMD_IQDCACPU	6
+#define M_FW_IQ_CMD_IQDCACPU	0x1f
+#define V_FW_IQ_CMD_IQDCACPU(x)	((x) << S_FW_IQ_CMD_IQDCACPU)
+#define G_FW_IQ_CMD_IQDCACPU(x)	\
+    (((x) >> S_FW_IQ_CMD_IQDCACPU) & M_FW_IQ_CMD_IQDCACPU)
+
+#define S_FW_IQ_CMD_IQINTCNTTHRESH	4
+#define M_FW_IQ_CMD_IQINTCNTTHRESH	0x3
+#define V_FW_IQ_CMD_IQINTCNTTHRESH(x)	((x) << S_FW_IQ_CMD_IQINTCNTTHRESH)
+#define G_FW_IQ_CMD_IQINTCNTTHRESH(x)	\
+    (((x) >> S_FW_IQ_CMD_IQINTCNTTHRESH) & M_FW_IQ_CMD_IQINTCNTTHRESH)
+
+#define S_FW_IQ_CMD_IQO		3
+#define M_FW_IQ_CMD_IQO		0x1
+#define V_FW_IQ_CMD_IQO(x)	((x) << S_FW_IQ_CMD_IQO)
+#define G_FW_IQ_CMD_IQO(x)	(((x) >> S_FW_IQ_CMD_IQO) & M_FW_IQ_CMD_IQO)
+#define F_FW_IQ_CMD_IQO	V_FW_IQ_CMD_IQO(1U)
+
+#define S_FW_IQ_CMD_IQCPRIO	2
+#define M_FW_IQ_CMD_IQCPRIO	0x1
+#define V_FW_IQ_CMD_IQCPRIO(x)	((x) << S_FW_IQ_CMD_IQCPRIO)
+#define G_FW_IQ_CMD_IQCPRIO(x)	\
+    (((x) >> S_FW_IQ_CMD_IQCPRIO) & M_FW_IQ_CMD_IQCPRIO)
+#define F_FW_IQ_CMD_IQCPRIO	V_FW_IQ_CMD_IQCPRIO(1U)
+
+#define S_FW_IQ_CMD_IQESIZE	0
+#define M_FW_IQ_CMD_IQESIZE	0x3
+#define V_FW_IQ_CMD_IQESIZE(x)	((x) << S_FW_IQ_CMD_IQESIZE)
+#define G_FW_IQ_CMD_IQESIZE(x)	\
+    (((x) >> S_FW_IQ_CMD_IQESIZE) & M_FW_IQ_CMD_IQESIZE)
+
+#define S_FW_IQ_CMD_IQNS	31
+#define M_FW_IQ_CMD_IQNS	0x1
+#define V_FW_IQ_CMD_IQNS(x)	((x) << S_FW_IQ_CMD_IQNS)
+#define G_FW_IQ_CMD_IQNS(x)	(((x) >> S_FW_IQ_CMD_IQNS) & M_FW_IQ_CMD_IQNS)
+#define F_FW_IQ_CMD_IQNS	V_FW_IQ_CMD_IQNS(1U)
+
+#define S_FW_IQ_CMD_IQRO	30
+#define M_FW_IQ_CMD_IQRO	0x1
+#define V_FW_IQ_CMD_IQRO(x)	((x) << S_FW_IQ_CMD_IQRO)
+#define G_FW_IQ_CMD_IQRO(x)	(((x) >> S_FW_IQ_CMD_IQRO) & M_FW_IQ_CMD_IQRO)
+#define F_FW_IQ_CMD_IQRO	V_FW_IQ_CMD_IQRO(1U)
+
+#define S_FW_IQ_CMD_IQFLINTIQHSEN	28
+#define M_FW_IQ_CMD_IQFLINTIQHSEN	0x3
+#define V_FW_IQ_CMD_IQFLINTIQHSEN(x)	((x) << S_FW_IQ_CMD_IQFLINTIQHSEN)
+#define G_FW_IQ_CMD_IQFLINTIQHSEN(x)	\
+    (((x) >> S_FW_IQ_CMD_IQFLINTIQHSEN) & M_FW_IQ_CMD_IQFLINTIQHSEN)
+
+#define S_FW_IQ_CMD_IQFLINTCONGEN	27
+#define M_FW_IQ_CMD_IQFLINTCONGEN	0x1
+#define V_FW_IQ_CMD_IQFLINTCONGEN(x)	((x) << S_FW_IQ_CMD_IQFLINTCONGEN)
+#define G_FW_IQ_CMD_IQFLINTCONGEN(x)	\
+    (((x) >> S_FW_IQ_CMD_IQFLINTCONGEN) & M_FW_IQ_CMD_IQFLINTCONGEN)
+#define F_FW_IQ_CMD_IQFLINTCONGEN	V_FW_IQ_CMD_IQFLINTCONGEN(1U)
+
+#define S_FW_IQ_CMD_IQFLINTISCSIC	26
+#define M_FW_IQ_CMD_IQFLINTISCSIC	0x1
+#define V_FW_IQ_CMD_IQFLINTISCSIC(x)	((x) << S_FW_IQ_CMD_IQFLINTISCSIC)
+#define G_FW_IQ_CMD_IQFLINTISCSIC(x)	\
+    (((x) >> S_FW_IQ_CMD_IQFLINTISCSIC) & M_FW_IQ_CMD_IQFLINTISCSIC)
+#define F_FW_IQ_CMD_IQFLINTISCSIC	V_FW_IQ_CMD_IQFLINTISCSIC(1U)
+
+#define S_FW_IQ_CMD_FL0CNGCHMAP		20
+#define M_FW_IQ_CMD_FL0CNGCHMAP		0xf
+#define V_FW_IQ_CMD_FL0CNGCHMAP(x)	((x) << S_FW_IQ_CMD_FL0CNGCHMAP)
+#define G_FW_IQ_CMD_FL0CNGCHMAP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CNGCHMAP) & M_FW_IQ_CMD_FL0CNGCHMAP)
+
+#define S_FW_IQ_CMD_FL0CACHELOCK	15
+#define M_FW_IQ_CMD_FL0CACHELOCK	0x1
+#define V_FW_IQ_CMD_FL0CACHELOCK(x)	((x) << S_FW_IQ_CMD_FL0CACHELOCK)
+#define G_FW_IQ_CMD_FL0CACHELOCK(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CACHELOCK) & M_FW_IQ_CMD_FL0CACHELOCK)
+#define F_FW_IQ_CMD_FL0CACHELOCK	V_FW_IQ_CMD_FL0CACHELOCK(1U)
+
+#define S_FW_IQ_CMD_FL0DBP	14
+#define M_FW_IQ_CMD_FL0DBP	0x1
+#define V_FW_IQ_CMD_FL0DBP(x)	((x) << S_FW_IQ_CMD_FL0DBP)
+#define G_FW_IQ_CMD_FL0DBP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0DBP) & M_FW_IQ_CMD_FL0DBP)
+#define F_FW_IQ_CMD_FL0DBP	V_FW_IQ_CMD_FL0DBP(1U)
+
+#define S_FW_IQ_CMD_FL0DATANS		13
+#define M_FW_IQ_CMD_FL0DATANS		0x1
+#define V_FW_IQ_CMD_FL0DATANS(x)	((x) << S_FW_IQ_CMD_FL0DATANS)
+#define G_FW_IQ_CMD_FL0DATANS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0DATANS) & M_FW_IQ_CMD_FL0DATANS)
+#define F_FW_IQ_CMD_FL0DATANS	V_FW_IQ_CMD_FL0DATANS(1U)
+
+#define S_FW_IQ_CMD_FL0DATARO		12
+#define M_FW_IQ_CMD_FL0DATARO		0x1
+#define V_FW_IQ_CMD_FL0DATARO(x)	((x) << S_FW_IQ_CMD_FL0DATARO)
+#define G_FW_IQ_CMD_FL0DATARO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0DATARO) & M_FW_IQ_CMD_FL0DATARO)
+#define F_FW_IQ_CMD_FL0DATARO	V_FW_IQ_CMD_FL0DATARO(1U)
+
+#define S_FW_IQ_CMD_FL0CONGCIF		11
+#define M_FW_IQ_CMD_FL0CONGCIF		0x1
+#define V_FW_IQ_CMD_FL0CONGCIF(x)	((x) << S_FW_IQ_CMD_FL0CONGCIF)
+#define G_FW_IQ_CMD_FL0CONGCIF(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CONGCIF) & M_FW_IQ_CMD_FL0CONGCIF)
+#define F_FW_IQ_CMD_FL0CONGCIF	V_FW_IQ_CMD_FL0CONGCIF(1U)
+
+#define S_FW_IQ_CMD_FL0ONCHIP		10
+#define M_FW_IQ_CMD_FL0ONCHIP		0x1
+#define V_FW_IQ_CMD_FL0ONCHIP(x)	((x) << S_FW_IQ_CMD_FL0ONCHIP)
+#define G_FW_IQ_CMD_FL0ONCHIP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0ONCHIP) & M_FW_IQ_CMD_FL0ONCHIP)
+#define F_FW_IQ_CMD_FL0ONCHIP	V_FW_IQ_CMD_FL0ONCHIP(1U)
+
+#define S_FW_IQ_CMD_FL0STATUSPGNS	9
+#define M_FW_IQ_CMD_FL0STATUSPGNS	0x1
+#define V_FW_IQ_CMD_FL0STATUSPGNS(x)	((x) << S_FW_IQ_CMD_FL0STATUSPGNS)
+#define G_FW_IQ_CMD_FL0STATUSPGNS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0STATUSPGNS) & M_FW_IQ_CMD_FL0STATUSPGNS)
+#define F_FW_IQ_CMD_FL0STATUSPGNS	V_FW_IQ_CMD_FL0STATUSPGNS(1U)
+
+#define S_FW_IQ_CMD_FL0STATUSPGRO	8
+#define M_FW_IQ_CMD_FL0STATUSPGRO	0x1
+#define V_FW_IQ_CMD_FL0STATUSPGRO(x)	((x) << S_FW_IQ_CMD_FL0STATUSPGRO)
+#define G_FW_IQ_CMD_FL0STATUSPGRO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0STATUSPGRO) & M_FW_IQ_CMD_FL0STATUSPGRO)
+#define F_FW_IQ_CMD_FL0STATUSPGRO	V_FW_IQ_CMD_FL0STATUSPGRO(1U)
+
+#define S_FW_IQ_CMD_FL0FETCHNS		7
+#define M_FW_IQ_CMD_FL0FETCHNS		0x1
+#define V_FW_IQ_CMD_FL0FETCHNS(x)	((x) << S_FW_IQ_CMD_FL0FETCHNS)
+#define G_FW_IQ_CMD_FL0FETCHNS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0FETCHNS) & M_FW_IQ_CMD_FL0FETCHNS)
+#define F_FW_IQ_CMD_FL0FETCHNS	V_FW_IQ_CMD_FL0FETCHNS(1U)
+
+#define S_FW_IQ_CMD_FL0FETCHRO		6
+#define M_FW_IQ_CMD_FL0FETCHRO		0x1
+#define V_FW_IQ_CMD_FL0FETCHRO(x)	((x) << S_FW_IQ_CMD_FL0FETCHRO)
+#define G_FW_IQ_CMD_FL0FETCHRO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0FETCHRO) & M_FW_IQ_CMD_FL0FETCHRO)
+#define F_FW_IQ_CMD_FL0FETCHRO	V_FW_IQ_CMD_FL0FETCHRO(1U)
+
+#define S_FW_IQ_CMD_FL0HOSTFCMODE	4
+#define M_FW_IQ_CMD_FL0HOSTFCMODE	0x3
+#define V_FW_IQ_CMD_FL0HOSTFCMODE(x)	((x) << S_FW_IQ_CMD_FL0HOSTFCMODE)
+#define G_FW_IQ_CMD_FL0HOSTFCMODE(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0HOSTFCMODE) & M_FW_IQ_CMD_FL0HOSTFCMODE)
+
+#define S_FW_IQ_CMD_FL0CPRIO	3
+#define M_FW_IQ_CMD_FL0CPRIO	0x1
+#define V_FW_IQ_CMD_FL0CPRIO(x)	((x) << S_FW_IQ_CMD_FL0CPRIO)
+#define G_FW_IQ_CMD_FL0CPRIO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CPRIO) & M_FW_IQ_CMD_FL0CPRIO)
+#define F_FW_IQ_CMD_FL0CPRIO	V_FW_IQ_CMD_FL0CPRIO(1U)
+
+#define S_FW_IQ_CMD_FL0PADEN	2
+#define M_FW_IQ_CMD_FL0PADEN	0x1
+#define V_FW_IQ_CMD_FL0PADEN(x)	((x) << S_FW_IQ_CMD_FL0PADEN)
+#define G_FW_IQ_CMD_FL0PADEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0PADEN) & M_FW_IQ_CMD_FL0PADEN)
+#define F_FW_IQ_CMD_FL0PADEN	V_FW_IQ_CMD_FL0PADEN(1U)
+
+#define S_FW_IQ_CMD_FL0PACKEN		1
+#define M_FW_IQ_CMD_FL0PACKEN		0x1
+#define V_FW_IQ_CMD_FL0PACKEN(x)	((x) << S_FW_IQ_CMD_FL0PACKEN)
+#define G_FW_IQ_CMD_FL0PACKEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0PACKEN) & M_FW_IQ_CMD_FL0PACKEN)
+#define F_FW_IQ_CMD_FL0PACKEN	V_FW_IQ_CMD_FL0PACKEN(1U)
+
+#define S_FW_IQ_CMD_FL0CONGEN		0
+#define M_FW_IQ_CMD_FL0CONGEN		0x1
+#define V_FW_IQ_CMD_FL0CONGEN(x)	((x) << S_FW_IQ_CMD_FL0CONGEN)
+#define G_FW_IQ_CMD_FL0CONGEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CONGEN) & M_FW_IQ_CMD_FL0CONGEN)
+#define F_FW_IQ_CMD_FL0CONGEN	V_FW_IQ_CMD_FL0CONGEN(1U)
+
+#define S_FW_IQ_CMD_FL0DCAEN	15
+#define M_FW_IQ_CMD_FL0DCAEN	0x1
+#define V_FW_IQ_CMD_FL0DCAEN(x)	((x) << S_FW_IQ_CMD_FL0DCAEN)
+#define G_FW_IQ_CMD_FL0DCAEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0DCAEN) & M_FW_IQ_CMD_FL0DCAEN)
+#define F_FW_IQ_CMD_FL0DCAEN	V_FW_IQ_CMD_FL0DCAEN(1U)
+
+#define S_FW_IQ_CMD_FL0DCACPU		10
+#define M_FW_IQ_CMD_FL0DCACPU		0x1f
+#define V_FW_IQ_CMD_FL0DCACPU(x)	((x) << S_FW_IQ_CMD_FL0DCACPU)
+#define G_FW_IQ_CMD_FL0DCACPU(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0DCACPU) & M_FW_IQ_CMD_FL0DCACPU)
+
+#define S_FW_IQ_CMD_FL0FBMIN	7
+#define M_FW_IQ_CMD_FL0FBMIN	0x7
+#define V_FW_IQ_CMD_FL0FBMIN(x)	((x) << S_FW_IQ_CMD_FL0FBMIN)
+#define G_FW_IQ_CMD_FL0FBMIN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0FBMIN) & M_FW_IQ_CMD_FL0FBMIN)
+
+#define S_FW_IQ_CMD_FL0FBMAX	4
+#define M_FW_IQ_CMD_FL0FBMAX	0x7
+#define V_FW_IQ_CMD_FL0FBMAX(x)	((x) << S_FW_IQ_CMD_FL0FBMAX)
+#define G_FW_IQ_CMD_FL0FBMAX(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0FBMAX) & M_FW_IQ_CMD_FL0FBMAX)
+
+#define S_FW_IQ_CMD_FL0CIDXFTHRESHO	3
+#define M_FW_IQ_CMD_FL0CIDXFTHRESHO	0x1
+#define V_FW_IQ_CMD_FL0CIDXFTHRESHO(x)	((x) << S_FW_IQ_CMD_FL0CIDXFTHRESHO)
+#define G_FW_IQ_CMD_FL0CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CIDXFTHRESHO) & M_FW_IQ_CMD_FL0CIDXFTHRESHO)
+#define F_FW_IQ_CMD_FL0CIDXFTHRESHO	V_FW_IQ_CMD_FL0CIDXFTHRESHO(1U)
+
+#define S_FW_IQ_CMD_FL0CIDXFTHRESH	0
+#define M_FW_IQ_CMD_FL0CIDXFTHRESH	0x7
+#define V_FW_IQ_CMD_FL0CIDXFTHRESH(x)	((x) << S_FW_IQ_CMD_FL0CIDXFTHRESH)
+#define G_FW_IQ_CMD_FL0CIDXFTHRESH(x)	\
+    (((x) >> S_FW_IQ_CMD_FL0CIDXFTHRESH) & M_FW_IQ_CMD_FL0CIDXFTHRESH)
+
+#define S_FW_IQ_CMD_FL1CNGCHMAP		20
+#define M_FW_IQ_CMD_FL1CNGCHMAP		0xf
+#define V_FW_IQ_CMD_FL1CNGCHMAP(x)	((x) << S_FW_IQ_CMD_FL1CNGCHMAP)
+#define G_FW_IQ_CMD_FL1CNGCHMAP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CNGCHMAP) & M_FW_IQ_CMD_FL1CNGCHMAP)
+
+#define S_FW_IQ_CMD_FL1CACHELOCK	15
+#define M_FW_IQ_CMD_FL1CACHELOCK	0x1
+#define V_FW_IQ_CMD_FL1CACHELOCK(x)	((x) << S_FW_IQ_CMD_FL1CACHELOCK)
+#define G_FW_IQ_CMD_FL1CACHELOCK(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CACHELOCK) & M_FW_IQ_CMD_FL1CACHELOCK)
+#define F_FW_IQ_CMD_FL1CACHELOCK	V_FW_IQ_CMD_FL1CACHELOCK(1U)
+
+#define S_FW_IQ_CMD_FL1DBP	14
+#define M_FW_IQ_CMD_FL1DBP	0x1
+#define V_FW_IQ_CMD_FL1DBP(x)	((x) << S_FW_IQ_CMD_FL1DBP)
+#define G_FW_IQ_CMD_FL1DBP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1DBP) & M_FW_IQ_CMD_FL1DBP)
+#define F_FW_IQ_CMD_FL1DBP	V_FW_IQ_CMD_FL1DBP(1U)
+
+#define S_FW_IQ_CMD_FL1DATANS		13
+#define M_FW_IQ_CMD_FL1DATANS		0x1
+#define V_FW_IQ_CMD_FL1DATANS(x)	((x) << S_FW_IQ_CMD_FL1DATANS)
+#define G_FW_IQ_CMD_FL1DATANS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1DATANS) & M_FW_IQ_CMD_FL1DATANS)
+#define F_FW_IQ_CMD_FL1DATANS	V_FW_IQ_CMD_FL1DATANS(1U)
+
+#define S_FW_IQ_CMD_FL1DATARO		12
+#define M_FW_IQ_CMD_FL1DATARO		0x1
+#define V_FW_IQ_CMD_FL1DATARO(x)	((x) << S_FW_IQ_CMD_FL1DATARO)
+#define G_FW_IQ_CMD_FL1DATARO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1DATARO) & M_FW_IQ_CMD_FL1DATARO)
+#define F_FW_IQ_CMD_FL1DATARO	V_FW_IQ_CMD_FL1DATARO(1U)
+
+#define S_FW_IQ_CMD_FL1CONGCIF		11
+#define M_FW_IQ_CMD_FL1CONGCIF		0x1
+#define V_FW_IQ_CMD_FL1CONGCIF(x)	((x) << S_FW_IQ_CMD_FL1CONGCIF)
+#define G_FW_IQ_CMD_FL1CONGCIF(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CONGCIF) & M_FW_IQ_CMD_FL1CONGCIF)
+#define F_FW_IQ_CMD_FL1CONGCIF	V_FW_IQ_CMD_FL1CONGCIF(1U)
+
+#define S_FW_IQ_CMD_FL1ONCHIP		10
+#define M_FW_IQ_CMD_FL1ONCHIP		0x1
+#define V_FW_IQ_CMD_FL1ONCHIP(x)	((x) << S_FW_IQ_CMD_FL1ONCHIP)
+#define G_FW_IQ_CMD_FL1ONCHIP(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1ONCHIP) & M_FW_IQ_CMD_FL1ONCHIP)
+#define F_FW_IQ_CMD_FL1ONCHIP	V_FW_IQ_CMD_FL1ONCHIP(1U)
+
+#define S_FW_IQ_CMD_FL1STATUSPGNS	9
+#define M_FW_IQ_CMD_FL1STATUSPGNS	0x1
+#define V_FW_IQ_CMD_FL1STATUSPGNS(x)	((x) << S_FW_IQ_CMD_FL1STATUSPGNS)
+#define G_FW_IQ_CMD_FL1STATUSPGNS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1STATUSPGNS) & M_FW_IQ_CMD_FL1STATUSPGNS)
+#define F_FW_IQ_CMD_FL1STATUSPGNS	V_FW_IQ_CMD_FL1STATUSPGNS(1U)
+
+#define S_FW_IQ_CMD_FL1STATUSPGRO	8
+#define M_FW_IQ_CMD_FL1STATUSPGRO	0x1
+#define V_FW_IQ_CMD_FL1STATUSPGRO(x)	((x) << S_FW_IQ_CMD_FL1STATUSPGRO)
+#define G_FW_IQ_CMD_FL1STATUSPGRO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1STATUSPGRO) & M_FW_IQ_CMD_FL1STATUSPGRO)
+#define F_FW_IQ_CMD_FL1STATUSPGRO	V_FW_IQ_CMD_FL1STATUSPGRO(1U)
+
+#define S_FW_IQ_CMD_FL1FETCHNS		7
+#define M_FW_IQ_CMD_FL1FETCHNS		0x1
+#define V_FW_IQ_CMD_FL1FETCHNS(x)	((x) << S_FW_IQ_CMD_FL1FETCHNS)
+#define G_FW_IQ_CMD_FL1FETCHNS(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1FETCHNS) & M_FW_IQ_CMD_FL1FETCHNS)
+#define F_FW_IQ_CMD_FL1FETCHNS	V_FW_IQ_CMD_FL1FETCHNS(1U)
+
+#define S_FW_IQ_CMD_FL1FETCHRO		6
+#define M_FW_IQ_CMD_FL1FETCHRO		0x1
+#define V_FW_IQ_CMD_FL1FETCHRO(x)	((x) << S_FW_IQ_CMD_FL1FETCHRO)
+#define G_FW_IQ_CMD_FL1FETCHRO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1FETCHRO) & M_FW_IQ_CMD_FL1FETCHRO)
+#define F_FW_IQ_CMD_FL1FETCHRO	V_FW_IQ_CMD_FL1FETCHRO(1U)
+
+#define S_FW_IQ_CMD_FL1HOSTFCMODE	4
+#define M_FW_IQ_CMD_FL1HOSTFCMODE	0x3
+#define V_FW_IQ_CMD_FL1HOSTFCMODE(x)	((x) << S_FW_IQ_CMD_FL1HOSTFCMODE)
+#define G_FW_IQ_CMD_FL1HOSTFCMODE(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1HOSTFCMODE) & M_FW_IQ_CMD_FL1HOSTFCMODE)
+
+#define S_FW_IQ_CMD_FL1CPRIO	3
+#define M_FW_IQ_CMD_FL1CPRIO	0x1
+#define V_FW_IQ_CMD_FL1CPRIO(x)	((x) << S_FW_IQ_CMD_FL1CPRIO)
+#define G_FW_IQ_CMD_FL1CPRIO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CPRIO) & M_FW_IQ_CMD_FL1CPRIO)
+#define F_FW_IQ_CMD_FL1CPRIO	V_FW_IQ_CMD_FL1CPRIO(1U)
+
+#define S_FW_IQ_CMD_FL1PADEN	2
+#define M_FW_IQ_CMD_FL1PADEN	0x1
+#define V_FW_IQ_CMD_FL1PADEN(x)	((x) << S_FW_IQ_CMD_FL1PADEN)
+#define G_FW_IQ_CMD_FL1PADEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1PADEN) & M_FW_IQ_CMD_FL1PADEN)
+#define F_FW_IQ_CMD_FL1PADEN	V_FW_IQ_CMD_FL1PADEN(1U)
+
+#define S_FW_IQ_CMD_FL1PACKEN		1
+#define M_FW_IQ_CMD_FL1PACKEN		0x1
+#define V_FW_IQ_CMD_FL1PACKEN(x)	((x) << S_FW_IQ_CMD_FL1PACKEN)
+#define G_FW_IQ_CMD_FL1PACKEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1PACKEN) & M_FW_IQ_CMD_FL1PACKEN)
+#define F_FW_IQ_CMD_FL1PACKEN	V_FW_IQ_CMD_FL1PACKEN(1U)
+
+#define S_FW_IQ_CMD_FL1CONGEN		0
+#define M_FW_IQ_CMD_FL1CONGEN		0x1
+#define V_FW_IQ_CMD_FL1CONGEN(x)	((x) << S_FW_IQ_CMD_FL1CONGEN)
+#define G_FW_IQ_CMD_FL1CONGEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CONGEN) & M_FW_IQ_CMD_FL1CONGEN)
+#define F_FW_IQ_CMD_FL1CONGEN	V_FW_IQ_CMD_FL1CONGEN(1U)
+
+#define S_FW_IQ_CMD_FL1DCAEN	15
+#define M_FW_IQ_CMD_FL1DCAEN	0x1
+#define V_FW_IQ_CMD_FL1DCAEN(x)	((x) << S_FW_IQ_CMD_FL1DCAEN)
+#define G_FW_IQ_CMD_FL1DCAEN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1DCAEN) & M_FW_IQ_CMD_FL1DCAEN)
+#define F_FW_IQ_CMD_FL1DCAEN	V_FW_IQ_CMD_FL1DCAEN(1U)
+
+#define S_FW_IQ_CMD_FL1DCACPU		10
+#define M_FW_IQ_CMD_FL1DCACPU		0x1f
+#define V_FW_IQ_CMD_FL1DCACPU(x)	((x) << S_FW_IQ_CMD_FL1DCACPU)
+#define G_FW_IQ_CMD_FL1DCACPU(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1DCACPU) & M_FW_IQ_CMD_FL1DCACPU)
+
+#define S_FW_IQ_CMD_FL1FBMIN	7
+#define M_FW_IQ_CMD_FL1FBMIN	0x7
+#define V_FW_IQ_CMD_FL1FBMIN(x)	((x) << S_FW_IQ_CMD_FL1FBMIN)
+#define G_FW_IQ_CMD_FL1FBMIN(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1FBMIN) & M_FW_IQ_CMD_FL1FBMIN)
+
+#define S_FW_IQ_CMD_FL1FBMAX	4
+#define M_FW_IQ_CMD_FL1FBMAX	0x7
+#define V_FW_IQ_CMD_FL1FBMAX(x)	((x) << S_FW_IQ_CMD_FL1FBMAX)
+#define G_FW_IQ_CMD_FL1FBMAX(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1FBMAX) & M_FW_IQ_CMD_FL1FBMAX)
+
+#define S_FW_IQ_CMD_FL1CIDXFTHRESHO	3
+#define M_FW_IQ_CMD_FL1CIDXFTHRESHO	0x1
+#define V_FW_IQ_CMD_FL1CIDXFTHRESHO(x)	((x) << S_FW_IQ_CMD_FL1CIDXFTHRESHO)
+#define G_FW_IQ_CMD_FL1CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CIDXFTHRESHO) & M_FW_IQ_CMD_FL1CIDXFTHRESHO)
+#define F_FW_IQ_CMD_FL1CIDXFTHRESHO	V_FW_IQ_CMD_FL1CIDXFTHRESHO(1U)
+
+#define S_FW_IQ_CMD_FL1CIDXFTHRESH	0
+#define M_FW_IQ_CMD_FL1CIDXFTHRESH	0x7
+#define V_FW_IQ_CMD_FL1CIDXFTHRESH(x)	((x) << S_FW_IQ_CMD_FL1CIDXFTHRESH)
+#define G_FW_IQ_CMD_FL1CIDXFTHRESH(x)	\
+    (((x) >> S_FW_IQ_CMD_FL1CIDXFTHRESH) & M_FW_IQ_CMD_FL1CIDXFTHRESH)
+
+struct fw_eq_mngt_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be32 cmpliqid_eqid;
+	__be32 physeqid_pkd;
+	__be32 fetchszm_to_iqid;
+	__be32 dcaen_to_eqsize;
+	__be64 eqaddr;
+};
+
+#define S_FW_EQ_MNGT_CMD_PFN	8
+#define M_FW_EQ_MNGT_CMD_PFN	0x7
+#define V_FW_EQ_MNGT_CMD_PFN(x)	((x) << S_FW_EQ_MNGT_CMD_PFN)
+#define G_FW_EQ_MNGT_CMD_PFN(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_PFN) & M_FW_EQ_MNGT_CMD_PFN)
+
+#define S_FW_EQ_MNGT_CMD_VFN	0
+#define M_FW_EQ_MNGT_CMD_VFN	0xff
+#define V_FW_EQ_MNGT_CMD_VFN(x)	((x) << S_FW_EQ_MNGT_CMD_VFN)
+#define G_FW_EQ_MNGT_CMD_VFN(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_VFN) & M_FW_EQ_MNGT_CMD_VFN)
+
+#define S_FW_EQ_MNGT_CMD_ALLOC		31
+#define M_FW_EQ_MNGT_CMD_ALLOC		0x1
+#define V_FW_EQ_MNGT_CMD_ALLOC(x)	((x) << S_FW_EQ_MNGT_CMD_ALLOC)
+#define G_FW_EQ_MNGT_CMD_ALLOC(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_ALLOC) & M_FW_EQ_MNGT_CMD_ALLOC)
+#define F_FW_EQ_MNGT_CMD_ALLOC	V_FW_EQ_MNGT_CMD_ALLOC(1U)
+
+#define S_FW_EQ_MNGT_CMD_FREE		30
+#define M_FW_EQ_MNGT_CMD_FREE		0x1
+#define V_FW_EQ_MNGT_CMD_FREE(x)	((x) << S_FW_EQ_MNGT_CMD_FREE)
+#define G_FW_EQ_MNGT_CMD_FREE(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FREE) & M_FW_EQ_MNGT_CMD_FREE)
+#define F_FW_EQ_MNGT_CMD_FREE	V_FW_EQ_MNGT_CMD_FREE(1U)
+
+#define S_FW_EQ_MNGT_CMD_MODIFY		29
+#define M_FW_EQ_MNGT_CMD_MODIFY		0x1
+#define V_FW_EQ_MNGT_CMD_MODIFY(x)	((x) << S_FW_EQ_MNGT_CMD_MODIFY)
+#define G_FW_EQ_MNGT_CMD_MODIFY(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_MODIFY) & M_FW_EQ_MNGT_CMD_MODIFY)
+#define F_FW_EQ_MNGT_CMD_MODIFY	V_FW_EQ_MNGT_CMD_MODIFY(1U)
+
+#define S_FW_EQ_MNGT_CMD_EQSTART	28
+#define M_FW_EQ_MNGT_CMD_EQSTART	0x1
+#define V_FW_EQ_MNGT_CMD_EQSTART(x)	((x) << S_FW_EQ_MNGT_CMD_EQSTART)
+#define G_FW_EQ_MNGT_CMD_EQSTART(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_EQSTART) & M_FW_EQ_MNGT_CMD_EQSTART)
+#define F_FW_EQ_MNGT_CMD_EQSTART	V_FW_EQ_MNGT_CMD_EQSTART(1U)
+
+#define S_FW_EQ_MNGT_CMD_EQSTOP		27
+#define M_FW_EQ_MNGT_CMD_EQSTOP		0x1
+#define V_FW_EQ_MNGT_CMD_EQSTOP(x)	((x) << S_FW_EQ_MNGT_CMD_EQSTOP)
+#define G_FW_EQ_MNGT_CMD_EQSTOP(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_EQSTOP) & M_FW_EQ_MNGT_CMD_EQSTOP)
+#define F_FW_EQ_MNGT_CMD_EQSTOP	V_FW_EQ_MNGT_CMD_EQSTOP(1U)
+
+#define S_FW_EQ_MNGT_CMD_CMPLIQID	20
+#define M_FW_EQ_MNGT_CMD_CMPLIQID	0xfff
+#define V_FW_EQ_MNGT_CMD_CMPLIQID(x)	((x) << S_FW_EQ_MNGT_CMD_CMPLIQID)
+#define G_FW_EQ_MNGT_CMD_CMPLIQID(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_CMPLIQID) & M_FW_EQ_MNGT_CMD_CMPLIQID)
+
+#define S_FW_EQ_MNGT_CMD_EQID		0
+#define M_FW_EQ_MNGT_CMD_EQID		0xfffff
+#define V_FW_EQ_MNGT_CMD_EQID(x)	((x) << S_FW_EQ_MNGT_CMD_EQID)
+#define G_FW_EQ_MNGT_CMD_EQID(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_EQID) & M_FW_EQ_MNGT_CMD_EQID)
+
+#define S_FW_EQ_MNGT_CMD_PHYSEQID	0
+#define M_FW_EQ_MNGT_CMD_PHYSEQID	0xfffff
+#define V_FW_EQ_MNGT_CMD_PHYSEQID(x)	((x) << S_FW_EQ_MNGT_CMD_PHYSEQID)
+#define G_FW_EQ_MNGT_CMD_PHYSEQID(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_PHYSEQID) & M_FW_EQ_MNGT_CMD_PHYSEQID)
+
+#define S_FW_EQ_MNGT_CMD_FETCHSZM	26
+#define M_FW_EQ_MNGT_CMD_FETCHSZM	0x1
+#define V_FW_EQ_MNGT_CMD_FETCHSZM(x)	((x) << S_FW_EQ_MNGT_CMD_FETCHSZM)
+#define G_FW_EQ_MNGT_CMD_FETCHSZM(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FETCHSZM) & M_FW_EQ_MNGT_CMD_FETCHSZM)
+#define F_FW_EQ_MNGT_CMD_FETCHSZM	V_FW_EQ_MNGT_CMD_FETCHSZM(1U)
+
+#define S_FW_EQ_MNGT_CMD_STATUSPGNS	25
+#define M_FW_EQ_MNGT_CMD_STATUSPGNS	0x1
+#define V_FW_EQ_MNGT_CMD_STATUSPGNS(x)	((x) << S_FW_EQ_MNGT_CMD_STATUSPGNS)
+#define G_FW_EQ_MNGT_CMD_STATUSPGNS(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_STATUSPGNS) & M_FW_EQ_MNGT_CMD_STATUSPGNS)
+#define F_FW_EQ_MNGT_CMD_STATUSPGNS	V_FW_EQ_MNGT_CMD_STATUSPGNS(1U)
+
+#define S_FW_EQ_MNGT_CMD_STATUSPGRO	24
+#define M_FW_EQ_MNGT_CMD_STATUSPGRO	0x1
+#define V_FW_EQ_MNGT_CMD_STATUSPGRO(x)	((x) << S_FW_EQ_MNGT_CMD_STATUSPGRO)
+#define G_FW_EQ_MNGT_CMD_STATUSPGRO(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_STATUSPGRO) & M_FW_EQ_MNGT_CMD_STATUSPGRO)
+#define F_FW_EQ_MNGT_CMD_STATUSPGRO	V_FW_EQ_MNGT_CMD_STATUSPGRO(1U)
+
+#define S_FW_EQ_MNGT_CMD_FETCHNS	23
+#define M_FW_EQ_MNGT_CMD_FETCHNS	0x1
+#define V_FW_EQ_MNGT_CMD_FETCHNS(x)	((x) << S_FW_EQ_MNGT_CMD_FETCHNS)
+#define G_FW_EQ_MNGT_CMD_FETCHNS(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FETCHNS) & M_FW_EQ_MNGT_CMD_FETCHNS)
+#define F_FW_EQ_MNGT_CMD_FETCHNS	V_FW_EQ_MNGT_CMD_FETCHNS(1U)
+
+#define S_FW_EQ_MNGT_CMD_FETCHRO	22
+#define M_FW_EQ_MNGT_CMD_FETCHRO	0x1
+#define V_FW_EQ_MNGT_CMD_FETCHRO(x)	((x) << S_FW_EQ_MNGT_CMD_FETCHRO)
+#define G_FW_EQ_MNGT_CMD_FETCHRO(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FETCHRO) & M_FW_EQ_MNGT_CMD_FETCHRO)
+#define F_FW_EQ_MNGT_CMD_FETCHRO	V_FW_EQ_MNGT_CMD_FETCHRO(1U)
+
+#define S_FW_EQ_MNGT_CMD_HOSTFCMODE	20
+#define M_FW_EQ_MNGT_CMD_HOSTFCMODE	0x3
+#define V_FW_EQ_MNGT_CMD_HOSTFCMODE(x)	((x) << S_FW_EQ_MNGT_CMD_HOSTFCMODE)
+#define G_FW_EQ_MNGT_CMD_HOSTFCMODE(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_HOSTFCMODE) & M_FW_EQ_MNGT_CMD_HOSTFCMODE)
+
+#define S_FW_EQ_MNGT_CMD_CPRIO		19
+#define M_FW_EQ_MNGT_CMD_CPRIO		0x1
+#define V_FW_EQ_MNGT_CMD_CPRIO(x)	((x) << S_FW_EQ_MNGT_CMD_CPRIO)
+#define G_FW_EQ_MNGT_CMD_CPRIO(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_CPRIO) & M_FW_EQ_MNGT_CMD_CPRIO)
+#define F_FW_EQ_MNGT_CMD_CPRIO	V_FW_EQ_MNGT_CMD_CPRIO(1U)
+
+#define S_FW_EQ_MNGT_CMD_ONCHIP		18
+#define M_FW_EQ_MNGT_CMD_ONCHIP		0x1
+#define V_FW_EQ_MNGT_CMD_ONCHIP(x)	((x) << S_FW_EQ_MNGT_CMD_ONCHIP)
+#define G_FW_EQ_MNGT_CMD_ONCHIP(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_ONCHIP) & M_FW_EQ_MNGT_CMD_ONCHIP)
+#define F_FW_EQ_MNGT_CMD_ONCHIP	V_FW_EQ_MNGT_CMD_ONCHIP(1U)
+
+#define S_FW_EQ_MNGT_CMD_PCIECHN	16
+#define M_FW_EQ_MNGT_CMD_PCIECHN	0x3
+#define V_FW_EQ_MNGT_CMD_PCIECHN(x)	((x) << S_FW_EQ_MNGT_CMD_PCIECHN)
+#define G_FW_EQ_MNGT_CMD_PCIECHN(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_PCIECHN) & M_FW_EQ_MNGT_CMD_PCIECHN)
+
+#define S_FW_EQ_MNGT_CMD_IQID		0
+#define M_FW_EQ_MNGT_CMD_IQID		0xffff
+#define V_FW_EQ_MNGT_CMD_IQID(x)	((x) << S_FW_EQ_MNGT_CMD_IQID)
+#define G_FW_EQ_MNGT_CMD_IQID(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_IQID) & M_FW_EQ_MNGT_CMD_IQID)
+
+#define S_FW_EQ_MNGT_CMD_DCAEN		31
+#define M_FW_EQ_MNGT_CMD_DCAEN		0x1
+#define V_FW_EQ_MNGT_CMD_DCAEN(x)	((x) << S_FW_EQ_MNGT_CMD_DCAEN)
+#define G_FW_EQ_MNGT_CMD_DCAEN(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_DCAEN) & M_FW_EQ_MNGT_CMD_DCAEN)
+#define F_FW_EQ_MNGT_CMD_DCAEN	V_FW_EQ_MNGT_CMD_DCAEN(1U)
+
+#define S_FW_EQ_MNGT_CMD_DCACPU		26
+#define M_FW_EQ_MNGT_CMD_DCACPU		0x1f
+#define V_FW_EQ_MNGT_CMD_DCACPU(x)	((x) << S_FW_EQ_MNGT_CMD_DCACPU)
+#define G_FW_EQ_MNGT_CMD_DCACPU(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_DCACPU) & M_FW_EQ_MNGT_CMD_DCACPU)
+
+#define S_FW_EQ_MNGT_CMD_FBMIN		23
+#define M_FW_EQ_MNGT_CMD_FBMIN		0x7
+#define V_FW_EQ_MNGT_CMD_FBMIN(x)	((x) << S_FW_EQ_MNGT_CMD_FBMIN)
+#define G_FW_EQ_MNGT_CMD_FBMIN(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FBMIN) & M_FW_EQ_MNGT_CMD_FBMIN)
+
+#define S_FW_EQ_MNGT_CMD_FBMAX		20
+#define M_FW_EQ_MNGT_CMD_FBMAX		0x7
+#define V_FW_EQ_MNGT_CMD_FBMAX(x)	((x) << S_FW_EQ_MNGT_CMD_FBMAX)
+#define G_FW_EQ_MNGT_CMD_FBMAX(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_FBMAX) & M_FW_EQ_MNGT_CMD_FBMAX)
+
+#define S_FW_EQ_MNGT_CMD_CIDXFTHRESHO		19
+#define M_FW_EQ_MNGT_CMD_CIDXFTHRESHO		0x1
+#define V_FW_EQ_MNGT_CMD_CIDXFTHRESHO(x)	\
+    ((x) << S_FW_EQ_MNGT_CMD_CIDXFTHRESHO)
+#define G_FW_EQ_MNGT_CMD_CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_CIDXFTHRESHO) & M_FW_EQ_MNGT_CMD_CIDXFTHRESHO)
+#define F_FW_EQ_MNGT_CMD_CIDXFTHRESHO	V_FW_EQ_MNGT_CMD_CIDXFTHRESHO(1U)
+
+#define S_FW_EQ_MNGT_CMD_CIDXFTHRESH	16
+#define M_FW_EQ_MNGT_CMD_CIDXFTHRESH	0x7
+#define V_FW_EQ_MNGT_CMD_CIDXFTHRESH(x)	((x) << S_FW_EQ_MNGT_CMD_CIDXFTHRESH)
+#define G_FW_EQ_MNGT_CMD_CIDXFTHRESH(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_CIDXFTHRESH) & M_FW_EQ_MNGT_CMD_CIDXFTHRESH)
+
+#define S_FW_EQ_MNGT_CMD_EQSIZE		0
+#define M_FW_EQ_MNGT_CMD_EQSIZE		0xffff
+#define V_FW_EQ_MNGT_CMD_EQSIZE(x)	((x) << S_FW_EQ_MNGT_CMD_EQSIZE)
+#define G_FW_EQ_MNGT_CMD_EQSIZE(x)	\
+    (((x) >> S_FW_EQ_MNGT_CMD_EQSIZE) & M_FW_EQ_MNGT_CMD_EQSIZE)
+
+struct fw_eq_eth_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be32 eqid_pkd;
+	__be32 physeqid_pkd;
+	__be32 fetchszm_to_iqid;
+	__be32 dcaen_to_eqsize;
+	__be64 eqaddr;
+	__be32 viid_pkd;
+	__be32 r8_lo;
+	__be64 r9;
+};
+
+#define S_FW_EQ_ETH_CMD_PFN	8
+#define M_FW_EQ_ETH_CMD_PFN	0x7
+#define V_FW_EQ_ETH_CMD_PFN(x)	((x) << S_FW_EQ_ETH_CMD_PFN)
+#define G_FW_EQ_ETH_CMD_PFN(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_PFN) & M_FW_EQ_ETH_CMD_PFN)
+
+#define S_FW_EQ_ETH_CMD_VFN	0
+#define M_FW_EQ_ETH_CMD_VFN	0xff
+#define V_FW_EQ_ETH_CMD_VFN(x)	((x) << S_FW_EQ_ETH_CMD_VFN)
+#define G_FW_EQ_ETH_CMD_VFN(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_VFN) & M_FW_EQ_ETH_CMD_VFN)
+
+#define S_FW_EQ_ETH_CMD_ALLOC		31
+#define M_FW_EQ_ETH_CMD_ALLOC		0x1
+#define V_FW_EQ_ETH_CMD_ALLOC(x)	((x) << S_FW_EQ_ETH_CMD_ALLOC)
+#define G_FW_EQ_ETH_CMD_ALLOC(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_ALLOC) & M_FW_EQ_ETH_CMD_ALLOC)
+#define F_FW_EQ_ETH_CMD_ALLOC	V_FW_EQ_ETH_CMD_ALLOC(1U)
+
+#define S_FW_EQ_ETH_CMD_FREE	30
+#define M_FW_EQ_ETH_CMD_FREE	0x1
+#define V_FW_EQ_ETH_CMD_FREE(x)	((x) << S_FW_EQ_ETH_CMD_FREE)
+#define G_FW_EQ_ETH_CMD_FREE(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FREE) & M_FW_EQ_ETH_CMD_FREE)
+#define F_FW_EQ_ETH_CMD_FREE	V_FW_EQ_ETH_CMD_FREE(1U)
+
+#define S_FW_EQ_ETH_CMD_MODIFY		29
+#define M_FW_EQ_ETH_CMD_MODIFY		0x1
+#define V_FW_EQ_ETH_CMD_MODIFY(x)	((x) << S_FW_EQ_ETH_CMD_MODIFY)
+#define G_FW_EQ_ETH_CMD_MODIFY(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_MODIFY) & M_FW_EQ_ETH_CMD_MODIFY)
+#define F_FW_EQ_ETH_CMD_MODIFY	V_FW_EQ_ETH_CMD_MODIFY(1U)
+
+#define S_FW_EQ_ETH_CMD_EQSTART		28
+#define M_FW_EQ_ETH_CMD_EQSTART		0x1
+#define V_FW_EQ_ETH_CMD_EQSTART(x)	((x) << S_FW_EQ_ETH_CMD_EQSTART)
+#define G_FW_EQ_ETH_CMD_EQSTART(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_EQSTART) & M_FW_EQ_ETH_CMD_EQSTART)
+#define F_FW_EQ_ETH_CMD_EQSTART	V_FW_EQ_ETH_CMD_EQSTART(1U)
+
+#define S_FW_EQ_ETH_CMD_EQSTOP		27
+#define M_FW_EQ_ETH_CMD_EQSTOP		0x1
+#define V_FW_EQ_ETH_CMD_EQSTOP(x)	((x) << S_FW_EQ_ETH_CMD_EQSTOP)
+#define G_FW_EQ_ETH_CMD_EQSTOP(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_EQSTOP) & M_FW_EQ_ETH_CMD_EQSTOP)
+#define F_FW_EQ_ETH_CMD_EQSTOP	V_FW_EQ_ETH_CMD_EQSTOP(1U)
+
+#define S_FW_EQ_ETH_CMD_EQID	0
+#define M_FW_EQ_ETH_CMD_EQID	0xfffff
+#define V_FW_EQ_ETH_CMD_EQID(x)	((x) << S_FW_EQ_ETH_CMD_EQID)
+#define G_FW_EQ_ETH_CMD_EQID(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_EQID) & M_FW_EQ_ETH_CMD_EQID)
+
+#define S_FW_EQ_ETH_CMD_PHYSEQID	0
+#define M_FW_EQ_ETH_CMD_PHYSEQID	0xfffff
+#define V_FW_EQ_ETH_CMD_PHYSEQID(x)	((x) << S_FW_EQ_ETH_CMD_PHYSEQID)
+#define G_FW_EQ_ETH_CMD_PHYSEQID(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_PHYSEQID) & M_FW_EQ_ETH_CMD_PHYSEQID)
+
+#define S_FW_EQ_ETH_CMD_FETCHSZM	26
+#define M_FW_EQ_ETH_CMD_FETCHSZM	0x1
+#define V_FW_EQ_ETH_CMD_FETCHSZM(x)	((x) << S_FW_EQ_ETH_CMD_FETCHSZM)
+#define G_FW_EQ_ETH_CMD_FETCHSZM(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FETCHSZM) & M_FW_EQ_ETH_CMD_FETCHSZM)
+#define F_FW_EQ_ETH_CMD_FETCHSZM	V_FW_EQ_ETH_CMD_FETCHSZM(1U)
+
+#define S_FW_EQ_ETH_CMD_STATUSPGNS	25
+#define M_FW_EQ_ETH_CMD_STATUSPGNS	0x1
+#define V_FW_EQ_ETH_CMD_STATUSPGNS(x)	((x) << S_FW_EQ_ETH_CMD_STATUSPGNS)
+#define G_FW_EQ_ETH_CMD_STATUSPGNS(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_STATUSPGNS) & M_FW_EQ_ETH_CMD_STATUSPGNS)
+#define F_FW_EQ_ETH_CMD_STATUSPGNS	V_FW_EQ_ETH_CMD_STATUSPGNS(1U)
+
+#define S_FW_EQ_ETH_CMD_STATUSPGRO	24
+#define M_FW_EQ_ETH_CMD_STATUSPGRO	0x1
+#define V_FW_EQ_ETH_CMD_STATUSPGRO(x)	((x) << S_FW_EQ_ETH_CMD_STATUSPGRO)
+#define G_FW_EQ_ETH_CMD_STATUSPGRO(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_STATUSPGRO) & M_FW_EQ_ETH_CMD_STATUSPGRO)
+#define F_FW_EQ_ETH_CMD_STATUSPGRO	V_FW_EQ_ETH_CMD_STATUSPGRO(1U)
+
+#define S_FW_EQ_ETH_CMD_FETCHNS		23
+#define M_FW_EQ_ETH_CMD_FETCHNS		0x1
+#define V_FW_EQ_ETH_CMD_FETCHNS(x)	((x) << S_FW_EQ_ETH_CMD_FETCHNS)
+#define G_FW_EQ_ETH_CMD_FETCHNS(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FETCHNS) & M_FW_EQ_ETH_CMD_FETCHNS)
+#define F_FW_EQ_ETH_CMD_FETCHNS	V_FW_EQ_ETH_CMD_FETCHNS(1U)
+
+#define S_FW_EQ_ETH_CMD_FETCHRO		22
+#define M_FW_EQ_ETH_CMD_FETCHRO		0x1
+#define V_FW_EQ_ETH_CMD_FETCHRO(x)	((x) << S_FW_EQ_ETH_CMD_FETCHRO)
+#define G_FW_EQ_ETH_CMD_FETCHRO(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FETCHRO) & M_FW_EQ_ETH_CMD_FETCHRO)
+#define F_FW_EQ_ETH_CMD_FETCHRO	V_FW_EQ_ETH_CMD_FETCHRO(1U)
+
+#define S_FW_EQ_ETH_CMD_HOSTFCMODE	20
+#define M_FW_EQ_ETH_CMD_HOSTFCMODE	0x3
+#define V_FW_EQ_ETH_CMD_HOSTFCMODE(x)	((x) << S_FW_EQ_ETH_CMD_HOSTFCMODE)
+#define G_FW_EQ_ETH_CMD_HOSTFCMODE(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_HOSTFCMODE) & M_FW_EQ_ETH_CMD_HOSTFCMODE)
+
+#define S_FW_EQ_ETH_CMD_CPRIO		19
+#define M_FW_EQ_ETH_CMD_CPRIO		0x1
+#define V_FW_EQ_ETH_CMD_CPRIO(x)	((x) << S_FW_EQ_ETH_CMD_CPRIO)
+#define G_FW_EQ_ETH_CMD_CPRIO(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_CPRIO) & M_FW_EQ_ETH_CMD_CPRIO)
+#define F_FW_EQ_ETH_CMD_CPRIO	V_FW_EQ_ETH_CMD_CPRIO(1U)
+
+#define S_FW_EQ_ETH_CMD_ONCHIP		18
+#define M_FW_EQ_ETH_CMD_ONCHIP		0x1
+#define V_FW_EQ_ETH_CMD_ONCHIP(x)	((x) << S_FW_EQ_ETH_CMD_ONCHIP)
+#define G_FW_EQ_ETH_CMD_ONCHIP(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_ONCHIP) & M_FW_EQ_ETH_CMD_ONCHIP)
+#define F_FW_EQ_ETH_CMD_ONCHIP	V_FW_EQ_ETH_CMD_ONCHIP(1U)
+
+#define S_FW_EQ_ETH_CMD_PCIECHN		16
+#define M_FW_EQ_ETH_CMD_PCIECHN		0x3
+#define V_FW_EQ_ETH_CMD_PCIECHN(x)	((x) << S_FW_EQ_ETH_CMD_PCIECHN)
+#define G_FW_EQ_ETH_CMD_PCIECHN(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_PCIECHN) & M_FW_EQ_ETH_CMD_PCIECHN)
+
+#define S_FW_EQ_ETH_CMD_IQID	0
+#define M_FW_EQ_ETH_CMD_IQID	0xffff
+#define V_FW_EQ_ETH_CMD_IQID(x)	((x) << S_FW_EQ_ETH_CMD_IQID)
+#define G_FW_EQ_ETH_CMD_IQID(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_IQID) & M_FW_EQ_ETH_CMD_IQID)
+
+#define S_FW_EQ_ETH_CMD_DCAEN		31
+#define M_FW_EQ_ETH_CMD_DCAEN		0x1
+#define V_FW_EQ_ETH_CMD_DCAEN(x)	((x) << S_FW_EQ_ETH_CMD_DCAEN)
+#define G_FW_EQ_ETH_CMD_DCAEN(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_DCAEN) & M_FW_EQ_ETH_CMD_DCAEN)
+#define F_FW_EQ_ETH_CMD_DCAEN	V_FW_EQ_ETH_CMD_DCAEN(1U)
+
+#define S_FW_EQ_ETH_CMD_DCACPU		26
+#define M_FW_EQ_ETH_CMD_DCACPU		0x1f
+#define V_FW_EQ_ETH_CMD_DCACPU(x)	((x) << S_FW_EQ_ETH_CMD_DCACPU)
+#define G_FW_EQ_ETH_CMD_DCACPU(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_DCACPU) & M_FW_EQ_ETH_CMD_DCACPU)
+
+#define S_FW_EQ_ETH_CMD_FBMIN		23
+#define M_FW_EQ_ETH_CMD_FBMIN		0x7
+#define V_FW_EQ_ETH_CMD_FBMIN(x)	((x) << S_FW_EQ_ETH_CMD_FBMIN)
+#define G_FW_EQ_ETH_CMD_FBMIN(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FBMIN) & M_FW_EQ_ETH_CMD_FBMIN)
+
+#define S_FW_EQ_ETH_CMD_FBMAX		20
+#define M_FW_EQ_ETH_CMD_FBMAX		0x7
+#define V_FW_EQ_ETH_CMD_FBMAX(x)	((x) << S_FW_EQ_ETH_CMD_FBMAX)
+#define G_FW_EQ_ETH_CMD_FBMAX(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_FBMAX) & M_FW_EQ_ETH_CMD_FBMAX)
+
+#define S_FW_EQ_ETH_CMD_CIDXFTHRESHO	19
+#define M_FW_EQ_ETH_CMD_CIDXFTHRESHO	0x1
+#define V_FW_EQ_ETH_CMD_CIDXFTHRESHO(x)	((x) << S_FW_EQ_ETH_CMD_CIDXFTHRESHO)
+#define G_FW_EQ_ETH_CMD_CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_CIDXFTHRESHO) & M_FW_EQ_ETH_CMD_CIDXFTHRESHO)
+#define F_FW_EQ_ETH_CMD_CIDXFTHRESHO	V_FW_EQ_ETH_CMD_CIDXFTHRESHO(1U)
+
+#define S_FW_EQ_ETH_CMD_CIDXFTHRESH	16
+#define M_FW_EQ_ETH_CMD_CIDXFTHRESH	0x7
+#define V_FW_EQ_ETH_CMD_CIDXFTHRESH(x)	((x) << S_FW_EQ_ETH_CMD_CIDXFTHRESH)
+#define G_FW_EQ_ETH_CMD_CIDXFTHRESH(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_CIDXFTHRESH) & M_FW_EQ_ETH_CMD_CIDXFTHRESH)
+
+#define S_FW_EQ_ETH_CMD_EQSIZE		0
+#define M_FW_EQ_ETH_CMD_EQSIZE		0xffff
+#define V_FW_EQ_ETH_CMD_EQSIZE(x)	((x) << S_FW_EQ_ETH_CMD_EQSIZE)
+#define G_FW_EQ_ETH_CMD_EQSIZE(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_EQSIZE) & M_FW_EQ_ETH_CMD_EQSIZE)
+
+#define S_FW_EQ_ETH_CMD_VIID	16
+#define M_FW_EQ_ETH_CMD_VIID	0xfff
+#define V_FW_EQ_ETH_CMD_VIID(x)	((x) << S_FW_EQ_ETH_CMD_VIID)
+#define G_FW_EQ_ETH_CMD_VIID(x)	\
+    (((x) >> S_FW_EQ_ETH_CMD_VIID) & M_FW_EQ_ETH_CMD_VIID)
+
+struct fw_eq_ctrl_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be32 cmpliqid_eqid;
+	__be32 physeqid_pkd;
+	__be32 fetchszm_to_iqid;
+	__be32 dcaen_to_eqsize;
+	__be64 eqaddr;
+};
+
+#define S_FW_EQ_CTRL_CMD_PFN	8
+#define M_FW_EQ_CTRL_CMD_PFN	0x7
+#define V_FW_EQ_CTRL_CMD_PFN(x)	((x) << S_FW_EQ_CTRL_CMD_PFN)
+#define G_FW_EQ_CTRL_CMD_PFN(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_PFN) & M_FW_EQ_CTRL_CMD_PFN)
+
+#define S_FW_EQ_CTRL_CMD_VFN	0
+#define M_FW_EQ_CTRL_CMD_VFN	0xff
+#define V_FW_EQ_CTRL_CMD_VFN(x)	((x) << S_FW_EQ_CTRL_CMD_VFN)
+#define G_FW_EQ_CTRL_CMD_VFN(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_VFN) & M_FW_EQ_CTRL_CMD_VFN)
+
+#define S_FW_EQ_CTRL_CMD_ALLOC		31
+#define M_FW_EQ_CTRL_CMD_ALLOC		0x1
+#define V_FW_EQ_CTRL_CMD_ALLOC(x)	((x) << S_FW_EQ_CTRL_CMD_ALLOC)
+#define G_FW_EQ_CTRL_CMD_ALLOC(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_ALLOC) & M_FW_EQ_CTRL_CMD_ALLOC)
+#define F_FW_EQ_CTRL_CMD_ALLOC	V_FW_EQ_CTRL_CMD_ALLOC(1U)
+
+#define S_FW_EQ_CTRL_CMD_FREE		30
+#define M_FW_EQ_CTRL_CMD_FREE		0x1
+#define V_FW_EQ_CTRL_CMD_FREE(x)	((x) << S_FW_EQ_CTRL_CMD_FREE)
+#define G_FW_EQ_CTRL_CMD_FREE(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FREE) & M_FW_EQ_CTRL_CMD_FREE)
+#define F_FW_EQ_CTRL_CMD_FREE	V_FW_EQ_CTRL_CMD_FREE(1U)
+
+#define S_FW_EQ_CTRL_CMD_MODIFY		29
+#define M_FW_EQ_CTRL_CMD_MODIFY		0x1
+#define V_FW_EQ_CTRL_CMD_MODIFY(x)	((x) << S_FW_EQ_CTRL_CMD_MODIFY)
+#define G_FW_EQ_CTRL_CMD_MODIFY(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_MODIFY) & M_FW_EQ_CTRL_CMD_MODIFY)
+#define F_FW_EQ_CTRL_CMD_MODIFY	V_FW_EQ_CTRL_CMD_MODIFY(1U)
+
+#define S_FW_EQ_CTRL_CMD_EQSTART	28
+#define M_FW_EQ_CTRL_CMD_EQSTART	0x1
+#define V_FW_EQ_CTRL_CMD_EQSTART(x)	((x) << S_FW_EQ_CTRL_CMD_EQSTART)
+#define G_FW_EQ_CTRL_CMD_EQSTART(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_EQSTART) & M_FW_EQ_CTRL_CMD_EQSTART)
+#define F_FW_EQ_CTRL_CMD_EQSTART	V_FW_EQ_CTRL_CMD_EQSTART(1U)
+
+#define S_FW_EQ_CTRL_CMD_EQSTOP		27
+#define M_FW_EQ_CTRL_CMD_EQSTOP		0x1
+#define V_FW_EQ_CTRL_CMD_EQSTOP(x)	((x) << S_FW_EQ_CTRL_CMD_EQSTOP)
+#define G_FW_EQ_CTRL_CMD_EQSTOP(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_EQSTOP) & M_FW_EQ_CTRL_CMD_EQSTOP)
+#define F_FW_EQ_CTRL_CMD_EQSTOP	V_FW_EQ_CTRL_CMD_EQSTOP(1U)
+
+#define S_FW_EQ_CTRL_CMD_CMPLIQID	20
+#define M_FW_EQ_CTRL_CMD_CMPLIQID	0xfff
+#define V_FW_EQ_CTRL_CMD_CMPLIQID(x)	((x) << S_FW_EQ_CTRL_CMD_CMPLIQID)
+#define G_FW_EQ_CTRL_CMD_CMPLIQID(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_CMPLIQID) & M_FW_EQ_CTRL_CMD_CMPLIQID)
+
+#define S_FW_EQ_CTRL_CMD_EQID		0
+#define M_FW_EQ_CTRL_CMD_EQID		0xfffff
+#define V_FW_EQ_CTRL_CMD_EQID(x)	((x) << S_FW_EQ_CTRL_CMD_EQID)
+#define G_FW_EQ_CTRL_CMD_EQID(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_EQID) & M_FW_EQ_CTRL_CMD_EQID)
+
+#define S_FW_EQ_CTRL_CMD_PHYSEQID	0
+#define M_FW_EQ_CTRL_CMD_PHYSEQID	0xfffff
+#define V_FW_EQ_CTRL_CMD_PHYSEQID(x)	((x) << S_FW_EQ_CTRL_CMD_PHYSEQID)
+#define G_FW_EQ_CTRL_CMD_PHYSEQID(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_PHYSEQID) & M_FW_EQ_CTRL_CMD_PHYSEQID)
+
+#define S_FW_EQ_CTRL_CMD_FETCHSZM	26
+#define M_FW_EQ_CTRL_CMD_FETCHSZM	0x1
+#define V_FW_EQ_CTRL_CMD_FETCHSZM(x)	((x) << S_FW_EQ_CTRL_CMD_FETCHSZM)
+#define G_FW_EQ_CTRL_CMD_FETCHSZM(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FETCHSZM) & M_FW_EQ_CTRL_CMD_FETCHSZM)
+#define F_FW_EQ_CTRL_CMD_FETCHSZM	V_FW_EQ_CTRL_CMD_FETCHSZM(1U)
+
+#define S_FW_EQ_CTRL_CMD_STATUSPGNS	25
+#define M_FW_EQ_CTRL_CMD_STATUSPGNS	0x1
+#define V_FW_EQ_CTRL_CMD_STATUSPGNS(x)	((x) << S_FW_EQ_CTRL_CMD_STATUSPGNS)
+#define G_FW_EQ_CTRL_CMD_STATUSPGNS(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_STATUSPGNS) & M_FW_EQ_CTRL_CMD_STATUSPGNS)
+#define F_FW_EQ_CTRL_CMD_STATUSPGNS	V_FW_EQ_CTRL_CMD_STATUSPGNS(1U)
+
+#define S_FW_EQ_CTRL_CMD_STATUSPGRO	24
+#define M_FW_EQ_CTRL_CMD_STATUSPGRO	0x1
+#define V_FW_EQ_CTRL_CMD_STATUSPGRO(x)	((x) << S_FW_EQ_CTRL_CMD_STATUSPGRO)
+#define G_FW_EQ_CTRL_CMD_STATUSPGRO(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_STATUSPGRO) & M_FW_EQ_CTRL_CMD_STATUSPGRO)
+#define F_FW_EQ_CTRL_CMD_STATUSPGRO	V_FW_EQ_CTRL_CMD_STATUSPGRO(1U)
+
+#define S_FW_EQ_CTRL_CMD_FETCHNS	23
+#define M_FW_EQ_CTRL_CMD_FETCHNS	0x1
+#define V_FW_EQ_CTRL_CMD_FETCHNS(x)	((x) << S_FW_EQ_CTRL_CMD_FETCHNS)
+#define G_FW_EQ_CTRL_CMD_FETCHNS(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FETCHNS) & M_FW_EQ_CTRL_CMD_FETCHNS)
+#define F_FW_EQ_CTRL_CMD_FETCHNS	V_FW_EQ_CTRL_CMD_FETCHNS(1U)
+
+#define S_FW_EQ_CTRL_CMD_FETCHRO	22
+#define M_FW_EQ_CTRL_CMD_FETCHRO	0x1
+#define V_FW_EQ_CTRL_CMD_FETCHRO(x)	((x) << S_FW_EQ_CTRL_CMD_FETCHRO)
+#define G_FW_EQ_CTRL_CMD_FETCHRO(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FETCHRO) & M_FW_EQ_CTRL_CMD_FETCHRO)
+#define F_FW_EQ_CTRL_CMD_FETCHRO	V_FW_EQ_CTRL_CMD_FETCHRO(1U)
+
+#define S_FW_EQ_CTRL_CMD_HOSTFCMODE	20
+#define M_FW_EQ_CTRL_CMD_HOSTFCMODE	0x3
+#define V_FW_EQ_CTRL_CMD_HOSTFCMODE(x)	((x) << S_FW_EQ_CTRL_CMD_HOSTFCMODE)
+#define G_FW_EQ_CTRL_CMD_HOSTFCMODE(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_HOSTFCMODE) & M_FW_EQ_CTRL_CMD_HOSTFCMODE)
+
+#define S_FW_EQ_CTRL_CMD_CPRIO		19
+#define M_FW_EQ_CTRL_CMD_CPRIO		0x1
+#define V_FW_EQ_CTRL_CMD_CPRIO(x)	((x) << S_FW_EQ_CTRL_CMD_CPRIO)
+#define G_FW_EQ_CTRL_CMD_CPRIO(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_CPRIO) & M_FW_EQ_CTRL_CMD_CPRIO)
+#define F_FW_EQ_CTRL_CMD_CPRIO	V_FW_EQ_CTRL_CMD_CPRIO(1U)
+
+#define S_FW_EQ_CTRL_CMD_ONCHIP		18
+#define M_FW_EQ_CTRL_CMD_ONCHIP		0x1
+#define V_FW_EQ_CTRL_CMD_ONCHIP(x)	((x) << S_FW_EQ_CTRL_CMD_ONCHIP)
+#define G_FW_EQ_CTRL_CMD_ONCHIP(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_ONCHIP) & M_FW_EQ_CTRL_CMD_ONCHIP)
+#define F_FW_EQ_CTRL_CMD_ONCHIP	V_FW_EQ_CTRL_CMD_ONCHIP(1U)
+
+#define S_FW_EQ_CTRL_CMD_PCIECHN	16
+#define M_FW_EQ_CTRL_CMD_PCIECHN	0x3
+#define V_FW_EQ_CTRL_CMD_PCIECHN(x)	((x) << S_FW_EQ_CTRL_CMD_PCIECHN)
+#define G_FW_EQ_CTRL_CMD_PCIECHN(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_PCIECHN) & M_FW_EQ_CTRL_CMD_PCIECHN)
+
+#define S_FW_EQ_CTRL_CMD_IQID		0
+#define M_FW_EQ_CTRL_CMD_IQID		0xffff
+#define V_FW_EQ_CTRL_CMD_IQID(x)	((x) << S_FW_EQ_CTRL_CMD_IQID)
+#define G_FW_EQ_CTRL_CMD_IQID(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_IQID) & M_FW_EQ_CTRL_CMD_IQID)
+
+#define S_FW_EQ_CTRL_CMD_DCAEN		31
+#define M_FW_EQ_CTRL_CMD_DCAEN		0x1
+#define V_FW_EQ_CTRL_CMD_DCAEN(x)	((x) << S_FW_EQ_CTRL_CMD_DCAEN)
+#define G_FW_EQ_CTRL_CMD_DCAEN(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_DCAEN) & M_FW_EQ_CTRL_CMD_DCAEN)
+#define F_FW_EQ_CTRL_CMD_DCAEN	V_FW_EQ_CTRL_CMD_DCAEN(1U)
+
+#define S_FW_EQ_CTRL_CMD_DCACPU		26
+#define M_FW_EQ_CTRL_CMD_DCACPU		0x1f
+#define V_FW_EQ_CTRL_CMD_DCACPU(x)	((x) << S_FW_EQ_CTRL_CMD_DCACPU)
+#define G_FW_EQ_CTRL_CMD_DCACPU(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_DCACPU) & M_FW_EQ_CTRL_CMD_DCACPU)
+
+#define S_FW_EQ_CTRL_CMD_FBMIN		23
+#define M_FW_EQ_CTRL_CMD_FBMIN		0x7
+#define V_FW_EQ_CTRL_CMD_FBMIN(x)	((x) << S_FW_EQ_CTRL_CMD_FBMIN)
+#define G_FW_EQ_CTRL_CMD_FBMIN(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FBMIN) & M_FW_EQ_CTRL_CMD_FBMIN)
+
+#define S_FW_EQ_CTRL_CMD_FBMAX		20
+#define M_FW_EQ_CTRL_CMD_FBMAX		0x7
+#define V_FW_EQ_CTRL_CMD_FBMAX(x)	((x) << S_FW_EQ_CTRL_CMD_FBMAX)
+#define G_FW_EQ_CTRL_CMD_FBMAX(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_FBMAX) & M_FW_EQ_CTRL_CMD_FBMAX)
+
+#define S_FW_EQ_CTRL_CMD_CIDXFTHRESHO		19
+#define M_FW_EQ_CTRL_CMD_CIDXFTHRESHO		0x1
+#define V_FW_EQ_CTRL_CMD_CIDXFTHRESHO(x)	\
+    ((x) << S_FW_EQ_CTRL_CMD_CIDXFTHRESHO)
+#define G_FW_EQ_CTRL_CMD_CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_CIDXFTHRESHO) & M_FW_EQ_CTRL_CMD_CIDXFTHRESHO)
+#define F_FW_EQ_CTRL_CMD_CIDXFTHRESHO	V_FW_EQ_CTRL_CMD_CIDXFTHRESHO(1U)
+
+#define S_FW_EQ_CTRL_CMD_CIDXFTHRESH	16
+#define M_FW_EQ_CTRL_CMD_CIDXFTHRESH	0x7
+#define V_FW_EQ_CTRL_CMD_CIDXFTHRESH(x)	((x) << S_FW_EQ_CTRL_CMD_CIDXFTHRESH)
+#define G_FW_EQ_CTRL_CMD_CIDXFTHRESH(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_CIDXFTHRESH) & M_FW_EQ_CTRL_CMD_CIDXFTHRESH)
+
+#define S_FW_EQ_CTRL_CMD_EQSIZE		0
+#define M_FW_EQ_CTRL_CMD_EQSIZE		0xffff
+#define V_FW_EQ_CTRL_CMD_EQSIZE(x)	((x) << S_FW_EQ_CTRL_CMD_EQSIZE)
+#define G_FW_EQ_CTRL_CMD_EQSIZE(x)	\
+    (((x) >> S_FW_EQ_CTRL_CMD_EQSIZE) & M_FW_EQ_CTRL_CMD_EQSIZE)
+
+struct fw_eq_ofld_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be32 eqid_pkd;
+	__be32 physeqid_pkd;
+	__be32 fetchszm_to_iqid;
+	__be32 dcaen_to_eqsize;
+	__be64 eqaddr;
+};
+
+#define S_FW_EQ_OFLD_CMD_PFN	8
+#define M_FW_EQ_OFLD_CMD_PFN	0x7
+#define V_FW_EQ_OFLD_CMD_PFN(x)	((x) << S_FW_EQ_OFLD_CMD_PFN)
+#define G_FW_EQ_OFLD_CMD_PFN(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_PFN) & M_FW_EQ_OFLD_CMD_PFN)
+
+#define S_FW_EQ_OFLD_CMD_VFN	0
+#define M_FW_EQ_OFLD_CMD_VFN	0xff
+#define V_FW_EQ_OFLD_CMD_VFN(x)	((x) << S_FW_EQ_OFLD_CMD_VFN)
+#define G_FW_EQ_OFLD_CMD_VFN(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_VFN) & M_FW_EQ_OFLD_CMD_VFN)
+
+#define S_FW_EQ_OFLD_CMD_ALLOC		31
+#define M_FW_EQ_OFLD_CMD_ALLOC		0x1
+#define V_FW_EQ_OFLD_CMD_ALLOC(x)	((x) << S_FW_EQ_OFLD_CMD_ALLOC)
+#define G_FW_EQ_OFLD_CMD_ALLOC(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_ALLOC) & M_FW_EQ_OFLD_CMD_ALLOC)
+#define F_FW_EQ_OFLD_CMD_ALLOC	V_FW_EQ_OFLD_CMD_ALLOC(1U)
+
+#define S_FW_EQ_OFLD_CMD_FREE		30
+#define M_FW_EQ_OFLD_CMD_FREE		0x1
+#define V_FW_EQ_OFLD_CMD_FREE(x)	((x) << S_FW_EQ_OFLD_CMD_FREE)
+#define G_FW_EQ_OFLD_CMD_FREE(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FREE) & M_FW_EQ_OFLD_CMD_FREE)
+#define F_FW_EQ_OFLD_CMD_FREE	V_FW_EQ_OFLD_CMD_FREE(1U)
+
+#define S_FW_EQ_OFLD_CMD_MODIFY		29
+#define M_FW_EQ_OFLD_CMD_MODIFY		0x1
+#define V_FW_EQ_OFLD_CMD_MODIFY(x)	((x) << S_FW_EQ_OFLD_CMD_MODIFY)
+#define G_FW_EQ_OFLD_CMD_MODIFY(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_MODIFY) & M_FW_EQ_OFLD_CMD_MODIFY)
+#define F_FW_EQ_OFLD_CMD_MODIFY	V_FW_EQ_OFLD_CMD_MODIFY(1U)
+
+#define S_FW_EQ_OFLD_CMD_EQSTART	28
+#define M_FW_EQ_OFLD_CMD_EQSTART	0x1
+#define V_FW_EQ_OFLD_CMD_EQSTART(x)	((x) << S_FW_EQ_OFLD_CMD_EQSTART)
+#define G_FW_EQ_OFLD_CMD_EQSTART(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_EQSTART) & M_FW_EQ_OFLD_CMD_EQSTART)
+#define F_FW_EQ_OFLD_CMD_EQSTART	V_FW_EQ_OFLD_CMD_EQSTART(1U)
+
+#define S_FW_EQ_OFLD_CMD_EQSTOP		27
+#define M_FW_EQ_OFLD_CMD_EQSTOP		0x1
+#define V_FW_EQ_OFLD_CMD_EQSTOP(x)	((x) << S_FW_EQ_OFLD_CMD_EQSTOP)
+#define G_FW_EQ_OFLD_CMD_EQSTOP(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_EQSTOP) & M_FW_EQ_OFLD_CMD_EQSTOP)
+#define F_FW_EQ_OFLD_CMD_EQSTOP	V_FW_EQ_OFLD_CMD_EQSTOP(1U)
+
+#define S_FW_EQ_OFLD_CMD_EQID		0
+#define M_FW_EQ_OFLD_CMD_EQID		0xfffff
+#define V_FW_EQ_OFLD_CMD_EQID(x)	((x) << S_FW_EQ_OFLD_CMD_EQID)
+#define G_FW_EQ_OFLD_CMD_EQID(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_EQID) & M_FW_EQ_OFLD_CMD_EQID)
+
+#define S_FW_EQ_OFLD_CMD_PHYSEQID	0
+#define M_FW_EQ_OFLD_CMD_PHYSEQID	0xfffff
+#define V_FW_EQ_OFLD_CMD_PHYSEQID(x)	((x) << S_FW_EQ_OFLD_CMD_PHYSEQID)
+#define G_FW_EQ_OFLD_CMD_PHYSEQID(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_PHYSEQID) & M_FW_EQ_OFLD_CMD_PHYSEQID)
+
+#define S_FW_EQ_OFLD_CMD_FETCHSZM	26
+#define M_FW_EQ_OFLD_CMD_FETCHSZM	0x1
+#define V_FW_EQ_OFLD_CMD_FETCHSZM(x)	((x) << S_FW_EQ_OFLD_CMD_FETCHSZM)
+#define G_FW_EQ_OFLD_CMD_FETCHSZM(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FETCHSZM) & M_FW_EQ_OFLD_CMD_FETCHSZM)
+#define F_FW_EQ_OFLD_CMD_FETCHSZM	V_FW_EQ_OFLD_CMD_FETCHSZM(1U)
+
+#define S_FW_EQ_OFLD_CMD_STATUSPGNS	25
+#define M_FW_EQ_OFLD_CMD_STATUSPGNS	0x1
+#define V_FW_EQ_OFLD_CMD_STATUSPGNS(x)	((x) << S_FW_EQ_OFLD_CMD_STATUSPGNS)
+#define G_FW_EQ_OFLD_CMD_STATUSPGNS(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_STATUSPGNS) & M_FW_EQ_OFLD_CMD_STATUSPGNS)
+#define F_FW_EQ_OFLD_CMD_STATUSPGNS	V_FW_EQ_OFLD_CMD_STATUSPGNS(1U)
+
+#define S_FW_EQ_OFLD_CMD_STATUSPGRO	24
+#define M_FW_EQ_OFLD_CMD_STATUSPGRO	0x1
+#define V_FW_EQ_OFLD_CMD_STATUSPGRO(x)	((x) << S_FW_EQ_OFLD_CMD_STATUSPGRO)
+#define G_FW_EQ_OFLD_CMD_STATUSPGRO(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_STATUSPGRO) & M_FW_EQ_OFLD_CMD_STATUSPGRO)
+#define F_FW_EQ_OFLD_CMD_STATUSPGRO	V_FW_EQ_OFLD_CMD_STATUSPGRO(1U)
+
+#define S_FW_EQ_OFLD_CMD_FETCHNS	23
+#define M_FW_EQ_OFLD_CMD_FETCHNS	0x1
+#define V_FW_EQ_OFLD_CMD_FETCHNS(x)	((x) << S_FW_EQ_OFLD_CMD_FETCHNS)
+#define G_FW_EQ_OFLD_CMD_FETCHNS(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FETCHNS) & M_FW_EQ_OFLD_CMD_FETCHNS)
+#define F_FW_EQ_OFLD_CMD_FETCHNS	V_FW_EQ_OFLD_CMD_FETCHNS(1U)
+
+#define S_FW_EQ_OFLD_CMD_FETCHRO	22
+#define M_FW_EQ_OFLD_CMD_FETCHRO	0x1
+#define V_FW_EQ_OFLD_CMD_FETCHRO(x)	((x) << S_FW_EQ_OFLD_CMD_FETCHRO)
+#define G_FW_EQ_OFLD_CMD_FETCHRO(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FETCHRO) & M_FW_EQ_OFLD_CMD_FETCHRO)
+#define F_FW_EQ_OFLD_CMD_FETCHRO	V_FW_EQ_OFLD_CMD_FETCHRO(1U)
+
+#define S_FW_EQ_OFLD_CMD_HOSTFCMODE	20
+#define M_FW_EQ_OFLD_CMD_HOSTFCMODE	0x3
+#define V_FW_EQ_OFLD_CMD_HOSTFCMODE(x)	((x) << S_FW_EQ_OFLD_CMD_HOSTFCMODE)
+#define G_FW_EQ_OFLD_CMD_HOSTFCMODE(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_HOSTFCMODE) & M_FW_EQ_OFLD_CMD_HOSTFCMODE)
+
+#define S_FW_EQ_OFLD_CMD_CPRIO		19
+#define M_FW_EQ_OFLD_CMD_CPRIO		0x1
+#define V_FW_EQ_OFLD_CMD_CPRIO(x)	((x) << S_FW_EQ_OFLD_CMD_CPRIO)
+#define G_FW_EQ_OFLD_CMD_CPRIO(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_CPRIO) & M_FW_EQ_OFLD_CMD_CPRIO)
+#define F_FW_EQ_OFLD_CMD_CPRIO	V_FW_EQ_OFLD_CMD_CPRIO(1U)
+
+#define S_FW_EQ_OFLD_CMD_ONCHIP		18
+#define M_FW_EQ_OFLD_CMD_ONCHIP		0x1
+#define V_FW_EQ_OFLD_CMD_ONCHIP(x)	((x) << S_FW_EQ_OFLD_CMD_ONCHIP)
+#define G_FW_EQ_OFLD_CMD_ONCHIP(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_ONCHIP) & M_FW_EQ_OFLD_CMD_ONCHIP)
+#define F_FW_EQ_OFLD_CMD_ONCHIP	V_FW_EQ_OFLD_CMD_ONCHIP(1U)
+
+#define S_FW_EQ_OFLD_CMD_PCIECHN	16
+#define M_FW_EQ_OFLD_CMD_PCIECHN	0x3
+#define V_FW_EQ_OFLD_CMD_PCIECHN(x)	((x) << S_FW_EQ_OFLD_CMD_PCIECHN)
+#define G_FW_EQ_OFLD_CMD_PCIECHN(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_PCIECHN) & M_FW_EQ_OFLD_CMD_PCIECHN)
+
+#define S_FW_EQ_OFLD_CMD_IQID		0
+#define M_FW_EQ_OFLD_CMD_IQID		0xffff
+#define V_FW_EQ_OFLD_CMD_IQID(x)	((x) << S_FW_EQ_OFLD_CMD_IQID)
+#define G_FW_EQ_OFLD_CMD_IQID(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_IQID) & M_FW_EQ_OFLD_CMD_IQID)
+
+#define S_FW_EQ_OFLD_CMD_DCAEN		31
+#define M_FW_EQ_OFLD_CMD_DCAEN		0x1
+#define V_FW_EQ_OFLD_CMD_DCAEN(x)	((x) << S_FW_EQ_OFLD_CMD_DCAEN)
+#define G_FW_EQ_OFLD_CMD_DCAEN(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_DCAEN) & M_FW_EQ_OFLD_CMD_DCAEN)
+#define F_FW_EQ_OFLD_CMD_DCAEN	V_FW_EQ_OFLD_CMD_DCAEN(1U)
+
+#define S_FW_EQ_OFLD_CMD_DCACPU		26
+#define M_FW_EQ_OFLD_CMD_DCACPU		0x1f
+#define V_FW_EQ_OFLD_CMD_DCACPU(x)	((x) << S_FW_EQ_OFLD_CMD_DCACPU)
+#define G_FW_EQ_OFLD_CMD_DCACPU(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_DCACPU) & M_FW_EQ_OFLD_CMD_DCACPU)
+
+#define S_FW_EQ_OFLD_CMD_FBMIN		23
+#define M_FW_EQ_OFLD_CMD_FBMIN		0x7
+#define V_FW_EQ_OFLD_CMD_FBMIN(x)	((x) << S_FW_EQ_OFLD_CMD_FBMIN)
+#define G_FW_EQ_OFLD_CMD_FBMIN(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FBMIN) & M_FW_EQ_OFLD_CMD_FBMIN)
+
+#define S_FW_EQ_OFLD_CMD_FBMAX		20
+#define M_FW_EQ_OFLD_CMD_FBMAX		0x7
+#define V_FW_EQ_OFLD_CMD_FBMAX(x)	((x) << S_FW_EQ_OFLD_CMD_FBMAX)
+#define G_FW_EQ_OFLD_CMD_FBMAX(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_FBMAX) & M_FW_EQ_OFLD_CMD_FBMAX)
+
+#define S_FW_EQ_OFLD_CMD_CIDXFTHRESHO		19
+#define M_FW_EQ_OFLD_CMD_CIDXFTHRESHO		0x1
+#define V_FW_EQ_OFLD_CMD_CIDXFTHRESHO(x)	\
+    ((x) << S_FW_EQ_OFLD_CMD_CIDXFTHRESHO)
+#define G_FW_EQ_OFLD_CMD_CIDXFTHRESHO(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_CIDXFTHRESHO) & M_FW_EQ_OFLD_CMD_CIDXFTHRESHO)
+#define F_FW_EQ_OFLD_CMD_CIDXFTHRESHO	V_FW_EQ_OFLD_CMD_CIDXFTHRESHO(1U)
+
+#define S_FW_EQ_OFLD_CMD_CIDXFTHRESH	16
+#define M_FW_EQ_OFLD_CMD_CIDXFTHRESH	0x7
+#define V_FW_EQ_OFLD_CMD_CIDXFTHRESH(x)	((x) << S_FW_EQ_OFLD_CMD_CIDXFTHRESH)
+#define G_FW_EQ_OFLD_CMD_CIDXFTHRESH(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_CIDXFTHRESH) & M_FW_EQ_OFLD_CMD_CIDXFTHRESH)
+
+#define S_FW_EQ_OFLD_CMD_EQSIZE		0
+#define M_FW_EQ_OFLD_CMD_EQSIZE		0xffff
+#define V_FW_EQ_OFLD_CMD_EQSIZE(x)	((x) << S_FW_EQ_OFLD_CMD_EQSIZE)
+#define G_FW_EQ_OFLD_CMD_EQSIZE(x)	\
+    (((x) >> S_FW_EQ_OFLD_CMD_EQSIZE) & M_FW_EQ_OFLD_CMD_EQSIZE)
+
+/* Macros for VIID parsing:
+   VIID - [10:8] PFN, [7] VI Valid, [6:0] VI number */
+#define S_FW_VIID_PFN		8
+#define M_FW_VIID_PFN		0x7
+#define V_FW_VIID_PFN(x)	((x) << S_FW_VIID_PFN)
+#define G_FW_VIID_PFN(x)	(((x) >> S_FW_VIID_PFN) & M_FW_VIID_PFN)
+
+#define S_FW_VIID_VIVLD		7
+#define M_FW_VIID_VIVLD		0x1
+#define V_FW_VIID_VIVLD(x)	((x) << S_FW_VIID_VIVLD)
+#define G_FW_VIID_VIVLD(x)	(((x) >> S_FW_VIID_VIVLD) & M_FW_VIID_VIVLD)
+
+#define S_FW_VIID_VIN		0
+#define M_FW_VIID_VIN		0x7F
+#define V_FW_VIID_VIN(x)	((x) << S_FW_VIID_VIN)
+#define G_FW_VIID_VIN(x)	(((x) >> S_FW_VIID_VIN) & M_FW_VIID_VIN)
+
+enum fw_vi_func {
+	FW_VI_FUNC_ETH,
+	FW_VI_FUNC_OFLD,
+	FW_VI_FUNC_IWARP,
+	FW_VI_FUNC_OPENISCSI,
+	FW_VI_FUNC_OPENFCOE,
+	FW_VI_FUNC_FOISCSI,
+	FW_VI_FUNC_FOFCOE,
+	FW_VI_FUNC_FW,
+};
+
+struct fw_vi_cmd {
+	__be32 op_to_vfn;
+	__be32 alloc_to_len16;
+	__be16 type_to_viid;
+	__u8   mac[6];
+	__u8   portid_pkd;
+	__u8   nmac;
+	__u8   nmac0[6];
+	__be16 rsssize_pkd;
+	__u8   nmac1[6];
+	__be16 idsiiq_pkd;
+	__u8   nmac2[6];
+	__be16 idseiq_pkd;
+	__u8   nmac3[6];
+	__be64 r9;
+	__be64 r10;
+};
+
+#define S_FW_VI_CMD_PFN		8
+#define M_FW_VI_CMD_PFN		0x7
+#define V_FW_VI_CMD_PFN(x)	((x) << S_FW_VI_CMD_PFN)
+#define G_FW_VI_CMD_PFN(x)	(((x) >> S_FW_VI_CMD_PFN) & M_FW_VI_CMD_PFN)
+
+#define S_FW_VI_CMD_VFN		0
+#define M_FW_VI_CMD_VFN		0xff
+#define V_FW_VI_CMD_VFN(x)	((x) << S_FW_VI_CMD_VFN)
+#define G_FW_VI_CMD_VFN(x)	(((x) >> S_FW_VI_CMD_VFN) & M_FW_VI_CMD_VFN)
+
+#define S_FW_VI_CMD_ALLOC	31
+#define M_FW_VI_CMD_ALLOC	0x1
+#define V_FW_VI_CMD_ALLOC(x)	((x) << S_FW_VI_CMD_ALLOC)
+#define G_FW_VI_CMD_ALLOC(x)	\
+    (((x) >> S_FW_VI_CMD_ALLOC) & M_FW_VI_CMD_ALLOC)
+#define F_FW_VI_CMD_ALLOC	V_FW_VI_CMD_ALLOC(1U)
+
+#define S_FW_VI_CMD_FREE	30
+#define M_FW_VI_CMD_FREE	0x1
+#define V_FW_VI_CMD_FREE(x)	((x) << S_FW_VI_CMD_FREE)
+#define G_FW_VI_CMD_FREE(x)	(((x) >> S_FW_VI_CMD_FREE) & M_FW_VI_CMD_FREE)
+#define F_FW_VI_CMD_FREE	V_FW_VI_CMD_FREE(1U)
+
+#define S_FW_VI_CMD_TYPE	15
+#define M_FW_VI_CMD_TYPE	0x1
+#define V_FW_VI_CMD_TYPE(x)	((x) << S_FW_VI_CMD_TYPE)
+#define G_FW_VI_CMD_TYPE(x)	(((x) >> S_FW_VI_CMD_TYPE) & M_FW_VI_CMD_TYPE)
+#define F_FW_VI_CMD_TYPE	V_FW_VI_CMD_TYPE(1U)
+
+#define S_FW_VI_CMD_FUNC	12
+#define M_FW_VI_CMD_FUNC	0x7
+#define V_FW_VI_CMD_FUNC(x)	((x) << S_FW_VI_CMD_FUNC)
+#define G_FW_VI_CMD_FUNC(x)	(((x) >> S_FW_VI_CMD_FUNC) & M_FW_VI_CMD_FUNC)
+
+#define S_FW_VI_CMD_VIID	0
+#define M_FW_VI_CMD_VIID	0xfff
+#define V_FW_VI_CMD_VIID(x)	((x) << S_FW_VI_CMD_VIID)
+#define G_FW_VI_CMD_VIID(x)	(((x) >> S_FW_VI_CMD_VIID) & M_FW_VI_CMD_VIID)
+
+#define S_FW_VI_CMD_PORTID	4
+#define M_FW_VI_CMD_PORTID	0xf
+#define V_FW_VI_CMD_PORTID(x)	((x) << S_FW_VI_CMD_PORTID)
+#define G_FW_VI_CMD_PORTID(x)	\
+    (((x) >> S_FW_VI_CMD_PORTID) & M_FW_VI_CMD_PORTID)
+
+#define S_FW_VI_CMD_RSSSIZE	0
+#define M_FW_VI_CMD_RSSSIZE	0x7ff
+#define V_FW_VI_CMD_RSSSIZE(x)	((x) << S_FW_VI_CMD_RSSSIZE)
+#define G_FW_VI_CMD_RSSSIZE(x)	\
+    (((x) >> S_FW_VI_CMD_RSSSIZE) & M_FW_VI_CMD_RSSSIZE)
+
+#define S_FW_VI_CMD_IDSIIQ	0
+#define M_FW_VI_CMD_IDSIIQ	0x3ff
+#define V_FW_VI_CMD_IDSIIQ(x)	((x) << S_FW_VI_CMD_IDSIIQ)
+#define G_FW_VI_CMD_IDSIIQ(x)	\
+    (((x) >> S_FW_VI_CMD_IDSIIQ) & M_FW_VI_CMD_IDSIIQ)
+
+#define S_FW_VI_CMD_IDSEIQ	0
+#define M_FW_VI_CMD_IDSEIQ	0x3ff
+#define V_FW_VI_CMD_IDSEIQ(x)	((x) << S_FW_VI_CMD_IDSEIQ)
+#define G_FW_VI_CMD_IDSEIQ(x)	\
+    (((x) >> S_FW_VI_CMD_IDSEIQ) & M_FW_VI_CMD_IDSEIQ)
+
+/* Special VI_MAC command index ids */
+#define FW_VI_MAC_ADD_MAC		0x3FF
+#define FW_VI_MAC_ADD_PERSIST_MAC	0x3FE
+#define FW_VI_MAC_MAC_BASED_FREE	0x3FD
+
+enum fw_vi_mac_smac {
+	FW_VI_MAC_MPS_TCAM_ENTRY,
+	FW_VI_MAC_MPS_TCAM_ONLY,
+	FW_VI_MAC_SMT_ONLY,
+	FW_VI_MAC_SMT_AND_MPSTCAM
+};
+
+enum fw_vi_mac_result {
+	FW_VI_MAC_R_SUCCESS,
+	FW_VI_MAC_R_F_NONEXISTENT_NOMEM,
+	FW_VI_MAC_R_SMAC_FAIL,
+	FW_VI_MAC_R_F_ACL_CHECK
+};
+
+struct fw_vi_mac_cmd {
+	__be32 op_to_viid;
+	__be32 freemacs_to_len16;
+	union fw_vi_mac {
+		struct fw_vi_mac_exact {
+			__be16 valid_to_idx;
+			__u8   macaddr[6];
+		} exact[7];
+		struct fw_vi_mac_hash {
+			__be64 hashvec;
+		} hash;
+	} u;
+};
+
+#define S_FW_VI_MAC_CMD_VIID	0
+#define M_FW_VI_MAC_CMD_VIID	0xfff
+#define V_FW_VI_MAC_CMD_VIID(x)	((x) << S_FW_VI_MAC_CMD_VIID)
+#define G_FW_VI_MAC_CMD_VIID(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_VIID) & M_FW_VI_MAC_CMD_VIID)
+
+#define S_FW_VI_MAC_CMD_FREEMACS	31
+#define M_FW_VI_MAC_CMD_FREEMACS	0x1
+#define V_FW_VI_MAC_CMD_FREEMACS(x)	((x) << S_FW_VI_MAC_CMD_FREEMACS)
+#define G_FW_VI_MAC_CMD_FREEMACS(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_FREEMACS) & M_FW_VI_MAC_CMD_FREEMACS)
+#define F_FW_VI_MAC_CMD_FREEMACS	V_FW_VI_MAC_CMD_FREEMACS(1U)
+
+#define S_FW_VI_MAC_CMD_HASHVECEN	23
+#define M_FW_VI_MAC_CMD_HASHVECEN	0x1
+#define V_FW_VI_MAC_CMD_HASHVECEN(x)	((x) << S_FW_VI_MAC_CMD_HASHVECEN)
+#define G_FW_VI_MAC_CMD_HASHVECEN(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_HASHVECEN) & M_FW_VI_MAC_CMD_HASHVECEN)
+#define F_FW_VI_MAC_CMD_HASHVECEN	V_FW_VI_MAC_CMD_HASHVECEN(1U)
+
+#define S_FW_VI_MAC_CMD_HASHUNIEN	22
+#define M_FW_VI_MAC_CMD_HASHUNIEN	0x1
+#define V_FW_VI_MAC_CMD_HASHUNIEN(x)	((x) << S_FW_VI_MAC_CMD_HASHUNIEN)
+#define G_FW_VI_MAC_CMD_HASHUNIEN(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_HASHUNIEN) & M_FW_VI_MAC_CMD_HASHUNIEN)
+#define F_FW_VI_MAC_CMD_HASHUNIEN	V_FW_VI_MAC_CMD_HASHUNIEN(1U)
+
+#define S_FW_VI_MAC_CMD_VALID		15
+#define M_FW_VI_MAC_CMD_VALID		0x1
+#define V_FW_VI_MAC_CMD_VALID(x)	((x) << S_FW_VI_MAC_CMD_VALID)
+#define G_FW_VI_MAC_CMD_VALID(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_VALID) & M_FW_VI_MAC_CMD_VALID)
+#define F_FW_VI_MAC_CMD_VALID	V_FW_VI_MAC_CMD_VALID(1U)
+
+#define S_FW_VI_MAC_CMD_PRIO	12
+#define M_FW_VI_MAC_CMD_PRIO	0x7
+#define V_FW_VI_MAC_CMD_PRIO(x)	((x) << S_FW_VI_MAC_CMD_PRIO)
+#define G_FW_VI_MAC_CMD_PRIO(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_PRIO) & M_FW_VI_MAC_CMD_PRIO)
+
+#define S_FW_VI_MAC_CMD_SMAC_RESULT	10
+#define M_FW_VI_MAC_CMD_SMAC_RESULT	0x3
+#define V_FW_VI_MAC_CMD_SMAC_RESULT(x)	((x) << S_FW_VI_MAC_CMD_SMAC_RESULT)
+#define G_FW_VI_MAC_CMD_SMAC_RESULT(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_SMAC_RESULT) & M_FW_VI_MAC_CMD_SMAC_RESULT)
+
+#define S_FW_VI_MAC_CMD_IDX	0
+#define M_FW_VI_MAC_CMD_IDX	0x3ff
+#define V_FW_VI_MAC_CMD_IDX(x)	((x) << S_FW_VI_MAC_CMD_IDX)
+#define G_FW_VI_MAC_CMD_IDX(x)	\
+    (((x) >> S_FW_VI_MAC_CMD_IDX) & M_FW_VI_MAC_CMD_IDX)
+
+/* T4 max MTU supported */
+#define T4_MAX_MTU_SUPPORTED	9600
+#define FW_RXMODE_MTU_NO_CHG	65535
+
+struct fw_vi_rxmode_cmd {
+	__be32 op_to_viid;
+	__be32 retval_len16;
+	__be32 mtu_to_vlanexen;
+	__be32 r4_lo;
+};
+
+#define S_FW_VI_RXMODE_CMD_VIID		0
+#define M_FW_VI_RXMODE_CMD_VIID		0xfff
+#define V_FW_VI_RXMODE_CMD_VIID(x)	((x) << S_FW_VI_RXMODE_CMD_VIID)
+#define G_FW_VI_RXMODE_CMD_VIID(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_VIID) & M_FW_VI_RXMODE_CMD_VIID)
+
+#define S_FW_VI_RXMODE_CMD_MTU		16
+#define M_FW_VI_RXMODE_CMD_MTU		0xffff
+#define V_FW_VI_RXMODE_CMD_MTU(x)	((x) << S_FW_VI_RXMODE_CMD_MTU)
+#define G_FW_VI_RXMODE_CMD_MTU(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_MTU) & M_FW_VI_RXMODE_CMD_MTU)
+
+#define S_FW_VI_RXMODE_CMD_PROMISCEN	14
+#define M_FW_VI_RXMODE_CMD_PROMISCEN	0x3
+#define V_FW_VI_RXMODE_CMD_PROMISCEN(x)	((x) << S_FW_VI_RXMODE_CMD_PROMISCEN)
+#define G_FW_VI_RXMODE_CMD_PROMISCEN(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_PROMISCEN) & M_FW_VI_RXMODE_CMD_PROMISCEN)
+
+#define S_FW_VI_RXMODE_CMD_ALLMULTIEN		12
+#define M_FW_VI_RXMODE_CMD_ALLMULTIEN		0x3
+#define V_FW_VI_RXMODE_CMD_ALLMULTIEN(x)	\
+    ((x) << S_FW_VI_RXMODE_CMD_ALLMULTIEN)
+#define G_FW_VI_RXMODE_CMD_ALLMULTIEN(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_ALLMULTIEN) & M_FW_VI_RXMODE_CMD_ALLMULTIEN)
+
+#define S_FW_VI_RXMODE_CMD_BROADCASTEN		10
+#define M_FW_VI_RXMODE_CMD_BROADCASTEN		0x3
+#define V_FW_VI_RXMODE_CMD_BROADCASTEN(x)	\
+    ((x) << S_FW_VI_RXMODE_CMD_BROADCASTEN)
+#define G_FW_VI_RXMODE_CMD_BROADCASTEN(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_BROADCASTEN) & M_FW_VI_RXMODE_CMD_BROADCASTEN)
+
+#define S_FW_VI_RXMODE_CMD_VLANEXEN	8
+#define M_FW_VI_RXMODE_CMD_VLANEXEN	0x3
+#define V_FW_VI_RXMODE_CMD_VLANEXEN(x)	((x) << S_FW_VI_RXMODE_CMD_VLANEXEN)
+#define G_FW_VI_RXMODE_CMD_VLANEXEN(x)	\
+    (((x) >> S_FW_VI_RXMODE_CMD_VLANEXEN) & M_FW_VI_RXMODE_CMD_VLANEXEN)
+
+struct fw_vi_enable_cmd {
+	__be32 op_to_viid;
+	__be32 ien_to_len16;
+	__be16 blinkdur;
+	__be16 r3;
+	__be32 r4;
+};
+
+#define S_FW_VI_ENABLE_CMD_VIID		0
+#define M_FW_VI_ENABLE_CMD_VIID		0xfff
+#define V_FW_VI_ENABLE_CMD_VIID(x)	((x) << S_FW_VI_ENABLE_CMD_VIID)
+#define G_FW_VI_ENABLE_CMD_VIID(x)	\
+    (((x) >> S_FW_VI_ENABLE_CMD_VIID) & M_FW_VI_ENABLE_CMD_VIID)
+
+#define S_FW_VI_ENABLE_CMD_IEN		31
+#define M_FW_VI_ENABLE_CMD_IEN		0x1
+#define V_FW_VI_ENABLE_CMD_IEN(x)	((x) << S_FW_VI_ENABLE_CMD_IEN)
+#define G_FW_VI_ENABLE_CMD_IEN(x)	\
+    (((x) >> S_FW_VI_ENABLE_CMD_IEN) & M_FW_VI_ENABLE_CMD_IEN)
+#define F_FW_VI_ENABLE_CMD_IEN	V_FW_VI_ENABLE_CMD_IEN(1U)
+
+#define S_FW_VI_ENABLE_CMD_EEN		30
+#define M_FW_VI_ENABLE_CMD_EEN		0x1
+#define V_FW_VI_ENABLE_CMD_EEN(x)	((x) << S_FW_VI_ENABLE_CMD_EEN)
+#define G_FW_VI_ENABLE_CMD_EEN(x)	\
+    (((x) >> S_FW_VI_ENABLE_CMD_EEN) & M_FW_VI_ENABLE_CMD_EEN)
+#define F_FW_VI_ENABLE_CMD_EEN	V_FW_VI_ENABLE_CMD_EEN(1U)
+
+#define S_FW_VI_ENABLE_CMD_LED		29
+#define M_FW_VI_ENABLE_CMD_LED		0x1
+#define V_FW_VI_ENABLE_CMD_LED(x)	((x) << S_FW_VI_ENABLE_CMD_LED)
+#define G_FW_VI_ENABLE_CMD_LED(x)	\
+    (((x) >> S_FW_VI_ENABLE_CMD_LED) & M_FW_VI_ENABLE_CMD_LED)
+#define F_FW_VI_ENABLE_CMD_LED	V_FW_VI_ENABLE_CMD_LED(1U)
+
+/* VI VF stats offset definitions */
+#define VI_VF_NUM_STATS	16
+enum fw_vi_stats_vf_index {
+	FW_VI_VF_STAT_TX_BCAST_BYTES_IX,
+	FW_VI_VF_STAT_TX_BCAST_FRAMES_IX,
+	FW_VI_VF_STAT_TX_MCAST_BYTES_IX,
+	FW_VI_VF_STAT_TX_MCAST_FRAMES_IX,
+	FW_VI_VF_STAT_TX_UCAST_BYTES_IX,
+	FW_VI_VF_STAT_TX_UCAST_FRAMES_IX,
+	FW_VI_VF_STAT_TX_DROP_FRAMES_IX,
+	FW_VI_VF_STAT_TX_OFLD_BYTES_IX,
+	FW_VI_VF_STAT_TX_OFLD_FRAMES_IX,
+	FW_VI_VF_STAT_RX_BCAST_BYTES_IX,
+	FW_VI_VF_STAT_RX_BCAST_FRAMES_IX,
+	FW_VI_VF_STAT_RX_MCAST_BYTES_IX,
+	FW_VI_VF_STAT_RX_MCAST_FRAMES_IX,
+	FW_VI_VF_STAT_RX_UCAST_BYTES_IX,
+	FW_VI_VF_STAT_RX_UCAST_FRAMES_IX,
+	FW_VI_VF_STAT_RX_ERR_FRAMES_IX
+};
+
+/* VI PF stats offset definitions */
+#define VI_PF_NUM_STATS	17
+enum fw_vi_stats_pf_index {
+	FW_VI_PF_STAT_TX_BCAST_BYTES_IX,
+	FW_VI_PF_STAT_TX_BCAST_FRAMES_IX,
+	FW_VI_PF_STAT_TX_MCAST_BYTES_IX,
+	FW_VI_PF_STAT_TX_MCAST_FRAMES_IX,
+	FW_VI_PF_STAT_TX_UCAST_BYTES_IX,
+	FW_VI_PF_STAT_TX_UCAST_FRAMES_IX,
+	FW_VI_PF_STAT_TX_OFLD_BYTES_IX,
+	FW_VI_PF_STAT_TX_OFLD_FRAMES_IX,
+	FW_VI_PF_STAT_RX_BYTES_IX,
+	FW_VI_PF_STAT_RX_FRAMES_IX,
+	FW_VI_PF_STAT_RX_BCAST_BYTES_IX,
+	FW_VI_PF_STAT_RX_BCAST_FRAMES_IX,
+	FW_VI_PF_STAT_RX_MCAST_BYTES_IX,
+	FW_VI_PF_STAT_RX_MCAST_FRAMES_IX,
+	FW_VI_PF_STAT_RX_UCAST_BYTES_IX,
+	FW_VI_PF_STAT_RX_UCAST_FRAMES_IX,
+	FW_VI_PF_STAT_RX_ERR_FRAMES_IX
+};
+
+struct fw_vi_stats_cmd {
+	__be32 op_to_viid;
+	__be32 retval_len16;
+	union fw_vi_stats {
+		struct fw_vi_stats_ctl {
+			__be16 nstats_ix;
+			__be16 r6;
+			__be32 r7;
+			__be64 stat0;
+			__be64 stat1;
+			__be64 stat2;
+			__be64 stat3;
+			__be64 stat4;
+			__be64 stat5;
+		} ctl;
+		struct fw_vi_stats_pf {
+			__be64 tx_bcast_bytes;
+			__be64 tx_bcast_frames;
+			__be64 tx_mcast_bytes;
+			__be64 tx_mcast_frames;
+			__be64 tx_ucast_bytes;
+			__be64 tx_ucast_frames;
+			__be64 tx_offload_bytes;
+			__be64 tx_offload_frames;
+			__be64 rx_pf_bytes;
+			__be64 rx_pf_frames;
+			__be64 rx_bcast_bytes;
+			__be64 rx_bcast_frames;
+			__be64 rx_mcast_bytes;
+			__be64 rx_mcast_frames;
+			__be64 rx_ucast_bytes;
+			__be64 rx_ucast_frames;
+			__be64 rx_err_frames;
+		} pf;
+		struct fw_vi_stats_vf {
+			__be64 tx_bcast_bytes;
+			__be64 tx_bcast_frames;
+			__be64 tx_mcast_bytes;
+			__be64 tx_mcast_frames;
+			__be64 tx_ucast_bytes;
+			__be64 tx_ucast_frames;
+			__be64 tx_drop_frames;
+			__be64 tx_offload_bytes;
+			__be64 tx_offload_frames;
+			__be64 rx_bcast_bytes;
+			__be64 rx_bcast_frames;
+			__be64 rx_mcast_bytes;
+			__be64 rx_mcast_frames;
+			__be64 rx_ucast_bytes;
+			__be64 rx_ucast_frames;
+			__be64 rx_err_frames;
+		} vf;
+	} u;
+};
+
+#define S_FW_VI_STATS_CMD_VIID		0
+#define M_FW_VI_STATS_CMD_VIID		0xfff
+#define V_FW_VI_STATS_CMD_VIID(x)	((x) << S_FW_VI_STATS_CMD_VIID)
+#define G_FW_VI_STATS_CMD_VIID(x)	\
+    (((x) >> S_FW_VI_STATS_CMD_VIID) & M_FW_VI_STATS_CMD_VIID)
+
+#define S_FW_VI_STATS_CMD_NSTATS	12
+#define M_FW_VI_STATS_CMD_NSTATS	0x7
+#define V_FW_VI_STATS_CMD_NSTATS(x)	((x) << S_FW_VI_STATS_CMD_NSTATS)
+#define G_FW_VI_STATS_CMD_NSTATS(x)	\
+    (((x) >> S_FW_VI_STATS_CMD_NSTATS) & M_FW_VI_STATS_CMD_NSTATS)
+
+#define S_FW_VI_STATS_CMD_IX	0
+#define M_FW_VI_STATS_CMD_IX	0x1f
+#define V_FW_VI_STATS_CMD_IX(x)	((x) << S_FW_VI_STATS_CMD_IX)
+#define G_FW_VI_STATS_CMD_IX(x)	\
+    (((x) >> S_FW_VI_STATS_CMD_IX) & M_FW_VI_STATS_CMD_IX)
+
+struct fw_acl_mac_cmd {
+	__be32 op_to_vfn;
+	__be32 en_to_len16;
+	__u8   nmac;
+	__u8   r3[7];
+	__be16 r4;
+	__u8   macaddr0[6];
+	__be16 r5;
+	__u8   macaddr1[6];
+	__be16 r6;
+	__u8   macaddr2[6];
+	__be16 r7;
+	__u8   macaddr3[6];
+};
+
+#define S_FW_ACL_MAC_CMD_PFN	8
+#define M_FW_ACL_MAC_CMD_PFN	0x7
+#define V_FW_ACL_MAC_CMD_PFN(x)	((x) << S_FW_ACL_MAC_CMD_PFN)
+#define G_FW_ACL_MAC_CMD_PFN(x)	\
+    (((x) >> S_FW_ACL_MAC_CMD_PFN) & M_FW_ACL_MAC_CMD_PFN)
+
+#define S_FW_ACL_MAC_CMD_VFN	0
+#define M_FW_ACL_MAC_CMD_VFN	0xff
+#define V_FW_ACL_MAC_CMD_VFN(x)	((x) << S_FW_ACL_MAC_CMD_VFN)
+#define G_FW_ACL_MAC_CMD_VFN(x)	\
+    (((x) >> S_FW_ACL_MAC_CMD_VFN) & M_FW_ACL_MAC_CMD_VFN)
+
+#define S_FW_ACL_MAC_CMD_EN	31
+#define M_FW_ACL_MAC_CMD_EN	0x1
+#define V_FW_ACL_MAC_CMD_EN(x)	((x) << S_FW_ACL_MAC_CMD_EN)
+#define G_FW_ACL_MAC_CMD_EN(x)	\
+    (((x) >> S_FW_ACL_MAC_CMD_EN) & M_FW_ACL_MAC_CMD_EN)
+#define F_FW_ACL_MAC_CMD_EN	V_FW_ACL_MAC_CMD_EN(1U)
+
+struct fw_acl_vlan_cmd {
+	__be32 op_to_vfn;
+	__be32 en_to_len16;
+	__u8   nvlan;
+	__u8   dropnovlan_fm;
+	__u8   r3_lo[6];
+	__be16 vlanid[16];
+};
+
+#define S_FW_ACL_VLAN_CMD_PFN		8
+#define M_FW_ACL_VLAN_CMD_PFN		0x7
+#define V_FW_ACL_VLAN_CMD_PFN(x)	((x) << S_FW_ACL_VLAN_CMD_PFN)
+#define G_FW_ACL_VLAN_CMD_PFN(x)	\
+    (((x) >> S_FW_ACL_VLAN_CMD_PFN) & M_FW_ACL_VLAN_CMD_PFN)
+
+#define S_FW_ACL_VLAN_CMD_VFN		0
+#define M_FW_ACL_VLAN_CMD_VFN		0xff
+#define V_FW_ACL_VLAN_CMD_VFN(x)	((x) << S_FW_ACL_VLAN_CMD_VFN)
+#define G_FW_ACL_VLAN_CMD_VFN(x)	\
+    (((x) >> S_FW_ACL_VLAN_CMD_VFN) & M_FW_ACL_VLAN_CMD_VFN)
+
+#define S_FW_ACL_VLAN_CMD_EN	31
+#define M_FW_ACL_VLAN_CMD_EN	0x1
+#define V_FW_ACL_VLAN_CMD_EN(x)	((x) << S_FW_ACL_VLAN_CMD_EN)
+#define G_FW_ACL_VLAN_CMD_EN(x)	\
+    (((x) >> S_FW_ACL_VLAN_CMD_EN) & M_FW_ACL_VLAN_CMD_EN)
+#define F_FW_ACL_VLAN_CMD_EN	V_FW_ACL_VLAN_CMD_EN(1U)
+
+#define S_FW_ACL_VLAN_CMD_DROPNOVLAN	7
+#define M_FW_ACL_VLAN_CMD_DROPNOVLAN	0x1
+#define V_FW_ACL_VLAN_CMD_DROPNOVLAN(x)	((x) << S_FW_ACL_VLAN_CMD_DROPNOVLAN)
+#define G_FW_ACL_VLAN_CMD_DROPNOVLAN(x)	\
+    (((x) >> S_FW_ACL_VLAN_CMD_DROPNOVLAN) & M_FW_ACL_VLAN_CMD_DROPNOVLAN)
+#define F_FW_ACL_VLAN_CMD_DROPNOVLAN	V_FW_ACL_VLAN_CMD_DROPNOVLAN(1U)
+
+#define S_FW_ACL_VLAN_CMD_FM	6
+#define M_FW_ACL_VLAN_CMD_FM	0x1
+#define V_FW_ACL_VLAN_CMD_FM(x)	((x) << S_FW_ACL_VLAN_CMD_FM)
+#define G_FW_ACL_VLAN_CMD_FM(x)	\
+    (((x) >> S_FW_ACL_VLAN_CMD_FM) & M_FW_ACL_VLAN_CMD_FM)
+#define F_FW_ACL_VLAN_CMD_FM	V_FW_ACL_VLAN_CMD_FM(1U)
+
+/* port capabilities bitmap */
+enum fw_port_cap {
+	FW_PORT_CAP_SPEED_100M		= 0x0001,
+	FW_PORT_CAP_SPEED_1G		= 0x0002,
+	FW_PORT_CAP_SPEED_2_5G		= 0x0004,
+	FW_PORT_CAP_SPEED_10G		= 0x0008,
+	FW_PORT_CAP_SPEED_40G		= 0x0010,
+	FW_PORT_CAP_SPEED_100G		= 0x0020,
+	FW_PORT_CAP_FC_RX		= 0x0040,
+	FW_PORT_CAP_FC_TX		= 0x0080,
+	FW_PORT_CAP_ANEG		= 0x0100,
+	FW_PORT_CAP_MDIX		= 0x0200,
+	FW_PORT_CAP_MDIAUTO		= 0x0400,
+	FW_PORT_CAP_FEC			= 0x0800,
+	FW_PORT_CAP_TECHKR		= 0x1000,
+	FW_PORT_CAP_TECHKX4		= 0x2000,
+};
+
+#define S_FW_PORT_AUXLINFO_MDI		3
+#define M_FW_PORT_AUXLINFO_MDI		0x3
+#define V_FW_PORT_AUXLINFO_MDI(x)	((x) << S_FW_PORT_AUXLINFO_MDI)
+#define G_FW_PORT_AUXLINFO_MDI(x) \
+    (((x) >> S_FW_PORT_AUXLINFO_MDI) & M_FW_PORT_AUXLINFO_MDI)
+
+#define S_FW_PORT_AUXLINFO_KX4		2
+#define M_FW_PORT_AUXLINFO_KX4		0x1
+#define V_FW_PORT_AUXLINFO_KX4(x)	((x) << S_FW_PORT_AUXLINFO_KX4)
+#define G_FW_PORT_AUXLINFO_KX4(x) \
+    (((x) >> S_FW_PORT_AUXLINFO_KX4) & M_FW_PORT_AUXLINFO_KX4)
+#define F_FW_PORT_AUXLINFO_KX4		V_FW_PORT_AUXLINFO_KX4(1U)
+
+#define S_FW_PORT_AUXLINFO_KR		1
+#define M_FW_PORT_AUXLINFO_KR		0x1
+#define V_FW_PORT_AUXLINFO_KR(x)	((x) << S_FW_PORT_AUXLINFO_KR)
+#define G_FW_PORT_AUXLINFO_KR(x) \
+    (((x) >> S_FW_PORT_AUXLINFO_KR) & M_FW_PORT_AUXLINFO_KR)
+#define F_FW_PORT_AUXLINFO_KR		V_FW_PORT_AUXLINFO_KR(1U)
+
+#define S_FW_PORT_AUXLINFO_FEC		0
+#define M_FW_PORT_AUXLINFO_FEC		0x1
+#define V_FW_PORT_AUXLINFO_FEC(x)	((x) << S_FW_PORT_AUXLINFO_FEC)
+#define G_FW_PORT_AUXLINFO_FEC(x) \
+    (((x) >> S_FW_PORT_AUXLINFO_FEC) & M_FW_PORT_AUXLINFO_FEC) 
+#define F_FW_PORT_AUXLINFO_FEC		V_FW_PORT_AUXLINFO_FEC(1U)
+
+#define S_FW_PORT_RCAP_AUX	11
+#define M_FW_PORT_RCAP_AUX	0x7
+#define V_FW_PORT_RCAP_AUX(x)	((x) << S_FW_PORT_RCAP_AUX)
+#define G_FW_PORT_RCAP_AUX(x) \
+    (((x) >> S_FW_PORT_RCAP_AUX) & M_FW_PORT_RCAP_AUX)
+
+#define S_FW_PORT_CAP_SPEED	0
+#define M_FW_PORT_CAP_SPEED	0x3f
+#define V_FW_PORT_CAP_SPEED(x)	((x) << S_FW_PORT_CAP_SPEED)
+#define G_FW_PORT_CAP_SPEED(x) \
+    (((x) >> S_FW_PORT_CAP_SPEED) & M_FW_PORT_CAP_SPEED)
+
+#define S_FW_PORT_CAP_FC	6
+#define M_FW_PORT_CAP_FC	0x3
+#define V_FW_PORT_CAP_FC(x)	((x) << S_FW_PORT_CAP_FC)
+#define G_FW_PORT_CAP_FC(x) \
+    (((x) >> S_FW_PORT_CAP_FC) & M_FW_PORT_CAP_FC)
+
+#define S_FW_PORT_CAP_ANEG	8
+#define M_FW_PORT_CAP_ANEG	0x1
+#define V_FW_PORT_CAP_ANEG(x)	((x) << S_FW_PORT_CAP_ANEG)
+#define G_FW_PORT_CAP_ANEG(x) \
+    (((x) >> S_FW_PORT_CAP_ANEG) & M_FW_PORT_CAP_ANEG)
+
+enum fw_port_mdi {
+	FW_PORT_CAP_MDI_UNCHANGED,
+	FW_PORT_CAP_MDI_AUTO,
+	FW_PORT_CAP_MDI_F_STRAIGHT,
+	FW_PORT_CAP_MDI_F_CROSSOVER
+};
+
+#define S_FW_PORT_CAP_MDI 9
+#define M_FW_PORT_CAP_MDI 3
+#define V_FW_PORT_CAP_MDI(x) ((x) << S_FW_PORT_CAP_MDI)
+#define G_FW_PORT_CAP_MDI(x) (((x) >> S_FW_PORT_CAP_MDI) & M_FW_PORT_CAP_MDI)
+
+enum fw_port_action {
+	FW_PORT_ACTION_L1_CFG		= 0x0001,
+	FW_PORT_ACTION_L2_CFG		= 0x0002,
+	FW_PORT_ACTION_GET_PORT_INFO	= 0x0003,
+	FW_PORT_ACTION_L2_PPP_CFG	= 0x0004,
+	FW_PORT_ACTION_L2_DCB_CFG	= 0x0005,
+	FW_PORT_ACTION_LOW_PWR_TO_NORMAL = 0x0010,
+	FW_PORT_ACTION_L1_LOW_PWR_EN	= 0x0011,
+	FW_PORT_ACTION_L2_WOL_MODE_EN	= 0x0012,
+	FW_PORT_ACTION_LPBK_TO_NORMAL	= 0x0020,
+	FW_PORT_ACTION_L1_SS_LPBK_ASIC	= 0x0021,
+	FW_PORT_ACTION_MAC_LPBK		= 0x0022,
+	FW_PORT_ACTION_L1_WS_LPBK_ASIC	= 0x0023,
+	FW_PORT_ACTION_L1_EXT_LPBK      = 0x0026,
+	FW_PORT_ACTION_PCS_LPBK		= 0x0028,
+	FW_PORT_ACTION_PHY_RESET	= 0x0040,
+	FW_PORT_ACTION_PMA_RESET	= 0x0041,
+	FW_PORT_ACTION_PCS_RESET	= 0x0042,
+	FW_PORT_ACTION_PHYXS_RESET	= 0x0043,
+	FW_PORT_ACTION_DTEXS_REEST	= 0x0044,
+	FW_PORT_ACTION_AN_RESET		= 0x0045
+};
+
+enum fw_port_l2cfg_ctlbf {
+	FW_PORT_L2_CTLBF_OVLAN0	= 0x01,
+	FW_PORT_L2_CTLBF_OVLAN1	= 0x02,
+	FW_PORT_L2_CTLBF_OVLAN2	= 0x04,
+	FW_PORT_L2_CTLBF_OVLAN3	= 0x08,
+	FW_PORT_L2_CTLBF_IVLAN	= 0x10,
+	FW_PORT_L2_CTLBF_TXIPG	= 0x20,
+	FW_PORT_L2_CTLBF_MTU	= 0x40
+};
+
+enum fw_port_dcb_cfg {
+	FW_PORT_DCB_CFG_PG	= 0x01,
+	FW_PORT_DCB_CFG_PFC	= 0x02,
+	FW_PORT_DCB_CFG_APPL	= 0x04
+};
+
+enum fw_port_dcb_cfg_rc {
+	FW_PORT_DCB_CFG_SUCCESS	= 0x0,
+	FW_PORT_DCB_CFG_ERROR	= 0x1
+};
+
+enum fw_port_dcb_type {
+	FW_PORT_DCB_TYPE_PGID		= 0x00,
+	FW_PORT_DCB_TYPE_PGRATE		= 0x01,
+	FW_PORT_DCB_TYPE_PRIORATE	= 0x02,
+	FW_PORT_DCB_TYPE_PFC		= 0x03,
+	FW_PORT_DCB_TYPE_APP_ID		= 0x04,
+};
+
+struct fw_port_cmd {
+	__be32 op_to_portid;
+	__be32 action_to_len16;
+	union fw_port {
+		struct fw_port_l1cfg {
+			__be32 rcap;
+			__be32 r;
+		} l1cfg;
+		struct fw_port_l2cfg {
+			__u8   ctlbf;
+			__u8   ovlan3_to_ivlan0;
+			__be16 ivlantype;
+			__be16 txipg_force_pinfo;
+			__be16 mtu;
+			__be16 ovlan0mask;
+			__be16 ovlan0type;
+			__be16 ovlan1mask;
+			__be16 ovlan1type;
+			__be16 ovlan2mask;
+			__be16 ovlan2type;
+			__be16 ovlan3mask;
+			__be16 ovlan3type;
+		} l2cfg;
+		struct fw_port_info {
+			__be32 lstatus_to_modtype;
+			__be16 pcap;
+			__be16 acap;
+			__be16 mtu;
+			__u8   cbllen;
+			__u8   auxlinfo;
+			__be32 r8;
+			__be64 r9;
+		} info;
+		union fw_port_dcb {
+			struct fw_port_dcb_pgid {
+				__u8   type;
+				__u8   apply_pkd;
+				__u8   r10_lo[2];
+				__be32 pgid;
+				__be64 r11;
+			} pgid;
+			struct fw_port_dcb_pgrate {
+				__u8   type;
+				__u8   apply_pkd;
+				__u8   r10_lo[5];
+				__u8   num_tcs_supported;
+				__u8   pgrate[8];
+			} pgrate;
+			struct fw_port_dcb_priorate {
+				__u8   type;
+				__u8   apply_pkd;
+				__u8   r10_lo[6];
+				__u8   strict_priorate[8];
+			} priorate;
+			struct fw_port_dcb_pfc {
+				__u8   type;
+				__u8   pfcen;
+				__be16 r10[3];
+				__be64 r11;
+			} pfc;
+			struct fw_port_app_priority {
+				__u8   type;
+				__u8   r10[2];
+				__u8   idx;
+				__u8   user_prio_map;
+				__u8   sel_field;
+				__be16 protocolid;
+				__be64 r12;
+			} app_priority;
+		} dcb;
+	} u;
+};
+
+#define S_FW_PORT_CMD_READ	22
+#define M_FW_PORT_CMD_READ	0x1
+#define V_FW_PORT_CMD_READ(x)	((x) << S_FW_PORT_CMD_READ)
+#define G_FW_PORT_CMD_READ(x)	\
+    (((x) >> S_FW_PORT_CMD_READ) & M_FW_PORT_CMD_READ)
+#define F_FW_PORT_CMD_READ	V_FW_PORT_CMD_READ(1U)
+
+#define S_FW_PORT_CMD_PORTID	0
+#define M_FW_PORT_CMD_PORTID	0xf
+#define V_FW_PORT_CMD_PORTID(x)	((x) << S_FW_PORT_CMD_PORTID)
+#define G_FW_PORT_CMD_PORTID(x)	\
+    (((x) >> S_FW_PORT_CMD_PORTID) & M_FW_PORT_CMD_PORTID)
+
+#define S_FW_PORT_CMD_ACTION	16
+#define M_FW_PORT_CMD_ACTION	0xffff
+#define V_FW_PORT_CMD_ACTION(x)	((x) << S_FW_PORT_CMD_ACTION)
+#define G_FW_PORT_CMD_ACTION(x)	\
+    (((x) >> S_FW_PORT_CMD_ACTION) & M_FW_PORT_CMD_ACTION)
+
+#define S_FW_PORT_CMD_OVLAN3	7
+#define M_FW_PORT_CMD_OVLAN3	0x1
+#define V_FW_PORT_CMD_OVLAN3(x)	((x) << S_FW_PORT_CMD_OVLAN3)
+#define G_FW_PORT_CMD_OVLAN3(x)	\
+    (((x) >> S_FW_PORT_CMD_OVLAN3) & M_FW_PORT_CMD_OVLAN3)
+#define F_FW_PORT_CMD_OVLAN3	V_FW_PORT_CMD_OVLAN3(1U)
+
+#define S_FW_PORT_CMD_OVLAN2	6
+#define M_FW_PORT_CMD_OVLAN2	0x1
+#define V_FW_PORT_CMD_OVLAN2(x)	((x) << S_FW_PORT_CMD_OVLAN2)
+#define G_FW_PORT_CMD_OVLAN2(x)	\
+    (((x) >> S_FW_PORT_CMD_OVLAN2) & M_FW_PORT_CMD_OVLAN2)
+#define F_FW_PORT_CMD_OVLAN2	V_FW_PORT_CMD_OVLAN2(1U)
+
+#define S_FW_PORT_CMD_OVLAN1	5
+#define M_FW_PORT_CMD_OVLAN1	0x1
+#define V_FW_PORT_CMD_OVLAN1(x)	((x) << S_FW_PORT_CMD_OVLAN1)
+#define G_FW_PORT_CMD_OVLAN1(x)	\
+    (((x) >> S_FW_PORT_CMD_OVLAN1) & M_FW_PORT_CMD_OVLAN1)
+#define F_FW_PORT_CMD_OVLAN1	V_FW_PORT_CMD_OVLAN1(1U)
+
+#define S_FW_PORT_CMD_OVLAN0	4
+#define M_FW_PORT_CMD_OVLAN0	0x1
+#define V_FW_PORT_CMD_OVLAN0(x)	((x) << S_FW_PORT_CMD_OVLAN0)
+#define G_FW_PORT_CMD_OVLAN0(x)	\
+    (((x) >> S_FW_PORT_CMD_OVLAN0) & M_FW_PORT_CMD_OVLAN0)
+#define F_FW_PORT_CMD_OVLAN0	V_FW_PORT_CMD_OVLAN0(1U)
+
+#define S_FW_PORT_CMD_IVLAN0	3
+#define M_FW_PORT_CMD_IVLAN0	0x1
+#define V_FW_PORT_CMD_IVLAN0(x)	((x) << S_FW_PORT_CMD_IVLAN0)
+#define G_FW_PORT_CMD_IVLAN0(x)	\
+    (((x) >> S_FW_PORT_CMD_IVLAN0) & M_FW_PORT_CMD_IVLAN0)
+#define F_FW_PORT_CMD_IVLAN0	V_FW_PORT_CMD_IVLAN0(1U)
+
+#define S_FW_PORT_CMD_TXIPG	3
+#define M_FW_PORT_CMD_TXIPG	0x1fff
+#define V_FW_PORT_CMD_TXIPG(x)	((x) << S_FW_PORT_CMD_TXIPG)
+#define G_FW_PORT_CMD_TXIPG(x)	\
+    (((x) >> S_FW_PORT_CMD_TXIPG) & M_FW_PORT_CMD_TXIPG)
+
+#define S_FW_PORT_CMD_FORCE_PINFO	0
+#define M_FW_PORT_CMD_FORCE_PINFO	0x1
+#define V_FW_PORT_CMD_FORCE_PINFO(x)	((x) << S_FW_PORT_CMD_FORCE_PINFO)
+#define G_FW_PORT_CMD_FORCE_PINFO(x)	\
+    (((x) >> S_FW_PORT_CMD_FORCE_PINFO) & M_FW_PORT_CMD_FORCE_PINFO)
+#define F_FW_PORT_CMD_FORCE_PINFO	V_FW_PORT_CMD_FORCE_PINFO(1U)
+
+#define S_FW_PORT_CMD_LSTATUS		31
+#define M_FW_PORT_CMD_LSTATUS		0x1
+#define V_FW_PORT_CMD_LSTATUS(x)	((x) << S_FW_PORT_CMD_LSTATUS)
+#define G_FW_PORT_CMD_LSTATUS(x)	\
+    (((x) >> S_FW_PORT_CMD_LSTATUS) & M_FW_PORT_CMD_LSTATUS)
+#define F_FW_PORT_CMD_LSTATUS	V_FW_PORT_CMD_LSTATUS(1U)
+
+#define S_FW_PORT_CMD_LSPEED	24
+#define M_FW_PORT_CMD_LSPEED	0x3f
+#define V_FW_PORT_CMD_LSPEED(x)	((x) << S_FW_PORT_CMD_LSPEED)
+#define G_FW_PORT_CMD_LSPEED(x)	\
+    (((x) >> S_FW_PORT_CMD_LSPEED) & M_FW_PORT_CMD_LSPEED)
+
+#define S_FW_PORT_CMD_TXPAUSE		23
+#define M_FW_PORT_CMD_TXPAUSE		0x1
+#define V_FW_PORT_CMD_TXPAUSE(x)	((x) << S_FW_PORT_CMD_TXPAUSE)
+#define G_FW_PORT_CMD_TXPAUSE(x)	\
+    (((x) >> S_FW_PORT_CMD_TXPAUSE) & M_FW_PORT_CMD_TXPAUSE)
+#define F_FW_PORT_CMD_TXPAUSE	V_FW_PORT_CMD_TXPAUSE(1U)
+
+#define S_FW_PORT_CMD_RXPAUSE		22
+#define M_FW_PORT_CMD_RXPAUSE		0x1
+#define V_FW_PORT_CMD_RXPAUSE(x)	((x) << S_FW_PORT_CMD_RXPAUSE)
+#define G_FW_PORT_CMD_RXPAUSE(x)	\
+    (((x) >> S_FW_PORT_CMD_RXPAUSE) & M_FW_PORT_CMD_RXPAUSE)
+#define F_FW_PORT_CMD_RXPAUSE	V_FW_PORT_CMD_RXPAUSE(1U)
+
+#define S_FW_PORT_CMD_MDIOCAP		21
+#define M_FW_PORT_CMD_MDIOCAP		0x1
+#define V_FW_PORT_CMD_MDIOCAP(x)	((x) << S_FW_PORT_CMD_MDIOCAP)
+#define G_FW_PORT_CMD_MDIOCAP(x)	\
+    (((x) >> S_FW_PORT_CMD_MDIOCAP) & M_FW_PORT_CMD_MDIOCAP)
+#define F_FW_PORT_CMD_MDIOCAP	V_FW_PORT_CMD_MDIOCAP(1U)
+
+#define S_FW_PORT_CMD_MDIOADDR		16
+#define M_FW_PORT_CMD_MDIOADDR		0x1f
+#define V_FW_PORT_CMD_MDIOADDR(x)	((x) << S_FW_PORT_CMD_MDIOADDR)
+#define G_FW_PORT_CMD_MDIOADDR(x)	\
+    (((x) >> S_FW_PORT_CMD_MDIOADDR) & M_FW_PORT_CMD_MDIOADDR)
+
+#define S_FW_PORT_CMD_LPTXPAUSE		15
+#define M_FW_PORT_CMD_LPTXPAUSE		0x1
+#define V_FW_PORT_CMD_LPTXPAUSE(x)	((x) << S_FW_PORT_CMD_LPTXPAUSE)
+#define G_FW_PORT_CMD_LPTXPAUSE(x)	\
+    (((x) >> S_FW_PORT_CMD_LPTXPAUSE) & M_FW_PORT_CMD_LPTXPAUSE)
+#define F_FW_PORT_CMD_LPTXPAUSE	V_FW_PORT_CMD_LPTXPAUSE(1U)
+
+#define S_FW_PORT_CMD_LPRXPAUSE		14
+#define M_FW_PORT_CMD_LPRXPAUSE		0x1
+#define V_FW_PORT_CMD_LPRXPAUSE(x)	((x) << S_FW_PORT_CMD_LPRXPAUSE)
+#define G_FW_PORT_CMD_LPRXPAUSE(x)	\
+    (((x) >> S_FW_PORT_CMD_LPRXPAUSE) & M_FW_PORT_CMD_LPRXPAUSE)
+#define F_FW_PORT_CMD_LPRXPAUSE	V_FW_PORT_CMD_LPRXPAUSE(1U)
+
+#define S_FW_PORT_CMD_PTYPE	8
+#define M_FW_PORT_CMD_PTYPE	0x1f
+#define V_FW_PORT_CMD_PTYPE(x)	((x) << S_FW_PORT_CMD_PTYPE)
+#define G_FW_PORT_CMD_PTYPE(x)	\
+    (((x) >> S_FW_PORT_CMD_PTYPE) & M_FW_PORT_CMD_PTYPE)
+
+#define S_FW_PORT_CMD_LINKDNRC		5
+#define M_FW_PORT_CMD_LINKDNRC		0x7
+#define V_FW_PORT_CMD_LINKDNRC(x)	((x) << S_FW_PORT_CMD_LINKDNRC)
+#define G_FW_PORT_CMD_LINKDNRC(x)	\
+    (((x) >> S_FW_PORT_CMD_LINKDNRC) & M_FW_PORT_CMD_LINKDNRC)
+
+#define S_FW_PORT_CMD_MODTYPE		0
+#define M_FW_PORT_CMD_MODTYPE		0x1f
+#define V_FW_PORT_CMD_MODTYPE(x)	((x) << S_FW_PORT_CMD_MODTYPE)
+#define G_FW_PORT_CMD_MODTYPE(x)	\
+    (((x) >> S_FW_PORT_CMD_MODTYPE) & M_FW_PORT_CMD_MODTYPE)
+
+#define S_FW_PORT_CMD_APPLY	7
+#define M_FW_PORT_CMD_APPLY	0x1
+#define V_FW_PORT_CMD_APPLY(x)	((x) << S_FW_PORT_CMD_APPLY)
+#define G_FW_PORT_CMD_APPLY(x)	\
+    (((x) >> S_FW_PORT_CMD_APPLY) & M_FW_PORT_CMD_APPLY)
+#define F_FW_PORT_CMD_APPLY	V_FW_PORT_CMD_APPLY(1U)
+
+/*
+ *	These are configured into the VPD and hence tools that generate
+ *	VPD may use this enumeration.
+ *	extPHY	#lanes	T4_I2C	extI2C	BP_Eq	BP_ANEG	Speed
+ */
+enum fw_port_type {
+	FW_PORT_TYPE_FIBER_XFI	=  0,	/* Y, 1, N, Y, N, N, 10G */
+	FW_PORT_TYPE_FIBER_XAUI	=  1,	/* Y, 4, N, Y, N, N, 10G */
+	FW_PORT_TYPE_BT_SGMII	=  2,	/* Y, 1, No, No, No, No, 1G/100M */
+	FW_PORT_TYPE_BT_XFI	=  3,	/* Y, 1, No, No, No, No, 10G */
+	FW_PORT_TYPE_BT_XAUI	=  4,	/* Y, 4, No, No, No, No, 10G/1G/100M? */
+	FW_PORT_TYPE_KX4	=  5,	/* No, 4, No, No, Yes, Yes, 10G */
+	FW_PORT_TYPE_CX4	=  6,	/* No, 4, No, No, No, No, 10G */
+	FW_PORT_TYPE_KX		=  7,	/* No, 1, No, No, Yes, No, 1G */
+	FW_PORT_TYPE_KR		=  8,	/* No, 1, No, No, Yes, Yes, 10G */
+	FW_PORT_TYPE_SFP	=  9,	/* No, 1, Yes, No, No, No, 10G */
+	FW_PORT_TYPE_BP_AP	= 10,	/* No, 1, No, No, Yes, Yes, 10G, BP ANGE */
+	FW_PORT_TYPE_BP4_AP	= 11,	/* No, 4, No, No, Yes, Yes, 10G, BP ANGE */
+
+	FW_PORT_TYPE_NONE = M_FW_PORT_CMD_PTYPE
+};
+
+/* These are read from module's EEPROM and determined once the 
+   module is inserted. */
+enum fw_port_module_type {
+	FW_PORT_MOD_TYPE_NA		= 0x0,
+	FW_PORT_MOD_TYPE_LR		= 0x1,
+	FW_PORT_MOD_TYPE_SR		= 0x2,
+	FW_PORT_MOD_TYPE_ER		= 0x3,
+	FW_PORT_MOD_TYPE_TWINAX_PASSIVE	= 0x4,
+	FW_PORT_MOD_TYPE_TWINAX_ACTIVE	= 0x5,
+	FW_PORT_MOD_TYPE_LRM		= 0x6,
+	FW_PORT_MOD_TYPE_ERROR		= M_FW_PORT_CMD_MODTYPE - 3,
+	FW_PORT_MOD_TYPE_UNKNOWN	= M_FW_PORT_CMD_MODTYPE - 2,
+	FW_PORT_MOD_TYPE_NOTSUPPORTED	= M_FW_PORT_CMD_MODTYPE - 1,
+	FW_PORT_MOD_TYPE_NONE		= M_FW_PORT_CMD_MODTYPE
+};
+
+/* used by FW and tools may use this to generate VPD */
+enum fw_port_mod_sub_type {
+	FW_PORT_MOD_SUB_TYPE_NA,
+	FW_PORT_MOD_SUB_TYPE_MV88E114X=0x1,
+	FW_PORT_MOD_SUB_TYPE_BT_VSC8634=0x8,
+
+	/*
+	 * The following will never been in the VPD.  They are TWINAX cable
+	 * lengths decoded from SFP+ module i2c PROMs.  These should almost
+	 * certainly go somewhere else ...
+	 */
+	FW_PORT_MOD_SUB_TYPE_TWINAX_1=0x9,
+	FW_PORT_MOD_SUB_TYPE_TWINAX_3=0xA,
+	FW_PORT_MOD_SUB_TYPE_TWINAX_5=0xB,
+	FW_PORT_MOD_SUB_TYPE_TWINAX_7=0xC,
+};
+
+/* link down reason codes (3b) */
+enum fw_port_link_dn_rc {
+	FW_PORT_LINK_DN_RC_NONE,
+	FW_PORT_LINK_DN_RC_REMFLT,
+	FW_PORT_LINK_DN_ANEG_F,
+	FW_PORT_LINK_DN_MS_RES_F,
+	FW_PORT_LINK_DN_UNKNOWN
+};
+
+/* port stats */
+#define FW_NUM_PORT_STATS 50
+#define FW_NUM_PORT_TX_STATS 23
+#define FW_NUM_PORT_RX_STATS 27
+
+enum fw_port_stats_tx_index {
+	FW_STAT_TX_PORT_BYTES_IX,
+	FW_STAT_TX_PORT_FRAMES_IX,
+	FW_STAT_TX_PORT_BCAST_IX,
+	FW_STAT_TX_PORT_MCAST_IX,
+	FW_STAT_TX_PORT_UCAST_IX,
+	FW_STAT_TX_PORT_ERROR_IX,
+	FW_STAT_TX_PORT_64B_IX,
+	FW_STAT_TX_PORT_65B_127B_IX,
+	FW_STAT_TX_PORT_128B_255B_IX,
+	FW_STAT_TX_PORT_256B_511B_IX,
+	FW_STAT_TX_PORT_512B_1023B_IX,
+	FW_STAT_TX_PORT_1024B_1518B_IX,
+	FW_STAT_TX_PORT_1519B_MAX_IX,
+	FW_STAT_TX_PORT_DROP_IX,
+	FW_STAT_TX_PORT_PAUSE_IX,
+	FW_STAT_TX_PORT_PPP0_IX,
+	FW_STAT_TX_PORT_PPP1_IX,
+	FW_STAT_TX_PORT_PPP2_IX,
+	FW_STAT_TX_PORT_PPP3_IX,
+	FW_STAT_TX_PORT_PPP4_IX,
+	FW_STAT_TX_PORT_PPP5_IX,
+	FW_STAT_TX_PORT_PPP6_IX,
+	FW_STAT_TX_PORT_PPP7_IX
+};
+
+enum fw_port_stat_rx_index {
+	FW_STAT_RX_PORT_BYTES_IX,
+	FW_STAT_RX_PORT_FRAMES_IX,
+	FW_STAT_RX_PORT_BCAST_IX,
+	FW_STAT_RX_PORT_MCAST_IX,
+	FW_STAT_RX_PORT_UCAST_IX,
+	FW_STAT_RX_PORT_MTU_ERROR_IX,
+	FW_STAT_RX_PORT_MTU_CRC_ERROR_IX,
+	FW_STAT_RX_PORT_CRC_ERROR_IX,
+	FW_STAT_RX_PORT_LEN_ERROR_IX,
+	FW_STAT_RX_PORT_SYM_ERROR_IX,
+	FW_STAT_RX_PORT_64B_IX,
+	FW_STAT_RX_PORT_65B_127B_IX,
+	FW_STAT_RX_PORT_128B_255B_IX,
+	FW_STAT_RX_PORT_256B_511B_IX,
+	FW_STAT_RX_PORT_512B_1023B_IX,
+	FW_STAT_RX_PORT_1024B_1518B_IX,
+	FW_STAT_RX_PORT_1519B_MAX_IX,
+	FW_STAT_RX_PORT_PAUSE_IX,
+	FW_STAT_RX_PORT_PPP0_IX,
+	FW_STAT_RX_PORT_PPP1_IX,
+	FW_STAT_RX_PORT_PPP2_IX,
+	FW_STAT_RX_PORT_PPP3_IX,
+	FW_STAT_RX_PORT_PPP4_IX,
+	FW_STAT_RX_PORT_PPP5_IX,
+	FW_STAT_RX_PORT_PPP6_IX,
+	FW_STAT_RX_PORT_PPP7_IX,
+	FW_STAT_RX_PORT_LESS_64B_IX
+};
+
+struct fw_port_stats_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	union fw_port_stats {
+		struct fw_port_stats_ctl {
+			__u8   nstats_bg_bm;
+			__u8   tx_ix;
+			__be16 r6;
+			__be32 r7;
+			__be64 stat0;
+			__be64 stat1;
+			__be64 stat2;
+			__be64 stat3;
+			__be64 stat4;
+			__be64 stat5;
+		} ctl;
+		struct fw_port_stats_all {
+			__be64 tx_bytes;
+			__be64 tx_frames;
+			__be64 tx_bcast;
+			__be64 tx_mcast;
+			__be64 tx_ucast;
+			__be64 tx_error;
+			__be64 tx_64b;
+			__be64 tx_65b_127b;
+			__be64 tx_128b_255b;
+			__be64 tx_256b_511b;
+			__be64 tx_512b_1023b;
+			__be64 tx_1024b_1518b;
+			__be64 tx_1519b_max;
+			__be64 tx_drop;
+			__be64 tx_pause;
+			__be64 tx_ppp0;
+			__be64 tx_ppp1;
+			__be64 tx_ppp2;
+			__be64 tx_ppp3;
+			__be64 tx_ppp4;
+			__be64 tx_ppp5;
+			__be64 tx_ppp6;
+			__be64 tx_ppp7;
+			__be64 rx_bytes;
+			__be64 rx_frames;
+			__be64 rx_bcast;
+			__be64 rx_mcast;
+			__be64 rx_ucast;
+			__be64 rx_mtu_error;
+			__be64 rx_mtu_crc_error;
+			__be64 rx_crc_error;
+			__be64 rx_len_error;
+			__be64 rx_sym_error;
+			__be64 rx_64b;
+			__be64 rx_65b_127b;
+			__be64 rx_128b_255b;
+			__be64 rx_256b_511b;
+			__be64 rx_512b_1023b;
+			__be64 rx_1024b_1518b;
+			__be64 rx_1519b_max;
+			__be64 rx_pause;
+			__be64 rx_ppp0;
+			__be64 rx_ppp1;
+			__be64 rx_ppp2;
+			__be64 rx_ppp3;
+			__be64 rx_ppp4;
+			__be64 rx_ppp5;
+			__be64 rx_ppp6;
+			__be64 rx_ppp7;
+			__be64 rx_less_64b;
+			__be64 rx_bg_drop;
+			__be64 rx_bg_trunc;
+		} all;
+	} u;
+};
+
+#define S_FW_PORT_STATS_CMD_NSTATS	4
+#define M_FW_PORT_STATS_CMD_NSTATS	0x7
+#define V_FW_PORT_STATS_CMD_NSTATS(x)	((x) << S_FW_PORT_STATS_CMD_NSTATS)
+#define G_FW_PORT_STATS_CMD_NSTATS(x)	\
+    (((x) >> S_FW_PORT_STATS_CMD_NSTATS) & M_FW_PORT_STATS_CMD_NSTATS)
+
+#define S_FW_PORT_STATS_CMD_BG_BM	0
+#define M_FW_PORT_STATS_CMD_BG_BM	0x3
+#define V_FW_PORT_STATS_CMD_BG_BM(x)	((x) << S_FW_PORT_STATS_CMD_BG_BM)
+#define G_FW_PORT_STATS_CMD_BG_BM(x)	\
+    (((x) >> S_FW_PORT_STATS_CMD_BG_BM) & M_FW_PORT_STATS_CMD_BG_BM)
+
+#define S_FW_PORT_STATS_CMD_TX		7
+#define M_FW_PORT_STATS_CMD_TX		0x1
+#define V_FW_PORT_STATS_CMD_TX(x)	((x) << S_FW_PORT_STATS_CMD_TX)
+#define G_FW_PORT_STATS_CMD_TX(x)	\
+    (((x) >> S_FW_PORT_STATS_CMD_TX) & M_FW_PORT_STATS_CMD_TX)
+#define F_FW_PORT_STATS_CMD_TX	V_FW_PORT_STATS_CMD_TX(1U)
+
+#define S_FW_PORT_STATS_CMD_IX		0
+#define M_FW_PORT_STATS_CMD_IX		0x3f
+#define V_FW_PORT_STATS_CMD_IX(x)	((x) << S_FW_PORT_STATS_CMD_IX)
+#define G_FW_PORT_STATS_CMD_IX(x)	\
+    (((x) >> S_FW_PORT_STATS_CMD_IX) & M_FW_PORT_STATS_CMD_IX)
+
+/* port loopback stats */
+#define FW_NUM_LB_STATS 14
+enum fw_port_lb_stats_index {
+	FW_STAT_LB_PORT_BYTES_IX,
+	FW_STAT_LB_PORT_FRAMES_IX,
+	FW_STAT_LB_PORT_BCAST_IX,
+	FW_STAT_LB_PORT_MCAST_IX,
+	FW_STAT_LB_PORT_UCAST_IX,
+	FW_STAT_LB_PORT_ERROR_IX,
+	FW_STAT_LB_PORT_64B_IX,
+	FW_STAT_LB_PORT_65B_127B_IX,
+	FW_STAT_LB_PORT_128B_255B_IX,
+	FW_STAT_LB_PORT_256B_511B_IX,
+	FW_STAT_LB_PORT_512B_1023B_IX,
+	FW_STAT_LB_PORT_1024B_1518B_IX,
+	FW_STAT_LB_PORT_1519B_MAX_IX,
+	FW_STAT_LB_PORT_DROP_FRAMES_IX
+};
+
+struct fw_port_lb_stats_cmd {
+	__be32 op_to_lbport;
+	__be32 retval_len16;
+	union fw_port_lb_stats {
+		struct fw_port_lb_stats_ctl {
+			__u8   nstats_bg_bm;
+			__u8   ix_pkd;
+			__be16 r6;
+			__be32 r7;
+			__be64 stat0;
+			__be64 stat1;
+			__be64 stat2;
+			__be64 stat3;
+			__be64 stat4;
+			__be64 stat5;
+		} ctl;
+		struct fw_port_lb_stats_all {
+			__be64 tx_bytes;
+			__be64 tx_frames;
+			__be64 tx_bcast;
+			__be64 tx_mcast;
+			__be64 tx_ucast;
+			__be64 tx_error;
+			__be64 tx_64b;
+			__be64 tx_65b_127b;
+			__be64 tx_128b_255b;
+			__be64 tx_256b_511b;
+			__be64 tx_512b_1023b;
+			__be64 tx_1024b_1518b;
+			__be64 tx_1519b_max;
+			__be64 rx_lb_drop;
+			__be64 rx_lb_trunc;
+		} all;
+	} u;
+};
+
+#define S_FW_PORT_LB_STATS_CMD_LBPORT		0
+#define M_FW_PORT_LB_STATS_CMD_LBPORT		0xf
+#define V_FW_PORT_LB_STATS_CMD_LBPORT(x)	\
+    ((x) << S_FW_PORT_LB_STATS_CMD_LBPORT)
+#define G_FW_PORT_LB_STATS_CMD_LBPORT(x)	\
+    (((x) >> S_FW_PORT_LB_STATS_CMD_LBPORT) & M_FW_PORT_LB_STATS_CMD_LBPORT)
+
+#define S_FW_PORT_LB_STATS_CMD_NSTATS		4
+#define M_FW_PORT_LB_STATS_CMD_NSTATS		0x7
+#define V_FW_PORT_LB_STATS_CMD_NSTATS(x)	\
+    ((x) << S_FW_PORT_LB_STATS_CMD_NSTATS)
+#define G_FW_PORT_LB_STATS_CMD_NSTATS(x)	\
+    (((x) >> S_FW_PORT_LB_STATS_CMD_NSTATS) & M_FW_PORT_LB_STATS_CMD_NSTATS)
+
+#define S_FW_PORT_LB_STATS_CMD_BG_BM	0
+#define M_FW_PORT_LB_STATS_CMD_BG_BM	0x3
+#define V_FW_PORT_LB_STATS_CMD_BG_BM(x)	((x) << S_FW_PORT_LB_STATS_CMD_BG_BM)
+#define G_FW_PORT_LB_STATS_CMD_BG_BM(x)	\
+    (((x) >> S_FW_PORT_LB_STATS_CMD_BG_BM) & M_FW_PORT_LB_STATS_CMD_BG_BM)
+
+#define S_FW_PORT_LB_STATS_CMD_IX	0
+#define M_FW_PORT_LB_STATS_CMD_IX	0xf
+#define V_FW_PORT_LB_STATS_CMD_IX(x)	((x) << S_FW_PORT_LB_STATS_CMD_IX)
+#define G_FW_PORT_LB_STATS_CMD_IX(x)	\
+    (((x) >> S_FW_PORT_LB_STATS_CMD_IX) & M_FW_PORT_LB_STATS_CMD_IX)
+
+/* Trace related defines */
+#define FW_TRACE_CAPTURE_MAX_SINGLE_FLT_MODE 10240
+#define FW_TRACE_CAPTURE_MAX_MULTI_FLT_MODE  2560
+
+struct fw_port_trace_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	__be16 traceen_to_pciech;
+	__be16 qnum;
+	__be32 r5;
+};
+
+#define S_FW_PORT_TRACE_CMD_PORTID	0
+#define M_FW_PORT_TRACE_CMD_PORTID	0xf
+#define V_FW_PORT_TRACE_CMD_PORTID(x)	((x) << S_FW_PORT_TRACE_CMD_PORTID)
+#define G_FW_PORT_TRACE_CMD_PORTID(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_PORTID) & M_FW_PORT_TRACE_CMD_PORTID)
+
+#define S_FW_PORT_TRACE_CMD_TRACEEN	15
+#define M_FW_PORT_TRACE_CMD_TRACEEN	0x1
+#define V_FW_PORT_TRACE_CMD_TRACEEN(x)	((x) << S_FW_PORT_TRACE_CMD_TRACEEN)
+#define G_FW_PORT_TRACE_CMD_TRACEEN(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_TRACEEN) & M_FW_PORT_TRACE_CMD_TRACEEN)
+#define F_FW_PORT_TRACE_CMD_TRACEEN	V_FW_PORT_TRACE_CMD_TRACEEN(1U)
+
+#define S_FW_PORT_TRACE_CMD_FLTMODE	14
+#define M_FW_PORT_TRACE_CMD_FLTMODE	0x1
+#define V_FW_PORT_TRACE_CMD_FLTMODE(x)	((x) << S_FW_PORT_TRACE_CMD_FLTMODE)
+#define G_FW_PORT_TRACE_CMD_FLTMODE(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_FLTMODE) & M_FW_PORT_TRACE_CMD_FLTMODE)
+#define F_FW_PORT_TRACE_CMD_FLTMODE	V_FW_PORT_TRACE_CMD_FLTMODE(1U)
+
+#define S_FW_PORT_TRACE_CMD_DUPLEN	13
+#define M_FW_PORT_TRACE_CMD_DUPLEN	0x1
+#define V_FW_PORT_TRACE_CMD_DUPLEN(x)	((x) << S_FW_PORT_TRACE_CMD_DUPLEN)
+#define G_FW_PORT_TRACE_CMD_DUPLEN(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_DUPLEN) & M_FW_PORT_TRACE_CMD_DUPLEN)
+#define F_FW_PORT_TRACE_CMD_DUPLEN	V_FW_PORT_TRACE_CMD_DUPLEN(1U)
+
+#define S_FW_PORT_TRACE_CMD_RUNTFLTSIZE		8
+#define M_FW_PORT_TRACE_CMD_RUNTFLTSIZE		0x1f
+#define V_FW_PORT_TRACE_CMD_RUNTFLTSIZE(x)	\
+    ((x) << S_FW_PORT_TRACE_CMD_RUNTFLTSIZE)
+#define G_FW_PORT_TRACE_CMD_RUNTFLTSIZE(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_RUNTFLTSIZE) & \
+     M_FW_PORT_TRACE_CMD_RUNTFLTSIZE)
+
+#define S_FW_PORT_TRACE_CMD_PCIECH	6
+#define M_FW_PORT_TRACE_CMD_PCIECH	0x3
+#define V_FW_PORT_TRACE_CMD_PCIECH(x)	((x) << S_FW_PORT_TRACE_CMD_PCIECH)
+#define G_FW_PORT_TRACE_CMD_PCIECH(x)	\
+    (((x) >> S_FW_PORT_TRACE_CMD_PCIECH) & M_FW_PORT_TRACE_CMD_PCIECH)
+
+struct fw_port_trace_mmap_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	__be32 fid_to_skipoffset;
+	__be32 minpktsize_capturemax;
+	__u8   map[224];
+};
+
+#define S_FW_PORT_TRACE_MMAP_CMD_PORTID		0
+#define M_FW_PORT_TRACE_MMAP_CMD_PORTID		0xf
+#define V_FW_PORT_TRACE_MMAP_CMD_PORTID(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_PORTID)
+#define G_FW_PORT_TRACE_MMAP_CMD_PORTID(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_PORTID) & \
+     M_FW_PORT_TRACE_MMAP_CMD_PORTID)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_FID	30
+#define M_FW_PORT_TRACE_MMAP_CMD_FID	0x3
+#define V_FW_PORT_TRACE_MMAP_CMD_FID(x)	((x) << S_FW_PORT_TRACE_MMAP_CMD_FID)
+#define G_FW_PORT_TRACE_MMAP_CMD_FID(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_FID) & M_FW_PORT_TRACE_MMAP_CMD_FID)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_MMAPEN		29
+#define M_FW_PORT_TRACE_MMAP_CMD_MMAPEN		0x1
+#define V_FW_PORT_TRACE_MMAP_CMD_MMAPEN(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_MMAPEN)
+#define G_FW_PORT_TRACE_MMAP_CMD_MMAPEN(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_MMAPEN) & \
+     M_FW_PORT_TRACE_MMAP_CMD_MMAPEN)
+#define F_FW_PORT_TRACE_MMAP_CMD_MMAPEN	V_FW_PORT_TRACE_MMAP_CMD_MMAPEN(1U)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_DCMAPEN	28
+#define M_FW_PORT_TRACE_MMAP_CMD_DCMAPEN	0x1
+#define V_FW_PORT_TRACE_MMAP_CMD_DCMAPEN(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_DCMAPEN)
+#define G_FW_PORT_TRACE_MMAP_CMD_DCMAPEN(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_DCMAPEN) & \
+     M_FW_PORT_TRACE_MMAP_CMD_DCMAPEN)
+#define F_FW_PORT_TRACE_MMAP_CMD_DCMAPEN	\
+    V_FW_PORT_TRACE_MMAP_CMD_DCMAPEN(1U)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH	8
+#define M_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH	0x1f
+#define V_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH)
+#define G_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH) & \
+     M_FW_PORT_TRACE_MMAP_CMD_SKIPLENGTH)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET	0
+#define M_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET	0x1f
+#define V_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET)
+#define G_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET) & \
+     M_FW_PORT_TRACE_MMAP_CMD_SKIPOFFSET)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE	18
+#define M_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE	0x3fff
+#define V_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE)
+#define G_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE) & \
+     M_FW_PORT_TRACE_MMAP_CMD_MINPKTSIZE)
+
+#define S_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX	0
+#define M_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX	0x3fff
+#define V_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX(x)	\
+    ((x) << S_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX)
+#define G_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX(x)	\
+    (((x) >> S_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX) & \
+     M_FW_PORT_TRACE_MMAP_CMD_CAPTUREMAX)
+
+struct fw_rss_ind_tbl_cmd {
+	__be32 op_to_viid;
+	__be32 retval_len16;
+	__be16 niqid;
+	__be16 startidx;
+	__be32 r3;
+	__be32 iq0_to_iq2;
+	__be32 iq3_to_iq5;
+	__be32 iq6_to_iq8;
+	__be32 iq9_to_iq11;
+	__be32 iq12_to_iq14;
+	__be32 iq15_to_iq17;
+	__be32 iq18_to_iq20;
+	__be32 iq21_to_iq23;
+	__be32 iq24_to_iq26;
+	__be32 iq27_to_iq29;
+	__be32 iq30_iq31;
+	__be32 r15_lo;
+};
+
+#define S_FW_RSS_IND_TBL_CMD_VIID	0
+#define M_FW_RSS_IND_TBL_CMD_VIID	0xfff
+#define V_FW_RSS_IND_TBL_CMD_VIID(x)	((x) << S_FW_RSS_IND_TBL_CMD_VIID)
+#define G_FW_RSS_IND_TBL_CMD_VIID(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_VIID) & M_FW_RSS_IND_TBL_CMD_VIID)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ0	20
+#define M_FW_RSS_IND_TBL_CMD_IQ0	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ0(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ0)
+#define G_FW_RSS_IND_TBL_CMD_IQ0(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ0) & M_FW_RSS_IND_TBL_CMD_IQ0)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ1	10
+#define M_FW_RSS_IND_TBL_CMD_IQ1	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ1(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ1)
+#define G_FW_RSS_IND_TBL_CMD_IQ1(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ1) & M_FW_RSS_IND_TBL_CMD_IQ1)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ2	0
+#define M_FW_RSS_IND_TBL_CMD_IQ2	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ2(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ2)
+#define G_FW_RSS_IND_TBL_CMD_IQ2(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ2) & M_FW_RSS_IND_TBL_CMD_IQ2)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ3	20
+#define M_FW_RSS_IND_TBL_CMD_IQ3	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ3(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ3)
+#define G_FW_RSS_IND_TBL_CMD_IQ3(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ3) & M_FW_RSS_IND_TBL_CMD_IQ3)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ4	10
+#define M_FW_RSS_IND_TBL_CMD_IQ4	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ4(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ4)
+#define G_FW_RSS_IND_TBL_CMD_IQ4(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ4) & M_FW_RSS_IND_TBL_CMD_IQ4)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ5	0
+#define M_FW_RSS_IND_TBL_CMD_IQ5	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ5(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ5)
+#define G_FW_RSS_IND_TBL_CMD_IQ5(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ5) & M_FW_RSS_IND_TBL_CMD_IQ5)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ6	20
+#define M_FW_RSS_IND_TBL_CMD_IQ6	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ6(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ6)
+#define G_FW_RSS_IND_TBL_CMD_IQ6(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ6) & M_FW_RSS_IND_TBL_CMD_IQ6)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ7	10
+#define M_FW_RSS_IND_TBL_CMD_IQ7	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ7(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ7)
+#define G_FW_RSS_IND_TBL_CMD_IQ7(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ7) & M_FW_RSS_IND_TBL_CMD_IQ7)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ8	0
+#define M_FW_RSS_IND_TBL_CMD_IQ8	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ8(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ8)
+#define G_FW_RSS_IND_TBL_CMD_IQ8(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ8) & M_FW_RSS_IND_TBL_CMD_IQ8)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ9	20
+#define M_FW_RSS_IND_TBL_CMD_IQ9	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ9(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ9)
+#define G_FW_RSS_IND_TBL_CMD_IQ9(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ9) & M_FW_RSS_IND_TBL_CMD_IQ9)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ10	10
+#define M_FW_RSS_IND_TBL_CMD_IQ10	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ10(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ10)
+#define G_FW_RSS_IND_TBL_CMD_IQ10(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ10) & M_FW_RSS_IND_TBL_CMD_IQ10)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ11	0
+#define M_FW_RSS_IND_TBL_CMD_IQ11	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ11(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ11)
+#define G_FW_RSS_IND_TBL_CMD_IQ11(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ11) & M_FW_RSS_IND_TBL_CMD_IQ11)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ12	20
+#define M_FW_RSS_IND_TBL_CMD_IQ12	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ12(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ12)
+#define G_FW_RSS_IND_TBL_CMD_IQ12(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ12) & M_FW_RSS_IND_TBL_CMD_IQ12)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ13	10
+#define M_FW_RSS_IND_TBL_CMD_IQ13	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ13(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ13)
+#define G_FW_RSS_IND_TBL_CMD_IQ13(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ13) & M_FW_RSS_IND_TBL_CMD_IQ13)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ14	0
+#define M_FW_RSS_IND_TBL_CMD_IQ14	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ14(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ14)
+#define G_FW_RSS_IND_TBL_CMD_IQ14(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ14) & M_FW_RSS_IND_TBL_CMD_IQ14)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ15	20
+#define M_FW_RSS_IND_TBL_CMD_IQ15	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ15(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ15)
+#define G_FW_RSS_IND_TBL_CMD_IQ15(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ15) & M_FW_RSS_IND_TBL_CMD_IQ15)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ16	10
+#define M_FW_RSS_IND_TBL_CMD_IQ16	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ16(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ16)
+#define G_FW_RSS_IND_TBL_CMD_IQ16(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ16) & M_FW_RSS_IND_TBL_CMD_IQ16)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ17	0
+#define M_FW_RSS_IND_TBL_CMD_IQ17	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ17(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ17)
+#define G_FW_RSS_IND_TBL_CMD_IQ17(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ17) & M_FW_RSS_IND_TBL_CMD_IQ17)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ18	20
+#define M_FW_RSS_IND_TBL_CMD_IQ18	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ18(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ18)
+#define G_FW_RSS_IND_TBL_CMD_IQ18(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ18) & M_FW_RSS_IND_TBL_CMD_IQ18)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ19	10
+#define M_FW_RSS_IND_TBL_CMD_IQ19	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ19(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ19)
+#define G_FW_RSS_IND_TBL_CMD_IQ19(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ19) & M_FW_RSS_IND_TBL_CMD_IQ19)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ20	0
+#define M_FW_RSS_IND_TBL_CMD_IQ20	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ20(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ20)
+#define G_FW_RSS_IND_TBL_CMD_IQ20(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ20) & M_FW_RSS_IND_TBL_CMD_IQ20)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ21	20
+#define M_FW_RSS_IND_TBL_CMD_IQ21	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ21(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ21)
+#define G_FW_RSS_IND_TBL_CMD_IQ21(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ21) & M_FW_RSS_IND_TBL_CMD_IQ21)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ22	10
+#define M_FW_RSS_IND_TBL_CMD_IQ22	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ22(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ22)
+#define G_FW_RSS_IND_TBL_CMD_IQ22(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ22) & M_FW_RSS_IND_TBL_CMD_IQ22)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ23	0
+#define M_FW_RSS_IND_TBL_CMD_IQ23	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ23(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ23)
+#define G_FW_RSS_IND_TBL_CMD_IQ23(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ23) & M_FW_RSS_IND_TBL_CMD_IQ23)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ24	20
+#define M_FW_RSS_IND_TBL_CMD_IQ24	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ24(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ24)
+#define G_FW_RSS_IND_TBL_CMD_IQ24(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ24) & M_FW_RSS_IND_TBL_CMD_IQ24)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ25	10
+#define M_FW_RSS_IND_TBL_CMD_IQ25	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ25(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ25)
+#define G_FW_RSS_IND_TBL_CMD_IQ25(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ25) & M_FW_RSS_IND_TBL_CMD_IQ25)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ26	0
+#define M_FW_RSS_IND_TBL_CMD_IQ26	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ26(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ26)
+#define G_FW_RSS_IND_TBL_CMD_IQ26(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ26) & M_FW_RSS_IND_TBL_CMD_IQ26)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ27	20
+#define M_FW_RSS_IND_TBL_CMD_IQ27	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ27(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ27)
+#define G_FW_RSS_IND_TBL_CMD_IQ27(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ27) & M_FW_RSS_IND_TBL_CMD_IQ27)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ28	10
+#define M_FW_RSS_IND_TBL_CMD_IQ28	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ28(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ28)
+#define G_FW_RSS_IND_TBL_CMD_IQ28(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ28) & M_FW_RSS_IND_TBL_CMD_IQ28)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ29	0
+#define M_FW_RSS_IND_TBL_CMD_IQ29	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ29(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ29)
+#define G_FW_RSS_IND_TBL_CMD_IQ29(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ29) & M_FW_RSS_IND_TBL_CMD_IQ29)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ30	20
+#define M_FW_RSS_IND_TBL_CMD_IQ30	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ30(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ30)
+#define G_FW_RSS_IND_TBL_CMD_IQ30(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ30) & M_FW_RSS_IND_TBL_CMD_IQ30)
+
+#define S_FW_RSS_IND_TBL_CMD_IQ31	10
+#define M_FW_RSS_IND_TBL_CMD_IQ31	0x3ff
+#define V_FW_RSS_IND_TBL_CMD_IQ31(x)	((x) << S_FW_RSS_IND_TBL_CMD_IQ31)
+#define G_FW_RSS_IND_TBL_CMD_IQ31(x)	\
+    (((x) >> S_FW_RSS_IND_TBL_CMD_IQ31) & M_FW_RSS_IND_TBL_CMD_IQ31)
+
+struct fw_rss_glb_config_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	union fw_rss_glb_config {
+		struct fw_rss_glb_config_manual {
+			__be32 mode_pkd;
+			__be32 r3;
+			__be64 r4;
+			__be64 r5;
+		} manual;
+		struct fw_rss_glb_config_basicvirtual {
+			__be32 mode_pkd;
+			__be32 synmapen_to_hashtoeplitz;
+			__be64 r8;
+			__be64 r9;
+		} basicvirtual;
+	} u;
+};
+
+#define S_FW_RSS_GLB_CONFIG_CMD_MODE	28
+#define M_FW_RSS_GLB_CONFIG_CMD_MODE	0xf
+#define V_FW_RSS_GLB_CONFIG_CMD_MODE(x)	((x) << S_FW_RSS_GLB_CONFIG_CMD_MODE)
+#define G_FW_RSS_GLB_CONFIG_CMD_MODE(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_MODE) & M_FW_RSS_GLB_CONFIG_CMD_MODE)
+
+#define FW_RSS_GLB_CONFIG_CMD_MODE_MANUAL	0
+#define FW_RSS_GLB_CONFIG_CMD_MODE_BASICVIRTUAL	1
+#define FW_RSS_GLB_CONFIG_CMD_MODE_MAX		1
+
+#define S_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN	8
+#define M_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN	0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN)
+#define G_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN) & \
+     M_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN)
+#define F_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN	\
+    V_FW_RSS_GLB_CONFIG_CMD_SYNMAPEN(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6		7
+#define M_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6		0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6)
+#define G_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6) & \
+     M_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6)
+#define F_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6	\
+    V_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV6(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6		6
+#define M_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6		0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6)
+#define G_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6) & \
+     M_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6)
+#define F_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6	\
+    V_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV6(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4		5
+#define M_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4		0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4)
+#define G_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4) & \
+     M_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4)
+#define F_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4	\
+    V_FW_RSS_GLB_CONFIG_CMD_SYN4TUPENIPV4(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4		4
+#define M_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4		0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4)
+#define G_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4) & \
+     M_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4)
+#define F_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4	\
+    V_FW_RSS_GLB_CONFIG_CMD_SYN2TUPENIPV4(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN	3
+#define M_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN	0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN)
+#define G_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN) & \
+     M_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN)
+#define F_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN	\
+    V_FW_RSS_GLB_CONFIG_CMD_OFDMAPEN(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN	2
+#define M_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN	0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN)
+#define G_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN) & \
+     M_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN)
+#define F_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN	\
+    V_FW_RSS_GLB_CONFIG_CMD_TNLMAPEN(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP	1
+#define M_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP	0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP)
+#define G_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP) & \
+     M_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP)
+#define F_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP	\
+    V_FW_RSS_GLB_CONFIG_CMD_TNLALLLKP(1U)
+
+#define S_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ	0
+#define M_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ	0x1
+#define V_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ(x)	\
+    ((x) << S_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ)
+#define G_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ(x)	\
+    (((x) >> S_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ) & \
+     M_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ)
+#define F_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ	\
+    V_FW_RSS_GLB_CONFIG_CMD_HASHTOEPLITZ(1U)
+
+struct fw_rss_vi_config_cmd {
+	__be32 op_to_viid;
+	__be32 retval_len16;
+	union fw_rss_vi_config {
+		struct fw_rss_vi_config_manual {
+			__be64 r3;
+			__be64 r4;
+			__be64 r5;
+		} manual;
+		struct fw_rss_vi_config_basicvirtual {
+			__be32 r6;
+			__be32 defaultq_to_udpen;
+			__be64 r9;
+			__be64 r10;
+		} basicvirtual;
+	} u;
+};
+
+#define S_FW_RSS_VI_CONFIG_CMD_VIID	0
+#define M_FW_RSS_VI_CONFIG_CMD_VIID	0xfff
+#define V_FW_RSS_VI_CONFIG_CMD_VIID(x)	((x) << S_FW_RSS_VI_CONFIG_CMD_VIID)
+#define G_FW_RSS_VI_CONFIG_CMD_VIID(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_VIID) & M_FW_RSS_VI_CONFIG_CMD_VIID)
+
+#define S_FW_RSS_VI_CONFIG_CMD_DEFAULTQ		16
+#define M_FW_RSS_VI_CONFIG_CMD_DEFAULTQ		0x3ff
+#define V_FW_RSS_VI_CONFIG_CMD_DEFAULTQ(x)	\
+    ((x) << S_FW_RSS_VI_CONFIG_CMD_DEFAULTQ)
+#define G_FW_RSS_VI_CONFIG_CMD_DEFAULTQ(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_DEFAULTQ) & \
+     M_FW_RSS_VI_CONFIG_CMD_DEFAULTQ)
+
+#define S_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN	4
+#define M_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN	0x1
+#define V_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN(x)	\
+    ((x) << S_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN)
+#define G_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN) & \
+     M_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN)
+#define F_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN	\
+    V_FW_RSS_VI_CONFIG_CMD_IP6FOURTUPEN(1U)
+
+#define S_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN	3
+#define M_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN	0x1
+#define V_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN(x)	\
+    ((x) << S_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN)
+#define G_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN) & \
+     M_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN)
+#define F_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN	\
+    V_FW_RSS_VI_CONFIG_CMD_IP6TWOTUPEN(1U)
+
+#define S_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN	2
+#define M_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN	0x1
+#define V_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN(x)	\
+    ((x) << S_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN)
+#define G_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN) & \
+     M_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN)
+#define F_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN	\
+    V_FW_RSS_VI_CONFIG_CMD_IP4FOURTUPEN(1U)
+
+#define S_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN	1
+#define M_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN	0x1
+#define V_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN(x)	\
+    ((x) << S_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN)
+#define G_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN) & \
+     M_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN)
+#define F_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN	\
+    V_FW_RSS_VI_CONFIG_CMD_IP4TWOTUPEN(1U)
+
+#define S_FW_RSS_VI_CONFIG_CMD_UDPEN	0
+#define M_FW_RSS_VI_CONFIG_CMD_UDPEN	0x1
+#define V_FW_RSS_VI_CONFIG_CMD_UDPEN(x)	((x) << S_FW_RSS_VI_CONFIG_CMD_UDPEN)
+#define G_FW_RSS_VI_CONFIG_CMD_UDPEN(x)	\
+    (((x) >> S_FW_RSS_VI_CONFIG_CMD_UDPEN) & M_FW_RSS_VI_CONFIG_CMD_UDPEN)
+#define F_FW_RSS_VI_CONFIG_CMD_UDPEN	V_FW_RSS_VI_CONFIG_CMD_UDPEN(1U)
+
+enum fw_sched_sc {
+	FW_SCHED_SC_CONFIG		= 0,
+	FW_SCHED_SC_PARAMS		= 1,
+};
+
+enum fw_sched_type {
+	FW_SCHED_TYPE_PKTSCHED	        = 0,
+	FW_SCHED_TYPE_STREAMSCHED       = 1,
+};
+
+enum fw_sched_params_level {
+	FW_SCHED_PARAMS_LEVEL_CL_RL	= 0,
+	FW_SCHED_PARAMS_LEVEL_CL_WRR	= 1,
+	FW_SCHED_PARAMS_LEVEL_CH_RL	= 2,
+	FW_SCHED_PARAMS_LEVEL_CH_WRR	= 3,
+};
+
+enum fw_sched_params_mode {
+	FW_SCHED_PARAMS_MODE_CLASS	= 0,
+	FW_SCHED_PARAMS_MODE_FLOW	= 1,
+};
+
+enum fw_sched_params_unit {
+	FW_SCHED_PARAMS_UNIT_BITRATE	= 0,
+	FW_SCHED_PARAMS_UNIT_PKTRATE	= 1,
+};
+
+enum fw_sched_params_rate {
+	FW_SCHED_PARAMS_RATE_REL	= 0,
+	FW_SCHED_PARAMS_RATE_ABS	= 1,
+};
+
+struct fw_sched_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	union fw_sched {
+		struct fw_sched_config {
+			__u8   sc;
+			__u8   type;
+			__u8   minmaxen;
+			__u8   r3[5];
+		} config;
+		struct fw_sched_params {
+			__u8   sc;
+			__u8   type;
+			__u8   level;
+			__u8   mode;
+			__u8   unit;
+			__u8   rate;
+			__u8   ch;
+			__u8   cl;
+			__be32 min;
+			__be32 max;
+			__be16 weight;
+			__be16 pktsize;
+			__be32 r4;
+		} params;
+	} u;
+};
+
+/*
+ *	length of the formatting string
+ */
+#define FW_DEVLOG_FMT_LEN	192
+
+/*
+ *	maximum number of the formatting string parameters
+ */
+#define FW_DEVLOG_FMT_PARAMS_NUM 8
+
+/*
+ *	priority levels
+ */
+enum fw_devlog_level {
+	FW_DEVLOG_LEVEL_EMERG	= 0x0,
+	FW_DEVLOG_LEVEL_CRIT	= 0x1,
+	FW_DEVLOG_LEVEL_ERR	= 0x2,
+	FW_DEVLOG_LEVEL_NOTICE	= 0x3,
+	FW_DEVLOG_LEVEL_INFO	= 0x4,
+	FW_DEVLOG_LEVEL_DEBUG	= 0x5,
+	FW_DEVLOG_LEVEL_MAX	= 0x5,
+};
+
+/*
+ *	facilities that may send a log message
+ */
+enum fw_devlog_facility {
+	FW_DEVLOG_FACILITY_CORE		= 0x00,
+	FW_DEVLOG_FACILITY_SCHED	= 0x02,
+	FW_DEVLOG_FACILITY_TIMER	= 0x04,
+	FW_DEVLOG_FACILITY_RES		= 0x06,
+	FW_DEVLOG_FACILITY_HW		= 0x08,
+	FW_DEVLOG_FACILITY_FLR		= 0x10,
+	FW_DEVLOG_FACILITY_DMAQ		= 0x12,
+	FW_DEVLOG_FACILITY_PHY		= 0x14,
+	FW_DEVLOG_FACILITY_MAC		= 0x16,
+	FW_DEVLOG_FACILITY_PORT		= 0x18,
+	FW_DEVLOG_FACILITY_VI		= 0x1A,
+	FW_DEVLOG_FACILITY_FILTER	= 0x1C,
+	FW_DEVLOG_FACILITY_ACL		= 0x1E,
+	FW_DEVLOG_FACILITY_TM		= 0x20,
+	FW_DEVLOG_FACILITY_QFC		= 0x22,
+	FW_DEVLOG_FACILITY_DCB		= 0x24,
+	FW_DEVLOG_FACILITY_ETH		= 0x26,
+	FW_DEVLOG_FACILITY_OFLD		= 0x28,
+	FW_DEVLOG_FACILITY_RI		= 0x2A,
+	FW_DEVLOG_FACILITY_ISCSI	= 0x2C,
+	FW_DEVLOG_FACILITY_FCOE		= 0x2E,
+	FW_DEVLOG_FACILITY_FOISCSI	= 0x30,
+	FW_DEVLOG_FACILITY_FOFCOE	= 0x32,
+	FW_DEVLOG_FACILITY_MAX		= 0x32,
+};
+
+/*
+ *	log message format
+ */
+struct fw_devlog_e {
+	__be64	timestamp;
+	__be32	seqno;
+	__be16	reserved1;
+	__u8	level;
+	__u8	facility;
+	__u8	fmt[FW_DEVLOG_FMT_LEN];
+	__be32	params[FW_DEVLOG_FMT_PARAMS_NUM];
+	__be32	reserved3[4];
+};
+
+struct fw_devlog_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__u8   level;
+	__u8   r2[7];
+	__be32 memtype_devlog_memaddr16_devlog;
+	__be32 memsize_devlog;
+	__be32 r3[2];
+};
+
+#define S_FW_DEVLOG_CMD_MEMTYPE_DEVLOG		28
+#define M_FW_DEVLOG_CMD_MEMTYPE_DEVLOG		0xf
+#define V_FW_DEVLOG_CMD_MEMTYPE_DEVLOG(x)	\
+    ((x) << S_FW_DEVLOG_CMD_MEMTYPE_DEVLOG)
+#define G_FW_DEVLOG_CMD_MEMTYPE_DEVLOG(x)	\
+    (((x) >> S_FW_DEVLOG_CMD_MEMTYPE_DEVLOG) & M_FW_DEVLOG_CMD_MEMTYPE_DEVLOG)
+
+#define S_FW_DEVLOG_CMD_MEMADDR16_DEVLOG	0
+#define M_FW_DEVLOG_CMD_MEMADDR16_DEVLOG	0xfffffff
+#define V_FW_DEVLOG_CMD_MEMADDR16_DEVLOG(x)	\
+    ((x) << S_FW_DEVLOG_CMD_MEMADDR16_DEVLOG)
+#define G_FW_DEVLOG_CMD_MEMADDR16_DEVLOG(x)	\
+    (((x) >> S_FW_DEVLOG_CMD_MEMADDR16_DEVLOG) & \
+     M_FW_DEVLOG_CMD_MEMADDR16_DEVLOG)
+
+enum fw_watchdog_actions {
+	FW_WATCHDOG_ACTION_FLR = 0x1,
+	FW_WATCHDOG_ACTION_BYPASS = 0x2,
+};
+
+#define FW_WATCHDOG_MAX_TIMEOUT_SECS	60
+
+struct fw_watchdog_cmd {
+	__be32 op_to_write;
+	__be32 retval_len16;
+	__be32 timeout;
+	__be32 actions;
+};
+
+struct fw_clip_cmd {
+	__be32 op_to_write;
+	__be32 alloc_to_len16;
+	__be64 ip_hi;
+	__be64 ip_lo;
+	__be32 r4[2];
+};
+
+#define S_FW_CLIP_CMD_ALLOC	31
+#define M_FW_CLIP_CMD_ALLOC	0x1
+#define V_FW_CLIP_CMD_ALLOC(x)	((x) << S_FW_CLIP_CMD_ALLOC)
+#define G_FW_CLIP_CMD_ALLOC(x)	\
+    (((x) >> S_FW_CLIP_CMD_ALLOC) & M_FW_CLIP_CMD_ALLOC)
+#define F_FW_CLIP_CMD_ALLOC	V_FW_CLIP_CMD_ALLOC(1U)
+
+#define S_FW_CLIP_CMD_FREE	30
+#define M_FW_CLIP_CMD_FREE	0x1
+#define V_FW_CLIP_CMD_FREE(x)	((x) << S_FW_CLIP_CMD_FREE)
+#define G_FW_CLIP_CMD_FREE(x)	\
+    (((x) >> S_FW_CLIP_CMD_FREE) & M_FW_CLIP_CMD_FREE)
+#define F_FW_CLIP_CMD_FREE	V_FW_CLIP_CMD_FREE(1U)
+
+/******************************************************************************
+ *   F O i S C S I   C O M M A N D s
+ **************************************/
+
+#define	FW_CHNET_IFACE_ADDR_MAX	3
+
+enum fw_chnet_iface_cmd_subop {
+	FW_CHNET_IFACE_CMD_SUBOP_NOOP = 0,
+	
+	FW_CHNET_IFACE_CMD_SUBOP_LINK_UP,
+	FW_CHNET_IFACE_CMD_SUBOP_LINK_DOWN,
+	
+	FW_CHNET_IFACE_CMD_SUBOP_MTU_SET,
+	FW_CHNET_IFACE_CMD_SUBOP_MTU_GET,
+
+	FW_CHNET_IFACE_CMD_SUBOP_MAX,
+};
+
+struct fw_chnet_iface_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	__u8   subop;
+	__u8   r2[3];
+	__be32 ifid_ifstate;
+	__be16 mtu;
+	__be16 vlanid;
+	__be32 r3;
+	__be16 r4;
+	__u8   mac[6];
+};
+
+#define S_FW_CHNET_IFACE_CMD_PORTID	0
+#define M_FW_CHNET_IFACE_CMD_PORTID	0xf
+#define V_FW_CHNET_IFACE_CMD_PORTID(x)	((x) << S_FW_CHNET_IFACE_CMD_PORTID)
+#define G_FW_CHNET_IFACE_CMD_PORTID(x)	\
+    (((x) >> S_FW_CHNET_IFACE_CMD_PORTID) & M_FW_CHNET_IFACE_CMD_PORTID)
+
+#define S_FW_CHNET_IFACE_CMD_IFID	8
+#define M_FW_CHNET_IFACE_CMD_IFID	0xffffff
+#define V_FW_CHNET_IFACE_CMD_IFID(x)	((x) << S_FW_CHNET_IFACE_CMD_IFID)
+#define G_FW_CHNET_IFACE_CMD_IFID(x)	\
+    (((x) >> S_FW_CHNET_IFACE_CMD_IFID) & M_FW_CHNET_IFACE_CMD_IFID)
+
+#define S_FW_CHNET_IFACE_CMD_IFSTATE	0
+#define M_FW_CHNET_IFACE_CMD_IFSTATE	0xff
+#define V_FW_CHNET_IFACE_CMD_IFSTATE(x)	((x) << S_FW_CHNET_IFACE_CMD_IFSTATE)
+#define G_FW_CHNET_IFACE_CMD_IFSTATE(x)	\
+    (((x) >> S_FW_CHNET_IFACE_CMD_IFSTATE) & M_FW_CHNET_IFACE_CMD_IFSTATE)
+
+/******************************************************************************
+ *   F O F C O E   C O M M A N D s
+ ************************************/
+
+struct fw_fcoe_res_info_cmd {
+	__be32 op_to_read;
+	__be32 retval_len16;
+	__be16 e_d_tov;
+	__be16 r_a_tov_seq;
+	__be16 r_a_tov_els;
+	__be16 r_r_tov;
+	__be32 max_xchgs;
+	__be32 max_ssns;
+	__be32 used_xchgs;
+	__be32 used_ssns;
+	__be32 max_fcfs;
+	__be32 max_vnps;
+	__be32 used_fcfs;
+	__be32 used_vnps;
+};
+
+struct fw_fcoe_link_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	__be32 sub_opcode_fcfi;
+	__u8   r3;
+	__u8   lstatus;
+	__be16 flags;
+	__u8   r4;
+	__u8   set_vlan;
+	__be16 vlan_id;
+	__be32 vnpi_pkd;
+	__be16 r6;
+	__u8   phy_mac[6];
+	__u8   vnport_wwnn[8];
+	__u8   vnport_wwpn[8];
+};
+
+#define S_FW_FCOE_LINK_CMD_PORTID	0
+#define M_FW_FCOE_LINK_CMD_PORTID	0xf
+#define V_FW_FCOE_LINK_CMD_PORTID(x)	((x) << S_FW_FCOE_LINK_CMD_PORTID)
+#define G_FW_FCOE_LINK_CMD_PORTID(x)	\
+    (((x) >> S_FW_FCOE_LINK_CMD_PORTID) & M_FW_FCOE_LINK_CMD_PORTID)
+
+#define S_FW_FCOE_LINK_CMD_SUB_OPCODE		24
+#define M_FW_FCOE_LINK_CMD_SUB_OPCODE		0xff
+#define V_FW_FCOE_LINK_CMD_SUB_OPCODE(x)	\
+    ((x) << S_FW_FCOE_LINK_CMD_SUB_OPCODE)
+#define G_FW_FCOE_LINK_CMD_SUB_OPCODE(x)	\
+    (((x) >> S_FW_FCOE_LINK_CMD_SUB_OPCODE) & M_FW_FCOE_LINK_CMD_SUB_OPCODE)
+
+#define S_FW_FCOE_LINK_CMD_FCFI		0
+#define M_FW_FCOE_LINK_CMD_FCFI		0xffffff
+#define V_FW_FCOE_LINK_CMD_FCFI(x)	((x) << S_FW_FCOE_LINK_CMD_FCFI)
+#define G_FW_FCOE_LINK_CMD_FCFI(x)	\
+    (((x) >> S_FW_FCOE_LINK_CMD_FCFI) & M_FW_FCOE_LINK_CMD_FCFI)
+
+#define S_FW_FCOE_LINK_CMD_VNPI		0
+#define M_FW_FCOE_LINK_CMD_VNPI		0xfffff
+#define V_FW_FCOE_LINK_CMD_VNPI(x)	((x) << S_FW_FCOE_LINK_CMD_VNPI)
+#define G_FW_FCOE_LINK_CMD_VNPI(x)	\
+    (((x) >> S_FW_FCOE_LINK_CMD_VNPI) & M_FW_FCOE_LINK_CMD_VNPI)
+
+struct fw_fcoe_vnp_cmd {
+	__be32 op_to_fcfi;
+	__be32 alloc_to_len16;
+	__be32 gen_wwn_to_vnpi;
+	__be32 vf_id;
+	__be16 iqid;
+	__u8   vnport_mac[6];
+	__u8   vnport_wwnn[8];
+	__u8   vnport_wwpn[8];
+	__u8   cmn_srv_parms[16];
+	__u8   clsp_word_0_1[8];
+};
+
+#define S_FW_FCOE_VNP_CMD_FCFI		0
+#define M_FW_FCOE_VNP_CMD_FCFI		0xfffff
+#define V_FW_FCOE_VNP_CMD_FCFI(x)	((x) << S_FW_FCOE_VNP_CMD_FCFI)
+#define G_FW_FCOE_VNP_CMD_FCFI(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_FCFI) & M_FW_FCOE_VNP_CMD_FCFI)
+
+#define S_FW_FCOE_VNP_CMD_ALLOC		31
+#define M_FW_FCOE_VNP_CMD_ALLOC		0x1
+#define V_FW_FCOE_VNP_CMD_ALLOC(x)	((x) << S_FW_FCOE_VNP_CMD_ALLOC)
+#define G_FW_FCOE_VNP_CMD_ALLOC(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_ALLOC) & M_FW_FCOE_VNP_CMD_ALLOC)
+#define F_FW_FCOE_VNP_CMD_ALLOC	V_FW_FCOE_VNP_CMD_ALLOC(1U)
+
+#define S_FW_FCOE_VNP_CMD_FREE		30
+#define M_FW_FCOE_VNP_CMD_FREE		0x1
+#define V_FW_FCOE_VNP_CMD_FREE(x)	((x) << S_FW_FCOE_VNP_CMD_FREE)
+#define G_FW_FCOE_VNP_CMD_FREE(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_FREE) & M_FW_FCOE_VNP_CMD_FREE)
+#define F_FW_FCOE_VNP_CMD_FREE	V_FW_FCOE_VNP_CMD_FREE(1U)
+
+#define S_FW_FCOE_VNP_CMD_MODIFY	29
+#define M_FW_FCOE_VNP_CMD_MODIFY	0x1
+#define V_FW_FCOE_VNP_CMD_MODIFY(x)	((x) << S_FW_FCOE_VNP_CMD_MODIFY)
+#define G_FW_FCOE_VNP_CMD_MODIFY(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_MODIFY) & M_FW_FCOE_VNP_CMD_MODIFY)
+#define F_FW_FCOE_VNP_CMD_MODIFY	V_FW_FCOE_VNP_CMD_MODIFY(1U)
+
+#define S_FW_FCOE_VNP_CMD_GEN_WWN	22
+#define M_FW_FCOE_VNP_CMD_GEN_WWN	0x1
+#define V_FW_FCOE_VNP_CMD_GEN_WWN(x)	((x) << S_FW_FCOE_VNP_CMD_GEN_WWN)
+#define G_FW_FCOE_VNP_CMD_GEN_WWN(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_GEN_WWN) & M_FW_FCOE_VNP_CMD_GEN_WWN)
+#define F_FW_FCOE_VNP_CMD_GEN_WWN	V_FW_FCOE_VNP_CMD_GEN_WWN(1U)
+
+#define S_FW_FCOE_VNP_CMD_PERSIST	21
+#define M_FW_FCOE_VNP_CMD_PERSIST	0x1
+#define V_FW_FCOE_VNP_CMD_PERSIST(x)	((x) << S_FW_FCOE_VNP_CMD_PERSIST)
+#define G_FW_FCOE_VNP_CMD_PERSIST(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_PERSIST) & M_FW_FCOE_VNP_CMD_PERSIST)
+#define F_FW_FCOE_VNP_CMD_PERSIST	V_FW_FCOE_VNP_CMD_PERSIST(1U)
+
+#define S_FW_FCOE_VNP_CMD_VFID_EN	20
+#define M_FW_FCOE_VNP_CMD_VFID_EN	0x1
+#define V_FW_FCOE_VNP_CMD_VFID_EN(x)	((x) << S_FW_FCOE_VNP_CMD_VFID_EN)
+#define G_FW_FCOE_VNP_CMD_VFID_EN(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_VFID_EN) & M_FW_FCOE_VNP_CMD_VFID_EN)
+#define F_FW_FCOE_VNP_CMD_VFID_EN	V_FW_FCOE_VNP_CMD_VFID_EN(1U)
+
+#define S_FW_FCOE_VNP_CMD_VNPI		0
+#define M_FW_FCOE_VNP_CMD_VNPI		0xfffff
+#define V_FW_FCOE_VNP_CMD_VNPI(x)	((x) << S_FW_FCOE_VNP_CMD_VNPI)
+#define G_FW_FCOE_VNP_CMD_VNPI(x)	\
+    (((x) >> S_FW_FCOE_VNP_CMD_VNPI) & M_FW_FCOE_VNP_CMD_VNPI)
+
+struct fw_fcoe_sparams_cmd {
+	__be32 op_to_portid;
+	__be32 retval_len16;
+	__u8   r3[7];
+	__u8   cos;
+	__u8   lport_wwnn[8];
+	__u8   lport_wwpn[8];
+	__u8   cmn_srv_parms[16];
+	__u8   cls_srv_parms[16];
+};
+
+#define S_FW_FCOE_SPARAMS_CMD_PORTID	0
+#define M_FW_FCOE_SPARAMS_CMD_PORTID	0xf
+#define V_FW_FCOE_SPARAMS_CMD_PORTID(x)	((x) << S_FW_FCOE_SPARAMS_CMD_PORTID)
+#define G_FW_FCOE_SPARAMS_CMD_PORTID(x)	\
+    (((x) >> S_FW_FCOE_SPARAMS_CMD_PORTID) & M_FW_FCOE_SPARAMS_CMD_PORTID)
+
+struct fw_fcoe_stats_cmd {
+	__be32 op_to_flowid;
+	__be32 free_to_len16;
+	union fw_fcoe_stats {
+		struct fw_fcoe_stats_ctl {
+			__u8   nstats_port;
+			__u8   port_valid_ix;
+			__be16 r6;
+			__be32 r7;
+			__be64 stat0;
+			__be64 stat1;
+			__be64 stat2;
+			__be64 stat3;
+			__be64 stat4;
+			__be64 stat5;
+		} ctl;
+		struct fw_fcoe_port_stats {
+			__be64 tx_bcast_bytes;
+			__be64 tx_bcast_frames;
+			__be64 tx_mcast_bytes;
+			__be64 tx_mcast_frames;
+			__be64 tx_ucast_bytes;
+			__be64 tx_ucast_frames;
+			__be64 tx_drop_frames;
+			__be64 tx_offload_bytes;
+			__be64 tx_offload_frames;
+			__be64 rx_bcast_bytes;
+			__be64 rx_bcast_frames;
+			__be64 rx_mcast_bytes;
+			__be64 rx_mcast_frames;
+			__be64 rx_ucast_bytes;
+			__be64 rx_ucast_frames;
+			__be64 rx_err_frames;
+		} port_stats;
+		struct fw_fcoe_fcf_stats {
+			__be32 fip_tx_bytes;
+			__be32 fip_tx_fr;
+			__be64 fcf_ka;
+			__be64 mcast_adv_rcvd;
+			__be16 ucast_adv_rcvd;
+			__be16 sol_sent;
+			__be16 vlan_req;
+			__be16 vlan_rpl;
+			__be16 clr_vlink;
+			__be16 link_down;
+			__be16 link_up;
+			__be16 logo;
+			__be16 flogi_req;
+			__be16 flogi_rpl;
+			__be16 fdisc_req;
+			__be16 fdisc_rpl;
+			__be16 fka_prd_chg;
+			__be16 fc_map_chg;
+			__be16 vfid_chg;
+			__u8   no_fka_req;
+			__u8   no_vnp;
+		} fcf_stats;
+		struct fw_fcoe_pcb_stats {
+			__be64 tx_bytes;
+			__be64 tx_frames;
+			__be64 rx_bytes;
+			__be64 rx_frames;
+			__be32 vnp_ka;
+			__be32 unsol_els_rcvd;
+			__be64 unsol_cmd_rcvd;
+			__be16 implicit_logo;
+			__be16 flogi_inv_sparm;
+			__be16 fdisc_inv_sparm;
+			__be16 flogi_rjt;
+			__be16 fdisc_rjt;
+			__be16 no_ssn;
+			__be16 mac_flt_fail;
+			__be16 inv_fr_rcvd;
+		} pcb_stats;
+		struct fw_fcoe_scb_stats {
+			__be64 tx_bytes;
+			__be64 tx_frames;
+			__be64 rx_bytes;
+			__be64 rx_frames;
+			__be32 host_abrt_req;
+			__be32 adap_auto_abrt;
+			__be32 adap_abrt_rsp;
+			__be32 host_ios_req;
+			__be16 ssn_offl_ios;
+			__be16 ssn_not_rdy_ios;
+			__u8   rx_data_ddp_err;
+			__u8   ddp_flt_set_err;
+			__be16 rx_data_fr_err;
+			__u8   bad_st_abrt_req;
+			__u8   no_io_abrt_req;
+			__u8   abort_tmo;
+			__u8   abort_tmo_2;
+			__be32 abort_req;
+			__u8   no_ppod_res_tmo;
+			__u8   bp_tmo;
+			__u8   adap_auto_cls;
+			__u8   no_io_cls_req;
+			__be32 host_cls_req;
+			__be64 unsol_cmd_rcvd;
+			__be32 plogi_req_rcvd;
+			__be32 prli_req_rcvd;
+			__be16 logo_req_rcvd;
+			__be16 prlo_req_rcvd;
+			__be16 plogi_rjt_rcvd;
+			__be16 prli_rjt_rcvd;
+			__be32 adisc_req_rcvd;
+			__be32 rscn_rcvd;
+			__be32 rrq_req_rcvd;
+			__be32 unsol_els_rcvd;
+			__u8   adisc_rjt_rcvd;
+			__u8   scr_rjt;
+			__u8   ct_rjt;
+			__u8   inval_bls_rcvd;
+			__be32 ba_rjt_rcvd;
+		} scb_stats;
+	} u;
+};
+
+#define S_FW_FCOE_STATS_CMD_FLOWID	0
+#define M_FW_FCOE_STATS_CMD_FLOWID	0xfffff
+#define V_FW_FCOE_STATS_CMD_FLOWID(x)	((x) << S_FW_FCOE_STATS_CMD_FLOWID)
+#define G_FW_FCOE_STATS_CMD_FLOWID(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_FLOWID) & M_FW_FCOE_STATS_CMD_FLOWID)
+
+#define S_FW_FCOE_STATS_CMD_FREE	30
+#define M_FW_FCOE_STATS_CMD_FREE	0x1
+#define V_FW_FCOE_STATS_CMD_FREE(x)	((x) << S_FW_FCOE_STATS_CMD_FREE)
+#define G_FW_FCOE_STATS_CMD_FREE(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_FREE) & M_FW_FCOE_STATS_CMD_FREE)
+#define F_FW_FCOE_STATS_CMD_FREE	V_FW_FCOE_STATS_CMD_FREE(1U)
+
+#define S_FW_FCOE_STATS_CMD_NSTATS	4
+#define M_FW_FCOE_STATS_CMD_NSTATS	0x7
+#define V_FW_FCOE_STATS_CMD_NSTATS(x)	((x) << S_FW_FCOE_STATS_CMD_NSTATS)
+#define G_FW_FCOE_STATS_CMD_NSTATS(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_NSTATS) & M_FW_FCOE_STATS_CMD_NSTATS)
+
+#define S_FW_FCOE_STATS_CMD_PORT	0
+#define M_FW_FCOE_STATS_CMD_PORT	0x3
+#define V_FW_FCOE_STATS_CMD_PORT(x)	((x) << S_FW_FCOE_STATS_CMD_PORT)
+#define G_FW_FCOE_STATS_CMD_PORT(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_PORT) & M_FW_FCOE_STATS_CMD_PORT)
+
+#define S_FW_FCOE_STATS_CMD_PORT_VALID		7
+#define M_FW_FCOE_STATS_CMD_PORT_VALID		0x1
+#define V_FW_FCOE_STATS_CMD_PORT_VALID(x)	\
+    ((x) << S_FW_FCOE_STATS_CMD_PORT_VALID)
+#define G_FW_FCOE_STATS_CMD_PORT_VALID(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_PORT_VALID) & M_FW_FCOE_STATS_CMD_PORT_VALID)
+#define F_FW_FCOE_STATS_CMD_PORT_VALID	V_FW_FCOE_STATS_CMD_PORT_VALID(1U)
+
+#define S_FW_FCOE_STATS_CMD_IX		0
+#define M_FW_FCOE_STATS_CMD_IX		0x3f
+#define V_FW_FCOE_STATS_CMD_IX(x)	((x) << S_FW_FCOE_STATS_CMD_IX)
+#define G_FW_FCOE_STATS_CMD_IX(x)	\
+    (((x) >> S_FW_FCOE_STATS_CMD_IX) & M_FW_FCOE_STATS_CMD_IX)
+
+struct fw_fcoe_fcf_cmd {
+	__be32 op_to_fcfi;
+	__be32 retval_len16;
+	__be16 priority_pkd;
+	__u8   mac[6];
+	__u8   name_id[8];
+	__u8   fabric[8];
+	__be16 vf_id;
+	__be16 max_fcoe_size;
+	__u8   vlan_id;
+	__u8   fc_map[3];
+	__be32 fka_adv;
+	__be32 r6;
+	__u8   r7_hi;
+	__u8   fpma_to_portid;
+	__u8   spma_mac[6];
+	__be64 r8;
+};
+
+#define S_FW_FCOE_FCF_CMD_FCFI		0
+#define M_FW_FCOE_FCF_CMD_FCFI		0xfffff
+#define V_FW_FCOE_FCF_CMD_FCFI(x)	((x) << S_FW_FCOE_FCF_CMD_FCFI)
+#define G_FW_FCOE_FCF_CMD_FCFI(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_FCFI) & M_FW_FCOE_FCF_CMD_FCFI)
+
+#define S_FW_FCOE_FCF_CMD_PRIORITY	0
+#define M_FW_FCOE_FCF_CMD_PRIORITY	0xff
+#define V_FW_FCOE_FCF_CMD_PRIORITY(x)	((x) << S_FW_FCOE_FCF_CMD_PRIORITY)
+#define G_FW_FCOE_FCF_CMD_PRIORITY(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_PRIORITY) & M_FW_FCOE_FCF_CMD_PRIORITY)
+
+#define S_FW_FCOE_FCF_CMD_FPMA		6
+#define M_FW_FCOE_FCF_CMD_FPMA		0x1
+#define V_FW_FCOE_FCF_CMD_FPMA(x)	((x) << S_FW_FCOE_FCF_CMD_FPMA)
+#define G_FW_FCOE_FCF_CMD_FPMA(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_FPMA) & M_FW_FCOE_FCF_CMD_FPMA)
+#define F_FW_FCOE_FCF_CMD_FPMA	V_FW_FCOE_FCF_CMD_FPMA(1U)
+
+#define S_FW_FCOE_FCF_CMD_SPMA		5
+#define M_FW_FCOE_FCF_CMD_SPMA		0x1
+#define V_FW_FCOE_FCF_CMD_SPMA(x)	((x) << S_FW_FCOE_FCF_CMD_SPMA)
+#define G_FW_FCOE_FCF_CMD_SPMA(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_SPMA) & M_FW_FCOE_FCF_CMD_SPMA)
+#define F_FW_FCOE_FCF_CMD_SPMA	V_FW_FCOE_FCF_CMD_SPMA(1U)
+
+#define S_FW_FCOE_FCF_CMD_LOGIN		4
+#define M_FW_FCOE_FCF_CMD_LOGIN		0x1
+#define V_FW_FCOE_FCF_CMD_LOGIN(x)	((x) << S_FW_FCOE_FCF_CMD_LOGIN)
+#define G_FW_FCOE_FCF_CMD_LOGIN(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_LOGIN) & M_FW_FCOE_FCF_CMD_LOGIN)
+#define F_FW_FCOE_FCF_CMD_LOGIN	V_FW_FCOE_FCF_CMD_LOGIN(1U)
+
+#define S_FW_FCOE_FCF_CMD_PORTID	0
+#define M_FW_FCOE_FCF_CMD_PORTID	0xf
+#define V_FW_FCOE_FCF_CMD_PORTID(x)	((x) << S_FW_FCOE_FCF_CMD_PORTID)
+#define G_FW_FCOE_FCF_CMD_PORTID(x)	\
+    (((x) >> S_FW_FCOE_FCF_CMD_PORTID) & M_FW_FCOE_FCF_CMD_PORTID)
+
+/******************************************************************************
+ *   E R R O R   a n d   D E B U G   C O M M A N D s
+ ******************************************************/
+
+enum fw_error_type {
+	FW_ERROR_TYPE_EXCEPTION         = 0x0,
+	FW_ERROR_TYPE_HWMODULE          = 0x1,
+	FW_ERROR_TYPE_WR                = 0x2,
+	FW_ERROR_TYPE_ACL               = 0x3,
+};
+
+struct fw_error_cmd {
+	__be32 op_to_type;
+	__be32 len16_pkd;
+	union fw_error {
+		struct fw_error_exception {
+			__be32 info[6];
+		} exception;
+		struct fw_error_hwmodule {
+			__be32 regaddr;
+			__be32 regval;
+		} hwmodule;
+		struct fw_error_wr {
+			__be16 cidx;
+			__be16 pfn_vfn;
+			__be32 eqid;
+			__u8   wrhdr[16];
+		} wr;
+		struct fw_error_acl {
+			__be16 cidx;
+			__be16 pfn_vfn;
+			__be32 eqid;
+			__be16 mv_pkd;
+			__u8   val[6];
+			__be64 r4;
+		} acl;
+	} u;
+};
+
+#define S_FW_ERROR_CMD_FATAL	4
+#define M_FW_ERROR_CMD_FATAL	0x1
+#define V_FW_ERROR_CMD_FATAL(x)	((x) << S_FW_ERROR_CMD_FATAL)
+#define G_FW_ERROR_CMD_FATAL(x)	\
+    (((x) >> S_FW_ERROR_CMD_FATAL) & M_FW_ERROR_CMD_FATAL)
+#define F_FW_ERROR_CMD_FATAL	V_FW_ERROR_CMD_FATAL(1U)
+
+#define S_FW_ERROR_CMD_TYPE	0
+#define M_FW_ERROR_CMD_TYPE	0xf
+#define V_FW_ERROR_CMD_TYPE(x)	((x) << S_FW_ERROR_CMD_TYPE)
+#define G_FW_ERROR_CMD_TYPE(x)	\
+    (((x) >> S_FW_ERROR_CMD_TYPE) & M_FW_ERROR_CMD_TYPE)
+
+#define S_FW_ERROR_CMD_PFN	8
+#define M_FW_ERROR_CMD_PFN	0x7
+#define V_FW_ERROR_CMD_PFN(x)	((x) << S_FW_ERROR_CMD_PFN)
+#define G_FW_ERROR_CMD_PFN(x)	\
+    (((x) >> S_FW_ERROR_CMD_PFN) & M_FW_ERROR_CMD_PFN)
+
+#define S_FW_ERROR_CMD_VFN	0
+#define M_FW_ERROR_CMD_VFN	0xff
+#define V_FW_ERROR_CMD_VFN(x)	((x) << S_FW_ERROR_CMD_VFN)
+#define G_FW_ERROR_CMD_VFN(x)	\
+    (((x) >> S_FW_ERROR_CMD_VFN) & M_FW_ERROR_CMD_VFN)
+
+#define S_FW_ERROR_CMD_PFN	8
+#define M_FW_ERROR_CMD_PFN	0x7
+#define V_FW_ERROR_CMD_PFN(x)	((x) << S_FW_ERROR_CMD_PFN)
+#define G_FW_ERROR_CMD_PFN(x)	\
+    (((x) >> S_FW_ERROR_CMD_PFN) & M_FW_ERROR_CMD_PFN)
+
+#define S_FW_ERROR_CMD_VFN	0
+#define M_FW_ERROR_CMD_VFN	0xff
+#define V_FW_ERROR_CMD_VFN(x)	((x) << S_FW_ERROR_CMD_VFN)
+#define G_FW_ERROR_CMD_VFN(x)	\
+    (((x) >> S_FW_ERROR_CMD_VFN) & M_FW_ERROR_CMD_VFN)
+
+#define S_FW_ERROR_CMD_MV	15
+#define M_FW_ERROR_CMD_MV	0x1
+#define V_FW_ERROR_CMD_MV(x)	((x) << S_FW_ERROR_CMD_MV)
+#define G_FW_ERROR_CMD_MV(x)	\
+    (((x) >> S_FW_ERROR_CMD_MV) & M_FW_ERROR_CMD_MV)
+#define F_FW_ERROR_CMD_MV	V_FW_ERROR_CMD_MV(1U)
+
+struct fw_debug_cmd {
+	__be32 op_type;
+	__be32 len16_pkd;
+	union fw_debug {
+		struct fw_debug_assert {
+			__be32 fcid;
+			__be32 line;
+			__be32 x;
+			__be32 y;
+			__u8   filename_0_7[8];
+			__u8   filename_8_15[8];
+			__be64 r3;
+		} assert;
+		struct fw_debug_prt {
+			__be16 dprtstridx;
+			__be16 r3[3];
+			__be32 dprtstrparam0;
+			__be32 dprtstrparam1;
+			__be32 dprtstrparam2;
+			__be32 dprtstrparam3;
+		} prt;
+	} u;
+};
+
+#define S_FW_DEBUG_CMD_TYPE	0
+#define M_FW_DEBUG_CMD_TYPE	0xff
+#define V_FW_DEBUG_CMD_TYPE(x)	((x) << S_FW_DEBUG_CMD_TYPE)
+#define G_FW_DEBUG_CMD_TYPE(x)	\
+    (((x) >> S_FW_DEBUG_CMD_TYPE) & M_FW_DEBUG_CMD_TYPE)
+
+/******************************************************************************
+ *   P C I E   F W   R E G I S T E R
+ **************************************/
+
+/**
+ *	Register definitions for the PCIE_FW register which the firmware uses
+ *	to restain status across RESETs.  This register should be considered
+ *	as a READ-ONLY register for Host Software and only to be used to
+ *	track firmware initialization/error state, etc.
+ */
+#define S_PCIE_FW_ERR		31
+#define M_PCIE_FW_ERR		0x1
+#define V_PCIE_FW_ERR(x)	((x) << S_PCIE_FW_ERR)
+#define G_PCIE_FW_ERR(x)	(((x) >> S_PCIE_FW_ERR) & M_PCIE_FW_ERR)
+#define F_PCIE_FW_ERR		V_PCIE_FW_ERR(1U)
+
+#define S_PCIE_FW_INIT		30
+#define M_PCIE_FW_INIT		0x1
+#define V_PCIE_FW_INIT(x)	((x) << S_PCIE_FW_INIT)
+#define G_PCIE_FW_INIT(x)	(((x) >> S_PCIE_FW_INIT) & M_PCIE_FW_INIT)
+#define F_PCIE_FW_INIT		V_PCIE_FW_INIT(1U)
+
+#define S_PCIE_FW_HALT          29
+#define M_PCIE_FW_HALT          0x1
+#define V_PCIE_FW_HALT(x)       ((x) << S_PCIE_FW_HALT)
+#define G_PCIE_FW_HALT(x)       (((x) >> S_PCIE_FW_HALT) & M_PCIE_FW_HALT)
+#define F_PCIE_FW_HALT          V_PCIE_FW_HALT(1U)
+
+#define S_PCIE_FW_STAGE		21
+#define M_PCIE_FW_STAGE		0x7
+#define V_PCIE_FW_STAGE(x)	((x) << S_PCIE_FW_STAGE)
+#define G_PCIE_FW_STAGE(x)	(((x) >> S_PCIE_FW_STAGE) & M_PCIE_FW_STAGE)
+
+#define S_PCIE_FW_ASYNCNOT_VLD	20
+#define M_PCIE_FW_ASYNCNOT_VLD	0x1
+#define V_PCIE_FW_ASYNCNOT_VLD(x) \
+    ((x) << S_PCIE_FW_ASYNCNOT_VLD)
+#define G_PCIE_FW_ASYNCNOT_VLD(x) \
+    (((x) >> S_PCIE_FW_ASYNCNOT_VLD) & M_PCIE_FW_ASYNCNOT_VLD)
+#define F_PCIE_FW_ASYNCNOT_VLD	V_PCIE_FW_ASYNCNOT_VLD(1U)
+
+#define S_PCIE_FW_ASYNCNOTINT	19
+#define M_PCIE_FW_ASYNCNOTINT	0x1
+#define V_PCIE_FW_ASYNCNOTINT(x) \
+    ((x) << S_PCIE_FW_ASYNCNOTINT)
+#define G_PCIE_FW_ASYNCNOTINT(x) \
+    (((x) >> S_PCIE_FW_ASYNCNOTINT) & M_PCIE_FW_ASYNCNOTINT)
+#define F_PCIE_FW_ASYNCNOTINT	V_PCIE_FW_ASYNCNOTINT(1U)
+
+#define S_PCIE_FW_ASYNCNOT	16
+#define M_PCIE_FW_ASYNCNOT	0x7
+#define V_PCIE_FW_ASYNCNOT(x)	((x) << S_PCIE_FW_ASYNCNOT)
+#define G_PCIE_FW_ASYNCNOT(x)	\
+    (((x) >> S_PCIE_FW_ASYNCNOT) & M_PCIE_FW_ASYNCNOT)
+
+#define S_PCIE_FW_MASTER_VLD	15
+#define M_PCIE_FW_MASTER_VLD	0x1
+#define V_PCIE_FW_MASTER_VLD(x)	((x) << S_PCIE_FW_MASTER_VLD)
+#define G_PCIE_FW_MASTER_VLD(x)	\
+    (((x) >> S_PCIE_FW_MASTER_VLD) & M_PCIE_FW_MASTER_VLD)
+#define F_PCIE_FW_MASTER_VLD	V_PCIE_FW_MASTER_VLD(1U)
+
+#define S_PCIE_FW_MASTER	12
+#define M_PCIE_FW_MASTER	0x7
+#define V_PCIE_FW_MASTER(x)	((x) << S_PCIE_FW_MASTER)
+#define G_PCIE_FW_MASTER(x)	(((x) >> S_PCIE_FW_MASTER) & M_PCIE_FW_MASTER)
+
+#define S_PCIE_FW_RESET_VLD		11
+#define M_PCIE_FW_RESET_VLD		0x1
+#define V_PCIE_FW_RESET_VLD(x)	((x) << S_PCIE_FW_RESET_VLD)
+#define G_PCIE_FW_RESET_VLD(x)	\
+    (((x) >> S_PCIE_FW_RESET_VLD) & M_PCIE_FW_RESET_VLD)
+#define F_PCIE_FW_RESET_VLD	V_PCIE_FW_RESET_VLD(1U)
+
+#define S_PCIE_FW_RESET		8
+#define M_PCIE_FW_RESET		0x7
+#define V_PCIE_FW_RESET(x)	((x) << S_PCIE_FW_RESET)
+#define G_PCIE_FW_RESET(x)	\
+    (((x) >> S_PCIE_FW_RESET) & M_PCIE_FW_RESET)
+
+#define S_PCIE_FW_REGISTERED	0
+#define M_PCIE_FW_REGISTERED	0xff
+#define V_PCIE_FW_REGISTERED(x)	((x) << S_PCIE_FW_REGISTERED)
+#define G_PCIE_FW_REGISTERED(x)	\
+    (((x) >> S_PCIE_FW_REGISTERED) & M_PCIE_FW_REGISTERED)
+
+
+/******************************************************************************
+ *   B I N A R Y   H E A D E R   F O R M A T
+ **********************************************/
+
+/*
+ *	firmware binary header format
+ */
+struct fw_hdr {
+	__u8	ver;
+	__u8	chip;			/* terminator chip family */
+	__be16	len512;			/* bin length in units of 512-bytes */
+	__be32	fw_ver;			/* firmware version */
+	__be32	tp_microcode_ver;	/* tcp processor microcode version */
+	__u8	intfver_nic;
+	__u8	intfver_vnic;
+	__u8	intfver_ofld;
+	__u8	intfver_ri;
+	__u8	intfver_iscsipdu;
+	__u8	intfver_iscsi;
+	__u8	intfver_fcoe;
+	__u8	reserved2;
+	__u32	reserved3;
+	__u32	reserved4;
+	__u32	reserved5;
+	__be32	flags;
+	__be32	reserved6[23];
+};
+
+enum fw_hdr_chip {
+	FW_HDR_CHIP_T4,
+	FW_HDR_CHIP_T5
+};
+
+#define S_FW_HDR_FW_VER_MAJOR	24
+#define M_FW_HDR_FW_VER_MAJOR	0xff
+#define V_FW_HDR_FW_VER_MAJOR(x) \
+    ((x) << S_FW_HDR_FW_VER_MAJOR)
+#define G_FW_HDR_FW_VER_MAJOR(x) \
+    (((x) >> S_FW_HDR_FW_VER_MAJOR) & M_FW_HDR_FW_VER_MAJOR)
+
+#define S_FW_HDR_FW_VER_MINOR	16
+#define M_FW_HDR_FW_VER_MINOR	0xff
+#define V_FW_HDR_FW_VER_MINOR(x) \
+    ((x) << S_FW_HDR_FW_VER_MINOR)
+#define G_FW_HDR_FW_VER_MINOR(x) \
+    (((x) >> S_FW_HDR_FW_VER_MINOR) & M_FW_HDR_FW_VER_MINOR)
+
+#define S_FW_HDR_FW_VER_MICRO	8
+#define M_FW_HDR_FW_VER_MICRO	0xff
+#define V_FW_HDR_FW_VER_MICRO(x) \
+    ((x) << S_FW_HDR_FW_VER_MICRO)
+#define G_FW_HDR_FW_VER_MICRO(x) \
+    (((x) >> S_FW_HDR_FW_VER_MICRO) & M_FW_HDR_FW_VER_MICRO)
+
+#define S_FW_HDR_FW_VER_BUILD	0
+#define M_FW_HDR_FW_VER_BUILD	0xff
+#define V_FW_HDR_FW_VER_BUILD(x) \
+    ((x) << S_FW_HDR_FW_VER_BUILD)
+#define G_FW_HDR_FW_VER_BUILD(x) \
+    (((x) >> S_FW_HDR_FW_VER_BUILD) & M_FW_HDR_FW_VER_BUILD)
+
+enum fw_hdr_intfver {
+	FW_HDR_INTFVER_NIC	= 0x00,
+	FW_HDR_INTFVER_VNIC	= 0x00,
+	FW_HDR_INTFVER_OFLD	= 0x00,
+	FW_HDR_INTFVER_RI	= 0x00,
+	FW_HDR_INTFVER_ISCSIPDU	= 0x00,
+	FW_HDR_INTFVER_ISCSI	= 0x00,
+	FW_HDR_INTFVER_FCOE	= 0x00,
+};
+
+enum fw_hdr_flags {
+	FW_HDR_FLAGS_RESET_HALT	= 0x00000001,
+};
+
+#endif /* _T4FW_INTERFACE_H_ */
+
diff --git a/drivers/net/cxgb4/t4vf_defs.h b/drivers/net/cxgb4/t4vf_defs.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/t4vf_defs.h
@@ -0,0 +1,91 @@
+/*
+ * This file is part of the Chelsio T4 Virtual Function (VF) Ethernet driver
+ * for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __T4VF_DEFS_H__
+#define __T4VF_DEFS_H__
+
+#include "t4_regs.h"
+
+/*
+ * The VF Register Map.
+ *
+ * The Scatter Gather Engine (SGE), Multiport Support module (MPS), PIO Local
+ * bus module (PL) and CPU Interface Module (CIM) components are mapped via
+ * the Slice to Module Map Table (see below) in the Physical Function Register
+ * Map.  The Mail Box Data (MBDATA) range is mapped via the PCI-E Mailbox Base
+ * and Offset registers in the PF Register Map.  The MBDATA base address is
+ * quite constrained as it determines the Mailbox Data addresses for both PFs
+ * and VFs, and therefore must fit in both the VF and PF Register Maps without
+ * overlapping other registers.
+ */
+#define T4VF_SGE_BASE_ADDR	0x0000
+#define T4VF_MPS_BASE_ADDR	0x0100
+#define T4VF_PL_BASE_ADDR	0x0200
+#define T4VF_MBDATA_BASE_ADDR	0x0240
+#define T4VF_CIM_BASE_ADDR	0x0300
+
+#define T4VF_REGMAP_START	0x0000
+#define T4VF_REGMAP_SIZE	0x0400
+
+/*
+ * There's no hardware limitation which requires that the addresses of the
+ * Mailbox Data in the fixed CIM PF map and the programmable VF map must
+ * match.  However, it's a useful convention ...
+ */
+#if T4VF_MBDATA_BASE_ADDR != A_CIM_PF_MAILBOX_DATA
+#error T4VF_MBDATA_BASE_ADDR must match A_CIM_PF_MAILBOX_DATA!
+#endif
+
+/*
+ * Virtual Function "Slice to Module Map Table" definitions.
+ *
+ * This table allows us to map subsets of the various module register sets
+ * into the T4VF Register Map.  Each table entry identifies the index of the
+ * module whose registers are being mapped, the offset within the module's
+ * register set that the mapping should start at, the limit of the mapping,
+ * and the offset within the T4VF Register Map to which the module's registers
+ * are being mapped.  All addresses and qualtities are in terms of 32-bit
+ * words.  The "limit" value is also in terms of 32-bit words and is equal to
+ * the last address mapped in the T4VF Register Map 1 (i.e. it's a "<="
+ * relation rather than a "<").
+ */
+#define T4VF_MOD_MAP(module, index, first, last) \
+	T4VF_MOD_MAP_##module##_INDEX  = (index), \
+	T4VF_MOD_MAP_##module##_FIRST  = (first), \
+	T4VF_MOD_MAP_##module##_LAST   = (last), \
+	T4VF_MOD_MAP_##module##_OFFSET = ((first)/4), \
+	T4VF_MOD_MAP_##module##_BASE = \
+		(T4VF_##module##_BASE_ADDR/4 + (first)/4), \
+	T4VF_MOD_MAP_##module##_LIMIT = \
+		(T4VF_##module##_BASE_ADDR/4 + (last)/4),
+
+enum {
+    T4VF_MOD_MAP(SGE, 2, A_SGE_VF_KDOORBELL, A_SGE_VF_GTS)
+    T4VF_MOD_MAP(MPS, 0, A_MPS_VF_CTL, A_MPS_VF_STAT_RX_VF_ERR_FRAMES_H)
+    T4VF_MOD_MAP(PL,  3, A_PL_VF_WHOAMI, A_PL_VF_WHOAMI)
+    T4VF_MOD_MAP(CIM, 1, A_CIM_VF_EXT_MAILBOX_CTRL, A_CIM_VF_EXT_MAILBOX_STATUS)
+};
+
+/*
+ * There isn't a Slice to Module Map Table entry for the Mailbox Data
+ * registers, but it's convenient to use similar names as above.  There are 8
+ * little-endian 64-bit Mailbox Data registers.  Note that the "instances"
+ * value below is in terms of 32-bit words which matches the "word" addressing
+ * space we use above for the Slice to Module Map Space.
+ */
+#define NUM_CIM_VF_MAILBOX_DATA_INSTANCES \
+	NUM_CIM_PF_MAILBOX_DATA_INSTANCES
+
+#define T4VF_MBDATA_FIRST	0
+#define T4VF_MBDATA_LAST	((NUM_CIM_VF_MAILBOX_DATA_INSTANCES-1)*4)
+
+#endif /* __T4T4VF_DEFS_H__ */
diff --git a/drivers/net/cxgb4/trace.c b/drivers/net/cxgb4/trace.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/trace.c
@@ -0,0 +1,159 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/*
+ *      Routines to allocate and free T4 trace buffers.
+ *
+ *      Authors:
+ *              Felix Marti <felix@chelsio.com>
+ *
+ *      The code suffers from a trace buffer count increment race, which might
+ *      lead to entries being overwritten. I don't really care about this,
+ *      because the trace buffer is a simple debug/perfomance tuning aid.
+ *
+ *      Trace buffers are created in /proc, which needs to be fixed.
+ */
+
+#include "trace.h"
+
+#ifdef T4_TRACE
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/log2.h>
+#include <linux/slab.h>
+
+/*
+ * SEQ OPS
+ */
+static void *t4_trace_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	struct trace_buf *tb = seq->private;
+	struct trace_entry *e = NULL;
+	unsigned int start, count;
+
+	if (tb->idx > tb->capacity) {
+		start = tb->idx & (tb->capacity - 1);
+		count = tb->capacity;
+	} else {
+		start = 0;
+		count = tb->idx;
+	}
+
+	if (*pos < count)
+		e = &tb->ep[(start + *pos) & (tb->capacity - 1)];
+
+	return e;
+}
+
+static void *t4_trace_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	struct trace_buf *tb = seq->private;
+	struct trace_entry *e = v;
+	unsigned int count = min(tb->idx, tb->capacity);
+
+	if (++*pos < count) {
+		e++;
+		if (e >= &tb->ep[tb->capacity])
+			e = tb->ep;
+	} else
+		e = NULL;
+
+	return e;
+}
+
+static void t4_trace_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int t4_trace_seq_show(struct seq_file *seq, void *v)
+{
+	struct trace_entry *ep = v;
+
+	seq_printf(seq, "%016llx ", (unsigned long long) ep->tsc);
+	seq_printf(seq, ep->fmt, ep->param[0], ep->param[1], ep->param[2],
+		   ep->param[3], ep->param[4], ep->param[5]);
+	seq_printf(seq, "\n");
+	return 0;
+}
+
+static const struct seq_operations t4_trace_seq_ops = {
+	.start = t4_trace_seq_start,
+	.next  = t4_trace_seq_next,
+	.stop  = t4_trace_seq_stop,
+	.show  = t4_trace_seq_show
+};
+
+/*
+ * FILE OPS
+ */
+static int t4_trace_seq_open(struct inode *inode, struct file *file)
+{
+	int rc = seq_open(file, &t4_trace_seq_ops);
+
+	if (!rc) {
+		struct seq_file *seq = file->private_data;
+
+		seq->private = inode->i_private;
+	}
+
+	return rc;
+}
+
+static const struct file_operations t4_trace_seq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = t4_trace_seq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release
+};
+
+/*
+ * TRACEBUFFER API
+ */
+struct trace_buf *t4_trace_alloc(struct dentry *root, const char *name,
+		unsigned int capacity)
+{
+	struct trace_buf *tb;
+	unsigned int size;
+
+	if (!name || !is_power_of_2(capacity))
+		return NULL;
+
+	size = sizeof(*tb) + sizeof(struct trace_entry) * capacity;
+	tb = kmalloc(size, GFP_KERNEL);
+	if (!tb)
+		return NULL;
+
+	memset(tb, 0, size);
+	tb->capacity = capacity;
+	tb->debugfs_dentry = debugfs_create_file(name, S_IFREG | S_IRUGO, root,
+			tb, &t4_trace_seq_fops);
+	if (!tb->debugfs_dentry) {
+		kfree(tb);
+		return NULL;
+	}
+
+	return tb;
+}
+
+void t4_trace_free(struct trace_buf *tb)
+{
+	if (tb) {
+		if (tb->debugfs_dentry)
+			debugfs_remove(tb->debugfs_dentry);
+		kfree(tb);
+	}
+}
+EXPORT_SYMBOL(t4_trace_alloc);
+EXPORT_SYMBOL(t4_trace_free);
+#endif /* T4_TRACE */
diff --git a/drivers/net/cxgb4/trace.h b/drivers/net/cxgb4/trace.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb4/trace.h
@@ -0,0 +1,122 @@
+/*
+ * This file is part of the Chelsio T4 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2010 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/*
+ *      Definitions and inline functions for the T4 trace buffers.
+ *
+ *      Authors:
+ *              Felix Marti <felix@chelsio.com>
+ */
+
+#ifndef __T4_TRACE_H__
+#define __T4_TRACE_H__
+
+#ifdef T4_TRACE
+
+#include <linux/time.h>
+#include <linux/timex.h>
+
+#define T4_TRACE_NUM_PARAM 6
+
+typedef unsigned long tracearg_t;
+
+#define T4_TRACE0(b, s) \
+	if ((b) != NULL) \
+		(void) t4_trace((b), (s));
+#define	T4_TRACE1(b, s, p0) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+	}
+#define T4_TRACE2(b, s, p0, p1) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+		_p[1] = (tracearg_t) (p1); \
+	}
+#define T4_TRACE3(b, s, p0, p1, p2) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+		_p[1] = (tracearg_t) (p1); \
+		_p[2] = (tracearg_t) (p2); \
+	}
+#define T4_TRACE4(b, s, p0, p1, p2, p3) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+		_p[1] = (tracearg_t) (p1); \
+		_p[2] = (tracearg_t) (p2); \
+		_p[3] = (tracearg_t) (p3); \
+	}
+#define T4_TRACE5(b, s, p0, p1, p2, p3, p4) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+		_p[1] = (tracearg_t) (p1); \
+		_p[2] = (tracearg_t) (p2); \
+		_p[3] = (tracearg_t) (p3); \
+		_p[4] = (tracearg_t) (p4); \
+	}
+#define T4_TRACE6(b, s, p0, p1, p2, p3, p4, p5) \
+	if ((b) != NULL) { \
+		tracearg_t *_p = t4_trace((b), (s)); \
+		_p[0] = (tracearg_t) (p0); \
+		_p[1] = (tracearg_t) (p1); \
+		_p[2] = (tracearg_t) (p2); \
+		_p[3] = (tracearg_t) (p3); \
+		_p[4] = (tracearg_t) (p4); \
+		_p[5] = (tracearg_t) (p5); \
+	}
+
+struct trace_entry {
+	cycles_t   tsc;
+	char      *fmt;
+	tracearg_t param[T4_TRACE_NUM_PARAM];
+};
+
+struct dentry;
+
+struct trace_buf {
+	unsigned int capacity;          /* size of ring buffer */
+	unsigned int idx;               /* index of next entry to write */
+	struct dentry *debugfs_dentry;
+	struct trace_entry ep[0];       /* the ring buffer */
+};
+
+static inline unsigned long *t4_trace(struct trace_buf *tb, char *fmt)
+{
+	struct trace_entry *ep = &tb->ep[tb->idx++ & (tb->capacity - 1)];
+
+	ep->fmt = fmt;
+	ep->tsc = get_cycles();
+
+	return (unsigned long *) &ep->param[0];
+}
+
+struct trace_buf *t4_trace_alloc(struct dentry *root, const char *name,
+			      unsigned int capacity);
+void t4_trace_free(struct trace_buf *tb);
+
+#else
+#define T4_TRACE0(b, s)
+#define T4_TRACE1(b, s, p0)
+#define T4_TRACE2(b, s, p0, p1)
+#define T4_TRACE3(b, s, p0, p1, p2)
+#define T4_TRACE4(b, s, p0, p1, p2, p3)
+#define T4_TRACE5(b, s, p0, p1, p2, p3, p4)
+#define T4_TRACE6(b, s, p0, p1, p2, p3, p4, p5)
+
+#define t4_trace_alloc(root, name, capacity) NULL
+#define t4_trace_free(tb)
+#endif
+
+#endif /* __T4_TRACE_H__ */
