diff -r c23d1fc7e422 drivers/net/mlx4/Makefile
--- a/drivers/net/mlx4/Makefile
+++ b/drivers/net/mlx4/Makefile
@@ -1,9 +1,10 @@
 obj-$(CONFIG_MLX4_CORE)		+= mlx4_core.o
 
 mlx4_core-y :=	alloc.o catas.o cmd.o cq.o eq.o fw.o icm.o intf.o main.o mcg.o \
-		mr.o pd.o port.o profile.o qp.o reset.o sense.o srq.o
+ 		mr.o pd.o port.o profile.o qp.o reset.o srq.o xrcd.o \
+		resource_tracker.o
 
 obj-$(CONFIG_MLX4_EN)               += mlx4_en.o
 
 mlx4_en-y := 	en_main.o en_tx.o en_rx.o en_ethtool.o en_port.o en_cq.o \
-		en_resources.o en_netdev.o
+		en_resources.o en_netdev.o en_selftest.o sys_tune.o
diff -r c23d1fc7e422 drivers/net/mlx4/alloc.c
--- a/drivers/net/mlx4/alloc.c
+++ b/drivers/net/mlx4/alloc.c
@@ -62,6 +62,9 @@ u32 mlx4_bitmap_alloc(struct mlx4_bitmap
 	} else
 		obj = -1;
 
+	if (obj != -1)
+		--bitmap->avail;
+
 	spin_unlock(&bitmap->lock);
 
 	return obj;
@@ -74,14 +77,14 @@ void mlx4_bitmap_free(struct mlx4_bitmap
 
 static unsigned long find_aligned_range(unsigned long *bitmap,
 					u32 start, u32 nbits,
-					int len, int align)
+					int len, int align, u32 skip_mask)
 {
 	unsigned long end, i;
 
 again:
 	start = ALIGN(start, align);
 
-	while ((start < nbits) && test_bit(start, bitmap))
+	while ((start < nbits) && (test_bit(start, bitmap) || (start & skip_mask)))
 		start += align;
 
 	if (start >= nbits)
@@ -92,7 +95,7 @@ again:
 		return -1;
 
 	for (i = start + 1; i < end; i++) {
-		if (test_bit(i, bitmap)) {
+		if (test_bit(i, bitmap) || ((u32)i & skip_mask)) {
 			start = i + 1;
 			goto again;
 		}
@@ -101,22 +104,23 @@ again:
 	return start;
 }
 
-u32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt, int align)
+u32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt, int align,
+			    u32 skip_mask)
 {
 	u32 obj, i;
 
-	if (likely(cnt == 1 && align == 1))
+	if (likely(cnt == 1 && align == 1 && !skip_mask))
 		return mlx4_bitmap_alloc(bitmap);
 
 	spin_lock(&bitmap->lock);
 
 	obj = find_aligned_range(bitmap->table, bitmap->last,
-				 bitmap->max, cnt, align);
+				 bitmap->max, cnt, align, skip_mask);
 	if (obj >= bitmap->max) {
 		bitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)
 				& bitmap->mask;
 		obj = find_aligned_range(bitmap->table, 0, bitmap->max,
-					 cnt, align);
+					 cnt, align, skip_mask);
 	}
 
 	if (obj < bitmap->max) {
@@ -131,11 +135,19 @@ u32 mlx4_bitmap_alloc_range(struct mlx4_
 	} else
 		obj = -1;
 
+	if (obj != -1)
+		bitmap->avail -= cnt;
+
 	spin_unlock(&bitmap->lock);
 
 	return obj;
 }
 
+u32 mlx4_bitmap_avail(struct mlx4_bitmap *bitmap)
+{
+	return bitmap->avail;
+}
+
 void mlx4_bitmap_free_range(struct mlx4_bitmap *bitmap, u32 obj, int cnt)
 {
 	u32 i;
@@ -148,6 +160,7 @@ void mlx4_bitmap_free_range(struct mlx4_
 	bitmap->last = min(bitmap->last, obj);
 	bitmap->top = (bitmap->top + bitmap->max + bitmap->reserved_top)
 			& bitmap->mask;
+	bitmap->avail += cnt;
 	spin_unlock(&bitmap->lock);
 }
 
@@ -165,6 +178,7 @@ int mlx4_bitmap_init(struct mlx4_bitmap 
 	bitmap->max  = num - reserved_top;
 	bitmap->mask = mask;
 	bitmap->reserved_top = reserved_top;
+	bitmap->avail = num - reserved_top - reserved_bot;
 	spin_lock_init(&bitmap->lock);
 	bitmap->table = kzalloc(BITS_TO_LONGS(bitmap->max) *
 				sizeof (long), GFP_KERNEL);
@@ -177,6 +191,16 @@ int mlx4_bitmap_init(struct mlx4_bitmap 
 	return 0;
 }
 
+/* Like bitmap_init, but doesn't require 'num' to be a power of 2 or
+ * a non-trivial mask */
+int mlx4_bitmap_init_no_mask(struct mlx4_bitmap *bitmap, u32 num,
+			     u32 reserved_bot, u32 reserved_top)
+{
+	u32 num_rounded = roundup_pow_of_two(num);
+	return mlx4_bitmap_init(bitmap, num_rounded, num_rounded - 1,
+				reserved_bot, num_rounded - num + reserved_top);
+}
+
 void mlx4_bitmap_cleanup(struct mlx4_bitmap *bitmap)
 {
 	kfree(bitmap->table);
@@ -190,7 +214,7 @@ void mlx4_bitmap_cleanup(struct mlx4_bit
  */
 
 int mlx4_buf_alloc(struct mlx4_dev *dev, int size, int max_direct,
-		   struct mlx4_buf *buf)
+		   struct mlx4_buf *buf, int numa_node)
 {
 	dma_addr_t t;
 
@@ -214,11 +238,19 @@ int mlx4_buf_alloc(struct mlx4_dev *dev,
 	} else {
 		int i;
 
+		buf->direct.buf  = NULL;
 		buf->nbufs       = (size + PAGE_SIZE - 1) / PAGE_SIZE;
 		buf->npages      = buf->nbufs;
 		buf->page_shift  = PAGE_SHIFT;
-		buf->page_list   = kzalloc(buf->nbufs * sizeof *buf->page_list,
-					   GFP_KERNEL);
+		buf->page_list	 = kzalloc_node(
+					buf->nbufs * sizeof *buf->page_list,
+					GFP_KERNEL,
+					numa_node);
+
+		if (!buf->page_list)
+			buf->page_list = kzalloc(
+					buf->nbufs * sizeof *buf->page_list,
+					GFP_KERNEL);
 		if (!buf->page_list)
 			return -ENOMEM;
 
@@ -236,9 +268,16 @@ int mlx4_buf_alloc(struct mlx4_dev *dev,
 
 		if (BITS_PER_LONG == 64) {
 			struct page **pages;
-			pages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);
+			pages = kmalloc_node(sizeof *pages * buf->nbufs,
+					 GFP_KERNEL, numa_node);
+
+			if (!pages)
+				pages = kmalloc(sizeof *pages * buf->nbufs,
+						 GFP_KERNEL);
+
 			if (!pages)
 				goto err_free;
+
 			for (i = 0; i < buf->nbufs; ++i)
 				pages[i] = virt_to_page(buf->page_list[i].buf);
 			buf->direct.buf = vmap(pages, buf->nbufs, VM_MAP, PAGE_KERNEL);
@@ -265,7 +304,7 @@ void mlx4_buf_free(struct mlx4_dev *dev,
 		dma_free_coherent(&dev->pdev->dev, size, buf->direct.buf,
 				  buf->direct.map);
 	else {
-		if (BITS_PER_LONG == 64)
+		if (BITS_PER_LONG == 64 && buf->direct.buf)
 			vunmap(buf->direct.buf);
 
 		for (i = 0; i < buf->nbufs; ++i)
@@ -278,11 +317,16 @@ void mlx4_buf_free(struct mlx4_dev *dev,
 }
 EXPORT_SYMBOL_GPL(mlx4_buf_free);
 
-static struct mlx4_db_pgdir *mlx4_alloc_db_pgdir(struct device *dma_device)
+static struct mlx4_db_pgdir *mlx4_alloc_db_pgdir(struct device *dma_device,
+						 int numa_node)
 {
 	struct mlx4_db_pgdir *pgdir;
 
-	pgdir = kzalloc(sizeof *pgdir, GFP_KERNEL);
+	pgdir = kzalloc_node(sizeof *pgdir, GFP_KERNEL, numa_node);
+
+	if (!pgdir)
+		pgdir = kzalloc(sizeof *pgdir, GFP_KERNEL);
+
 	if (!pgdir)
 		return NULL;
 
@@ -330,7 +374,8 @@ found:
 	return 0;
 }
 
-int mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db, int order)
+int mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db,
+		  int order, int numa_node)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_db_pgdir *pgdir;
@@ -342,7 +387,7 @@ int mlx4_db_alloc(struct mlx4_dev *dev, 
 		if (!mlx4_alloc_db_from_pgdir(pgdir, db, order))
 			goto out;
 
-	pgdir = mlx4_alloc_db_pgdir(&(dev->pdev->dev));
+	pgdir = mlx4_alloc_db_pgdir(&(dev->pdev->dev), numa_node);
 	if (!pgdir) {
 		ret = -ENOMEM;
 		goto out;
@@ -390,17 +435,17 @@ void mlx4_db_free(struct mlx4_dev *dev, 
 EXPORT_SYMBOL_GPL(mlx4_db_free);
 
 int mlx4_alloc_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,
-		       int size, int max_direct)
+		       int size, int max_direct, int numa_node)
 {
 	int err;
 
-	err = mlx4_db_alloc(dev, &wqres->db, 1);
+	err = mlx4_db_alloc(dev, &wqres->db, 1, numa_node);
 	if (err)
 		return err;
 
 	*wqres->db.db = 0;
 
-	err = mlx4_buf_alloc(dev, size, max_direct, &wqres->buf);
+	err = mlx4_buf_alloc(dev, size, max_direct, &wqres->buf, numa_node);
 	if (err)
 		goto err_db;
 
diff -r c23d1fc7e422 drivers/net/mlx4/catas.c
--- a/drivers/net/mlx4/catas.c
+++ b/drivers/net/mlx4/catas.c
@@ -44,7 +44,7 @@ static DEFINE_SPINLOCK(catas_lock);
 static LIST_HEAD(catas_list);
 static struct work_struct catas_work;
 
-static int internal_err_reset = 0;
+static int internal_err_reset = 1;
 module_param(internal_err_reset, int, 0644);
 MODULE_PARM_DESC(internal_err_reset,
 		 "Reset device on internal errors if non-zero (default 1)");
@@ -91,6 +91,9 @@ static void catas_reset(struct work_stru
 	LIST_HEAD(tlist);
 	int ret;
 
+	if (!mutex_trylock(&drv_mutex))
+		return;
+
 	spin_lock_irq(&catas_lock);
 	list_splice_init(&catas_list, &tlist);
 	spin_unlock_irq(&catas_lock);
@@ -108,6 +111,7 @@ static void catas_reset(struct work_stru
 			mlx4_dbg(dev, "Reset succeeded\n");
 		}
 	}
+	mutex_unlock(&drv_mutex);
 }
 
 void mlx4_start_catas_poll(struct mlx4_dev *dev)
diff -r c23d1fc7e422 drivers/net/mlx4/cmd.c
--- a/drivers/net/mlx4/cmd.c
+++ b/drivers/net/mlx4/cmd.c
@@ -35,14 +35,17 @@
 #include <linux/sched.h>
 #include <linux/pci.h>
 #include <linux/errno.h>
+#include <linux/semaphore.h>
 
 #include <linux/mlx4/cmd.h>
 
 #include <asm/io.h>
 
 #include "mlx4.h"
+#include "fw.h"
 
 #define CMD_POLL_TOKEN 0xffff
+#define INBOX_MASK	0xffffffffffffff00ULL
 
 enum {
 	/* command completed successfully: */
@@ -108,6 +111,7 @@ struct mlx4_cmd_context {
 	int			next;
 	u64			out_param;
 	u16			token;
+	u8			fw_status;
 };
 
 static int mlx4_status_to_errno(u8 status)
@@ -140,6 +144,107 @@ static int mlx4_status_to_errno(u8 statu
 	return trans_table[status];
 }
 
+static int comm_pending(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u32 status = readl(&priv->mfunc.comm->slave_read);
+
+	return (swab32(status) >> 30) != priv->cmd.comm_toggle;
+}
+
+static void mlx4_comm_cmd_post(struct mlx4_dev *dev, u8 cmd, u16 param)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u32 val;
+
+	if (cmd == MLX4_COMM_CMD_RESET)
+		priv->cmd.comm_toggle = 0;
+	else if (++priv->cmd.comm_toggle > 2)
+		priv->cmd.comm_toggle = 1;
+	val = param | (cmd << 16) | (priv->cmd.comm_toggle << 30);
+	__raw_writel((__force u32) cpu_to_be32(val), &priv->mfunc.comm->slave_write);
+	wmb();
+}
+
+int mlx4_comm_cmd_poll(struct mlx4_dev *dev, u8 cmd, u16 param, unsigned long timeout)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	unsigned long end;
+	int err = 0;
+
+	/* First, verify that the master reports correct status */
+	if (comm_pending(dev)) {
+		mlx4_warn(dev, "Communication channel is not idle\n");
+		return -EAGAIN;
+	}
+
+	/* Write command */
+	down(&priv->cmd.poll_sem);
+	mlx4_comm_cmd_post(dev, cmd, param);
+
+	end = msecs_to_jiffies(timeout) + jiffies;
+	while (comm_pending(dev) && time_before(jiffies, end))
+		cond_resched();
+
+	if (comm_pending(dev)) {
+		mlx4_warn(dev, "Communication channel timed out\n");
+		err = -ETIMEDOUT;
+	}
+
+	up(&priv->cmd.poll_sem);
+	return 0;
+}
+
+static int mlx4_comm_cmd_wait(struct mlx4_dev *dev, u8 op,
+			      u16 param, unsigned long timeout)
+{
+	struct mlx4_cmd *cmd = &mlx4_priv(dev)->cmd;
+	struct mlx4_cmd_context *context;
+	int err = 0;
+
+	down(&cmd->event_sem);
+
+	spin_lock(&cmd->context_lock);
+	BUG_ON(cmd->free_head < 0);
+	context = &cmd->context[cmd->free_head];
+	context->token += cmd->token_mask + 1;
+	cmd->free_head = context->next;
+	spin_unlock(&cmd->context_lock);
+
+	init_completion(&context->done);
+
+	mlx4_comm_cmd_post(dev, op, param);
+
+	if (!wait_for_completion_timeout(&context->done, msecs_to_jiffies(timeout))) {
+		mlx4_warn(dev, "communication channel command 0x%x timed out\n", op);
+		err = -EBUSY;
+		goto out;
+	}
+
+	err = context->result;
+	if (err && context->fw_status != CMD_STAT_MULTI_FUNC_REQ) {
+		mlx4_err(dev, "command 0x%x failed: fw status = 0x%x\n",
+			 op, context->fw_status);
+		goto out;
+	}
+
+out:
+	spin_lock(&cmd->context_lock);
+	context->next = cmd->free_head;
+	cmd->free_head = context - cmd->context;
+	spin_unlock(&cmd->context_lock);
+
+	up(&cmd->event_sem);
+	return err;
+}
+
+int mlx4_comm_cmd(struct mlx4_dev *dev, u8 cmd, u16 param, unsigned long timeout)
+{
+	if (mlx4_priv(dev)->cmd.use_events)
+		return mlx4_comm_cmd_wait(dev, cmd, param, timeout);
+	return mlx4_comm_cmd_poll(dev, cmd, param, timeout);
+}
+
 static int cmd_pending(struct mlx4_dev *dev)
 {
 	u32 status = readl(mlx4_priv(dev)->cmd.hcr + HCR_STATUS_OFFSET);
@@ -165,8 +270,10 @@ static int mlx4_cmd_post(struct mlx4_dev
 		end += msecs_to_jiffies(GO_BIT_TIMEOUT_MSECS);
 
 	while (cmd_pending(dev)) {
-		if (time_after_eq(jiffies, end))
+		if (time_after_eq(jiffies, end)) {
+			mlx4_warn(dev, "cannot post the command 0x%x (go bit not cleared)\n", op);
 			goto out;
+		}
 		cond_resched();
 	}
 
@@ -207,6 +314,33 @@ out:
 	return ret;
 }
 
+static int mlx4_slave_cmd(struct mlx4_dev *dev, u64 in_param, u64 *out_param,
+			  int out_is_imm, u32 in_modifier, u8 op_modifier,
+			  u16 op, unsigned long timeout)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_vhcr *vhcr = priv->mfunc.vhcr;
+	int ret;
+
+	down(&priv->cmd.slave_sem);
+	vhcr->in_param = in_param;
+	vhcr->out_param = out_param ? *out_param : 0;
+	vhcr->in_modifier = in_modifier;
+	vhcr->timeout = timeout;
+	vhcr->op = op;
+	vhcr->token = CMD_POLL_TOKEN;
+	vhcr->op_modifier = op_modifier;
+	vhcr->errno = 0;
+	ret = mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR_POST, 0, MLX4_COMM_TIME + timeout);
+	if (!ret) {
+		if (out_is_imm)
+			*out_param = vhcr->out_param;
+		ret = vhcr->errno;
+	}
+	up(&priv->cmd.slave_sem);
+	return ret;
+}
+
 static int mlx4_cmd_poll(struct mlx4_dev *dev, u64 in_param, u64 *out_param,
 			 int out_is_imm, u32 in_modifier, u8 op_modifier,
 			 u16 op, unsigned long timeout)
@@ -215,6 +349,7 @@ static int mlx4_cmd_poll(struct mlx4_dev
 	void __iomem *hcr = priv->cmd.hcr;
 	int err = 0;
 	unsigned long end;
+	u32 stat;
 
 	down(&priv->cmd.poll_sem);
 
@@ -228,6 +363,7 @@ static int mlx4_cmd_poll(struct mlx4_dev
 		cond_resched();
 
 	if (cmd_pending(dev)) {
+		mlx4_warn(dev, "command 0x%x timed out (go bit not cleared)\n", op);
 		err = -ETIMEDOUT;
 		goto out;
 	}
@@ -238,9 +374,14 @@ static int mlx4_cmd_poll(struct mlx4_dev
 					  __raw_readl(hcr + HCR_OUT_PARAM_OFFSET)) << 32 |
 			(u64) be32_to_cpu((__force __be32)
 					  __raw_readl(hcr + HCR_OUT_PARAM_OFFSET + 4));
-
-	err = mlx4_status_to_errno(be32_to_cpu((__force __be32)
-					       __raw_readl(hcr + HCR_STATUS_OFFSET)) >> 24);
+	stat = be32_to_cpu((__force __be32) __raw_readl(hcr + HCR_STATUS_OFFSET)) >> 24;
+	err = mlx4_status_to_errno(stat);
+	if (err) {
+		if ((op != MLX4_CMD_SET_NODE || stat != CMD_STAT_BAD_OP) &&
+		    stat != CMD_STAT_MULTI_FUNC_REQ)
+			mlx4_err(dev, "command 0x%x failed: fw status = 0x%x\n",
+				 op, stat);
+	}
 
 out:
 	up(&priv->cmd.poll_sem);
@@ -257,6 +398,7 @@ void mlx4_cmd_event(struct mlx4_dev *dev
 	if (token != context->token)
 		return;
 
+	context->fw_status = status;
 	context->result    = mlx4_status_to_errno(status);
 	context->out_param = out_param;
 
@@ -286,15 +428,21 @@ static int mlx4_cmd_wait(struct mlx4_dev
 		      in_modifier, op_modifier, op, context->token, 1);
 
 	if (!wait_for_completion_timeout(&context->done, msecs_to_jiffies(timeout))) {
+		mlx4_warn(dev, "command 0x%x timed out (go bit not cleared)\n", op);
 		err = -EBUSY;
 		goto out;
 	}
 
 	err = context->result;
-	if (err)
+	if (err) {
+		if ((op != MLX4_CMD_SET_NODE || context->fw_status != CMD_STAT_BAD_OP) &&
+		    context->fw_status != CMD_STAT_MULTI_FUNC_REQ)
+			mlx4_err(dev, "command 0x%x failed: fw status = 0x%x\n",
+				 op, context->fw_status);
 		goto out;
+	}
 
-	if (out_is_imm)
+	if (out_is_imm && out_param)
 		*out_param = context->out_param;
 
 out:
@@ -311,6 +459,10 @@ int __mlx4_cmd(struct mlx4_dev *dev, u64
 	       int out_is_imm, u32 in_modifier, u8 op_modifier,
 	       u16 op, unsigned long timeout)
 {
+	if (mlx4_is_slave(dev))
+		return mlx4_slave_cmd(dev, in_param, out_param, out_is_imm,
+				      in_modifier, op_modifier, op, timeout);
+
 	if (mlx4_priv(dev)->cmd.use_events)
 		return mlx4_cmd_wait(dev, in_param, out_param, out_is_imm,
 				     in_modifier, op_modifier, op, timeout);
@@ -320,6 +472,1180 @@ int __mlx4_cmd(struct mlx4_dev *dev, u64
 }
 EXPORT_SYMBOL_GPL(__mlx4_cmd);
 
+
+static int mlx4_ARM_COMM_CHANNEL(struct mlx4_dev *dev)
+{
+	return mlx4_cmd(dev, 0, 0, 0, MLX4_CMD_ARM_COMM_CHANNEL, MLX4_CMD_TIME_CLASS_B);
+}
+
+int mlx4_GEN_EQE(struct mlx4_dev *dev, int slave, struct mlx4_eqe *eqe)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_event_eq_info *event_eq =
+		&priv->mfunc.master.slave_state[slave].event_eq;
+	struct mlx4_cmd_mailbox *mailbox;
+	u32 in_modifier = 0;
+	int err;
+
+	if (!event_eq->use_int)
+		return 0;
+
+	/* Create the event only if the slave is registered */
+	if ((event_eq->event_type & eqe->type) == 0)
+		return 0;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	if (eqe->type == MLX4_EVENT_TYPE_CMD) {
+		++event_eq->token;
+		eqe->event.cmd.token = cpu_to_be16(event_eq->token);
+	}
+
+	memcpy(mailbox->buf, (u8 *) eqe, 28);
+
+	in_modifier = (slave & 0xff) | ((event_eq->eqn & 0xff) << 16);
+
+	err = mlx4_cmd(dev, mailbox->dma, in_modifier, 0,
+		       MLX4_CMD_GEN_EQE, MLX4_CMD_TIME_CLASS_B);
+
+       mlx4_free_cmd_mailbox(dev, mailbox);
+       return err;
+}
+
+static int mlx4_ACCESS_MEM(struct mlx4_dev *dev, u64 master_addr,
+			   int slave, u64 slave_addr,
+			   int size, int is_read)
+{
+	u64 in_param;
+	u64 out_param;
+
+	if ((slave_addr & 0xfff) | (master_addr & 0xfff) |
+	    (slave & ~0x7f) | (size & 0xff)) {
+		mlx4_err(dev, "Bad access mem params - slave_addr:0x%llx "
+			      "master_addr:0x%llx slave_id:%d size:%d\n",
+			      slave_addr, master_addr, slave, size);
+		return -EINVAL;
+	}
+
+	if (is_read) {
+		in_param = (u64) slave | slave_addr;
+		out_param = (u64) dev->caps.function | master_addr;
+	} else {
+		in_param = (u64) dev->caps.function | master_addr;
+		out_param = (u64) slave | slave_addr;
+	}
+
+	return mlx4_cmd_imm(dev, in_param, &out_param, size, 0,
+					   MLX4_CMD_ACCESS_MEM,
+					   MLX4_CMD_TIME_CLASS_A);
+}
+
+static int mlx4_RESOURCE_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						       struct mlx4_cmd_mailbox *inbox,
+						       struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u32 param1 = *((u32 *) &vhcr->in_param);
+	u32 param2 = *(((u32 *) &vhcr->in_param) + 1);
+	int ret;
+	int rt_res;
+	u8 vep_num = priv->mfunc.master.slave_state[slave].vep_num;
+	u32 res_bit_mask = 0;
+	int res_id ;
+
+	vhcr->errno = 0;
+	switch (vhcr->in_modifier) {
+	case RES_QP:
+		switch (vhcr->op_modifier) {
+		case ICM_RESERVE:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+				vhcr->errno = mlx4_qp_reserve_range(dev, param1, param2, &ret, 0);
+				if (!vhcr->errno) {
+					vhcr->out_param = ret;
+					rt_res = mlx4_add_range_resource_for_slave(dev,
+										    RES_QP,
+										    slave,
+										    ret,
+										    param1);
+					if (rt_res) {
+						mlx4_info(dev, "mlx4_add_resource_for_slave"
+							       " failed, ret: %d for slave: %d",
+							   rt_res, slave);
+						mlx4_qp_release_range(dev, ret, param1);
+						return rt_res;
+					}
+				}
+			} else {
+				mlx4_qp_release_range(dev, param1, param2);
+				mlx4_delete_range_resource_for_slave(dev,
+								      RES_QP,
+								      slave, param1, param2);
+			}
+			break;
+		case ICM_ALLOC:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+					vhcr->errno = mlx4_qp_alloc_icm(dev, param1);
+				if (!vhcr->errno) {
+					rt_res = mlx4_add_resource_for_slave(
+						dev, RES_QP, slave, param1,
+						 RES_ALLOCATED_AFTER_RESERVATION);
+					if (rt_res) {
+						mlx4_info(dev, "mlx4_add_resource_for_slave"
+							       " failed, ret: %d for slave: %d",
+							   rt_res, slave);
+						mlx4_qp_free_icm(dev, param1);
+						return rt_res;
+					}
+				}
+
+			} else {
+				mlx4_qp_free_icm(dev, param1);
+				mlx4_delete_resource_for_slave(dev, RES_QP, slave, param1);
+			}
+			break;
+		default:
+			vhcr->errno = -EINVAL;
+		}
+		break;
+	case RES_CQ:
+		if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+			vhcr->errno = mlx4_cq_alloc_icm(dev, &ret);
+			if (!vhcr->errno) {
+					vhcr->out_param = ret;
+					rt_res = mlx4_add_resource_for_slave(dev, RES_CQ, slave, ret, RES_INIT);
+					if (rt_res) {
+						mlx4_info(dev, "mlx4_add_resource_for_slave failed,"
+							       " ret: %d for slave: %d", rt_res, slave);
+						mlx4_cq_free_icm(dev, ret);
+						return rt_res;
+					}
+			}
+		} else {
+				mlx4_cq_free_icm(dev, param1);
+				mlx4_delete_resource_for_slave(dev,
+							       RES_CQ,
+							       slave,
+							       param1);
+		}
+		break;
+	case RES_SRQ:
+		if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+			vhcr->errno = mlx4_srq_alloc_icm(dev, &ret);
+			if (!vhcr->errno) {
+					vhcr->out_param = ret;
+					rt_res = mlx4_add_resource_for_slave(dev,
+									      RES_SRQ,
+									      slave,
+									      ret,
+									      RES_INIT);
+					if (rt_res) {
+						mlx4_info(dev, "mlx4_add_resource_for_slave"
+							       " failed, ret: %d for slave:"
+							       " %d", rt_res, slave);
+						mlx4_srq_free_icm(dev, ret);
+						return rt_res;
+					}
+			}
+		} else {
+				mlx4_srq_free_icm(dev, param1);
+				mlx4_delete_resource_for_slave(dev, RES_SRQ, slave, param1);
+		}
+		break;
+	case RES_MPT:
+		switch (vhcr->op_modifier) {
+		case ICM_RESERVE:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+				ret = mlx4_mr_reserve(dev);
+				if (ret == -1)
+					vhcr->errno = -ENOMEM;
+				else {
+					res_bit_mask = calculate_bitmap_mask(dev, RES_MPT);
+					vhcr->out_param = ret;
+					res_id = ret & res_bit_mask;
+					rt_res = mlx4_add_resource_for_slave(dev, RES_MPT,
+									      slave,
+									      res_id,
+									      RES_INIT);
+					if (rt_res) {
+						mlx4_info(dev, "mlx4_add_resource_for_slave"
+							       " failed, ret: %d for "
+							       "slave: %d\n",
+							   rt_res, slave);
+						mlx4_mr_release(dev, ret);
+						return rt_res;
+					}
+				}
+			} else {
+				mlx4_mr_release(dev, param1);
+				res_bit_mask = calculate_bitmap_mask(dev, RES_MPT);
+				res_id = param1 & res_bit_mask ;
+				mlx4_delete_resource_for_slave(dev, RES_MPT, slave, res_id);
+			}
+			break;
+		case ICM_ALLOC:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+					vhcr->errno = mlx4_mr_alloc_icm(dev, param1);
+					if (!vhcr->errno)  {
+						res_bit_mask = calculate_bitmap_mask(dev, RES_MPT);
+						res_id = param1 & res_bit_mask ;
+						rt_res = mlx4_add_resource_for_slave
+							(dev, RES_MPT, slave, res_id,
+							  RES_ALLOCATED_AFTER_RESERVATION);
+						if (rt_res) {
+							mlx4_info(dev,
+								   "mlx4_add_resource_for_slave"
+								   "failed, ret: %d for"
+								   " slave: %d\n",
+								   rt_res, slave);
+							mlx4_mr_free_icm(dev, param1);
+							return rt_res;
+						}
+					}
+			} else {
+				mlx4_mr_free_icm(dev, param1);
+				res_bit_mask = calculate_bitmap_mask(dev, RES_MPT);
+				res_id = param1 & res_bit_mask ;
+				mlx4_delete_resource_for_slave(dev, RES_MPT, slave, res_id);
+			}
+			break;
+		default:
+			vhcr->errno = -EINVAL;
+		}
+		break;
+	case RES_MTT:
+		if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+		ret = mlx4_alloc_mtt_range(dev, param1 /* order */);
+		if (ret == -1) {
+			vhcr->errno = -ENOMEM;
+			mlx4_info(dev, "mlx4_alloc_mtt_range failed, ret: %d for slave: %d", ret, slave);
+		}
+		else {
+			vhcr->out_param = ret;
+			rt_res = mlx4_add_mtt_resource_for_slave(dev, slave, ret, RES_INIT, param1); 
+			if (rt_res) {
+				mlx4_info(dev, "mlx4_add_resource_for_slave failed, ret: %d for slave: %d", rt_res, slave);
+				mlx4_free_mtt_range(dev, ret /* first */, param1 /* order */);
+				return rt_res;
+			}
+
+		}
+		} else {
+			mlx4_free_mtt_range(dev, param1 /* first */, param2 /* order */);
+			mlx4_delete_resource_for_slave(dev, RES_MTT, slave, param1);
+		}
+		break;
+	case RES_MAC:
+		vhcr->in_param |= (u64) (vep_num) << 48;
+		switch (vhcr->op) {
+		case MLX4_CMD_ALLOC_RES:
+			ret = mlx4_register_mac(dev, vhcr->op_modifier,
+						vhcr->in_param, (int *) &vhcr->out_param, 1);
+			vhcr->errno = ret;
+			if (!ret) {
+				rt_res = mlx4_add_resource_for_slave(dev, RES_MAC, slave, vhcr->out_param, RES_INIT);
+				if (rt_res) {
+					mlx4_info(dev, "mlx4_add_resource_for_slave failed, ret: %d for slave: %d", rt_res, slave);
+					mlx4_unregister_mac(dev, vhcr->op_modifier, ret);
+					return rt_res;
+				}
+				rt_res = mlx4_add_port_to_tracked_mac(dev, (int) vhcr->out_param, vhcr->op_modifier);
+				if (rt_res) {
+					mlx4_info(dev, "mlx4_add_port_to_tracked_mac failed, ret: %d for slave: %d", rt_res, slave);
+					mlx4_unregister_mac(dev, vhcr->op_modifier, ret);
+					return rt_res;
+				}
+				/*When asking for mac the master already reserved that qp, the slave needs to allocate it */
+				rt_res = mlx4_add_resource_for_slave(dev, RES_QP, slave, (int) vhcr->out_param, RES_ALLOCATED_WITH_MASTER_RESERVATION);
+				if (rt_res) {
+					mlx4_info(dev, "mlx4_add_resource_for_slave failed, ret: %d for slave: %d", rt_res, slave);
+					mlx4_unregister_mac(dev, vhcr->op_modifier, ret);
+					return rt_res;
+				}
+			}
+			break;
+		case MLX4_CMD_FREE_RES:
+			mlx4_unregister_mac(dev, vhcr->op_modifier, vhcr->in_param);
+			mlx4_delete_resource_for_slave(dev, RES_MAC, slave, vhcr->in_param);
+			/*When removing mac the master also de-allocates qp*/
+			mlx4_delete_resource_for_slave(dev, RES_QP, slave, vhcr->in_param);
+			break;
+		case MLX4_CMD_REPLACE_RES:
+			ret = mlx4_replace_mac(dev, vhcr->op_modifier,
+					       vhcr->out_param, vhcr->in_param, 1);
+			vhcr->errno = ret;
+			break;
+		default:
+			vhcr->errno = -EINVAL;
+		}
+
+		break;
+	default:
+		vhcr->errno = -EINVAL;
+	}
+	return 0;
+}
+
+static int mlx4_DMA_wrapper(struct mlx4_dev *dev, int slave,
+			    struct mlx4_vhcr *vhcr,
+			    struct mlx4_cmd_mailbox *inbox,
+			    struct mlx4_cmd_mailbox *outbox)
+{
+	u64 in_param = inbox ? inbox->dma : vhcr->in_param;
+
+	in_param |= (u64) slave;
+	return mlx4_cmd(dev, in_param, vhcr->in_modifier,
+			vhcr->op_modifier, vhcr->op, MLX4_CMD_TIME_CLASS_C);
+}
+
+static int mlx4_DMA_outbox_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox)
+{
+	u64 in_param = inbox ? inbox->dma : vhcr->in_param;
+	u64 out_param = outbox ? outbox->dma : vhcr->out_param;
+
+	in_param |= (u64) slave;
+	return mlx4_cmd_box(dev, in_param, out_param,
+			    vhcr->in_modifier, vhcr->op_modifier, vhcr->op,
+			    MLX4_CMD_TIME_CLASS_C);
+}
+
+static struct mlx4_cmd_info {
+	u16 opcode;
+	bool has_inbox;
+	bool has_outbox;
+	bool out_is_imm;
+	int (*verify)(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+					    struct mlx4_cmd_mailbox *inbox);
+	int (*wrapper)(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+					     struct mlx4_cmd_mailbox *inbox,
+					     struct mlx4_cmd_mailbox *outbox);
+} cmd_info[] = {
+	{
+		.opcode = MLX4_CMD_QUERY_FW,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_SLAVE_CAP,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_QUERY_SLAVE_CAP_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_ADAPTER,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_GET_SLAVE_SQP,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_GET_SLAVE_SQP_wrapper
+	},
+
+	{
+		.opcode = MLX4_CMD_COMM_INT,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_COMM_INT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_INIT_PORT,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_INIT_PORT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_CLOSE_PORT,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm  = false,
+		.verify = NULL,
+		.wrapper = mlx4_CLOSE_PORT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_PORT,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_QUERY_PORT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SET_PORT,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_SET_PORT_wrapper
+	},
+
+	{
+		.opcode = MLX4_CMD_MAP_EQ,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_MAP_EQ_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SW2HW_EQ,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL, /*need verifier */
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_HW_HEALTH_CHECK,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_NOP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_ALLOC_RES,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = true,
+		.verify = mlx4_verify_resource_wrapper,
+		.wrapper = mlx4_RESOURCE_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_FREE_RES,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_RESOURCE_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_GET_EVENT,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = true,
+		.verify = NULL,
+		.wrapper = mlx4_GET_EVENT_wrapper
+	},
+
+	{
+		.opcode = MLX4_CMD_REPLACE_RES,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = true,
+		.verify = NULL,
+		.wrapper = mlx4_RESOURCE_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SW2HW_MPT,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_MPT,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = mlx4_verify_mpt_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_HW2SW_MPT,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_mpt_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_READ_MTT,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_WRITE_MTT,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = mlx4_WRITE_MTT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SYNC_TPT,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = NULL
+	},
+
+	{
+		.opcode = MLX4_CMD_HW2SW_EQ,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = mlx4_DMA_outbox_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_EQ,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_SW2HW_CQ,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_cq_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_HW2SW_CQ,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_cq_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_CQ,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = mlx4_verify_cq_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_MODIFY_CQ,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = true,
+		.verify = mlx4_verify_cq_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_SW2HW_SRQ,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_srq_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_HW2SW_SRQ,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_srq_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_SRQ,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = mlx4_verify_srq_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_ARM_SRQ,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_srq_aram,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_RST2INIT_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_INIT2RTR_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_RTR2RTS_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_RTR2RTS_QP_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_RTS2RTS_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SQERR2RTS_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_2ERR_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_RTS2SQD_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SQD2SQD_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SQD2RTS_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = mlx4_DMA_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_2RST_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_QP,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_INIT2INIT_QP,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_SUSPEND_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_UNSUSPEND_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = mlx4_verify_qp_index,
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_CONF_SPECIAL_QP,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_CONF_SPECIAL_QP_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_MAD_IFC,
+		.has_inbox = true,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = NULL
+	},
+
+	/* Native multicast commands are not available for guests */
+	{
+		.opcode = MLX4_CMD_MCAST_ATTACH,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_MCAST_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_PROMISC,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_PROMISC_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_DIAG_RPRT,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL, /* need verifier */
+		.wrapper = NULL
+	},
+	{
+		.opcode = MLX4_CMD_QUERY_IF_STAT,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = MLX4_CMD_QUERY_IF_STAT_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SET_NODE,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = MLX4_CMD_SET_NODE_wrapper
+	},
+
+
+	/* Ethernet specific commands */
+	{
+		.opcode = MLX4_CMD_SET_VLAN_FLTR,
+		.has_inbox = true,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_SET_VLAN_FLTR_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_SET_MCAST_FLTR,
+		.has_inbox = false,
+		.has_outbox = false,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_SET_MCAST_FLTR_wrapper
+	},
+	{
+		.opcode = MLX4_CMD_DUMP_ETH_STATS,
+		.has_inbox = false,
+		.has_outbox = true,
+		.out_is_imm = false,
+		.verify = NULL,
+		.wrapper = mlx4_DUMP_ETH_STATS_wrapper
+	},
+};
+
+static int mlx4_master_process_vhcr(struct mlx4_dev *dev, int slave)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_cmd_info *cmd = NULL;
+	struct mlx4_vhcr *vhcr = priv->mfunc.vhcr;
+	struct mlx4_cmd_mailbox *inbox = NULL;
+	struct mlx4_cmd_mailbox *outbox = NULL;
+	u64 in_param;
+	u64 out_param;
+	int ret;
+	int i;
+
+	/* DMA in the vHCR */
+	ret = mlx4_ACCESS_MEM(dev, priv->mfunc.vhcr_dma, slave,
+			      priv->mfunc.master.slave_state[slave].vhcr_dma,
+			      ALIGN(sizeof(struct mlx4_vhcr),
+				    MLX4_ACCESS_MEM_ALIGN), 1);
+	if (ret) {
+		mlx4_err(dev, "Failed reading vhcr\n");
+		return ret;
+	}
+
+	/* Lookup command */
+	for (i = 0; i < ARRAY_SIZE(cmd_info); ++i) {
+		if (vhcr->op == cmd_info[i].opcode) {
+			cmd = &cmd_info[i];
+			break;
+		}
+	}
+	if (!cmd) {
+		mlx4_err(dev, "Unknown command:0x%x accepted from slave:%d\n",
+							      vhcr->op, slave);
+		vhcr->errno = -EINVAL;
+		goto out_status;
+	}
+
+	/* Read inbox */
+	if (cmd->has_inbox) {
+		vhcr->in_param &= INBOX_MASK;
+		inbox = mlx4_alloc_cmd_mailbox(dev);
+		if (IS_ERR(inbox)) {
+			ret = PTR_ERR(inbox);
+			inbox = NULL;
+			goto out;
+		}
+
+		/* FIXME: add mailbox size per-command */
+		ret = mlx4_ACCESS_MEM(dev, inbox->dma, slave,
+				      vhcr->in_param,
+				      MLX4_MAILBOX_SIZE, 1);
+		if (ret) {
+			mlx4_err(dev, "Failed reading inbox\n");
+			goto out;
+		}
+	}
+
+	/* Apply permission and bound checks if applicable */
+	if (cmd->verify && cmd->verify(dev, slave, vhcr, inbox)) {
+		mlx4_warn(dev, "Command:0x%x from slave: %d failed protection checks for resource_id:%d\n", vhcr->op, slave, vhcr->in_modifier);
+		vhcr->errno = -EPERM;
+		goto out_status;
+	}
+
+	/* Allocate outbox */
+	if (cmd->has_outbox) {
+		outbox = mlx4_alloc_cmd_mailbox(dev);
+		if (IS_ERR(outbox)) {
+			ret = PTR_ERR(outbox);
+			outbox = NULL;
+			goto out;
+		}
+	}
+
+	/* Execute the command! */
+	if (cmd->wrapper)
+		vhcr->errno = cmd->wrapper(dev, slave, vhcr, inbox, outbox);
+	else {
+		in_param = cmd->has_inbox ? (u64) inbox->dma : vhcr->in_param;
+		out_param = cmd->has_outbox ? (u64) outbox->dma : vhcr->out_param;
+		vhcr->errno = __mlx4_cmd(dev, in_param, &out_param,
+							cmd->out_is_imm,
+							vhcr->in_modifier,
+							vhcr->op_modifier,
+							vhcr->op,
+							vhcr->timeout);
+		if (cmd->out_is_imm)
+			vhcr->out_param = out_param;
+	}
+
+	/* Write outbox if command completed successfully */
+	if (cmd->has_outbox && !vhcr->errno) {
+		ret = mlx4_ACCESS_MEM(dev, outbox->dma, slave,
+				      vhcr->out_param,
+				      MLX4_MAILBOX_SIZE, 0);
+		if (ret) {
+			mlx4_err(dev, "Failed writing outbox\n");
+			goto out;
+		}
+	}
+
+out_status:
+	/* DMA back vhcr result */
+	ret = mlx4_ACCESS_MEM(dev, priv->mfunc.vhcr_dma, slave,
+			      priv->mfunc.master.slave_state[slave].vhcr_dma,
+			      ALIGN(sizeof(struct mlx4_vhcr),
+				    MLX4_ACCESS_MEM_ALIGN), 0);
+	if (ret)
+		mlx4_err(dev, "Failed writing vhcr result\n");
+
+	if (vhcr->errno)
+		mlx4_warn(dev, "vhcr command:0x%x slave:%d failed with error:%d\n",
+							vhcr->op, slave, vhcr->errno);
+	/* Fall through... */
+
+out:
+	if (inbox)
+		inbox->dma &= INBOX_MASK;
+	mlx4_free_cmd_mailbox(dev, inbox);
+	mlx4_free_cmd_mailbox(dev, outbox);
+	return ret;
+}
+
+static void mlx4_master_do_cmd(struct mlx4_dev *dev, int slave, u8 cmd, u16 param, u8 toggle)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_state *slave_state = priv->mfunc.master.slave_state;
+	u8 toggle_next;
+	u32 reply;
+
+	if (cmd == MLX4_COMM_CMD_RESET) {
+		mlx4_warn(dev, "Received reset from slave:%d\n", slave);
+		slave_state[slave].active = false;
+		goto reset_slave;
+	}
+
+	/* Increment next toggle token */
+	toggle_next = slave_state[slave].comm_toggle + 1;
+	if (toggle_next > 2)
+		toggle_next = 1;
+	if (toggle != toggle_next) {
+		mlx4_warn(dev, "Incorrect token:%d from slave:%d expected:%d\n",
+							toggle, toggle_next, slave);
+		goto reset_slave;
+	}
+
+	switch (cmd) {
+	case MLX4_COMM_CMD_VHCR0:
+		if (slave_state[slave].last_cmd != MLX4_COMM_CMD_RESET)
+			goto reset_slave;
+		slave_state[slave].vhcr_dma = ((u64) param) << 48;
+		break;
+	case MLX4_COMM_CMD_VHCR1:
+		if (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR0)
+			goto reset_slave;
+		slave_state[slave].vhcr_dma |= ((u64) param) << 32;
+		break;
+	case MLX4_COMM_CMD_VHCR2:
+		if (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR1)
+			goto reset_slave;
+		slave_state[slave].vhcr_dma |= ((u64) param) << 16;
+		break;
+	case MLX4_COMM_CMD_VHCR_EN:
+		if (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR2)
+			goto reset_slave;
+		slave_state[slave].vhcr_dma |= param;
+		if (mlx4_QUERY_FUNC(dev, slave, &slave_state[slave].pf_num)) {
+			mlx4_err(dev, "Failed to determine physical function "
+				      "number for slave %d\n", slave);
+			goto reset_slave;
+		}
+		slave_state[slave].vep_num = slave_state[slave_state[slave].pf_num].vep_num;
+		slave_state[slave].port_num = slave_state[slave_state[slave].pf_num].port_num;
+		slave_state[slave].active = true;
+		break;
+	case MLX4_COMM_CMD_VHCR_POST:
+		if ((slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR_EN) &&
+		    (slave_state[slave].last_cmd != MLX4_COMM_CMD_VHCR_POST))
+			goto reset_slave;
+		if (mlx4_master_process_vhcr(dev, slave)) {
+			mlx4_err(dev, "Failed processing vhcr for slave:%d, reseting slave.\n", slave);
+			goto reset_slave;
+		}
+		break;
+	default:
+		mlx4_warn(dev, "Bad comm cmd:%d from slave:%d\n", cmd, slave);
+		goto reset_slave;
+	}
+
+	slave_state[slave].last_cmd = cmd;
+	slave_state[slave].comm_toggle = toggle_next;
+	reply = (u32) toggle_next << 30;
+	__raw_writel((__force u32) cpu_to_be32(reply),
+		     &priv->mfunc.comm[slave].slave_read);
+	wmb();
+	if (mlx4_GEN_EQE(dev, slave, &priv->mfunc.master.cmd_eqe))
+		mlx4_warn(dev, "Failed to generate command completion eqe "
+			       "for slave %d\n", slave);
+
+	return;
+
+reset_slave:
+	/* cleanup any slave resources */
+	mlx4_delete_all_resources_for_slave(dev, slave);
+	slave_state[slave].last_cmd = MLX4_COMM_CMD_RESET;
+	slave_state[slave].comm_toggle = 0;
+	memset(&slave_state[slave].event_eq, 0,
+	       sizeof(struct mlx4_slave_event_eq_info));
+	__raw_writel((__force u32) 0, &priv->mfunc.comm[slave].slave_write);
+	__raw_writel((__force u32) 0, &priv->mfunc.comm[slave].slave_read);
+	wmb();
+}
+
+/* master command processing */
+void mlx4_master_comm_channel(struct work_struct *work)
+{
+	struct mlx4_mfunc_master_ctx *master = container_of(work,
+							   struct mlx4_mfunc_master_ctx,
+							   comm_work);
+	struct mlx4_mfunc *mfunc = container_of(master, struct mlx4_mfunc, master);
+	struct mlx4_priv *priv = container_of(mfunc, struct mlx4_priv, mfunc);
+	struct mlx4_dev *dev = &priv->dev;
+	u32 *bit_vec;
+	u32 comm_cmd;
+	u32 vec;
+	int i, j, slave;
+
+	bit_vec = master->comm_arm_bit_vector;
+	for (i = 0; i < COMM_CHANNEL_BIT_ARRAY_SIZE; i++) {
+		vec = be32_to_cpu(bit_vec[i]);
+		for (j = 0; j < 32; j++) {
+			if (!(vec & (1 << j)))
+				continue;
+			slave = (i * 32) + j;
+			comm_cmd = swab32(readl(&mfunc->comm[slave].slave_write));
+			if (comm_cmd >> 30 != master->slave_state[slave].comm_toggle)
+				mlx4_master_do_cmd(dev, slave, comm_cmd >> 16, comm_cmd, comm_cmd >> 30);
+		}
+	}
+
+	if (mlx4_ARM_COMM_CHANNEL(dev))
+		mlx4_warn(dev, "Failed to arm comm channel events");
+}
+
+int mlx4_multi_func_init(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_state *s_state;
+	int i, err, port;
+
+	priv->mfunc.vhcr = dma_alloc_coherent(&(dev->pdev->dev), PAGE_SIZE,
+					    &priv->mfunc.vhcr_dma,
+					    GFP_KERNEL);
+	if (!priv->mfunc.vhcr) {
+		mlx4_err(dev, "Couldn't allocate vhcr.\n");
+		return -ENOMEM;
+	}
+
+	if (mlx4_is_master(dev))
+		priv->mfunc.comm = ioremap(pci_resource_start(dev->pdev,
+							    priv->fw.comm_bar) +
+								priv->fw.comm_base,
+							    MLX4_COMM_PAGESIZE);
+	else
+		priv->mfunc.comm = ioremap(pci_resource_start(dev->pdev, 2) +
+							    MLX4_SLAVE_COMM_BASE,
+							    MLX4_COMM_PAGESIZE);
+	if (!priv->mfunc.comm) {
+		mlx4_err(dev, "Couldn't map communication vector.");
+		goto err_vhcr;
+	}
+
+	if (mlx4_is_master(dev)) {
+		priv->mfunc.master.slave_state = kzalloc(dev->num_slaves *
+					   sizeof(struct mlx4_slave_state),
+					   GFP_KERNEL);
+		if (!priv->mfunc.master.slave_state)
+			goto err_comm;
+
+		for (i = 0; i < dev->num_slaves; ++i) {
+			s_state = &priv->mfunc.master.slave_state[i];
+			s_state->last_cmd = MLX4_COMM_CMD_RESET;
+			for (port = 1; port <= MLX4_MAX_PORTS; port++) {
+				s_state->vlan_filter[port] =
+					kzalloc(sizeof(struct mlx4_vlan_fltr),
+						GFP_KERNEL);
+				if (!s_state->vlan_filter[port]) {
+					if (--port)
+						kfree(s_state->vlan_filter[port]);
+					goto err_slaves;
+				}
+				INIT_LIST_HEAD(&s_state->mcast_filters[port]);
+			}
+			spin_lock_init(&s_state->lock);
+		}
+
+		memset(&priv->mfunc.master.cmd_eqe, 0, sizeof(struct mlx4_eqe));
+		priv->mfunc.master.cmd_eqe.type = MLX4_EVENT_TYPE_CMD;
+		INIT_WORK(&priv->mfunc.master.comm_work, mlx4_master_comm_channel);
+		INIT_WORK(&priv->mfunc.master.slave_event_work, mlx4_gen_slave_eqe);
+		INIT_WORK(&priv->mfunc.master.vep_config_work, mlx4_update_vep_config);
+		spin_lock_init(&priv->mfunc.master.vep_config_lock);
+		priv->mfunc.master.comm_wq = create_singlethread_workqueue("mlx4_comm");
+		if (!priv->mfunc.master.comm_wq)
+			goto err_slaves;
+
+		if (mlx4_init_resource_tracker(dev))
+			goto err_thread;
+
+
+		err = mlx4_set_vep_maps(dev);
+		if (err) {
+			mlx4_err(dev, "Failed to set veps mapping\n");
+			goto err_resource;
+		}
+		dev->caps.vep_num = priv->mfunc.master.slave_state[dev->caps.function].vep_num;
+
+		mlx4_QUERY_VEP_CFG(dev, priv->mfunc.master.slave_state[dev->caps.function].vep_num,
+				   priv->mfunc.master.slave_state[dev->caps.function].port_num,
+				   &priv->mfunc.master.slave_state[dev->caps.function].vep_cfg);
+		dev->caps.def_mac[priv->mfunc.master.slave_state[dev->caps.function].port_num] =
+			priv->mfunc.master.slave_state[dev->caps.function].vep_cfg.mac;
+
+		err = mlx4_ARM_COMM_CHANNEL(dev);
+		if (err) {
+			mlx4_err(dev, " Failed to arm comm channel eq: %x\n", err);
+			goto err_resource;
+		}
+
+	} else {
+		sema_init(&priv->cmd.slave_sem, 1);
+		priv->cmd.comm_toggle = 0;
+	}
+	return 0;
+
+err_resource:
+	mlx4_free_resource_tracker(dev);
+err_thread:
+	flush_workqueue(priv->mfunc.master.comm_wq);
+	destroy_workqueue(priv->mfunc.master.comm_wq);
+err_slaves:
+	while (--i) {
+		for (port = 1; port <= MLX4_MAX_PORTS; port++)
+			kfree(priv->mfunc.master.slave_state[i].vlan_filter[port]);
+	}
+	kfree(priv->mfunc.master.slave_state);
+err_comm:
+	iounmap(priv->mfunc.comm);
+err_vhcr:
+	dma_free_coherent(&(dev->pdev->dev), PAGE_SIZE,
+					     priv->mfunc.vhcr,
+					     priv->mfunc.vhcr_dma);
+	priv->mfunc.vhcr = NULL;
+	return -ENOMEM;
+}
+
 int mlx4_cmd_init(struct mlx4_dev *dev)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -329,22 +1655,52 @@ int mlx4_cmd_init(struct mlx4_dev *dev)
 	priv->cmd.use_events = 0;
 	priv->cmd.toggle     = 1;
 
-	priv->cmd.hcr = ioremap(pci_resource_start(dev->pdev, 0) + MLX4_HCR_BASE,
-				MLX4_HCR_SIZE);
-	if (!priv->cmd.hcr) {
-		mlx4_err(dev, "Couldn't map command register.");
-		return -ENOMEM;
+	priv->cmd.hcr = NULL;
+	priv->mfunc.vhcr = NULL;
+
+	if (!mlx4_is_slave(dev)) {
+		priv->cmd.hcr = ioremap(pci_resource_start(dev->pdev, 0) +
+					MLX4_HCR_BASE, MLX4_HCR_SIZE);
+		if (!priv->cmd.hcr) {
+			mlx4_err(dev, "Couldn't map command register.");
+			return -ENOMEM;
+		}
 	}
 
 	priv->cmd.pool = pci_pool_create("mlx4_cmd", dev->pdev,
 					 MLX4_MAILBOX_SIZE,
 					 MLX4_MAILBOX_SIZE, 0);
-	if (!priv->cmd.pool) {
+	if (!priv->cmd.pool)
+		goto err_hcr;
+
+	return 0;
+
+err_hcr:
+	if (!mlx4_is_slave(dev))
 		iounmap(priv->cmd.hcr);
-		return -ENOMEM;
+	return -ENOMEM;
+}
+
+void mlx4_multi_func_cleanup(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int i, port;
+
+	if (mlx4_is_master(dev)) {
+		flush_workqueue(priv->mfunc.master.comm_wq);
+		destroy_workqueue(priv->mfunc.master.comm_wq);
+		for (i = 0; i < dev->num_slaves; i++) {
+			for (port = 1; port <= MLX4_MAX_PORTS; port++)
+				kfree(priv->mfunc.master.slave_state[i].vlan_filter[port]);
+		}
+		kfree(priv->mfunc.master.slave_state);
 	}
 
-	return 0;
+	iounmap(priv->mfunc.comm);
+	dma_free_coherent(&(dev->pdev->dev), PAGE_SIZE,
+			  priv->mfunc.vhcr,
+			  priv->mfunc.vhcr_dma);
+	priv->mfunc.vhcr = NULL;
 }
 
 void mlx4_cmd_cleanup(struct mlx4_dev *dev)
@@ -352,7 +1708,9 @@ void mlx4_cmd_cleanup(struct mlx4_dev *d
 	struct mlx4_priv *priv = mlx4_priv(dev);
 
 	pci_pool_destroy(priv->cmd.pool);
-	iounmap(priv->cmd.hcr);
+
+	if (!mlx4_is_slave(dev))
+		iounmap(priv->cmd.hcr);
 }
 
 /*
@@ -363,6 +1721,7 @@ int mlx4_cmd_use_events(struct mlx4_dev 
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	int i;
+	int err = 0;
 
 	priv->cmd.context = kmalloc(priv->cmd.max_cmds *
 				   sizeof (struct mlx4_cmd_context),
@@ -389,9 +1748,17 @@ int mlx4_cmd_use_events(struct mlx4_dev 
 
 	priv->cmd.use_events = 1;
 
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd(dev, 0, 1, 0, MLX4_CMD_COMM_INT, MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			mlx4_err(dev, "Failed to move to events for the slave\n");
+			priv->cmd.use_events = 0;
+		}
+	}
+
 	down(&priv->cmd.poll_sem);
 
-	return 0;
+	return err;
 }
 
 /*
@@ -410,6 +1777,9 @@ void mlx4_cmd_use_polling(struct mlx4_de
 	kfree(priv->cmd.context);
 
 	up(&priv->cmd.poll_sem);
+
+	if (mlx4_is_slave(dev))
+		mlx4_cmd(dev, 0, 0, 0, MLX4_CMD_COMM_INT, MLX4_CMD_TIME_CLASS_A);
 }
 
 struct mlx4_cmd_mailbox *mlx4_alloc_cmd_mailbox(struct mlx4_dev *dev)
@@ -440,3 +1810,35 @@ void mlx4_free_cmd_mailbox(struct mlx4_d
 	kfree(mailbox);
 }
 EXPORT_SYMBOL_GPL(mlx4_free_cmd_mailbox);
+
+int mlx4_COMM_INT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+			  struct mlx4_cmd_mailbox *inbox,
+			  struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_event_eq_info *event_eq =
+		&priv->mfunc.master.slave_state[slave].event_eq;
+
+	if (vhcr->in_modifier)
+		event_eq->use_int = true;
+	else
+		event_eq->use_int = false;
+
+	return 0;
+}
+
+int MLX4_CMD_QUERY_IF_STAT_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox) {
+	return mlx4_cmd_box(dev, 0, outbox->dma, vhcr->in_modifier, 0,
+			   MLX4_CMD_QUERY_IF_STAT, MLX4_CMD_TIME_CLASS_C);
+}
+
+int MLX4_CMD_SET_NODE_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox) {
+	return 0;
+}
+
diff -r c23d1fc7e422 drivers/net/mlx4/cq.c
--- a/drivers/net/mlx4/cq.c
+++ b/drivers/net/mlx4/cq.c
@@ -34,6 +34,7 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/hardirq.h>
 
 #include <linux/mlx4/cmd.h>
@@ -79,7 +80,7 @@ void mlx4_cq_completion(struct mlx4_dev 
 	cq = radix_tree_lookup(&mlx4_priv(dev)->cq_table.tree,
 			       cqn & (dev->caps.num_cqs - 1));
 	if (!cq) {
-		mlx4_warn(dev, "Completion event for bogus CQ %08x\n", cqn);
+		mlx4_dbg(dev, "Completion event for bogus CQ %08x\n", cqn);
 		return;
 	}
 
@@ -115,8 +116,8 @@ void mlx4_cq_event(struct mlx4_dev *dev,
 static int mlx4_SW2HW_CQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			 int cq_num)
 {
-	return mlx4_cmd(dev, mailbox->dma, cq_num, 0, MLX4_CMD_SW2HW_CQ,
-			MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd(dev, mailbox->dma | dev->caps.function, cq_num, 0,
+			MLX4_CMD_SW2HW_CQ, MLX4_CMD_TIME_CLASS_A);
 }
 
 static int mlx4_MODIFY_CQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
@@ -129,8 +130,8 @@ static int mlx4_MODIFY_CQ(struct mlx4_de
 static int mlx4_HW2SW_CQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			 int cq_num)
 {
-	return mlx4_cmd_box(dev, 0, mailbox ? mailbox->dma : 0, cq_num,
-			    mailbox ? 0 : 1, MLX4_CMD_HW2SW_CQ,
+	return mlx4_cmd_box(dev, dev->caps.function, mailbox ? mailbox->dma : 0,
+			    cq_num, mailbox ? 0 : 1, MLX4_CMD_HW2SW_CQ,
 			    MLX4_CMD_TIME_CLASS_A);
 }
 
@@ -186,6 +187,108 @@ int mlx4_cq_resize(struct mlx4_dev *dev,
 }
 EXPORT_SYMBOL_GPL(mlx4_cq_resize);
 
+int mlx4_cq_ignore_overrun(struct mlx4_dev *dev, struct mlx4_cq *cq)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_cq_context *cq_context;
+	int err;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	cq_context = mailbox->buf;
+	memset(cq_context, 0, sizeof *cq_context);
+
+	cq_context->flags |= cpu_to_be32(MLX4_CQ_FLAG_OI); 
+
+	err = mlx4_MODIFY_CQ(dev, mailbox, cq->cqn, 3);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_cq_ignore_overrun);
+
+static int mlx4_find_least_loaded_vector(struct mlx4_priv *priv)
+{
+	int i;
+	int index = 0;
+	int min = priv->eq_table.eq[0].load;
+
+	for (i = 1; i < priv->dev.caps.num_comp_vectors; i++) {
+		if (priv->eq_table.eq[i].load < min) {
+			index = i;
+			min = priv->eq_table.eq[i].load;
+		}
+	}
+
+	return index;
+}
+
+int mlx4_cq_alloc_icm(struct mlx4_dev *dev, int *cqn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_cq_table *cq_table = &priv->cq_table;
+	u64 out_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd_imm(dev, 0, &out_param, RES_CQ,
+						       ICM_RESERVE_AND_ALLOC,
+						       MLX4_CMD_ALLOC_RES,
+						       MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			*cqn = -1;
+			return err;
+		} else {
+			*cqn = out_param;
+			return 0;
+		}
+	}
+
+	*cqn = mlx4_bitmap_alloc(&cq_table->bitmap);
+	if (*cqn == -1)
+		return -ENOMEM;
+
+	err = mlx4_table_get(dev, &cq_table->table, *cqn);
+	if (err)
+		goto err_out;
+
+	err = mlx4_table_get(dev, &cq_table->cmpt_table, *cqn);
+	if (err)
+		goto err_put;
+	return 0;
+
+err_put:
+	mlx4_table_put(dev, &cq_table->table, *cqn);
+
+err_out:
+	mlx4_bitmap_free(&cq_table->bitmap, *cqn);
+	return err;
+}
+
+void mlx4_cq_free_icm(struct mlx4_dev *dev, int cqn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_cq_table *cq_table = &priv->cq_table;
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = cqn;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd(dev, in_param, RES_CQ, ICM_RESERVE_AND_ALLOC,
+						      MLX4_CMD_FREE_RES,
+						      MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed freeing cq:%d\n", cqn);
+	} else {
+		mlx4_table_put(dev, &cq_table->cmpt_table, cqn);
+		mlx4_table_put(dev, &cq_table->table, cqn);
+		mlx4_bitmap_free(&cq_table->bitmap, cqn);
+	}
+}
+
 int mlx4_cq_alloc(struct mlx4_dev *dev, int nent, struct mlx4_mtt *mtt,
 		  struct mlx4_uar *uar, u64 db_rec, struct mlx4_cq *cq,
 		  unsigned vector, int collapsed)
@@ -197,28 +300,21 @@ int mlx4_cq_alloc(struct mlx4_dev *dev, 
 	u64 mtt_addr;
 	int err;
 
-	if (vector >= dev->caps.num_comp_vectors)
+	cq->vector = (vector == MLX4_LEAST_ATTACHED_VECTOR) ?
+		mlx4_find_least_loaded_vector(priv) : vector;
+
+	if (cq->vector > dev->caps.num_comp_vectors + dev->caps.poolsz)
 		return -EINVAL;
 
-	cq->vector = vector;
-
-	cq->cqn = mlx4_bitmap_alloc(&cq_table->bitmap);
-	if (cq->cqn == -1)
-		return -ENOMEM;
-
-	err = mlx4_table_get(dev, &cq_table->table, cq->cqn);
+	err = mlx4_cq_alloc_icm(dev, &cq->cqn);
 	if (err)
-		goto err_out;
-
-	err = mlx4_table_get(dev, &cq_table->cmpt_table, cq->cqn);
-	if (err)
-		goto err_put;
+		return err;
 
 	spin_lock_irq(&cq_table->lock);
 	err = radix_tree_insert(&cq_table->tree, cq->cqn, cq);
 	spin_unlock_irq(&cq_table->lock);
 	if (err)
-		goto err_cmpt_put;
+		goto err_icm;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox)) {
@@ -231,7 +327,7 @@ int mlx4_cq_alloc(struct mlx4_dev *dev, 
 
 	cq_context->flags	    = cpu_to_be32(!!collapsed << 18);
 	cq_context->logsize_usrpage = cpu_to_be32((ilog2(nent) << 24) | uar->index);
-	cq_context->comp_eqn	    = priv->eq_table.eq[vector].eqn;
+	cq_context->comp_eqn	    = priv->eq_table.eq[cq->vector].eqn;
 	cq_context->log_page_size   = mtt->page_shift - MLX4_ICM_PAGE_SHIFT;
 
 	mtt_addr = mlx4_mtt_addr(dev, mtt);
@@ -244,6 +340,7 @@ int mlx4_cq_alloc(struct mlx4_dev *dev, 
 	if (err)
 		goto err_radix;
 
+	priv->eq_table.eq[cq->vector].load++;
 	cq->cons_index = 0;
 	cq->arm_sn     = 1;
 	cq->uar        = uar;
@@ -257,14 +354,8 @@ err_radix:
 	radix_tree_delete(&cq_table->tree, cq->cqn);
 	spin_unlock_irq(&cq_table->lock);
 
-err_cmpt_put:
-	mlx4_table_put(dev, &cq_table->cmpt_table, cq->cqn);
-
-err_put:
-	mlx4_table_put(dev, &cq_table->table, cq->cqn);
-
-err_out:
-	mlx4_bitmap_free(&cq_table->bitmap, cq->cqn);
+err_icm:
+	mlx4_cq_free_icm(dev, cq->cqn);
 
 	return err;
 }
@@ -281,6 +372,7 @@ void mlx4_cq_free(struct mlx4_dev *dev, 
 		mlx4_warn(dev, "HW2SW_CQ failed (%d) for CQN %06x\n", err, cq->cqn);
 
 	synchronize_irq(priv->eq_table.eq[cq->vector].irq);
+	priv->eq_table.eq[cq->vector].load--;
 
 	spin_lock_irq(&cq_table->lock);
 	radix_tree_delete(&cq_table->tree, cq->cqn);
@@ -290,8 +382,7 @@ void mlx4_cq_free(struct mlx4_dev *dev, 
 		complete(&cq->free);
 	wait_for_completion(&cq->free);
 
-	mlx4_table_put(dev, &cq_table->table, cq->cqn);
-	mlx4_bitmap_free(&cq_table->bitmap, cq->cqn);
+	mlx4_cq_free_icm(dev, cq->cqn);
 }
 EXPORT_SYMBOL_GPL(mlx4_cq_free);
 
@@ -302,6 +393,8 @@ int mlx4_init_cq_table(struct mlx4_dev *
 
 	spin_lock_init(&cq_table->lock);
 	INIT_RADIX_TREE(&cq_table->tree, GFP_ATOMIC);
+	if (mlx4_is_slave(dev))
+		return 0;
 
 	err = mlx4_bitmap_init(&cq_table->bitmap, dev->caps.num_cqs,
 			       dev->caps.num_cqs - 1, dev->caps.reserved_cqs, 0);
@@ -313,6 +406,8 @@ int mlx4_init_cq_table(struct mlx4_dev *
 
 void mlx4_cleanup_cq_table(struct mlx4_dev *dev)
 {
+	if (mlx4_is_slave(dev))
+		return;
 	/* Nothing to do to clean up radix_tree */
 	mlx4_bitmap_cleanup(&mlx4_priv(dev)->cq_table.bitmap);
 }
diff -r c23d1fc7e422 drivers/net/mlx4/en_cq.c
--- a/drivers/net/mlx4/en_cq.c
+++ b/drivers/net/mlx4/en_cq.c
@@ -51,24 +51,17 @@ int mlx4_en_create_cq(struct mlx4_en_pri
 	int err;
 
 	cq->size = entries;
-	if (mode == RX) {
-		cq->buf_size = cq->size * sizeof(struct mlx4_cqe);
-		cq->vector   = ring % mdev->dev->caps.num_comp_vectors;
-	} else {
-		cq->buf_size = sizeof(struct mlx4_cqe);
-		cq->vector   = 0;
-	}
+	cq->buf_size = cq->size * mdev->dev->caps.cqe_size;
 
 	cq->ring = ring;
 	cq->is_tx = mode;
-	spin_lock_init(&cq->lock);
 
 	err = mlx4_alloc_hwq_res(mdev->dev, &cq->wqres,
-				cq->buf_size, 2 * PAGE_SIZE);
+				cq->buf_size, 2 * PAGE_SIZE, cq->numa_node);
 	if (err)
 		return err;
 
-	err = mlx4_en_map_buffer(&cq->wqres.buf);
+	err = mlx4_en_map_buffer(&cq->wqres.buf, cq->numa_node);
 	if (err)
 		mlx4_free_hwq_res(mdev->dev, &cq->wqres, cq->buf_size);
 	else
@@ -77,10 +70,11 @@ int mlx4_en_create_cq(struct mlx4_en_pri
 	return err;
 }
 
-int mlx4_en_activate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq)
+int mlx4_en_activate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq, int cq_idx)
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
-	int err;
+	int err = 0;
+	char name[25];
 
 	cq->dev = mdev->pndev[priv->port];
 	cq->mcq.set_ci_db  = cq->wqres.db.db;
@@ -89,11 +83,39 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 	*cq->mcq.arm_db    = 0;
 	memset(cq->buf, 0, cq->buf_size);
 
+	if (cq->is_tx == RX) {
+		if (mdev->dev->caps.poolsz) {
+			if (!cq->vector) {
+				sprintf(name , "%s-%d", priv->dev->name, cq->ring);
+				/* Set IRQ for specific name (per ring) */
+				if (mlx4_assign_eq(mdev->dev, name, &cq->vector)) {
+					cq->vector = (cq->ring + 1 + priv->port) %
+						mdev->dev->caps.num_comp_vectors;
+					mlx4_warn(mdev, "Failed Assigning an EQ to "
+						  "%s ,Falling back to legacy"
+						  " EQ's \n", name);
+				 }
+			}
+		} else
+			cq->vector = (cq->ring + 1 + priv->port) %
+					mdev->dev->caps.num_comp_vectors;
+	} else {
+		/* For TX we use the same irq per
+		ring we assigned for the RX    */
+		struct mlx4_en_cq *rx_cq;
+
+		cq_idx = cq_idx % priv->rx_ring_num;
+		rx_cq = priv->rx_cq[cq_idx];
+		cq->vector = rx_cq->vector;
+	}
+
 	if (!cq->is_tx)
-		cq->size = priv->rx_ring[cq->ring].actual_size;
+		cq->size = priv->rx_ring[cq->ring]->actual_size;
 
-	err = mlx4_cq_alloc(mdev->dev, cq->size, &cq->wqres.mtt, &mdev->priv_uar,
-			    cq->wqres.db.dma, &cq->mcq, cq->vector, cq->is_tx);
+	err = mlx4_cq_alloc(mdev->dev, cq->size, &cq->wqres.mtt,
+			&mdev->priv_uar, cq->wqres.db.dma,
+			&cq->mcq, cq->vector, 0);
+
 	if (err)
 		return err;
 
@@ -101,11 +123,14 @@ int mlx4_en_activate_cq(struct mlx4_en_p
 	cq->mcq.event = mlx4_en_cq_event;
 
 	if (cq->is_tx) {
-		init_timer(&cq->timer);
-		cq->timer.function = mlx4_en_poll_tx_cq;
-		cq->timer.data = (unsigned long) cq;
+		if (mdev->profile.use_tx_polling) {
+			init_timer(&cq->timer);
+			cq->timer.function = mlx4_en_poll_tx_cq;
+			cq->timer.data = (unsigned long) cq;
+		} else
+			mlx4_en_arm_cq(priv, cq);
 	} else {
-		netif_napi_add(cq->dev, &cq->napi, mlx4_en_poll_rx_cq, 64);
+		netif_napi_add(cq->dev, &cq->napi, mlx4_en_poll_rx_cq, MLX4_EN_RX_BUDGET);
 		napi_enable(&cq->napi);
 	}
 
@@ -118,6 +143,9 @@ void mlx4_en_destroy_cq(struct mlx4_en_p
 
 	mlx4_en_unmap_buffer(&cq->wqres.buf);
 	mlx4_free_hwq_res(mdev->dev, &cq->wqres, cq->buf_size);
+
+	if (priv->mdev->dev->caps.poolsz && cq->vector)
+		mlx4_release_eq(priv->mdev->dev , cq->vector);
 	cq->buf_size = 0;
 	cq->buf = NULL;
 }
@@ -126,12 +154,14 @@ void mlx4_en_deactivate_cq(struct mlx4_e
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
 
-	if (cq->is_tx)
-		del_timer(&cq->timer);
-	else {
+	if (cq->is_tx) {
+		if (mdev->profile.use_tx_polling)
+			del_timer(&cq->timer);
+	} else {
 		napi_disable(&cq->napi);
 		netif_napi_del(&cq->napi);
 	}
+	
 
 	mlx4_cq_free(mdev->dev, &cq->mcq);
 }
diff -r c23d1fc7e422 drivers/net/mlx4/en_ethtool.c
--- a/drivers/net/mlx4/en_ethtool.c
+++ b/drivers/net/mlx4/en_ethtool.c
@@ -34,38 +34,41 @@
 #include <linux/kernel.h>
 #include <linux/ethtool.h>
 #include <linux/netdevice.h>
+#include <linux/if_vlan.h>
 
 #include "mlx4_en.h"
 #include "en_port.h"
 
 
-static void mlx4_en_update_lro_stats(struct mlx4_en_priv *priv)
-{
-	int i;
-
-	priv->port_stats.lro_aggregated = 0;
-	priv->port_stats.lro_flushed = 0;
-	priv->port_stats.lro_no_desc = 0;
-
-	for (i = 0; i < priv->rx_ring_num; i++) {
-		priv->port_stats.lro_aggregated += priv->rx_ring[i].lro.stats.aggregated;
-		priv->port_stats.lro_flushed += priv->rx_ring[i].lro.stats.flushed;
-		priv->port_stats.lro_no_desc += priv->rx_ring[i].lro.stats.no_desc;
-	}
-}
-
 static void
 mlx4_en_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *drvinfo)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
 
-	sprintf(drvinfo->driver, DRV_NAME " (%s)", mdev->dev->board_id);
+	switch (mdev->dev->rev_id) {
+	case 0x0:	/* Kfir 0xa0 */
+	case 0x1:	/* Kfir 0xa1 */
+		sprintf(drvinfo->driver, DRV_NAME " (%s_CX-3)", mdev->dev->board_id);
+		break;
+	case 0xa0:
+		sprintf(drvinfo->driver, DRV_NAME " (%s_CX)", mdev->dev->board_id);
+		break;
+	case 0xb0:
+		sprintf(drvinfo->driver, DRV_NAME " (%s_CX-2)", mdev->dev->board_id);
+		break;
+	default:
+		sprintf(drvinfo->driver, DRV_NAME " (%s)", mdev->dev->board_id);
+		break;
+	}
 	strncpy(drvinfo->version, DRV_VERSION " (" DRV_RELDATE ")", 32);
 	sprintf(drvinfo->fw_version, "%d.%d.%d",
 		(u16) (mdev->dev->caps.fw_ver >> 32),
 		(u16) ((mdev->dev->caps.fw_ver >> 16) & 0xffff),
 		(u16) (mdev->dev->caps.fw_ver & 0xffff));
+	if (mdev->dev->caps.clp_ver)
+		sprintf(drvinfo->fw_version, "%s-%d", drvinfo->fw_version,
+			mdev->dev->caps.clp_ver);
 	strncpy(drvinfo->bus_info, pci_name(mdev->dev->pdev), 32);
 	drvinfo->n_stats = 0;
 	drvinfo->regdump_len = 0;
@@ -85,8 +88,39 @@ static int mlx4_en_set_tso(struct net_de
 		if (!priv->mdev->LSO_support)
 			return -EPERM;
 		dev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
-	} else
+#ifdef HAVE_NETDEV_VLAN_FEATURES
+		dev->vlan_features |= (NETIF_F_TSO | NETIF_F_TSO6);
+#else
+		if (priv->vlgrp) {
+			int i;
+			struct net_device *vdev;
+			for (i = 0; i < VLAN_GROUP_ARRAY_LEN; i++) {
+				vdev = vlan_group_get_device(priv->vlgrp, i);
+				if (vdev) {
+					vdev->features |= (NETIF_F_TSO | NETIF_F_TSO6);
+					vlan_group_set_device(priv->vlgrp, i, vdev);
+				}
+			}
+		}
+#endif
+	} else {
 		dev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#ifdef HAVE_NETDEV_VLAN_FEATURES
+		dev->vlan_features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+#else
+		if (priv->vlgrp) {
+			int i;
+			struct net_device *vdev;
+			for (i = 0; i < VLAN_GROUP_ARRAY_LEN; i++) {
+				vdev = vlan_group_get_device(priv->vlgrp, i);
+				if (vdev) {
+					vdev->features &= ~(NETIF_F_TSO | NETIF_F_TSO6);
+					vlan_group_set_device(priv->vlgrp, i, vdev);
+				}
+			}
+		}
+#endif
+	}
 	return 0;
 }
 
@@ -112,7 +146,7 @@ static const char main_strings[][ETH_GST
 	"tx_heartbeat_errors", "tx_window_errors",
 
 	/* port statistics */
-	"lro_aggregated", "lro_flushed", "lro_no_desc", "tso_packets",
+	"tso_packets",
 	"queue_stopped", "wake_queue", "tx_timeout", "rx_alloc_failed",
 	"rx_csum_good", "rx_csum_none", "tx_chksum_offload",
 
@@ -125,6 +159,14 @@ static const char main_strings[][ETH_GST
 #define NUM_MAIN_STATS	21
 #define NUM_ALL_STATS	(NUM_MAIN_STATS + NUM_PORT_STATS + NUM_PKT_STATS + NUM_PERF_STATS)
 
+static const char mlx4_en_test_names[][ETH_GSTRING_LEN]= {
+	"Interupt Test",
+	"Link Test",
+	"Speed Test",
+	"Register Test",
+	"Loopback Test",
+};
+
 static u32 mlx4_en_get_msglevel(struct net_device *dev)
 {
 	return ((struct mlx4_en_priv *) netdev_priv(dev))->msg_enable;
@@ -138,20 +180,111 @@ static void mlx4_en_set_msglevel(struct 
 static void mlx4_en_get_wol(struct net_device *netdev,
 			    struct ethtool_wolinfo *wol)
 {
-	wol->supported = 0;
-	wol->wolopts = 0;
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	int err = 0;
+	u64 config = 0;
+	u8 mask;
 
-	return;
+	switch (priv->port) {
+	case 1:
+		mask = MLX4_DEV_CAP_FLAG_WOL_PORT1;
+		break;
+	case 2:
+		mask = MLX4_DEV_CAP_FLAG_WOL_PORT2;
+		break;
+	default:
+		en_err(priv, "Failed to get WoL information for port = %d\n",
+		       priv->port);
+		return;
+	}
+
+	if (!(priv->mdev->dev->caps.wol & mask)) {
+		wol->supported = 0;
+		wol->wolopts = 0;
+		return;
+	}
+
+	err = mlx4_wol_read(priv->mdev->dev, &config, priv->port);
+	if (err) {
+		en_err(priv, "Failed to get WoL information\n");
+		return;
+	}
+
+	if (config & MLX4_EN_WOL_MAGIC)
+		wol->supported = WAKE_MAGIC;
+	else
+		wol->supported = 0;
+
+	if (config & MLX4_EN_WOL_ENABLED)
+		wol->wolopts = WAKE_MAGIC;
+	else
+		wol->wolopts = 0;
 }
 
+static int mlx4_en_set_wol(struct net_device *netdev,
+			    struct ethtool_wolinfo *wol)
+{
+	struct mlx4_en_priv *priv = netdev_priv(netdev);
+	u64 config = 0;
+	int err = 0;
+	u8 mask;
+
+	switch (priv->port) {
+	case 1:
+		mask = MLX4_DEV_CAP_FLAG_WOL_PORT1;
+		break;
+	case 2:
+		mask = MLX4_DEV_CAP_FLAG_WOL_PORT2;
+		break;
+	default:
+		en_err(priv, "Failed to get WoL information for port = %d\n",
+		       priv->port);
+		return -EOPNOTSUPP;
+	}
+
+	if (!(priv->mdev->dev->caps.wol & mask)) {
+		wol->supported = 0;
+		wol->wolopts = 0;
+		return -EOPNOTSUPP;
+	}
+
+	if (wol->supported & ~WAKE_MAGIC)
+		return -EINVAL;
+
+	err = mlx4_wol_read(priv->mdev->dev, &config, priv->port);
+	if (err)
+		en_err(priv, "Failed to get WoL information\n");
+
+	if (wol->wolopts & WAKE_MAGIC) {
+		config |= MLX4_EN_WOL_DO_MODIFY | MLX4_EN_WOL_ENABLED |
+				MLX4_EN_WOL_MAGIC;
+	} else {
+		config &= 0x1fffffffffffffffULL;
+		config |= MLX4_EN_WOL_DO_MODIFY & ~MLX4_EN_WOL_MAGIC;
+	}
+	err = mlx4_wol_write(priv->mdev->dev, config, priv->port);
+
+	if (err) {
+		en_err(priv, "Failed to set WoL information\n");
+		return err;
+	}
+
+	return 0;
+ }
+
 static int mlx4_en_get_sset_count(struct net_device *dev, int sset)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 
-	if (sset != ETH_SS_STATS)
+	switch (sset) {
+	case ETH_SS_STATS:
+		return NUM_ALL_STATS +
+			(priv->tx_ring_num + priv->rx_ring_num) * 2;
+	case ETH_SS_TEST:
+		return MLX4_EN_NUM_SELF_TEST - !(priv->mdev->dev->caps.loopback_support) * 2;
+	default:
 		return -EOPNOTSUPP;
-
-	return NUM_ALL_STATS + (priv->tx_ring_num + priv->rx_ring_num) * 2;
+	}
 }
 
 static void mlx4_en_get_ethtool_stats(struct net_device *dev,
@@ -163,19 +296,17 @@ static void mlx4_en_get_ethtool_stats(st
 
 	spin_lock_bh(&priv->stats_lock);
 
-	mlx4_en_update_lro_stats(priv);
-
 	for (i = 0; i < NUM_MAIN_STATS; i++)
 		data[index++] = ((unsigned long *) &priv->stats)[i];
 	for (i = 0; i < NUM_PORT_STATS; i++)
 		data[index++] = ((unsigned long *) &priv->port_stats)[i];
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		data[index++] = priv->tx_ring[i].packets;
-		data[index++] = priv->tx_ring[i].bytes;
+		data[index++] = priv->tx_ring[i]->packets;
+		data[index++] = priv->tx_ring[i]->bytes;
 	}
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		data[index++] = priv->rx_ring[i].packets;
-		data[index++] = priv->rx_ring[i].bytes;
+		data[index++] = priv->rx_ring[i]->packets;
+		data[index++] = priv->rx_ring[i]->bytes;
 	}
 	for (i = 0; i < NUM_PKT_STATS; i++)
 		data[index++] = ((unsigned long *) &priv->pkstats)[i];
@@ -183,6 +314,12 @@ static void mlx4_en_get_ethtool_stats(st
 
 }
 
+static void mlx4_en_self_test(struct net_device *dev,
+			      struct ethtool_test *etest, u64 *buf)
+{
+	mlx4_en_ex_selftest(dev, &etest->flags, buf);
+}
+
 static void mlx4_en_get_strings(struct net_device *dev,
 				uint32_t stringset, uint8_t *data)
 {
@@ -190,44 +327,88 @@ static void mlx4_en_get_strings(struct n
 	int index = 0;
 	int i;
 
-	if (stringset != ETH_SS_STATS)
-		return;
+	switch (stringset) {
+	case ETH_SS_TEST:
+		for (i = 0; i < MLX4_EN_NUM_SELF_TEST - 2; i++)
+			strcpy(data + i * ETH_GSTRING_LEN, mlx4_en_test_names[i]);
+		if (priv->mdev->dev->caps.loopback_support)
+			for (; i < MLX4_EN_NUM_SELF_TEST; i++)
+				strcpy(data + i * ETH_GSTRING_LEN, mlx4_en_test_names[i]);
+		break;
 
-	/* Add main counters */
-	for (i = 0; i < NUM_MAIN_STATS; i++)
-		strcpy(data + (index++) * ETH_GSTRING_LEN, main_strings[i]);
-	for (i = 0; i < NUM_PORT_STATS; i++)
-		strcpy(data + (index++) * ETH_GSTRING_LEN,
+	case ETH_SS_STATS:
+		/* Add main counters */
+		for (i = 0; i < NUM_MAIN_STATS; i++)
+			strcpy(data + (index++) * ETH_GSTRING_LEN, main_strings[i]);
+		for (i = 0; i< NUM_PORT_STATS; i++)
+			strcpy(data + (index++) * ETH_GSTRING_LEN,
 			main_strings[i + NUM_MAIN_STATS]);
-	for (i = 0; i < priv->tx_ring_num; i++) {
-		sprintf(data + (index++) * ETH_GSTRING_LEN,
-			"tx%d_packets", i);
-		sprintf(data + (index++) * ETH_GSTRING_LEN,
-			"tx%d_bytes", i);
+		for (i = 0; i < priv->tx_ring_num; i++) {
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"tx%d_packets", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"tx%d_bytes", i);
+		}
+		for (i = 0; i < priv->rx_ring_num; i++) {
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_packets", i);
+			sprintf(data + (index++) * ETH_GSTRING_LEN,
+				"rx%d_bytes", i);
+		}
+		for (i = 0; i< NUM_PKT_STATS; i++)
+			strcpy(data + (index++) * ETH_GSTRING_LEN,
+			main_strings[i + NUM_MAIN_STATS + NUM_PORT_STATS]);
+		break;
 	}
-	for (i = 0; i < priv->rx_ring_num; i++) {
-		sprintf(data + (index++) * ETH_GSTRING_LEN,
-			"rx%d_packets", i);
-		sprintf(data + (index++) * ETH_GSTRING_LEN,
-			"rx%d_bytes", i);
-	}
-	for (i = 0; i < NUM_PKT_STATS; i++)
-		strcpy(data + (index++) * ETH_GSTRING_LEN,
-			main_strings[i + NUM_MAIN_STATS + NUM_PORT_STATS]);
 }
 
 static int mlx4_en_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
-	cmd->autoneg = AUTONEG_DISABLE;
-	cmd->supported = SUPPORTED_10000baseT_Full;
-	cmd->advertising = ADVERTISED_1000baseT_Full;
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	int trans_type;
+
+	switch (priv->mdev->pdev->device) {
+	case 0x6764:
+	case 0x6765:
+	case 0x6746:
+	case 0x6747:
+		cmd->autoneg = AUTONEG_ENABLE;
+		cmd->supported = SUPPORTED_10000baseT_Full |
+				 SUPPORTED_1000baseT_Full |
+				 SUPPORTED_Autoneg;
+		cmd->advertising = ADVERTISED_10000baseT_Full |
+				   ADVERTISED_1000baseT_Full |
+				   ADVERTISED_Autoneg;
+		break;
+	default:
+		cmd->autoneg = AUTONEG_DISABLE;
+		cmd->supported = SUPPORTED_10000baseT_Full;
+		cmd->advertising = ADVERTISED_10000baseT_Full;
+	}
+
+	trans_type = priv->port_state.transciver;
 	if (netif_carrier_ok(dev)) {
-		cmd->speed = SPEED_10000;
+		cmd->speed = priv->port_state.link_speed;
 		cmd->duplex = DUPLEX_FULL;
 	} else {
 		cmd->speed = -1;
 		cmd->duplex = -1;
 	}
+
+	if (trans_type > 0 && trans_type <= 0xC) {
+		cmd->port = PORT_FIBRE;
+		cmd->transceiver = XCVR_EXTERNAL;
+		cmd->supported |= SUPPORTED_FIBRE;
+		cmd->advertising |= ADVERTISED_FIBRE;
+	} else if (trans_type == 0x80 || trans_type == 0) {
+		cmd->port = PORT_TP;
+		cmd->transceiver = XCVR_INTERNAL;
+		cmd->supported |= SUPPORTED_TP;
+		cmd->advertising |= ADVERTISED_TP;
+	} else  {
+		cmd->port = -1;
+		cmd->transceiver = -1;
+	}
 	return 0;
 }
 
@@ -268,7 +449,8 @@ static int mlx4_en_set_coalesce(struct n
 
 	priv->rx_frames = (coal->rx_max_coalesced_frames ==
 			   MLX4_EN_AUTO_CONF) ?
-				MLX4_EN_RX_COAL_TARGET :
+				MLX4_EN_RX_COAL_TARGET /
+				priv->dev->mtu + 1 :
 				coal->rx_max_coalesced_frames;
 	priv->rx_usecs = (coal->rx_coalesce_usecs ==
 			  MLX4_EN_AUTO_CONF) ?
@@ -282,14 +464,14 @@ static int mlx4_en_set_coalesce(struct n
 	priv->rx_usecs_high = coal->rx_coalesce_usecs_high;
 	priv->sample_interval = coal->rate_sample_interval;
 	priv->adaptive_rx_coal = coal->use_adaptive_rx_coalesce;
-	priv->last_moder_time = MLX4_EN_AUTO_CONF;
 	if (priv->adaptive_rx_coal)
 		return 0;
 
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		priv->rx_cq[i].moder_cnt = priv->rx_frames;
-		priv->rx_cq[i].moder_time = priv->rx_usecs;
-		err = mlx4_en_set_cq_moder(priv, &priv->rx_cq[i]);
+		priv->rx_cq[i]->moder_cnt = priv->rx_frames;
+		priv->rx_cq[i]->moder_time = priv->rx_usecs;
+		priv->last_moder_time[i] = MLX4_EN_AUTO_CONF;
+		err = mlx4_en_set_cq_moder(priv, priv->rx_cq[i]);
 		if (err)
 			return err;
 	}
@@ -305,12 +487,15 @@ static int mlx4_en_set_pauseparam(struct
 
 	priv->prof->tx_pause = pause->tx_pause != 0;
 	priv->prof->rx_pause = pause->rx_pause != 0;
-	err = mlx4_SET_PORT_general(mdev->dev, priv->port,
+	err = mlx4_SET_PORT_general(mdev->mlx4_intf, mdev->dev, priv->port,
 				    priv->rx_skb_size + ETH_FCS_LEN,
-				    priv->prof->tx_pause,
-				    priv->prof->tx_ppp,
-				    priv->prof->rx_pause,
-				    priv->prof->rx_ppp);
+				    &priv->prof->tx_pause,
+				    &priv->prof->rx_pause);
+
+	if ((priv->prof->tx_pause != (pause->tx_pause != 0)) ||
+	    (priv->prof->rx_pause != (pause->rx_pause != 0)))
+		err = -EINVAL;
+
 	if (err)
 		en_err(priv, "Failed setting pause params\n");
 
@@ -334,6 +519,7 @@ static int mlx4_en_set_ringparam(struct 
 	u32 rx_size, tx_size;
 	int port_up = 0;
 	int err = 0;
+	int i;
 
 	if (param->rx_jumbo_pending || param->rx_mini_pending)
 		return -EINVAL;
@@ -345,8 +531,9 @@ static int mlx4_en_set_ringparam(struct 
 	tx_size = max_t(u32, tx_size, MLX4_EN_MIN_TX_SIZE);
 	tx_size = min_t(u32, tx_size, MLX4_EN_MAX_TX_SIZE);
 
-	if (rx_size == priv->prof->rx_ring_size &&
-	    tx_size == priv->prof->tx_ring_size)
+	if (rx_size == (priv->port_up ? priv->rx_ring[0]->actual_size :
+					priv->rx_ring[0]->size) &&
+	    tx_size == priv->tx_ring[0]->size)
 		return 0;
 
 	mutex_lock(&mdev->state_lock);
@@ -371,6 +558,15 @@ static int mlx4_en_set_ringparam(struct 
 			en_err(priv, "Failed starting port\n");
 	}
 
+	for (i = 0; i < priv->rx_ring_num; i++) {
+		priv->rx_cq[i]->moder_cnt = priv->rx_frames;
+		priv->rx_cq[i]->moder_time = priv->rx_usecs;
+		priv->last_moder_time[i] = MLX4_EN_AUTO_CONF;
+		err = mlx4_en_set_cq_moder(priv, priv->rx_cq[i]);
+		if (err)
+			goto out;
+	}
+
 out:
 	mutex_unlock(&mdev->state_lock);
 	return err;
@@ -380,13 +576,13 @@ static void mlx4_en_get_ringparam(struct
 				  struct ethtool_ringparam *param)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct mlx4_en_dev *mdev = priv->mdev;
 
 	memset(param, 0, sizeof(*param));
 	param->rx_max_pending = MLX4_EN_MAX_RX_SIZE;
 	param->tx_max_pending = MLX4_EN_MAX_TX_SIZE;
-	param->rx_pending = mdev->profile.prof[priv->port].rx_ring_size;
-	param->tx_pending = mdev->profile.prof[priv->port].tx_ring_size;
+	param->rx_pending = priv->port_up ?
+		priv->rx_ring[0]->actual_size : priv->rx_ring[0]->size;
+	param->tx_pending = priv->tx_ring[0]->size;
 }
 
 const struct ethtool_ops mlx4_en_ethtool_ops = {
@@ -407,7 +603,9 @@ const struct ethtool_ops mlx4_en_ethtool
 	.get_strings = mlx4_en_get_strings,
 	.get_sset_count = mlx4_en_get_sset_count,
 	.get_ethtool_stats = mlx4_en_get_ethtool_stats,
+	.self_test = mlx4_en_self_test,
 	.get_wol = mlx4_en_get_wol,
+	.set_wol = mlx4_en_set_wol,
 	.get_msglevel = mlx4_en_get_msglevel,
 	.set_msglevel = mlx4_en_set_msglevel,
 	.get_coalesce = mlx4_en_get_coalesce,
diff -r c23d1fc7e422 drivers/net/mlx4/en_frag.c
--- /dev/null
+++ b/drivers/net/mlx4/en_frag.c
@@ -0,0 +1,219 @@
+/*
+ * Copyright (c) 2007 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+#include <linux/etherdevice.h>
+
+#include "mlx4_en.h"
+
+
+static struct mlx4_en_ipfrag *find_session(struct mlx4_en_rx_ring *ring,
+					   struct iphdr *iph)
+{
+	struct mlx4_en_ipfrag *session;
+	int i;
+
+	for (i = 0; i < MLX4_EN_NUM_IPFRAG_SESSIONS; i++) {
+		session = &ring->ipfrag[i];
+		if (session->fragments == NULL)
+			continue;
+		if (session->daddr == iph->daddr &&
+		    session->saddr == iph->saddr &&
+		    session->id == iph->id &&
+		    session->protocol == iph->protocol) {
+			return session;
+		}
+	}
+	return NULL;
+}
+
+static struct mlx4_en_ipfrag *start_session(struct mlx4_en_rx_ring *ring,
+					    struct iphdr *iph)
+{
+	struct mlx4_en_ipfrag *session;
+	int index = -1;
+	int i;
+
+	for (i = 0; i < MLX4_EN_NUM_IPFRAG_SESSIONS; i++) {
+		if (ring->ipfrag[i].fragments == NULL) {
+			index = i;
+			break;
+		}
+	}
+	if (index < 0)
+		return NULL;
+
+	session = &ring->ipfrag[index];
+
+	return session;
+}
+
+
+static void flush_session(struct mlx4_en_priv *priv,
+			  struct mlx4_en_ipfrag *session,
+			  u16 more)
+{
+	struct sk_buff *skb = session->fragments;
+	struct iphdr *iph = ip_hdr(skb);
+	struct net_device *dev = skb->dev;
+
+	/* Update IP length and checksum */
+	iph->tot_len = htons(session->total_len);
+	iph->frag_off = htons(more | (session->offset >> 3));
+	iph->check = 0;
+	iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
+
+	if (session->vlan) {
+		if (priv->mdev->profile.use_napi)
+			vlan_hwaccel_receive_skb(skb, priv->vlgrp,
+						 be16_to_cpu(session->sl_vid));
+		else
+			vlan_hwaccel_rx(skb, priv->vlgrp,
+					be16_to_cpu(session->sl_vid));
+	} else {
+		if (priv->mdev->profile.use_napi)
+			netif_receive_skb(skb);
+		else
+			netif_rx(skb);
+	}
+	dev->last_rx = jiffies;
+	session->fragments = NULL;
+	session->last = NULL;
+}
+
+
+static inline void frag_append(struct mlx4_en_priv *priv,
+			       struct mlx4_en_ipfrag *session,
+			       struct sk_buff *skb,
+			       unsigned int data_len)
+{
+	struct sk_buff *parent = session->fragments;
+
+	/* Update skb bookkeeping */
+	parent->len += data_len;
+	parent->data_len += data_len;
+	session->total_len += data_len;
+
+	skb_pull(skb, skb->len - data_len);
+	parent->truesize += skb->truesize;
+
+	if (session->last)
+		session->last->next = skb;
+	else
+		skb_shinfo(parent)->frag_list = skb;
+
+	session->last = skb;
+}
+
+int mlx4_en_rx_frags(struct mlx4_en_priv *priv, struct mlx4_en_rx_ring *ring,
+		     struct sk_buff *skb, struct mlx4_cqe *cqe)
+{
+	struct mlx4_en_ipfrag *session;
+	struct iphdr *iph;
+	u16 ip_len;
+	u16 ip_hlen;
+	int data_len;
+	u16 offset;
+
+	skb_reset_network_header(skb);
+	skb_reset_transport_header(skb);
+	iph = ip_hdr(skb);
+	ip_len = ntohs(iph->tot_len);
+	ip_hlen = iph->ihl * 4;
+	data_len = ip_len - ip_hlen;
+	offset = ntohs(iph->frag_off);
+	offset &= IP_OFFSET;
+	offset <<= 3;
+
+	session = find_session(ring, iph);
+	if (unlikely(ip_fast_csum((u8 *)iph, iph->ihl))) {
+		if (session)
+			flush_session(priv, session, IP_MF);
+		return -EINVAL;
+	}
+	if (session) {
+		if (unlikely(session->offset + session->total_len !=
+			     offset + ip_hlen)) {
+			flush_session(priv, session, IP_MF);
+			goto new_session;
+		}
+		/* Packets smaller then 60 bytes are padded to that size
+		 * Need to fix len field of the skb to fit the actual data size
+		 * Since ethernet header already removed, the IP total length
+		 * is exactly the data size (the skb is linear)
+		 */
+		skb->len = ip_len;
+
+		frag_append(priv, session, skb, data_len);
+	} else {
+new_session:
+		session = start_session(ring, iph);
+		if (unlikely(!session))
+			return -ENOSPC;
+
+		session->fragments = skb;
+		session->daddr = iph->daddr;
+		session->saddr = iph->saddr;
+		session->id = iph->id;
+		session->protocol = iph->protocol;
+		session->total_len = ip_len;
+		session->offset = offset;
+		session->vlan = (priv->vlgrp &&
+				 (be32_to_cpu(cqe->vlan_my_qpn) &
+				  MLX4_CQE_VLAN_PRESENT_MASK)) ? 1 : 0;
+		session->sl_vid = cqe->sl_vid;
+	}
+	if (!(ntohs(iph->frag_off) & IP_MF))
+		flush_session(priv, session, 0);
+	else if (session->fragments->len + priv->dev->mtu > 65536)
+		flush_session(priv, session, IP_MF);
+
+	return 0;
+}
+
+
+void mlx4_en_flush_frags(struct mlx4_en_priv *priv,
+			 struct mlx4_en_rx_ring *ring)
+{
+	struct mlx4_en_ipfrag *session;
+	int i;
+
+	for (i = 0; i < MLX4_EN_NUM_IPFRAG_SESSIONS; i++) {
+		session = &ring->ipfrag[i];
+		if (session->fragments)
+			flush_session(priv, session, IP_MF);
+	}
+}
diff -r c23d1fc7e422 drivers/net/mlx4/en_lro.c
--- /dev/null
+++ b/drivers/net/mlx4/en_lro.c
@@ -0,0 +1,745 @@
+/*
+ * Copyright (c) 2007 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <net/ipv6.h>
+#include <net/ip6_checksum.h>
+#include <linux/tcp.h>
+#include <net/tcp.h>
+#include <linux/if_vlan.h>
+#include <linux/delay.h>
+
+#include "mlx4_en.h"
+
+#define IPV4_VERSION 0x4
+#define IPV6_VERSION 0x6
+
+/* LRO hash function - using sum of source and destination port LSBs is
+ * good enough */
+#define LRO_INDEX(th, size) \
+	((*((u8 *) &th->source + 1) + *((u8 *) &th->dest + 1)) & (size - 1))
+
+/* #define CONFIG_MLX4_EN_DEBUG_LRO */
+
+#ifdef CONFIG_MLX4_EN_DEBUG_LRO
+static void mlx4_en_lro_validate(struct mlx4_en_priv *priv, struct mlx4_en_lro *lro)
+{
+	int i;
+	int size, size2;
+	struct sk_buff *skb = lro->skb;
+	skb_frag_t *frags;
+	int len, len2;
+	int cur_skb = 0;
+
+	/* Sum fragment sizes of first skb */
+	len = skb->len;
+	size = skb_headlen(skb);
+	frags = skb_shinfo(skb)->frags;
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+		size += frags[i].size;
+
+	/* Add in fragments of linked skb's */
+	skb = skb_shinfo(skb)->frag_list;
+	while (skb) {
+		cur_skb++;
+		len2 = skb->len;
+		if (skb_headlen(skb)) {
+			mlx4_err(priv->mdev, "Bad LRO format: non-zero headlen "
+				  "in fraglist (skb:%d)\n", cur_skb);
+			return;
+		}
+
+		size2 = 0;
+		frags = skb_shinfo(skb)->frags;
+		for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+			size2 += frags[i].size;
+
+		if (size2 != len2) {
+			mlx4_err(priv->mdev, "Bad skb size:%d in LRO fraglist. "
+				 "Expected:%d (skb:%d)\n", size2, len2, cur_skb);
+			return;
+		}
+		size += size2;
+		skb = skb->next;
+	}
+
+	if (size != len)
+		mlx4_err(priv->mdev, "Bad LRO size:%d expected:%d\n", size, len);
+}
+#endif /* MLX4_EN_DEBUG_LRO */
+
+/**
+ *	mlx4_en_lro_flush_single - push the lro packet up the stack
+ *	@priv: private data of net_device
+ *	@ring: RX ring we currently operate on
+ *	@lro: lro session to flush
+ *
+ *	Finalize skb by setting it's size and updating IP and TCP
+ *	headers.
+ */
+static void mlx4_en_lro_flush_single(struct mlx4_en_priv *priv,
+		   struct mlx4_en_rx_ring *ring, struct mlx4_en_lro *lro)
+{
+	struct sk_buff *skb = lro->skb;
+	struct iphdr *iph;
+	struct ipv6hdr *ipv6h;
+	struct tcphdr *th;
+	unsigned int headlen = skb_headlen(skb);
+	__wsum tcp_hdr_csum;
+	u32 *ts;
+
+	if (lro->version == IPV4_VERSION) {
+		iph = (struct iphdr *) skb->data;
+		th = (struct tcphdr *)(iph + 1);
+
+		/* Update IP length and checksum */
+		iph->tot_len = htons(lro->tot_len);
+		iph->check = 0;
+		iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
+
+		/* TCP + IPv4 header checksum */
+		th->check = 0;
+		tcp_hdr_csum = csum_partial((u8 *)th, th->doff << 2, 0);
+		lro->data_csum = csum_add(lro->data_csum, tcp_hdr_csum);
+		th->check = csum_tcpudp_magic(iph->saddr, iph->daddr,
+				      lro->tot_len - (iph->ihl << 2),
+				      IPPROTO_TCP, lro->data_csum);
+	} else {
+		ipv6h = (struct ipv6hdr *) skb->data;
+		th = (struct tcphdr *)(ipv6h + 1);
+
+		/* Update IP length */
+		ipv6h->payload_len = htons(lro->tot_len -
+					   sizeof(struct ipv6hdr));
+
+		/* TCP + IPv6 header checksum */
+		th->check = 0;
+		tcp_hdr_csum = csum_partial((u8 *)th, th->doff << 2, 0);
+		lro->data_csum = csum_add(lro->data_csum, tcp_hdr_csum);
+		th->check = csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,
+					    lro->tot_len - sizeof(*ipv6h),
+					    IPPROTO_TCP, lro->data_csum);
+	}
+
+	/* Update latest TCP ack, window, psh, and timestamp */
+	th->ack_seq = lro->ack_seq;
+	th->window = lro->window;
+	th->psh = !!lro->psh;
+	if (lro->has_timestamp) {
+		ts = (u32 *) (th + 1);
+		ts[1] = htonl(lro->tsval);
+		ts[2] = lro->tsecr;
+	}
+
+	/* Update skb */
+	skb->len = lro->tot_len;
+	skb->data_len = lro->tot_len - headlen;
+	skb->truesize = skb->len + sizeof(struct sk_buff);
+	skb_shinfo(skb)->gso_size = lro->mss;
+	skb_shinfo(skb)->gso_type |= SKB_GSO_TCPV4;
+
+#ifdef CONFIG_MLX4_EN_DEBUG_LRO
+	mlx4_en_lro_validate(priv, lro);
+#endif /* CONFIG_MLX4_EN_DEBUG_LRO */
+
+       /* Push it up the stack */
+       if (priv->vlgrp && lro->has_vlan)
+	       vlan_hwaccel_receive_skb(skb, priv->vlgrp,
+					be16_to_cpu(lro->vlan_prio));
+       else
+	       netif_receive_skb(skb);
+       priv->dev->last_rx = jiffies;
+
+	/* Increment stats */
+	priv->port_stats.lro_flushed++;
+
+	/* Move session back to the free list */
+	hlist_del(&lro->node);
+	hlist_del(&lro->flush_node);
+	hlist_add_head(&lro->node, &ring->lro_free);
+}
+
+/**
+ *	mlx4_en_lro_flush - flush expired lro sessions
+ *	@priv: private data of net_device
+ *	@ring: RX ring we currently operate on
+ *	@all: if set will flush all lro sessions
+ *
+ *	Check expired (or all) lro sessions and push them up the stack
+ */
+void mlx4_en_lro_flush(struct mlx4_en_priv *priv, struct mlx4_en_rx_ring *ring, u8 all)
+{
+	struct mlx4_en_lro *lro;
+	struct hlist_node *node, *tmp;
+
+	hlist_for_each_entry_safe(lro, node, tmp, &ring->lro_flush, flush_node) {
+		if (all || time_after(jiffies, lro->expires))
+			mlx4_en_lro_flush_single(priv, ring, lro);
+	}
+}
+
+/**
+ *	mlx4_en_lro_append - append a packet to lro session
+ *	@priv: private data of net_device
+ *	@lro: lro session to append the packet to
+ *	@rx_desc: RX descriptor that contains WQE dma address
+ *	@skb_frags: an array of skb_frags that contain actual data
+ *	@page_alloc: replacement pages
+ *	@data_len: length of data only, without headers
+ *	@hlen: length of all headers
+ *
+ *	Returns 0 if packet appened succesfully
+ */
+static inline int mlx4_en_lro_append(struct mlx4_en_priv *priv,
+				   struct mlx4_en_lro *lro,
+				   struct mlx4_en_rx_desc *rx_desc,
+				   struct skb_frag_struct *skb_frags,
+				   struct mlx4_en_rx_alloc *page_alloc,
+				   unsigned int data_len,
+				   int hlen)
+{
+	struct sk_buff *skb = lro->skb_last;
+	struct skb_shared_info *info;
+	struct skb_frag_struct *frags_copy;
+	int nr_frags;
+
+	if (skb_shinfo(skb)->nr_frags + priv->num_frags > MAX_SKB_FRAGS)
+		return -ENOMEM;
+
+	info = skb_shinfo(skb);
+
+	/* Copy fragments from descriptor ring to skb */
+	frags_copy = info->frags + info->nr_frags;
+	nr_frags = mlx4_en_complete_rx_desc(priv, rx_desc, skb_frags,
+						frags_copy,
+						page_alloc,
+						data_len + hlen);
+	if (!nr_frags) {
+		en_dbg(DRV, priv, "Failed completing rx desc during LRO append\n");
+		return -ENOMEM;
+	}
+
+	/* Skip over headers */
+	frags_copy[0].page_offset += hlen;
+
+	if (nr_frags == 1)
+		frags_copy[0].size = data_len;
+	else {
+		/* Adjust size of last fragment to match packet length.
+		 * Note: if this fragment is also the first one, the
+		 *       operation is completed in the next line */
+		frags_copy[nr_frags - 1].size = hlen + data_len -
+				priv->frag_info[nr_frags - 1].frag_prefix_size;
+
+		/* Adjust size of first fragment */
+		frags_copy[0].size -= hlen;
+	}
+
+	/* Update skb bookkeeping */
+	skb->len += data_len;
+	skb->data_len += data_len;
+	info->nr_frags += nr_frags;
+	return 0;
+}
+
+/**
+ *	mlx4_en_lro_find_session - find lro session that matches src/dst
+ *	@mdev: mlx4_en_dev that we operate on
+ *	@priv: private data of net_device
+ *	@ring: RX ring we currently operate on
+ *	@iph: IPv4 header to use if applied
+ *	@ipv6h: IPv6 header to use if applied
+ *	@th: TCP header with src/dst ports
+ *	@version: the IPvX version to use
+ *
+ *	Returns the lro sessions matching src/dst IP and ports
+ *	uses the IP version specified in @version
+ */
+static inline struct mlx4_en_lro *mlx4_en_lro_find_session(struct mlx4_en_dev *mdev,
+						       struct mlx4_en_rx_ring *ring,
+						       struct iphdr *iph,
+						       struct ipv6hdr *ipv6h,
+						       struct tcphdr *th,
+						       int version)
+{
+	struct mlx4_en_lro *lro;
+	struct hlist_node *node;
+	int index = LRO_INDEX(th, mdev->profile.num_lro);
+	struct hlist_head *list = &ring->lro_hash[index];
+
+	if (version == IPV4_VERSION)
+		hlist_for_each_entry(lro, node, list, node) {
+			if (lro->sport_dport == *((u32 *) &th->source) &&
+			    lro->saddr == iph->saddr &&
+			    lro->daddr == iph->daddr)
+				return lro;
+		}
+	else
+		hlist_for_each_entry(lro, node, list, node) {
+			if (lro->sport_dport == *((u32 *) &th->source) &&
+			    ipv6_addr_equal(&lro->saddr_v6, &ipv6h->saddr) &&
+			    ipv6_addr_equal(&lro->daddr_v6, &ipv6h->daddr))
+				return lro;
+		}
+
+	return NULL;
+}
+
+/**
+ *	mlx4_en_lro_alloc_session - allocate a new lro session
+ *	@ring: RX ring we currently operate on
+ *
+ *	Returns a new lro session or NULL if no empty sessions left
+ */
+static inline struct mlx4_en_lro*
+mlx4_en_lro_alloc_session(struct mlx4_en_rx_ring *ring)
+{
+	return hlist_empty(&ring->lro_free) ? NULL :
+		hlist_entry(ring->lro_free.first, struct mlx4_en_lro, node);
+}
+
+/**
+ *	mlx4_en_lro_tcp_data_csum - calculate unfolded checksum
+ *	@iph: IPv4 header to use if applied
+ *	@ipv6h: IPv6 header to use if applied
+ *	@th: TCP header with src/dst ports
+ *	@len: TCP data len, no headers
+ *	@version: the IPvX version to use
+ *
+ *	Returns the unfolded checksum for the tcp data only
+ */
+static __wsum mlx4_en_lro_tcp_data_csum(struct iphdr *iph,
+					struct ipv6hdr *ipv6h,
+					struct tcphdr *th, int len,
+					u32 version)
+{
+	__wsum tcp_csum;
+	__wsum tcp_hdr_csum;
+	__wsum tcp_ps_hdr_csum;
+
+	tcp_csum = ~csum_unfold(th->check);
+	tcp_hdr_csum = csum_partial((u8 *)th, th->doff << 2, tcp_csum);
+
+	if (version == IPV4_VERSION) {
+		tcp_ps_hdr_csum = csum_tcpudp_nofold(iph->saddr, iph->daddr,
+						     len + (th->doff << 2),
+						     IPPROTO_TCP, 0);
+	} else {
+		tcp_ps_hdr_csum = csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,
+					    len + (th->doff << 2),
+					    IPPROTO_TCP, 0);
+		tcp_ps_hdr_csum = csum_unfold(tcp_ps_hdr_csum);
+	}
+
+	return csum_sub(csum_sub(tcp_csum, tcp_hdr_csum),
+			tcp_ps_hdr_csum);
+}
+
+/**
+ *	mlx4_en_can_lro - returns true if lro supported for this CQE
+ *	@status - CQE status flags
+ *
+ *	Returns 1 for supported lro
+ */
+static inline int mlx4_en_can_lro(__be16 status)
+{
+	static __be16 status_all;
+	static __be16 status_ipv4_ipok_tcp;
+	static __be16 status_ipv6_ipok_tcp;
+
+	status_all			   = cpu_to_be16(
+					     MLX4_CQE_STATUS_IPV4    |
+					     MLX4_CQE_STATUS_IPV4F   |
+					     MLX4_CQE_STATUS_IPV6    |
+					     MLX4_CQE_STATUS_IPV4OPT |
+					     MLX4_CQE_STATUS_TCP     |
+					     MLX4_CQE_STATUS_UDP     |
+					     MLX4_CQE_STATUS_IPOK);
+	status_ipv4_ipok_tcp		   = cpu_to_be16(
+					     MLX4_CQE_STATUS_IPV4    |
+					     MLX4_CQE_STATUS_IPOK    |
+					     MLX4_CQE_STATUS_TCP);
+	status_ipv6_ipok_tcp		   = cpu_to_be16(
+					     MLX4_CQE_STATUS_IPV6    |
+					     MLX4_CQE_STATUS_IPOK    |
+					     MLX4_CQE_STATUS_TCP);
+
+	status &= status_all;
+	return (status == status_ipv4_ipok_tcp ||
+		status == status_ipv6_ipok_tcp);
+}
+
+/**
+ *	mlx4_en_lro_rx - handle large recieve offload packets
+ *	@priv: private data of net_device
+ *	@ring: RX ring we operate on
+ *	@rx_desc: RX descriptor that contains WQE dma address
+ *	@skb_frags: an array of skb_frags that contain actual data
+ *	@lenght: byte count in the cqe
+ *	@cqe: cqe we operate on
+ *
+ *	Returns 0 for success
+ */
+int mlx4_en_lro_rx(struct mlx4_en_priv *priv, struct mlx4_en_rx_ring *ring,
+					  struct mlx4_en_rx_desc *rx_desc,
+					  struct skb_frag_struct *skb_frags,
+					  unsigned int length,
+					  struct mlx4_cqe *cqe)
+{
+	struct mlx4_en_dev *mdev = priv->mdev;
+	struct mlx4_en_lro *lro;
+	struct sk_buff *skb;
+	struct iphdr *iph;
+	struct ipv6hdr *ipv6h = NULL;
+	struct tcphdr *th;
+	struct mlx4_en_port_stats *pstats = &priv->port_stats;
+	dma_addr_t dma;
+	int tcp_hlen;
+	int tcp_data_len;
+	int hlen;
+	u16 ip_len;
+	void *va;
+	u32 *ts;
+	u32 seq;
+	u32 tsval = (u32) ~0UL;
+	u32 tsecr = 0;
+	u32 ack_seq;
+	u16 window;
+	u32 version;
+	int max_lro_header;
+	/* This packet is eligible for LRO if it is:
+	 * - DIX Ethernet (type interpretation)
+	 * - TCP/IP (v4)
+	 * - without IP options
+	 * - not an IP fragment
+	 * - TCP/IP (v6) */
+	if (!mlx4_en_can_lro(cqe->status))
+			return -1;
+
+	/* Get pointer to TCP header. We already know that the packet is DIX Ethernet/IPv4/TCP
+	 * with no VLAN (HW stripped it) and no IP options */
+	va = page_address(skb_frags[0].page) + skb_frags[0].page_offset;
+	iph = va + ETH_HLEN;
+	if (iph->version == IPV4_VERSION) {
+		/* IPV4 */
+		version = IPV4_VERSION;
+		th = (struct tcphdr *)(iph + 1);
+
+		/* Preset constant size fields to avoid
+		   additional IP version checking later */
+		hlen = sizeof(struct iphdr);
+		max_lro_header = ETH_HLEN +
+				 sizeof(struct iphdr) +
+				 sizeof(struct tcphdr) +
+				 TCPOLEN_TSTAMP_ALIGNED;
+	} else {
+		/*IPV6*/
+		version = IPV6_VERSION;
+		ipv6h = va + ETH_HLEN;
+		th = (struct tcphdr *)(ipv6h + 1);
+
+		hlen = sizeof(struct ipv6hdr);
+		max_lro_header = ETH_HLEN +
+				 sizeof(struct ipv6hdr) +
+				 sizeof(struct tcphdr) +
+				 TCPOLEN_TSTAMP_ALIGNED;
+	}
+
+	/* Synchronsize headers for processing */
+	dma = be64_to_cpu(rx_desc->data[0].addr);
+	dma_sync_single_range_for_cpu(&mdev->pdev->dev, dma, 0,
+				      max_lro_header, DMA_FROM_DEVICE);
+
+	/* We only handle aligned timestamp options */
+	tcp_hlen = (th->doff << 2);
+	if (tcp_hlen == sizeof(*th) + TCPOLEN_TSTAMP_ALIGNED) {
+		ts = (u32 *) (th + 1);
+		if (unlikely(*ts != htonl((TCPOPT_NOP << 24) |
+					  (TCPOPT_NOP << 16) |
+					  (TCPOPT_TIMESTAMP << 8) |
+					  TCPOLEN_TIMESTAMP)))
+			goto sync_device;
+		tsval = ntohl(ts[1]);
+		tsecr = ts[2];
+	} else if (tcp_hlen != sizeof(*th))
+		goto sync_device;
+
+
+	/* At this point we know we have a TCP packet that is likely to be
+	 * eligible for LRO. Therefore, see now if we have an oustanding
+	 * session that corresponds to this packet so we could flush it if
+	 * something still prevents LRO */
+	lro = mlx4_en_lro_find_session(mdev, ring, iph, ipv6h, th, version);
+
+	/* ensure no bits set besides ack or psh */
+	if (th->fin || th->syn || th->rst || th->urg || th->ece ||
+	    th->cwr || !th->ack) {
+		if (lro) {
+			/* First flush session to keep packets in-order */
+			mlx4_en_lro_flush_single(priv, ring, lro);
+		}
+		goto sync_device;
+	}
+
+	/* Get IP and TCP payload length */
+	if (version == IPV4_VERSION) {
+		ip_len = ntohs(iph->tot_len);
+		tcp_data_len = ip_len - tcp_hlen - sizeof(struct iphdr);
+	} else {
+		ip_len = ntohs(ipv6h->payload_len) + sizeof(struct ipv6hdr);
+		tcp_data_len = ip_len - tcp_hlen - sizeof(struct ipv6hdr);
+	}
+
+	/* Verify that the frame is big enough */
+	if (unlikely(length < ETH_HLEN + ip_len)) {
+		en_warn(priv, "Cannot LRO - ip payload exceeds frame!\n");
+		goto sync_device;
+	}
+
+	seq = ntohl(th->seq);
+	if (!tcp_data_len)
+		goto flush_session;
+
+	if (lro) {
+		/* Check VLAN tag */
+		if (be32_to_cpu(cqe->vlan_my_qpn) & MLX4_CQE_VLAN_PRESENT_MASK) {
+			if (cqe->sl_vid != lro->vlan_prio || !lro->has_vlan) {
+				mlx4_en_lro_flush_single(priv, ring, lro);
+				goto sync_device;
+			}
+		} else if (lro->has_vlan) {
+			mlx4_en_lro_flush_single(priv, ring, lro);
+			goto sync_device;
+		}
+
+		/* Check sequence number */
+		if (unlikely(seq != lro->next_seq)) {
+			mlx4_en_lro_flush_single(priv, ring, lro);
+			goto sync_device;
+		}
+
+		/* If the cummulative IP length is over 64K, flush and start
+		 * a new session */
+		if (lro->tot_len + tcp_data_len > 0xffff) {
+			mlx4_en_lro_flush_single(priv, ring, lro);
+			goto new_session;
+		}
+
+		/* Check timestamps */
+		if (tcp_hlen != sizeof(*th)) {
+			if (unlikely(lro->tsval > tsval || !tsecr))
+				goto sync_device;
+		}
+
+		window = th->window;
+		ack_seq = th->ack_seq;
+		if (likely(tcp_data_len)) {
+			/* Append the data! */
+			hlen += ETH_HLEN + tcp_hlen;
+			if (mlx4_en_lro_append(priv, lro, rx_desc, skb_frags,
+							ring->page_alloc,
+							tcp_data_len, hlen)) {
+				mlx4_en_lro_flush_single(priv, ring, lro);
+				goto sync_device;
+			}
+		} else {
+			/* No data */
+			dma_sync_single_range_for_device(&mdev->pdev->dev, dma,
+							 0, max_lro_header,
+							 DMA_FROM_DEVICE);
+		}
+
+		/* Update session */
+		lro->psh |= th->psh;
+		lro->next_seq += tcp_data_len;
+		lro->data_csum = csum_block_add(lro->data_csum,
+					mlx4_en_lro_tcp_data_csum(iph, ipv6h,
+						th, tcp_data_len, version),
+					lro->tot_len);
+		lro->tot_len += tcp_data_len;
+		lro->tsval = tsval;
+		lro->tsecr = tsecr;
+		lro->ack_seq = ack_seq;
+		lro->window = window;
+		if (tcp_data_len > lro->mss)
+			lro->mss = tcp_data_len;
+		pstats->lro_aggregated++;
+		if (th->psh)
+			mlx4_en_lro_flush_single(priv, ring, lro);
+		return 0;
+	}
+
+new_session:
+	if (th->psh)
+		goto sync_device;
+	lro = mlx4_en_lro_alloc_session(ring);
+	if (lro) {
+		skb = mlx4_en_rx_skb(priv, rx_desc, skb_frags, ring->page_alloc,
+							     ETH_HLEN + ip_len);
+		if (skb) {
+			int index;
+
+			/* Add in the skb */
+			lro->skb = skb;
+			lro->skb_last = skb;
+			skb->protocol = eth_type_trans(skb, priv->dev);
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			skb_record_rx_queue(skb, (be32_to_cpu(cqe->vlan_my_qpn) & 0xffffff) - priv->rss_map.base_qpn);
+
+			/* Initialize session */
+			if (version == IPV4_VERSION) {
+				lro->version = IPV4_VERSION;
+				lro->saddr = iph->saddr;
+				lro->daddr = iph->daddr;
+			} else {
+				lro->version = IPV6_VERSION;
+				ipv6_addr_copy(&lro->saddr_v6, &ipv6h->saddr);
+				ipv6_addr_copy(&lro->daddr_v6, &ipv6h->daddr);
+			}
+			lro->sport_dport = *((u32 *) &th->source);
+
+			lro->next_seq = seq + tcp_data_len;
+			lro->tot_len = ip_len;
+			lro->psh = th->psh;
+			lro->ack_seq = th->ack_seq;
+			lro->window = th->window;
+			lro->mss = tcp_data_len;
+			lro->data_csum = mlx4_en_lro_tcp_data_csum(iph, ipv6h,
+						th, tcp_data_len, version);
+
+			/* Handle vlans */
+			if (be32_to_cpu(cqe->vlan_my_qpn) & MLX4_CQE_VLAN_PRESENT_MASK) {
+				lro->vlan_prio = cqe->sl_vid;
+				lro->has_vlan = 1;
+			} else
+				lro->has_vlan = 0;
+
+			/* Handle timestamps */
+			if (tcp_hlen != sizeof(*th)) {
+				lro->tsval = tsval;
+				lro->tsecr = tsecr;
+				lro->has_timestamp = 1;
+			} else {
+				lro->tsval = (u32) ~0UL;
+				lro->has_timestamp = 0;
+			}
+
+			/* Activate this session */
+			lro->expires = jiffies + HZ / 25;
+			hlist_del(&lro->node);
+			index = LRO_INDEX(th, mdev->profile.num_lro);
+
+			hlist_add_head(&lro->node, &ring->lro_hash[index]);
+			hlist_add_head(&lro->flush_node, &ring->lro_flush);
+			pstats->lro_aggregated++;
+			return 0;
+		} else {
+			/* Packet is dropped because we were not able to allocate new
+			 * page for fragments */
+			dma_sync_single_range_for_device(&mdev->pdev->dev, dma,
+							 0, max_lro_header,
+							 DMA_FROM_DEVICE);
+			return 0;
+		}
+	} else {
+		pstats->lro_no_desc++;
+	}
+
+flush_session:
+	if (lro)
+		mlx4_en_lro_flush_single(priv, ring, lro);
+sync_device:
+	dma_sync_single_range_for_device(&mdev->pdev->dev, dma, 0,
+					 max_lro_header, DMA_FROM_DEVICE);
+	return -1;
+}
+
+/**
+ *	mlx4_en_lro_destroy - destory all lro sessions in a ring
+ *	@ring: RX ring we operate on
+ */
+void mlx4_en_lro_destroy(struct mlx4_en_rx_ring *ring)
+{
+	struct mlx4_en_lro *lro;
+	struct hlist_node *node, *tmp;
+
+	hlist_for_each_entry_safe(lro, node, tmp, &ring->lro_free, node) {
+		hlist_del(&lro->node);
+		kfree(lro);
+	}
+	kfree(ring->lro_hash);
+}
+
+/**
+ *	mlx4_en_lro_init - init lro hash table in a ring
+ *	@ring: RX ring we operate on
+ *	@num_lro: number of lro session to allocate for this ring
+ *
+ *	Returns 0 for success
+ */
+int mlx4_en_lro_init(struct mlx4_en_rx_ring *ring, int num_lro)
+{
+	struct mlx4_en_lro *lro;
+	int i;
+
+	INIT_HLIST_HEAD(&ring->lro_free);
+	INIT_HLIST_HEAD(&ring->lro_flush);
+	ring->lro_hash = kmalloc(sizeof(struct hlist_head) * num_lro,
+				 GFP_KERNEL);
+	if (!ring->lro_hash)
+		return -ENOMEM;
+
+	for (i = 0; i < num_lro; i++) {
+		INIT_HLIST_HEAD(&ring->lro_hash[i]);
+
+		lro = kzalloc_node(sizeof(struct mlx4_en_lro),
+				GFP_KERNEL, ring->numa_node);
+
+		if (!lro)
+			lro = kzalloc(sizeof(struct mlx4_en_lro), GFP_KERNEL);
+
+		if (!lro) {
+			mlx4_en_lro_destroy(ring);
+			return -ENOMEM;
+		}
+		INIT_HLIST_NODE(&lro->node);
+		INIT_HLIST_NODE(&lro->flush_node);
+		hlist_add_head(&lro->node, &ring->lro_free);
+	}
+	return 0;
+}
+
+
diff -r c23d1fc7e422 drivers/net/mlx4/en_main.c
--- a/drivers/net/mlx4/en_main.c
+++ b/drivers/net/mlx4/en_main.c
@@ -56,55 +56,66 @@ static const char mlx4_en_version[] =
 	module_param(X , uint, 0444); \
 	MODULE_PARM_DESC(X, desc);
 
-
+#define MLX4_EN_PARM_BOOL(X, def_val, desc) \
+	static unsigned int X = def_val;\
+	module_param(X , bool, 0444); \
+	MODULE_PARM_DESC(X, desc);
 /*
  * Device scope module parameters
  */
 
 
-/* Use a XOR rathern than Toeplitz hash function for RSS */
-MLX4_EN_PARM_INT(rss_xor, 0, "Use XOR hash function for RSS");
+/* Total number of RX Rings */
+MLX4_EN_PARM_INT(num_rx_rings, MAX_RX_RINGS,
+		 "Total number of RX Rings (default 16, range 1-16, power of 2)");
 
-/* RSS hash type mask - default to <saddr, daddr, sport, dport> */
-MLX4_EN_PARM_INT(rss_mask, 0xf, "RSS hash type bitmask");
+/* Enable RSS UDP traffic */
+MLX4_EN_PARM_BOOL(udp_rss, true,
+		 "Enable RSS for incomming UDP traffic or disabled (0)");
 
-/* Number of LRO sessions per Rx ring (rounded up to a power of two) */
-MLX4_EN_PARM_INT(num_lro, MLX4_EN_MAX_LRO_DESCRIPTORS,
-		 "Number of LRO sessions per ring or disabled (0)");
+MLX4_EN_PARM_BOOL(use_tx_polling, true, "Use polling for TX processing (default 0)");
 
-/* Priority pausing */
-MLX4_EN_PARM_INT(pfctx, 0, "Priority based Flow Control policy on TX[7:0]."
-			   " Per priority bit mask");
-MLX4_EN_PARM_INT(pfcrx, 0, "Priority based Flow Control policy on RX[7:0]."
-			   " Per priority bit mask");
+MLX4_EN_PARM_BOOL(enable_sys_tune, false, "Tune the cpu's for better performance (default 0)");
 
 static int mlx4_en_get_profile(struct mlx4_en_dev *mdev)
 {
 	struct mlx4_en_profile *params = &mdev->profile;
 	int i;
+	u8 pfctx, pfcrx;
 
-	params->rss_xor = (rss_xor != 0);
-	params->rss_mask = rss_mask & 0x1f;
-	params->num_lro = min_t(int, num_lro , MLX4_EN_MAX_LRO_DESCRIPTORS);
+	params->udp_rss = udp_rss;
+	if (params->udp_rss && !mdev->dev->caps.udp_rss) {
+		mlx4_warn(mdev, "UDP RSS is not supported on this device.\n");
+		params->udp_rss = 0;
+	}
+	params->use_tx_polling = use_tx_polling;
 	for (i = 1; i <= MLX4_MAX_PORTS; i++) {
+		mlx4_get_port_pfc(mdev->dev, i, &pfctx, &pfcrx);
 		params->prof[i].rx_pause = 1;
-		params->prof[i].rx_ppp = pfcrx;
 		params->prof[i].tx_pause = 1;
-		params->prof[i].tx_ppp = pfctx;
 		params->prof[i].tx_ring_size = MLX4_EN_DEF_TX_RING_SIZE;
 		params->prof[i].rx_ring_size = MLX4_EN_DEF_RX_RING_SIZE;
-		params->prof[i].tx_ring_num = MLX4_EN_NUM_TX_RINGS +
+		params->prof[i].tx_ring_num = MLX4_EN_NUM_HASH_RINGS +
 			(!!pfcrx) * MLX4_EN_NUM_PPP_RINGS;
+		params->prof[i].rx_ppp = pfcrx;
 	}
 
 	return 0;
 }
 
+static void *get_netdev(struct mlx4_dev *dev, void *ctx, u8 port)
+{
+	struct mlx4_en_dev *endev = ctx;
+
+	return endev->pndev[port];
+}
+
 static void mlx4_en_event(struct mlx4_dev *dev, void *endev_ptr,
 			  enum mlx4_dev_event event, int port)
 {
 	struct mlx4_en_dev *mdev = (struct mlx4_en_dev *) endev_ptr;
 	struct mlx4_en_priv *priv;
+	int i;
 
 	if (!mdev->pndev[port])
 		return;
@@ -119,6 +130,15 @@ static void mlx4_en_event(struct mlx4_de
 		queue_work(mdev->workqueue, &priv->linkstate_task);
 		break;
 
+	case MLX4_EVENT_TYPE_MAC_UPDATE:
+		priv->mac = dev->caps.def_mac[port];
+		for (i = 0; i < ETH_ALEN; i++) {
+			priv->dev->dev_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));
+			priv->dev->perm_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));
+		}
+		queue_work(mdev->workqueue, &priv->mac_task);
+		break;
+
 	case MLX4_DEV_EVENT_CATASTROPHIC_ERROR:
 		mlx4_err(mdev, "Internal error detected, restarting device\n");
 		break;
@@ -144,11 +164,14 @@ static void mlx4_en_remove(struct mlx4_d
 	flush_workqueue(mdev->workqueue);
 	destroy_workqueue(mdev->workqueue);
 	mlx4_mr_free(dev, &mdev->mr);
+	iounmap(mdev->uar_map);
 	mlx4_uar_free(dev, &mdev->priv_uar);
 	mlx4_pd_free(dev, mdev->priv_pdn);
 	kfree(mdev);
 }
 
+static struct mlx4_interface mlx4_en_interface;
+
 static void *mlx4_en_add(struct mlx4_dev *dev)
 {
 	static int mlx4_en_version_printed;
@@ -184,6 +207,7 @@ static void *mlx4_en_add(struct mlx4_dev
 	mdev->dma_device = &(dev->pdev->dev);
 	mdev->pdev = dev->pdev;
 	mdev->device_up = false;
+	mdev->mlx4_intf = &mlx4_en_interface;
 
 	mdev->LSO_support = !!(dev->caps.flags & (1 << 15));
 	if (!mdev->LSO_support)
@@ -194,7 +218,7 @@ static void *mlx4_en_add(struct mlx4_dev
 			 MLX4_PERM_LOCAL_WRITE |  MLX4_PERM_LOCAL_READ,
 			 0, 0, &mdev->mr)) {
 		mlx4_err(mdev, "Failed allocating memory region\n");
-		goto err_uar;
+		goto err_map;
 	}
 	if (mlx4_mr_enable(mdev->dev, &mdev->mr)) {
 		mlx4_err(mdev, "Failed enabling memory region\n");
@@ -213,16 +237,20 @@ static void *mlx4_en_add(struct mlx4_dev
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH)
 		mdev->port_cnt++;
 
-	/* If we did not receive an explicit number of Rx rings, default to
-	 * the number of completion vectors populated by the mlx4_core */
+	/* Number of RX rings is between (MIN_RX_RINGS, MAX_RX_RINGS) + 1
+	 * and depends on number of completion vectors */
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH) {
-		mlx4_info(mdev, "Using %d tx rings for port:%d\n",
-			  mdev->profile.prof[i].tx_ring_num, i);
-		mdev->profile.prof[i].rx_ring_num = min_t(int,
-			roundup_pow_of_two(dev->caps.num_comp_vectors),
-			MAX_RX_RINGS);
-		mlx4_info(mdev, "Defaulting to %d rx rings for port:%d\n",
-			  mdev->profile.prof[i].rx_ring_num, i);
+			if (!dev->caps.poolsz) {
+				int def_rings = max_t(int, dev->caps.num_comp_vectors,
+						      MIN_DEF_RX_RINGS);
+				mdev->profile.prof[i].rx_ring_num =
+					rounddown_pow_of_two(min_t(int, def_rings,
+								   min_t(int, MAX_RX_RINGS, num_rx_rings)));
+			} else {
+				mdev->profile.prof[i].rx_ring_num =
+					rounddown_pow_of_two(min_t(int, dev->caps.poolsz/
+					      dev->caps.num_ports - 1,
+					      min_t(int, MAX_RX_RINGS, num_rx_rings)));			}
 	}
 
 	/* Create our own workqueue for reset/multicast tasks
@@ -244,13 +272,33 @@ static void *mlx4_en_add(struct mlx4_dev
 	/* Create a netdev for each port */
 	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH) {
 		mlx4_info(mdev, "Activating port:%d\n", i);
-		if (mlx4_en_init_netdev(mdev, i, &mdev->profile.prof[i]))
+		if (mlx4_en_init_netdev(mdev, i, &mdev->profile.prof[i])) {
 			mdev->pndev[i] = NULL;
+			goto err_free_netdev;
+		}
 	}
 	return mdev;
 
+
+err_free_netdev:
+	mlx4_foreach_port(i, dev, MLX4_PORT_TYPE_ETH) {
+		if (mdev->pndev[i])
+			mlx4_en_destroy_netdev(mdev->pndev[i]);
+	}
+
+	mutex_lock(&mdev->state_lock);
+	mdev->device_up = false;
+	mutex_unlock(&mdev->state_lock);
+	flush_workqueue(mdev->workqueue);
+
+	/* Stop event queue before we drop down to release shared SW state */
+	destroy_workqueue(mdev->workqueue);
+
 err_mr:
 	mlx4_mr_free(dev, &mdev->mr);
+err_map:
+	if (!mdev->uar_map)
+		iounmap(mdev->uar_map);
 err_uar:
 	mlx4_uar_free(dev, &mdev->priv_uar);
 err_pd:
@@ -261,19 +309,98 @@ err_free_res:
 	return NULL;
 }
 
+enum mlx4_query_reply mlx4_en_query(void *endev_ptr, void *int_dev)
+{
+	struct mlx4_en_dev *mdev = endev_ptr;
+	struct net_device *netdev = int_dev;
+	int p;
+	
+	for (p = 1; p <= MLX4_MAX_PORTS; ++p)
+		if (mdev->pndev[p] == netdev)
+			return p;
+
+	return MLX4_QUERY_NOT_MINE;
+}
+
+static struct pci_device_id mlx4_en_pci_table[] = {
+	{ PCI_VDEVICE(MELLANOX, 0x6340) }, /* MT25408 "Hermon" SDR */
+	{ PCI_VDEVICE(MELLANOX, 0x634a) }, /* MT25408 "Hermon" DDR */
+	{ PCI_VDEVICE(MELLANOX, 0x6354) }, /* MT25408 "Hermon" QDR */
+	{ PCI_VDEVICE(MELLANOX, 0x6732) }, /* MT25408 "Hermon" DDR PCIe gen2 */
+	{ PCI_VDEVICE(MELLANOX, 0x673c) }, /* MT25408 "Hermon" QDR PCIe gen2 */
+	{ PCI_VDEVICE(MELLANOX, 0x6368) }, /* MT25408 "Hermon" EN 10GigE */
+	{ PCI_VDEVICE(MELLANOX, 0x6750) }, /* MT25408 "Hermon" EN 10GigE PCIe gen2 */
+	{ PCI_VDEVICE(MELLANOX, 0x6372) }, /* MT25458 ConnectX EN 10GBASE-T 10GigE */
+	{ PCI_VDEVICE(MELLANOX, 0x675a) }, /* MT25458 ConnectX EN 10GBASE-T+Gen2 10GigE */
+	{ PCI_VDEVICE(MELLANOX, 0x6764) }, /* MT26468 ConnectX EN 10GigE PCIe gen2 */
+	{ PCI_VDEVICE(MELLANOX, 0x6746) }, /* MT26438 ConnectX VPI PCIe 2.0 5GT/s - IB QDR / 10GigE Virt+ */
+	{ PCI_VDEVICE(MELLANOX, 0x676e) }, /* MT26478 ConnectX EN 40GigE PCIe 2.0 5GT/s */
+	{ PCI_VDEVICE(MELLANOX, 0x6778) }, /* MT26488 ConnectX VPI PCIe 2.0 5GT/s - IB DDR / 10GigE Virt+ */
+	{ PCI_VDEVICE(MELLANOX, 0x1000) },
+	{ PCI_VDEVICE(MELLANOX, 0x1001) },
+	{ PCI_VDEVICE(MELLANOX, 0x1002) },
+	{ PCI_VDEVICE(MELLANOX, 0x1003) },
+	{ PCI_VDEVICE(MELLANOX, 0x1004) },
+	{ PCI_VDEVICE(MELLANOX, 0x1005) },
+	{ PCI_VDEVICE(MELLANOX, 0x1006) },
+	{ PCI_VDEVICE(MELLANOX, 0x1007) },
+	{ PCI_VDEVICE(MELLANOX, 0x1008) },
+	{ PCI_VDEVICE(MELLANOX, 0x1009) },
+	{ PCI_VDEVICE(MELLANOX, 0x100a) },
+	{ PCI_VDEVICE(MELLANOX, 0x100b) },
+	{ PCI_VDEVICE(MELLANOX, 0x100c) },
+	{ PCI_VDEVICE(MELLANOX, 0x100d) },
+	{ PCI_VDEVICE(MELLANOX, 0x100e) },
+	{ PCI_VDEVICE(MELLANOX, 0x100f) },
+	{ 0, }
+};
+
+MODULE_DEVICE_TABLE(pci, mlx4_en_pci_table);
+
 static struct mlx4_interface mlx4_en_interface = {
-	.add	= mlx4_en_add,
-	.remove	= mlx4_en_remove,
-	.event	= mlx4_en_event,
+	.add		= mlx4_en_add,
+	.remove		= mlx4_en_remove,
+	.event		= mlx4_en_event,
+	.query		= mlx4_en_query,
+	.get_prot_dev	= get_netdev,
+	.protocol	= MLX4_PROT_EN,
 };
 
+void mlx4_en_verify_params(void)
+{
+	if (num_rx_rings < MIN_RX_RINGS || num_rx_rings > MAX_RX_RINGS) {
+		printk(KERN_WARNING "mlx4_en: WARNING: illegal module parameter num_rx_rings %d - "
+		       "should be in range %d-%d, will be changed to %d\n",
+		       num_rx_rings, MIN_RX_RINGS, MAX_RX_RINGS, MAX_RX_RINGS);
+		num_rx_rings = MAX_RX_RINGS;
+	} else if (rounddown_pow_of_two(num_rx_rings) != num_rx_rings) {
+		printk(KERN_WARNING "mlx4_en: WARNING: illegal module parameter num_rx_rings %d - "
+		       "should be power of 2, will be changed to %lu\n",
+		       num_rx_rings, rounddown_pow_of_two(num_rx_rings));
+		num_rx_rings = rounddown_pow_of_two(num_rx_rings);
+	}
+
+}
+
 static int __init mlx4_en_init(void)
 {
-	return mlx4_register_interface(&mlx4_en_interface);
+	int err;
+
+	mlx4_en_verify_params();
+	if (enable_sys_tune)
+		sys_tune_init();
+
+	 err = mlx4_register_interface(&mlx4_en_interface);
+	 if (err && enable_sys_tune)
+		 sys_tune_fini();
+	 return err;
+		 
 }
 
 static void __exit mlx4_en_cleanup(void)
 {
+	if (enable_sys_tune)
+		sys_tune_fini();
 	mlx4_unregister_interface(&mlx4_en_interface);
 }
 
diff -r c23d1fc7e422 drivers/net/mlx4/en_netdev.c
--- a/drivers/net/mlx4/en_netdev.c
+++ b/drivers/net/mlx4/en_netdev.c
@@ -35,6 +35,8 @@
 #include <linux/tcp.h>
 #include <linux/if_vlan.h>
 #include <linux/delay.h>
+#include <linux/cpufreq.h>
+#include <linux/topology.h>
 
 #include <linux/mlx4/driver.h>
 #include <linux/mlx4/device.h>
@@ -48,26 +50,23 @@
 static void mlx4_en_vlan_rx_register(struct net_device *dev, struct vlan_group *grp)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct mlx4_en_dev *mdev = priv->mdev;
-	int err;
 
 	en_dbg(HW, priv, "Registering VLAN group:%p\n", grp);
+
+	spin_lock_bh(&priv->vlan_lock);
 	priv->vlgrp = grp;
-
-	mutex_lock(&mdev->state_lock);
-	if (mdev->device_up && priv->port_up) {
-		err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, grp);
-		if (err)
-			en_err(priv, "Failed configuring VLAN filter\n");
-	}
-	mutex_unlock(&mdev->state_lock);
+	priv->vlgrp_modified = true;
+	spin_unlock_bh(&priv->vlan_lock);
 }
 
 static void mlx4_en_vlan_rx_add_vid(struct net_device *dev, unsigned short vid)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct mlx4_en_dev *mdev = priv->mdev;
-	int err;
+	int idx;
+	u8 field;
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+	struct net_device *vdev;
+#endif
 
 	if (!priv->vlgrp)
 		return;
@@ -75,40 +74,65 @@ static void mlx4_en_vlan_rx_add_vid(stru
 	en_dbg(HW, priv, "adding VLAN:%d (vlgrp entry:%p)\n",
 	       vid, vlan_group_get_device(priv->vlgrp, vid));
 
-	/* Add VID to port VLAN filter */
-	mutex_lock(&mdev->state_lock);
-	if (mdev->device_up && priv->port_up) {
-		err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, priv->vlgrp);
-		if (err)
-			en_err(priv, "Failed configuring VLAN filter\n");
+	spin_lock_bh(&priv->vlan_lock);
+	priv->vlgrp_modified = true;
+
+	/*
+	 * Each bit in vlan_register and vlan_unregister represents a vlan
+	 */
+	idx = vid >> 3;
+	field = 1 << (vid & 0x7);
+
+	if (priv->vlan_unregister[idx] & field)
+		/* if bit is set unset it */
+		priv->vlan_unregister[idx] &= ~field;
+	else
+		/* if bit unset set it */
+		priv->vlan_register[idx] |= field;
+
+	spin_unlock_bh(&priv->vlan_lock);
+#ifndef HAVE_NETDEV_VLAN_FEATURES
+	vdev = vlan_group_get_device(priv->vlgrp, vid);
+	if (vdev) {
+		vdev->features |= dev->features;
+		vdev->features |= NETIF_F_LLTX;
+		vlan_group_set_device(priv->vlgrp, vid, vdev);
 	}
-	mutex_unlock(&mdev->state_lock);
+#endif
 }
 
 static void mlx4_en_vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct mlx4_en_dev *mdev = priv->mdev;
-	int err;
+	int idx;
+	u8 field;
 
 	if (!priv->vlgrp)
 		return;
 
 	en_dbg(HW, priv, "Killing VID:%d (vlgrp:%p vlgrp entry:%p)\n",
 	       vid, priv->vlgrp, vlan_group_get_device(priv->vlgrp, vid));
+	spin_lock_bh(&priv->vlan_lock);
+	priv->vlgrp_modified = true;
 	vlan_group_set_device(priv->vlgrp, vid, NULL);
 
-	/* Remove VID from port VLAN filter */
-	mutex_lock(&mdev->state_lock);
-	if (mdev->device_up && priv->port_up) {
-		err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, priv->vlgrp);
-		if (err)
-			en_err(priv, "Failed configuring VLAN filter\n");
-	}
-	mutex_unlock(&mdev->state_lock);
+	/*
+	 * Each bit in vlan_register and vlan_unregister represents a vlan
+	 */
+	idx = vid >> 3;
+	field = 1 << (vid & 0x7);
+
+	if (priv->vlan_register[idx] & field)
+		/* if bit is set unset it */
+		priv->vlan_register[idx] &= ~field;
+	else
+		/* if bit is unset set it */
+		priv->vlan_unregister[idx] |= field;
+
+	spin_unlock_bh(&priv->vlan_lock);
 }
 
-static u64 mlx4_en_mac_to_u64(u8 *addr)
+u64 mlx4_en_mac_to_u64(u8 *addr)
 {
 	u64 mac = 0;
 	int i;
@@ -143,11 +167,10 @@ static void mlx4_en_do_set_mac(struct wo
 	int err = 0;
 
 	mutex_lock(&mdev->state_lock);
-	if (priv->port_up) {
+	if (mdev->device_up && priv->port_up) {
 		/* Remove old MAC and insert the new one */
-		mlx4_unregister_mac(mdev->dev, priv->port, priv->mac_index);
-		err = mlx4_register_mac(mdev->dev, priv->port,
-					priv->mac, &priv->mac_index);
+		err = mlx4_replace_mac(mdev->dev, priv->port,
+				       priv->base_qpn, priv->mac, 0);
 		if (err)
 			en_err(priv, "Failed changing HW MAC address\n");
 	} else
@@ -160,8 +183,8 @@ static void mlx4_en_do_set_mac(struct wo
 static void mlx4_en_clear_list(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct dev_mc_list *plist = priv->mc_list;
-	struct dev_mc_list *next;
+	struct mlx4_en_mc_list *plist = priv->mc_list;
+	struct mlx4_en_mc_list *next;
 
 	while (plist) {
 		next = plist->next;
@@ -175,17 +198,18 @@ static void mlx4_en_cache_mclist(struct 
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct dev_mc_list *mclist;
-	struct dev_mc_list *tmp;
-	struct dev_mc_list *plist = NULL;
+	struct mlx4_en_mc_list *tmp;
+	struct mlx4_en_mc_list *plist = NULL;
 
+	mlx4_en_clear_list(dev);
 	for (mclist = dev->mc_list; mclist; mclist = mclist->next) {
-		tmp = kmalloc(sizeof(struct dev_mc_list), GFP_ATOMIC);
+		tmp = kzalloc(sizeof(struct mlx4_en_mc_list), GFP_ATOMIC);
 		if (!tmp) {
 			en_err(priv, "failed to allocate multicast list\n");
 			mlx4_en_clear_list(dev);
 			return;
 		}
-		memcpy(tmp, mclist, sizeof(struct dev_mc_list));
+		memcpy(tmp->addr, mclist->dmi_addr, ETH_ALEN);
 		tmp->next = NULL;
 		if (plist)
 			plist->next = tmp;
@@ -195,6 +219,49 @@ static void mlx4_en_cache_mclist(struct 
 	}
 }
 
+static void update_mclist_flags(struct mlx4_en_mc_list *dst,
+				struct mlx4_en_mc_list *src)
+{
+	struct mlx4_en_mc_list *dst_i, *src_i, *tail;
+	bool found;
+
+	/* Find all the entries that should be removed from dst,
+	 * These are the entries that are not found in src */
+	for (dst_i = dst; dst_i->next; dst_i = dst_i->next) {
+		found = false;
+		for (src_i = src; src_i; src_i = src_i->next) {
+			if (!memcmp(dst_i->next->addr, src_i->addr, ETH_ALEN)) {
+				found = true;
+				break;
+			}
+		}
+		if (!found)
+			dst_i->next->action = MCLIST_REM;
+	}
+	tail = dst_i;
+
+	/* Add entries that exist in src but not in dst, mark them as need to add */
+	for (src_i = src; src_i; src_i = src_i->next) {
+		found = false;
+		for (dst_i = dst; dst_i->next; dst_i = dst_i->next) {
+			if (!memcmp(dst_i->next->addr, src_i->addr, ETH_ALEN)) {
+				dst_i->next->action = MCLIST_NONE;
+				found = true;
+				break;
+			}
+		}
+		if (!found) {
+			tail->next = kmalloc(sizeof(struct mlx4_en_mc_list), GFP_KERNEL);
+			if (!tail->next)
+				continue;
+			memcpy(tail->next, src_i, sizeof(struct mlx4_en_mc_list));
+			tail->next->next = NULL;
+			tail->next->action = MCLIST_ADD;
+			tail = tail->next;
+		}
+	}
+}
+
 
 static void mlx4_en_set_multicast(struct net_device *dev)
 {
@@ -212,8 +279,9 @@ static void mlx4_en_do_set_multicast(str
 						 mcast_task);
 	struct mlx4_en_dev *mdev = priv->mdev;
 	struct net_device *dev = priv->dev;
-	struct dev_mc_list *mclist;
+	struct mlx4_en_mc_list *mclist;
 	u64 mcast_addr = 0;
+	u8 mc_list[16] = {0};
 	int err;
 
 	mutex_lock(&mdev->state_lock);
@@ -228,6 +296,16 @@ static void mlx4_en_do_set_multicast(str
 		goto out;
 	}
 
+	if (!netif_carrier_ok(dev)) {
+		if (!mlx4_en_QUERY_PORT(mdev, priv->port)) {
+			if (priv->port_state.link_state) {
+				priv->last_link_state = MLX4_DEV_EVENT_PORT_UP;
+				netif_carrier_on(dev);
+				en_dbg(LINK, priv, "Link Up\n");
+			}
+		}
+	}
+
 	/*
 	 * Promsicuous mode: disable all filters
 	 */
@@ -239,8 +317,12 @@ static void mlx4_en_do_set_multicast(str
 			priv->flags |= MLX4_EN_FLAG_PROMISC;
 
 			/* Enable promiscouos mode */
-			err = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port,
-						     priv->base_qpn, 1);
+			if (!mdev->dev->caps.vep_uc_steering)
+				err = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port,
+							     priv->base_qpn, 1);
+			else
+				err = mlx4_unicast_promisc_add(mdev->dev, priv->base_qpn,
+							       priv->port);
 			if (err)
 				en_err(priv, "Failed enabling "
 					     "promiscous mode\n");
@@ -252,10 +334,21 @@ static void mlx4_en_do_set_multicast(str
 				en_err(priv, "Failed disabling "
 					     "multicast filter\n");
 
-			/* Disable port VLAN filter */
-			err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, NULL);
-			if (err)
-				en_err(priv, "Failed disabling VLAN filter\n");
+			/* Add the default qp number as multicast promisc */
+			if (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {
+				err = mlx4_multicast_promisc_add(mdev->dev, priv->base_qpn,
+								 priv->port);
+				if (err)
+					en_err(priv, "Failed entering multicast promisc mode\n");
+				priv->flags |= MLX4_EN_FLAG_MC_PROMISC;
+			}
+
+			if (priv->vlgrp) {
+				/* Disable port VLAN filter */
+				err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, NULL);
+				if (err)
+					en_err(priv, "Failed disabling VLAN filter\n");
+			}
 		}
 		goto out;
 	}
@@ -270,15 +363,30 @@ static void mlx4_en_do_set_multicast(str
 		priv->flags &= ~MLX4_EN_FLAG_PROMISC;
 
 		/* Disable promiscouos mode */
-		err = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port,
-					     priv->base_qpn, 0);
+		if (!mdev->dev->caps.vep_uc_steering)
+			err = mlx4_SET_PORT_qpn_calc(mdev->dev, priv->port,
+						     priv->base_qpn, 0);
+		else
+			err = mlx4_unicast_promisc_remove(mdev->dev, priv->base_qpn,
+							  priv->port);
 		if (err)
 			en_err(priv, "Failed disabling promiscous mode\n");
 
+		/* Disable Multicast promisc */
+		if (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {
+			err = mlx4_multicast_promisc_remove(mdev->dev, priv->base_qpn,
+							    priv->port);
+			if (err)
+				en_err(priv, "Failed disabling multicast promiscous mode\n");
+			priv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;
+		}
+
 		/* Enable port VLAN filter */
-		err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, priv->vlgrp);
-		if (err)
-			en_err(priv, "Failed enabling VLAN filter\n");
+		if (priv->vlgrp) {
+			err = mlx4_SET_VLAN_FLTR(mdev->dev, priv->port, priv->vlgrp);
+			if (err)
+				en_err(priv, "Failed enabling VLAN filter\n");
+		}
 	}
 
 	/* Enable/disable the multicast filter according to IFF_ALLMULTI */
@@ -287,13 +395,32 @@ static void mlx4_en_do_set_multicast(str
 					  0, MLX4_MCAST_DISABLE);
 		if (err)
 			en_err(priv, "Failed disabling multicast filter\n");
+
+		/* Add the default qp number as multicast promisc */
+		if (!(priv->flags & MLX4_EN_FLAG_MC_PROMISC)) {
+			err = mlx4_multicast_promisc_add(mdev->dev, priv->base_qpn,
+							 priv->port);
+			if (err)
+				en_err(priv, "Failed entering multicast promisc mode\n");
+			priv->flags |= MLX4_EN_FLAG_MC_PROMISC;
+		}
 	} else {
+
+		/* Disable Multicast promisc */
+		if (priv->flags & MLX4_EN_FLAG_MC_PROMISC) {
+			err = mlx4_multicast_promisc_remove(mdev->dev, priv->base_qpn,
+							    priv->port);
+			if (err)
+				en_err(priv, "Failed disabling multicast promiscous mode\n");
+			priv->flags &= ~MLX4_EN_FLAG_MC_PROMISC;
+		}
+
 		err = mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0,
 					  0, MLX4_MCAST_DISABLE);
 		if (err)
 			en_err(priv, "Failed disabling multicast filter\n");
 
-		/* Flush mcast filter and init it with broadcast address */
+                /* Flush mcast filter and init it with broadcast address */
 		mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, ETH_BCAST,
 				    1, MLX4_MCAST_CONFIG);
 
@@ -303,7 +430,7 @@ static void mlx4_en_do_set_multicast(str
 		mlx4_en_cache_mclist(dev);
 		netif_tx_unlock_bh(dev);
 		for (mclist = priv->mc_list; mclist; mclist = mclist->next) {
-			mcast_addr = mlx4_en_mac_to_u64(mclist->dmi_addr);
+			mcast_addr = mlx4_en_mac_to_u64(mclist->addr);
 			mlx4_SET_MCAST_FLTR(mdev->dev, priv->port,
 					    mcast_addr, 0, MLX4_MCAST_CONFIG);
 		}
@@ -312,7 +439,34 @@ static void mlx4_en_do_set_multicast(str
 		if (err)
 			en_err(priv, "Failed enabling multicast filter\n");
 
-		mlx4_en_clear_list(dev);
+		update_mclist_flags(&priv->curr_list, priv->mc_list);
+		for (mclist = &priv->curr_list; mclist->next; mclist = mclist->next) {
+			if (mclist->next->action == MCLIST_REM) {
+				/* detach this address and delete from list */
+				struct mlx4_en_mc_list *tmp = mclist->next->next;
+
+				memcpy(&mc_list[10], mclist->next->addr, ETH_ALEN);
+				mc_list[5] = priv->port;
+				mlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp,
+					      mc_list, MLX4_PROT_ETH, 0);
+
+				/* remove from list */
+				kfree(mclist->next);
+				mclist->next = tmp;
+				if (!mclist->next)
+					break;
+			}
+
+			if (mclist->next->action == MCLIST_ADD) {
+				/* attach the address */
+				memcpy(&mc_list[10], mclist->next->addr, ETH_ALEN);
+				mc_list[5] = priv->port;
+				mlx4_multicast_attach(mdev->dev, &priv->rss_map.indir_qp,
+					      mc_list, 0, MLX4_PROT_ETH, 0);
+			}
+
+		}
+
 	}
 out:
 	mutex_unlock(&mdev->state_lock);
@@ -323,15 +477,11 @@ static void mlx4_en_netpoll(struct net_d
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_cq *cq;
-	unsigned long flags;
 	int i;
 
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		cq = &priv->rx_cq[i];
-		spin_lock_irqsave(&cq->lock, flags);
-		napi_synchronize(&cq->napi);
-		mlx4_en_process_rx_cq(dev, cq, 0);
-		spin_unlock_irqrestore(&cq->lock, flags);
+		cq = priv->rx_cq[i];
+		napi_schedule(&cq->napi);
 	}
 }
 #endif
@@ -367,12 +517,12 @@ static void mlx4_en_set_default_moderati
 	int i;
 
 	/* If we haven't received a specific coalescing setting
-	 * (module param), we set the moderation parameters as follows:
+	 * (module param), we set the moderation paramters as follows:
 	 * - moder_cnt is set to the number of mtu sized packets to
 	 *   satisfy our coelsing target.
 	 * - moder_time is set to a fixed value.
 	 */
-	priv->rx_frames = MLX4_EN_RX_COAL_TARGET;
+	priv->rx_frames = MLX4_EN_RX_COAL_TARGET / priv->dev->mtu + 1;
 	priv->rx_usecs = MLX4_EN_RX_COAL_TIME;
 	en_dbg(INTR, priv, "Default coalesing params for mtu:%d - "
 			   "rx_frames:%d rx_usecs:%d\n",
@@ -380,13 +530,16 @@ static void mlx4_en_set_default_moderati
 
 	/* Setup cq moderation params */
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		cq = &priv->rx_cq[i];
+		cq = priv->rx_cq[i];
 		cq->moder_cnt = priv->rx_frames;
 		cq->moder_time = priv->rx_usecs;
+		priv->last_moder_time[i] = MLX4_EN_AUTO_CONF;
+		priv->last_moder_packets[i] = 0;
+		priv->last_moder_bytes[i] = 0;
 	}
 
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		cq = &priv->tx_cq[i];
+		cq = priv->tx_cq[i];
 		cq->moder_cnt = MLX4_EN_TX_COAL_PKTS;
 		cq->moder_time = MLX4_EN_TX_COAL_TIME;
 	}
@@ -397,12 +550,12 @@ static void mlx4_en_set_default_moderati
 	priv->pkt_rate_high = MLX4_EN_RX_RATE_HIGH;
 	priv->rx_usecs_high = MLX4_EN_RX_COAL_TIME_HIGH;
 	priv->sample_interval = MLX4_EN_SAMPLE_INTERVAL;
-	priv->adaptive_rx_coal = 1;
-	priv->last_moder_time = MLX4_EN_AUTO_CONF;
+	priv->adaptive_rx_coal = 0;
 	priv->last_moder_jiffies = 0;
-	priv->last_moder_packets = 0;
 	priv->last_moder_tx_packets = 0;
-	priv->last_moder_bytes = 0;
+
+	/* Set stored params flag */
+	priv->stored_mparams = true;
 }
 
 static void mlx4_en_auto_moderation(struct mlx4_en_priv *priv)
@@ -414,50 +567,31 @@ static void mlx4_en_auto_moderation(stru
 	unsigned long avg_pkt_size;
 	unsigned long rx_packets;
 	unsigned long rx_bytes;
-	unsigned long rx_byte_diff;
-	unsigned long tx_packets;
-	unsigned long tx_pkt_diff;
 	unsigned long rx_pkt_diff;
 	int moder_time;
-	int i, err;
+	int ring, err;
 
 	if (!priv->adaptive_rx_coal || period < priv->sample_interval * HZ)
 		return;
 
-	spin_lock_bh(&priv->stats_lock);
-	rx_packets = priv->stats.rx_packets;
-	rx_bytes = priv->stats.rx_bytes;
-	tx_packets = priv->stats.tx_packets;
-	spin_unlock_bh(&priv->stats_lock);
+	for (ring = 0; ring < priv->rx_ring_num; ring++) {
+		spin_lock_bh(&priv->stats_lock);
+		rx_packets = priv->rx_ring[ring]->packets;
+		rx_bytes = priv->rx_ring[ring]->bytes;
+		spin_unlock_bh(&priv->stats_lock);
 
-	if (!priv->last_moder_jiffies || !period)
-		goto out;
-
-	tx_pkt_diff = ((unsigned long) (tx_packets -
-					priv->last_moder_tx_packets));
-	rx_pkt_diff = ((unsigned long) (rx_packets -
-					priv->last_moder_packets));
-	packets = max(tx_pkt_diff, rx_pkt_diff);
-	rx_byte_diff = rx_bytes - priv->last_moder_bytes;
-	rx_byte_diff = rx_byte_diff ? rx_byte_diff : 1;
-	rate = packets * HZ / period;
-	avg_pkt_size = packets ? ((unsigned long) (rx_bytes -
-				 priv->last_moder_bytes)) / packets : 0;
-
-	/* Apply auto-moderation only when packet rate exceeds a rate that
-	 * it matters */
-	if (rate > MLX4_EN_RX_RATE_THRESH) {
-		/* If tx and rx packet rates are not balanced, assume that
-		 * traffic is mainly BW bound and apply maximum moderation.
-		 * Otherwise, moderate according to packet rate */
-		if (2 * tx_pkt_diff > 3 * rx_pkt_diff &&
-		    rx_pkt_diff / rx_byte_diff <
-		    MLX4_EN_SMALL_PKT_SIZE)
-			moder_time = priv->rx_usecs_low;
-		else if (2 * rx_pkt_diff > 3 * tx_pkt_diff)
-			moder_time = priv->rx_usecs_high;
-		else {
-			if (rate < priv->pkt_rate_low)
+		rx_pkt_diff = ((unsigned long) (rx_packets -
+						priv->last_moder_packets[ring]));
+		packets = rx_pkt_diff;
+		rate = packets * HZ / period;
+		avg_pkt_size = packets ? ((unsigned long) (rx_bytes -
+					 priv->last_moder_bytes[ring])) / packets : 0;
+	
+		/* Apply auto-moderation only when packet rate exceeds a rate that
+		 * it matters */
+		if (rate > (MLX4_EN_RX_RATE_THRESH / priv->rx_ring_num) &&
+		    avg_pkt_size > MLX4_EN_AVG_PKT_SMALL) {
+                        if (rate < priv->pkt_rate_low)
 				moder_time = priv->rx_usecs_low;
 			else if (rate > priv->pkt_rate_high)
 				moder_time = priv->rx_usecs_high;
@@ -466,39 +600,113 @@ static void mlx4_en_auto_moderation(stru
 					(priv->rx_usecs_high - priv->rx_usecs_low) /
 					(priv->pkt_rate_high - priv->pkt_rate_low) +
 					priv->rx_usecs_low;
+		} else {
+			moder_time = priv->rx_usecs_low;
 		}
-	} else {
-		/* When packet rate is low, use default moderation rather than
-		 * 0 to prevent interrupt storms if traffic suddenly increases */
-		moder_time = priv->rx_usecs;
+	
+		if (moder_time != priv->last_moder_time[ring]) {
+			priv->last_moder_time[ring] = moder_time;
+			cq = priv->rx_cq[ring];
+			cq->moder_time = moder_time;
+			err = mlx4_en_set_cq_moder(priv, cq);
+			if (err)
+				en_err(priv, "Failed modifying moderation for cq:%d\n", ring);
+		}
+		priv->last_moder_packets[ring] = rx_packets;
+		priv->last_moder_bytes[ring] = rx_bytes;
 	}
 
-	en_dbg(INTR, priv, "tx rate:%lu rx_rate:%lu\n",
-	       tx_pkt_diff * HZ / period, rx_pkt_diff * HZ / period);
+	priv->last_moder_jiffies = jiffies;
+}
 
-	en_dbg(INTR, priv, "Rx moder_time changed from:%d to %d period:%lu "
-	       "[jiff] packets:%lu avg_pkt_size:%lu rate:%lu [p/s])\n",
-		 priv->last_moder_time, moder_time, period, packets,
-		 avg_pkt_size, rate);
+static void mlx4_en_set_stats(struct mlx4_en_priv *priv,
+			      struct mlx4_eth_common_counters *eth_counters)
+{
+	struct net_device_stats *stats = &priv->stats;
+	int i;
 
-	if (moder_time != priv->last_moder_time) {
-		priv->last_moder_time = moder_time;
-		for (i = 0; i < priv->rx_ring_num; i++) {
-			cq = &priv->rx_cq[i];
-			cq->moder_time = moder_time;
-			err = mlx4_en_set_cq_moder(priv, cq);
-			if (err) {
-				en_err(priv, "Failed modifying moderation for cq:%d\n", i);
-				break;
+	spin_lock_bh(&priv->stats_lock);
+
+	stats->rx_packets = eth_counters->iboe_rx_packets;
+	stats->rx_bytes = eth_counters->iboe_rx_bytess;
+	priv->port_stats.rx_chksum_good = 0;
+	priv->port_stats.rx_chksum_none = 0;
+	for (i = 0; i < priv->rx_ring_num; i++) {
+		stats->rx_packets += priv->rx_ring[i]->packets;
+		stats->rx_bytes += priv->rx_ring[i]->bytes;
+		priv->port_stats.rx_chksum_good += priv->rx_ring[i]->csum_ok;
+		priv->port_stats.rx_chksum_none += priv->rx_ring[i]->csum_none;
+	}
+	stats->tx_packets = eth_counters->iboe_tx_packets;
+	stats->tx_bytes = eth_counters->iboe_tx_bytess;
+	priv->port_stats.tx_chksum_offload = 0;
+	for (i = 0; i <= priv->tx_ring_num; i++) {
+		stats->tx_packets += priv->tx_ring[i]->packets;
+		stats->tx_bytes += priv->tx_ring[i]->bytes;
+		priv->port_stats.tx_chksum_offload += priv->tx_ring[i]->tx_csum;
+	}
+
+	stats->rx_errors = eth_counters->rx_errors;
+
+	stats->tx_errors = eth_counters->tx_errors;
+	stats->multicast = eth_counters->multicast;
+	stats->collisions = 0;
+	stats->rx_length_errors = eth_counters->rx_length_errors;
+	stats->rx_over_errors = eth_counters->rx_over_errors;
+	stats->rx_crc_errors = eth_counters->rx_crc_errors;
+	stats->rx_frame_errors = 0;
+	stats->rx_fifo_errors = eth_counters->rx_fifo_errors;
+	stats->rx_missed_errors = eth_counters->rx_missed_errors;
+	stats->tx_aborted_errors = 0;
+	stats->tx_carrier_errors = 0;
+	stats->tx_fifo_errors = 0;
+	stats->tx_heartbeat_errors = 0;
+	stats->tx_window_errors = 0;
+
+	priv->pkstats.broadcast = eth_counters->broadcast;
+
+	spin_unlock_bh(&priv->stats_lock);
+}
+
+static void mlx4_en_handle_vlans(struct mlx4_en_priv *priv)
+{
+	u8 vlan_register[MLX4_VLREG_SIZE];
+	u8 vlan_unregister[MLX4_VLREG_SIZE];
+	int i, j, idx;
+	u16 vid;
+
+	/* cache the vlan data for processing 
+	 * done under lock to avoid changes during work */
+	spin_lock_bh(&priv->vlan_lock);
+	for (i = 0; i < MLX4_VLREG_SIZE; i++) {
+		vlan_register[i] = priv->vlan_register[i];
+		priv->vlan_register[i] = 0;
+		vlan_unregister[i] = priv->vlan_unregister[i];
+		priv->vlan_unregister[i] = 0;
+	}
+	priv->vlgrp_modified = false;
+	spin_unlock_bh(&priv->vlan_lock);
+
+	/* Configure the vlan filter 
+	 * The vlgrp is updated with all the vids that need to be allowed */
+	if (mlx4_SET_VLAN_FLTR(priv->mdev->dev, priv->port, priv->vlgrp))
+		en_err(priv, "Failed configuring VLAN filter\n");
+
+	/* Configure the VLAN table */
+	for (i = 0; i < MLX4_VLREG_SIZE; i++) {
+		for (j = 0; j < 8; j++) {
+			vid = (i << 3) + j;
+			if (vlan_register[i] & (1 << j))
+				if (mlx4_register_vlan(priv->mdev->dev, priv->port, vid, &idx))
+					en_dbg(HW, priv, "failed registering vlan %d\n", vid);
+			if (vlan_unregister[i] & (1 << j)) {
+				if (!mlx4_find_cached_vlan(priv->mdev->dev, priv->port, vid, &idx))
+					mlx4_unregister_vlan(priv->mdev->dev, priv->port, idx);
+				else
+					en_dbg(HW, priv, "could not find vid %d in cache\n", vid);
 			}
 		}
 	}
-
-out:
-	priv->last_moder_packets = rx_packets;
-	priv->last_moder_tx_packets = tx_packets;
-	priv->last_moder_bytes = rx_bytes;
-	priv->last_moder_jiffies = jiffies;
 }
 
 static void mlx4_en_do_get_stats(struct work_struct *work)
@@ -507,17 +715,32 @@ static void mlx4_en_do_get_stats(struct 
 	struct mlx4_en_priv *priv = container_of(delay, struct mlx4_en_priv,
 						 stats_task);
 	struct mlx4_en_dev *mdev = priv->mdev;
+	struct mlx4_eth_common_counters eth_counters;
 	int err;
 
-	err = mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 0);
-	if (err)
-		en_dbg(HW, priv, "Could not update stats \n");
+	memset(&eth_counters, 0, sizeof(eth_counters));
+
+	err = mlx4_DUMP_ETH_STATS(mdev->dev, priv->port, 0, &eth_counters);
+	if (!err)
+		mlx4_en_set_stats(priv, &eth_counters);
+	else
+		en_dbg(HW, priv, "Could not update stats\n");
+
+	mlx4_en_QUERY_PORT(priv->mdev, priv->port);
 
 	mutex_lock(&mdev->state_lock);
 	if (mdev->device_up) {
-		if (priv->port_up)
+		if (priv->port_up) {
+			if (priv->vlgrp_modified)
+				mlx4_en_handle_vlans(priv);
+
 			mlx4_en_auto_moderation(priv);
+		}
 
+		if (mdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port]) {
+			queue_work(mdev->workqueue, &priv->mac_task);
+			mdev->mac_removed[MLX4_MAX_PORTS + 1 - priv->port] = 0;
+		}
 		queue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);
 	}
 	mutex_unlock(&mdev->state_lock);
@@ -535,18 +758,17 @@ static void mlx4_en_linkstate(struct wor
 	 * report to system log */
 	if (priv->last_link_state != linkstate) {
 		if (linkstate == MLX4_DEV_EVENT_PORT_DOWN) {
-			en_dbg(LINK, priv, "Link Down\n");
+			en_info(priv, "Link Down\n");
 			netif_carrier_off(priv->dev);
 		} else {
-			en_dbg(LINK, priv, "Link Up\n");
+			en_info(priv, "Link Up\n");
 			netif_carrier_on(priv->dev);
 		}
+		priv->last_link_state = linkstate;
 	}
-	priv->last_link_state = linkstate;
 	mutex_unlock(&mdev->state_lock);
 }
 
-
 int mlx4_en_start_port(struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
@@ -558,6 +780,7 @@ int mlx4_en_start_port(struct net_device
 	int err = 0;
 	int i;
 	int j;
+	u8 mc_list[16] = {0};
 
 	if (priv->port_up) {
 		en_dbg(DRV, priv, "start port called while port already up\n");
@@ -575,10 +798,11 @@ int mlx4_en_start_port(struct net_device
 		en_err(priv, "Failed to activate RX rings\n");
 		return err;
 	}
+
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		cq = &priv->rx_cq[i];
+		cq = priv->rx_cq[i];
 
-		err = mlx4_en_activate_cq(priv, cq);
+		err = mlx4_en_activate_cq(priv, cq, i);
 		if (err) {
 			en_err(priv, "Failed activating Rx CQ\n");
 			goto cq_err;
@@ -592,25 +816,36 @@ int mlx4_en_start_port(struct net_device
 			goto cq_err;
 		}
 		mlx4_en_arm_cq(priv, cq);
-		priv->rx_ring[i].cqn = cq->mcq.cqn;
+		priv->rx_ring[i]->cqn = cq->mcq.cqn;
 		++rx_index;
 	}
 
+	/* Set port mac number */
+	en_dbg(DRV, priv, "Setting mac for port %d\n", priv->port);
+	err = mlx4_register_mac(mdev->dev, priv->port,
+				priv->mac, &priv->base_qpn, 0);
+	if (err) {
+		en_err(priv, "Failed setting port mac\n");
+		goto cq_err;
+	}
+	mdev->mac_removed[priv->port] = 0;
+
 	err = mlx4_en_config_rss_steer(priv);
 	if (err) {
 		en_err(priv, "Failed configuring rss steering\n");
-		goto cq_err;
+		goto mac_err;
 	}
 
 	/* Configure tx cq's and rings */
 	for (i = 0; i < priv->tx_ring_num; i++) {
 		/* Configure cq */
-		cq = &priv->tx_cq[i];
-		err = mlx4_en_activate_cq(priv, cq);
+		cq = priv->tx_cq[i];
+ 		err = mlx4_en_activate_cq(priv, cq, i);
 		if (err) {
 			en_err(priv, "Failed allocating Tx CQ\n");
 			goto tx_err;
 		}
+
 		err = mlx4_en_set_cq_moder(priv, cq);
 		if (err) {
 			en_err(priv, "Failed setting cq moderation parameters");
@@ -621,7 +856,7 @@ int mlx4_en_start_port(struct net_device
 		cq->buf->wqe_index = cpu_to_be16(0xffff);
 
 		/* Configure ring */
-		tx_ring = &priv->tx_ring[i];
+		tx_ring = priv->tx_ring[i];
 		err = mlx4_en_activate_tx_ring(priv, tx_ring, cq->mcq.cqn);
 		if (err) {
 			en_err(priv, "Failed allocating Tx ring\n");
@@ -635,12 +870,10 @@ int mlx4_en_start_port(struct net_device
 	}
 
 	/* Configure port */
-	err = mlx4_SET_PORT_general(mdev->dev, priv->port,
+	err = mlx4_SET_PORT_general(mdev->mlx4_intf, mdev->dev, priv->port,
 				    priv->rx_skb_size + ETH_FCS_LEN,
-				    priv->prof->tx_pause,
-				    priv->prof->tx_ppp,
-				    priv->prof->rx_pause,
-				    priv->prof->rx_ppp);
+				    &priv->prof->tx_pause,
+				    &priv->prof->rx_pause);
 	if (err) {
 		en_err(priv, "Failed setting port general configurations "
 			     "for port %d, with error %d\n", priv->port, err);
@@ -652,22 +885,22 @@ int mlx4_en_start_port(struct net_device
 		en_err(priv, "Failed setting default qp numbers\n");
 		goto tx_err;
 	}
-	/* Set port mac number */
-	en_dbg(DRV, priv, "Setting mac for port %d\n", priv->port);
-	err = mlx4_register_mac(mdev->dev, priv->port,
-				priv->mac, &priv->mac_index);
-	if (err) {
-		en_err(priv, "Failed setting port mac\n");
-		goto tx_err;
+
+	if (!priv->port_inited) {
+		err = mlx4_INIT_PORT(mdev->dev, priv->port);
+		if (err) {
+			en_err(priv, "Failed Initializing port\n");
+			goto tx_err;
+		}
+		priv->port_inited = true;
 	}
 
-	/* Init port */
-	en_dbg(HW, priv, "Initializing port\n");
-	err = mlx4_INIT_PORT(mdev->dev, priv->port);
-	if (err) {
-		en_err(priv, "Failed Initializing port\n");
-		goto mac_err;
-	}
+	/* Attach rx QP to bradcast address */
+	memset(&mc_list[10], 0xff, ETH_ALEN);
+	mc_list[5] = priv->port;
+	if (mlx4_multicast_attach(mdev->dev, &priv->rss_map.indir_qp, mc_list,
+				  0, MLX4_PROT_ETH, 0))
+		mlx4_warn(mdev, "Failed Attaching Broadcast\n");
 
 	/* Schedule multicast task to populate multicast list */
 	queue_work(mdev->workqueue, &priv->mcast_task);
@@ -676,20 +909,20 @@ int mlx4_en_start_port(struct net_device
 	netif_tx_start_all_queues(dev);
 	return 0;
 
-mac_err:
-	mlx4_unregister_mac(mdev->dev, priv->port, priv->mac_index);
 tx_err:
 	while (tx_index--) {
-		mlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[tx_index]);
-		mlx4_en_deactivate_cq(priv, &priv->tx_cq[tx_index]);
+		mlx4_en_deactivate_tx_ring(priv, priv->tx_ring[tx_index]);
+		mlx4_en_deactivate_cq(priv, priv->tx_cq[tx_index]);
 	}
 
 	mlx4_en_release_rss_steer(priv);
+mac_err:
+	mlx4_unregister_mac(mdev->dev, priv->port, priv->base_qpn);
 cq_err:
 	while (rx_index--)
-		mlx4_en_deactivate_cq(priv, &priv->rx_cq[rx_index]);
+		mlx4_en_deactivate_cq(priv, priv->rx_cq[rx_index]);
 	for (i = 0; i < priv->rx_ring_num; i++)
-		mlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);
+		mlx4_en_deactivate_rx_ring(priv, priv->rx_ring[i]);
 
 	return err; /* need to close devices */
 }
@@ -699,7 +932,10 @@ void mlx4_en_stop_port(struct net_device
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_en_dev *mdev = priv->mdev;
+	struct mlx4_en_mc_list *mclist;
+	struct mlx4_en_mc_list *next;
 	int i;
+	u8 mc_list[16] = {0};
 
 	if (!priv->port_up) {
 		en_dbg(DRV, priv, "stop port called while port already down\n");
@@ -709,34 +945,62 @@ void mlx4_en_stop_port(struct net_device
 	/* Synchronize with tx routine */
 	netif_tx_lock_bh(dev);
 	netif_tx_stop_all_queues(dev);
+	dev->trans_start = jiffies;
 	netif_tx_unlock_bh(dev);
 
+	/* Set port as not active */
+	priv->port_up = false;
+
+	/* Detach All multicasts */
+	memset(&mc_list[10], 0xff, ETH_ALEN);
+	mc_list[5] = priv->port;
+	mlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp, mc_list,
+			      MLX4_PROT_ETH, 0);
+	for (mclist = priv->mc_list; mclist; mclist = mclist->next) {
+		memcpy(&mc_list[10], mclist->addr, ETH_ALEN);
+		mc_list[5] = priv->port;
+		mlx4_multicast_detach(mdev->dev, &priv->rss_map.indir_qp,
+				      mc_list, MLX4_PROT_ETH, 0);
+	}
+	mlx4_en_clear_list(dev);
+
+	mclist = priv->curr_list.next;
+	while (mclist) {
+		next = mclist->next;
+		kfree(mclist);
+		mclist = next;
+	}
+	priv->curr_list.next = NULL;
+
+	/* Flush multicast filter */
+	mlx4_SET_MCAST_FLTR(mdev->dev, priv->port, 0, 1, MLX4_MCAST_CONFIG);
+
 	/* close port*/
-	priv->port_up = false;
 	mlx4_CLOSE_PORT(mdev->dev, priv->port);
-
-	/* Unregister Mac address for the port */
-	mlx4_unregister_mac(mdev->dev, priv->port, priv->mac_index);
+	priv->port_inited = false;
 
 	/* Free TX Rings */
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		mlx4_en_deactivate_tx_ring(priv, &priv->tx_ring[i]);
-		mlx4_en_deactivate_cq(priv, &priv->tx_cq[i]);
+		mlx4_en_deactivate_tx_ring(priv, priv->tx_ring[i]);
+		mlx4_en_deactivate_cq(priv, priv->tx_cq[i]);
 	}
 	msleep(10);
 
 	for (i = 0; i < priv->tx_ring_num; i++)
-		mlx4_en_free_tx_buf(dev, &priv->tx_ring[i]);
+		mlx4_en_free_tx_buf(dev, priv->tx_ring[i]);
 
 	/* Free RSS qps */
 	mlx4_en_release_rss_steer(priv);
 
+	/* Unregister Mac address for the port */
+	mlx4_unregister_mac(mdev->dev, priv->port, priv->base_qpn);
+	mdev->mac_removed[priv->port] = 1;
+
 	/* Free RX Rings */
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		mlx4_en_deactivate_rx_ring(priv, &priv->rx_ring[i]);
-		while (test_bit(NAPI_STATE_SCHED, &priv->rx_cq[i].napi.state))
-			msleep(1);
-		mlx4_en_deactivate_cq(priv, &priv->rx_cq[i]);
+		mlx4_en_deactivate_rx_ring(priv, priv->rx_ring[i]);
+		napi_synchronize(&priv->rx_cq[i]->napi);
+		mlx4_en_deactivate_cq(priv, priv->rx_cq[i]);
 	}
 }
 
@@ -775,22 +1039,23 @@ static int mlx4_en_open(struct net_devic
 	}
 
 	/* Reset HW statistics and performance counters */
-	if (mlx4_en_DUMP_ETH_STATS(mdev, priv->port, 1))
+	if (mlx4_DUMP_ETH_STATS(mdev->dev, priv->port, 1, NULL))
 		en_dbg(HW, priv, "Failed dumping statistics\n");
 
 	memset(&priv->stats, 0, sizeof(priv->stats));
 	memset(&priv->pstats, 0, sizeof(priv->pstats));
 
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		priv->tx_ring[i].bytes = 0;
-		priv->tx_ring[i].packets = 0;
+		priv->tx_ring[i]->bytes = 0;
+		priv->tx_ring[i]->packets = 0;
 	}
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		priv->rx_ring[i].bytes = 0;
-		priv->rx_ring[i].packets = 0;
+		priv->rx_ring[i]->bytes = 0;
+		priv->rx_ring[i]->packets = 0;
 	}
 
-	mlx4_en_set_default_moderation(priv);
+	if (!priv->stored_mparams)
+		mlx4_en_set_default_moderation(priv);
 	err = mlx4_en_start_port(dev);
 	if (err)
 		en_err(priv, "Failed starting port:%d\n", priv->port);
@@ -811,61 +1076,197 @@ static int mlx4_en_close(struct net_devi
 	mutex_lock(&mdev->state_lock);
 
 	mlx4_en_stop_port(dev);
-	netif_carrier_off(dev);
 
 	mutex_unlock(&mdev->state_lock);
 	return 0;
 }
 
+static void mlx4_en_free_numa(struct mlx4_en_priv *priv)
+{
+	int i;
+
+	/* free tx rings */
+	for (i = 0; i < MAX_TX_RINGS; i++) {
+		if (priv->tx_ring[i])
+			kfree(priv->tx_ring[i]);
+		if (priv->tx_cq[i])
+			kfree(priv->tx_cq[i]);
+	}
+
+	/* free rx rings */
+	for (i = 0; i < MAX_RX_RINGS; i++) {
+		if (priv->rx_ring[i])
+			kfree(priv->rx_ring[i]);
+		if (priv->rx_cq[i])
+			kfree(priv->rx_cq[i]);
+	}
+}
+
+static int mlx4_en_alloc_numa(struct mlx4_en_priv *priv)
+{
+	int i;
+	int numa_node;
+	int cpu_num = num_online_cpus();
+	int this_cpu = numa_node_id();
+
+	/*
+	 * Numa allocation, each ring and cq goes to same numa node
+	 * Each ring and cq saves its numa node
+	 * Upon failure, attempt regular allocation
+	 */
+
+
+	/* allocate tx rings */
+	for (i = 0; i < MAX_TX_RINGS; i++) {
+		numa_node = cpu_to_node(i % cpu_num);
+
+		priv->tx_ring[i] = kzalloc_node(sizeof(struct mlx4_en_tx_ring),
+						GFP_KERNEL, numa_node);
+
+		if (priv->tx_ring[i])
+			priv->tx_ring[i]->numa_node = numa_node;
+		else {
+			priv->tx_ring[i] =
+				kzalloc(sizeof(struct mlx4_en_tx_ring),
+					GFP_KERNEL);
+
+			if (!priv->tx_ring[i])
+				goto err;
+			priv->tx_ring[i]->numa_node = this_cpu;
+		}
+
+		priv->tx_cq[i] = kzalloc_node(sizeof(struct mlx4_en_cq),
+						GFP_KERNEL, numa_node);
+		if (priv->tx_cq[i])
+			priv->tx_cq[i]->numa_node = numa_node;
+		else {
+			priv->tx_cq[i] = kzalloc(sizeof(struct mlx4_en_cq),
+						GFP_KERNEL);
+
+			if (!priv->tx_cq[i])
+				goto err;
+			priv->tx_cq[i]->numa_node = this_cpu;
+		}
+	}
+
+	/* allocate rx rings */
+	for (i = 0; i < MAX_RX_RINGS; i++) {
+		numa_node = cpu_to_node(i % cpu_num);
+
+		priv->rx_ring[i] = kzalloc_node(sizeof(struct mlx4_en_rx_ring),
+						GFP_KERNEL, numa_node);
+
+		if (priv->rx_ring[i])
+			priv->rx_ring[i]->numa_node = numa_node;
+		else {
+			priv->rx_ring[i] =
+				kzalloc(sizeof(struct mlx4_en_rx_ring),
+					GFP_KERNEL);
+
+			if (!priv->rx_ring[i])
+				goto err;
+			priv->rx_ring[i]->numa_node = this_cpu;
+		}
+
+		priv->rx_cq[i] = kzalloc_node(sizeof(struct mlx4_en_cq),
+						GFP_KERNEL, numa_node);
+
+		if (priv->rx_cq[i])
+			priv->rx_cq[i]->numa_node = numa_node;
+		else {
+			priv->rx_cq[i] = kzalloc(sizeof(struct mlx4_en_cq),
+						GFP_KERNEL);
+
+			if (!priv->rx_cq[i])
+				goto err;
+			priv->rx_cq[i]->numa_node = this_cpu;
+		}
+	}
+
+	return 0;
+
+err:
+	mlx4_en_free_numa(priv);
+	return -ENOMEM;
+}
+
 void mlx4_en_free_resources(struct mlx4_en_priv *priv)
 {
 	int i;
+	int base_tx_qpn;
+
+	if (!priv->resources_allocated)
+		return;
+
+	/* base QP number is ring 0 qpn */
+	base_tx_qpn = priv->tx_ring[0]->qpn;
 
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		if (priv->tx_ring[i].tx_info)
-			mlx4_en_destroy_tx_ring(priv, &priv->tx_ring[i]);
-		if (priv->tx_cq[i].buf)
-			mlx4_en_destroy_cq(priv, &priv->tx_cq[i]);
+		if (priv->tx_ring[i]->tx_info)
+			mlx4_en_destroy_tx_ring(priv, priv->tx_ring[i]);
+		if (priv->tx_cq[i]->buf)
+			mlx4_en_destroy_cq(priv, priv->tx_cq[i]);
 	}
 
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		if (priv->rx_ring[i].rx_info)
-			mlx4_en_destroy_rx_ring(priv, &priv->rx_ring[i]);
-		if (priv->rx_cq[i].buf)
-			mlx4_en_destroy_cq(priv, &priv->rx_cq[i]);
+		if (priv->rx_ring[i]->rx_info)
+			mlx4_en_destroy_rx_ring(priv, priv->rx_ring[i]);
+		if (priv->rx_cq[i]->buf)
+			mlx4_en_destroy_cq(priv, priv->rx_cq[i]);
 	}
+
+	mlx4_en_free_numa(priv);
+	mlx4_qp_release_range(priv->mdev->dev, base_tx_qpn, priv->tx_ring_num);
+	priv->resources_allocated = false;
 }
 
 int mlx4_en_alloc_resources(struct mlx4_en_priv *priv)
 {
 	struct mlx4_en_port_profile *prof = priv->prof;
 	int i;
+	int base_tx_qpn, err;
+
+	err = mlx4_qp_reserve_range(priv->mdev->dev, priv->tx_ring_num, 256, &base_tx_qpn, 0);
+	if (err) {
+		en_err(priv, "failed reserving range for TX rings\n");
+		return err;
+	}
+
+	err = mlx4_en_alloc_numa(priv);
+	if (err) {
+		en_err(priv, "failed to allocate rings and cqs\n");
+		goto err_numa;
+	}
 
 	/* Create tx Rings */
 	for (i = 0; i < priv->tx_ring_num; i++) {
-		if (mlx4_en_create_cq(priv, &priv->tx_cq[i],
+		if (mlx4_en_create_cq(priv, priv->tx_cq[i],
 				      prof->tx_ring_size, i, TX))
 			goto err;
 
-		if (mlx4_en_create_tx_ring(priv, &priv->tx_ring[i],
-					   prof->tx_ring_size, TXBB_SIZE))
+		if (mlx4_en_create_tx_ring(priv, priv->tx_ring[i],
+			base_tx_qpn + i, prof->tx_ring_size, TXBB_SIZE))
 			goto err;
 	}
 
 	/* Create rx Rings */
 	for (i = 0; i < priv->rx_ring_num; i++) {
-		if (mlx4_en_create_cq(priv, &priv->rx_cq[i],
+		if (mlx4_en_create_cq(priv, priv->rx_cq[i],
 				      prof->rx_ring_size, i, RX))
 			goto err;
 
-		if (mlx4_en_create_rx_ring(priv, &priv->rx_ring[i],
-					   prof->rx_ring_size, priv->stride))
+		if (mlx4_en_create_rx_ring(priv, priv->rx_ring[i],
+					   prof->rx_ring_size))
 			goto err;
 	}
 
+	priv->resources_allocated = true;
 	return 0;
 
 err:
+	mlx4_en_free_numa(priv);
+err_numa:
+	mlx4_qp_release_range(priv->mdev->dev,base_tx_qpn, priv->tx_ring_num);
 	en_err(priv, "Failed to allocate NIC resources\n");
 	return -ENOMEM;
 }
@@ -879,8 +1280,13 @@ void mlx4_en_destroy_netdev(struct net_d
 	en_dbg(DRV, priv, "Destroying netdev on port:%d\n", priv->port);
 
 	/* Unregister device - this will close the port if it was up */
-	if (priv->registered)
+	if (priv->registered) {
+		if (dev->master) {
+			netif_carrier_on(dev);
+			msleep(100);
+		}
 		unregister_netdev(dev);
+	}
 
 	if (priv->allocated)
 		mlx4_free_hwq_res(mdev->dev, &priv->res, MLX4_EN_PAGE_SIZE);
@@ -889,6 +1295,10 @@ void mlx4_en_destroy_netdev(struct net_d
 	/* flush any pending task for this netdev */
 	flush_workqueue(mdev->workqueue);
 
+	/* close port*/
+	mlx4_CLOSE_PORT(mdev->dev, priv->port);
+	priv->port_inited = false;
+
 	/* Detach the netdev so tasks would not attempt to access it */
 	mutex_lock(&mdev->state_lock);
 	mdev->pndev[priv->port] = NULL;
@@ -921,7 +1331,6 @@ static int mlx4_en_change_mtu(struct net
 			en_dbg(DRV, priv, "Change MTU called with card down!?\n");
 		} else {
 			mlx4_en_stop_port(dev);
-			mlx4_en_set_default_moderation(priv);
 			err = mlx4_en_start_port(dev);
 			if (err) {
 				en_err(priv, "Failed restarting port:%d\n",
@@ -968,6 +1377,7 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	}
 
 	SET_NETDEV_DEV(dev, &mdev->dev->pdev->dev);
+	dev->dev_id =  port - 1;
 
 	/*
 	 * Initialize driver private data
@@ -977,17 +1387,25 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	memset(priv, 0, sizeof(struct mlx4_en_priv));
 	priv->dev = dev;
 	priv->mdev = mdev;
+	priv->ddev = &mdev->pdev->dev;
 	priv->prof = prof;
 	priv->port = port;
 	priv->port_up = false;
+	priv->port_inited = false;
 	priv->rx_csum = 1;
 	priv->flags = prof->flags;
 	priv->tx_ring_num = prof->tx_ring_num;
 	priv->rx_ring_num = prof->rx_ring_num;
+	priv->cqe_factor = (mdev->dev->caps.cqe_size == 64)? 1 : 0;
+	if (prof->rx_ring_num == 1)
+		priv->udp_rings = 0;
+	else
+		priv->udp_rings = mdev->profile.udp_rss ? prof->rx_ring_num / 2 : 1;
 	priv->mc_list = NULL;
 	priv->mac_index = -1;
 	priv->msg_enable = MLX4_EN_MSG_LEVEL;
 	spin_lock_init(&priv->stats_lock);
+	spin_lock_init(&priv->vlan_lock);
 	INIT_WORK(&priv->mcast_task, mlx4_en_do_set_multicast);
 	INIT_WORK(&priv->mac_task, mlx4_en_do_set_mac);
 	INIT_WORK(&priv->watchdog_task, mlx4_en_restart);
@@ -1004,15 +1422,14 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 		goto out;
 	}
 
-	priv->stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +
-					  DS_SIZE * MLX4_EN_MAX_RX_FRAGS);
 	err = mlx4_en_alloc_resources(priv);
 	if (err)
 		goto out;
 
 	/* Allocate page for receive rings */
 	err = mlx4_alloc_hwq_res(mdev->dev, &priv->res,
-				MLX4_EN_PAGE_SIZE, MLX4_EN_PAGE_SIZE);
+				MLX4_EN_PAGE_SIZE, MLX4_EN_PAGE_SIZE,
+				numa_node_id());
 	if (err) {
 		en_err(priv, "Failed to allocate page for rx qps\n");
 		goto out;
@@ -1024,34 +1441,37 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	 */
 	dev->netdev_ops = &mlx4_netdev_ops;
 	dev->watchdog_timeo = MLX4_EN_WATCHDOG_TIMEOUT;
-	dev->real_num_tx_queues = MLX4_EN_NUM_TX_RINGS;
 
 	SET_ETHTOOL_OPS(dev, &mlx4_en_ethtool_ops);
 
 	/* Set defualt MAC */
 	dev->addr_len = ETH_ALEN;
-	for (i = 0; i < ETH_ALEN; i++)
-		dev->dev_addr[ETH_ALEN - 1 - i] =
-		(u8) (priv->mac >> (8 * i));
+	for (i = 0; i < ETH_ALEN; i++) {
+		dev->dev_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));
+		dev->perm_addr[ETH_ALEN - 1 - i] = (u8) (priv->mac >> (8 * i));
+	}
 
 	/*
 	 * Set driver features
 	 */
 	dev->features |= NETIF_F_SG;
+	dev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+#ifdef HAVE_NETDEV_VLAN_FEATURES
 	dev->vlan_features |= NETIF_F_SG;
-	dev->features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
 	dev->vlan_features |= NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;
+#endif
 	dev->features |= NETIF_F_HIGHDMA;
 	dev->features |= NETIF_F_HW_VLAN_TX |
 			 NETIF_F_HW_VLAN_RX |
 			 NETIF_F_HW_VLAN_FILTER;
-	if (mdev->profile.num_lro)
-		dev->features |= NETIF_F_LRO;
+	dev->features |= NETIF_F_GRO;
 	if (mdev->LSO_support) {
 		dev->features |= NETIF_F_TSO;
 		dev->features |= NETIF_F_TSO6;
+#ifdef HAVE_NETDEV_VLAN_FEATURES
 		dev->vlan_features |= NETIF_F_TSO;
 		dev->vlan_features |= NETIF_F_TSO6;
+#endif
 	}
 
 	mdev->pndev[port] = dev;
@@ -1059,14 +1479,44 @@ int mlx4_en_init_netdev(struct mlx4_en_d
 	netif_carrier_off(dev);
 	err = register_netdev(dev);
 	if (err) {
-		en_err(priv, "Netdev registration failed for port %d\n", port);
+		mlx4_err(mdev, "Netdev registration failed for port %d\n", port);
 		goto out;
 	}
+	priv->registered = 1;
 
 	en_warn(priv, "Using %d TX rings\n", prof->tx_ring_num);
 	en_warn(priv, "Using %d RX rings\n", prof->rx_ring_num);
 
-	priv->registered = 1;
+
+	/* Configure port */
+	err = mlx4_SET_PORT_general(mdev->mlx4_intf, mdev->dev, priv->port,
+				    dev->mtu, &prof->tx_pause, &prof->rx_pause);
+	if (err) {
+		en_err(priv, "Failed setting port general configurations "
+		       "for port %d, with error %d\n", priv->port, err);
+		goto out;
+	}
+
+	/* Init port */
+	en_warn(priv, "Initializing port\n");
+	if (!priv->port_inited) {
+		err = mlx4_INIT_PORT(mdev->dev, priv->port);
+		if (err) {
+			en_err(priv, "Failed Initializing port\n");
+			goto out;
+		}
+		priv->port_inited = true;
+	}
+
+	if (!netif_carrier_ok(dev)) {
+		if (!mlx4_en_QUERY_PORT(mdev, priv->port)) {
+			if (priv->port_state.link_state) {
+				priv->last_link_state = MLX4_DEV_EVENT_PORT_UP;
+				en_info(priv, "Link Up\n");
+				netif_carrier_on(dev);
+			}
+		}
+	}
 	queue_delayed_work(mdev->workqueue, &priv->stats_task, STATS_DELAY);
 	return 0;
 
diff -r c23d1fc7e422 drivers/net/mlx4/en_port.c
--- a/drivers/net/mlx4/en_port.c
+++ b/drivers/net/mlx4/en_port.c
@@ -41,199 +41,45 @@
 #include "mlx4_en.h"
 
 
-int mlx4_SET_MCAST_FLTR(struct mlx4_dev *dev, u8 port,
-			u64 mac, u64 clear, u8 mode)
+int mlx4_en_QUERY_PORT(struct mlx4_en_dev *mdev, u8 port)
 {
-	return mlx4_cmd(dev, (mac | (clear << 63)), port, mode,
-			MLX4_CMD_SET_MCAST_FLTR, MLX4_CMD_TIME_CLASS_B);
-}
-
-int mlx4_SET_VLAN_FLTR(struct mlx4_dev *dev, u8 port, struct vlan_group *grp)
-{
+	struct mlx4_en_query_port_context *qport_context;
+	struct mlx4_en_priv *priv = netdev_priv(mdev->pndev[port]);
+	struct mlx4_en_port_state *state = &priv->port_state;
 	struct mlx4_cmd_mailbox *mailbox;
-	struct mlx4_set_vlan_fltr_mbox *filter;
-	int i;
-	int j;
-	int index = 0;
-	u32 entry;
-	int err = 0;
-
-	mailbox = mlx4_alloc_cmd_mailbox(dev);
-	if (IS_ERR(mailbox))
-		return PTR_ERR(mailbox);
-
-	filter = mailbox->buf;
-	if (grp) {
-		memset(filter, 0, sizeof *filter);
-		for (i = VLAN_FLTR_SIZE - 1; i >= 0; i--) {
-			entry = 0;
-			for (j = 0; j < 32; j++)
-				if (vlan_group_get_device(grp, index++))
-					entry |= 1 << j;
-			filter->entry[i] = cpu_to_be32(entry);
-		}
-	} else {
-		/* When no vlans are configured we block all vlans */
-		memset(filter, 0, sizeof(*filter));
-	}
-	err = mlx4_cmd(dev, mailbox->dma, port, 0, MLX4_CMD_SET_VLAN_FLTR,
-		       MLX4_CMD_TIME_CLASS_B);
-	mlx4_free_cmd_mailbox(dev, mailbox);
-	return err;
-}
-
-
-int mlx4_SET_PORT_general(struct mlx4_dev *dev, u8 port, int mtu,
-			  u8 pptx, u8 pfctx, u8 pprx, u8 pfcrx)
-{
-	struct mlx4_cmd_mailbox *mailbox;
-	struct mlx4_set_port_general_context *context;
 	int err;
-	u32 in_mod;
-
-	mailbox = mlx4_alloc_cmd_mailbox(dev);
-	if (IS_ERR(mailbox))
-		return PTR_ERR(mailbox);
-	context = mailbox->buf;
-	memset(context, 0, sizeof *context);
-
-	context->flags = SET_PORT_GEN_ALL_VALID;
-	context->mtu = cpu_to_be16(mtu);
-	context->pptx = (pptx * (!pfctx)) << 7;
-	context->pfctx = pfctx;
-	context->pprx = (pprx * (!pfcrx)) << 7;
-	context->pfcrx = pfcrx;
-
-	in_mod = MLX4_SET_PORT_GENERAL << 8 | port;
-	err = mlx4_cmd(dev, mailbox->dma, in_mod, 1, MLX4_CMD_SET_PORT,
-		       MLX4_CMD_TIME_CLASS_B);
-
-	mlx4_free_cmd_mailbox(dev, mailbox);
-	return err;
-}
-
-int mlx4_SET_PORT_qpn_calc(struct mlx4_dev *dev, u8 port, u32 base_qpn,
-			   u8 promisc)
-{
-	struct mlx4_cmd_mailbox *mailbox;
-	struct mlx4_set_port_rqp_calc_context *context;
-	int err;
-	u32 in_mod;
-
-	mailbox = mlx4_alloc_cmd_mailbox(dev);
-	if (IS_ERR(mailbox))
-		return PTR_ERR(mailbox);
-	context = mailbox->buf;
-	memset(context, 0, sizeof *context);
-
-	context->base_qpn = cpu_to_be32(base_qpn);
-	context->promisc = cpu_to_be32(promisc << SET_PORT_PROMISC_SHIFT | base_qpn);
-	context->mcast = cpu_to_be32(1 << SET_PORT_PROMISC_SHIFT | base_qpn);
-	context->intra_no_vlan = 0;
-	context->no_vlan = MLX4_NO_VLAN_IDX;
-	context->intra_vlan_miss = 0;
-	context->vlan_miss = MLX4_VLAN_MISS_IDX;
-
-	in_mod = MLX4_SET_PORT_RQP_CALC << 8 | port;
-	err = mlx4_cmd(dev, mailbox->dma, in_mod, 1, MLX4_CMD_SET_PORT,
-		       MLX4_CMD_TIME_CLASS_B);
-
-	mlx4_free_cmd_mailbox(dev, mailbox);
-	return err;
-}
-
-
-int mlx4_en_DUMP_ETH_STATS(struct mlx4_en_dev *mdev, u8 port, u8 reset)
-{
-	struct mlx4_en_stat_out_mbox *mlx4_en_stats;
-	struct mlx4_en_priv *priv = netdev_priv(mdev->pndev[port]);
-	struct net_device_stats *stats = &priv->stats;
-	struct mlx4_cmd_mailbox *mailbox;
-	u64 in_mod = reset << 8 | port;
-	int err;
-	int i;
 
 	mailbox = mlx4_alloc_cmd_mailbox(mdev->dev);
 	if (IS_ERR(mailbox))
 		return PTR_ERR(mailbox);
-	memset(mailbox->buf, 0, sizeof(*mlx4_en_stats));
-	err = mlx4_cmd_box(mdev->dev, 0, mailbox->dma, in_mod, 0,
-			   MLX4_CMD_DUMP_ETH_STATS, MLX4_CMD_TIME_CLASS_B);
+	memset(mailbox->buf, 0, sizeof(*qport_context));
+	err = mlx4_QUERY_PORT(mdev->dev, mailbox, port);
 	if (err)
 		goto out;
+	qport_context = mailbox->buf;
 
-	mlx4_en_stats = mailbox->buf;
-
-	spin_lock_bh(&priv->stats_lock);
-
-	stats->rx_packets = 0;
-	stats->rx_bytes = 0;
-	for (i = 0; i < priv->rx_ring_num; i++) {
-		stats->rx_packets += priv->rx_ring[i].packets;
-		stats->rx_bytes += priv->rx_ring[i].bytes;
-	}
-	stats->tx_packets = 0;
-	stats->tx_bytes = 0;
-	for (i = 0; i <= priv->tx_ring_num; i++) {
-		stats->tx_packets += priv->tx_ring[i].packets;
-		stats->tx_bytes += priv->tx_ring[i].bytes;
-	}
-
-	stats->rx_errors = be64_to_cpu(mlx4_en_stats->PCS) +
-			   be32_to_cpu(mlx4_en_stats->RdropLength) +
-			   be32_to_cpu(mlx4_en_stats->RJBBR) +
-			   be32_to_cpu(mlx4_en_stats->RCRC) +
-			   be32_to_cpu(mlx4_en_stats->RRUNT);
-	stats->tx_errors = be32_to_cpu(mlx4_en_stats->TDROP);
-	stats->multicast = be64_to_cpu(mlx4_en_stats->MCAST_prio_0) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_1) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_2) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_3) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_4) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_5) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_6) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_prio_7) +
-			   be64_to_cpu(mlx4_en_stats->MCAST_novlan);
-	stats->collisions = 0;
-	stats->rx_length_errors = be32_to_cpu(mlx4_en_stats->RdropLength);
-	stats->rx_over_errors = be32_to_cpu(mlx4_en_stats->RdropOvflw);
-	stats->rx_crc_errors = be32_to_cpu(mlx4_en_stats->RCRC);
-	stats->rx_frame_errors = 0;
-	stats->rx_fifo_errors = be32_to_cpu(mlx4_en_stats->RdropOvflw);
-	stats->rx_missed_errors = be32_to_cpu(mlx4_en_stats->RdropOvflw);
-	stats->tx_aborted_errors = 0;
-	stats->tx_carrier_errors = 0;
-	stats->tx_fifo_errors = 0;
-	stats->tx_heartbeat_errors = 0;
-	stats->tx_window_errors = 0;
-
-	priv->pkstats.broadcast =
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_0) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_1) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_2) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_3) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_4) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_5) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_6) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_prio_7) +
-				be64_to_cpu(mlx4_en_stats->RBCAST_novlan);
-	priv->pkstats.rx_prio[0] = be64_to_cpu(mlx4_en_stats->RTOT_prio_0);
-	priv->pkstats.rx_prio[1] = be64_to_cpu(mlx4_en_stats->RTOT_prio_1);
-	priv->pkstats.rx_prio[2] = be64_to_cpu(mlx4_en_stats->RTOT_prio_2);
-	priv->pkstats.rx_prio[3] = be64_to_cpu(mlx4_en_stats->RTOT_prio_3);
-	priv->pkstats.rx_prio[4] = be64_to_cpu(mlx4_en_stats->RTOT_prio_4);
-	priv->pkstats.rx_prio[5] = be64_to_cpu(mlx4_en_stats->RTOT_prio_5);
-	priv->pkstats.rx_prio[6] = be64_to_cpu(mlx4_en_stats->RTOT_prio_6);
-	priv->pkstats.rx_prio[7] = be64_to_cpu(mlx4_en_stats->RTOT_prio_7);
-	priv->pkstats.tx_prio[0] = be64_to_cpu(mlx4_en_stats->TTOT_prio_0);
-	priv->pkstats.tx_prio[1] = be64_to_cpu(mlx4_en_stats->TTOT_prio_1);
-	priv->pkstats.tx_prio[2] = be64_to_cpu(mlx4_en_stats->TTOT_prio_2);
-	priv->pkstats.tx_prio[3] = be64_to_cpu(mlx4_en_stats->TTOT_prio_3);
-	priv->pkstats.tx_prio[4] = be64_to_cpu(mlx4_en_stats->TTOT_prio_4);
-	priv->pkstats.tx_prio[5] = be64_to_cpu(mlx4_en_stats->TTOT_prio_5);
-	priv->pkstats.tx_prio[6] = be64_to_cpu(mlx4_en_stats->TTOT_prio_6);
-	priv->pkstats.tx_prio[7] = be64_to_cpu(mlx4_en_stats->TTOT_prio_7);
-	spin_unlock_bh(&priv->stats_lock);
+	/* This command is always accessed from Ethtool context
+	 * already synchronized, no need in locking */
+	state->link_state = !!(qport_context->link_up & MLX4_EN_LINK_UP_MASK);
+	if (qport_context->link_speed != 0xff) {
+		switch (qport_context->link_speed & MLX4_EN_SPEED_MASK) {
+		case MLX4_EN_1G_SPEED:
+			state->link_speed = 1000;
+			break;
+		case MLX4_EN_10G_SPEED_XAUI:
+		case MLX4_EN_10G_SPEED_XFI:
+			state->link_speed = 10000;
+			break;
+		case MLX4_EN_40G_SPEED:
+			state->link_speed = 40000;
+			break;
+		default:
+			state->link_speed = -1;
+			break;
+		}
+	} else
+		state->link_speed = qport_context->actual_speed * 100;
+	state->transciver = qport_context->transceiver;
 
 out:
 	mlx4_free_cmd_mailbox(mdev->dev, mailbox);
diff -r c23d1fc7e422 drivers/net/mlx4/en_port.h
--- a/drivers/net/mlx4/en_port.h
+++ b/drivers/net/mlx4/en_port.h
@@ -35,536 +35,37 @@
 #define _MLX4_EN_PORT_H_
 
 
-#define SET_PORT_GEN_ALL_VALID	0x7
-#define SET_PORT_PROMISC_SHIFT	31
-
-enum {
-	MLX4_CMD_SET_VLAN_FLTR  = 0x47,
-	MLX4_CMD_SET_MCAST_FLTR = 0x48,
-	MLX4_CMD_DUMP_ETH_STATS = 0x49,
-};
-
-struct mlx4_set_port_general_context {
-	u8 reserved[3];
-	u8 flags;
-	u16 reserved2;
-	__be16 mtu;
-	u8 pptx;
-	u8 pfctx;
-	u16 reserved3;
-	u8 pprx;
-	u8 pfcrx;
-	u16 reserved4;
-};
-
-struct mlx4_set_port_rqp_calc_context {
-	__be32 base_qpn;
-	__be32 flags;
-	u8 reserved[3];
-	u8 mac_miss;
-	u8 intra_no_vlan;
-	u8 no_vlan;
-	u8 intra_vlan_miss;
-	u8 vlan_miss;
-	u8 reserved2[3];
-	u8 no_vlan_prio;
-	__be32 promisc;
-	__be32 mcast;
-};
-
 #define VLAN_FLTR_SIZE	128
 struct mlx4_set_vlan_fltr_mbox {
-	__be32 entry[VLAN_FLTR_SIZE];
+    __be32 entry[VLAN_FLTR_SIZE];
 };
 
-
 enum {
 	MLX4_MCAST_CONFIG       = 0,
 	MLX4_MCAST_DISABLE      = 1,
 	MLX4_MCAST_ENABLE       = 2,
 };
 
-
-struct mlx4_en_stat_out_mbox {
-	/* Received frames with a length of 64 octets */
-	__be64 R64_prio_0;
-	__be64 R64_prio_1;
-	__be64 R64_prio_2;
-	__be64 R64_prio_3;
-	__be64 R64_prio_4;
-	__be64 R64_prio_5;
-	__be64 R64_prio_6;
-	__be64 R64_prio_7;
-	__be64 R64_novlan;
-	/* Received frames with a length of 127 octets */
-	__be64 R127_prio_0;
-	__be64 R127_prio_1;
-	__be64 R127_prio_2;
-	__be64 R127_prio_3;
-	__be64 R127_prio_4;
-	__be64 R127_prio_5;
-	__be64 R127_prio_6;
-	__be64 R127_prio_7;
-	__be64 R127_novlan;
-	/* Received frames with a length of 255 octets */
-	__be64 R255_prio_0;
-	__be64 R255_prio_1;
-	__be64 R255_prio_2;
-	__be64 R255_prio_3;
-	__be64 R255_prio_4;
-	__be64 R255_prio_5;
-	__be64 R255_prio_6;
-	__be64 R255_prio_7;
-	__be64 R255_novlan;
-	/* Received frames with a length of 511 octets */
-	__be64 R511_prio_0;
-	__be64 R511_prio_1;
-	__be64 R511_prio_2;
-	__be64 R511_prio_3;
-	__be64 R511_prio_4;
-	__be64 R511_prio_5;
-	__be64 R511_prio_6;
-	__be64 R511_prio_7;
-	__be64 R511_novlan;
-	/* Received frames with a length of 1023 octets */
-	__be64 R1023_prio_0;
-	__be64 R1023_prio_1;
-	__be64 R1023_prio_2;
-	__be64 R1023_prio_3;
-	__be64 R1023_prio_4;
-	__be64 R1023_prio_5;
-	__be64 R1023_prio_6;
-	__be64 R1023_prio_7;
-	__be64 R1023_novlan;
-	/* Received frames with a length of 1518 octets */
-	__be64 R1518_prio_0;
-	__be64 R1518_prio_1;
-	__be64 R1518_prio_2;
-	__be64 R1518_prio_3;
-	__be64 R1518_prio_4;
-	__be64 R1518_prio_5;
-	__be64 R1518_prio_6;
-	__be64 R1518_prio_7;
-	__be64 R1518_novlan;
-	/* Received frames with a length of 1522 octets */
-	__be64 R1522_prio_0;
-	__be64 R1522_prio_1;
-	__be64 R1522_prio_2;
-	__be64 R1522_prio_3;
-	__be64 R1522_prio_4;
-	__be64 R1522_prio_5;
-	__be64 R1522_prio_6;
-	__be64 R1522_prio_7;
-	__be64 R1522_novlan;
-	/* Received frames with a length of 1548 octets */
-	__be64 R1548_prio_0;
-	__be64 R1548_prio_1;
-	__be64 R1548_prio_2;
-	__be64 R1548_prio_3;
-	__be64 R1548_prio_4;
-	__be64 R1548_prio_5;
-	__be64 R1548_prio_6;
-	__be64 R1548_prio_7;
-	__be64 R1548_novlan;
-	/* Received frames with a length of 1548 < octets < MTU */
-	__be64 R2MTU_prio_0;
-	__be64 R2MTU_prio_1;
-	__be64 R2MTU_prio_2;
-	__be64 R2MTU_prio_3;
-	__be64 R2MTU_prio_4;
-	__be64 R2MTU_prio_5;
-	__be64 R2MTU_prio_6;
-	__be64 R2MTU_prio_7;
-	__be64 R2MTU_novlan;
-	/* Received frames with a length of MTU< octets and good CRC */
-	__be64 RGIANT_prio_0;
-	__be64 RGIANT_prio_1;
-	__be64 RGIANT_prio_2;
-	__be64 RGIANT_prio_3;
-	__be64 RGIANT_prio_4;
-	__be64 RGIANT_prio_5;
-	__be64 RGIANT_prio_6;
-	__be64 RGIANT_prio_7;
-	__be64 RGIANT_novlan;
-	/* Received broadcast frames with good CRC */
-	__be64 RBCAST_prio_0;
-	__be64 RBCAST_prio_1;
-	__be64 RBCAST_prio_2;
-	__be64 RBCAST_prio_3;
-	__be64 RBCAST_prio_4;
-	__be64 RBCAST_prio_5;
-	__be64 RBCAST_prio_6;
-	__be64 RBCAST_prio_7;
-	__be64 RBCAST_novlan;
-	/* Received multicast frames with good CRC */
-	__be64 MCAST_prio_0;
-	__be64 MCAST_prio_1;
-	__be64 MCAST_prio_2;
-	__be64 MCAST_prio_3;
-	__be64 MCAST_prio_4;
-	__be64 MCAST_prio_5;
-	__be64 MCAST_prio_6;
-	__be64 MCAST_prio_7;
-	__be64 MCAST_novlan;
-	/* Received unicast not short or GIANT frames with good CRC */
-	__be64 RTOTG_prio_0;
-	__be64 RTOTG_prio_1;
-	__be64 RTOTG_prio_2;
-	__be64 RTOTG_prio_3;
-	__be64 RTOTG_prio_4;
-	__be64 RTOTG_prio_5;
-	__be64 RTOTG_prio_6;
-	__be64 RTOTG_prio_7;
-	__be64 RTOTG_novlan;
-
-	/* Count of total octets of received frames, includes framing characters */
-	__be64 RTTLOCT_prio_0;
-	/* Count of total octets of received frames, not including framing
-	   characters */
-	__be64 RTTLOCT_NOFRM_prio_0;
-	/* Count of Total number of octets received
-	   (only for frames without errors) */
-	__be64 ROCT_prio_0;
-
-	__be64 RTTLOCT_prio_1;
-	__be64 RTTLOCT_NOFRM_prio_1;
-	__be64 ROCT_prio_1;
-
-	__be64 RTTLOCT_prio_2;
-	__be64 RTTLOCT_NOFRM_prio_2;
-	__be64 ROCT_prio_2;
-
-	__be64 RTTLOCT_prio_3;
-	__be64 RTTLOCT_NOFRM_prio_3;
-	__be64 ROCT_prio_3;
-
-	__be64 RTTLOCT_prio_4;
-	__be64 RTTLOCT_NOFRM_prio_4;
-	__be64 ROCT_prio_4;
-
-	__be64 RTTLOCT_prio_5;
-	__be64 RTTLOCT_NOFRM_prio_5;
-	__be64 ROCT_prio_5;
-
-	__be64 RTTLOCT_prio_6;
-	__be64 RTTLOCT_NOFRM_prio_6;
-	__be64 ROCT_prio_6;
-
-	__be64 RTTLOCT_prio_7;
-	__be64 RTTLOCT_NOFRM_prio_7;
-	__be64 ROCT_prio_7;
-
-	__be64 RTTLOCT_novlan;
-	__be64 RTTLOCT_NOFRM_novlan;
-	__be64 ROCT_novlan;
-
-	/* Count of Total received frames including bad frames */
-	__be64 RTOT_prio_0;
-	/* Count of  Total number of received frames with 802.1Q encapsulation */
-	__be64 R1Q_prio_0;
-	__be64 reserved1;
-
-	__be64 RTOT_prio_1;
-	__be64 R1Q_prio_1;
-	__be64 reserved2;
-
-	__be64 RTOT_prio_2;
-	__be64 R1Q_prio_2;
-	__be64 reserved3;
-
-	__be64 RTOT_prio_3;
-	__be64 R1Q_prio_3;
-	__be64 reserved4;
-
-	__be64 RTOT_prio_4;
-	__be64 R1Q_prio_4;
-	__be64 reserved5;
-
-	__be64 RTOT_prio_5;
-	__be64 R1Q_prio_5;
-	__be64 reserved6;
-
-	__be64 RTOT_prio_6;
-	__be64 R1Q_prio_6;
-	__be64 reserved7;
-
-	__be64 RTOT_prio_7;
-	__be64 R1Q_prio_7;
-	__be64 reserved8;
-
-	__be64 RTOT_novlan;
-	__be64 R1Q_novlan;
-	__be64 reserved9;
-
-	/* Total number of Successfully Received Control Frames */
-	__be64 RCNTL;
-	__be64 reserved10;
-	__be64 reserved11;
-	__be64 reserved12;
-	/* Count of received frames with a length/type field  value between 46
-	   (42 for VLANtagged frames) and 1500 (also 1500 for VLAN-tagged frames),
-	   inclusive */
-	__be64 RInRangeLengthErr;
-	/* Count of received frames with length/type field between 1501 and 1535
-	   decimal, inclusive */
-	__be64 ROutRangeLengthErr;
-	/* Count of received frames that are longer than max allowed size for
-	   802.3 frames (1518/1522) */
-	__be64 RFrmTooLong;
-	/* Count frames received with PCS error */
-	__be64 PCS;
-
-	/* Transmit frames with a length of 64 octets */
-	__be64 T64_prio_0;
-	__be64 T64_prio_1;
-	__be64 T64_prio_2;
-	__be64 T64_prio_3;
-	__be64 T64_prio_4;
-	__be64 T64_prio_5;
-	__be64 T64_prio_6;
-	__be64 T64_prio_7;
-	__be64 T64_novlan;
-	__be64 T64_loopbk;
-	/* Transmit frames with a length of 65 to 127 octets. */
-	__be64 T127_prio_0;
-	__be64 T127_prio_1;
-	__be64 T127_prio_2;
-	__be64 T127_prio_3;
-	__be64 T127_prio_4;
-	__be64 T127_prio_5;
-	__be64 T127_prio_6;
-	__be64 T127_prio_7;
-	__be64 T127_novlan;
-	__be64 T127_loopbk;
-	/* Transmit frames with a length of 128 to 255 octets */
-	__be64 T255_prio_0;
-	__be64 T255_prio_1;
-	__be64 T255_prio_2;
-	__be64 T255_prio_3;
-	__be64 T255_prio_4;
-	__be64 T255_prio_5;
-	__be64 T255_prio_6;
-	__be64 T255_prio_7;
-	__be64 T255_novlan;
-	__be64 T255_loopbk;
-	/* Transmit frames with a length of 256 to 511 octets */
-	__be64 T511_prio_0;
-	__be64 T511_prio_1;
-	__be64 T511_prio_2;
-	__be64 T511_prio_3;
-	__be64 T511_prio_4;
-	__be64 T511_prio_5;
-	__be64 T511_prio_6;
-	__be64 T511_prio_7;
-	__be64 T511_novlan;
-	__be64 T511_loopbk;
-	/* Transmit frames with a length of 512 to 1023 octets */
-	__be64 T1023_prio_0;
-	__be64 T1023_prio_1;
-	__be64 T1023_prio_2;
-	__be64 T1023_prio_3;
-	__be64 T1023_prio_4;
-	__be64 T1023_prio_5;
-	__be64 T1023_prio_6;
-	__be64 T1023_prio_7;
-	__be64 T1023_novlan;
-	__be64 T1023_loopbk;
-	/* Transmit frames with a length of 1024 to 1518 octets */
-	__be64 T1518_prio_0;
-	__be64 T1518_prio_1;
-	__be64 T1518_prio_2;
-	__be64 T1518_prio_3;
-	__be64 T1518_prio_4;
-	__be64 T1518_prio_5;
-	__be64 T1518_prio_6;
-	__be64 T1518_prio_7;
-	__be64 T1518_novlan;
-	__be64 T1518_loopbk;
-	/* Counts transmit frames with a length of 1519 to 1522 bytes */
-	__be64 T1522_prio_0;
-	__be64 T1522_prio_1;
-	__be64 T1522_prio_2;
-	__be64 T1522_prio_3;
-	__be64 T1522_prio_4;
-	__be64 T1522_prio_5;
-	__be64 T1522_prio_6;
-	__be64 T1522_prio_7;
-	__be64 T1522_novlan;
-	__be64 T1522_loopbk;
-	/* Transmit frames with a length of 1523 to 1548 octets */
-	__be64 T1548_prio_0;
-	__be64 T1548_prio_1;
-	__be64 T1548_prio_2;
-	__be64 T1548_prio_3;
-	__be64 T1548_prio_4;
-	__be64 T1548_prio_5;
-	__be64 T1548_prio_6;
-	__be64 T1548_prio_7;
-	__be64 T1548_novlan;
-	__be64 T1548_loopbk;
-	/* Counts transmit frames with a length of 1549 to MTU bytes */
-	__be64 T2MTU_prio_0;
-	__be64 T2MTU_prio_1;
-	__be64 T2MTU_prio_2;
-	__be64 T2MTU_prio_3;
-	__be64 T2MTU_prio_4;
-	__be64 T2MTU_prio_5;
-	__be64 T2MTU_prio_6;
-	__be64 T2MTU_prio_7;
-	__be64 T2MTU_novlan;
-	__be64 T2MTU_loopbk;
-	/* Transmit frames with a length greater than MTU octets and a good CRC. */
-	__be64 TGIANT_prio_0;
-	__be64 TGIANT_prio_1;
-	__be64 TGIANT_prio_2;
-	__be64 TGIANT_prio_3;
-	__be64 TGIANT_prio_4;
-	__be64 TGIANT_prio_5;
-	__be64 TGIANT_prio_6;
-	__be64 TGIANT_prio_7;
-	__be64 TGIANT_novlan;
-	__be64 TGIANT_loopbk;
-	/* Transmit broadcast frames with a good CRC */
-	__be64 TBCAST_prio_0;
-	__be64 TBCAST_prio_1;
-	__be64 TBCAST_prio_2;
-	__be64 TBCAST_prio_3;
-	__be64 TBCAST_prio_4;
-	__be64 TBCAST_prio_5;
-	__be64 TBCAST_prio_6;
-	__be64 TBCAST_prio_7;
-	__be64 TBCAST_novlan;
-	__be64 TBCAST_loopbk;
-	/* Transmit multicast frames with a good CRC */
-	__be64 TMCAST_prio_0;
-	__be64 TMCAST_prio_1;
-	__be64 TMCAST_prio_2;
-	__be64 TMCAST_prio_3;
-	__be64 TMCAST_prio_4;
-	__be64 TMCAST_prio_5;
-	__be64 TMCAST_prio_6;
-	__be64 TMCAST_prio_7;
-	__be64 TMCAST_novlan;
-	__be64 TMCAST_loopbk;
-	/* Transmit good frames that are neither broadcast nor multicast */
-	__be64 TTOTG_prio_0;
-	__be64 TTOTG_prio_1;
-	__be64 TTOTG_prio_2;
-	__be64 TTOTG_prio_3;
-	__be64 TTOTG_prio_4;
-	__be64 TTOTG_prio_5;
-	__be64 TTOTG_prio_6;
-	__be64 TTOTG_prio_7;
-	__be64 TTOTG_novlan;
-	__be64 TTOTG_loopbk;
-
-	/* total octets of transmitted frames, including framing characters */
-	__be64 TTTLOCT_prio_0;
-	/* total octets of transmitted frames, not including framing characters */
-	__be64 TTTLOCT_NOFRM_prio_0;
-	/* ifOutOctets */
-	__be64 TOCT_prio_0;
-
-	__be64 TTTLOCT_prio_1;
-	__be64 TTTLOCT_NOFRM_prio_1;
-	__be64 TOCT_prio_1;
-
-	__be64 TTTLOCT_prio_2;
-	__be64 TTTLOCT_NOFRM_prio_2;
-	__be64 TOCT_prio_2;
-
-	__be64 TTTLOCT_prio_3;
-	__be64 TTTLOCT_NOFRM_prio_3;
-	__be64 TOCT_prio_3;
-
-	__be64 TTTLOCT_prio_4;
-	__be64 TTTLOCT_NOFRM_prio_4;
-	__be64 TOCT_prio_4;
-
-	__be64 TTTLOCT_prio_5;
-	__be64 TTTLOCT_NOFRM_prio_5;
-	__be64 TOCT_prio_5;
-
-	__be64 TTTLOCT_prio_6;
-	__be64 TTTLOCT_NOFRM_prio_6;
-	__be64 TOCT_prio_6;
-
-	__be64 TTTLOCT_prio_7;
-	__be64 TTTLOCT_NOFRM_prio_7;
-	__be64 TOCT_prio_7;
-
-	__be64 TTTLOCT_novlan;
-	__be64 TTTLOCT_NOFRM_novlan;
-	__be64 TOCT_novlan;
-
-	__be64 TTTLOCT_loopbk;
-	__be64 TTTLOCT_NOFRM_loopbk;
-	__be64 TOCT_loopbk;
-
-	/* Total frames transmitted with a good CRC that are not aborted  */
-	__be64 TTOT_prio_0;
-	/* Total number of frames transmitted with 802.1Q encapsulation */
-	__be64 T1Q_prio_0;
-	__be64 reserved13;
-
-	__be64 TTOT_prio_1;
-	__be64 T1Q_prio_1;
-	__be64 reserved14;
-
-	__be64 TTOT_prio_2;
-	__be64 T1Q_prio_2;
-	__be64 reserved15;
-
-	__be64 TTOT_prio_3;
-	__be64 T1Q_prio_3;
-	__be64 reserved16;
-
-	__be64 TTOT_prio_4;
-	__be64 T1Q_prio_4;
-	__be64 reserved17;
-
-	__be64 TTOT_prio_5;
-	__be64 T1Q_prio_5;
-	__be64 reserved18;
-
-	__be64 TTOT_prio_6;
-	__be64 T1Q_prio_6;
-	__be64 reserved19;
-
-	__be64 TTOT_prio_7;
-	__be64 T1Q_prio_7;
-	__be64 reserved20;
-
-	__be64 TTOT_novlan;
-	__be64 T1Q_novlan;
-	__be64 reserved21;
-
-	__be64 TTOT_loopbk;
-	__be64 T1Q_loopbk;
-	__be64 reserved22;
-
-	/* Received frames with a length greater than MTU octets and a bad CRC */
-	__be32 RJBBR;
-	/* Received frames with a bad CRC that are not runts, jabbers,
-	   or alignment errors */
-	__be32 RCRC;
-	/* Received frames with SFD with a length of less than 64 octets and a
-	   bad CRC */
-	__be32 RRUNT;
-	/* Received frames with a length less than 64 octets and a good CRC */
-	__be32 RSHORT;
-	/* Total Number of Received Packets Dropped */
-	__be32 RDROP;
-	/* Drop due to overflow  */
-	__be32 RdropOvflw;
-	/* Drop due to overflow */
-	__be32 RdropLength;
-	/* Total of good frames. Does not include frames received with
-	   frame-too-long, FCS, or length errors */
-	__be32 RTOTFRMS;
-	/* Total dropped Xmited packets */
-	__be32 TDROP;
+enum {
+	MLX4_EN_1G_SPEED	= 0x02,
+	MLX4_EN_10G_SPEED_XFI	= 0x01,
+	MLX4_EN_10G_SPEED_XAUI	= 0x00,
+	MLX4_EN_40G_SPEED	= 0x40,
+	MLX4_EN_OTHER_SPEED	= 0x0f,
 };
 
+struct mlx4_en_query_port_context {
+	u8 link_up;
+#define MLX4_EN_LINK_UP_MASK	0x80
+	u8 reserved;
+	__be16 mtu;
+	u8 reserved2;
+	u8 link_speed;
+#define MLX4_EN_SPEED_MASK	0x43
+	u16 reserved3[5];
+	__be64 mac;
+	u8 transceiver;
+	u8 actual_speed;
+};
 
 #endif
diff -r c23d1fc7e422 drivers/net/mlx4/en_resources.c
--- a/drivers/net/mlx4/en_resources.c
+++ b/drivers/net/mlx4/en_resources.c
@@ -43,11 +43,12 @@ void mlx4_en_fill_qp_context(struct mlx4
 	struct mlx4_en_dev *mdev = priv->mdev;
 
 	memset(context, 0, sizeof *context);
-	context->flags = cpu_to_be32(7 << 16 | rss << 13);
+	context->flags = cpu_to_be32(7 << 16 | rss << MLX4_RSS_QPC_FLAG_OFFSET);
 	context->pd = cpu_to_be32(mdev->priv_pdn);
 	context->mtu_msgmax = 0xff;
-	if (!is_tx && !rss)
+	if (!is_tx && !rss) {
 		context->rq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);
+	}
 	if (is_tx)
 		context->sq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);
 	else
@@ -63,7 +64,7 @@ void mlx4_en_fill_qp_context(struct mlx4
 }
 
 
-int mlx4_en_map_buffer(struct mlx4_buf *buf)
+int mlx4_en_map_buffer(struct mlx4_buf *buf, int numa_node)
 {
 	struct page **pages;
 	int i;
@@ -71,7 +72,11 @@ int mlx4_en_map_buffer(struct mlx4_buf *
 	if (BITS_PER_LONG == 64 || buf->nbufs == 1)
 		return 0;
 
-	pages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);
+	pages = kmalloc_node(sizeof *pages * buf->nbufs, GFP_KERNEL, numa_node);
+
+	if (!pages)
+		pages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);
+
 	if (!pages)
 		return -ENOMEM;
 
diff -r c23d1fc7e422 drivers/net/mlx4/en_rx.c
--- a/drivers/net/mlx4/en_rx.c
+++ b/drivers/net/mlx4/en_rx.c
@@ -40,18 +40,9 @@
 
 #include "mlx4_en.h"
 
-
-static int mlx4_en_get_frag_header(struct skb_frag_struct *frags, void **mac_hdr,
-				   void **ip_hdr, void **tcpudp_hdr,
-				   u64 *hdr_flags, void *priv)
-{
-	*mac_hdr = page_address(frags->page) + frags->page_offset;
-	*ip_hdr = *mac_hdr + ETH_HLEN;
-	*tcpudp_hdr = (struct tcphdr *)(*ip_hdr + sizeof(struct iphdr));
-	*hdr_flags = LRO_IPV4 | LRO_TCP;
-
-	return 0;
-}
+enum {
+	MIN_RX_ARM = 2048,
+};
 
 static int mlx4_en_alloc_frag(struct mlx4_en_priv *priv,
 			      struct mlx4_en_rx_desc *rx_desc,
@@ -59,7 +50,6 @@ static int mlx4_en_alloc_frag(struct mlx
 			      struct mlx4_en_rx_alloc *ring_alloc,
 			      int i)
 {
-	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_frag_info *frag_info = &priv->frag_info[i];
 	struct mlx4_en_rx_alloc *page_alloc = &ring_alloc[i];
 	struct page *page;
@@ -83,105 +73,57 @@ static int mlx4_en_alloc_frag(struct mlx
 		skb_frags[i].page_offset = page_alloc->offset;
 		page_alloc->offset += frag_info->frag_stride;
 	}
-	dma = pci_map_single(mdev->pdev, page_address(skb_frags[i].page) +
+	dma = dma_map_single(priv->ddev, page_address(skb_frags[i].page) +
 			     skb_frags[i].page_offset, frag_info->frag_size,
 			     PCI_DMA_FROMDEVICE);
 	rx_desc->data[i].addr = cpu_to_be64(dma);
 	return 0;
 }
 
-static int mlx4_en_init_allocator(struct mlx4_en_priv *priv,
-				  struct mlx4_en_rx_ring *ring)
+static void
+mlx4_en_init_rx_desc_skb(struct mlx4_en_priv *priv,
+			 struct mlx4_en_rx_ring *ring, int index)
 {
-	struct mlx4_en_rx_alloc *page_alloc;
-	int i;
+	struct mlx4_en_rx_desc *rx_desc = ring->buf + ring->stride * index;
 
-	for (i = 0; i < priv->num_frags; i++) {
-		page_alloc = &ring->page_alloc[i];
-		page_alloc->page = alloc_pages(GFP_ATOMIC | __GFP_COMP,
-					       MLX4_EN_ALLOC_ORDER);
-		if (!page_alloc->page)
-			goto out;
-
-		page_alloc->offset = priv->frag_info[i].frag_align;
-		en_dbg(DRV, priv, "Initialized allocator:%d with page:%p\n",
-		       i, page_alloc->page);
-	}
-	return 0;
-
-out:
-	while (i--) {
-		page_alloc = &ring->page_alloc[i];
-		put_page(page_alloc->page);
-		page_alloc->page = NULL;
-	}
-	return -ENOMEM;
+	rx_desc->data->byte_count = cpu_to_be32(priv->rx_skb_size);
+	rx_desc->data->lkey = cpu_to_be32(priv->mdev->mr.key);
 }
 
-static void mlx4_en_destroy_allocator(struct mlx4_en_priv *priv,
-				      struct mlx4_en_rx_ring *ring)
+static int
+mlx4_en_alloc_rx_skb(struct mlx4_en_priv *priv,
+		     struct mlx4_en_rx_desc *rx_desc,
+		     struct sk_buff **pskb, int unmap)
 {
-	struct mlx4_en_rx_alloc *page_alloc;
-	int i;
+	dma_addr_t dma;
+	int size = priv->rx_skb_size;
+	/* Implement netdev_alloc_skb_ip_align here */
+	struct sk_buff *new_skb = __netdev_alloc_skb(priv->dev, size + NET_IP_ALIGN, GFP_ATOMIC);
 
-	for (i = 0; i < priv->num_frags; i++) {
-		page_alloc = &ring->page_alloc[i];
-		en_dbg(DRV, priv, "Freeing allocator:%d count:%d\n",
-		       i, page_count(page_alloc->page));
+	if (NET_IP_ALIGN && new_skb)
+		skb_reserve(new_skb, NET_IP_ALIGN);
 
-		put_page(page_alloc->page);
-		page_alloc->page = NULL;
-	}
+	if (unlikely(new_skb == NULL))
+		return -ENOMEM;
+
+	if (unmap)
+		dma_unmap_single(priv->ddev, be64_to_cpu(rx_desc->data->addr),
+				 be32_to_cpu(rx_desc->data->byte_count),
+				 PCI_DMA_FROMDEVICE);
+	dma = dma_map_single(priv->ddev, new_skb->data, size, DMA_FROM_DEVICE);
+	*pskb = new_skb;
+	rx_desc->data->addr = cpu_to_be64(dma);
+	return 0;
 }
 
-
-static void mlx4_en_init_rx_desc(struct mlx4_en_priv *priv,
-				 struct mlx4_en_rx_ring *ring, int index)
-{
-	struct mlx4_en_rx_desc *rx_desc = ring->buf + ring->stride * index;
-	struct skb_frag_struct *skb_frags = ring->rx_info +
-					    (index << priv->log_rx_info);
-	int possible_frags;
-	int i;
-
-	/* Set size and memtype fields */
-	for (i = 0; i < priv->num_frags; i++) {
-		skb_frags[i].size = priv->frag_info[i].frag_size;
-		rx_desc->data[i].byte_count =
-			cpu_to_be32(priv->frag_info[i].frag_size);
-		rx_desc->data[i].lkey = cpu_to_be32(priv->mdev->mr.key);
-	}
-
-	/* If the number of used fragments does not fill up the ring stride,
-	 * remaining (unused) fragments must be padded with null address/size
-	 * and a special memory key */
-	possible_frags = (ring->stride - sizeof(struct mlx4_en_rx_desc)) / DS_SIZE;
-	for (i = priv->num_frags; i < possible_frags; i++) {
-		rx_desc->data[i].byte_count = 0;
-		rx_desc->data[i].lkey = cpu_to_be32(MLX4_EN_MEMTYPE_PAD);
-		rx_desc->data[i].addr = 0;
-	}
-}
-
-
-static int mlx4_en_prepare_rx_desc(struct mlx4_en_priv *priv,
-				   struct mlx4_en_rx_ring *ring, int index)
+static int
+mlx4_en_prepare_rx_desc_skb(struct mlx4_en_priv *priv,
+			    struct mlx4_en_rx_ring *ring, int index)
 {
 	struct mlx4_en_rx_desc *rx_desc = ring->buf + (index * ring->stride);
-	struct skb_frag_struct *skb_frags = ring->rx_info +
-					    (index << priv->log_rx_info);
-	int i;
+	struct sk_buff **pskb = (struct sk_buff **) ring->rx_info + index;
 
-	for (i = 0; i < priv->num_frags; i++)
-		if (mlx4_en_alloc_frag(priv, rx_desc, skb_frags, ring->page_alloc, i))
-			goto err;
-
-	return 0;
-
-err:
-	while (i--)
-		put_page(skb_frags[i].page);
-	return -ENOMEM;
+	return mlx4_en_alloc_rx_skb(priv, rx_desc, pskb, 0);
 }
 
 static inline void mlx4_en_update_rx_prod_db(struct mlx4_en_rx_ring *ring)
@@ -193,22 +135,17 @@ static void mlx4_en_free_rx_desc(struct 
 				 struct mlx4_en_rx_ring *ring,
 				 int index)
 {
-	struct mlx4_en_dev *mdev = priv->mdev;
-	struct skb_frag_struct *skb_frags;
+	struct sk_buff *skb;
 	struct mlx4_en_rx_desc *rx_desc = ring->buf + (index << ring->log_stride);
 	dma_addr_t dma;
-	int nr;
 
-	skb_frags = ring->rx_info + (index << priv->log_rx_info);
-	for (nr = 0; nr < priv->num_frags; nr++) {
-		en_dbg(DRV, priv, "Freeing fragment:%d\n", nr);
-		dma = be64_to_cpu(rx_desc->data[nr].addr);
 
-		en_dbg(DRV, priv, "Unmaping buffer at dma:0x%llx\n", (u64) dma);
-		pci_unmap_single(mdev->pdev, dma, skb_frags[nr].size,
-				 PCI_DMA_FROMDEVICE);
-		put_page(skb_frags[nr].page);
-	}
+	skb = *((struct sk_buff **) ring->rx_info + index);
+	dma = be64_to_cpu(rx_desc->data->addr);
+	dma_unmap_single(priv->ddev, dma,
+			 priv->rx_skb_size + NET_IP_ALIGN,
+			 PCI_DMA_FROMDEVICE);
+	kfree_skb(skb);
 }
 
 static int mlx4_en_fill_rx_buffers(struct mlx4_en_priv *priv)
@@ -217,13 +154,15 @@ static int mlx4_en_fill_rx_buffers(struc
 	int ring_ind;
 	int buf_ind;
 	int new_size;
+	int err;
 
 	for (buf_ind = 0; buf_ind < priv->prof->rx_ring_size; buf_ind++) {
 		for (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {
-			ring = &priv->rx_ring[ring_ind];
+			ring = priv->rx_ring[ring_ind];
 
-			if (mlx4_en_prepare_rx_desc(priv, ring,
-						    ring->actual_size)) {
+			err = mlx4_en_prepare_rx_desc_skb(priv, ring,
+							  ring->actual_size);
+			if (err) {
 				if (ring->actual_size < MLX4_EN_MIN_RX_SIZE) {
 					en_err(priv, "Failed to allocate "
 						     "enough rx buffers\n");
@@ -231,7 +170,7 @@ static int mlx4_en_fill_rx_buffers(struc
 				} else {
 					new_size = rounddown_pow_of_two(ring->actual_size);
 					en_warn(priv, "Only %d buffers allocated "
-						      "reducing ring size to %d",
+						      "reducing ring size to %d\n",
 						ring->actual_size, new_size);
 					goto reduce_rings;
 				}
@@ -244,13 +183,12 @@ static int mlx4_en_fill_rx_buffers(struc
 
 reduce_rings:
 	for (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {
-		ring = &priv->rx_ring[ring_ind];
+		ring = priv->rx_ring[ring_ind];
 		while (ring->actual_size > new_size) {
 			ring->actual_size--;
 			ring->prod--;
 			mlx4_en_free_rx_desc(priv, ring, ring->actual_size);
 		}
-		ring->size_mask = ring->actual_size - 1;
 	}
 
 	return 0;
@@ -274,25 +212,26 @@ static void mlx4_en_free_rx_buf(struct m
 	}
 }
 
+
 int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
-			   struct mlx4_en_rx_ring *ring, u32 size, u16 stride)
+			   struct mlx4_en_rx_ring *ring, u32 size)
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
 	int err;
 	int tmp;
 
-
 	ring->prod = 0;
 	ring->cons = 0;
 	ring->size = size;
 	ring->size_mask = size - 1;
-	ring->stride = stride;
+	ring->stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +
+					  DS_SIZE);
 	ring->log_stride = ffs(ring->stride) - 1;
 	ring->buf_size = ring->size * ring->stride + TXBB_SIZE;
 
-	tmp = size * roundup_pow_of_two(MLX4_EN_MAX_RX_FRAGS *
-					sizeof(struct skb_frag_struct));
-	ring->rx_info = vmalloc(tmp);
+	tmp = size * sizeof(struct sk_buff *);
+
+	ring->rx_info = vmalloc_node(tmp, ring->numa_node);
 	if (!ring->rx_info) {
 		en_err(priv, "Failed allocating rx_info ring\n");
 		return -ENOMEM;
@@ -301,39 +240,19 @@ int mlx4_en_create_rx_ring(struct mlx4_e
 		 ring->rx_info, tmp);
 
 	err = mlx4_alloc_hwq_res(mdev->dev, &ring->wqres,
-				 ring->buf_size, 2 * PAGE_SIZE);
+				ring->buf_size, 2 * PAGE_SIZE, ring->numa_node);
 	if (err)
 		goto err_ring;
 
-	err = mlx4_en_map_buffer(&ring->wqres.buf);
+	err = mlx4_en_map_buffer(&ring->wqres.buf, ring->numa_node);
 	if (err) {
 		en_err(priv, "Failed to map RX buffer\n");
 		goto err_hwq;
 	}
 	ring->buf = ring->wqres.buf.direct.buf;
 
-	/* Configure lro mngr */
-	memset(&ring->lro, 0, sizeof(struct net_lro_mgr));
-	ring->lro.dev = priv->dev;
-	ring->lro.features = LRO_F_NAPI;
-	ring->lro.frag_align_pad = NET_IP_ALIGN;
-	ring->lro.ip_summed = CHECKSUM_UNNECESSARY;
-	ring->lro.ip_summed_aggr = CHECKSUM_UNNECESSARY;
-	ring->lro.max_desc = mdev->profile.num_lro;
-	ring->lro.max_aggr = MAX_SKB_FRAGS;
-	ring->lro.lro_arr = kzalloc(mdev->profile.num_lro *
-				    sizeof(struct net_lro_desc),
-				    GFP_KERNEL);
-	if (!ring->lro.lro_arr) {
-		en_err(priv, "Failed to allocate lro array\n");
-		goto err_map;
-	}
-	ring->lro.get_frag_header = mlx4_en_get_frag_header;
-
 	return 0;
 
-err_map:
-	mlx4_en_unmap_buffer(&ring->wqres.buf);
 err_hwq:
 	mlx4_free_hwq_res(mdev->dev, &ring->wqres, ring->buf_size);
 err_ring:
@@ -348,18 +267,15 @@ int mlx4_en_activate_rx_rings(struct mlx
 	int i;
 	int ring_ind;
 	int err;
-	int stride = roundup_pow_of_two(sizeof(struct mlx4_en_rx_desc) +
-					DS_SIZE * priv->num_frags);
 
 	for (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {
-		ring = &priv->rx_ring[ring_ind];
+		ring = priv->rx_ring[ring_ind];
 
 		ring->prod = 0;
 		ring->cons = 0;
 		ring->actual_size = 0;
-		ring->cqn = priv->rx_cq[ring_ind].mcq.cqn;
+		ring->cqn = priv->rx_cq[ring_ind]->mcq.cqn;
 
-		ring->stride = stride;
 		if (ring->stride <= TXBB_SIZE)
 			ring->buf += TXBB_SIZE;
 
@@ -369,25 +285,18 @@ int mlx4_en_activate_rx_rings(struct mlx
 		memset(ring->buf, 0, ring->buf_size);
 		mlx4_en_update_rx_prod_db(ring);
 
-		/* Initailize all descriptors */
 		for (i = 0; i < ring->size; i++)
-			mlx4_en_init_rx_desc(priv, ring, i);
+			mlx4_en_init_rx_desc_skb(priv, ring, i);
 
-		/* Initialize page allocators */
-		err = mlx4_en_init_allocator(priv, ring);
-		if (err) {
-			en_err(priv, "Failed initializing ring allocator\n");
-			ring_ind--;
-			goto err_allocator;
-		}
 	}
 	err = mlx4_en_fill_rx_buffers(priv);
 	if (err)
 		goto err_buffers;
 
 	for (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++) {
-		ring = &priv->rx_ring[ring_ind];
+		ring = priv->rx_ring[ring_ind];
 
+		ring->size_mask = ring->actual_size - 1;
 		mlx4_en_update_rx_prod_db(ring);
 	}
 
@@ -395,14 +304,9 @@ int mlx4_en_activate_rx_rings(struct mlx
 
 err_buffers:
 	for (ring_ind = 0; ring_ind < priv->rx_ring_num; ring_ind++)
-		mlx4_en_free_rx_buf(priv, &priv->rx_ring[ring_ind]);
+		mlx4_en_free_rx_buf(priv, priv->rx_ring[ring_ind]);
 
 	ring_ind = priv->rx_ring_num - 1;
-err_allocator:
-	while (ring_ind >= 0) {
-		mlx4_en_destroy_allocator(priv, &priv->rx_ring[ring_ind]);
-		ring_ind--;
-	}
 	return err;
 }
 
@@ -411,7 +315,6 @@ void mlx4_en_destroy_rx_ring(struct mlx4
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
 
-	kfree(ring->lro.lro_arr);
 	mlx4_en_unmap_buffer(&ring->wqres.buf);
 	mlx4_free_hwq_res(mdev->dev, &ring->wqres, ring->buf_size + TXBB_SIZE);
 	vfree(ring->rx_info);
@@ -424,19 +327,17 @@ void mlx4_en_deactivate_rx_ring(struct m
 	mlx4_en_free_rx_buf(priv, ring);
 	if (ring->stride <= TXBB_SIZE)
 		ring->buf -= TXBB_SIZE;
-	mlx4_en_destroy_allocator(priv, ring);
 }
 
 
 /* Unmap a completed descriptor and free unused pages */
-static int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,
-				    struct mlx4_en_rx_desc *rx_desc,
-				    struct skb_frag_struct *skb_frags,
-				    struct skb_frag_struct *skb_frags_rx,
-				    struct mlx4_en_rx_alloc *page_alloc,
-				    int length)
+int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,
+			     struct mlx4_en_rx_desc *rx_desc,
+			     struct skb_frag_struct *skb_frags,
+			     struct skb_frag_struct *skb_frags_rx,
+			     struct mlx4_en_rx_alloc *page_alloc,
+			     int length)
 {
-	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_frag_info *frag_info;
 	int nr;
 	dma_addr_t dma;
@@ -458,13 +359,12 @@ static int mlx4_en_complete_rx_desc(stru
 			goto fail;
 
 		/* Unmap buffer */
-		pci_unmap_single(mdev->pdev, dma, skb_frags[nr].size,
+		dma_unmap_single(priv->ddev, dma, skb_frags_rx[nr].size,
 				 PCI_DMA_FROMDEVICE);
 	}
 	/* Adjust size of last fragment to match actual length */
-	if (nr > 0)
-		skb_frags_rx[nr - 1].size = length -
-			priv->frag_info[nr - 1].frag_prefix_size;
+	skb_frags_rx[nr - 1].size = length -
+		priv->frag_info[nr - 1].frag_prefix_size;
 	return nr;
 
 fail:
@@ -478,25 +378,25 @@ fail:
 }
 
 
-static struct sk_buff *mlx4_en_rx_skb(struct mlx4_en_priv *priv,
-				      struct mlx4_en_rx_desc *rx_desc,
-				      struct skb_frag_struct *skb_frags,
-				      struct mlx4_en_rx_alloc *page_alloc,
-				      unsigned int length)
+struct sk_buff *mlx4_en_rx_skb(struct mlx4_en_priv *priv,
+			       struct mlx4_en_rx_desc *rx_desc,
+			       struct skb_frag_struct *skb_frags,
+			       struct mlx4_en_rx_alloc *page_alloc,
+			       unsigned int length)
 {
-	struct mlx4_en_dev *mdev = priv->mdev;
 	struct sk_buff *skb;
 	void *va;
 	int used_frags;
-	dma_addr_t dma;
 
-	skb = dev_alloc_skb(SMALL_PACKET_SIZE + NET_IP_ALIGN);
-	if (!skb) {
+	skb = __netdev_alloc_skb(priv->dev, SMALL_PACKET_SIZE + NET_IP_ALIGN, GFP_ATOMIC);
+
+	if (NET_IP_ALIGN && skb)
+		skb_reserve(skb, NET_IP_ALIGN);
+
+	if (unlikely(!skb)) {
 		en_dbg(RX_ERR, priv, "Failed allocating skb\n");
 		return NULL;
 	}
-	skb->dev = priv->dev;
-	skb_reserve(skb, NET_IP_ALIGN);
 	skb->len = length;
 	skb->truesize = length + sizeof(struct sk_buff);
 
@@ -505,14 +405,8 @@ static struct sk_buff *mlx4_en_rx_skb(st
 	va = page_address(skb_frags[0].page) + skb_frags[0].page_offset;
 
 	if (length <= SMALL_PACKET_SIZE) {
-		/* We are copying all relevant data to the skb - temporarily
-		 * synch buffers for the copy */
-		dma = be64_to_cpu(rx_desc->data[0].addr);
-		dma_sync_single_range_for_cpu(&mdev->pdev->dev, dma, 0,
-					      length, DMA_FROM_DEVICE);
+		/* We are copying all relevant data to the skb */
 		skb_copy_to_linear_data(skb, va, length);
-		dma_sync_single_range_for_device(&mdev->pdev->dev, dma, 0,
-						 length, DMA_FROM_DEVICE);
 		skb->tail += length;
 	} else {
 
@@ -540,21 +434,94 @@ static struct sk_buff *mlx4_en_rx_skb(st
 	return skb;
 }
 
+static inline int invalid_cqe(struct mlx4_cqe *cqe)
+{
+	/* Drop packet on bad receive or bad checksum */
+	if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
+		     MLX4_CQE_OPCODE_ERROR)) {
+		return 1;
+	}
+	if (unlikely(cqe->badfcs_enc & MLX4_CQE_BAD_FCS)) {
+		return 1;;
+	}
 
-int mlx4_en_process_rx_cq(struct net_device *dev, struct mlx4_en_cq *cq, int budget)
+	return 0;
+}
+
+static struct sk_buff *
+mlx4_en_get_rx_skb(struct mlx4_en_priv *priv,
+		   struct mlx4_en_rx_desc *rx_desc,
+		   struct sk_buff **pskb,
+		   unsigned int length)
+{
+	struct mlx4_en_dev *mdev = priv->mdev;
+	struct sk_buff *skb;
+	dma_addr_t dma;
+
+	if (length <= SMALL_PACKET_SIZE) {
+		skb = dev_alloc_skb(length + NET_IP_ALIGN);
+		if (unlikely(!skb))
+			return NULL;
+
+		skb->dev = priv->dev;
+		skb_reserve(skb, NET_IP_ALIGN);
+		/* We are copying all relevant data to the skb - temporarily
+		 * synch buffers for the copy */
+		dma = be64_to_cpu(rx_desc->data->addr);
+		dma_sync_single_range_for_cpu(&mdev->pdev->dev, dma, 0,
+					      length, DMA_FROM_DEVICE);
+		skb_copy_to_linear_data(skb, (*pskb)->data, length);
+		dma_sync_single_range_for_device(&mdev->pdev->dev, dma, 0,
+						 length, DMA_FROM_DEVICE);
+
+	} else {
+		skb = *pskb;
+		if (unlikely(mlx4_en_alloc_rx_skb(priv, rx_desc, pskb, 1)))
+			return NULL;
+	}
+
+	skb->tail += length;
+	skb->len = length;
+	skb->truesize = length + sizeof(struct sk_buff);
+	return skb;
+}
+
+static void validate_loopback(struct mlx4_en_priv *priv, struct sk_buff *skb)
+{
+	int i;
+	int offset = ETH_HLEN;
+
+	for (i = 0; i < MLX4_LOOPBACK_TEST_PAYLOAD; i++, offset++) {
+		if (*(skb->data + offset) != (unsigned char) (i & 0xff))
+			goto out_loopback;
+	}
+	/* Loopback found */
+	priv->loopback_ok = 1;
+
+out_loopback:
+	dev_kfree_skb_any(skb);
+}
+
+int mlx4_en_process_rx_cq_skb(struct net_device *dev,
+			      struct mlx4_en_cq *cq, int budget)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_cqe *cqe;
-	struct mlx4_en_rx_ring *ring = &priv->rx_ring[cq->ring];
-	struct skb_frag_struct *skb_frags;
-	struct skb_frag_struct lro_frags[MLX4_EN_MAX_RX_FRAGS];
+	struct mlx4_cq *mcq = &cq->mcq;
+	struct mlx4_en_rx_ring *ring = priv->rx_ring[cq->ring];
 	struct mlx4_en_rx_desc *rx_desc;
+	struct sk_buff **pskb;
 	struct sk_buff *skb;
+	struct net_device_stats *stats = &priv->stats;
 	int index;
-	int nr;
 	unsigned int length;
 	int polled = 0;
-	int ip_summed;
+	int factor = priv->cqe_factor;
+	u32 cons_index = mcq->cons_index;
+	u32 size_mask = ring->size_mask;
+	int size = cq->size;
+	struct mlx4_cqe *buf = cq->buf;
+	u32 csum_none = 0, csum_ok = 0;
 
 	if (!priv->port_up)
 		return 0;
@@ -562,14 +529,14 @@ int mlx4_en_process_rx_cq(struct net_dev
 	/* We assume a 1:1 mapping between CQEs and Rx descriptors, so Rx
 	 * descriptor offset can be deduced from the CQE index instead of
 	 * reading 'cqe->index' */
-	index = cq->mcq.cons_index & ring->size_mask;
-	cqe = &cq->buf[index];
+	index = cons_index & size_mask;
+	cqe = &buf[(index << factor) + factor];
 
 	/* Process all completed CQEs */
 	while (XNOR(cqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK,
-		    cq->mcq.cons_index & cq->size)) {
+		    cons_index & size)) {
 
-		skb_frags = ring->rx_info + (index << priv->log_rx_info);
+		pskb = (struct sk_buff **) ring->rx_info + index;
 		rx_desc = ring->buf + (index << ring->log_stride);
 
 		/*
@@ -577,120 +544,82 @@ int mlx4_en_process_rx_cq(struct net_dev
 		 */
 		rmb();
 
-		/* Drop packet on bad receive or bad checksum */
-		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
-						MLX4_CQE_OPCODE_ERROR)) {
-			en_err(priv, "CQE completed in error - vendor "
-				  "syndrom:%d syndrom:%d\n",
-				  ((struct mlx4_err_cqe *) cqe)->vendor_err_syndrome,
-				  ((struct mlx4_err_cqe *) cqe)->syndrome);
+		if (unlikely(invalid_cqe(cqe)))
 			goto next;
-		}
-		if (unlikely(cqe->badfcs_enc & MLX4_CQE_BAD_FCS)) {
-			en_dbg(RX_ERR, priv, "Accepted frame with bad FCS\n");
-			goto next;
-		}
 
 		/*
 		 * Packet is OK - process it.
 		 */
 		length = be32_to_cpu(cqe->byte_cnt);
+		length -= ring->len_red;
 		ring->bytes += length;
 		ring->packets++;
 
-		if (likely(priv->rx_csum)) {
-			if ((cqe->status & cpu_to_be16(MLX4_CQE_STATUS_IPOK)) &&
-			    (cqe->checksum == cpu_to_be16(0xffff))) {
-				priv->port_stats.rx_chksum_good++;
-				/* This packet is eligible for LRO if it is:
-				 * - DIX Ethernet (type interpretation)
-				 * - TCP/IP (v4)
-				 * - without IP options
-				 * - not an IP fragment */
-				if (mlx4_en_can_lro(cqe->status) &&
-				    dev->features & NETIF_F_LRO) {
-
-					nr = mlx4_en_complete_rx_desc(
-						priv, rx_desc,
-						skb_frags, lro_frags,
-						ring->page_alloc, length);
-					if (!nr)
-						goto next;
-
-					if (priv->vlgrp && (cqe->vlan_my_qpn &
-							    cpu_to_be32(MLX4_CQE_VLAN_PRESENT_MASK))) {
-						lro_vlan_hwaccel_receive_frags(
-						       &ring->lro, lro_frags,
-						       length, length,
-						       priv->vlgrp,
-						       be16_to_cpu(cqe->sl_vid),
-						       NULL, 0);
-					} else
-						lro_receive_frags(&ring->lro,
-								  lro_frags,
-								  length,
-								  length,
-								  NULL, 0);
-
-					goto next;
-				}
-
-				/* LRO not possible, complete processing here */
-				ip_summed = CHECKSUM_UNNECESSARY;
-				INC_PERF_COUNTER(priv->pstats.lro_misses);
-			} else {
-				ip_summed = CHECKSUM_NONE;
-				priv->port_stats.rx_chksum_none++;
-			}
-		} else {
-			ip_summed = CHECKSUM_NONE;
-			priv->port_stats.rx_chksum_none++;
-		}
-
-		skb = mlx4_en_rx_skb(priv, rx_desc, skb_frags,
-				     ring->page_alloc, length);
-		if (!skb) {
-			priv->stats.rx_dropped++;
+		skb = mlx4_en_get_rx_skb(priv, rx_desc, pskb, length);
+		if (unlikely(!skb)) {
+			stats->rx_dropped++;
 			goto next;
 		}
 
-		skb->ip_summed = ip_summed;
+		if (unlikely(priv->validate_loopback)) {
+			validate_loopback(priv, skb);
+			goto next;
+		}
+
 		skb->protocol = eth_type_trans(skb, dev);
 		skb_record_rx_queue(skb, cq->ring);
 
+		if (likely(priv->rx_csum && cqe->checksum == 0xffff)) {
+			csum_ok++;
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+		} else {
+			csum_none++;
+			skb->ip_summed = CHECKSUM_NONE;
+		}
 		/* Push it up the stack */
 		if (priv->vlgrp && (be32_to_cpu(cqe->vlan_my_qpn) &
-				    MLX4_CQE_VLAN_PRESENT_MASK)) {
+				    MLX4_CQE_VLAN_PRESENT_MASK))
 			vlan_hwaccel_receive_skb(skb, priv->vlgrp,
-						be16_to_cpu(cqe->sl_vid));
-		} else
+						 be16_to_cpu(cqe->sl_vid));
+		else
 			netif_receive_skb(skb);
 
+		dev->last_rx = jiffies;
+
 next:
-		++cq->mcq.cons_index;
-		index = (cq->mcq.cons_index) & ring->size_mask;
-		cqe = &cq->buf[index];
-		if (++polled == budget) {
-			/* We are here because we reached the NAPI budget -
-			 * flush only pending LRO sessions */
-			lro_flush_all(&ring->lro);
+		++cons_index;
+		index = cons_index & size_mask;
+		cqe = &buf[(index << factor) + factor];
+		if (++polled == budget)
 			goto out;
-		}
 	}
 
-	/* If CQ is empty flush all LRO sessions unconditionally */
-	lro_flush_all(&ring->lro);
-
 out:
 	AVG_PERF_COUNTER(priv->pstats.rx_coal_avg, polled);
+	mcq->cons_index = cons_index;
 	mlx4_cq_set_ci(&cq->mcq);
 	wmb(); /* ensure HW sees CQ consumer before we post new buffers */
-	ring->cons = cq->mcq.cons_index;
+	ring->cons = mcq->cons_index;
 	ring->prod += polled; /* Polled descriptors were realocated in place */
 	mlx4_en_update_rx_prod_db(ring);
+	ring->csum_ok += csum_ok;
+	ring->csum_none += csum_none;
 	return polled;
 }
 
+void mlx4_en_process_rx(struct mlx4_en_cq *cq)
+{
+	struct net_device *dev = cq->dev;
+	int polled = 0;
+	int processed = 0;
+
+	while (polled < MLX4_EN_RX_LIMIT) {
+		processed = mlx4_en_process_rx_cq_skb(dev, cq, MLX4_EN_RX_BUDGET);
+		if (processed < MLX4_EN_RX_BUDGET)
+			break;
+		polled += processed;
+	}
+}
 
 void mlx4_en_rx_irq(struct mlx4_cq *mcq)
 {
@@ -711,17 +640,26 @@ int mlx4_en_poll_rx_cq(struct napi_struc
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	int done;
 
-	done = mlx4_en_process_rx_cq(dev, cq, budget);
+	done = mlx4_en_process_rx_cq_skb(dev, cq, budget);
 
 	/* If we used up all the quota - we're probably not done yet... */
-	if (done == budget)
+	cq->tot_rx += done;
+	if (done == budget) {
 		INC_PERF_COUNTER(priv->pstats.napi_quota);
-	else {
+		if (cq->tot_rx >= MIN_RX_ARM) {
+			napi_complete(napi);
+			mlx4_en_arm_cq(priv, cq);
+			cq->tot_rx = 0;
+			return 0;
+		}
+	} else {
 		/* Done for now */
 		napi_complete(napi);
 		mlx4_en_arm_cq(priv, cq);
+		cq->tot_rx = 0;
+		return done;
 	}
-	return done;
+	return budget;
 }
 
 
@@ -758,9 +696,9 @@ void mlx4_en_calc_rx_buf(struct net_devi
 				frag_sizes[i] : eff_mtu - buf_size;
 		priv->frag_info[i].frag_prefix_size = buf_size;
 		if (!i)	{
-			priv->frag_info[i].frag_align = NET_IP_ALIGN;
+			priv->frag_info[i].frag_align = 0;
 			priv->frag_info[i].frag_stride =
-				ALIGN(frag_sizes[i] + NET_IP_ALIGN, SMP_CACHE_BYTES);
+				ALIGN(frag_sizes[i], SMP_CACHE_BYTES);
 		} else {
 			priv->frag_info[i].frag_align = 0;
 			priv->frag_info[i].frag_stride =
@@ -815,10 +753,16 @@ static int mlx4_en_config_rss_qp(struct 
 	qp->event = mlx4_en_sqp_event;
 
 	memset(context, 0, sizeof *context);
-	mlx4_en_fill_qp_context(priv, ring->size, ring->stride, 0, 0,
+	mlx4_en_fill_qp_context(priv, ring->actual_size, ring->stride, 0, 0,
 				qpn, ring->cqn, context);
 	context->db_rec_addr = cpu_to_be64(ring->wqres.db.dma);
 
+	if (mdev->dev->caps.flags & (1ull << 34)) {
+		context->param3 |= cpu_to_be32(1 << 29);
+		ring->len_red = 4;
+	} else
+		ring->len_red = 0;
+
 	err = mlx4_qp_to_ready(mdev->dev, &ring->wqres.mtt, context, qp, state);
 	if (err) {
 		mlx4_qp_remove(mdev->dev, qp);
@@ -836,18 +780,33 @@ int mlx4_en_config_rss_steer(struct mlx4
 	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_rss_map *rss_map = &priv->rss_map;
 	struct mlx4_qp_context context;
-	struct mlx4_en_rss_context *rss_context;
+	struct mlx4_rss_context *rss_context;
 	void *ptr;
-	int rss_xor = mdev->profile.rss_xor;
-	u8 rss_mask = mdev->profile.rss_mask;
+	u8 rss_mask = (MLX4_RSS_IPV4 | MLX4_RSS_TCP_IPV4 | MLX4_RSS_IPV6 |
+		       MLX4_RSS_TCP_IPV6);
 	int i, qpn;
 	int err = 0;
 	int good_qps = 0;
 
+	if (priv->rx_ring_num == 1) {
+		en_dbg(DRV, priv, "Configuring single RX ring (without RSS)\n");
+		err = mlx4_en_config_rss_qp(priv, priv->base_qpn,
+					    priv->rx_ring[0],
+					    &rss_map->state[0],
+					    &rss_map->qps[0]);
+		if (err)
+			en_err(priv, "Failed to configure single RX ring\n");
+		rss_map->indir_qp.event = rss_map->qps[0].event;
+		rss_map->indir_qp.qpn = rss_map->qps[0].qpn;
+		rss_map->indir_qp.refcount = rss_map->qps[0].refcount;
+		rss_map->indir_qp.free = rss_map->qps[0].free;
+		return err;
+	}
+
 	en_dbg(DRV, priv, "Configuring rss steering\n");
 	err = mlx4_qp_reserve_range(mdev->dev, priv->rx_ring_num,
-				    priv->rx_ring_num,
-				    &rss_map->base_qpn);
+				    roundup_pow_of_two(priv->rx_ring_num),
+				    &rss_map->base_qpn, 0);
 	if (err) {
 		en_err(priv, "Failed reserving %d qps\n", priv->rx_ring_num);
 		return err;
@@ -855,7 +814,8 @@ int mlx4_en_config_rss_steer(struct mlx4
 
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		qpn = rss_map->base_qpn + i;
-		err = mlx4_en_config_rss_qp(priv, qpn, &priv->rx_ring[i],
+		err = mlx4_en_config_rss_qp(priv, qpn,
+					    priv->rx_ring[i],
 					    &rss_map->state[i],
 					    &rss_map->qps[i]);
 		if (err)
@@ -865,28 +825,27 @@ int mlx4_en_config_rss_steer(struct mlx4
 	}
 
 	/* Configure RSS indirection qp */
-	err = mlx4_qp_reserve_range(mdev->dev, 1, 1, &priv->base_qpn);
-	if (err) {
-		en_err(priv, "Failed to reserve range for RSS "
-			     "indirection qp\n");
-		goto rss_err;
-	}
 	err = mlx4_qp_alloc(mdev->dev, priv->base_qpn, &rss_map->indir_qp);
 	if (err) {
 		en_err(priv, "Failed to allocate RSS indirection QP\n");
-		goto reserve_err;
+		goto rss_err;
 	}
 	rss_map->indir_qp.event = mlx4_en_sqp_event;
 	mlx4_en_fill_qp_context(priv, 0, 0, 0, 1, priv->base_qpn,
-				priv->rx_ring[0].cqn, &context);
+				priv->rx_ring[0]->cqn, &context);
 
-	ptr = ((void *) &context) + 0x3c;
-	rss_context = (struct mlx4_en_rss_context *) ptr;
-	rss_context->base_qpn = cpu_to_be32(ilog2(priv->rx_ring_num) << 24 |
+	ptr = ((void *) &context) + offsetof(struct mlx4_qp_context, pri_path)
+					+ MLX4_RSS_OFFSET_IN_QPC_PRI_PATH;
+	rss_context = (struct mlx4_rss_context *) ptr;
+	rss_context->base_qpn = cpu_to_be32(ilog2(priv->rx_ring_num -
+						  priv->udp_rings) << 24 |
 					    (rss_map->base_qpn));
+
 	rss_context->default_qpn = cpu_to_be32(rss_map->base_qpn);
-	rss_context->hash_fn = rss_xor & 0x3;
-	rss_context->flags = rss_mask << 2;
+
+	rss_mask |= MLX4_RSS_UDP_IPV4 | MLX4_RSS_UDP_IPV6;;
+	rss_context->flags = rss_mask;
+	rss_context->base_qpn_udp = rss_context->default_qpn;
 
 	err = mlx4_qp_to_ready(mdev->dev, &priv->res.mtt, &context,
 			       &rss_map->indir_qp, &rss_map->indir_state);
@@ -900,8 +859,6 @@ indir_err:
 		       MLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->indir_qp);
 	mlx4_qp_remove(mdev->dev, &rss_map->indir_qp);
 	mlx4_qp_free(mdev->dev, &rss_map->indir_qp);
-reserve_err:
-	mlx4_qp_release_range(mdev->dev, priv->base_qpn, 1);
 rss_err:
 	for (i = 0; i < good_qps; i++) {
 		mlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],
@@ -919,11 +876,18 @@ void mlx4_en_release_rss_steer(struct ml
 	struct mlx4_en_rss_map *rss_map = &priv->rss_map;
 	int i;
 
+	if (priv->rx_ring_num == 1) {
+		mlx4_qp_modify(mdev->dev, NULL, rss_map->state[0],
+			       MLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->qps[0]);
+		mlx4_qp_remove(mdev->dev, &rss_map->qps[0]);
+		mlx4_qp_free(mdev->dev, &rss_map->qps[0]);
+		return;
+	}
+
 	mlx4_qp_modify(mdev->dev, NULL, rss_map->indir_state,
 		       MLX4_QP_STATE_RST, NULL, 0, 0, &rss_map->indir_qp);
 	mlx4_qp_remove(mdev->dev, &rss_map->indir_qp);
 	mlx4_qp_free(mdev->dev, &rss_map->indir_qp);
-	mlx4_qp_release_range(mdev->dev, priv->base_qpn, 1);
 
 	for (i = 0; i < priv->rx_ring_num; i++) {
 		mlx4_qp_modify(mdev->dev, NULL, rss_map->state[i],
diff -r c23d1fc7e422 drivers/net/mlx4/en_selftest.c
--- /dev/null
+++ b/drivers/net/mlx4/en_selftest.c
@@ -0,0 +1,173 @@
+/*
+ * Copyright (c) 2007 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/ethtool.h>
+#include <linux/netdevice.h>
+#include <linux/delay.h>
+#include <linux/mlx4/driver.h>
+
+#include "mlx4_en.h"
+
+
+static int mlx4_en_test_registers(struct mlx4_en_priv *priv)
+{
+	return mlx4_cmd(priv->mdev->dev, 0, 0, 0, MLX4_CMD_HW_HEALTH_CHECK,
+			MLX4_CMD_TIME_CLASS_A);
+}
+
+static int mlx4_en_test_loopback_xmit(struct mlx4_en_priv *priv)
+{
+	struct sk_buff *skb;
+	struct ethhdr *ethh;
+	unsigned char *packet;
+	unsigned int packet_size = MLX4_LOOPBACK_TEST_PAYLOAD;
+	unsigned int i;
+	int err;
+
+
+	/* build the pkt before xmit */
+	skb = netdev_alloc_skb(priv->dev, MLX4_LOOPBACK_TEST_PAYLOAD + ETH_HLEN + NET_IP_ALIGN);
+	if (!skb) {
+		en_err(priv, "-LOOPBACK_TEST_XMIT- failed to create skb for xmit\n");
+		return -ENOMEM;
+	}
+	skb_reserve(skb, NET_IP_ALIGN);
+
+	ethh = (struct ethhdr *)skb_put(skb, sizeof(struct ethhdr));
+	packet	= (unsigned char *)skb_put(skb, packet_size);
+	memcpy(ethh->h_dest, priv->dev->dev_addr, ETH_ALEN);
+	memset(ethh->h_source, 0, ETH_ALEN);
+	ethh->h_proto = htons(ETH_P_ARP);
+	skb_set_mac_header(skb, 0);
+	for (i = 0; i < packet_size; ++i)	/* fill our packet */
+		packet[i] = (unsigned char)(i & 0xff);
+
+	/* xmit the pkt */
+	err = mlx4_en_xmit(skb, priv->dev);
+	return err;
+}
+
+static int mlx4_en_test_loopback(struct mlx4_en_priv *priv)
+{
+	u32 loopback_ok = 0;
+	int i;
+
+
+        priv->loopback_ok = 0;
+	priv->validate_loopback = 1;
+
+	/* xmit */
+	if (mlx4_en_test_loopback_xmit(priv)) {
+		en_err(priv, "Transmitting loopback packet failed\n");
+		goto mlx4_en_test_loopback_exit;
+	}
+
+	/* polling for result */
+	for (i = 0; i < MLX4_EN_LOOPBACK_RETRIES; ++i) {
+		msleep(MLX4_EN_LOOPBACK_TIMEOUT);
+		if (priv->loopback_ok) {
+			loopback_ok = 1;
+			break;
+		}
+	}
+	if (!loopback_ok)
+		en_err(priv, "Loopback packet didn't arrive\n");
+
+mlx4_en_test_loopback_exit:
+
+	priv->validate_loopback = 0;
+	return (!loopback_ok);
+}
+
+
+static int mlx4_en_test_link(struct mlx4_en_priv *priv)
+{
+	if (mlx4_en_QUERY_PORT(priv->mdev, priv->port))
+		return -ENOMEM;
+	if (priv->port_state.link_state == 1)
+		return 0;
+	else
+		return 1;
+}
+
+static int mlx4_en_test_speed(struct mlx4_en_priv *priv)
+{
+
+	if (mlx4_en_QUERY_PORT(priv->mdev, priv->port))
+		return -ENOMEM;
+
+	/* The device currently only supports 10G speed */
+	if (priv->port_state.link_speed != SPEED_10000)
+		return priv->port_state.link_speed;
+	return 0;
+}
+
+
+void mlx4_en_ex_selftest(struct net_device *dev, u32 *flags, u64 *buf)
+{
+	struct mlx4_en_priv *priv = netdev_priv(dev);
+	struct mlx4_en_dev *mdev = priv->mdev;
+	int i, carrier_ok;
+
+	memset(buf, 0, sizeof(u64) * MLX4_EN_NUM_SELF_TEST);
+
+	if (*flags & ETH_TEST_FL_OFFLINE) {
+		/* disable the interface */
+		carrier_ok = netif_carrier_ok(dev);
+
+		netif_carrier_off(dev);
+		/* Wait untill all tx queues are empty.
+		 * there should not be any additional incoming traffic
+		 * since we turned the carrier off */
+		msleep(200);
+
+		if (priv->mdev->dev->caps.loopback_support) {
+			buf[3] = mlx4_en_test_registers(priv);
+			if (priv->port_up)
+				buf[4] = mlx4_en_test_loopback(priv);
+		}
+
+		if (carrier_ok)
+			netif_carrier_on(dev);
+
+	}
+	buf[0] = mlx4_test_interrupts(mdev->dev);
+	buf[1] = mlx4_en_test_link(priv);
+	buf[2] = mlx4_en_test_speed(priv);
+
+	for (i = 0; i < MLX4_EN_NUM_SELF_TEST; i++) {
+		if (buf[i])
+			*flags |= ETH_TEST_FL_FAILED;
+	}
+}
diff -r c23d1fc7e422 drivers/net/mlx4/en_tx.c
--- a/drivers/net/mlx4/en_tx.c
+++ b/drivers/net/mlx4/en_tx.c
@@ -42,6 +42,7 @@
 
 enum {
 	MAX_INLINE = 104, /* 128 - 16 - 4 - 4 */
+	MAX_BF = 256,
 };
 
 static int inline_thold __read_mostly = MAX_INLINE;
@@ -50,7 +51,7 @@ module_param_named(inline_thold, inline_
 MODULE_PARM_DESC(inline_thold, "treshold for using inline data");
 
 int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,
-			   struct mlx4_en_tx_ring *ring, u32 size,
+			   struct mlx4_en_tx_ring *ring, int qpn, u32 size,
 			   u16 stride)
 {
 	struct mlx4_en_dev *mdev = priv->mdev;
@@ -66,7 +67,7 @@ int mlx4_en_create_tx_ring(struct mlx4_e
 	spin_lock_init(&ring->comp_lock);
 
 	tmp = size * sizeof(struct mlx4_en_tx_info);
-	ring->tx_info = vmalloc(tmp);
+	ring->tx_info = vmalloc_node(tmp, ring->numa_node);
 	if (!ring->tx_info) {
 		en_err(priv, "Failed allocating tx_info ring\n");
 		return -ENOMEM;
@@ -74,7 +75,9 @@ int mlx4_en_create_tx_ring(struct mlx4_e
 	en_dbg(DRV, priv, "Allocated tx_info ring at addr:%p size:%d\n",
 		 ring->tx_info, tmp);
 
-	ring->bounce_buf = kmalloc(MAX_DESC_SIZE, GFP_KERNEL);
+	ring->bounce_buf = kmalloc_node(MAX_DESC_SIZE, GFP_KERNEL, ring->numa_node);
+	if (!ring->bounce_buf)
+		ring->bounce_buf = kmalloc(MAX_DESC_SIZE, GFP_KERNEL);
 	if (!ring->bounce_buf) {
 		en_err(priv, "Failed allocating bounce buffer\n");
 		err = -ENOMEM;
@@ -83,13 +86,13 @@ int mlx4_en_create_tx_ring(struct mlx4_e
 	ring->buf_size = ALIGN(size * ring->stride, MLX4_EN_PAGE_SIZE);
 
 	err = mlx4_alloc_hwq_res(mdev->dev, &ring->wqres, ring->buf_size,
-				 2 * PAGE_SIZE);
+				 2 * PAGE_SIZE, ring->numa_node);
 	if (err) {
 		en_err(priv, "Failed allocating hwq resources\n");
 		goto err_bounce;
 	}
 
-	err = mlx4_en_map_buffer(&ring->wqres.buf);
+	err = mlx4_en_map_buffer(&ring->wqres.buf, ring->numa_node);
 	if (err) {
 		en_err(priv, "Failed to map TX buffer\n");
 		goto err_hwq_res;
@@ -101,23 +104,25 @@ int mlx4_en_create_tx_ring(struct mlx4_e
 	       "buf_size:%d dma:%llx\n", ring, ring->buf, ring->size,
 	       ring->buf_size, (unsigned long long) ring->wqres.buf.direct.map);
 
-	err = mlx4_qp_reserve_range(mdev->dev, 1, 1, &ring->qpn);
-	if (err) {
-		en_err(priv, "Failed reserving qp for tx ring.\n");
-		goto err_map;
-	}
-
+	ring->qpn = qpn;
 	err = mlx4_qp_alloc(mdev->dev, ring->qpn, &ring->qp);
 	if (err) {
 		en_err(priv, "Failed allocating qp %d\n", ring->qpn);
-		goto err_reserve;
+		goto err_map;
 	}
 	ring->qp.event = mlx4_en_sqp_event;
 
+	err = mlx4_bf_alloc(mdev->dev, &ring->bf, ring->numa_node);
+	if (err) {
+		en_dbg(DRV, priv, "working without blueflame (%d)", err);
+		ring->bf.uar = &mdev->priv_uar;
+		ring->bf.uar->map = mdev->uar_map;
+		ring->bf_enabled = false;
+	} else
+		ring->bf_enabled = true;
+
 	return 0;
 
-err_reserve:
-	mlx4_qp_release_range(mdev->dev, ring->qpn, 1);
 err_map:
 	mlx4_en_unmap_buffer(&ring->wqres.buf);
 err_hwq_res:
@@ -137,9 +142,10 @@ void mlx4_en_destroy_tx_ring(struct mlx4
 	struct mlx4_en_dev *mdev = priv->mdev;
 	en_dbg(DRV, priv, "Destroying tx ring, qpn: %d\n", ring->qpn);
 
+	if (ring->bf_enabled)
+		mlx4_bf_free(mdev->dev, &ring->bf);
 	mlx4_qp_remove(mdev->dev, &ring->qp);
 	mlx4_qp_free(mdev->dev, &ring->qp);
-	mlx4_qp_release_range(mdev->dev, ring->qpn, 1);
 	mlx4_en_unmap_buffer(&ring->wqres.buf);
 	mlx4_free_hwq_res(mdev->dev, &ring->wqres, ring->buf_size);
 	kfree(ring->bounce_buf);
@@ -165,10 +171,12 @@ int mlx4_en_activate_tx_ring(struct mlx4
 	memset(ring->buf, 0, ring->buf_size);
 
 	ring->qp_state = MLX4_QP_STATE_RST;
-	ring->doorbell_qpn = swab32(ring->qp.qpn << 8);
+	ring->doorbell_qpn = ring->qp.qpn << 8;
 
 	mlx4_en_fill_qp_context(priv, ring->size, ring->stride, 1, 0, ring->qpn,
 				ring->cqn, &ring->context);
+	if (ring->bf_enabled)
+		ring->context.usr_page = cpu_to_be32(ring->bf.uar->index);
 
 	err = mlx4_qp_to_ready(mdev->dev, &ring->wqres.mtt, &ring->context,
 			       &ring->qp, &ring->qp_state);
@@ -190,7 +198,6 @@ static u32 mlx4_en_free_tx_desc(struct m
 				struct mlx4_en_tx_ring *ring,
 				int index, u8 owner)
 {
-	struct mlx4_en_dev *mdev = priv->mdev;
 	struct mlx4_en_tx_info *tx_info = &ring->tx_info[index];
 	struct mlx4_en_tx_desc *tx_desc = ring->buf + index * TXBB_SIZE;
 	struct mlx4_wqe_data_seg *data = (void *) tx_desc + tx_info->data_offset;
@@ -206,7 +213,7 @@ static u32 mlx4_en_free_tx_desc(struct m
 	if (likely((void *) tx_desc + tx_info->nr_txbb * TXBB_SIZE <= end)) {
 		if (!tx_info->inl) {
 			if (tx_info->linear) {
-				pci_unmap_single(mdev->pdev,
+				dma_unmap_single(priv->ddev,
 					(dma_addr_t) be64_to_cpu(data->addr),
 					 be32_to_cpu(data->byte_count),
 					 PCI_DMA_TODEVICE);
@@ -215,7 +222,7 @@ static u32 mlx4_en_free_tx_desc(struct m
 
 			for (i = 0; i < frags; i++) {
 				frag = &skb_shinfo(skb)->frags[i];
-				pci_unmap_page(mdev->pdev,
+				dma_unmap_page(priv->ddev,
 					(dma_addr_t) be64_to_cpu(data[i].addr),
 					frag->size, PCI_DMA_TODEVICE);
 			}
@@ -234,7 +241,7 @@ static u32 mlx4_en_free_tx_desc(struct m
 			}
 
 			if (tx_info->linear) {
-				pci_unmap_single(mdev->pdev,
+				dma_unmap_single(priv->ddev,
 					(dma_addr_t) be64_to_cpu(data->addr),
 					 be32_to_cpu(data->byte_count),
 					 PCI_DMA_TODEVICE);
@@ -246,7 +253,7 @@ static u32 mlx4_en_free_tx_desc(struct m
 				if ((void *) data >= end)
 					data = (struct mlx4_wqe_data_seg *) ring->buf;
 				frag = &skb_shinfo(skb)->frags[i];
-				pci_unmap_page(mdev->pdev,
+				dma_unmap_page(priv->ddev,
 					(dma_addr_t) be64_to_cpu(data->addr),
 					 frag->size, PCI_DMA_TODEVICE);
 				++data;
@@ -298,59 +305,61 @@ int mlx4_en_free_tx_buf(struct net_devic
 	return cnt;
 }
 
-
 static void mlx4_en_process_tx_cq(struct net_device *dev, struct mlx4_en_cq *cq)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	struct mlx4_cq *mcq = &cq->mcq;
-	struct mlx4_en_tx_ring *ring = &priv->tx_ring[cq->ring];
-	struct mlx4_cqe *cqe = cq->buf;
+	struct mlx4_en_tx_ring *ring = priv->tx_ring[cq->ring];
+	struct mlx4_cqe *cqe;
 	u16 index;
-	u16 new_index;
+	u16 new_index, ring_index;
 	u32 txbbs_skipped = 0;
-	u32 cq_last_sav;
-
-	/* index always points to the first TXBB of the last polled descriptor */
-	index = ring->cons & ring->size_mask;
-	new_index = be16_to_cpu(cqe->wqe_index) & ring->size_mask;
-	if (index == new_index)
-		return;
+	int factor = priv->cqe_factor;
+	u32 cons_index = mcq->cons_index;
+	int size = cq->size;
+	u32 size_mask = ring->size_mask;
+	struct mlx4_cqe *buf = cq->buf;
 
 	if (!priv->port_up)
 		return;
 
-	/*
-	 * We use a two-stage loop:
-	 * - the first samples the HW-updated CQE
-	 * - the second frees TXBBs until the last sample
-	 * This lets us amortize CQE cache misses, while still polling the CQ
-	 * until is quiescent.
-	 */
-	cq_last_sav = mcq->cons_index;
-	do {
+	index = cons_index & size_mask;
+	cqe = &buf[(index << factor) + factor];
+	ring_index = ring->cons & size_mask;
+
+	/* Process all completed CQEs */
+	while (XNOR(cqe->owner_sr_opcode & MLX4_CQE_OWNER_MASK,
+			cons_index & size)) {
+		/*
+		 * make sure we read the CQE after we read the
+		 * ownership bit
+		 */
+		rmb();
+
+		/* Skip over last polled CQE */
+		new_index = be16_to_cpu(cqe->wqe_index) & size_mask;
+
 		do {
-			/* Skip over last polled CQE */
-			index = (index + ring->last_nr_txbb) & ring->size_mask;
 			txbbs_skipped += ring->last_nr_txbb;
+			ring_index = (ring_index + ring->last_nr_txbb) & size_mask;
+			/* free next descriptor */
+			ring->last_nr_txbb = mlx4_en_free_tx_desc(
+					priv, ring, ring_index,
+					!!((ring->cons + txbbs_skipped) &
+							ring->size));
+		} while (ring_index != new_index);
 
-			/* Poll next CQE */
-			ring->last_nr_txbb = mlx4_en_free_tx_desc(
-						priv, ring, index,
-						!!((ring->cons + txbbs_skipped) &
-						   ring->size));
-			++mcq->cons_index;
+		++cons_index;
+		index = cons_index & size_mask;
+		cqe = &buf[(index << factor) + factor];
+	}
 
-		} while (index != new_index);
-
-		new_index = be16_to_cpu(cqe->wqe_index) & ring->size_mask;
-	} while (index != new_index);
-	AVG_PERF_COUNTER(priv->pstats.tx_coal_avg,
-			 (u32) (mcq->cons_index - cq_last_sav));
 
 	/*
 	 * To prevent CQ overflow we first update CQ consumer and only then
 	 * the ring consumer.
 	 */
+	mcq->cons_index = cons_index;
 	mlx4_cq_set_ci(mcq);
 	wmb();
 	ring->cons += txbbs_skipped;
@@ -370,12 +379,15 @@ void mlx4_en_tx_irq(struct mlx4_cq *mcq)
 {
 	struct mlx4_en_cq *cq = container_of(mcq, struct mlx4_en_cq, mcq);
 	struct mlx4_en_priv *priv = netdev_priv(cq->dev);
-	struct mlx4_en_tx_ring *ring = &priv->tx_ring[cq->ring];
+	struct mlx4_en_tx_ring *ring = priv->tx_ring[cq->ring];
 
 	if (!spin_trylock(&ring->comp_lock))
 		return;
 	mlx4_en_process_tx_cq(cq->dev, cq);
-	mod_timer(&cq->timer, jiffies + 1);
+	if (priv->mdev->profile.use_tx_polling)
+		mod_timer(&cq->timer, jiffies + 1);
+	else
+		mlx4_en_arm_cq(priv, cq);
 	spin_unlock(&ring->comp_lock);
 }
 
@@ -384,7 +396,7 @@ void mlx4_en_poll_tx_cq(unsigned long da
 {
 	struct mlx4_en_cq *cq = (struct mlx4_en_cq *) data;
 	struct mlx4_en_priv *priv = netdev_priv(cq->dev);
-	struct mlx4_en_tx_ring *ring = &priv->tx_ring[cq->ring];
+	struct mlx4_en_tx_ring *ring = priv->tx_ring[cq->ring];
 	u32 inflight;
 
 	INC_PERF_COUNTER(priv->pstats.tx_poll);
@@ -405,10 +417,9 @@ void mlx4_en_poll_tx_cq(unsigned long da
 	spin_unlock_irq(&ring->comp_lock);
 }
 
-static struct mlx4_en_tx_desc *mlx4_en_bounce_to_desc(struct mlx4_en_priv *priv,
-						      struct mlx4_en_tx_ring *ring,
-						      u32 index,
-						      unsigned int desc_size)
+static struct mlx4_en_tx_desc*
+mlx4_en_bounce_to_desc(struct mlx4_en_tx_ring *ring, u32 index,
+		       unsigned int desc_size)
 {
 	u32 copy = (ring->size - index) * TXBB_SIZE;
 	int i;
@@ -435,13 +446,13 @@ static struct mlx4_en_tx_desc *mlx4_en_b
 
 static inline void mlx4_en_xmit_poll(struct mlx4_en_priv *priv, int tx_ind)
 {
-	struct mlx4_en_cq *cq = &priv->tx_cq[tx_ind];
-	struct mlx4_en_tx_ring *ring = &priv->tx_ring[tx_ind];
+	struct mlx4_en_cq *cq		= priv->tx_cq[tx_ind];
+	struct mlx4_en_tx_ring *ring	= priv->tx_ring[tx_ind];
 	unsigned long flags;
 
 	/* If we don't have a pending timer, set one up to catch our recent
 	   post in case the interface becomes idle */
-	if (!timer_pending(&cq->timer))
+	if (unlikely(!timer_pending(&cq->timer)))
 		mod_timer(&cq->timer, jiffies + MLX4_EN_TX_POLL_TIMEOUT);
 
 	/* Poll the CQ every mlx4_en_TX_MODER_POLL packets */
@@ -532,10 +543,10 @@ static int get_real_size(struct sk_buff 
 }
 
 static void build_inline_wqe(struct mlx4_en_tx_desc *tx_desc, struct sk_buff *skb,
-			     int real_size, u16 *vlan_tag, int tx_ind, void *fragptr)
+			     void *fragptr)
 {
 	struct mlx4_wqe_inline_seg *inl = &tx_desc->inl;
-	int spc = MLX4_INLINE_ALIGN - CTRL_SIZE - sizeof *inl;
+	static int spc = MLX4_INLINE_ALIGN - CTRL_SIZE - sizeof *inl;
 
 	if (skb->len <= spc) {
 		inl->byte_count = cpu_to_be32(1 << 31 | skb->len);
@@ -568,9 +579,6 @@ static void build_inline_wqe(struct mlx4
 		wmb();
 		inl->byte_count = cpu_to_be32(1 << 31 | (skb->len - spc));
 	}
-	tx_desc->ctrl.vlan_tag = cpu_to_be16(*vlan_tag);
-	tx_desc->ctrl.ins_vlan = MLX4_WQE_CTRL_INS_VLAN * !!(*vlan_tag);
-	tx_desc->ctrl.fence_size = (real_size / 16) & 0x3f;
 }
 
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb)
@@ -578,38 +586,53 @@ u16 mlx4_en_select_queue(struct net_devi
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u16 vlan_tag = 0;
 
-	/* If we support per priority flow control and the packet contains
-	 * a vlan tag, send the packet to the TX ring assigned to that priority
-	 */
-	if (priv->prof->rx_ppp && priv->vlgrp && vlan_tx_tag_present(skb)) {
+	if (priv->prof->rx_ppp && vlan_tx_tag_present(skb)) {
 		vlan_tag = vlan_tx_tag_get(skb);
-		return MLX4_EN_NUM_TX_RINGS + (vlan_tag >> 13);
+		return MLX4_EN_NUM_HASH_RINGS + (vlan_tag >> 13);
 	}
 
 	return skb_tx_hash(dev, skb);
 }
 
-netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev)
+static inline void mlx4_bf_copy(unsigned long *dst, unsigned long *src,
+				unsigned bytecnt)
+{
+	__iowrite64_copy(dst, src, bytecnt / 8);
+}
+
+int mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
-	struct mlx4_en_dev *mdev = priv->mdev;
+	struct mlx4_en_dev *mdev		= priv->mdev;
+	struct mlx4_en_port_stats *port_stats	= &priv->port_stats;
+	struct device *ddev			= priv->ddev;
+	struct mlx4_mr *mr			= &mdev->mr;
 	struct mlx4_en_tx_ring *ring;
 	struct mlx4_en_cq *cq;
 	struct mlx4_en_tx_desc *tx_desc;
 	struct mlx4_wqe_data_seg *data;
 	struct skb_frag_struct *frag;
 	struct mlx4_en_tx_info *tx_info;
+	struct ethhdr *ethh;
+	struct mlx4_wqe_ctrl_seg *ctrl;
+	u64 mac;
+	u32 mac_l, mac_h;
 	int tx_ind = 0;
 	int nr_txbb;
 	int desc_size;
 	int real_size;
 	dma_addr_t dma;
-	u32 index;
+	u32 index, bf_index;
 	__be32 op_own;
 	u16 vlan_tag = 0;
 	int i;
 	int lso_header_size;
 	void *fragptr;
+	bool bounce = false;
+	int ring_busy;
+
+	if (unlikely(!priv->port_up))
+		goto tx_drop;
 
 	real_size = get_real_size(skb, dev, &lso_header_size);
 	if (unlikely(!real_size))
@@ -625,37 +648,41 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff 
 	}
 
 	tx_ind = skb->queue_mapping;
-	ring = &priv->tx_ring[tx_ind];
+	ring = priv->tx_ring[tx_ind];
 	if (priv->vlgrp && vlan_tx_tag_present(skb))
 		vlan_tag = vlan_tx_tag_get(skb);
 
 	/* Check available TXBBs And 2K spare for prefetch */
-	if (unlikely(((int)(ring->prod - ring->cons)) >
-		     ring->size - HEADROOM - MAX_DESC_TXBBS)) {
+	ring_busy = (int)(ring->prod - ring->cons) - (ring->size - HEADROOM - MAX_DESC_TXBBS);
+	if (unlikely(ring_busy > 0)) {
 		/* every full Tx ring stops queue */
 		netif_tx_stop_queue(netdev_get_tx_queue(dev, tx_ind));
 		ring->blocked = 1;
-		priv->port_stats.queue_stopped++;
+		port_stats->queue_stopped++;
 
 		/* Use interrupts to find out when queue opened */
-		cq = &priv->tx_cq[tx_ind];
+		cq = priv->tx_cq[tx_ind];
 		mlx4_en_arm_cq(priv, cq);
 		return NETDEV_TX_BUSY;
 	}
 
 	/* Track current inflight packets for performance analysis */
-	AVG_PERF_COUNTER(priv->pstats.inflight_avg,
+	AVG_PERF_COUNTER(pstats->inflight_avg,
 			 (u32) (ring->prod - ring->cons - 1));
 
 	/* Packet is good - grab an index and transmit it */
 	index = ring->prod & ring->size_mask;
+	bf_index = ring->prod;
 
 	/* See if we have enough space for whole descriptor TXBB for setting
 	 * SW ownership on next descriptor; if not, use a bounce buffer. */
 	if (likely(index + nr_txbb <= ring->size))
 		tx_desc = ring->buf + index * TXBB_SIZE;
-	else
+	else {
 		tx_desc = (struct mlx4_en_tx_desc *) ring->bounce_buf;
+		bounce = true;
+	}
+	ctrl = &tx_desc->ctrl;
 
 	/* Save skb in tx_info ring */
 	tx_info = &ring->tx_info[index];
@@ -664,22 +691,42 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff 
 
 	/* Prepare ctrl segement apart opcode+ownership, which depends on
 	 * whether LSO is used */
-	tx_desc->ctrl.vlan_tag = cpu_to_be16(vlan_tag);
-	tx_desc->ctrl.ins_vlan = MLX4_WQE_CTRL_INS_VLAN * !!vlan_tag;
-	tx_desc->ctrl.fence_size = (real_size / 16) & 0x3f;
-	tx_desc->ctrl.srcrb_flags = cpu_to_be32(MLX4_WQE_CTRL_CQ_UPDATE |
-						MLX4_WQE_CTRL_SOLICITED);
+	ctrl->vlan_tag = cpu_to_be16(vlan_tag);
+	ctrl->ins_vlan = MLX4_WQE_CTRL_INS_VLAN *
+		!!vlan_tx_tag_present(skb);
+	ctrl->fence_size = (real_size / 16) & 0x3f;
+	ctrl->srcrb_flags = cpu_to_be32(MLX4_WQE_CTRL_SOLICITED);
+	/* Completion is done every 16 packets
+	 * for vlan tagged packets completion must be immediate */
+	if (!(index & 0xf) || vlan_tag || (ring_busy > -MAX_DESC_TXBBS))
+		ctrl->srcrb_flags = cpu_to_be32(MLX4_WQE_CTRL_SOLICITED
+						| MLX4_WQE_CTRL_CQ_UPDATE);
+
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
-		tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
-							 MLX4_WQE_CTRL_TCP_UDP_CSUM);
-		priv->port_stats.tx_chksum_offload++;
+		ctrl->srcrb_flags = cpu_to_be32(MLX4_WQE_CTRL_SOLICITED
+			    | MLX4_WQE_CTRL_CQ_UPDATE
+			    | MLX4_WQE_CTRL_IP_CSUM
+			    | MLX4_WQE_CTRL_TCP_UDP_CSUM);
+		ring->tx_csum++;
+	}
+
+	if (unlikely(priv->validate_loopback)) {
+		/* Copy dst mac address to wqe */
+		skb_reset_mac_header(skb);
+		ethh = eth_hdr(skb);
+		if (ethh) {
+			mac = mlx4_en_mac_to_u64(ethh->h_dest);
+			mac_h = (u32) ((mac & 0xffff00000000ULL) >> 16);
+			mac_l = (u32) (mac & 0xffffffff);
+			ctrl->srcrb_flags |= cpu_to_be32(mac_h);
+			ctrl->imm = cpu_to_be32(mac_l);
+		}
 	}
 
 	/* Handle LSO (TSO) packets */
 	if (lso_header_size) {
 		/* Mark opcode as LSO */
-		op_own = cpu_to_be32(MLX4_OPCODE_LSO | (1 << 6)) |
-			((ring->prod & ring->size) ?
+		op_own = cpu_to_be32(MLX4_OPCODE_LSO | (1 << 6)) | ((ring->prod & ring->size) ?
 				cpu_to_be32(MLX4_EN_BIT_DESC_OWN) : 0);
 
 		/* Fill in the LSO prefix */
@@ -692,22 +739,21 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff 
 		data = ((void *) &tx_desc->lso +
 			ALIGN(lso_header_size + 4, DS_SIZE));
 
-		priv->port_stats.tso_packets++;
+		port_stats->tso_packets++;
 		i = ((skb->len - lso_header_size) / skb_shinfo(skb)->gso_size) +
 			!!((skb->len - lso_header_size) % skb_shinfo(skb)->gso_size);
 		ring->bytes += skb->len + (i - 1) * lso_header_size;
 		ring->packets += i;
 	} else {
 		/* Normal (Non LSO) packet */
-		op_own = cpu_to_be32(MLX4_OPCODE_SEND) |
-			((ring->prod & ring->size) ?
+		op_own = cpu_to_be32(MLX4_OPCODE_SEND) | ((ring->prod & ring->size) ?
 			 cpu_to_be32(MLX4_EN_BIT_DESC_OWN) : 0);
 		data = &tx_desc->data;
 		ring->bytes += max(skb->len, (unsigned int) ETH_ZLEN);
 		ring->packets++;
 
 	}
-	AVG_PERF_COUNTER(priv->pstats.tx_pktsz_avg, skb->len);
+	AVG_PERF_COUNTER(pstats->tx_pktsz_avg, skb->len);
 
 
 	/* valid only for none inline segments */
@@ -720,10 +766,10 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff 
 		/* Map fragments */
 		for (i = skb_shinfo(skb)->nr_frags - 1; i >= 0; i--) {
 			frag = &skb_shinfo(skb)->frags[i];
-			dma = pci_map_page(mdev->dev->pdev, frag->page, frag->page_offset,
+			dma = dma_map_page(ddev, frag->page, frag->page_offset,
 					   frag->size, PCI_DMA_TODEVICE);
 			data->addr = cpu_to_be64(dma);
-			data->lkey = cpu_to_be32(mdev->mr.key);
+			data->lkey = cpu_to_be32(mr->key);
 			wmb();
 			data->byte_count = cpu_to_be32(frag->size);
 			--data;
@@ -731,42 +777,59 @@ netdev_tx_t mlx4_en_xmit(struct sk_buff 
 
 		/* Map linear part */
 		if (tx_info->linear) {
-			dma = pci_map_single(mdev->dev->pdev, skb->data + lso_header_size,
+			dma = dma_map_single(ddev, skb->data + lso_header_size,
 					     skb_headlen(skb) - lso_header_size, PCI_DMA_TODEVICE);
 			data->addr = cpu_to_be64(dma);
-			data->lkey = cpu_to_be32(mdev->mr.key);
+			data->lkey = cpu_to_be32(mr->key);
 			wmb();
 			data->byte_count = cpu_to_be32(skb_headlen(skb) - lso_header_size);
 		}
 		tx_info->inl = 0;
 	} else {
-		build_inline_wqe(tx_desc, skb, real_size, &vlan_tag, tx_ind, fragptr);
+		build_inline_wqe(tx_desc, skb, fragptr);
 		tx_info->inl = 1;
 	}
 
 	ring->prod += nr_txbb;
 
 	/* If we used a bounce buffer then copy descriptor back into place */
-	if (tx_desc == (struct mlx4_en_tx_desc *) ring->bounce_buf)
-		tx_desc = mlx4_en_bounce_to_desc(priv, ring, index, desc_size);
+	if (bounce)
+		tx_desc = mlx4_en_bounce_to_desc(ring, index, desc_size);
 
 	/* Run destructor before passing skb to HW */
 	if (likely(!skb_shared(skb)))
 		skb_orphan(skb);
 
-	/* Ensure new descirptor hits memory
-	 * before setting ownership of this descriptor to HW */
-	wmb();
-	tx_desc->ctrl.owner_opcode = op_own;
+	if (ring->bf_enabled && desc_size <= MAX_BF && !bounce && !vlan_tag) {
+		*(__be32 *) (&tx_desc->ctrl.vlan_tag) |= cpu_to_be32(ring->doorbell_qpn);
+		op_own |= htonl((bf_index & 0xffff) << 8);
+		/* Ensure new descirptor hits memory
+		* before setting ownership of this descriptor to HW */
+		wmb();
+		ctrl->owner_opcode = op_own;
 
-	/* Ring doorbell! */
-	wmb();
-	writel(ring->doorbell_qpn, mdev->uar_map + MLX4_SEND_DOORBELL);
+		wmb();
 
-	/* Poll CQ here */
-	mlx4_en_xmit_poll(priv, tx_ind);
+		mlx4_bf_copy(ring->bf.reg + ring->bf.offset, (unsigned long *) ctrl,
+		     desc_size);
 
-	return NETDEV_TX_OK;
+		wmb();
+
+		ring->bf.offset ^= ring->bf.buf_size;
+	} else {
+		/* Ensure new descirptor hits memory
+		* before setting ownership of this descriptor to HW */
+		wmb();
+		tx_desc->ctrl.owner_opcode = op_own;
+		wmb();
+		iowrite32be(ring->doorbell_qpn, ring->bf.uar->map + MLX4_SEND_DOORBELL);
+	}
+
+	/* Poll CQ here if we are running in polling mode */
+	if (mdev->profile.use_tx_polling)
+		mlx4_en_xmit_poll(priv, tx_ind);
+
+	return 0;
 
 tx_drop:
 	dev_kfree_skb_any(skb);
diff -r c23d1fc7e422 drivers/net/mlx4/eq.c
--- a/drivers/net/mlx4/eq.c
+++ b/drivers/net/mlx4/eq.c
@@ -31,6 +31,7 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/interrupt.h>
 #include <linux/mm.h>
 #include <linux/dma-mapping.h>
@@ -41,10 +42,6 @@
 #include "fw.h"
 
 enum {
-	MLX4_IRQNAME_SIZE	= 64
-};
-
-enum {
 	MLX4_NUM_ASYNC_EQE	= 0x100,
 	MLX4_NUM_SPARE_EQE	= 0x80,
 	MLX4_EQ_ENTRY_SIZE	= 0x20
@@ -98,46 +95,12 @@ struct mlx4_eq_context {
 			       (1ull << MLX4_EVENT_TYPE_SRQ_CATAS_ERROR)    | \
 			       (1ull << MLX4_EVENT_TYPE_SRQ_QP_LAST_WQE)    | \
 			       (1ull << MLX4_EVENT_TYPE_SRQ_LIMIT)	    | \
-			       (1ull << MLX4_EVENT_TYPE_CMD))
-
-struct mlx4_eqe {
-	u8			reserved1;
-	u8			type;
-	u8			reserved2;
-	u8			subtype;
-	union {
-		u32		raw[6];
-		struct {
-			__be32	cqn;
-		} __attribute__((packed)) comp;
-		struct {
-			u16	reserved1;
-			__be16	token;
-			u32	reserved2;
-			u8	reserved3[3];
-			u8	status;
-			__be64	out_param;
-		} __attribute__((packed)) cmd;
-		struct {
-			__be32	qpn;
-		} __attribute__((packed)) qp;
-		struct {
-			__be32	srqn;
-		} __attribute__((packed)) srq;
-		struct {
-			__be32	cqn;
-			u32	reserved1;
-			u8	reserved2[3];
-			u8	syndrome;
-		} __attribute__((packed)) cq_err;
-		struct {
-			u32	reserved1[2];
-			__be32	port;
-		} __attribute__((packed)) port_change;
-	}			event;
-	u8			reserved3[3];
-	u8			owner;
-} __attribute__((packed));
+			       (1ull << MLX4_EVENT_TYPE_CMD)		    | \
+			       (1ull << MLX4_EVENT_TYPE_VEP_UPDATE)	    | \
+				(1ull << MLX4_EVENT_TYPE_MAC_UPDATE)	    | \
+			       (1ull << MLX4_EVENT_TYPE_COMM_CHANNEL)	    | \
+			       (1ull << MLX4_EVENT_TYPE_OP_REQUIRED)	    | \
+			       (1ull << MLX4_EVENT_TYPE_FATAL_WARNING))
 
 static void eq_set_ci(struct mlx4_eq *eq, int req_not)
 {
@@ -148,27 +111,227 @@ static void eq_set_ci(struct mlx4_eq *eq
 	mb();
 }
 
-static struct mlx4_eqe *get_eqe(struct mlx4_eq *eq, u32 entry)
+static struct mlx4_eqe *get_eqe(struct mlx4_eq *eq, u32 entry, u8 eqe_factor)
 {
-	unsigned long off = (entry & (eq->nent - 1)) * MLX4_EQ_ENTRY_SIZE;
-	return eq->page_list[off / PAGE_SIZE].buf + off % PAGE_SIZE;
+	/* (entry & (eq->nent - 1)) gives us a cyclic array */
+	unsigned long offset = (entry & (eq->nent - 1)) * (MLX4_EQ_ENTRY_SIZE << eqe_factor);
+	/* CX3 is capable of extending the EQE from 32 to 64 bytes.
+	   When this feature is enabled, the first (in the lower addresses)
+	   32 bytes in the 64 byte EQE are reserved and the next 32
+	   bytes contain the legacy EQE information. */
+	return eq->page_list[offset / PAGE_SIZE].buf + (offset + (eqe_factor ? MLX4_EQ_ENTRY_SIZE : 0)) % PAGE_SIZE;
 }
 
-static struct mlx4_eqe *next_eqe_sw(struct mlx4_eq *eq)
+static struct mlx4_eqe *next_eqe_sw(struct mlx4_eq *eq, u8 eqe_factor)
 {
-	struct mlx4_eqe *eqe = get_eqe(eq, eq->cons_index);
+	struct mlx4_eqe *eqe = get_eqe(eq, eq->cons_index, eqe_factor);
 	return !!(eqe->owner & 0x80) ^ !!(eq->cons_index & eq->nent) ? NULL : eqe;
 }
 
+static struct mlx4_eqe *next_slave_event_eqe(struct mlx4_slave_event_eq *slave_eq)
+{
+	struct mlx4_eqe *eqe =
+		&slave_eq->event_eqe[slave_eq->cons & (SLAVE_EVENT_EQ_SIZE - 1)];
+	return (!!(eqe->owner & 0x80) ^ !!(slave_eq->cons & SLAVE_EVENT_EQ_SIZE)) ?
+		eqe : NULL;
+}
+void mlx4_gen_slave_eqe(struct work_struct *work)
+{
+	struct mlx4_mfunc_master_ctx *master = container_of(work,
+							   struct mlx4_mfunc_master_ctx,
+							   slave_event_work);
+	struct mlx4_mfunc *mfunc = container_of(master, struct mlx4_mfunc, master);
+	struct mlx4_priv *priv = container_of(mfunc, struct mlx4_priv, mfunc);
+	struct mlx4_dev *dev = &priv->dev;
+	struct mlx4_slave_event_eq *slave_eq = &mfunc->master.slave_eq;
+	struct mlx4_eqe *eqe;
+	u8 slave;
+	int i;
+
+	for (eqe = next_slave_event_eqe(slave_eq); eqe;
+	      eqe = next_slave_event_eqe(slave_eq)) {
+		slave = eqe->slave_id;
+
+		/* All active slaves need to receive the event */
+		if (slave == ALL_SLAVES) {
+			for (i = 0; i < dev->num_slaves; i++) {
+				if (master->slave_state[i].active)
+					if (mlx4_GEN_EQE(dev, i, eqe))
+						mlx4_warn(dev, "Failed to generate event "
+							       "for slave %d\n", i);
+			}
+		} else {
+			if (mlx4_GEN_EQE(dev, slave, eqe))
+				mlx4_warn(dev, "Failed to generate event "
+					       "for slave %d\n", slave);
+		}
+		++slave_eq->cons;
+	}
+}
+
+
+static void slave_event(struct mlx4_dev *dev, u8 slave, struct mlx4_eqe *eqe)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_event_eq *slave_eq = &priv->mfunc.master.slave_eq;
+	struct mlx4_eqe *s_eqe =
+		&slave_eq->event_eqe[slave_eq->prod & (SLAVE_EVENT_EQ_SIZE - 1)];
+
+	if ((!!(s_eqe->owner & 0x80)) ^ (!!(slave_eq->prod & SLAVE_EVENT_EQ_SIZE))) {
+		mlx4_warn(dev, "Master failed to generate an EQE for slave: %d. "
+			  "No free EQE on slave events queue\n", slave);
+		return;
+	}
+
+	memcpy(s_eqe, eqe, sizeof(struct mlx4_eqe) - 1);
+	s_eqe->slave_id = slave;
+	/* ensure all information is written before setting the ownersip bit */
+	wmb();
+	s_eqe->owner = !!(slave_eq->prod & SLAVE_EVENT_EQ_SIZE) ? 0x0 : 0x80;
+	++slave_eq->prod;
+
+	queue_work(priv->mfunc.master.comm_wq, &priv->mfunc.master.slave_event_work);
+}
+
+static void mlx4_slave_event(struct mlx4_dev *dev, int slave, struct mlx4_eqe* eqe)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_state *s_slave = &priv->mfunc.master.slave_state[slave];
+
+	if (!s_slave->active) {
+		mlx4_warn(dev, "Trying to pass event to inactive slave\n");
+		return;
+	}
+
+	slave_event(dev, slave, eqe);
+}
+
+static void mlx4_slave_event_all(struct mlx4_dev *dev, struct mlx4_eqe* eqe)
+{
+	slave_event(dev, ALL_SLAVES, eqe);
+}
+
+int mlx4_GET_EVENT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						 struct mlx4_cmd_mailbox *inbox,
+						 struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_state *ctx = &priv->mfunc.master.slave_state[slave];
+	unsigned long flags;
+
+	spin_lock_irqsave(&ctx->lock, flags);
+	if (ctx->eq_ci == ctx->eq_pi) {
+		vhcr->out_param = MLX4_EVENT_TYPE_NONE;
+	} else if ((u16) (ctx->eq_pi - ctx->eq_ci) > MLX4_MFUNC_MAX_EQES) {
+		ctx->eq_ci = ctx->eq_pi - MLX4_MFUNC_MAX_EQES;
+		vhcr->out_param = MLX4_EVENT_TYPE_EQ_OVERFLOW;
+	} else {
+		vhcr->out_param = ctx->eq[ctx->eq_ci & MLX4_MFUNC_EQE_MASK].type |
+				  ((u64) ctx->eq[ctx->eq_ci & MLX4_MFUNC_EQE_MASK].port << 8) |
+				  ((u64) ctx->eq[ctx->eq_ci & MLX4_MFUNC_EQE_MASK].param << 32);
+		++ctx->eq_ci;
+	}
+	spin_unlock_irqrestore(&ctx->lock, flags);
+	return 0;
+}
+
+void mlx4_update_vep_config(struct work_struct *work)
+{
+	struct mlx4_mfunc_master_ctx *master = container_of(work,
+							   struct mlx4_mfunc_master_ctx,
+							   vep_config_work);
+	struct mlx4_mfunc *mfunc = container_of(master, struct mlx4_mfunc, master);
+	struct mlx4_priv *priv = container_of(mfunc, struct mlx4_priv, mfunc);
+	struct mlx4_dev *dev = &priv->dev;
+	struct mlx4_vep_cfg vep_cfg;
+	struct mlx4_eqe new_eqe;
+	int vep_num;
+	u8 pf_num;
+	int port;
+	int i;
+	bool port_updated[MLX4_MAX_PORTS + 1] = {false};
+	u16 vep_config_map;
+
+	spin_lock_irq(&mfunc->master.vep_config_lock);
+	vep_config_map = mfunc->master.vep_config_bitmap;
+	mfunc->master.vep_config_bitmap = 0;
+	spin_unlock_irq(&mfunc->master.vep_config_lock);
+
+	while (vep_config_map) {
+		for (pf_num = 0; pf_num < 16; pf_num++) {
+			if (!(vep_config_map & (1 << pf_num)))
+				continue;
+			vep_num = mfunc->master.slave_state[pf_num].vep_num;
+			port = mfunc->master.slave_state[pf_num].port_num;
+			port_updated[port] = true;
+
+			if (mlx4_QUERY_VEP_CFG(dev, vep_num, port, &vep_cfg)) {
+				mlx4_warn(dev, "failed to read VEP configuration "
+					  "for function %d\n", vep_num);
+				continue;
+			}
+			if (vep_cfg.link != mfunc->master.slave_state[pf_num].vep_cfg.link) {
+				new_eqe.type =  MLX4_EVENT_TYPE_PORT_CHANGE;
+				new_eqe.event.port_change.port = cpu_to_be32(port << 28);
+				new_eqe.subtype = vep_cfg.link ?
+					MLX4_PORT_CHANGE_SUBTYPE_ACTIVE :
+					MLX4_PORT_CHANGE_SUBTYPE_DOWN;
+				if (priv->link_up[port]) {
+					if (pf_num == dev->caps.function)
+						mlx4_dispatch_event(dev, vep_cfg.link ?
+								    MLX4_DEV_EVENT_PORT_UP :
+								    MLX4_DEV_EVENT_PORT_DOWN,
+								    port);
+					else
+						mlx4_slave_event(dev, pf_num,
+								 &new_eqe);
+				}
+				mfunc->master.slave_state[pf_num].vep_cfg.link = vep_cfg.link;
+	
+			}
+	
+			if (vep_cfg.mac != mfunc->master.slave_state[pf_num].vep_cfg.mac) {
+				mfunc->master.slave_state[pf_num].vep_cfg.mac = vep_cfg.mac;
+				if (pf_num == dev->caps.function) {
+					dev->caps.def_mac[port] = vep_cfg.mac;
+					mlx4_dispatch_event(dev,
+							    MLX4_EVENT_TYPE_MAC_UPDATE,
+							    port);
+				} else {
+					new_eqe.type = MLX4_EVENT_TYPE_MAC_UPDATE;
+					new_eqe.event.mac_update.port = port;
+					new_eqe.event.mac_update.mac = cpu_to_be64(vep_cfg.mac);
+					mlx4_slave_event(dev, pf_num, &new_eqe);
+				}
+			}
+		}
+		spin_lock_irq(&mfunc->master.vep_config_lock);
+		vep_config_map = mfunc->master.vep_config_bitmap;
+		mfunc->master.vep_config_bitmap = 0;
+		spin_unlock_irq(&mfunc->master.vep_config_lock);
+	}
+	for (i = 1; i <= dev->caps.num_ports; i++) {
+		if (port_updated[i])
+			mlx4_update_uplink_arbiter(dev, i);
+	}
+}
+
 static int mlx4_eq_int(struct mlx4_dev *dev, struct mlx4_eq *eq)
 {
+	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_eqe *eqe;
 	int cqn;
 	int eqes_found = 0;
 	int set_ci = 0;
 	int port;
+	int slave;
+	int ret;
+	int i;
+	u8 vep_num;
+	u8 pf_num = 0;
+	u64 mac;
 
-	while ((eqe = next_eqe_sw(eq))) {
+	while ((eqe = next_eqe_sw(eq, dev->caps.eqe_factor))) {
 		/*
 		 * Make sure we read EQ entry contents after we've
 		 * checked the ownership bit.
@@ -189,14 +352,27 @@ static int mlx4_eq_int(struct mlx4_dev *
 		case MLX4_EVENT_TYPE_PATH_MIG_FAILED:
 		case MLX4_EVENT_TYPE_WQ_INVAL_REQ_ERROR:
 		case MLX4_EVENT_TYPE_WQ_ACCESS_ERROR:
-			mlx4_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) & 0xffffff,
-				      eqe->type);
+			if (mlx4_is_master(dev)) {
+				/* forward only to slave owning the QP */
+				ret = mlx4_get_slave_from_resource_id(dev, RES_QP, eqe->event.qp.qpn, &slave);
+				if (!ret)
+					mlx4_slave_event(dev, slave, eqe);
+
+			} else
+				mlx4_qp_event(dev, be32_to_cpu(eqe->event.qp.qpn) &
+						   0xffffff, eqe->type);
 			break;
 
 		case MLX4_EVENT_TYPE_SRQ_LIMIT:
 		case MLX4_EVENT_TYPE_SRQ_CATAS_ERROR:
-			mlx4_srq_event(dev, be32_to_cpu(eqe->event.srq.srqn) & 0xffffff,
-				      eqe->type);
+			if (mlx4_is_master(dev)) {
+				/* forward only to slave owning the SRQ */
+				ret = mlx4_get_slave_from_resource_id(dev, RES_SRQ, eqe->event.srq.srqn, &slave);
+				if (!ret)
+					mlx4_slave_event(dev, slave, eqe);
+			} else
+				mlx4_srq_event(dev, be32_to_cpu(eqe->event.srq.srqn) &
+						    0xffffff, eqe->type);
 			break;
 
 		case MLX4_EVENT_TYPE_CMD:
@@ -209,13 +385,35 @@ static int mlx4_eq_int(struct mlx4_dev *
 		case MLX4_EVENT_TYPE_PORT_CHANGE:
 			port = be32_to_cpu(eqe->event.port_change.port) >> 28;
 			if (eqe->subtype == MLX4_PORT_CHANGE_SUBTYPE_DOWN) {
+				priv->link_up[port] = false;
 				mlx4_dispatch_event(dev, MLX4_DEV_EVENT_PORT_DOWN,
 						    port);
 				mlx4_priv(dev)->sense.do_sense_port[port] = 1;
+				if (mlx4_is_master(dev))
+					mlx4_slave_event_all(dev, eqe);
 			} else {
-				mlx4_dispatch_event(dev, MLX4_DEV_EVENT_PORT_UP,
-						    port);
-				mlx4_priv(dev)->sense.do_sense_port[port] = 0;
+				struct mlx4_slave_state *s_state = priv->mfunc.master.slave_state;
+				priv->link_up[port] = true;
+				/* Link UP event is acceptable only in case VEP link is enabled*/
+				if (!mlx4_is_master(dev) ||
+				    s_state[dev->caps.function].vep_cfg.link) {
+					mlx4_dispatch_event(dev, MLX4_DEV_EVENT_PORT_UP,
+							    port);
+					mlx4_priv(dev)->sense.do_sense_port[port] = 0;
+				}
+				if (mlx4_is_master(dev)) {
+					u8 vep_num;
+					for (i = 0; i < dev->num_slaves; i++) {
+						if (i == dev->caps.function || !(s_state[i].active))
+							continue;
+						vep_num = s_state[i].pf_num;
+						spin_lock(&priv->mfunc.master.vep_config_lock);
+						if (s_state[vep_num].vep_cfg.link)
+							mlx4_slave_event(dev, i, eqe);
+						spin_unlock(&priv->mfunc.master.vep_config_lock);
+					}
+				}
+
 			}
 			break;
 
@@ -224,14 +422,79 @@ static int mlx4_eq_int(struct mlx4_dev *
 				  eqe->event.cq_err.syndrome == 1 ?
 				  "overrun" : "access violation",
 				  be32_to_cpu(eqe->event.cq_err.cqn) & 0xffffff);
-			mlx4_cq_event(dev, be32_to_cpu(eqe->event.cq_err.cqn),
-				      eqe->type);
+			if (mlx4_is_master(dev)) {
+				ret = mlx4_get_slave_from_resource_id(dev, RES_CQ,
+								      eqe->event.cq_err.cqn,
+								      &slave);
+				if (!ret)
+					mlx4_slave_event(dev, slave, eqe);
+			} else
+				mlx4_cq_event(dev, be32_to_cpu(eqe->event.cq_err.cqn),
+									   eqe->type);
 			break;
 
 		case MLX4_EVENT_TYPE_EQ_OVERFLOW:
 			mlx4_warn(dev, "EQ overrun on EQN %d\n", eq->eqn);
 			break;
 
+		case MLX4_EVENT_TYPE_OP_REQUIRED:
+			atomic_inc(&priv->opreq_count);
+			/* FW commands can't be executed from interrupt context
+			   working in deferred task */
+			queue_work(priv->opreq_queue, &priv->opreq_task);
+			break;
+
+		case MLX4_EVENT_TYPE_COMM_CHANNEL:
+			if (!mlx4_is_master(dev)) {
+				mlx4_warn(dev, "Received comm channel event "
+					       "for non master device\n");
+				break;
+			}
+			memcpy(&priv->mfunc.master.comm_arm_bit_vector,
+			       eqe->event.comm_channel_arm.bit_vec,
+			       sizeof(u32) * COMM_CHANNEL_BIT_ARRAY_SIZE);
+			queue_work(priv->mfunc.master.comm_wq,
+				   &priv->mfunc.master.comm_work);
+			break;
+
+		case MLX4_EVENT_TYPE_MAC_UPDATE:
+			port = eqe->event.mac_update.port;
+			mac = be64_to_cpu(eqe->event.mac_update.mac);
+			dev->caps.def_mac[port] = mac;
+			mlx4_dispatch_event(dev, MLX4_EVENT_TYPE_MAC_UPDATE, port);
+			break;
+
+		case MLX4_EVENT_TYPE_VEP_UPDATE:
+			if (!mlx4_is_master(dev)) {
+				mlx4_warn(dev, "Non-master function received"
+					       "VEP_UPDATE event\n");
+				break;
+			}
+			vep_num = eqe->event.vep_config.vep_num;
+			port = eqe->event.vep_config.port;
+			for (i = 0; i < 16; i++) {
+				if (priv->mfunc.master.slave_state[i].vep_num == vep_num &&
+				    priv->mfunc.master.slave_state[i].port_num == port) {
+					pf_num = i;
+					break;
+				}
+			}
+			spin_lock(&priv->mfunc.master.vep_config_lock);
+			priv->mfunc.master.vep_config_bitmap |= 1 << pf_num;
+			spin_unlock(&priv->mfunc.master.vep_config_lock);
+			queue_work(priv->mfunc.master.comm_wq, &priv->mfunc.master.vep_config_work);
+			break;
+
+		case MLX4_EVENT_TYPE_FATAL_WARNING:
+			if (eqe->subtype == MLX4_FATAL_WARNING_SUBTYPE_WARMING) {
+				mlx4_err(dev, "Temperature Threshold was reached! "
+					      "Threshold: %d celsius degrees; "
+					      "Current Temperature: %d\n",
+					 be16_to_cpu(eqe->event.warming.warning_threshold),
+					 be16_to_cpu(eqe->event.warming.current_temperature));
+			}
+			break;
+
 		case MLX4_EVENT_TYPE_EEC_CATAS_ERROR:
 		case MLX4_EVENT_TYPE_ECC_DETECT:
 		default:
@@ -288,6 +551,30 @@ static irqreturn_t mlx4_msi_x_interrupt(
 	return IRQ_HANDLED;
 }
 
+int mlx4_MAP_EQ_wrapper(struct mlx4_dev *dev, int slave,
+			struct mlx4_vhcr *vhcr,
+			struct mlx4_cmd_mailbox *inbox,
+			struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_slave_event_eq_info *event_eq =
+		&priv->mfunc.master.slave_state[slave].event_eq;
+	u32 in_modifier = vhcr->in_modifier;
+	u32 eqn = in_modifier & 0x1FF;
+	u64 in_param =  vhcr->in_param;
+
+       if (in_modifier >> 31) {
+	       /* unmap */
+	       event_eq->event_type &= ~in_param;
+	       return 0;
+       }
+
+       event_eq->eqn = eqn;
+       event_eq->event_type = in_param;
+
+       return 0;
+}
+
 static int mlx4_MAP_EQ(struct mlx4_dev *dev, u64 event_mask, int unmap,
 			int eq_num)
 {
@@ -298,15 +585,15 @@ static int mlx4_MAP_EQ(struct mlx4_dev *
 static int mlx4_SW2HW_EQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			 int eq_num)
 {
-	return mlx4_cmd(dev, mailbox->dma, eq_num, 0, MLX4_CMD_SW2HW_EQ,
-			MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd(dev, mailbox->dma | dev->caps.function, eq_num, 0,
+			MLX4_CMD_SW2HW_EQ, MLX4_CMD_TIME_CLASS_A);
 }
 
 static int mlx4_HW2SW_EQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			 int eq_num)
 {
-	return mlx4_cmd_box(dev, 0, mailbox->dma, eq_num, 0, MLX4_CMD_HW2SW_EQ,
-			    MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd_box(dev, dev->caps.function, mailbox->dma, eq_num,
+			    0, MLX4_CMD_HW2SW_EQ, MLX4_CMD_TIME_CLASS_A);
 }
 
 static int mlx4_num_eq_uar(struct mlx4_dev *dev)
@@ -316,8 +603,8 @@ static int mlx4_num_eq_uar(struct mlx4_d
 	 * we need to map, take the difference of highest index and
 	 * the lowest index we'll use and add 1.
 	 */
-	return (dev->caps.num_comp_vectors + 1 + dev->caps.reserved_eqs) / 4 -
-		dev->caps.reserved_eqs / 4 + 1;
+	return (dev->caps.num_comp_vectors + 1 + dev->caps.reserved_eqs +
+		 dev->caps.poolsz)/4 - dev->caps.reserved_eqs/4 + 1;
 }
 
 static void __iomem *mlx4_get_eq_uar(struct mlx4_dev *dev, struct mlx4_eq *eq)
@@ -357,7 +644,8 @@ static int mlx4_create_eq(struct mlx4_de
 
 	eq->dev   = dev;
 	eq->nent  = roundup_pow_of_two(max(nent, 2));
-	npages = PAGE_ALIGN(eq->nent * MLX4_EQ_ENTRY_SIZE) / PAGE_SIZE;
+	/* CX3 is capable of extending the CQE\EQE from 32 to 64 bytes */
+	npages = PAGE_ALIGN(eq->nent * (MLX4_EQ_ENTRY_SIZE << dev->caps.eqe_factor)) / PAGE_SIZE;
 
 	eq->page_list = kmalloc(npages * sizeof *eq->page_list,
 				GFP_KERNEL);
@@ -369,7 +657,7 @@ static int mlx4_create_eq(struct mlx4_de
 
 	dma_list = kmalloc(npages * sizeof *dma_list, GFP_KERNEL);
 	if (!dma_list)
-		goto err_out_free;
+		goto err_out_page_list;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -446,8 +734,10 @@ err_out_free_pages:
 	mlx4_free_cmd_mailbox(dev, mailbox);
 
 err_out_free:
+	kfree(dma_list);
+
+err_out_page_list:
 	kfree(eq->page_list);
-	kfree(dma_list);
 
 err_out:
 	return err;
@@ -459,8 +749,9 @@ static void mlx4_free_eq(struct mlx4_dev
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_cmd_mailbox *mailbox;
 	int err;
-	int npages = PAGE_ALIGN(MLX4_EQ_ENTRY_SIZE * eq->nent) / PAGE_SIZE;
 	int i;
+	/* CX3 is capable of extending the CQE\EQE from 32 to 64 bytes */
+	int npages = PAGE_ALIGN((MLX4_EQ_ENTRY_SIZE << dev->caps.eqe_factor) * eq->nent) / PAGE_SIZE;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -483,7 +774,7 @@ static void mlx4_free_eq(struct mlx4_dev
 
 	mlx4_mtt_cleanup(dev, &eq->mtt);
 	for (i = 0; i < npages; ++i)
-		pci_free_consistent(dev->pdev, PAGE_SIZE,
+		dma_free_coherent(&dev->pdev->dev, PAGE_SIZE,
 				    eq->page_list[i].buf,
 				    eq->page_list[i].map);
 
@@ -495,16 +786,32 @@ static void mlx4_free_eq(struct mlx4_dev
 static void mlx4_free_irqs(struct mlx4_dev *dev)
 {
 	struct mlx4_eq_table *eq_table = &mlx4_priv(dev)->eq_table;
-	int i;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int	i, vec;
 
 	if (eq_table->have_irq)
 		free_irq(dev->pdev->irq, dev);
+
 	for (i = 0; i < dev->caps.num_comp_vectors + 1; ++i)
 		if (eq_table->eq[i].have_irq) {
 			free_irq(eq_table->eq[i].irq, eq_table->eq + i);
 			eq_table->eq[i].have_irq = 0;
 		}
 
+	for (i = 0; i < dev->caps.poolsz; i++) {
+		/*
+		 * Freeing the assigned irq's
+		 * all bits should be 0, but we need to validate
+		 */
+		if (priv->msix_ctl.pool_bm & 1ULL << i) {
+			/* NO need protecting*/
+			vec = dev->caps.num_comp_vectors + 1 + i;
+			free_irq(priv->eq_table.eq[vec].irq,
+				 &priv->eq_table.eq[vec]);
+		}
+	}
+
+
 	kfree(eq_table->irq_names);
 }
 
@@ -567,31 +874,33 @@ int mlx4_init_eq_table(struct mlx4_dev *
 	for (i = 0; i < mlx4_num_eq_uar(dev); ++i)
 		priv->eq_table.uar_map[i] = NULL;
 
-	err = mlx4_map_clr_int(dev);
-	if (err)
-		goto err_out_bitmap;
+	if (!mlx4_is_slave(dev)) {
+		err = mlx4_map_clr_int(dev);
+		if (err)
+			goto err_out_bitmap;
 
-	priv->eq_table.clr_mask =
-		swab32(1 << (priv->eq_table.inta_pin & 31));
-	priv->eq_table.clr_int  = priv->clr_base +
-		(priv->eq_table.inta_pin < 32 ? 4 : 0);
+		priv->eq_table.clr_mask =
+			swab32(1 << (priv->eq_table.inta_pin & 31));
+		priv->eq_table.clr_int  = priv->clr_base +
+			(priv->eq_table.inta_pin < 32 ? 4 : 0);
+	}
 
-	priv->eq_table.irq_names =
-		kmalloc(MLX4_IRQNAME_SIZE * (dev->caps.num_comp_vectors + 1),
-			GFP_KERNEL);
+	priv->eq_table.irq_names = kmalloc(32 * (dev->caps.num_comp_vectors
+						 + 1 + dev->caps.poolsz), GFP_KERNEL);
 	if (!priv->eq_table.irq_names) {
 		err = -ENOMEM;
-		goto err_out_bitmap;
+		i = 0;
+		goto err_out_unmap;
 	}
 
 	for (i = 0; i < dev->caps.num_comp_vectors; ++i) {
-		err = mlx4_create_eq(dev, dev->caps.num_cqs + MLX4_NUM_SPARE_EQE,
+		err = mlx4_create_eq(dev, dev->caps.num_cqs -
+				     dev->caps.reserved_cqs +
+					  MLX4_NUM_SPARE_EQE,
 				     (dev->flags & MLX4_FLAG_MSI_X) ? i : 0,
 				     &priv->eq_table.eq[i]);
-		if (err) {
-			--i;
+		if (err)
 			goto err_out_unmap;
-		}
 	}
 
 	err = mlx4_create_eq(dev, MLX4_NUM_ASYNC_EQE + MLX4_NUM_SPARE_EQE,
@@ -600,26 +909,33 @@ int mlx4_init_eq_table(struct mlx4_dev *
 	if (err)
 		goto err_out_comp;
 
+	/*if poolsize is 0 this loop will not run*/
+	for (i = dev->caps.num_comp_vectors + 1;
+	      i < dev->caps.num_comp_vectors + dev->caps.poolsz + 1; ++i) {
+
+		err = mlx4_create_eq(dev, dev->caps.num_cqs -
+					  dev->caps.reserved_cqs +
+					  MLX4_NUM_SPARE_EQE,
+				     (dev->flags & MLX4_FLAG_MSI_X) ? i : 0,
+				     &priv->eq_table.eq[i]);
+		if (err)
+			goto err_out_unmap;
+			/* this is the right flag to go to*/
+	}
+
+
 	if (dev->flags & MLX4_FLAG_MSI_X) {
+		static const char async_eq_name[] = DRV_NAME "(async)";
 		const char *eq_name;
 
 		for (i = 0; i < dev->caps.num_comp_vectors + 1; ++i) {
 			if (i < dev->caps.num_comp_vectors) {
-				snprintf(priv->eq_table.irq_names +
-					 i * MLX4_IRQNAME_SIZE,
-					 MLX4_IRQNAME_SIZE,
-					 "mlx4-comp-%d@pci:%s", i,
-					 pci_name(dev->pdev));
-			} else {
-				snprintf(priv->eq_table.irq_names +
-					 i * MLX4_IRQNAME_SIZE,
-					 MLX4_IRQNAME_SIZE,
-					 "mlx4-async@pci:%s",
-					 pci_name(dev->pdev));
-			}
+				snprintf(priv->eq_table.irq_names + i * 16, 16,
+					 "eth-mlx4-%d", i);
+				eq_name = priv->eq_table.irq_names + i * 16;
+			} else
+				eq_name = async_eq_name;
 
-			eq_name = priv->eq_table.irq_names +
-				  i * MLX4_IRQNAME_SIZE;
 			err = request_irq(priv->eq_table.eq[i].irq,
 					  mlx4_msi_x_interrupt, 0, eq_name,
 					  priv->eq_table.eq + i);
@@ -629,12 +945,8 @@ int mlx4_init_eq_table(struct mlx4_dev *
 			priv->eq_table.eq[i].have_irq = 1;
 		}
 	} else {
-		snprintf(priv->eq_table.irq_names,
-			 MLX4_IRQNAME_SIZE,
-			 DRV_NAME "@pci:%s",
-			 pci_name(dev->pdev));
 		err = request_irq(dev->pdev->irq, mlx4_interrupt,
-				  IRQF_SHARED, priv->eq_table.irq_names, dev);
+				  IRQF_SHARED, DRV_NAME, dev);
 		if (err)
 			goto err_out_async;
 
@@ -656,14 +968,15 @@ err_out_async:
 	mlx4_free_eq(dev, &priv->eq_table.eq[dev->caps.num_comp_vectors]);
 
 err_out_comp:
-	i = dev->caps.num_comp_vectors - 1;
+	i = dev->caps.num_comp_vectors;
 
 err_out_unmap:
-	while (i >= 0) {
+	while (i > 0) {
+		--i;
 		mlx4_free_eq(dev, &priv->eq_table.eq[i]);
-		--i;
 	}
-	mlx4_unmap_clr_int(dev);
+	if (!mlx4_is_slave(dev))
+		mlx4_unmap_clr_int(dev);
 	mlx4_free_irqs(dev);
 
 err_out_bitmap:
@@ -685,10 +998,11 @@ void mlx4_cleanup_eq_table(struct mlx4_d
 
 	mlx4_free_irqs(dev);
 
-	for (i = 0; i < dev->caps.num_comp_vectors + 1; ++i)
+	for (i = 0; i < dev->caps.num_comp_vectors + dev->caps.poolsz + 1; ++i)
 		mlx4_free_eq(dev, &priv->eq_table.eq[i]);
 
-	mlx4_unmap_clr_int(dev);
+	if (!mlx4_is_slave(dev))
+		mlx4_unmap_clr_int(dev);
 
 	for (i = 0; i < mlx4_num_eq_uar(dev); ++i)
 		if (priv->eq_table.uar_map[i])
@@ -698,3 +1012,107 @@ void mlx4_cleanup_eq_table(struct mlx4_d
 
 	kfree(priv->eq_table.uar_map);
 }
+
+/* A test that verifies that we can accept interrupts on all
+ * the irq vectors of the device.
+ * Interrupts are checked using the NOP command.
+ */
+int mlx4_test_interrupts(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int i;
+	int err;
+
+	err = mlx4_NOP(dev);
+	/* When not in MSI_X, there is only one irq to check */
+	if (!(dev->flags & MLX4_FLAG_MSI_X) || mlx4_is_slave(dev))
+		return err;
+
+	/* A loop over all completion vectors, for each vector we will check
+	 * whether it works by mapping command completions to that vector
+	 * and performing a NOP command
+	 */
+	for(i = 0; !err && (i < dev->caps.num_comp_vectors); ++i) {
+		/* Temporary use polling for command completions */
+		mlx4_cmd_use_polling(dev);
+
+		/* Map the new eq to handle all asyncronous events */
+		err = mlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 0,
+				  priv->eq_table.eq[i].eqn);
+		if (err) {
+			mlx4_warn(dev, "Failed mapping eq for interrupt test\n");
+			mlx4_cmd_use_events(dev);
+			break;
+		}
+
+		/* Go back to using events */
+		mlx4_cmd_use_events(dev);
+		err = mlx4_NOP(dev);
+	}
+
+	/* Return to default */
+	mlx4_MAP_EQ(dev, MLX4_ASYNC_EVENT_MASK, 0,
+		    priv->eq_table.eq[dev->caps.num_comp_vectors].eqn);
+	return err;
+}
+EXPORT_SYMBOL(mlx4_test_interrupts);
+
+int mlx4_assign_eq(struct mlx4_dev *dev, char* name, int * vector)
+{
+
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int vec = 0, err = 0, i;
+
+	mutex_lock(&priv->msix_ctl.pool_lock);
+	for (i = 0; !vec && i < dev->caps.poolsz; i++) {
+		if (~priv->msix_ctl.pool_bm & 1ULL << i) {
+			priv->msix_ctl.pool_bm |= 1ULL << i;
+			vec = dev->caps.num_comp_vectors + 1 + i;
+			sprintf(&priv->eq_table.irq_names[vec<<5], "%s", name);
+			err = request_irq(priv->eq_table.eq[vec].irq,
+					  mlx4_msi_x_interrupt, 0,
+					  &priv->eq_table.irq_names[vec<<5],
+					  priv->eq_table.eq + vec);
+			if (err) {
+				/*zero out bit by fliping it*/
+				priv->msix_ctl.pool_bm ^= 1 << i;
+				vec = 0;
+				continue;
+				/*we dont want to break here*/
+			}
+			eq_set_ci(&priv->eq_table.eq[vec], 1);
+		}
+	}
+	mutex_unlock(&priv->msix_ctl.pool_lock);
+
+	if (vec) {
+		*vector = vec;
+	} else {
+		*vector = 0;
+		err = (i == dev->caps.poolsz) ? -ENOSPC : err;
+	}
+	return err;
+}
+EXPORT_SYMBOL(mlx4_assign_eq);
+
+void mlx4_release_eq(struct mlx4_dev *dev, int vec)
+{
+	struct mlx4_priv *priv 	= mlx4_priv(dev);
+	/*bm index*/
+	int i 			= vec - dev->caps.num_comp_vectors - 1;
+
+	if (likely(i >= 0)) {
+		/*sanity check , making sure were not trying to free irq's
+		  Belonging to a legacy EQ*/
+		mutex_lock(&priv->msix_ctl.pool_lock);
+		if (priv->msix_ctl.pool_bm & 1ULL << i) {
+			free_irq(priv->eq_table.eq[vec].irq,
+				 &priv->eq_table.eq[vec]);
+			priv->msix_ctl.pool_bm &= ~(1ULL << i);
+		}
+		mutex_unlock(&priv->msix_ctl.pool_lock);
+	}
+
+}
+EXPORT_SYMBOL(mlx4_release_eq);
+
diff -r c23d1fc7e422 drivers/net/mlx4/fw.c
--- a/drivers/net/mlx4/fw.c
+++ b/drivers/net/mlx4/fw.c
@@ -32,8 +32,8 @@
  * SOFTWARE.
  */
 
+#include <linux/etherdevice.h>
 #include <linux/mlx4/cmd.h>
-#include <linux/cache.h>
 
 #include "fw.h"
 #include "icm.h"
@@ -51,6 +51,10 @@ static int enable_qos;
 module_param(enable_qos, bool, 0444);
 MODULE_PARM_DESC(enable_qos, "Enable Quality of Service support in the HCA (default: off)");
 
+static int mlx4_pre_t11_mode = 0;
+module_param_named(enable_pre_t11_mode, mlx4_pre_t11_mode, int, 0644);
+MODULE_PARM_DESC(enable_pre_t11_mode, "For FCoXX, enable pre-t11 mode if non-zero (default: 0)");
+
 #define MLX4_GET(dest, source, offset)				      \
 	do {							      \
 		void *__p = (char *) (source) + (offset);	      \
@@ -75,7 +79,7 @@ MODULE_PARM_DESC(enable_qos, "Enable Qua
 		}						      \
 	} while (0)
 
-static void dump_dev_cap_flags(struct mlx4_dev *dev, u32 flags)
+static void dump_dev_cap_flags(struct mlx4_dev *dev, u64 flags)
 {
 	static const char *fname[] = {
 		[ 0] = "RC transport",
@@ -97,13 +101,17 @@ static void dump_dev_cap_flags(struct ml
 		[20] = "Address vector port checking support",
 		[21] = "UD multicast support",
 		[24] = "Demand paging support",
-		[25] = "Router support"
+		[25] = "Router support",
+		[30] = "IBoE support",
+		[48] = "Basic counters support",
+		[49] = "Extended counters support",
+		[62] = "64 byte CQE support",
 	};
 	int i;
 
 	mlx4_dbg(dev, "DEV_CAP flags:\n");
 	for (i = 0; i < ARRAY_SIZE(fname); ++i)
-		if (fname[i] && (flags & (1 << i)))
+		if (fname[i] && (flags & (1LL << i)))
 			mlx4_dbg(dev, "    %s\n", fname[i]);
 }
 
@@ -135,15 +143,335 @@ int mlx4_MOD_STAT_CFG(struct mlx4_dev *d
 	return err;
 }
 
+int mlx4_QUERY_VEP_CFG(struct mlx4_dev *dev, u8 vep_num, u8 port,
+		       struct mlx4_vep_cfg *cfg)
+{
+	int err;
+	u32 in_mod;
+	u64 output;
+
+#define QUERY_VEP_CFG_OPMOD		3
+
+#define QUERY_VEP_CFG_INMOD		(2 << 28)
+#define QUERY_VEP_CFG_INMOD_VEP_OFFSET	16
+#define QUERY_VEP_CFG_INMOD_PORT_OFFSET	8
+
+#define QUERY_VEP_CFG_MAC_OFFSET	0x90
+#define QUERY_VEP_CFG_LINK_OFFSET	0xa0
+
+
+	in_mod = QUERY_VEP_CFG_INMOD | (vep_num << QUERY_VEP_CFG_INMOD_VEP_OFFSET) |
+		(port << QUERY_VEP_CFG_INMOD_PORT_OFFSET);
+
+	err = mlx4_cmd_imm(dev, 0, &output, in_mod | QUERY_VEP_CFG_MAC_OFFSET,
+			   QUERY_VEP_CFG_OPMOD, MLX4_CMD_MOD_STAT_CFG,
+			   MLX4_CMD_TIME_CLASS_A);
+	if (err) {
+		mlx4_err(dev, "Failed to retrieve mac for function %d\n", vep_num);
+		return err;
+	}
+	cfg->mac = output & 0xffffffffffffULL;
+
+	err = mlx4_cmd_imm(dev, 0, &output, in_mod | QUERY_VEP_CFG_LINK_OFFSET,
+			   QUERY_VEP_CFG_OPMOD, MLX4_CMD_MOD_STAT_CFG,
+			   MLX4_CMD_TIME_CLASS_A);
+	if (err) {
+		mlx4_err(dev, "Failed to retrieve link for function %d\n", vep_num);
+		return err;
+	}
+	cfg->link = (output >> 32) & 1;
+
+	return 0;
+}
+
+static int get_slave(struct mlx4_dev* dev, u8 port, u8 vep_num)
+{
+	struct mlx4_priv* priv = mlx4_priv(dev);
+	struct mlx4_slave_state* slaves = priv->mfunc.master.slave_state;
+	int i;
+	
+	for(i = 0; i < dev->num_slaves; ++i)
+		if ((slaves[i].port_num == port) && (slaves[i].vep_num == vep_num))
+			return i;
+
+		return -1;
+}
+
+int mlx4_update_uplink_arbiter(struct mlx4_dev *dev, u8 port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_cmd_mailbox *mailbox;
+	u32 in_mod;
+	int err;
+	int i;
+	u8 *buf;
+	u64 *buf64;
+	int slave;
+	u8 bw_units, bw_value;
+
+#define QUERY_UPLINK_ARB_OPMOD		2
+#define QUERY_UPLINK_ARB_INMOD		(3 << 28)
+#define QUERY_UPLINK_ARB_PORT_OFFSET	8
+#define SET_PORT_ARB_MOD		2
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	buf = mailbox->buf;
+
+	in_mod = QUERY_UPLINK_ARB_INMOD | (port << QUERY_UPLINK_ARB_PORT_OFFSET);
+	err = mlx4_cmd_box(dev, 0, mailbox->dma, in_mod, QUERY_UPLINK_ARB_OPMOD,
+			   MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+	if (err) {
+		mlx4_err(dev, "Failed to read uplink arbiter configuration "
+			      "for port %d\n", port);
+		goto out;
+	}
+
+#define UPLINK_VEP_MODE		2
+#define VEP_CONFIG_OFFSET	0x40
+#define VEP_CONFIG_SIZE		0x8
+#define VEP_ENABLE_MASK		(1ull << 63 | 1ull << 39 | 1ull << 31)
+
+	if (buf[3] != UPLINK_VEP_MODE) {
+		/* not running in vep mode, nothing to do */
+		/* TODO: config ets mode */
+		mlx4_priv(dev)->vep_mode[port] = false;
+		goto out;
+	}
+
+	mlx4_priv(dev)->vep_mode[port] = true;
+	
+	buf[0] = 1 << 7;
+        for (i = 0; i < priv->mfunc.master.num_veps[port]; i++) {
+		slave = get_slave(dev, port, i);
+		if (slave < 0) {
+			mlx4_err(dev, "No function for Vep %d\n", i);
+			goto out;
+		}
+		buf64 = (u64 *) (&buf[VEP_CONFIG_OFFSET + i * VEP_CONFIG_SIZE]);
+		*buf64 |= cpu_to_be64(VEP_ENABLE_MASK);
+		bw_units = ((u8 *) buf64)[5] & 0xf;
+		bw_value = ((u8 *) buf64)[7];
+		switch(bw_units) {
+#define VEP_UNITS_100Mbps	3
+#define VEP_UNITS_1Gbps		4
+		case VEP_UNITS_100Mbps:
+			priv->mfunc.master.slave_state[slave].vep_cfg.bw_value = bw_value;
+			break;
+		case VEP_UNITS_1Gbps:
+			priv->mfunc.master.slave_state[slave].vep_cfg.bw_value = bw_value;
+			break;
+		default:
+			mlx4_warn(dev, "Illegal bw units for vep %d on port %d\n", i, port);
+		}
+	}
+	err = mlx4_cmd(dev, mailbox->dma, (u32) port, SET_PORT_ARB_MOD,
+		       MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_A);
+	if (err)
+		mlx4_err(dev, "Failed to set uplink arbiter configuration "
+			      "for port %d\n", port);
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+static bool is_mfunc(struct mlx4_dev *dev)
+{
+	u8 output[8];
+	int err;
+
+	err = mlx4_cmd_imm(dev, 0, (u64 *) output, 0xa0, 0x3,
+			   MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+	if (err) {
+		mlx4_warn(dev, "Failed to qurey multi/single function mode: %d\n", err);
+		return false;
+	}
+
+	if (output[4] & 0x10)
+		return true;
+	return false;
+
+}
+
+int mlx4_set_vep_maps(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u8 num_veps;
+	u8 pf_num;
+	u8 port, i;
+	u64 output;
+	u32 in_mod;
+	int err;
+
+	for (port = 1; port <= dev->caps.num_ports; port++) {
+		in_mod = (1ul << 28) | (u32) port << 8 | 0x98;
+		err = mlx4_cmd_imm(dev, 0, &output, in_mod, 0x3,
+				   MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			mlx4_err(dev, "Failed to retrieve number of veps\n");
+			return err;
+		}
+		num_veps = (u8) ((output >> 48) & 0x7f);
+		priv->mfunc.master.num_veps[port] = num_veps;
+
+		for (i = 0; i < num_veps; i++) {
+			in_mod = (2ull << 28) | (u32) i << 16 | (u32) port << 8 | 0xa0;
+			err = mlx4_cmd_imm(dev, 0, &output, in_mod, 0x3,
+					   MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+			if (err) {
+				mlx4_err(dev, "failed to retieve func number for vep\n");
+				return err;
+			}
+			pf_num = (u8) ((output >> 48) & 0xff);
+			priv->mfunc.master.slave_state[pf_num].function = pf_num;
+			priv->mfunc.master.slave_state[pf_num].pf_num = pf_num;
+			priv->mfunc.master.slave_state[pf_num].vep_num = i;
+			priv->mfunc.master.slave_state[pf_num].port_num = port;
+		}
+	}
+	mlx4_set_port_mask(dev, &dev->caps, dev->caps.function);
+	return 0;
+}
+
+#define STAT_CLP_OFFSET		0x88
+#define CLP_VER_MASK		0xffff
+static u16 mlx4_QUERY_CLP(struct mlx4_dev* dev)
+{
+	u64 output;
+	int err;
+
+	err = mlx4_cmd_imm(dev, 0, &output, STAT_CLP_OFFSET, 0x3,
+			   MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+
+	if (err) {
+		mlx4_warn(dev, "failed to retrieve clp version : %d", err);
+		return 0;
+	}
+
+	return (u16) ((output >> 32) & CLP_VER_MASK);
+}
+
+static int query_port_common(struct mlx4_dev *dev,
+				  struct mlx4_cmd_mailbox *outbox, u8 port,
+				  u8 function)
+{
+	int err;
+	u8 *buf;
+	u8 pf_num;
+
+	err = mlx4_cmd_box(dev, 0, outbox->dma, port, 0, MLX4_CMD_QUERY_PORT,
+			   MLX4_CMD_TIME_CLASS_B);
+	if (!err) {
+		buf = outbox->buf;
+		pf_num = mlx4_priv(dev)->mfunc.master.slave_state[function].pf_num;
+		buf[0] &= (mlx4_priv(dev)->mfunc.master.slave_state[pf_num].vep_cfg.link) << 7;
+
+		if (mlx4_priv(dev)->vep_mode[port]) {
+			buf[5] = 0xff;
+			buf[25] = mlx4_priv(dev)->mfunc.master.slave_state[pf_num].vep_cfg.bw_value;
+		}
+	}
+	return err;
+}
+
+int mlx4_QUERY_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox)
+{
+	return query_port_common(dev, outbox, vhcr->in_modifier, slave);
+}
+
+int mlx4_QUERY_PORT(struct mlx4_dev *dev, void *ptr, u8 port)
+{
+	struct mlx4_cmd_mailbox *outbox = ptr;
+	if (mlx4_is_master(dev))
+		return query_port_common(dev, outbox, port, dev->caps.function);
+	else
+		return mlx4_cmd_box(dev, 0, outbox->dma, port, 0,
+				    MLX4_CMD_QUERY_PORT, MLX4_CMD_TIME_CLASS_B);
+}
+EXPORT_SYMBOL_GPL(mlx4_QUERY_PORT);
+
+int mlx4_QUERY_SLAVE_CAP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						       struct mlx4_cmd_mailbox *inbox,
+						       struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_mfunc_master_ctx *master = &priv->mfunc.master;
+	struct mlx4_slave_state *slave_st = &master->slave_state[slave];
+	struct mlx4_caps *caps = outbox->buf;
+	u8 pf_num = slave_st->pf_num;
+	int i, err = 0;
+
+	memcpy(caps, &dev->caps, sizeof *caps);
+
+	/* The Master function is in charge for qp1 of al slaves */
+	caps->sqp_demux = 0;
+
+	caps->vep_num = slave_st->vep_num;
+	
+	if (pf_num == slave) {
+		err = mlx4_QUERY_VEP_CFG(dev, slave_st->vep_num,
+					 slave_st->port_num, &slave_st->vep_cfg);
+		if (err)
+			mlx4_warn(dev, "Failed to retreive mac address for vep %d\n", pf_num);
+		else
+			caps->def_mac[slave_st->port_num] = slave_st->vep_cfg.mac;
+	}
+	if (pf_num != slave || err) {
+		for (i = 1; i <= dev->caps.num_ports; ++i)
+			caps->def_mac[i] = dev->caps.def_mac[i] + (slave << 8);
+	}
+
+
+	/* Ports are activated according to physical function number */
+	
+	mlx4_set_port_mask(dev, caps, slave_st->pf_num);
+
+	caps->function = slave;
+
+	/* All other resources are allocated by the master, but we still report
+	 * 'num' and 'reserved' capabilities as follows:
+	 * - num remains the maximum resource index
+	 * - 'num - reserved' is the total available objects of a resource, but
+	 *   resource indices may be less than 'reserved'
+	 * TODO: set per-resource quotas */
+	return 0;
+}
+
+int mlx4_QUERY_SLAVE_CAP(struct mlx4_dev *dev, struct mlx4_caps *caps)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	int err;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	err = mlx4_cmd_box(dev, 0, mailbox->dma, 0, 0, MLX4_CMD_QUERY_SLAVE_CAP,
+			   MLX4_CMD_TIME_CLASS_A);
+	if (!err)
+		memcpy(caps, mailbox->buf, sizeof *caps);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
 int mlx4_QUERY_DEV_CAP(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap)
 {
 	struct mlx4_cmd_mailbox *mailbox;
 	u32 *outbox;
 	u8 field;
+	u32 field32;
 	u16 size;
 	u16 stat_rate;
 	int err;
 	int i;
+	u32 in_modifier;
+	u64 out_param;
+	u32 tmp1, tmp2;
 
 #define QUERY_DEV_CAP_OUT_SIZE		       0x100
 #define QUERY_DEV_CAP_MAX_SRQ_SZ_OFFSET		0x10
@@ -154,11 +482,12 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_DEV_CAP_MAX_SRQ_OFFSET		0x15
 #define QUERY_DEV_CAP_RSVD_EEC_OFFSET		0x16
 #define QUERY_DEV_CAP_MAX_EEC_OFFSET		0x17
+#define QUERY_DEV_CAP_RSVD_EQ_OFFSET		0x18
 #define QUERY_DEV_CAP_MAX_CQ_SZ_OFFSET		0x19
 #define QUERY_DEV_CAP_RSVD_CQ_OFFSET		0x1a
 #define QUERY_DEV_CAP_MAX_CQ_OFFSET		0x1b
 #define QUERY_DEV_CAP_MAX_MPT_OFFSET		0x1d
-#define QUERY_DEV_CAP_RSVD_EQ_OFFSET		0x1e
+#define QUERY_DEV_CAP_LOG_RSVD_EQ_OFFSET	0x1e
 #define QUERY_DEV_CAP_MAX_EQ_OFFSET		0x1f
 #define QUERY_DEV_CAP_RSVD_MTT_OFFSET		0x20
 #define QUERY_DEV_CAP_MAX_MRW_SZ_OFFSET		0x21
@@ -169,6 +498,7 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_DEV_CAP_MAX_RES_QP_OFFSET		0x2b
 #define QUERY_DEV_CAP_MAX_GSO_OFFSET		0x2d
 #define QUERY_DEV_CAP_MAX_RDMA_OFFSET		0x2f
+#define QUERY_DEV_CAP_STAT_CFG_INL_OFFSET	0x31
 #define QUERY_DEV_CAP_RSZ_SRQ_OFFSET		0x33
 #define QUERY_DEV_CAP_ACK_DELAY_OFFSET		0x35
 #define QUERY_DEV_CAP_MTU_WIDTH_OFFSET		0x36
@@ -177,6 +507,9 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_DEV_CAP_MAX_GID_OFFSET		0x3b
 #define QUERY_DEV_CAP_RATE_SUPPORT_OFFSET	0x3c
 #define QUERY_DEV_CAP_MAX_PKEY_OFFSET		0x3f
+#define QUERY_DEV_CAP_EXT_FLAGS_OFFSET		0x40
+#define QUERY_DEV_CAP_UDP_RSS_OFFSET		0x42
+#define QUERY_DEV_CAP_ETH_UC_LOOPBACK_OFFSET	0x43
 #define QUERY_DEV_CAP_FLAGS_OFFSET		0x44
 #define QUERY_DEV_CAP_RSVD_UAR_OFFSET		0x48
 #define QUERY_DEV_CAP_UAR_SZ_OFFSET		0x49
@@ -194,6 +527,8 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_DEV_CAP_MAX_MCG_OFFSET		0x63
 #define QUERY_DEV_CAP_RSVD_PD_OFFSET		0x64
 #define QUERY_DEV_CAP_MAX_PD_OFFSET		0x65
+#define QUERY_DEV_CAP_RSVD_XRC_OFFSET		0x66
+#define QUERY_DEV_CAP_MAX_XRC_OFFSET		0x67
 #define QUERY_DEV_CAP_RDMARC_ENTRY_SZ_OFFSET	0x80
 #define QUERY_DEV_CAP_QPC_ENTRY_SZ_OFFSET	0x82
 #define QUERY_DEV_CAP_AUX_ENTRY_SZ_OFFSET	0x84
@@ -207,6 +542,8 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_DEV_CAP_BMME_FLAGS_OFFSET		0x94
 #define QUERY_DEV_CAP_RSVD_LKEY_OFFSET		0x98
 #define QUERY_DEV_CAP_MAX_ICM_SZ_OFFSET		0xa0
+#define QUERY_DEV_CAP_MAX_BASIC_CNT_OFFSET	0x68
+#define QUERY_DEV_CAP_MAX_EXT_CNT_OFFSET	0x6c
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -235,7 +572,11 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_MPT_OFFSET);
 	dev_cap->max_mpts = 1 << (field & 0x3f);
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSVD_EQ_OFFSET);
-	dev_cap->reserved_eqs = 1 << (field & 0xf);
+	if (!field) {
+		MLX4_GET(field, outbox, QUERY_DEV_CAP_LOG_RSVD_EQ_OFFSET);
+		dev_cap->reserved_eqs = 1 << (field & 0xf);
+	} else
+		dev_cap->reserved_eqs = field;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_EQ_OFFSET);
 	dev_cap->max_eqs = 1 << (field & 0xf);
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSVD_MTT_OFFSET);
@@ -261,13 +602,29 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	dev_cap->max_rdma_global = 1 << (field & 0x3f);
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_ACK_DELAY_OFFSET);
 	dev_cap->local_ca_ack_delay = field & 0x1f;
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_MTU_WIDTH_OFFSET);
+	dev_cap->pf_num = field;
+	if (is_mfunc(dev))
+		dev->flags |= MLX4_FLAG_MASTER;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_VL_PORT_OFFSET);
 	dev_cap->num_ports = field & 0xf;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_MSG_SZ_OFFSET);
 	dev_cap->max_msg_sz = 1 << (field & 0x1f);
 	MLX4_GET(stat_rate, outbox, QUERY_DEV_CAP_RATE_SUPPORT_OFFSET);
 	dev_cap->stat_rate_support = stat_rate;
-	MLX4_GET(dev_cap->flags, outbox, QUERY_DEV_CAP_FLAGS_OFFSET);
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_EXT_FLAGS_OFFSET);
+	dev_cap->fast_drop = field & 0x2;
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_UDP_RSS_OFFSET);
+	dev_cap->udp_rss = field & 0x1;
+	dev_cap->vep_uc_steering = field & 0x2;
+	dev_cap->vep_mc_steering = field & 0x4;
+	dev_cap->sync_qp = field & 0x10;
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_ETH_UC_LOOPBACK_OFFSET);
+	dev_cap->loopback_support = field & 0x1;
+	dev_cap->wol = field & 0x60;
+	MLX4_GET(tmp1, outbox, QUERY_DEV_CAP_EXT_FLAGS_OFFSET);
+	MLX4_GET(tmp2, outbox, QUERY_DEV_CAP_FLAGS_OFFSET);
+	dev_cap->flags = tmp2 | (u64)tmp1 << 32;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSVD_UAR_OFFSET);
 	dev_cap->reserved_uars = field >> 4;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_UAR_SZ_OFFSET);
@@ -280,6 +637,10 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 		MLX4_GET(field, outbox, QUERY_DEV_CAP_LOG_BF_REG_SZ_OFFSET);
 		dev_cap->bf_reg_size = 1 << (field & 0x1f);
 		MLX4_GET(field, outbox, QUERY_DEV_CAP_LOG_MAX_BF_REGS_PER_PAGE_OFFSET);
+		if ((1 << (field & 0x3f)) > (PAGE_SIZE / dev_cap->bf_reg_size)) {
+			mlx4_dbg(dev, "log blue flame is invalid (%d), forcing 3\n", field & 0x1f);
+			field = 3;
+		}
 		dev_cap->bf_regs_per_page = 1 << (field & 0x3f);
 		mlx4_dbg(dev, "BlueFlame available (reg size %d, regs/page %d)\n",
 			 dev_cap->bf_reg_size, dev_cap->bf_regs_per_page);
@@ -304,6 +665,11 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_PD_OFFSET);
 	dev_cap->max_pds = 1 << (field & 0x3f);
 
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSVD_XRC_OFFSET);
+	dev_cap->reserved_xrcds = field >> 4;
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_XRC_OFFSET);
+	dev_cap->max_xrcds = 1 << (field & 0x1f);
+
 	MLX4_GET(size, outbox, QUERY_DEV_CAP_RDMARC_ENTRY_SZ_OFFSET);
 	dev_cap->rdmarc_entry_sz = size;
 	MLX4_GET(size, outbox, QUERY_DEV_CAP_QPC_ENTRY_SZ_OFFSET);
@@ -329,6 +695,8 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	dev_cap->max_srq_sz = 1 << field;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_QP_SZ_OFFSET);
 	dev_cap->max_qp_sz = 1 << field;
+	MLX4_GET(field, outbox, QUERY_DEV_CAP_STAT_CFG_INL_OFFSET);
+	dev_cap->inline_cfg = field & 1;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_RSZ_SRQ_OFFSET);
 	dev_cap->resize_srq = field & 1;
 	MLX4_GET(field, outbox, QUERY_DEV_CAP_MAX_SG_RQ_OFFSET);
@@ -338,10 +706,15 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 
 	MLX4_GET(dev_cap->bmme_flags, outbox,
 		 QUERY_DEV_CAP_BMME_FLAGS_OFFSET);
+	dev_cap->qinq = (dev_cap->bmme_flags & 0x4000) >> 14;
 	MLX4_GET(dev_cap->reserved_lkey, outbox,
 		 QUERY_DEV_CAP_RSVD_LKEY_OFFSET);
 	MLX4_GET(dev_cap->max_icm_sz, outbox,
 		 QUERY_DEV_CAP_MAX_ICM_SZ_OFFSET);
+	MLX4_GET(dev_cap->max_basic_counters, outbox,
+		 QUERY_DEV_CAP_MAX_BASIC_CNT_OFFSET);
+	MLX4_GET(dev_cap->max_ext_counters, outbox,
+		 QUERY_DEV_CAP_MAX_EXT_CNT_OFFSET);
 
 	if (dev->flags & MLX4_FLAG_OLD_PORT_CMDS) {
 		for (i = 1; i <= dev_cap->num_ports; ++i) {
@@ -364,6 +737,14 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 #define QUERY_PORT_MAX_MACVLAN_OFFSET		0x0a
 #define QUERY_PORT_MAX_VL_OFFSET		0x0b
 #define QUERY_PORT_MAC_OFFSET			0x10
+#define QUERY_PORT_TRANS_VENDOR_OFFSET		0x18
+#define QUERY_PORT_WAVELENGTH_OFFSET		0x1c
+#define QUERY_PORT_TRANS_CODE_OFFSET		0x20
+
+#define STAT_CFG_PORT_MODE	(1 << 28)
+#define STAT_CFG_PORT_OFFSET	0x8
+#define STAT_CFG_PORT_MASK	(1 << 20)
+#define STAT_CFG_MOD_INLINE	0x3
 
 		for (i = 1; i <= dev_cap->num_ports; ++i) {
 			err = mlx4_cmd_box(dev, 0, mailbox->dma, i, 0, MLX4_CMD_QUERY_PORT,
@@ -373,6 +754,8 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 
 			MLX4_GET(field, outbox, QUERY_PORT_SUPPORTED_TYPE_OFFSET);
 			dev_cap->supported_port_types[i] = field & 3;
+			dev_cap->suggested_port[i] = (field >> 3) & 1;
+			dev_cap->default_sense[i] = (field >> 4) & 1;
 			MLX4_GET(field, outbox, QUERY_PORT_MTU_OFFSET);
 			dev_cap->ib_mtu[i]	   = field & 0xf;
 			MLX4_GET(field, outbox, QUERY_PORT_WIDTH_OFFSET);
@@ -387,6 +770,25 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 			dev_cap->log_max_vlans[i] = field >> 4;
 			MLX4_GET(dev_cap->eth_mtu[i], outbox, QUERY_PORT_ETH_MTU_OFFSET);
 			MLX4_GET(dev_cap->def_mac[i], outbox, QUERY_PORT_MAC_OFFSET);
+			MLX4_GET(field32, outbox, QUERY_PORT_TRANS_VENDOR_OFFSET);
+			dev_cap->trans_type[i] = field32 >> 24;
+			dev_cap->vendor_oui[i] = field32 & 0xffffff;
+			MLX4_GET(dev_cap->wavelength[i], outbox, QUERY_PORT_WAVELENGTH_OFFSET);
+			MLX4_GET(dev_cap->trans_code[i], outbox, QUERY_PORT_TRANS_CODE_OFFSET);
+
+			/* Query stat cfg for port enablement */
+			if (dev_cap->inline_cfg) {
+				in_modifier = STAT_CFG_PORT_MODE | i << 8 |
+							STAT_CFG_PORT_OFFSET;
+				err = mlx4_cmd_imm(dev, 0, &out_param,
+						   in_modifier,
+						   STAT_CFG_MOD_INLINE,
+						   MLX4_CMD_MOD_STAT_CFG,
+						   MLX4_CMD_TIME_CLASS_B);
+				if (!err)
+					if (!(out_param & STAT_CFG_PORT_MASK))
+						dev_cap->supported_port_types[i] = 0;
+			}
 		}
 	}
 
@@ -401,6 +803,8 @@ int mlx4_QUERY_DEV_CAP(struct mlx4_dev *
 	dev_cap->reserved_eqs = max(dev_cap->reserved_uars * 4,
 				    dev_cap->reserved_eqs);
 
+	dev_cap->clp_ver = mlx4_QUERY_CLP(dev);
+
 	mlx4_dbg(dev, "Max ICM size %lld MB\n",
 		 (unsigned long long) dev_cap->max_icm_sz >> 20);
 	mlx4_dbg(dev, "Max QPs: %d, reserved QPs: %d, entry size: %d\n",
@@ -544,6 +948,8 @@ int mlx4_QUERY_FW(struct mlx4_dev *dev)
 
 #define QUERY_FW_OUT_SIZE             0x100
 #define QUERY_FW_VER_OFFSET            0x00
+#define MC_PROMISC_VER		       0x2000702bcull
+#define QUERY_FW_PPF_ID		       0x09
 #define QUERY_FW_CMD_IF_REV_OFFSET     0x0a
 #define QUERY_FW_MAX_CMD_OFFSET        0x0f
 #define QUERY_FW_ERR_START_OFFSET      0x30
@@ -554,6 +960,9 @@ int mlx4_QUERY_FW(struct mlx4_dev *dev)
 #define QUERY_FW_CLR_INT_BASE_OFFSET   0x20
 #define QUERY_FW_CLR_INT_BAR_OFFSET    0x28
 
+#define QUERY_FW_COMM_BASE_OFFSET      0x40
+#define QUERY_FW_COMM_BAR_OFFSET       0x48
+
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
 		return PTR_ERR(mailbox);
@@ -572,6 +981,13 @@ int mlx4_QUERY_FW(struct mlx4_dev *dev)
 	dev->caps.fw_ver = (fw_ver & 0xffff00000000ull) |
 		((fw_ver & 0xffff0000ull) >> 16) |
 		((fw_ver & 0x0000ffffull) << 16);
+	if (dev->caps.fw_ver < MC_PROMISC_VER)
+		dev->caps.mc_promisc_mode = 2;
+	else
+		dev->caps.mc_promisc_mode = 1;
+
+	MLX4_GET(lg, outbox, QUERY_FW_PPF_ID);
+	dev->caps.function = lg;
 
 	MLX4_GET(cmd_if_rev, outbox, QUERY_FW_CMD_IF_REV_OFFSET);
 	if (cmd_if_rev < MLX4_COMMAND_INTERFACE_MIN_REV ||
@@ -614,6 +1030,11 @@ int mlx4_QUERY_FW(struct mlx4_dev *dev)
 	MLX4_GET(fw->clr_int_bar,  outbox, QUERY_FW_CLR_INT_BAR_OFFSET);
 	fw->clr_int_bar = (fw->clr_int_bar >> 6) * 2;
 
+	MLX4_GET(fw->comm_base, outbox, QUERY_FW_COMM_BASE_OFFSET);
+	MLX4_GET(fw->comm_bar,  outbox, QUERY_FW_COMM_BAR_OFFSET);
+	fw->comm_bar = (fw->comm_bar >> 6) * 2;
+	mlx4_dbg(dev, "Communication vector bar:%d offset:0x%llx\n", fw->comm_bar,
+								     fw->comm_base);
 	mlx4_dbg(dev, "FW size %d KB\n", fw->fw_pages >> 2);
 
 	/*
@@ -700,14 +1121,17 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 #define INIT_HCA_VERSION_OFFSET		 0x000
 #define	 INIT_HCA_VERSION		 2
 #define INIT_HCA_CACHELINE_SZ_OFFSET	 0x0e
+#define INIT_HCA_X86_64_BYTE_CACHELINE_SZ	 0x40
 #define INIT_HCA_FLAGS_OFFSET		 0x014
 #define INIT_HCA_QPC_OFFSET		 0x020
+#define INIT_HCA_EQE_CQE_OFFSETS	 (INIT_HCA_QPC_OFFSET + 0x38)
 #define	 INIT_HCA_QPC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x10)
 #define	 INIT_HCA_LOG_QP_OFFSET		 (INIT_HCA_QPC_OFFSET + 0x17)
 #define	 INIT_HCA_SRQC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x28)
 #define	 INIT_HCA_LOG_SRQ_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x2f)
 #define	 INIT_HCA_CQC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x30)
 #define	 INIT_HCA_LOG_CQ_OFFSET		 (INIT_HCA_QPC_OFFSET + 0x37)
+#define	 INIT_HCA_EQE_CQE_OFFSETS	 (INIT_HCA_QPC_OFFSET + 0x38)
 #define	 INIT_HCA_ALTC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x40)
 #define	 INIT_HCA_AUXC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x50)
 #define	 INIT_HCA_EQC_BASE_OFFSET	 (INIT_HCA_QPC_OFFSET + 0x60)
@@ -718,6 +1142,7 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 #define	 INIT_HCA_MC_BASE_OFFSET	 (INIT_HCA_MCAST_OFFSET + 0x00)
 #define	 INIT_HCA_LOG_MC_ENTRY_SZ_OFFSET (INIT_HCA_MCAST_OFFSET + 0x12)
 #define	 INIT_HCA_LOG_MC_HASH_SZ_OFFSET	 (INIT_HCA_MCAST_OFFSET + 0x16)
+#define  INIT_HCA_UC_STEERING_OFFSET	 (INIT_HCA_MCAST_OFFSET + 0x18)
 #define	 INIT_HCA_LOG_MC_TABLE_SZ_OFFSET (INIT_HCA_MCAST_OFFSET + 0x1b)
 #define INIT_HCA_TPT_OFFSET		 0x0f0
 #define	 INIT_HCA_DMPT_BASE_OFFSET	 (INIT_HCA_TPT_OFFSET + 0x00)
@@ -736,9 +1161,9 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 	memset(inbox, 0, INIT_HCA_IN_SIZE);
 
 	*((u8 *) mailbox->buf + INIT_HCA_VERSION_OFFSET) = INIT_HCA_VERSION;
-
-	*((u8 *) mailbox->buf + INIT_HCA_CACHELINE_SZ_OFFSET) =
-		(ilog2(cache_line_size()) - 4) << 5;
+#if defined(__x86_64__) || defined(__PPC64__)
+	*((u8 *) mailbox->buf + INIT_HCA_CACHELINE_SZ_OFFSET) = INIT_HCA_X86_64_BYTE_CACHELINE_SZ;
+#endif
 
 #if defined(__LITTLE_ENDIAN)
 	*(inbox + INIT_HCA_FLAGS_OFFSET / 4) &= ~cpu_to_be32(1 << 1);
@@ -758,9 +1183,23 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 	if (enable_qos)
 		*(inbox + INIT_HCA_FLAGS_OFFSET / 4) |= cpu_to_be32(1 << 2);
 
+	/* Enable fast drop performance optimization */
+	if (dev->caps.fast_drop)
+		*(inbox + INIT_HCA_FLAGS_OFFSET / 4) |= cpu_to_be32(1 << 7);
+
+	/* counters mode */
+	*(inbox + INIT_HCA_FLAGS_OFFSET / 4) |=
+		cpu_to_be32(dev->caps.counters_mode << 4);
+
 	/* QPC/EEC/CQC/EQC/RDMARC attributes */
 
-	MLX4_PUT(inbox, param->qpc_base,      INIT_HCA_QPC_BASE_OFFSET);
+    /* CX3 is capable of extending the CQE\EQE from 32 to 64 bytes */
+	if (dev->caps.cqe_size == 64)
+		*(inbox + INIT_HCA_EQE_CQE_OFFSETS / 4) |= cpu_to_be32(1 << 30);
+    if (dev->caps.eqe_size == 64)
+        *(inbox + INIT_HCA_EQE_CQE_OFFSETS / 4) |= cpu_to_be32(1 << 29);
+
+    MLX4_PUT(inbox, param->qpc_base,      INIT_HCA_QPC_BASE_OFFSET);
 	MLX4_PUT(inbox, param->log_num_qps,   INIT_HCA_LOG_QP_OFFSET);
 	MLX4_PUT(inbox, param->srqc_base,     INIT_HCA_SRQC_BASE_OFFSET);
 	MLX4_PUT(inbox, param->log_num_srqs,  INIT_HCA_LOG_SRQ_OFFSET);
@@ -778,6 +1217,8 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 	MLX4_PUT(inbox, param->mc_base,		INIT_HCA_MC_BASE_OFFSET);
 	MLX4_PUT(inbox, param->log_mc_entry_sz, INIT_HCA_LOG_MC_ENTRY_SZ_OFFSET);
 	MLX4_PUT(inbox, param->log_mc_hash_sz,  INIT_HCA_LOG_MC_HASH_SZ_OFFSET);
+	if (dev->caps.vep_uc_steering)
+		MLX4_PUT(inbox, (u8) (1 << 3),	INIT_HCA_UC_STEERING_OFFSET);
 	MLX4_PUT(inbox, param->log_mc_table_sz, INIT_HCA_LOG_MC_TABLE_SZ_OFFSET);
 
 	/* TPT attributes */
@@ -791,6 +1232,9 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 
 	MLX4_PUT(inbox, (u8) (PAGE_SHIFT - 12), INIT_HCA_UAR_PAGE_SZ_OFFSET);
 	MLX4_PUT(inbox, param->log_uar_sz,      INIT_HCA_LOG_UAR_SZ_OFFSET);
+	if (!mlx4_pre_t11_mode && dev->caps.flags & (u32) MLX4_DEV_CAP_FLAG_FC_T11)
+		*(inbox + INIT_HCA_FLAGS_OFFSET / 4) |= cpu_to_be32(1 << 10);
+
 
 	err = mlx4_cmd(dev, mailbox->dma, 0, 0, MLX4_CMD_INIT_HCA, 10000);
 
@@ -801,14 +1245,86 @@ int mlx4_INIT_HCA(struct mlx4_dev *dev, 
 	return err;
 }
 
+
+
+int mlx4_SET_VEP(struct mlx4_dev *dev, int slave, u8 vep_link)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	u8 *buffer;
+	int ret;
+	u8 vep_num, port;
+	u32 in_param;
+
+	vep_num = mlx4_priv(dev)->mfunc.master.slave_state[slave].vep_num;
+	port = mlx4_priv(dev)->mfunc.master.slave_state[slave].port_num;
+	in_param = ((u32) vep_num << 8) | port;
+ 
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	buffer = mailbox->buf;
+
+	buffer[0] = 1 << 6;
+	buffer[3] = vep_link << 1;
+
+	ret = mlx4_cmd(dev, mailbox->dma, in_param, 0, MLX4_CMD_SET_VEP,
+		       MLX4_CMD_TIME_CLASS_A);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return ret;
+}
+
+static int mlx4_common_init_port(struct mlx4_dev *dev, int function, int port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int err;
+
+	if (function == dev->caps.function ||
+	    function == priv->mfunc.master.slave_state[function].pf_num)
+		mlx4_SET_VEP(dev, function, 1);
+
+	if (priv->mfunc.master.slave_state[function].init_port_mask & (1 << port))
+		return 0;
+
+	/* Enable port only if it was previously disabled */
+	if (!priv->mfunc.master.init_port_ref[port]) {
+		mlx4_update_uplink_arbiter(dev, port);
+		err = mlx4_cmd(dev, 0, port, 0, MLX4_CMD_INIT_PORT,
+			       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			return err;
+	}
+	++priv->mfunc.master.init_port_ref[port];
+	priv->mfunc.master.slave_state[function].init_port_mask |= (1 << port);
+	return 0;
+}
+
+int mlx4_INIT_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							 struct mlx4_cmd_mailbox *inbox,
+							 struct mlx4_cmd_mailbox *outbox)
+{
+	int port = vhcr->in_modifier;
+
+	return mlx4_common_init_port(dev, slave, port);
+}
+
 int mlx4_INIT_PORT(struct mlx4_dev *dev, int port)
 {
+	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_cmd_mailbox *mailbox;
 	u32 *inbox;
-	int err;
+	int err = 0;
 	u32 flags;
 	u16 field;
 
+	if (mlx4_is_master(dev))
+		return mlx4_common_init_port(dev, dev->caps.function,
+			 		     port);	
+
+        mutex_lock(&priv->port_ops_mutex);
+	if (priv->init_port_ref[port]++)
+		goto out;
+
 	if (dev->flags & MLX4_FLAG_OLD_PORT_CMDS) {
 #define INIT_PORT_IN_SIZE          256
 #define INIT_PORT_FLAGS_OFFSET     0x00
@@ -825,8 +1341,10 @@ int mlx4_INIT_PORT(struct mlx4_dev *dev,
 #define INIT_PORT_SI_GUID_OFFSET   0x20
 
 		mailbox = mlx4_alloc_cmd_mailbox(dev);
-		if (IS_ERR(mailbox))
-			return PTR_ERR(mailbox);
+		if (IS_ERR(mailbox)) {
+			err = PTR_ERR(mailbox);
+			goto out;
+		}
 		inbox = mailbox->buf;
 
 		memset(inbox, 0, INIT_PORT_IN_SIZE);
@@ -847,23 +1365,78 @@ int mlx4_INIT_PORT(struct mlx4_dev *dev,
 			       MLX4_CMD_TIME_CLASS_A);
 
 		mlx4_free_cmd_mailbox(dev, mailbox);
-	} else
+	} else {
+
 		err = mlx4_cmd(dev, 0, port, 0, MLX4_CMD_INIT_PORT,
 			       MLX4_CMD_TIME_CLASS_A);
+	}
 
+out:
+	mutex_unlock(&priv->port_ops_mutex);
 	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_INIT_PORT);
 
+static int mlx4_common_close_port(struct mlx4_dev *dev, int function, int port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int err;
+
+	if (function == dev->caps.function ||
+	    function == priv->mfunc.master.slave_state[function].pf_num)
+		mlx4_SET_VEP(dev, function, 0);
+
+	if (!(priv->mfunc.master.slave_state[function].init_port_mask & (1 << port)))
+		return 0;
+
+	if (priv->mfunc.master.init_port_ref[port] == 1) {
+		err = mlx4_cmd(dev, 0, port, 0, MLX4_CMD_CLOSE_PORT, MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			return err;
+	}
+	--priv->mfunc.master.init_port_ref[port];
+	priv->mfunc.master.slave_state[function].init_port_mask &= ~(1 << port);
+	return 0;
+}
+
+int mlx4_CLOSE_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox)
+{
+	int port = vhcr->in_modifier;
+
+	return mlx4_common_close_port(dev, slave, port);
+}
+
 int mlx4_CLOSE_PORT(struct mlx4_dev *dev, int port)
 {
-	return mlx4_cmd(dev, 0, port, 0, MLX4_CMD_CLOSE_PORT, 1000);
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int err = 0;
+
+       	if (mlx4_is_master(dev))
+                return mlx4_common_close_port(dev, dev->caps.function, port);
+
+	mutex_lock(&priv->port_ops_mutex);
+	if (priv->init_port_ref[port] == 0) {
+		mlx4_dbg(dev, "CLOSE_PORT called for port %d but refcount is already 0\n",
+			       port);
+		goto out;
+	}
+	--priv->init_port_ref[port];
+	if (priv->init_port_ref[port] > 0)
+		goto out;
+
+	err = mlx4_cmd(dev, 0, port, 0, MLX4_CMD_CLOSE_PORT, MLX4_CMD_TIME_CLASS_A);
+
+out:
+	mutex_unlock(&priv->port_ops_mutex);
+	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_CLOSE_PORT);
 
 int mlx4_CLOSE_HCA(struct mlx4_dev *dev, int panic)
 {
-	return mlx4_cmd(dev, 0, 0, panic, MLX4_CMD_CLOSE_HCA, 1000);
+	return mlx4_cmd(dev, 0, 0, panic, MLX4_CMD_CLOSE_HCA, MLX4_CMD_TIME_CLASS_A);
 }
 
 int mlx4_SET_ICM_SIZE(struct mlx4_dev *dev, u64 icm_size, u64 *aux_pages)
@@ -889,3 +1462,170 @@ int mlx4_NOP(struct mlx4_dev *dev)
 	/* Input modifier of 0x1f means "finish as soon as possible." */
 	return mlx4_cmd(dev, 0, 0x1f, 0, MLX4_CMD_NOP, 100);
 }
+
+int mlx4_QUERY_FUNC(struct mlx4_dev *dev, int func, u8 *pf_num)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	u8 *outbox;
+	int ret;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	outbox = mailbox->buf;
+
+	ret = mlx4_cmd_box(dev, 0, mailbox->dma, func & 0xff, 0,
+			   MLX4_CMD_QUERY_FUNC, MLX4_CMD_TIME_CLASS_A);
+	if (ret)
+		goto out;
+
+	*pf_num = outbox[3];
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return ret;
+}
+
+int mlx4_query_diag_counters(struct mlx4_dev *dev, int array_length,
+			     u8 op_modifier, u32 in_offset[], u32 counter_out[])
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	u32 *outbox;
+	int ret;
+	int i;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	outbox = mailbox->buf;
+
+	ret = mlx4_cmd_box(dev, 0, mailbox->dma, 0, op_modifier,
+			   MLX4_CMD_DIAG_RPRT, MLX4_CMD_TIME_CLASS_A);
+	if (ret)
+		goto out;
+
+	for (i=0; i < array_length; i++) {
+		if (in_offset[i] > MLX4_MAILBOX_SIZE) {
+			ret = -EINVAL;
+			goto out;
+		}
+
+		MLX4_GET(counter_out[i], outbox, in_offset[i]);
+	}
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(mlx4_query_diag_counters);
+
+void mlx4_get_fc_t11_settings(struct mlx4_dev *dev, int *enable_pre_t11, int *t11_supported)
+{
+	*enable_pre_t11 = !!mlx4_pre_t11_mode;
+	*t11_supported = !!(dev->caps.flags & MLX4_DEV_CAP_FLAG_FC_T11);
+}
+EXPORT_SYMBOL_GPL(mlx4_get_fc_t11_settings);
+
+#define MLX4_WOL_SETUP_MODE (5 << 28)
+int mlx4_wol_read(struct mlx4_dev *dev, u64 *config, int port)
+{
+	u32 in_mod = MLX4_WOL_SETUP_MODE | port << 8;
+
+	return mlx4_cmd_imm(dev, 0, config, in_mod, 0x3,
+			    MLX4_CMD_MOD_STAT_CFG, MLX4_CMD_TIME_CLASS_A);
+}
+EXPORT_SYMBOL_GPL(mlx4_wol_read);
+
+int mlx4_wol_write(struct mlx4_dev *dev, u64 config, int port)
+{
+	u32 in_mod = MLX4_WOL_SETUP_MODE | port << 8;
+
+	return mlx4_cmd(dev, config, in_mod, 0x1, MLX4_CMD_MOD_STAT_CFG,
+					MLX4_CMD_TIME_CLASS_A);
+}
+EXPORT_SYMBOL_GPL(mlx4_wol_write);
+
+enum {
+	ADD_TO_MCG = 0x26,
+};
+
+
+void mlx4_opreq_action(struct work_struct *work)
+{
+	struct mlx4_priv *priv = container_of(work, struct mlx4_priv, opreq_task);
+	struct mlx4_dev *dev = &priv->dev;
+	int num_tasks = atomic_read(&priv->opreq_count);
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm;
+	u32 *outbox;
+	u32 modifier;
+	u16 token;
+	u16 type_m;
+	u16 type;
+	int err;
+	u32 num_qps;
+	struct mlx4_qp qp;
+	int i;
+	u8 rem_mcg;
+	u8 prot;
+
+#define GET_OP_REQ_MODIFIER_OFFSET	0x08
+#define GET_OP_REQ_TOKEN_OFFSET		0x14
+#define GET_OP_REQ_TYPE_OFFSET		0x1a
+#define GET_OP_REQ_DATA_OFFSET		0x20
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox)) {
+		mlx4_err(dev, "Failed to allocate mailbox for GET_OP_REQ\n");
+		return;
+	}
+	outbox = mailbox->buf;
+
+	while (num_tasks) {
+		err = mlx4_cmd_box(dev, 0, mailbox->dma, 0, 0,
+				   MLX4_CMD_GET_OP_REQ, MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			mlx4_err(dev, "Failed to retreive required operation: %d\n", err);
+			return;
+		}
+		MLX4_GET(modifier, outbox, GET_OP_REQ_MODIFIER_OFFSET);
+		MLX4_GET(token, outbox, GET_OP_REQ_TOKEN_OFFSET);
+		MLX4_GET(type, outbox, GET_OP_REQ_TYPE_OFFSET);
+		type_m = type >> 12;
+		type &= 0xfff;
+
+		switch (type) {
+		case ADD_TO_MCG:
+			mgm = (struct mlx4_mgm *) ((u8 *) (outbox) + GET_OP_REQ_DATA_OFFSET);
+			num_qps = be32_to_cpu(mgm->members_count) & MGM_QPN_MASK;
+			rem_mcg = ((u8 *) (&mgm->members_count))[0] & 1;
+			prot = ((u8 *) (&mgm->members_count))[0] >> 6;
+
+			for (i = 0; i < num_qps; i++) {
+				qp.qpn = be32_to_cpu(mgm->qp[i]);
+				if (rem_mcg)
+					err = mlx4_multicast_detach(dev, &qp, mgm->gid, prot, 0);
+				else
+					err = mlx4_multicast_attach(dev, &qp, mgm->gid, 0, prot, 0);
+				if (err)
+					break;
+			}
+			break;
+		default:
+			mlx4_warn(dev, "Bad type for required operation\n");
+			err = EINVAL;
+			break;
+		}
+		err = mlx4_cmd(dev, 0, ((u32) err | cpu_to_be32(token) << 16), 1,
+			       MLX4_CMD_GET_OP_REQ, MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			mlx4_err(dev, "Failed to acknowledge required request: %d\n", err);
+			goto out;
+		}
+		memset(outbox, 0, 0xffc);
+		num_tasks = atomic_dec_return(&priv->opreq_count);
+	}
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+}
diff -r c23d1fc7e422 drivers/net/mlx4/fw.h
--- a/drivers/net/mlx4/fw.h
+++ b/drivers/net/mlx4/fw.h
@@ -64,6 +64,7 @@ struct mlx4_dev_cap {
 	int max_responder_per_qp;
 	int max_rdma_global;
 	int local_ca_ack_delay;
+	int pf_num;
 	int num_ports;
 	u32 max_msg_sz;
 	int ib_mtu[MLX4_MAX_PORTS + 1];
@@ -73,8 +74,16 @@ struct mlx4_dev_cap {
 	int max_pkeys[MLX4_MAX_PORTS + 1];
 	u64 def_mac[MLX4_MAX_PORTS + 1];
 	u16 eth_mtu[MLX4_MAX_PORTS + 1];
+	int trans_type[MLX4_MAX_PORTS + 1];
+	int vendor_oui[MLX4_MAX_PORTS + 1];
+	u16 wavelength[MLX4_MAX_PORTS + 1];
+	u64 trans_code[MLX4_MAX_PORTS + 1];
 	u16 stat_rate_support;
-	u32 flags;
+	int udp_rss;
+	int loopback_support;
+	int vep_uc_steering;
+	int vep_mc_steering;
+	u64 flags;
 	int reserved_uars;
 	int uar_size;
 	int min_page_sz;
@@ -89,6 +98,8 @@ struct mlx4_dev_cap {
 	int max_mcgs;
 	int reserved_pds;
 	int max_pds;
+	int reserved_xrcds;
+	int max_xrcds;
 	int qpc_entry_sz;
 	int rdmarc_entry_sz;
 	int altc_entry_sz;
@@ -99,14 +110,24 @@ struct mlx4_dev_cap {
 	int dmpt_entry_sz;
 	int cmpt_entry_sz;
 	int mtt_entry_sz;
+	int inline_cfg;
 	int resize_srq;
 	u32 bmme_flags;
 	u32 reserved_lkey;
 	u64 max_icm_sz;
 	int max_gso_sz;
 	u8  supported_port_types[MLX4_MAX_PORTS + 1];
+	u8  suggested_port[MLX4_MAX_PORTS + 1];
+	u8  default_sense[MLX4_MAX_PORTS + 1];
 	u8  log_max_macs[MLX4_MAX_PORTS + 1];
 	u8  log_max_vlans[MLX4_MAX_PORTS + 1];
+	u32 max_basic_counters;
+	u32 max_ext_counters;
+	int wol;
+	u16 clp_ver;
+	u32 sync_qp;
+	int fast_drop;
+	int qinq;
 };
 
 struct mlx4_adapter {
@@ -160,6 +181,10 @@ struct mlx4_set_ib_param {
 };
 
 int mlx4_QUERY_DEV_CAP(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap);
+int mlx4_QUERY_SLAVE_CAP(struct mlx4_dev *dev, struct mlx4_caps *caps);
+int mlx4_QUERY_SLAVE_CAP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						    struct mlx4_cmd_mailbox *inbox,
+						    struct mlx4_cmd_mailbox *outbox);
 int mlx4_MAP_FA(struct mlx4_dev *dev, struct mlx4_icm *icm);
 int mlx4_UNMAP_FA(struct mlx4_dev *dev);
 int mlx4_RUN_FW(struct mlx4_dev *dev);
@@ -173,5 +198,10 @@ int mlx4_MAP_ICM_AUX(struct mlx4_dev *de
 int mlx4_UNMAP_ICM_AUX(struct mlx4_dev *dev);
 int mlx4_NOP(struct mlx4_dev *dev);
 int mlx4_MOD_STAT_CFG(struct mlx4_dev *dev, struct mlx4_mod_stat_cfg *cfg);
+int mlx4_QUERY_FUNC(struct mlx4_dev *dev, int func, u8 *pf_num);
+int mlx4_QUERY_VEP_CFG(struct mlx4_dev *dev, u8 vep_num, u8 port, struct mlx4_vep_cfg *cfg);
+int mlx4_update_uplink_arbiter(struct mlx4_dev *dev, u8 port);
+int mlx4_set_vep_maps(struct mlx4_dev *dev);
+void mlx4_opreq_action(struct work_struct *work);
 
 #endif /* MLX4_FW_H */
diff -r c23d1fc7e422 drivers/net/mlx4/icm.c
--- a/drivers/net/mlx4/icm.c
+++ b/drivers/net/mlx4/icm.c
@@ -31,6 +31,7 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/errno.h>
 #include <linux/mm.h>
 #include <linux/scatterlist.h>
@@ -436,6 +437,7 @@ err:
 			mlx4_free_icm(dev, table->icm[i], use_coherent);
 		}
 
+	kfree(table->icm);
 	return -ENOMEM;
 }
 
diff -r c23d1fc7e422 drivers/net/mlx4/icm.h
--- a/drivers/net/mlx4/icm.h
+++ b/drivers/net/mlx4/icm.h
@@ -81,13 +81,7 @@ int mlx4_init_icm_table(struct mlx4_dev 
 			u64 virt, int obj_size,	int nobj, int reserved,
 			int use_lowmem, int use_coherent);
 void mlx4_cleanup_icm_table(struct mlx4_dev *dev, struct mlx4_icm_table *table);
-int mlx4_table_get(struct mlx4_dev *dev, struct mlx4_icm_table *table, int obj);
-void mlx4_table_put(struct mlx4_dev *dev, struct mlx4_icm_table *table, int obj);
 void *mlx4_table_find(struct mlx4_icm_table *table, int obj, dma_addr_t *dma_handle);
-int mlx4_table_get_range(struct mlx4_dev *dev, struct mlx4_icm_table *table,
-			 int start, int end);
-void mlx4_table_put_range(struct mlx4_dev *dev, struct mlx4_icm_table *table,
-			  int start, int end);
 
 static inline void mlx4_icm_first(struct mlx4_icm *icm,
 				  struct mlx4_icm_iter *iter)
diff -r c23d1fc7e422 drivers/net/mlx4/intf.c
--- a/drivers/net/mlx4/intf.c
+++ b/drivers/net/mlx4/intf.c
@@ -37,6 +37,7 @@ struct mlx4_device_context {
 	struct list_head	list;
 	struct mlx4_interface  *intf;
 	void		       *context;
+	u16                     port_mtu[MLX4_MAX_PORTS + 1];
 };
 
 static LIST_HEAD(intf_list);
@@ -46,20 +47,28 @@ static DEFINE_MUTEX(intf_mutex);
 static void mlx4_add_device(struct mlx4_interface *intf, struct mlx4_priv *priv)
 {
 	struct mlx4_device_context *dev_ctx;
+	void *context;
 
-	dev_ctx = kmalloc(sizeof *dev_ctx, GFP_KERNEL);
+	dev_ctx = kzalloc(sizeof *dev_ctx, GFP_KERNEL);
 	if (!dev_ctx)
 		return;
 
 	dev_ctx->intf    = intf;
-	dev_ctx->context = intf->add(&priv->dev);
+	spin_lock_irq(&priv->ctx_lock);
+	list_add_tail(&dev_ctx->list, &priv->ctx_list);
+	spin_unlock_irq(&priv->ctx_lock);
 
-	if (dev_ctx->context) {
+	context = intf->add(&priv->dev);
+	if (!context) {
 		spin_lock_irq(&priv->ctx_lock);
-		list_add_tail(&dev_ctx->list, &priv->ctx_list);
+		list_del(&dev_ctx->list);
 		spin_unlock_irq(&priv->ctx_lock);
-	} else
 		kfree(dev_ctx);
+	} else {
+		spin_lock_irq(&priv->ctx_lock);
+		dev_ctx->context = context;
+		spin_unlock_irq(&priv->ctx_lock);
+	}
 }
 
 static void mlx4_remove_device(struct mlx4_interface *intf, struct mlx4_priv *priv)
@@ -78,6 +87,46 @@ static void mlx4_remove_device(struct ml
 		}
 }
 
+u16 mlx4_set_interface_mtu_get_max(struct mlx4_interface *intf,
+		struct mlx4_dev *dev, int port, u16 new_mtu)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_device_context *dev_ctx;
+	unsigned long flags;
+	int max_mtu = 0;
+
+	spin_lock_irqsave(&priv->ctx_lock, flags);
+	list_for_each_entry(dev_ctx, &priv->ctx_list, list) {
+		if (dev_ctx->intf == intf)
+			dev_ctx->port_mtu[port] = new_mtu;
+		if (dev_ctx->port_mtu[port] > max_mtu)
+			max_mtu = dev_ctx->port_mtu[port];
+	}
+	spin_unlock_irqrestore(&priv->ctx_lock, flags);
+
+	return max_mtu;
+}
+
+void *mlx4_get_prot_dev(struct mlx4_dev *dev, enum mlx4_prot proto, int port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_device_context *dev_ctx;
+	unsigned long flags;
+	void *result = NULL;
+
+	spin_lock_irqsave(&priv->ctx_lock, flags);
+	list_for_each_entry(dev_ctx, &priv->ctx_list, list)
+		if ((dev_ctx->context) &&
+			(dev_ctx->intf->protocol == proto && dev_ctx->intf->get_prot_dev)) {
+			result = dev_ctx->intf->get_prot_dev(dev, dev_ctx->context, port);
+			break;
+	}
+	spin_unlock_irqrestore(&priv->ctx_lock, flags);
+
+	return result;
+}
+EXPORT_SYMBOL_GPL(mlx4_get_prot_dev);
+
 int mlx4_register_interface(struct mlx4_interface *intf)
 {
 	struct mlx4_priv *priv;
@@ -112,6 +161,36 @@ void mlx4_unregister_interface(struct ml
 }
 EXPORT_SYMBOL_GPL(mlx4_unregister_interface);
 
+struct mlx4_dev *mlx4_query_interface(void *int_dev, int *port)
+{
+	struct mlx4_priv *priv;
+	struct mlx4_device_context *dev_ctx;
+	enum mlx4_query_reply r;
+	unsigned long flags;
+
+	mutex_lock(&intf_mutex);
+
+	list_for_each_entry(priv, &dev_list, dev_list) {
+		spin_lock_irqsave(&priv->ctx_lock, flags);
+		list_for_each_entry(dev_ctx, &priv->ctx_list, list) {
+			if (!dev_ctx->intf->query || !dev_ctx->context)
+				continue;
+			r = dev_ctx->intf->query(dev_ctx->context, int_dev);
+			if (r != MLX4_QUERY_NOT_MINE) {
+				*port = r;
+				spin_unlock_irqrestore(&priv->ctx_lock, flags);
+				mutex_unlock(&intf_mutex);
+				return &priv->dev;
+			}
+		}
+		spin_unlock_irqrestore(&priv->ctx_lock, flags);
+	}
+
+	mutex_unlock(&intf_mutex);
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(mlx4_query_interface);
+
 void mlx4_dispatch_event(struct mlx4_dev *dev, enum mlx4_dev_event type, int port)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -121,7 +200,7 @@ void mlx4_dispatch_event(struct mlx4_dev
 	spin_lock_irqsave(&priv->ctx_lock, flags);
 
 	list_for_each_entry(dev_ctx, &priv->ctx_list, list)
-		if (dev_ctx->intf->event)
+		if ( (dev_ctx->intf->event) && (dev_ctx->context) )
 			dev_ctx->intf->event(dev, dev_ctx->context, type, port);
 
 	spin_unlock_irqrestore(&priv->ctx_lock, flags);
@@ -139,7 +218,8 @@ int mlx4_register_device(struct mlx4_dev
 		mlx4_add_device(intf, priv);
 
 	mutex_unlock(&intf_mutex);
-	mlx4_start_catas_poll(dev);
+	if (!mlx4_is_slave(dev))
+		mlx4_start_catas_poll(dev);
 
 	return 0;
 }
@@ -149,7 +229,8 @@ void mlx4_unregister_device(struct mlx4_
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_interface *intf;
 
-	mlx4_stop_catas_poll(dev);
+	if (!mlx4_is_slave(dev))
+		mlx4_stop_catas_poll(dev);
 	mutex_lock(&intf_mutex);
 
 	list_for_each_entry(intf, &intf_list, list)
@@ -159,3 +240,4 @@ void mlx4_unregister_device(struct mlx4_
 
 	mutex_unlock(&intf_mutex);
 }
+
diff -r c23d1fc7e422 drivers/net/mlx4/main.c
--- a/drivers/net/mlx4/main.c
+++ b/drivers/net/mlx4/main.c
@@ -38,6 +38,7 @@
 #include <linux/errno.h>
 #include <linux/pci.h>
 #include <linux/dma-mapping.h>
+#include <linux/io-mapping.h>
 
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/doorbell.h>
@@ -61,29 +62,73 @@ MODULE_PARM_DESC(debug_level, "Enable de
 
 #endif /* CONFIG_MLX4_DEBUG */
 
+int mlx4_blck_lb=1;
+module_param_named(block_loopback, mlx4_blck_lb, int, 0644);
+MODULE_PARM_DESC(block_loopback, "Block multicast loopback packets if > 0");
+
 #ifdef CONFIG_PCI_MSI
 
 static int msi_x = 1;
 module_param(msi_x, int, 0444);
 MODULE_PARM_DESC(msi_x, "attempt to use MSI-X if nonzero");
 
+static int high_rate_steer;
+module_param(high_rate_steer, int, 0444);
+MODULE_PARM_DESC(high_rate_steer, "Enable steering mode for higher packet rate"
+				  " (default off)");
+static int enable_qinq;
+module_param(enable_qinq, bool, 0444);
+MODULE_PARM_DESC(enable_qinq, "Set the device skips the first q-tag(vlan) in the packet and treat the secound vlan as the vlan tag."
+			"(0/1 default: 0)");
 #else /* CONFIG_PCI_MSI */
 
 #define msi_x (0)
 
 #endif /* CONFIG_PCI_MSI */
 
+#ifdef CONFIG_PCI_IOV
+
+#ifdef MLX4_SRIOV
+
+static int sr_iov;
+module_param(sr_iov, int, 0444);
+MODULE_PARM_DESC(sr_iov, "enable #sr_iov functions if sr_iov > 0");
+
+static int probe_vf;
+module_param(probe_vf, int, 0444);
+MODULE_PARM_DESC(probe_vf, "number of vfs to probe by pf driver (sr_iov > 0)");
+
+#else /* MLX4_SRIOV */
+
+#define sr_iov 0
+#define probe_vf 0
+
+#endif /* MLX4_SRIOV */
+
+#else /* CONFIG_PCI_IOV */
+
+#define sr_iov 0
+#define probe_vf 0
+
+#endif /* CONFIG_PCI_IOV */
+
+#define MAX_MSIX		64
+#define MSIX_LEGACY_SZ		4
+#define MIN_MSIX_P_PORT		5
+
 static char mlx4_version[] __devinitdata =
 	DRV_NAME ": Mellanox ConnectX core driver v"
 	DRV_VERSION " (" DRV_RELDATE ")\n";
 
+struct mutex drv_mutex;
+
 static struct mlx4_profile default_profile = {
-	.num_qp		= 1 << 17,
+	.num_qp		= 1 << 18,
 	.num_srq	= 1 << 16,
 	.rdmarc_per_qp	= 1 << 4,
 	.num_cq		= 1 << 16,
 	.num_mcg	= 1 << 13,
-	.num_mpt	= 1 << 17,
+	.num_mpt	= 1 << 19,
 	.num_mtt	= 1 << 20,
 };
 
@@ -91,18 +136,103 @@ static int log_num_mac = 2;
 module_param_named(log_num_mac, log_num_mac, int, 0444);
 MODULE_PARM_DESC(log_num_mac, "Log2 max number of MACs per ETH port (1-7)");
 
-static int log_num_vlan;
-module_param_named(log_num_vlan, log_num_vlan, int, 0444);
-MODULE_PARM_DESC(log_num_vlan, "Log2 max number of VLANs per ETH port (0-7)");
-
 static int use_prio;
 module_param_named(use_prio, use_prio, bool, 0444);
 MODULE_PARM_DESC(use_prio, "Enable steering by VLAN priority on ETH ports "
 		  "(0/1, default 0)");
 
-static int log_mtts_per_seg = ilog2(MLX4_MTT_ENTRY_PER_SEG);
+static int fast_drop;
+module_param_named(fast_drop, fast_drop, int, 0444);
+MODULE_PARM_DESC(fast_drop,
+		 "Enable fast packet drop when no recieve WQEs are posted");
+
+static struct mlx4_profile mod_param_profile = { 0 };
+
+module_param_named(log_num_qp, mod_param_profile.num_qp, int, 0444);
+MODULE_PARM_DESC(log_num_qp, "log maximum number of QPs per HCA");
+
+module_param_named(log_num_srq, mod_param_profile.num_srq, int, 0444);
+MODULE_PARM_DESC(log_num_srq, "log maximum number of SRQs per HCA");
+
+module_param_named(log_rdmarc_per_qp, mod_param_profile.rdmarc_per_qp, int, 0444);
+MODULE_PARM_DESC(log_rdmarc_per_qp, "log number of RDMARC buffers per QP");
+
+module_param_named(log_num_cq, mod_param_profile.num_cq, int, 0444);
+MODULE_PARM_DESC(log_num_cq, "log maximum number of CQs per HCA");
+
+module_param_named(log_num_mcg, mod_param_profile.num_mcg, int, 0444);
+MODULE_PARM_DESC(log_num_mcg, "log maximum number of multicast groups per HCA");
+
+module_param_named(log_num_mpt, mod_param_profile.num_mpt, int, 0444);
+MODULE_PARM_DESC(log_num_mpt,
+		"log maximum number of memory protection table entries per HCA");
+
+module_param_named(log_num_mtt, mod_param_profile.num_mtt, int, 0444);
+MODULE_PARM_DESC(log_num_mtt,
+		 "log maximum number of memory translation table segments per HCA");
+
+static int log_mtts_per_seg = ilog2(1);
 module_param_named(log_mtts_per_seg, log_mtts_per_seg, int, 0444);
-MODULE_PARM_DESC(log_mtts_per_seg, "Log2 number of MTT entries per segment (1-5)");
+MODULE_PARM_DESC(log_mtts_per_seg, "Log2 number of MTT entries per segment (0-7)");
+
+static void process_mod_param_profile(void)
+{
+	default_profile.num_qp = (mod_param_profile.num_qp ?
+				  1 << mod_param_profile.num_qp :
+				  default_profile.num_qp);
+	default_profile.num_srq = (mod_param_profile.num_srq ?
+				  1 << mod_param_profile.num_srq :
+				  default_profile.num_srq);
+	default_profile.rdmarc_per_qp = (mod_param_profile.rdmarc_per_qp ?
+				  1 << mod_param_profile.rdmarc_per_qp :
+				  default_profile.rdmarc_per_qp);
+	default_profile.num_cq = (mod_param_profile.num_cq ?
+				  1 << mod_param_profile.num_cq :
+				  default_profile.num_cq);
+	default_profile.num_mcg = (mod_param_profile.num_mcg ?
+				  1 << mod_param_profile.num_mcg :
+				  default_profile.num_mcg);
+	default_profile.num_mpt = (mod_param_profile.num_mpt ?
+				  1 << mod_param_profile.num_mpt :
+				  default_profile.num_mpt);
+	default_profile.num_mtt = (mod_param_profile.num_mtt ?
+				  1 << mod_param_profile.num_mtt :
+				  default_profile.num_mtt);
+}
+
+struct mlx4_port_config
+{
+	struct list_head list;
+	enum mlx4_port_type port_type[MLX4_MAX_PORTS + 1];
+	struct pci_dev *pdev;
+};
+static LIST_HEAD(config_list);
+
+static void mlx4_config_cleanup(void)
+{
+	struct mlx4_port_config *config, *tmp;
+
+	list_for_each_entry_safe(config, tmp, &config_list, list) {
+		list_del(&config->list);
+		kfree(config);
+	}
+}
+
+void mlx4_set_iboe_counter(struct mlx4_dev *dev, int index, u8 port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	priv->iboe_counter_index[port - 1] = index;
+}
+EXPORT_SYMBOL(mlx4_set_iboe_counter);
+
+int mlx4_get_iboe_counter(struct mlx4_dev *dev, u8 port)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	return priv->iboe_counter_index[port - 1];
+}
+EXPORT_SYMBOL(mlx4_get_iboe_counter);
 
 int mlx4_check_port_params(struct mlx4_dev *dev,
 			   enum mlx4_port_type *port_type)
@@ -132,15 +262,32 @@ int mlx4_check_port_params(struct mlx4_d
 	return 0;
 }
 
-static void mlx4_set_port_mask(struct mlx4_dev *dev)
+void mlx4_set_port_mask(struct mlx4_dev *dev, struct mlx4_caps *caps, int function)
 {
 	int i;
 
-	dev->caps.port_mask = 0;
-	for (i = 1; i <= dev->caps.num_ports; ++i)
-		if (dev->caps.port_type[i] == MLX4_PORT_TYPE_IB)
-			dev->caps.port_mask |= 1 << (i - 1);
+	for (i = 1; i <= caps->num_ports; ++i) {
+		if (mlx4_is_master(dev) && (dev->caps.pf_num > 1) &&
+		    mlx4_priv(dev)->mfunc.master.slave_state[function].port_num != i)
+			caps->port_mask[i] = 0;
+		else
+			caps->port_mask[i] = caps->port_type[i];
+	}
 }
+
+static u8 get_counters_mode(u64 flags)
+{
+	switch (flags >> 48 & 3) {
+	case 2:
+	case 3:
+		return MLX4_CUNTERS_EXT;
+	case 1:
+		return MLX4_CUNTERS_BASIC;
+	default:
+		return MLX4_CUNTERS_DISABLED;
+	}
+}
+
 static int mlx4_dev_cap(struct mlx4_dev *dev, struct mlx4_dev_cap *dev_cap)
 {
 	int err;
@@ -173,6 +320,12 @@ static int mlx4_dev_cap(struct mlx4_dev 
 		return -ENODEV;
 	}
 
+	if (enable_qinq && !dev->caps.qinq) {
+		mlx4_warn(dev, "Ignoring setting of QinQ"
+				"No HW capability\n");
+	}
+
+	dev->caps.pf_num = dev_cap->pf_num;
 	dev->caps.num_ports	     = dev_cap->num_ports;
 	for (i = 1; i <= dev->caps.num_ports; ++i) {
 		dev->caps.vl_cap[i]	    = dev_cap->max_vl[i];
@@ -183,8 +336,15 @@ static int mlx4_dev_cap(struct mlx4_dev 
 		dev->caps.eth_mtu_cap[i]    = dev_cap->eth_mtu[i];
 		dev->caps.def_mac[i]        = dev_cap->def_mac[i];
 		dev->caps.supported_type[i] = dev_cap->supported_port_types[i];
+		dev->caps.suggested_type[i] = dev_cap->suggested_port[i];
+		dev->caps.default_sense[i] = dev_cap->default_sense[i];
+		dev->caps.trans_type[i]	    = dev_cap->trans_type[i];
+		dev->caps.vendor_oui[i]     = dev_cap->vendor_oui[i];
+		dev->caps.wavelength[i]     = dev_cap->wavelength[i];
+		dev->caps.trans_code[i]     = dev_cap->trans_code[i];
 	}
 
+	dev->caps.uar_page_size	     = PAGE_SIZE;
 	dev->caps.num_uars	     = dev_cap->uar_size / PAGE_SIZE;
 	dev->caps.local_ca_ack_delay = dev_cap->local_ca_ack_delay;
 	dev->caps.bf_reg_size	     = dev_cap->bf_reg_size;
@@ -208,10 +368,14 @@ static int mlx4_dev_cap(struct mlx4_dev 
 	dev->caps.reserved_cqs	     = dev_cap->reserved_cqs;
 	dev->caps.reserved_eqs	     = dev_cap->reserved_eqs;
 	dev->caps.mtts_per_seg	     = 1 << log_mtts_per_seg;
+	if (mlx4_is_mfunc(dev))
+		dev->caps.mtts_per_seg = 1 << ilog2(MLX4_MTT_ENTRY_PER_SEG);
 	dev->caps.reserved_mtts	     = DIV_ROUND_UP(dev_cap->reserved_mtts,
 						    dev->caps.mtts_per_seg);
 	dev->caps.reserved_mrws	     = dev_cap->reserved_mrws;
-	dev->caps.reserved_uars	     = dev_cap->reserved_uars;
+
+	/* The first 128 UARs are used for EQ doorbells */
+	dev->caps.reserved_uars	     = max_t(int, 128, dev_cap->reserved_uars);
 	dev->caps.reserved_pds	     = dev_cap->reserved_pds;
 	dev->caps.mtt_entry_sz	     = dev->caps.mtts_per_seg * dev_cap->mtt_entry_sz;
 	dev->caps.max_msg_sz         = dev_cap->max_msg_sz;
@@ -220,20 +384,45 @@ static int mlx4_dev_cap(struct mlx4_dev 
 	dev->caps.bmme_flags	     = dev_cap->bmme_flags;
 	dev->caps.reserved_lkey	     = dev_cap->reserved_lkey;
 	dev->caps.stat_rate_support  = dev_cap->stat_rate_support;
+	dev->caps.udp_rss	     = dev_cap->udp_rss;
+	dev->caps.loopback_support   = dev_cap->loopback_support;
+	dev->caps.vep_uc_steering    = dev_cap->vep_uc_steering;
+	dev->caps.vep_mc_steering    = dev_cap->vep_mc_steering;
+	if (high_rate_steer && !mlx4_is_mfunc(dev)) {
+		dev->caps.vep_uc_steering = 0;
+		dev->caps.vep_mc_steering = 0;
+	}
+	dev->caps.wol                = dev_cap->wol;
 	dev->caps.max_gso_sz	     = dev_cap->max_gso_sz;
+	dev->caps.reserved_xrcds     = (dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC) ?
+		dev_cap->reserved_xrcds : 0;
+	dev->caps.max_xrcds	     = (dev->caps.flags & MLX4_DEV_CAP_FLAG_XRC) ?
+		dev_cap->max_xrcds : 0;
+
+	/* Sense port always allowed on supported devices for ConnectX1 and 2 */
+	if (dev->rev_id == 0xa0 || dev->rev_id == 0xb0)
+		dev->caps.flags |= MLX4_DEV_CAP_SENSE_SUPPORT;
 
 	dev->caps.log_num_macs  = log_num_mac;
-	dev->caps.log_num_vlans = log_num_vlan;
 	dev->caps.log_num_prios = use_prio ? 3 : 0;
+	dev->caps.fast_drop	= fast_drop ? dev_cap->fast_drop : 0;
+	dev->caps.qinq          = dev_cap->qinq && enable_qinq;
 
 	for (i = 1; i <= dev->caps.num_ports; ++i) {
-		if (dev->caps.supported_type[i] != MLX4_PORT_TYPE_ETH)
-			dev->caps.port_type[i] = MLX4_PORT_TYPE_IB;
-		else
-			dev->caps.port_type[i] = MLX4_PORT_TYPE_ETH;
-		dev->caps.possible_type[i] = dev->caps.port_type[i];
-		mlx4_priv(dev)->sense.sense_allowed[i] =
-			dev->caps.supported_type[i] == MLX4_PORT_TYPE_AUTO;
+		dev->caps.port_type[i] = MLX4_PORT_TYPE_NONE;
+		/*
+		 * Port type is defaulted to Ethernet in 1 of 2 cases:
+		 * 1. Ethernet is the only supported type.
+		 * 2. Ethernet is supported and suggested type is also Ethernet
+		 * Otherwise port type is IB by default
+		 */
+		if (dev->caps.supported_type[i]) {
+			if (!(dev->caps.supported_type[i] & MLX4_PORT_TYPE_ETH))
+				dev->caps.port_type[i] = MLX4_PORT_TYPE_IB;
+			else
+				dev->caps.port_type[i] = MLX4_PORT_TYPE_ETH;
+		}
+
 
 		if (dev->caps.log_num_macs > dev_cap->log_max_macs[i]) {
 			dev->caps.log_num_macs = dev_cap->log_max_macs[i];
@@ -241,15 +430,12 @@ static int mlx4_dev_cap(struct mlx4_dev 
 				  "for port %d, reducing to %d.\n",
 				  i, 1 << dev->caps.log_num_macs);
 		}
-		if (dev->caps.log_num_vlans > dev_cap->log_max_vlans[i]) {
-			dev->caps.log_num_vlans = dev_cap->log_max_vlans[i];
-			mlx4_warn(dev, "Requested number of VLANs is too much "
-				  "for port %d, reducing to %d.\n",
-				  i, 1 << dev->caps.log_num_vlans);
-		}
+		dev->caps.log_num_vlans = dev_cap->log_max_vlans[i];
 	}
 
-	mlx4_set_port_mask(dev);
+	dev->caps.counters_mode = get_counters_mode(dev_cap->flags);
+	dev->caps.max_basic_counters = 1 << ilog2(dev_cap->max_basic_counters);
+	dev->caps.max_ext_counters = 1 << ilog2(dev_cap->max_ext_counters);
 
 	dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW] = dev_cap->reserved_qps;
 	dev->caps.reserved_qps_cnt[MLX4_QP_REGION_ETH_ADDR] =
@@ -258,12 +444,107 @@ static int mlx4_dev_cap(struct mlx4_dev 
 		(1 << dev->caps.log_num_vlans) *
 		(1 << dev->caps.log_num_prios) *
 		dev->caps.num_ports;
-	dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_EXCH] = MLX4_NUM_FEXCH;
 
 	dev->caps.reserved_qps = dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW] +
 		dev->caps.reserved_qps_cnt[MLX4_QP_REGION_ETH_ADDR] +
-		dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_ADDR] +
-		dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_EXCH];
+		dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FC_ADDR];
+
+	dev->caps.sync_qp = dev_cap->sync_qp;
+	/* CX3 is capable of extending the CQE\EQE from 32 to 64 bytes */
+	dev->caps.cqe_size   = (dev_cap->flags & (1ull << 62)) ? 64 : 32;
+	dev->caps.eqe_size   = (dev_cap->flags & (1ull << 61)) ? 64 : 32;
+	dev->caps.eqe_factor = (dev->caps.eqe_size == 64) ? 1 : 0;
+
+	/* Master function demultiplexes mads */
+	dev->caps.sqp_demux = MLX4_MAX_NUM_SLAVES;
+	dev->caps.clp_ver = dev_cap->clp_ver;
+	return 0;
+}
+
+int mlx4_slave_cap(struct mlx4_dev *dev)
+{
+	int err;
+	u32 page_size;
+
+	err = mlx4_QUERY_SLAVE_CAP(dev, &dev->caps);
+	if (err)
+		return err;
+
+	page_size = ~dev->caps.page_size_cap + 1;
+	mlx4_warn(dev, "HCA minimum page size:%d\n", page_size);
+	if (page_size > PAGE_SIZE) {
+		mlx4_err(dev, "HCA minimum page size of %d bigger than "
+			 "kernel PAGE_SIZE of %ld, aborting.\n",
+			 page_size, PAGE_SIZE);
+		return -ENODEV;
+	}
+
+	/* TODO: relax this assumption */
+	if (dev->caps.uar_page_size != PAGE_SIZE) {
+		mlx4_err(dev, "UAR size:%d != kernel PAGE_SIZE of %ld\n",
+			 dev->caps.uar_page_size, PAGE_SIZE);
+		return -ENODEV;
+	}
+
+	if (dev->caps.num_ports > MLX4_MAX_PORTS) {
+		mlx4_err(dev, "HCA has %d ports, but we only support %d, "
+			 "aborting.\n", dev->caps.num_ports, MLX4_MAX_PORTS);
+		return -ENODEV;
+	}
+
+	if (dev->caps.uar_page_size * (dev->caps.num_uars -
+				       dev->caps.reserved_uars) >
+				       pci_resource_len(dev->pdev, 2)) {
+		mlx4_err(dev, "HCA reported UAR region size of 0x%x bigger than "
+			 "PCI resource 2 size of 0x%llx, aborting.\n",
+			 dev->caps.uar_page_size * dev->caps.num_uars,
+			 (unsigned long long) pci_resource_len(dev->pdev, 2));
+		return -ENODEV;
+	}
+
+	/* Adjust eq number */
+	if (dev->caps.num_eqs - dev->caps.reserved_eqs > num_possible_cpus() + 1)
+		dev->caps.num_eqs = dev->caps.reserved_eqs + num_possible_cpus() + 1;
+
+#if 0
+	mlx4_warn(dev, "sqp_demux:%d\n", dev->caps.sqp_demux);
+	mlx4_warn(dev, "num_uars:%d reserved_uars:%d uar region:0x%x bar2:0x%llx\n",
+					  dev->caps.num_uars, dev->caps.reserved_uars,
+					  dev->caps.uar_page_size * dev->caps.num_uars,
+					  pci_resource_len(dev->pdev, 2));
+	mlx4_warn(dev, "num_eqs:%d reserved_eqs:%d\n", dev->caps.num_eqs,
+						       dev->caps.reserved_eqs);
+	mlx4_warn(dev, "num_pds:%d reserved_pds:%d slave_pd_shift:%d pd_base:%d\n",
+							dev->caps.num_pds,
+							dev->caps.reserved_pds,
+							dev->caps.slave_pd_shift,
+							dev->caps.pd_base);
+#endif
+	return 0;
+}
+
+static int mlx4_save_config(struct mlx4_dev *dev)
+{
+	struct mlx4_port_config *config;
+	int i;
+
+	list_for_each_entry(config, &config_list, list) {
+		if (config->pdev == dev->pdev) {
+			for (i = 1; i <= dev->caps.num_ports; i++)
+				config->port_type[i] = dev->caps.possible_type[i];
+			return 0;
+		}
+	}
+
+	config = kmalloc(sizeof(struct mlx4_port_config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	config->pdev = dev->pdev;
+	for (i = 1; i <= dev->caps.num_ports; i++)
+		config->port_type[i] = dev->caps.possible_type[i];
+
+	list_add_tail(&config->list, &config_list);
 
 	return 0;
 }
@@ -282,15 +563,14 @@ int mlx4_change_port_types(struct mlx4_d
 	for (port = 0; port <  dev->caps.num_ports; port++) {
 		/* Change the port type only if the new type is different
 		 * from the current, and not set to Auto */
-		if (port_types[port] != dev->caps.port_type[port + 1]) {
+		if (port_types[port] != dev->caps.port_type[port + 1])
 			change = 1;
-			dev->caps.port_type[port + 1] = port_types[port];
-		}
 	}
 	if (change) {
 		mlx4_unregister_device(dev);
 		for (port = 1; port <= dev->caps.num_ports; port++) {
 			mlx4_CLOSE_PORT(dev, port);
+			dev->caps.port_type[port] = port_types[port - 1];
 			err = mlx4_SET_PORT(dev, port);
 			if (err) {
 				mlx4_err(dev, "Failed to set port %d, "
@@ -298,7 +578,8 @@ int mlx4_change_port_types(struct mlx4_d
 				goto out;
 			}
 		}
-		mlx4_set_port_mask(dev);
+		mlx4_set_port_mask(dev, &dev->caps, dev->caps.function);
+		mlx4_save_config(dev);
 		err = mlx4_register_device(dev);
 	}
 
@@ -306,96 +587,22 @@ out:
 	return err;
 }
 
-static ssize_t show_port_type(struct device *dev,
-			      struct device_attribute *attr,
-			      char *buf)
+static ssize_t trigger_port(struct device *dev, struct device_attribute *attr,
+			    const char *buf, size_t count)
 {
-	struct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,
-						   port_attr);
-	struct mlx4_dev *mdev = info->dev;
-	char type[8];
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct mlx4_dev *mdev = pci_get_drvdata(pdev);
+	struct mlx4_priv *priv = container_of(mdev, struct mlx4_priv, dev);
 
-	sprintf(type, "%s",
-		(mdev->caps.port_type[info->port] == MLX4_PORT_TYPE_IB) ?
-		"ib" : "eth");
-	if (mdev->caps.possible_type[info->port] == MLX4_PORT_TYPE_AUTO)
-		sprintf(buf, "auto (%s)\n", type);
-	else
-		sprintf(buf, "%s\n", type);
+	if (!priv)
+		return -ENODEV;
 
-	return strlen(buf);
+	mutex_lock(&priv->port_mutex);
+	priv->trig = 1;
+	mutex_unlock(&priv->port_mutex);
+	return count;
 }
-
-static ssize_t set_port_type(struct device *dev,
-			     struct device_attribute *attr,
-			     const char *buf, size_t count)
-{
-	struct mlx4_port_info *info = container_of(attr, struct mlx4_port_info,
-						   port_attr);
-	struct mlx4_dev *mdev = info->dev;
-	struct mlx4_priv *priv = mlx4_priv(mdev);
-	enum mlx4_port_type types[MLX4_MAX_PORTS];
-	enum mlx4_port_type new_types[MLX4_MAX_PORTS];
-	int i;
-	int err = 0;
-
-	if (!strcmp(buf, "ib\n"))
-		info->tmp_type = MLX4_PORT_TYPE_IB;
-	else if (!strcmp(buf, "eth\n"))
-		info->tmp_type = MLX4_PORT_TYPE_ETH;
-	else if (!strcmp(buf, "auto\n"))
-		info->tmp_type = MLX4_PORT_TYPE_AUTO;
-	else {
-		mlx4_err(mdev, "%s is not supported port type\n", buf);
-		return -EINVAL;
-	}
-
-	mlx4_stop_sense(mdev);
-	mutex_lock(&priv->port_mutex);
-	/* Possible type is always the one that was delivered */
-	mdev->caps.possible_type[info->port] = info->tmp_type;
-
-	for (i = 0; i < mdev->caps.num_ports; i++) {
-		types[i] = priv->port[i+1].tmp_type ? priv->port[i+1].tmp_type :
-					mdev->caps.possible_type[i+1];
-		if (types[i] == MLX4_PORT_TYPE_AUTO)
-			types[i] = mdev->caps.port_type[i+1];
-	}
-
-	if (!(mdev->caps.flags & MLX4_DEV_CAP_FLAG_DPDP)) {
-		for (i = 1; i <= mdev->caps.num_ports; i++) {
-			if (mdev->caps.possible_type[i] == MLX4_PORT_TYPE_AUTO) {
-				mdev->caps.possible_type[i] = mdev->caps.port_type[i];
-				err = -EINVAL;
-			}
-		}
-	}
-	if (err) {
-		mlx4_err(mdev, "Auto sensing is not supported on this HCA. "
-			       "Set only 'eth' or 'ib' for both ports "
-			       "(should be the same)\n");
-		goto out;
-	}
-
-	mlx4_do_sense_ports(mdev, new_types, types);
-
-	err = mlx4_check_port_params(mdev, new_types);
-	if (err)
-		goto out;
-
-	/* We are about to apply the changes after the configuration
-	 * was verified, no need to remember the temporary types
-	 * any more */
-	for (i = 0; i < mdev->caps.num_ports; i++)
-		priv->port[i + 1].tmp_type = 0;
-
-	err = mlx4_change_port_types(mdev, new_types);
-
-out:
-	mlx4_start_sense(mdev);
-	mutex_unlock(&priv->port_mutex);
-	return err ? err : count;
-}
+DEVICE_ATTR(port_trigger, S_IWUGO, NULL, trigger_port);
 
 static int mlx4_load_fw(struct mlx4_dev *dev)
 {
@@ -436,6 +643,7 @@ static int mlx4_init_cmpt_table(struct m
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	int err;
+	int num_eqs;
 
 	err = mlx4_init_icm_table(dev, &priv->qp_table.cmpt_table,
 				  cmpt_base +
@@ -465,12 +673,12 @@ static int mlx4_init_cmpt_table(struct m
 	if (err)
 		goto err_srq;
 
+	num_eqs = mlx4_is_master(dev) ? 512 : dev->caps.num_eqs;
 	err = mlx4_init_icm_table(dev, &priv->eq_table.cmpt_table,
 				  cmpt_base +
 				  ((u64) (MLX4_CMPT_TYPE_EQ *
 					  cmpt_entry_sz) << MLX4_CMPT_SHIFT),
-				  cmpt_entry_sz,
-				  dev->caps.num_eqs, dev->caps.num_eqs, 0, 0);
+				  cmpt_entry_sz, num_eqs, num_eqs, 0, 0);
 	if (err)
 		goto err_cq;
 
@@ -494,6 +702,7 @@ static int mlx4_init_icm(struct mlx4_dev
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	u64 aux_pages;
+	int num_eqs;
 	int err;
 
 	err = mlx4_SET_ICM_SIZE(dev, icm_size, &aux_pages);
@@ -525,10 +734,11 @@ static int mlx4_init_icm(struct mlx4_dev
 		goto err_unmap_aux;
 	}
 
+
+	num_eqs = mlx4_is_master(dev) ? 512 : dev->caps.num_eqs;
 	err = mlx4_init_icm_table(dev, &priv->eq_table.table,
 				  init_hca->eqc_base, dev_cap->eqc_entry_sz,
-				  dev->caps.num_eqs, dev->caps.num_eqs,
-				  0, 0);
+				  num_eqs, num_eqs, 0, 0);
 	if (err) {
 		mlx4_err(dev, "Failed to map EQ context memory, aborting.\n");
 		goto err_unmap_cmpt;
@@ -711,12 +921,80 @@ static void mlx4_free_icms(struct mlx4_d
 	mlx4_free_icm(dev, priv->fw.aux_icm, 0);
 }
 
+static int map_bf_area(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	resource_size_t bf_start;
+	resource_size_t bf_len;
+	int err = 0;
+
+	bf_start = pci_resource_start(dev->pdev, 2) + (dev->caps.num_uars << PAGE_SHIFT);
+	bf_len = pci_resource_len(dev->pdev, 2) - (dev->caps.num_uars << PAGE_SHIFT);
+	priv->bf_mapping = io_mapping_create_wc(bf_start, bf_len);
+	if (!priv->bf_mapping)
+		err = -ENOMEM;
+
+	return err;
+}
+
+static void unmap_bf_area(struct mlx4_dev *dev)
+{
+	if (mlx4_priv(dev)->bf_mapping)
+		io_mapping_free(mlx4_priv(dev)->bf_mapping);
+}
+
+static void mlx4_slave_exit(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	down(&priv->cmd.slave_sem);
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0, MLX4_COMM_TIME))
+		mlx4_warn(dev, "Failed to close slave function.\n");
+	up(&priv->cmd.slave_sem);
+}
+
 static void mlx4_close_hca(struct mlx4_dev *dev)
 {
-	mlx4_CLOSE_HCA(dev, 0);
-	mlx4_free_icms(dev);
-	mlx4_UNMAP_FA(dev);
-	mlx4_free_icm(dev, mlx4_priv(dev)->fw.fw_icm, 0);
+	if (mlx4_is_slave(dev))
+		mlx4_slave_exit(dev);
+	else {
+		unmap_bf_area(dev);
+		mlx4_CLOSE_HCA(dev, 0);
+		mlx4_free_icms(dev);
+		mlx4_UNMAP_FA(dev);
+		mlx4_free_icm(dev, mlx4_priv(dev)->fw.fw_icm, 0);
+	}
+}
+
+static int mlx4_init_slave(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u64 dma = (u64) priv->mfunc.vhcr_dma;
+
+	down(&priv->cmd.slave_sem);
+	priv->cmd.max_cmds = 1;
+	mlx4_warn(dev, "Sending reset\n");
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0, MLX4_COMM_TIME))
+		goto err;
+	mlx4_warn(dev, "Sending vhcr0\n");
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR0, dma >> 48,
+						    MLX4_COMM_TIME))
+		goto err;
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR1, dma >> 32,
+						    MLX4_COMM_TIME))
+		goto err;
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR2, dma >> 16,
+						    MLX4_COMM_TIME))
+		goto err;
+	if (mlx4_comm_cmd(dev, MLX4_COMM_CMD_VHCR_EN, dma, MLX4_COMM_TIME))
+		goto err;
+	up(&priv->cmd.slave_sem);
+	return 0;
+
+err:
+	mlx4_comm_cmd(dev, MLX4_COMM_CMD_RESET, 0, 0);
+	up(&priv->cmd.slave_sem);
+	return -EIO;
 }
 
 static int mlx4_init_hca(struct mlx4_dev *dev)
@@ -727,55 +1005,88 @@ static int mlx4_init_hca(struct mlx4_dev
 	struct mlx4_mod_stat_cfg   mlx4_cfg;
 	struct mlx4_profile	   profile;
 	struct mlx4_init_hca_param init_hca;
+	struct mlx4_port_config	  *config;
 	u64 icm_size;
 	int err;
+	int i;
 
-	err = mlx4_QUERY_FW(dev);
-	if (err) {
-		if (err == -EACCES)
-			mlx4_info(dev, "non-primary physical function, skipping.\n");
-		else
-			mlx4_err(dev, "QUERY_FW command failed, aborting.\n");
-		return err;
+	if (!mlx4_is_slave(dev)) {
+		err = mlx4_QUERY_FW(dev);
+		if (err) {
+			if (err == -EACCES)
+				mlx4_info(dev, "non-primary physical function, skipping.\n");
+			else
+				mlx4_err(dev, "QUERY_FW command failed, aborting.\n");
+			return err;
+		}
+
+		err = mlx4_load_fw(dev);
+		if (err) {
+			mlx4_err(dev, "Failed to start FW, aborting.\n");
+			return err;
+		}
+
+		mlx4_cfg.log_pg_sz_m = 1;
+		mlx4_cfg.log_pg_sz = 0;
+		err = mlx4_MOD_STAT_CFG(dev, &mlx4_cfg);
+		if (err)
+			mlx4_warn(dev, "Failed to override log_pg_sz parameter\n");
+
+		err = mlx4_dev_cap(dev, &dev_cap);
+		if (err) {
+			mlx4_err(dev, "QUERY_DEV_CAP command failed, aborting.\n");
+			goto err_stop_fw;
+		}
+
+		process_mod_param_profile();
+		profile = default_profile;
+
+		list_for_each_entry(config, &config_list, list) {
+			if (config->pdev == dev->pdev) {
+				for (i = 1; i <= dev->caps.num_ports; i++) {
+					dev->caps.possible_type[i] = config->port_type[i];
+					if (config->port_type[i] != MLX4_PORT_TYPE_AUTO)
+						dev->caps.port_type[i] = config->port_type[i];
+				}
+			}
+		}
+
+		icm_size = mlx4_make_profile(dev, &profile, &dev_cap, &init_hca);
+		if ((long long) icm_size < 0) {
+			err = icm_size;
+			goto err_stop_fw;
+		}
+
+		if (map_bf_area(dev))
+		mlx4_dbg(dev, "Kernel support for blue flame is not available for kernels < 2.6.28\n");
+
+		init_hca.log_uar_sz = ilog2(dev->caps.num_uars);
+
+		err = mlx4_init_icm(dev, &dev_cap, &init_hca, icm_size);
+		if (err)
+			goto err_stop_fw;
+
+		err = mlx4_INIT_HCA(dev, &init_hca);
+		if (err) {
+			mlx4_err(dev, "INIT_HCA command failed, aborting.\n");
+			goto err_free_icm;
+		}
+	} else {
+		err = mlx4_init_slave(dev);
+		if (err) {
+			mlx4_err(dev, "Failed to initialize slave\n");
+			return err;
+		}
+
+		err = mlx4_slave_cap(dev);
+		if (err) {
+			mlx4_err(dev, "Failed to obtain slave caps\n");
+			goto err_close;
+		}
 	}
 
-	err = mlx4_load_fw(dev);
-	if (err) {
-		mlx4_err(dev, "Failed to start FW, aborting.\n");
-		return err;
-	}
-
-	mlx4_cfg.log_pg_sz_m = 1;
-	mlx4_cfg.log_pg_sz = 0;
-	err = mlx4_MOD_STAT_CFG(dev, &mlx4_cfg);
-	if (err)
-		mlx4_warn(dev, "Failed to override log_pg_sz parameter\n");
-
-	err = mlx4_dev_cap(dev, &dev_cap);
-	if (err) {
-		mlx4_err(dev, "QUERY_DEV_CAP command failed, aborting.\n");
-		goto err_stop_fw;
-	}
-
-	profile = default_profile;
-
-	icm_size = mlx4_make_profile(dev, &profile, &dev_cap, &init_hca);
-	if ((long long) icm_size < 0) {
-		err = icm_size;
-		goto err_stop_fw;
-	}
-
-	init_hca.log_uar_sz = ilog2(dev->caps.num_uars);
-
-	err = mlx4_init_icm(dev, &dev_cap, &init_hca, icm_size);
-	if (err)
-		goto err_stop_fw;
-
-	err = mlx4_INIT_HCA(dev, &init_hca);
-	if (err) {
-		mlx4_err(dev, "INIT_HCA command failed, aborting.\n");
-		goto err_free_icm;
-	}
+	if (!mlx4_is_mfunc(dev))
+		mlx4_set_port_mask(dev, &dev->caps, dev->caps.function);
 
 	err = mlx4_QUERY_ADAPTER(dev, &adapter);
 	if (err) {
@@ -792,15 +1103,83 @@ err_close:
 	mlx4_CLOSE_HCA(dev, 0);
 
 err_free_icm:
-	mlx4_free_icms(dev);
+	if (!mlx4_is_slave(dev))
+		mlx4_free_icms(dev);
 
 err_stop_fw:
-	mlx4_UNMAP_FA(dev);
-	mlx4_free_icm(dev, priv->fw.fw_icm, 0);
-
+	if (!mlx4_is_slave(dev)) {
+		unmap_bf_area(dev);
+		mlx4_UNMAP_FA(dev);
+		mlx4_free_icm(dev, priv->fw.fw_icm, 0);
+	}
 	return err;
 }
 
+static int mlx4_init_counters_table(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int err;
+	int nent;
+
+	switch (dev->caps.counters_mode) {
+	case MLX4_CUNTERS_BASIC:
+		nent = dev->caps.max_basic_counters;
+		break;
+	case MLX4_CUNTERS_EXT:
+		nent = dev->caps.max_ext_counters;
+		break;
+	default:
+		return -ENOENT;
+	}
+	err = mlx4_bitmap_init(&priv->counters_bitmap, nent, nent - 1, 0, 0);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static void mlx4_cleanup_counters_table(struct mlx4_dev *dev)
+{
+	switch (dev->caps.counters_mode) {
+	case MLX4_CUNTERS_BASIC:
+	case MLX4_CUNTERS_EXT:
+		mlx4_bitmap_cleanup(&mlx4_priv(dev)->counters_bitmap);
+		break;
+	default:
+		break;
+	}
+}
+
+int mlx4_counter_alloc(struct mlx4_dev *dev, u32 *idx)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	switch (dev->caps.counters_mode) {
+	case MLX4_CUNTERS_BASIC:
+	case MLX4_CUNTERS_EXT:
+		*idx = mlx4_bitmap_alloc(&priv->counters_bitmap);
+		if (*idx == -1)
+			return -ENOMEM;
+		return 0;
+	default:
+		return -ENOMEM;
+	}
+}
+EXPORT_SYMBOL_GPL(mlx4_counter_alloc);
+
+void mlx4_counter_free(struct mlx4_dev *dev, u32 idx)
+{
+	switch (dev->caps.counters_mode) {
+	case MLX4_CUNTERS_BASIC:
+	case MLX4_CUNTERS_EXT:
+		mlx4_bitmap_free(&mlx4_priv(dev)->counters_bitmap, idx);
+		return;
+	default:
+		return;
+	}
+}
+EXPORT_SYMBOL_GPL(mlx4_counter_free);
+
 static int mlx4_setup_hca(struct mlx4_dev *dev)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -837,24 +1216,38 @@ static int mlx4_setup_hca(struct mlx4_de
 		goto err_kar_unmap;
 	}
 
+	err = mlx4_init_xrcd_table(dev);
+	if (err) {
+		mlx4_err(dev, "Failed to initialize extended "
+			 "reliably connected domain table, aborting.\n");
+		goto err_pd_table_free;
+	}
+
 	err = mlx4_init_mr_table(dev);
 	if (err) {
 		mlx4_err(dev, "Failed to initialize "
 			 "memory region table, aborting.\n");
-		goto err_pd_table_free;
+		goto err_xrcd_table_free;
+	}
+
+	err = mlx4_init_mcg_table(dev);
+	if (err) {
+		mlx4_err(dev, "Failed to initialize "
+			 "multicast group table, aborting.\n");
+		goto err_mr_table_free;
 	}
 
 	err = mlx4_init_eq_table(dev);
 	if (err) {
 		mlx4_err(dev, "Failed to initialize "
 			 "event queue table, aborting.\n");
-		goto err_mr_table_free;
+		goto err_mcg_table_free;
 	}
 
 	err = mlx4_cmd_use_events(dev);
 	if (err) {
 		mlx4_err(dev, "Failed to switch to event-driven "
-			 "firmware commands, aborting.\n");
+			      "firmware commands, aborting.\n");
 		goto err_eq_table_free;
 	}
 
@@ -898,33 +1291,42 @@ static int mlx4_setup_hca(struct mlx4_de
 		goto err_srq_table_free;
 	}
 
-	err = mlx4_init_mcg_table(dev);
-	if (err) {
-		mlx4_err(dev, "Failed to initialize "
-			 "multicast group table, aborting.\n");
+
+	err = mlx4_init_counters_table(dev);
+	if (err && err != -ENOENT) {
+		mlx4_err(dev, "Failed to initialize counters table, aborting.\n");
 		goto err_qp_table_free;
 	}
 
-	for (port = 1; port <= dev->caps.num_ports; port++) {
-		ib_port_default_caps = 0;
-		err = mlx4_get_port_ib_caps(dev, port, &ib_port_default_caps);
-		if (err)
-			mlx4_warn(dev, "failed to get port %d default "
-				  "ib capabilities (%d). Continuing with "
-				  "caps = 0\n", port, err);
-		dev->caps.ib_port_def_cap[port] = ib_port_default_caps;
-		err = mlx4_SET_PORT(dev, port);
-		if (err) {
-			mlx4_err(dev, "Failed to set port %d, aborting\n",
-				port);
-			goto err_mcg_table_free;
+	if (!mlx4_is_slave(dev)) {
+		for (port = 1; port <= dev->caps.num_ports; port++) {
+			ib_port_default_caps = 0;
+			err = mlx4_get_port_ib_caps(dev, port, &ib_port_default_caps);
+			if (err)
+				mlx4_warn(dev, "failed to get port %d default "
+					  "ib capabilities (%d). Continuing with "
+					  "caps = 0\n", port, err);
+			dev->caps.ib_port_def_cap[port] = ib_port_default_caps;
+
+		      	err = mlx4_check_ext_port_caps(dev, port);
+		      	if (err)
+				mlx4_warn(dev, "failed to get port %d extended "
+		      			  "port capabilities support info (%d)."
+					  " Assuming not supported\n", port, err);
+
+			err = mlx4_SET_PORT(dev, port);
+			if (err) {
+				mlx4_err(dev, "Failed to set port %d, aborting\n",
+					port);
+				goto err_counters_table_free;
+			}
 		}
 	}
 
 	return 0;
 
-err_mcg_table_free:
-	mlx4_cleanup_mcg_table(dev);
+err_counters_table_free:
+	mlx4_cleanup_counters_table(dev);
 
 err_qp_table_free:
 	mlx4_cleanup_qp_table(dev);
@@ -941,9 +1343,15 @@ err_cmd_poll:
 err_eq_table_free:
 	mlx4_cleanup_eq_table(dev);
 
+err_mcg_table_free:
+	mlx4_cleanup_mcg_table(dev);
+
 err_mr_table_free:
 	mlx4_cleanup_mr_table(dev);
 
+err_xrcd_table_free:
+	mlx4_cleanup_xrcd_table(dev);
+
 err_pd_table_free:
 	mlx4_cleanup_pd_table(dev);
 
@@ -962,13 +1370,23 @@ static void mlx4_enable_msi_x(struct mlx
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct msix_entry *entries;
-	int nreq;
+	int nreq = min_t(int, dev->caps.num_ports *
+			 min_t(int, num_possible_cpus() + 1, MAX_MSIX_P_PORT)
+				+ MSIX_LEGACY_SZ, MAX_MSIX);
 	int err;
 	int i;
 
 	if (msi_x) {
-		nreq = min_t(int, dev->caps.num_eqs - dev->caps.reserved_eqs,
-			     num_possible_cpus() + 1);
+		/* In multifunction mode each function gets 2 msi-X vectors
+		 * one for data path completions anf the other for asynch events
+		 * or command completions */
+		if (mlx4_is_mfunc(dev)) {
+			nreq = 4;
+		} else {
+			nreq = min_t(int, dev->caps.num_eqs -
+				     dev->caps.reserved_eqs, nreq);
+		}
+
 		entries = kcalloc(nreq, sizeof *entries, GFP_KERNEL);
 		if (!entries)
 			goto no_msi;
@@ -991,7 +1409,15 @@ static void mlx4_enable_msi_x(struct mlx
 			goto no_msi;
 		}
 
-		dev->caps.num_comp_vectors = nreq - 1;
+		if (nreq <
+		    MSIX_LEGACY_SZ + dev->caps.num_ports * MIN_MSIX_P_PORT) {
+			/*Working in legacy mode , all EQ's shared*/
+			dev->caps.poolsz           = 0;
+			dev->caps.num_comp_vectors = nreq - 1;
+		} else {
+			dev->caps.poolsz           = nreq - MSIX_LEGACY_SZ;
+			dev->caps.num_comp_vectors = MSIX_LEGACY_SZ - 1;
+		}
 		for (i = 0; i < nreq; ++i)
 			priv->eq_table.eq[i].irq = entries[i].vector;
 
@@ -1003,6 +1429,7 @@ static void mlx4_enable_msi_x(struct mlx
 
 no_msi:
 	dev->caps.num_comp_vectors = 1;
+	dev->caps.poolsz	   = 0;
 
 	for (i = 0; i < 2; ++i)
 		priv->eq_table.eq[i].irq = dev->pdev->irq;
@@ -1015,19 +1442,12 @@ static int mlx4_init_port_info(struct ml
 
 	info->dev = dev;
 	info->port = port;
-	mlx4_init_mac_table(dev, &info->mac_table);
-	mlx4_init_vlan_table(dev, &info->vlan_table);
-
-	sprintf(info->dev_name, "mlx4_port%d", port);
-	info->port_attr.attr.name = info->dev_name;
-	info->port_attr.attr.mode = S_IRUGO | S_IWUSR;
-	info->port_attr.show      = show_port_type;
-	info->port_attr.store     = set_port_type;
-
-	err = device_create_file(&dev->pdev->dev, &info->port_attr);
-	if (err) {
-		mlx4_err(dev, "Failed to create file for port %d\n", port);
-		info->port = -1;
+	if (!mlx4_is_slave(dev)) {
+		INIT_RADIX_TREE(&info->mac_tree, GFP_KERNEL);
+		mlx4_init_mac_table(dev, &info->mac_table);
+		mlx4_init_vlan_table(dev, &info->vlan_table);
+		info->base_qpn = dev->caps.reserved_qps_base[MLX4_QP_REGION_ETH_ADDR] +
+			(port - 1) * (1 << log_num_mac);
 	}
 
 	return err;
@@ -1037,8 +1457,59 @@ static void mlx4_cleanup_port_info(struc
 {
 	if (info->port < 0)
 		return;
+}
 
-	device_remove_file(&info->dev->pdev->dev, &info->port_attr);
+static int mlx4_init_steering(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int num_entries = max(dev->caps.num_ports, dev->caps.pf_num);
+	int i, j;
+
+	priv->steer = kzalloc(sizeof(struct mlx4_steer) * num_entries, GFP_KERNEL);
+	if (!priv->steer)
+		return -ENOMEM;
+
+	for (i = 0; i < num_entries; i++) {
+		for (j = 0; j < MLX4_NUM_STEERS; j++) {
+			INIT_LIST_HEAD(&priv->steer[i].promisc_qps[j]);
+			INIT_LIST_HEAD(&priv->steer[i].steer_entries[j]);
+		}
+		INIT_LIST_HEAD(&priv->steer[i].high_prios);
+	}
+	return 0;
+}
+
+static void mlx4_clear_steering(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_steer_index *entry, *tmp_entry;
+	struct mlx4_promisc_qp *pqp, *tmp_pqp;
+	int num_entries = max(dev->caps.num_ports, dev->caps.pf_num);
+	int i, j;
+
+	for (i = 0; i < num_entries; i++) {
+		for (j = 0; j < MLX4_NUM_STEERS; j++) {
+			list_for_each_entry_safe(pqp, tmp_pqp,
+						 &priv->steer[i].promisc_qps[j],
+						 list) {
+				list_del(&pqp->list);
+				kfree(pqp);
+			}
+			list_for_each_entry_safe(entry, tmp_entry,
+						 &priv->steer[i].steer_entries[j],
+						 list) {
+				list_del(&entry->list);
+				list_for_each_entry_safe(pqp, tmp_pqp,
+							 &entry->duplicates,
+							 list) {
+					list_del(&pqp->list);
+					kfree(pqp);
+				}
+				kfree(entry);
+			}
+		}
+	}
+	kfree(priv->steer);
 }
 
 static int __mlx4_init_one(struct pci_dev *pdev, const struct pci_device_id *id)
@@ -1047,9 +1518,9 @@ static int __mlx4_init_one(struct pci_de
 	struct mlx4_dev *dev;
 	int err;
 	int port;
+	int i;
 
-	printk(KERN_INFO PFX "Initializing %s\n",
-	       pci_name(pdev));
+	printk(KERN_INFO PFX "Initializing %s\n", pci_name(pdev));
 
 	err = pci_enable_device(pdev);
 	if (err) {
@@ -1061,7 +1532,8 @@ static int __mlx4_init_one(struct pci_de
 	/*
 	 * Check for BARs.  We expect 0: 1MB
 	 */
-	if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM) ||
+	if ((((id == NULL) || !(id->driver_data & MLX4_VF)) &&
+	     !(pci_resource_flags(pdev, 0) & IORESOURCE_MEM)) ||
 	    pci_resource_len(pdev, 0) != 1 << 20) {
 		dev_err(&pdev->dev, "Missing DCS, aborting.\n");
 		err = -ENODEV;
@@ -1073,12 +1545,18 @@ static int __mlx4_init_one(struct pci_de
 		goto err_disable_pdev;
 	}
 
-	err = pci_request_regions(pdev, DRV_NAME);
+	err = pci_request_region(pdev, 0, DRV_NAME);
 	if (err) {
-		dev_err(&pdev->dev, "Couldn't get PCI resources, aborting\n");
+		dev_err(&pdev->dev, "Cannot request control region, aborting.\n");
 		goto err_disable_pdev;
 	}
 
+	err = pci_request_region(pdev, 2, DRV_NAME);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot request UAR region, aborting.\n");
+		goto err_release_bar0;
+	}
+
 	pci_set_master(pdev);
 
 	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
@@ -1087,7 +1565,7 @@ static int __mlx4_init_one(struct pci_de
 		err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));
 		if (err) {
 			dev_err(&pdev->dev, "Can't set PCI DMA mask, aborting.\n");
-			goto err_release_regions;
+			goto err_release_bar2;
 		}
 	}
 	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));
@@ -1098,7 +1576,7 @@ static int __mlx4_init_one(struct pci_de
 		if (err) {
 			dev_err(&pdev->dev, "Can't set consistent PCI DMA mask, "
 				"aborting.\n");
-			goto err_release_regions;
+			goto err_release_bar2;
 		}
 	}
 
@@ -1107,7 +1585,7 @@ static int __mlx4_init_one(struct pci_de
 		dev_err(&pdev->dev, "Device struct alloc failed, "
 			"aborting.\n");
 		err = -ENOMEM;
-		goto err_release_regions;
+		goto err_release_bar2;
 	}
 
 	dev       = &priv->dev;
@@ -1116,45 +1594,144 @@ static int __mlx4_init_one(struct pci_de
 	spin_lock_init(&priv->ctx_lock);
 
 	mutex_init(&priv->port_mutex);
+	mutex_init(&priv->port_ops_mutex);
 
 	INIT_LIST_HEAD(&priv->pgdir_list);
 	mutex_init(&priv->pgdir_mutex);
+	for (i = 0; i < MLX4_MAX_PORTS; ++i)
+		priv->iboe_counter_index[i] = -1;
 
-	/*
-	 * Now reset the HCA before we touch the PCI capabilities or
-	 * attempt a firmware command, since a boot ROM may have left
-	 * the HCA in an undefined state.
-	 */
-	err = mlx4_reset(dev);
-	if (err) {
-		mlx4_err(dev, "Failed to reset HCA, aborting.\n");
+	INIT_LIST_HEAD(&priv->bf_list);
+	mutex_init(&priv->bf_mutex);
+
+	atomic_set(&priv->opreq_count, 0);
+	priv->opreq_queue = create_singlethread_workqueue("mlx4_opreq");
+	if (!priv->opreq_queue) {
+		mlx4_err(dev, "Failed to create workqueue for FW tasks");
+		err = -ENOMEM;
 		goto err_free_dev;
 	}
+	INIT_WORK(&priv->opreq_task, mlx4_opreq_action);
 
+	pci_read_config_byte(pdev, PCI_REVISION_ID, &dev->rev_id);
+
+	/* Detect if this device is a virtual function */
+	if (id && id->driver_data & MLX4_VF) {
+		/* When acting as pf, we normally skip vfs unless explicitly
+		 * requested to probe them. */
+		if (sr_iov && PCI_FUNC(pdev->devfn) > probe_vf) {
+			mlx4_warn(dev, "Skipping virtual function:%d\n",
+						PCI_FUNC(pdev->devfn));
+			err = -ENODEV;
+			goto err_workqueue;
+		}
+		mlx4_warn(dev, "Detected virtual function - running in slave mode\n");
+		dev->flags |= MLX4_FLAG_SLAVE;
+	}
+
+	/* We reset the device and enable SRIOV only for physical devices */
+	if (!mlx4_is_slave(dev)) {
+		/* Claim ownership on the device,
+		 * if already taken, act as slave*/
+		err = mlx4_get_ownership(dev);
+		if (err) {
+			if (err < 0) {
+				goto err_workqueue;
+			}
+			else {
+				err = 0;
+				dev->flags |= MLX4_FLAG_SLAVE;
+				goto slave_start;
+			}
+		}
+
+		/*
+		 * Now reset the HCA before we touch the PCI capabilities or
+		 * attempt a firmware command, since a boot ROM may have left
+		 * the HCA in an undefined state.
+		 */
+		err = mlx4_reset(dev);
+		if (err) {
+			mlx4_err(dev, "Failed to reset HCA, aborting.\n");
+			goto err_workqueue;
+		}
+		if (sr_iov) {
+			mlx4_warn(dev, "Enabling sriov with:%d vfs\n", sr_iov);
+			if (pci_enable_sriov(pdev, sr_iov)) {
+				mlx4_err(dev, "Failed to enable sriov, aborting.\n");
+				goto err_workqueue;
+			}
+			mlx4_warn(dev, "Running in master mode\n");
+			dev->flags |= MLX4_FLAG_SRIOV | MLX4_FLAG_MASTER;
+		}
+	}
+
+slave_start:
 	if (mlx4_cmd_init(dev)) {
 		mlx4_err(dev, "Failed to init command interface, aborting.\n");
-		goto err_free_dev;
+		goto err_sriov;
+	}
+
+	/* In slave functions, the communication channel must be initialized before
+	 * posting commands */
+	if (mlx4_is_slave(dev)) {
+		if (mlx4_multi_func_init(dev)) {
+			mlx4_err(dev, "Failed to init slave mfunc interface, aborting.\n");
+			goto err_cmd;
+		}
 	}
 
 	err = mlx4_init_hca(dev);
-	if (err)
-		goto err_cmd;
+	if (err) {
+		if (err == -EACCES) {
+			/* Not primary Physical function
+			 * Running in slave mode */
+			mlx4_cmd_cleanup(dev);
+			dev->flags |= MLX4_FLAG_SLAVE;
+			dev->flags &= ~MLX4_FLAG_MASTER;
+			goto slave_start;
+		} else
+			goto err_cmd;
+	}
+
+	/* In master functions, the communication channel must be initialized after obtaining
+	 * its address from fw */
+	if (mlx4_is_master(dev)) {
+		dev->num_slaves = MLX4_MAX_NUM_SLAVES;
+		if (mlx4_multi_func_init(dev)) {
+			mlx4_err(dev, "Failed to init master mfunc interface, aborting.\n");
+			goto err_close;
+		}
+	}
 
 	err = mlx4_alloc_eq_table(dev);
 	if (err)
 		goto err_close;
 
+	priv->msix_ctl.pool_bm = 0;
+	mutex_init(&priv->msix_ctl.pool_lock);
+
 	mlx4_enable_msi_x(dev);
+	if (mlx4_is_slave(dev) && !(dev->flags & MLX4_FLAG_MSI_X)) {
+		mlx4_err(dev, "INTx is not supported in slave mode, aborting.\n");
+		goto err_free_eq;
+	}
+
+	if (!mlx4_is_slave(dev)) {
+		err = mlx4_init_steering(dev);
+		if (err)
+			goto err_free_eq;
+	}
 
 	err = mlx4_setup_hca(dev);
-	if (err == -EBUSY && (dev->flags & MLX4_FLAG_MSI_X)) {
+	if (err == -EBUSY && (dev->flags & MLX4_FLAG_MSI_X) && !mlx4_is_slave(dev)) {
 		dev->flags &= ~MLX4_FLAG_MSI_X;
 		pci_disable_msix(pdev);
 		err = mlx4_setup_hca(dev);
 	}
 
 	if (err)
-		goto err_free_eq;
+		goto err_steer;
 
 	for (port = 1; port <= dev->caps.num_ports; port++) {
 		err = mlx4_init_port_info(dev, port);
@@ -1166,17 +1743,15 @@ static int __mlx4_init_one(struct pci_de
 	if (err)
 		goto err_port;
 
-	mlx4_sense_init(dev);
-	mlx4_start_sense(dev);
-
 	pci_set_drvdata(pdev, dev);
 
 	return 0;
 
 err_port:
-	for (port = 1; port <= dev->caps.num_ports; port++)
+	for (--port; port >= 1; --port)
 		mlx4_cleanup_port_info(&priv->port[port]);
 
+	mlx4_cleanup_counters_table(dev);
 	mlx4_cleanup_mcg_table(dev);
 	mlx4_cleanup_qp_table(dev);
 	mlx4_cleanup_srq_table(dev);
@@ -1184,9 +1759,14 @@ err_port:
 	mlx4_cmd_use_polling(dev);
 	mlx4_cleanup_eq_table(dev);
 	mlx4_cleanup_mr_table(dev);
+	mlx4_cleanup_xrcd_table(dev);
 	mlx4_cleanup_pd_table(dev);
 	mlx4_cleanup_uar_table(dev);
 
+err_steer:
+	if (!mlx4_is_slave(dev))
+		mlx4_clear_steering(dev);
+
 err_free_eq:
 	mlx4_free_eq_table(dev);
 
@@ -1199,11 +1779,24 @@ err_close:
 err_cmd:
 	mlx4_cmd_cleanup(dev);
 
+err_sriov:
+	if (mlx4_is_mfunc(dev))
+		mlx4_multi_func_cleanup(dev);
+	if (sr_iov && (dev->flags & MLX4_FLAG_SRIOV))
+		pci_disable_sriov(pdev);
+
+err_workqueue:
+	destroy_workqueue(priv->opreq_queue);
 err_free_dev:
+	if (!mlx4_is_slave(dev))
+		mlx4_free_ownership(dev);
 	kfree(priv);
 
-err_release_regions:
-	pci_release_regions(pdev);
+err_release_bar2:
+	pci_release_region(pdev, 2);
+
+err_release_bar0:
+	pci_release_region(pdev, 0);
 
 err_disable_pdev:
 	pci_disable_device(pdev);
@@ -1231,14 +1824,22 @@ static void mlx4_remove_one(struct pci_d
 	int p;
 
 	if (dev) {
-		mlx4_stop_sense(dev);
 		mlx4_unregister_device(dev);
 
 		for (p = 1; p <= dev->caps.num_ports; p++) {
 			mlx4_cleanup_port_info(&priv->port[p]);
 			mlx4_CLOSE_PORT(dev, p);
 		}
+		if (mlx4_is_master(dev)) {
+			mlx4_free_resource_tracker(dev);
 
+		if (priv->init_port_ref[p] != 0) {
+				mlx4_warn(dev, "port %d was not properly closed\n",
+						p);
+			}
+		}
+
+		mlx4_cleanup_counters_table(dev);
 		mlx4_cleanup_mcg_table(dev);
 		mlx4_cleanup_qp_table(dev);
 		mlx4_cleanup_srq_table(dev);
@@ -1246,20 +1847,33 @@ static void mlx4_remove_one(struct pci_d
 		mlx4_cmd_use_polling(dev);
 		mlx4_cleanup_eq_table(dev);
 		mlx4_cleanup_mr_table(dev);
+		mlx4_cleanup_xrcd_table(dev);
 		mlx4_cleanup_pd_table(dev);
 
 		iounmap(priv->kar);
 		mlx4_uar_free(dev, &priv->driver_uar);
 		mlx4_cleanup_uar_table(dev);
+		if (!mlx4_is_slave(dev))
+			mlx4_clear_steering(dev);
 		mlx4_free_eq_table(dev);
 		mlx4_close_hca(dev);
+		if (mlx4_is_mfunc(dev))
+			mlx4_multi_func_cleanup(dev);
 		mlx4_cmd_cleanup(dev);
 
 		if (dev->flags & MLX4_FLAG_MSI_X)
 			pci_disable_msix(pdev);
+		if (sr_iov && (dev->flags & MLX4_FLAG_SRIOV)) {
+			mlx4_warn(dev, "Disabling sriov\n");
+			pci_disable_sriov(pdev);
+		}
 
+		destroy_workqueue(priv->opreq_queue);
+		if (!mlx4_is_slave(dev))
+			mlx4_free_ownership(dev);
 		kfree(priv);
-		pci_release_regions(pdev);
+		pci_release_region(pdev, 2);
+		pci_release_region(pdev, 0);
 		pci_disable_device(pdev);
 		pci_set_drvdata(pdev, NULL);
 	}
@@ -1272,29 +1886,71 @@ int mlx4_restart_one(struct pci_dev *pde
 }
 
 static struct pci_device_id mlx4_pci_table[] = {
-	{ PCI_VDEVICE(MELLANOX, 0x6340) }, /* MT25408 "Hermon" SDR */
-	{ PCI_VDEVICE(MELLANOX, 0x634a) }, /* MT25408 "Hermon" DDR */
-	{ PCI_VDEVICE(MELLANOX, 0x6354) }, /* MT25408 "Hermon" QDR */
-	{ PCI_VDEVICE(MELLANOX, 0x6732) }, /* MT25408 "Hermon" DDR PCIe gen2 */
-	{ PCI_VDEVICE(MELLANOX, 0x673c) }, /* MT25408 "Hermon" QDR PCIe gen2 */
-	{ PCI_VDEVICE(MELLANOX, 0x6368) }, /* MT25408 "Hermon" EN 10GigE */
-	{ PCI_VDEVICE(MELLANOX, 0x6750) }, /* MT25408 "Hermon" EN 10GigE PCIe gen2 */
-	{ PCI_VDEVICE(MELLANOX, 0x6372) }, /* MT25458 ConnectX EN 10GBASE-T 10GigE */
-	{ PCI_VDEVICE(MELLANOX, 0x675a) }, /* MT25458 ConnectX EN 10GBASE-T+Gen2 10GigE */
-	{ PCI_VDEVICE(MELLANOX, 0x6764) }, /* MT26468 ConnectX EN 10GigE PCIe gen2*/
-	{ PCI_VDEVICE(MELLANOX, 0x6746) }, /* MT26438 ConnectX EN 40GigE PCIe gen2 5GT/s */
-	{ PCI_VDEVICE(MELLANOX, 0x676e) }, /* MT26478 ConnectX2 40GigE PCIe gen2 */
-	{ PCI_VDEVICE(MELLANOX, 0x6778) }, /* MT26488 ConnectX VPI PCIe 2.0 5GT/s - IB DDR / 10GigE Virt+ */
+	{ MLX4_VDEVICE(MELLANOX, 0x6340, 0) }, /* MT25408 "Hermon" SDR */
+	{ MLX4_VDEVICE(MELLANOX, 0x6341, MLX4_VF) }, /* MT25408 "Hermon" SDR VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x634a, 0) }, /* MT25408 "Hermon" DDR */
+	{ MLX4_VDEVICE(MELLANOX, 0x634b, MLX4_VF) }, /* MT25408 "Hermon" DDR VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x6354, 0) }, /* MT25408 "Hermon" QDR */
+	{ MLX4_VDEVICE(MELLANOX, 0x6732, 0) }, /* MT25408 "Hermon" DDR PCIe gen2 */
+	{ MLX4_VDEVICE(MELLANOX, 0x6733, MLX4_VF) }, /* MT25408 "Hermon" DDR PCIe gen2 VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x673c, 0) }, /* MT25408 "Hermon" QDR PCIe gen2 */
+	{ MLX4_VDEVICE(MELLANOX, 0x673d, MLX4_VF) }, /* MT25408 "Hermon" QDR PCIe gen2 VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x6368, 0) }, /* MT25408 "Hermon" EN 10GigE */
+	{ MLX4_VDEVICE(MELLANOX, 0x6369, MLX4_VF) }, /* MT25408 "Hermon" EN 10GigE VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x6750, 0) }, /* MT25408 "Hermon" EN 10GigE PCIe gen2 */
+	{ MLX4_VDEVICE(MELLANOX, 0x6751, MLX4_VF) }, /* MT25408 "Hermon" EN 10GigE PCIe gen2 VF */
+	{ MLX4_VDEVICE(MELLANOX, 0x6372, 0) }, /* MT25458 ConnectX EN 10GBASE-T 10GigE */
+	{ MLX4_VDEVICE(MELLANOX, 0x6373, MLX4_VF) }, /* MT25458 ConnectX EN 10GBASE-T 10GigE */
+	{ MLX4_VDEVICE(MELLANOX, 0x675a, 0) }, /* MT25458 ConnectX EN 10GBASE-T+Gen2 10GigE */
+	{ MLX4_VDEVICE(MELLANOX, 0x675b, MLX4_VF) }, /* MT25458 ConnectX EN 10GBASE-T+Gen2 10GigE */
+	{ MLX4_VDEVICE(MELLANOX, 0x6764, 0) }, /* MT26468 ConnectX EN 10GigE PCIe gen2*/
+	{ MLX4_VDEVICE(MELLANOX, 0x6765, MLX4_VF) }, /* MT26468 ConnectX EN 10GigE PCIe gen2 VF*/
+	{ MLX4_VDEVICE(MELLANOX, 0x6746, 0) }, /* MT26438 ConnectX VPI PCIe 2.0 5GT/s - IB QDR / 10GigE Virt+ */
+	{ MLX4_VDEVICE(MELLANOX, 0x6747, MLX4_VF) }, /* MT26438 ConnectX VPI PCIe 2.0 5GT/s - IB QDR / 10GigE Virt+ VF*/
+	{ MLX4_VDEVICE(MELLANOX, 0x676e, 0) }, /* MT26478 ConnectX EN 40GigE PCIe 2.0 5GT/s */
+	{ MLX4_VDEVICE(MELLANOX, 0x676f, MLX4_VF) }, /* MT26478 ConnectX EN 40GigE PCIe 2.0 5GT/s VF*/
+	{ MLX4_VDEVICE(MELLANOX, 0x6778, 0) }, /* MT26488 ConnectX VPI PCIe 2.0 5GT/s - IB DDR / 10GigE Virt+ */
+	{ MLX4_VDEVICE(MELLANOX, 0x6779, MLX4_VF) }, /* MT26488 ConnectX VPI PCIe 2.0 5GT/s - IB DDR / 10GigE Virt+ VF*/
+	{ PCI_VDEVICE(MELLANOX, 0x1000) },
+	{ PCI_VDEVICE(MELLANOX, 0x1001) },
+	{ PCI_VDEVICE(MELLANOX, 0x1002) },
+	{ PCI_VDEVICE(MELLANOX, 0x1003) },
+	{ PCI_VDEVICE(MELLANOX, 0x1004) },
+	{ PCI_VDEVICE(MELLANOX, 0x1005) },
+	{ PCI_VDEVICE(MELLANOX, 0x1006) },
+	{ PCI_VDEVICE(MELLANOX, 0x1007) },
+	{ PCI_VDEVICE(MELLANOX, 0x1008) },
+	{ PCI_VDEVICE(MELLANOX, 0x1009) },
+	{ PCI_VDEVICE(MELLANOX, 0x100a) },
+	{ PCI_VDEVICE(MELLANOX, 0x100b) },
+	{ PCI_VDEVICE(MELLANOX, 0x100c) },
+	{ PCI_VDEVICE(MELLANOX, 0x100d) },
+	{ PCI_VDEVICE(MELLANOX, 0x100e) },
+	{ PCI_VDEVICE(MELLANOX, 0x100f) },
 	{ 0, }
 };
 
 MODULE_DEVICE_TABLE(pci, mlx4_pci_table);
 
+static int suspend(struct pci_dev *pdev, pm_message_t state)
+{
+	mlx4_remove_one(pdev);
+
+	return 0;
+}
+
+static int resume(struct pci_dev *pdev)
+{
+	return __mlx4_init_one(pdev, NULL);
+}
+
 static struct pci_driver mlx4_driver = {
 	.name		= DRV_NAME,
 	.id_table	= mlx4_pci_table,
 	.probe		= mlx4_init_one,
-	.remove		= __devexit_p(mlx4_remove_one)
+	.remove		= __devexit_p(mlx4_remove_one),
+	.suspend	= suspend,
+	.resume		= resume,
 };
 
 static int __init mlx4_verify_params(void)
@@ -1304,13 +1960,33 @@ static int __init mlx4_verify_params(voi
 		return -1;
 	}
 
-	if ((log_num_vlan < 0) || (log_num_vlan > 7)) {
-		printk(KERN_WARNING "mlx4_core: bad num_vlan: %d\n", log_num_vlan);
+	if ((log_mtts_per_seg < 0) || (log_mtts_per_seg > 7)) {
+		printk(KERN_WARNING "mlx4_core: bad log_mtts_per_seg: %d\n", log_mtts_per_seg);
 		return -1;
 	}
 
-	if ((log_mtts_per_seg < 1) || (log_mtts_per_seg > 5)) {
-		printk(KERN_WARNING "mlx4_core: bad log_mtts_per_seg: %d\n", log_mtts_per_seg);
+	if (mod_param_profile.num_qp && mod_param_profile.num_qp < 12) {
+		printk(KERN_WARNING "mlx4_core: too low log_num_qp: %d\n", mod_param_profile.num_qp);
+		return -1;
+	}
+
+	if (mod_param_profile.num_srq && mod_param_profile.num_srq < 10) {
+		printk(KERN_WARNING "mlx4_core: too low log_num_srq: %d\n", mod_param_profile.num_srq);
+		return -1;
+	}
+
+	if (mod_param_profile.num_cq && mod_param_profile.num_cq < 10) {
+		printk(KERN_WARNING "mlx4_core: too low log_num_cq: %d\n", mod_param_profile.num_cq);
+		return -1;
+	}
+
+	if (mod_param_profile.num_mpt && mod_param_profile.num_mpt < 10) {
+		printk(KERN_WARNING "mlx4_core: too low log_num_mpt: %d\n", mod_param_profile.num_mpt);
+		return -1;
+	}
+
+	if (mod_param_profile.num_mtt && mod_param_profile.num_mtt < 15) {
+		printk(KERN_WARNING "mlx4_core: too low log_num_mtt: %d\n", mod_param_profile.num_mtt);
 		return -1;
 	}
 
@@ -1321,6 +1997,8 @@ static int __init mlx4_init(void)
 {
 	int ret;
 
+	mutex_init(&drv_mutex);
+
 	if (mlx4_verify_params())
 		return -EINVAL;
 
@@ -1336,7 +2014,10 @@ static int __init mlx4_init(void)
 
 static void __exit mlx4_cleanup(void)
 {
+	mutex_lock(&drv_mutex);
+	mlx4_config_cleanup();
 	pci_unregister_driver(&mlx4_driver);
+	mutex_unlock(&drv_mutex);
 	destroy_workqueue(mlx4_wq);
 }
 
diff -r c23d1fc7e422 drivers/net/mlx4/mcg.c
--- a/drivers/net/mlx4/mcg.c
+++ b/drivers/net/mlx4/mcg.c
@@ -31,48 +31,54 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/string.h>
 #include <linux/slab.h>
+#include <linux/etherdevice.h>
 
 #include <linux/mlx4/cmd.h>
+#include <linux/mlx4/driver.h>
 
 #include "mlx4.h"
 
-#define MGM_QPN_MASK       0x00FFFFFF
-#define MGM_BLCK_LB_BIT    30
-
-struct mlx4_mgm {
-	__be32			next_gid_index;
-	__be32			members_count;
-	u32			reserved[2];
-	u8			gid[16];
-	__be32			qp[MLX4_QP_PER_MGM];
+struct mlx4_steer_prio {
+	struct list_head list;
+	int hash;
+	u32 num_high_prio;
 };
 
-static const u8 zero_gid[16];	/* automatically initialized to 0 */
-
-static int mlx4_READ_MCG(struct mlx4_dev *dev, int index,
-			 struct mlx4_cmd_mailbox *mailbox)
+static int mlx4_READ_ENTRY(struct mlx4_dev *dev, int index,
+			   struct mlx4_cmd_mailbox *mailbox)
 {
-	return mlx4_cmd_box(dev, 0, mailbox->dma, index, 0, MLX4_CMD_READ_MCG,
-			    MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd_box(dev, 0, mailbox->dma, index, 0,
+			    MLX4_CMD_READ_MCG, MLX4_CMD_TIME_CLASS_A);
 }
 
-static int mlx4_WRITE_MCG(struct mlx4_dev *dev, int index,
-			  struct mlx4_cmd_mailbox *mailbox)
+static int mlx4_WRITE_ENTRY(struct mlx4_dev *dev, int index,
+			    struct mlx4_cmd_mailbox *mailbox)
 {
-	return mlx4_cmd(dev, mailbox->dma, index, 0, MLX4_CMD_WRITE_MCG,
-			MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd(dev, mailbox->dma, index, 0,
+			MLX4_CMD_WRITE_MCG, MLX4_CMD_TIME_CLASS_A);
 }
 
-static int mlx4_MGID_HASH(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
-			  u16 *hash)
+static int mlx4_WRITE_PROMISC(struct mlx4_dev *dev, u8 vep_num, u8 port, u8 steer,
+			      struct mlx4_cmd_mailbox *mailbox)
+{
+	u32 in_mod;
+
+	in_mod = (u32) vep_num << 24 | (u32) port << 16 | steer << 1;
+	return mlx4_cmd(dev, mailbox->dma, in_mod, 0x1,
+			MLX4_CMD_WRITE_MCG, MLX4_CMD_TIME_CLASS_A);
+}
+
+static int mlx4_GID_HASH(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
+			 u16 *hash, u8 op_mod)
 {
 	u64 imm;
 	int err;
 
-	err = mlx4_cmd_imm(dev, mailbox->dma, &imm, 0, 0, MLX4_CMD_MGID_HASH,
-			   MLX4_CMD_TIME_CLASS_A);
+	err = mlx4_cmd_imm(dev, mailbox->dma, &imm, 0, op_mod,
+			   MLX4_CMD_MGID_HASH, MLX4_CMD_TIME_CLASS_A);
 
 	if (!err)
 		*hash = imm;
@@ -81,6 +87,603 @@ static int mlx4_MGID_HASH(struct mlx4_de
 }
 
 /*
+ * Helper functions to manage multifunction steering data structures.
+ * Used only for Ethernet steering.
+ */
+
+static struct mlx4_promisc_qp *get_promisc_qp(struct mlx4_dev *dev, u8 pf_num,
+					      enum mlx4_steer_type steer,
+					      u32 qpn)
+{
+	struct mlx4_steer *s_steer = &mlx4_priv(dev)->steer[pf_num];
+	struct mlx4_promisc_qp *pqp;
+
+	list_for_each_entry(pqp, &s_steer->promisc_qps[steer], list) {
+		if (pqp->qpn == qpn)
+			return pqp;
+	}
+	/* not found */
+	return NULL;
+}
+
+/*
+ * Add new entry to steering data structure.
+ * All promisc QPs should be added as well
+ */
+static int new_steering_entry(struct mlx4_dev *dev, u8 vep_num, u8 port,
+			      enum mlx4_steer_type steer,
+			      unsigned int index, u32 qpn)
+{
+	struct mlx4_steer *s_steer;
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm;
+	u32 members_count;
+	struct mlx4_steer_index *new_entry;
+	struct mlx4_promisc_qp *pqp;
+	struct mlx4_promisc_qp *dqp = NULL;
+	u32 prot;
+	int err;
+	u8 pf_num;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+	new_entry = kzalloc(sizeof *new_entry, GFP_KERNEL);
+	if (!new_entry)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&new_entry->duplicates);
+	new_entry->index = index;
+	list_add_tail(&new_entry->list, &s_steer->steer_entries[steer]);
+
+	/* If the given qpn is also a promisc qp,
+	 * it should be inserted to duplicates list
+	 */
+	pqp = get_promisc_qp(dev, pf_num, steer, qpn);
+	if (pqp) {
+		dqp = kmalloc(sizeof *dqp, GFP_KERNEL);
+		if (!dqp) {
+			err = -ENOMEM;
+			goto out_alloc;
+		}
+		dqp->qpn = qpn;
+		list_add_tail(&dqp->list, &new_entry->duplicates);
+	}
+
+	/* if no promisc qps for this vep, we are done */
+	if (list_empty(&s_steer->promisc_qps[steer]))
+		return 0;
+
+	/* now need to add all the promisc qps to the new
+	 * steering entry, as they should also receive the packets
+	 * destined to this address */
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox)) {
+		err = -ENOMEM;
+		goto out_alloc;
+	}
+	mgm = mailbox->buf;
+
+	err = mlx4_READ_ENTRY(dev, index, mailbox);
+	if (err)
+		goto out_mailbox;
+
+	members_count = be32_to_cpu(mgm->members_count) & 0xffffff;
+	prot = be32_to_cpu(mgm->members_count) >> 30;
+	list_for_each_entry(pqp, &s_steer->promisc_qps[steer], list) {
+		/* don't add already existing qpn */
+		if (pqp->qpn == qpn)
+			continue;
+		if (members_count == MLX4_QP_PER_MGM) {
+			/* out of space */
+			err = -ENOMEM;
+			goto out_mailbox;
+		}
+
+		/* add the qpn */
+		mgm->qp[members_count++] = cpu_to_be32(pqp->qpn & MGM_QPN_MASK);
+	}
+	/* update the qps count and update the entry with all the promisc qps*/
+	mgm->members_count = cpu_to_be32(members_count | (prot << 30));
+	err = mlx4_WRITE_ENTRY(dev, index, mailbox);
+
+out_mailbox:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	if (!err)
+		return 0;
+out_alloc:
+	if (dqp) {
+		list_del(&dqp->list);
+		kfree(dqp);
+	}
+	list_del(&new_entry->list);
+	kfree(new_entry);
+	return err;
+}
+
+/* update the data structures with existing steering entry */
+static int existing_steering_entry(struct mlx4_dev *dev, u8 vep_num, u8 port,
+				   enum mlx4_steer_type steer,
+				   unsigned int index, u32 qpn)
+{
+	struct mlx4_steer *s_steer;
+	struct mlx4_steer_index *tmp_entry, *entry = NULL;
+	struct mlx4_promisc_qp *pqp;
+	struct mlx4_promisc_qp *dqp;
+	u8 pf_num;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+
+	pqp = get_promisc_qp(dev, pf_num, steer, qpn);
+	if (!pqp)
+		return 0; /* nothing to do */
+
+	list_for_each_entry(tmp_entry, &s_steer->steer_entries[steer], list) {
+		if (tmp_entry->index == index) {
+			entry = tmp_entry;
+			break;
+		}
+	}
+	if (unlikely(!entry)) {
+		mlx4_warn(dev, "Steering entry at index %x is not registered\n", index);
+		return -EINVAL;
+	}
+
+	/* the given qpn is listed as a promisc qpn
+	 * we need to add it as a duplicate to this entry
+	 * for future refernce */
+	list_for_each_entry(dqp, &entry->duplicates, list) {
+		if (qpn == dqp->qpn)
+			return 0; /* qp is already duplicated */
+	}
+
+	/* add the qp as a duplicate on this index */
+	dqp = kmalloc(sizeof *dqp, GFP_KERNEL);
+	if (!dqp)
+		return -ENOMEM;
+	dqp->qpn = qpn;
+	list_add_tail(&dqp->list, &entry->duplicates);
+
+	return 0;
+}
+
+/* Check whether a qpn is a duplicate on steering entry
+ * If so, it should not be removed from mgm */
+static bool check_duplicate_entry(struct mlx4_dev *dev, u8 vep_num, u8 port,
+				  enum mlx4_steer_type steer,
+				  unsigned int index, u32 qpn)
+{
+	struct mlx4_steer *s_steer;
+	struct mlx4_steer_index *tmp_entry, *entry = NULL;
+	struct mlx4_promisc_qp *dqp, *tmp_dqp;
+	u8 pf_num;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+
+	/* if qp is not promisc, it cannot be duplicated */
+	if (!get_promisc_qp(dev, pf_num, steer, qpn))
+		return false;
+
+	/* The qp is promisc qp so it is a duplicate on this index
+	 * Find the index entry, and remove the duplicate */
+	list_for_each_entry(tmp_entry, &s_steer->steer_entries[steer], list) {
+		if (tmp_entry->index == index) {
+			entry = tmp_entry;
+			break;
+		}
+	}
+	if (unlikely(!entry)) {
+		mlx4_warn(dev, "Steering entry for index %x is not registered\n", index);
+		return false;
+	}
+	list_for_each_entry_safe(dqp, tmp_dqp, &entry->duplicates, list) {
+		if (dqp->qpn == qpn) {
+			list_del(&dqp->list);
+			kfree(dqp);
+		}
+	}
+	return true;
+}
+
+/* I a steering entry contains only promisc QPs, it can be removed. */
+static bool can_remove_steering_entry(struct mlx4_dev *dev, u8 vep_num, u8 port,
+				      enum mlx4_steer_type steer,
+				      unsigned int index, u32 tqpn)
+{
+	struct mlx4_steer *s_steer;
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm;
+	struct mlx4_steer_index *entry = NULL, *tmp_entry;
+	u32 qpn;
+	u32 members_count;
+	bool ret = false;
+	int i;
+	u8 pf_num;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return false;
+	mgm = mailbox->buf;
+
+	if (mlx4_READ_ENTRY(dev, index, mailbox))
+		goto out;
+	members_count = be32_to_cpu(mgm->members_count) & 0xffffff;
+	for (i = 0;  i < members_count; i++) {
+		qpn = be32_to_cpu(mgm->qp[i]) & MGM_QPN_MASK;
+		if (!get_promisc_qp(dev, pf_num, steer, qpn) && qpn != tqpn) {
+			/* the qp is not promisc, the entry can't be removed */
+			goto out;
+		}
+	}
+	 /* All the qps currently registered for this entry are promiscuous,
+	  * Checking for duplicates */
+	ret = true;
+	list_for_each_entry_safe(entry, tmp_entry, &s_steer->steer_entries[steer], list) {
+		if (entry->index == index) {
+			if (list_empty(&entry->duplicates)) {
+				list_del(&entry->list);
+				kfree(entry);
+			} else {
+				/* This entry contains duplicates so it shouldn't be removed */
+				ret = false;
+				goto out;
+			}
+		}
+	}
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return ret;
+}
+
+/* Adjust the index of an existing steering entry*/
+static void adjust_steering_entry(struct mlx4_dev *dev, u8 vep_num, u8 port,
+                                  enum mlx4_steer_type steer,
+                                  unsigned int old_index, unsigned int new_index)
+{
+	struct mlx4_steer *s_steer;
+	u8 pf_num;
+	struct mlx4_steer_index *entry;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+	list_for_each_entry(entry, &s_steer->steer_entries[steer], list) {
+		if (entry->index == old_index) {
+			entry->index = new_index;
+			return;
+		}
+	}
+}
+
+static int add_promisc_qp(struct mlx4_dev *dev, u8 vep_num, u8 port,
+			  enum mlx4_steer_type steer, u32 qpn)
+{
+	struct mlx4_steer *s_steer;
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm;
+	struct mlx4_steer_index *entry;
+	struct mlx4_promisc_qp *pqp;
+	struct mlx4_promisc_qp *dqp;
+	u32 members_count;
+	u32 prot;
+	int i;
+	bool found;
+	int last_index;
+	int err;
+	u8 pf_num;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+
+	mutex_lock(&priv->mcg_table.mutex);
+
+	if (get_promisc_qp(dev, pf_num, steer, qpn)){
+		err = 0;  /* Noting to do, already exists */
+		goto out_mutex;
+	}
+
+	pqp = kmalloc(sizeof *pqp, GFP_KERNEL);
+	if (!pqp) {
+		err = -ENOMEM;
+    	goto out_mutex;
+	}
+	pqp->qpn = qpn;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox)) {
+		err = -ENOMEM;
+		goto out_alloc;
+	}
+	mgm = mailbox->buf;
+
+	/* the promisc qp needs to be added for each one of the steering
+	 * entries, if it already exists, needs to be added as a duplicate
+	 * for this entry */
+	list_for_each_entry(entry, &s_steer->steer_entries[steer], list) {
+		err = mlx4_READ_ENTRY(dev, entry->index, mailbox);
+		if (err)
+			goto out_mailbox;
+
+		members_count = be32_to_cpu(mgm->members_count) & 0xffffff;
+		prot = be32_to_cpu(mgm->members_count) >> 30;
+		found = false;
+		for (i = 0; i < members_count; i++) {
+			if ((be32_to_cpu(mgm->qp[i]) & MGM_QPN_MASK) == qpn) {
+				/* Entry already exists, add to duplicates */
+				dqp = kmalloc(sizeof *dqp, GFP_KERNEL);
+				if (!dqp)
+					goto out_mailbox;
+				dqp->qpn = qpn;
+				list_add_tail(&dqp->list, &entry->duplicates);
+				found = true;
+			}
+		}
+		if (!found) {
+			/* Need to add the qpn to mgm */
+			if (members_count == MLX4_QP_PER_MGM) {
+				/* entry is full */
+				err = -ENOMEM;
+				goto out_mailbox;
+			}
+			mgm->qp[members_count++] = cpu_to_be32(qpn & MGM_QPN_MASK);
+			mgm->members_count = cpu_to_be32(members_count | (prot << 30));
+			err = mlx4_WRITE_ENTRY(dev, entry->index, mailbox);
+			if (err)
+				goto out_mailbox;
+		}
+		last_index = entry->index;
+	}
+
+	/* add the new qpn to list of promisc qps */
+	list_add_tail(&pqp->list, &s_steer->promisc_qps[steer]);
+	/* now need to add all the promisc qps to default entry */
+	memset(mgm, 0, sizeof *mgm);
+	members_count = 0;
+	list_for_each_entry(dqp, &s_steer->promisc_qps[steer], list)
+		mgm->qp[members_count++] = cpu_to_be32(dqp->qpn & MGM_QPN_MASK);
+	mgm->members_count = cpu_to_be32(members_count | MLX4_PROT_ETH << 30);
+
+	err = mlx4_WRITE_PROMISC(dev, vep_num, port, steer, mailbox);
+	if (err)
+		goto out_list;
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	mutex_unlock(&priv->mcg_table.mutex);
+	return 0;
+
+out_list:
+	list_del(&pqp->list);
+out_mailbox:
+	/* TODO: undo partial addition of promisc qps */
+	mlx4_free_cmd_mailbox(dev, mailbox);
+out_alloc:
+	kfree(pqp);
+out_mutex:
+    mutex_unlock(&priv->mcg_table.mutex);
+    return err;
+}
+
+static int remove_promisc_qp(struct mlx4_dev *dev, u8 vep_num, u8 port,
+			     enum mlx4_steer_type steer, u32 qpn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_steer *s_steer;
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm;
+	struct mlx4_steer_index *entry;
+	struct mlx4_promisc_qp *pqp;
+	struct mlx4_promisc_qp *dqp;
+	u32 members_count;
+	bool found;
+	bool back_to_list = false;
+	int loc, i;
+	int err;
+	u8 pf_num;
+
+	pf_num = (dev->caps.num_ports == 1) ? vep_num : (vep_num << 1) | (port - 1);
+	s_steer = &mlx4_priv(dev)->steer[pf_num];
+	mutex_lock(&priv->mcg_table.mutex);
+
+	pqp = get_promisc_qp(dev, pf_num, steer, qpn);
+	if (unlikely(!pqp)) {
+		mlx4_warn(dev, "QP %x is not promiscuous QP\n", qpn);
+		/* nothing to do */
+		err = 0;
+		goto out_mutex;
+	}
+
+	/*remove from list of promisc qps */
+	list_del(&pqp->list);
+
+	/* set the default entry not to include the removed one */
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox)) {
+		err = -ENOMEM;
+		back_to_list = true;
+		goto out_list;
+	}
+	mgm = mailbox->buf;
+	memset(mgm, 0, sizeof *mgm);
+	members_count = 0;
+	list_for_each_entry(dqp, &s_steer->promisc_qps[steer], list)
+		mgm->qp[members_count++] = cpu_to_be32(dqp->qpn & MGM_QPN_MASK);
+	mgm->members_count = cpu_to_be32(members_count | MLX4_PROT_ETH << 30);
+
+	err = mlx4_WRITE_PROMISC(dev, vep_num, port, steer, mailbox);
+	if (err)
+		goto out_mailbox;
+
+	/* remove the qp from all the steering entries*/
+	list_for_each_entry(entry, &s_steer->steer_entries[steer], list) {
+		found = false;
+		list_for_each_entry(dqp, &entry->duplicates, list) {
+			if (dqp->qpn == qpn) {
+				found = true;
+				break;
+			}
+		}
+		if (found) {
+			/* a duplicate, no need to change the mgm,
+			 * only update the duplicates list */
+			list_del(&dqp->list);
+			kfree(dqp);
+		} else {
+			err = mlx4_READ_ENTRY(dev, entry->index, mailbox);
+				if (err)
+					goto out_mailbox;
+			members_count = be32_to_cpu(mgm->members_count) & 0xffffff;
+			for (loc = -1, i = 0; i < members_count; ++i)
+				if ((be32_to_cpu(mgm->qp[i]) & MGM_QPN_MASK) == qpn)
+					loc = i;
+
+			mgm->members_count = cpu_to_be32(--members_count |
+							 (MLX4_PROT_ETH << 30));
+			mgm->qp[loc] = mgm->qp[i - 1];
+			mgm->qp[i - 1] = 0;
+
+			err = mlx4_WRITE_ENTRY(dev, entry->index, mailbox);
+				if (err)
+					goto out_mailbox;
+		}
+
+	}
+
+out_mailbox:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+out_list:
+	if (back_to_list)
+		list_add_tail(&pqp->list, &s_steer->promisc_qps[steer]);
+	else
+		kfree(pqp);
+out_mutex:
+	mutex_unlock(&priv->mcg_table.mutex);
+	return err;
+}
+
+static int set_num_high_prio(struct mlx4_dev *dev, u8 port, int hash, int val,
+			      enum mlx4_protocol prot)
+{
+	struct mlx4_steer_prio *high_prio, *prev_high_prio;
+	struct list_head *high_prios;
+
+	if (prot != MLX4_PROT_ETH || port > dev->caps.num_ports)
+		return -EINVAL;
+
+	high_prios = &mlx4_priv(dev)->steer[port - 1].high_prios;
+	list_for_each_entry_safe(high_prio, prev_high_prio, high_prios, list) {
+		if (high_prio->hash == hash) {
+			high_prio->num_high_prio += val;
+			if (!high_prio->num_high_prio) {
+				list_del(&high_prio->list);
+				kfree(high_prio);
+			}
+			return 0;
+		}
+	}
+	high_prio = kmalloc(sizeof *high_prio, GFP_KERNEL);
+	if (!high_prio)
+		return -ENOMEM;
+	high_prio->hash = hash;
+	high_prio->num_high_prio = val;
+	list_add_tail(&high_prio->list, high_prios);
+	return 0;
+}
+
+static int inc_num_high_prio(struct mlx4_dev *dev, u8 port, int hash,
+			      enum mlx4_protocol prot)
+{
+	return set_num_high_prio(dev, port, hash, 1, prot);
+}
+
+static int dec_num_high_prio(struct mlx4_dev *dev, u8 port, int hash,
+			      enum mlx4_protocol prot)
+{
+	return set_num_high_prio(dev, port, hash, -1, prot);
+}
+
+static u32 get_num_high_prio(struct mlx4_dev *dev, u8 port, int hash,
+			     enum mlx4_protocol prot)
+{
+	struct mlx4_steer_prio *high_prio;
+	struct list_head *high_prios;
+
+	if (prot != MLX4_PROT_ETH || port > dev->caps.num_ports)
+		return 0;
+	high_prios = &mlx4_priv(dev)->steer[port - 1].high_prios;
+	list_for_each_entry(high_prio, high_prios, list) {
+		if (high_prio->hash == hash)
+			return high_prio->num_high_prio;
+	}
+	return 0;
+}
+
+/* Find and return the first MGM/AMGM entry which is not of high priority */
+/* TODO merge with find_entry */
+static int find_first_low_entry(struct mlx4_dev *dev, u8 port,
+				u8 *gid, enum mlx4_protocol prot,
+				struct mlx4_cmd_mailbox *mgm_mailbox,
+				u16 *hash, int *prev, int *index)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_mgm *mgm = mgm_mailbox->buf;
+	u8 *mgid;
+	int err;
+	u8 op_mod = (prot == MLX4_PROT_ETH) ? !!(dev->caps.vep_mc_steering) : 0;
+	int high_prio_left;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return -ENOMEM;
+	mgid = mailbox->buf;
+
+	memcpy(mgid, gid, 16);
+
+	err = mlx4_GID_HASH(dev, mailbox, hash, op_mod);
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	if (err)
+		return err;
+
+	*index = *hash;
+	*prev  = -1;
+
+	high_prio_left = get_num_high_prio(dev, port, *index, prot);
+
+	do {
+		err = mlx4_READ_ENTRY(dev, *index, mgm_mailbox);
+		if (err)
+			return err;
+
+		if (!(be32_to_cpu(mgm->members_count) & 0xffffff)) {
+			if (*index != *hash) {
+				mlx4_err(dev, "Found zero MGID in AMGM.\n");
+				err = -EINVAL;
+			} else if (high_prio_left) {
+				mlx4_err(dev, "Invalid high prio entries.\n");
+				err = -EINVAL;
+			}
+			return err;
+		}
+
+		if (!high_prio_left)
+			return err;
+
+		*prev = *index;
+		*index = be32_to_cpu(mgm->next_gid_index) >> 6;
+		high_prio_left--;
+	} while (*index);
+	if (high_prio_left) {
+		mlx4_err(dev, "Not enough high prio MGIDs in MGM.\n");
+		return -EINVAL;
+	}
+	*index = -1;
+	return err;
+}
+
+/*
  * Caller must hold MCG table semaphore.  gid and mgm parameters must
  * be properly aligned for command interface.
  *
@@ -95,14 +698,18 @@ static int mlx4_MGID_HASH(struct mlx4_de
  * If no AMGM exists for given gid, *index = -1, *prev = index of last
  * entry in hash chain and *mgm holds end of hash chain.
  */
-static int find_mgm(struct mlx4_dev *dev,
-		    u8 *gid, struct mlx4_cmd_mailbox *mgm_mailbox,
-		    u16 *hash, int *prev, int *index)
+static int find_entry(struct mlx4_dev *dev, u8 port,
+		      u8 *gid, enum mlx4_protocol prot,
+		      enum mlx4_steer_type steer, u8 high_prio,
+		      struct mlx4_cmd_mailbox *mgm_mailbox,
+		      u16 *hash, int *prev, int *index)
 {
 	struct mlx4_cmd_mailbox *mailbox;
 	struct mlx4_mgm *mgm = mgm_mailbox->buf;
 	u8 *mgid;
 	int err;
+	u8 op_mod = (prot == MLX4_PROT_ETH) ? !!(dev->caps.vep_mc_steering) : 0;
+	int high_prio_left = 0;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -111,7 +718,7 @@ static int find_mgm(struct mlx4_dev *dev
 
 	memcpy(mgid, gid, 16);
 
-	err = mlx4_MGID_HASH(dev, mailbox, hash);
+	err = mlx4_GID_HASH(dev, mailbox, hash, op_mod);
 	mlx4_free_cmd_mailbox(dev, mailbox);
 	if (err)
 		return err;
@@ -122,12 +729,17 @@ static int find_mgm(struct mlx4_dev *dev
 	*index = *hash;
 	*prev  = -1;
 
+	/* We distinguish low from high priority entries by keeping the high
+	   entries before the low entries and saving their number */
+
+	high_prio_left = get_num_high_prio(dev, port, *hash, prot);
+
 	do {
-		err = mlx4_READ_MCG(dev, *index, mgm_mailbox);
+		err = mlx4_READ_ENTRY(dev, *index, mgm_mailbox);
 		if (err)
 			return err;
 
-		if (!memcmp(mgm->gid, zero_gid, 16)) {
+		if (!(be32_to_cpu(mgm->members_count) & 0xffffff)) {
 			if (*index != *hash) {
 				mlx4_err(dev, "Found zero MGID in AMGM.\n");
 				err = -EINVAL;
@@ -135,29 +747,39 @@ static int find_mgm(struct mlx4_dev *dev
 			return err;
 		}
 
-		if (!memcmp(mgm->gid, gid, 16))
+		if (!memcmp(mgm->gid, gid, 16) &&
+		    (prot == be32_to_cpu(mgm->members_count) >> 30) &&
+		    ((high_prio && high_prio_left > 0) ||
+		     (!high_prio && high_prio_left <= 0)))
 			return err;
 
 		*prev = *index;
 		*index = be32_to_cpu(mgm->next_gid_index) >> 6;
+		high_prio_left--;
 	} while (*index);
 
 	*index = -1;
 	return err;
 }
 
-int mlx4_multicast_attach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
-			  int block_mcast_loopback)
+int mlx4_qp_attach_common(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  int block_mcast_loopback, enum mlx4_protocol prot,
+			  enum mlx4_steer_type steer, u8 high_prio)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
-	struct mlx4_cmd_mailbox *mailbox;
-	struct mlx4_mgm *mgm;
+	struct mlx4_cmd_mailbox *mailbox, *low_mailbox = NULL;
+	struct mlx4_mgm *mgm, *low_mgm;
 	u32 members_count;
 	u16 hash;
 	int index, prev;
+	int low_index = -1, low_prev;
 	int link = 0;
 	int i;
 	int err;
+	u8 vep_num = gid[4];
+	u8 port = gid[5];
+	u8 new_entry = 0;
+	u8 low_to_high = 0;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -166,68 +788,120 @@ int mlx4_multicast_attach(struct mlx4_de
 
 	mutex_lock(&priv->mcg_table.mutex);
 
-	err = find_mgm(dev, gid, mailbox, &hash, &prev, &index);
+	err = find_entry(dev, port, gid, prot, steer, high_prio,
+			 mailbox, &hash, &prev, &index);
 	if (err)
 		goto out;
 
 	if (index != -1) {
-		if (!memcmp(mgm->gid, zero_gid, 16))
+		if (!(be32_to_cpu(mgm->members_count) & 0xffffff)) {
+			new_entry = 1;
 			memcpy(mgm->gid, gid, 16);
+		}
 	} else {
 		link = 1;
 
 		index = mlx4_bitmap_alloc(&priv->mcg_table.bitmap);
 		if (index == -1) {
-			mlx4_err(dev, "No AMGM entries left\n");
+			mlx4_warn(dev, "No AMGM entries left\n");
 			err = -ENOMEM;
 			goto out;
 		}
 		index += dev->caps.num_mgms;
 
+		new_entry = 1;
 		memset(mgm, 0, sizeof *mgm);
 		memcpy(mgm->gid, gid, 16);
+
+		/* if High priority attach was requested, we want to keep it
+		   before low entries, so we take the first low priority entry
+		   and swap it for our newly created entry (which is last) */
+		if (high_prio) {
+			mlx4_dbg(dev, "High priority steer was requested\n");
+			low_mailbox = mlx4_alloc_cmd_mailbox(dev);
+			if (IS_ERR(low_mailbox)) {
+				err = PTR_ERR(low_mailbox);
+				low_mailbox = NULL;
+				goto out;
+			}
+			err = find_first_low_entry(dev, port, gid, prot,
+						   low_mailbox, &hash,
+						   &low_prev, &low_index);
+			if (!err && low_index != -1) {
+				low_to_high = 1;
+				low_mgm = low_mailbox->buf;
+				mlx4_dbg(dev, "Found a low prio steering entry. Switching entries\n");
+				memcpy(mgm->gid, low_mgm->gid, 16);
+				mgm->members_count = low_mgm->members_count;
+				memcpy(mgm->qp, low_mgm->qp,
+				       (be32_to_cpu(mgm->members_count) & 0xffffff) * sizeof low_mgm->qp[0]);
+				low_mgm->members_count = 0;
+				memcpy(low_mgm->gid, gid, 16);
+				err = mlx4_WRITE_ENTRY(dev, index, mailbox);
+				if (err)
+					goto out;
+				adjust_steering_entry(dev, 0, port, steer, low_index, index);
+			}
+		}
 	}
 
-	members_count = be32_to_cpu(mgm->members_count);
+	if (!low_to_high) {
+		low_mgm = mgm;
+		low_index = index;
+		low_mailbox = mailbox;
+	}
+
+	members_count = be32_to_cpu(low_mgm->members_count) & 0xffffff;
 	if (members_count == MLX4_QP_PER_MGM) {
-		mlx4_err(dev, "MGM at index %x is full.\n", index);
+		mlx4_err(dev, "MGM at index %x is full.\n", low_index);
 		err = -ENOMEM;
 		goto out;
 	}
 
 	for (i = 0; i < members_count; ++i)
-		if ((be32_to_cpu(mgm->qp[i]) & MGM_QPN_MASK) == qp->qpn) {
+		if ((be32_to_cpu(low_mgm->qp[i]) & MGM_QPN_MASK) == qp->qpn) {
 			mlx4_dbg(dev, "QP %06x already a member of MGM\n", qp->qpn);
 			err = 0;
 			goto out;
 		}
 
-	if (block_mcast_loopback)
-		mgm->qp[members_count++] = cpu_to_be32((qp->qpn & MGM_QPN_MASK) |
-						       (1U << MGM_BLCK_LB_BIT));
-	else
-		mgm->qp[members_count++] = cpu_to_be32(qp->qpn & MGM_QPN_MASK);
+	low_mgm->qp[members_count++] = cpu_to_be32((qp->qpn & MGM_QPN_MASK) |
+						   (!!mlx4_blck_lb << MGM_BLCK_LB_BIT));
 
-	mgm->members_count       = cpu_to_be32(members_count);
+	low_mgm->members_count = cpu_to_be32(members_count | ((u32) prot << 30));
 
-	err = mlx4_WRITE_MCG(dev, index, mailbox);
+	err = mlx4_WRITE_ENTRY(dev, low_index, low_mailbox);
+	if (err)
+		goto out;
+
+	if (high_prio && new_entry)
+		err = inc_num_high_prio(dev, port, hash, prot);
 	if (err)
 		goto out;
 
 	if (!link)
 		goto out;
 
-	err = mlx4_READ_MCG(dev, prev, mailbox);
+	err = mlx4_READ_ENTRY(dev, prev, mailbox);
 	if (err)
 		goto out;
 
 	mgm->next_gid_index = cpu_to_be32(index << 6);
 
-	err = mlx4_WRITE_MCG(dev, prev, mailbox);
+	err = mlx4_WRITE_ENTRY(dev, prev, mailbox);
 	if (err)
 		goto out;
 
 out:
+	if (prot == MLX4_PROT_ETH) {
+		/* manage the steering entry for promisc mode */
+		if (new_entry)
+			new_steering_entry(dev, vep_num, port, steer, low_index, qp->qpn);
+		else
+			existing_steering_entry(dev, vep_num, port, steer,
+						low_index, qp->qpn);
+		/* TODO handle an error flow here, need to clean the MGMS */
+	}
 	if (err && link && index != -1) {
 		if (index < dev->caps.num_mgms)
 			mlx4_warn(dev, "Got AMGM index %d < %d",
@@ -239,11 +913,15 @@ out:
 	mutex_unlock(&priv->mcg_table.mutex);
 
 	mlx4_free_cmd_mailbox(dev, mailbox);
+	if (low_to_high && low_mailbox)
+		mlx4_free_cmd_mailbox(dev, low_mailbox);
 	return err;
 }
-EXPORT_SYMBOL_GPL(mlx4_multicast_attach);
+EXPORT_SYMBOL_GPL(mlx4_qp_attach_common);
 
-int mlx4_multicast_detach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16])
+int mlx4_qp_detach_common(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  enum mlx4_protocol prot, enum mlx4_steer_type steer,
+			  u8 high_prio)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_cmd_mailbox *mailbox;
@@ -253,6 +931,9 @@ int mlx4_multicast_detach(struct mlx4_de
 	int prev, index;
 	int i, loc;
 	int err;
+	u8 vep_num = gid[4];
+	u8 port = gid[5];
+	bool removed_entry = false;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox))
@@ -261,7 +942,8 @@ int mlx4_multicast_detach(struct mlx4_de
 
 	mutex_lock(&priv->mcg_table.mutex);
 
-	err = find_mgm(dev, gid, mailbox, &hash, &prev, &index);
+	err = find_entry(dev, port, gid, prot, steer, high_prio,
+			 mailbox, &hash, &prev, &index);
 	if (err)
 		goto out;
 
@@ -271,7 +953,12 @@ int mlx4_multicast_detach(struct mlx4_de
 		goto out;
 	}
 
-	members_count = be32_to_cpu(mgm->members_count);
+	/* if this pq is also a promisc qp, it shouldn't be removed */
+	if (prot == MLX4_PROT_ETH &&
+	    check_duplicate_entry(dev, vep_num, port, steer, index, qp->qpn))
+		goto out;
+
+	members_count = be32_to_cpu(mgm->members_count) & 0xffffff;
 	for (loc = -1, i = 0; i < members_count; ++i)
 		if ((be32_to_cpu(mgm->qp[i]) & MGM_QPN_MASK) == qp->qpn)
 			loc = i;
@@ -283,26 +970,34 @@ int mlx4_multicast_detach(struct mlx4_de
 	}
 
 
-	mgm->members_count = cpu_to_be32(--members_count);
+	mgm->members_count = cpu_to_be32(--members_count | ((u32) prot << 30));
 	mgm->qp[loc]       = mgm->qp[i - 1];
 	mgm->qp[i - 1]     = 0;
 
-	if (i != 1) {
-		err = mlx4_WRITE_MCG(dev, index, mailbox);
+	
+	if (prot == MLX4_PROT_ETH)
+		removed_entry = can_remove_steering_entry(dev, vep_num, port, steer, index, qp->qpn);
+	if (i != 1 && (prot != MLX4_PROT_ETH || !removed_entry)) {
+		err = mlx4_WRITE_ENTRY(dev, index, mailbox);
 		goto out;
 	}
 
+	/* We are going to delete the entry, members count should be 0 */
+	mgm->members_count = cpu_to_be32((u32) prot << 30);
+
 	if (prev == -1) {
 		/* Remove entry from MGM */
 		int amgm_index = be32_to_cpu(mgm->next_gid_index) >> 6;
 		if (amgm_index) {
-			err = mlx4_READ_MCG(dev, amgm_index, mailbox);
+			err = mlx4_READ_ENTRY(dev, amgm_index, mailbox);
+			if (!memcmp(mgm->gid, gid, 16))
+				adjust_steering_entry(dev, 0, port, steer, amgm_index, index);
 			if (err)
 				goto out;
 		} else
 			memset(mgm->gid, 0, 16);
 
-		err = mlx4_WRITE_MCG(dev, index, mailbox);
+		err = mlx4_WRITE_ENTRY(dev, index, mailbox);
 		if (err)
 			goto out;
 
@@ -316,14 +1011,14 @@ int mlx4_multicast_detach(struct mlx4_de
 		}
 	} else {
 		/* Remove entry from AMGM */
-		int cur_next_index = be32_to_cpu(mgm->next_gid_index) >> 6;
-		err = mlx4_READ_MCG(dev, prev, mailbox);
+		int cur_next_index = be32_to_cpu(mgm->next_gid_index);
+		err = mlx4_READ_ENTRY(dev, prev, mailbox);
 		if (err)
 			goto out;
 
-		mgm->next_gid_index = cpu_to_be32(cur_next_index << 6);
+		mgm->next_gid_index = cpu_to_be32(cur_next_index);
 
-		err = mlx4_WRITE_MCG(dev, prev, mailbox);
+		err = mlx4_WRITE_ENTRY(dev, prev, mailbox);
 		if (err)
 			goto out;
 
@@ -334,20 +1029,206 @@ int mlx4_multicast_detach(struct mlx4_de
 			mlx4_bitmap_free(&priv->mcg_table.bitmap,
 					 index - dev->caps.num_mgms);
 	}
-
+	if (high_prio)
+		dec_num_high_prio(dev, port, hash, prot);
 out:
 	mutex_unlock(&priv->mcg_table.mutex);
 
 	mlx4_free_cmd_mailbox(dev, mailbox);
 	return err;
 }
+EXPORT_SYMBOL_GPL(mlx4_qp_detach_common);
+
+
+int mlx4_MCAST_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						     struct mlx4_cmd_mailbox *inbox,
+						     struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_qp qp; /* dummy for calling attach/detach */
+	u8 *gid = inbox->buf;
+	enum mlx4_protocol prot = (vhcr->in_modifier >> 28) & 0x7;
+	u8 vep_num = mlx4_priv(dev)->mfunc.master.slave_state[slave].vep_num;
+	int err = 0;
+
+	if (prot == MLX4_PROT_ETH) {
+		gid[4] = vep_num;
+		gid[7] |= (MLX4_MC_STEER << 1);
+	}
+
+	qp.qpn = vhcr->in_modifier & 0xffffff;
+	if (vhcr->op_modifier)
+		err =  mlx4_qp_attach_common(dev, &qp, gid,
+					     vhcr->in_modifier >> 31, prot, MLX4_MC_STEER, 0);
+	else
+		err = mlx4_qp_detach_common(dev, &qp, gid, prot, MLX4_MC_STEER, 0);
+	if (!err) {
+		if (vhcr->op_modifier) {
+				err = mlx4_add_mcg_to_tracked_qp(dev, qp.qpn, gid, prot) ;
+				if (err) {
+					mlx4_err(dev, "Failed to mlx4_add_mcg_to_tracked_qp\n");
+					/*return it back*/
+					mlx4_qp_detach_common(dev, &qp, gid, prot, MLX4_MC_STEER, 0);
+				}
+		} else
+			err = mlx4_remove_mcg_from_tracked_qp(dev, qp.qpn, gid);
+	}
+	return err;
+}
+
+static int mlx4_MCAST(struct mlx4_dev *dev, struct mlx4_qp *qp,
+		      u8 gid[16], u8 attach, u8 block_loopback,
+		      enum mlx4_protocol prot)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	int err;
+	int qpn;
+
+	if (!mlx4_is_slave(dev))
+		return -EBADF;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	memcpy(mailbox->buf, gid, 16);
+	qpn = qp->qpn;
+	qpn |= (prot << 28);
+	if (attach && block_loopback)
+		qpn |= (1 << 31);
+
+	err = mlx4_cmd(dev, mailbox->dma, qpn, attach, MLX4_CMD_MCAST_ATTACH,
+						       MLX4_CMD_TIME_CLASS_A);
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+
+int mlx4_multicast_attach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  int block_mcast_loopback, enum mlx4_protocol prot,
+			  u8 high_prio)
+{
+	enum mlx4_steer_type steer;
+
+	steer = (is_valid_ether_addr(&gid[10])) ? MLX4_UC_STEER : MLX4_MC_STEER;
+
+	if (prot == MLX4_PROT_ETH && !dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_MCAST(dev, qp, gid, 1, block_mcast_loopback, prot);
+
+	if (prot == MLX4_PROT_ETH) {
+		gid[4] = (dev->caps.vep_num);
+		gid[7] |= (steer << 1);
+	}
+
+	return mlx4_qp_attach_common(dev, qp, gid,
+				     block_mcast_loopback, prot,
+				     steer, high_prio);
+}
+EXPORT_SYMBOL_GPL(mlx4_multicast_attach);
+
+int mlx4_multicast_detach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  enum mlx4_protocol prot, u8 high_prio)
+{
+	enum mlx4_steer_type steer;
+
+	steer = (is_valid_ether_addr(&gid[10])) ? MLX4_UC_STEER : MLX4_MC_STEER;
+
+	if (prot == MLX4_PROT_ETH && !dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_MCAST(dev, qp, gid, 0, 0, prot);
+
+	if (prot == MLX4_PROT_ETH) {
+		gid[4] = (dev->caps.vep_num);
+		gid[7] |= (steer << 1);
+	}
+
+	return mlx4_qp_detach_common(dev, qp, gid, prot, steer, high_prio);
+}
 EXPORT_SYMBOL_GPL(mlx4_multicast_detach);
 
+int mlx4_PROMISC_wrapper(struct mlx4_dev *dev, int slave,
+			 struct mlx4_vhcr *vhcr,
+			 struct mlx4_cmd_mailbox *inbox,
+			 struct mlx4_cmd_mailbox *outbox)
+{
+	u8 vep_num = mlx4_priv(dev)->mfunc.master.slave_state[slave].vep_num;
+	u32 qpn = (u32) vhcr->in_param & 0xffffffff;
+	u8 port = vhcr->in_param >> 62;
+	enum mlx4_steer_type steer = vhcr->in_modifier;
+	if (vhcr->op_modifier)
+		return add_promisc_qp(dev, vep_num, port, steer, qpn);
+	else
+		return remove_promisc_qp(dev, vep_num, port, steer, qpn);
+}
+
+static int mlx4_PROMISC(struct mlx4_dev *dev, u32 qpn,
+			enum mlx4_steer_type steer, u8 add, u8 port)
+{
+	return mlx4_cmd(dev, (u64) qpn | (u64) port << 62, (u32) steer, add,
+			MLX4_CMD_PROMISC, MLX4_CMD_TIME_CLASS_A);
+}
+
+int mlx4_multicast_promisc_add(struct mlx4_dev *dev, u32 qpn, u8 port)
+{
+	if (!dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_PROMISC(dev, qpn, MLX4_MC_STEER, 1, port);
+
+	return add_promisc_qp(dev, dev->caps.vep_num, port, MLX4_MC_STEER, qpn);
+}
+EXPORT_SYMBOL_GPL(mlx4_multicast_promisc_add);
+
+int mlx4_multicast_promisc_remove(struct mlx4_dev *dev, u32 qpn, u8 port)
+{
+	if (!dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_PROMISC(dev, qpn, MLX4_MC_STEER, 0, port);
+
+	return remove_promisc_qp(dev, dev->caps.vep_num, port, MLX4_MC_STEER, qpn);
+}
+EXPORT_SYMBOL_GPL(mlx4_multicast_promisc_remove);
+
+int mlx4_unicast_promisc_add(struct mlx4_dev *dev, u32 qpn, u8 port)
+{
+	if (!dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_PROMISC(dev, qpn, MLX4_UC_STEER, 1, port);
+
+	return add_promisc_qp(dev, dev->caps.vep_num, port, MLX4_UC_STEER, qpn);
+}
+EXPORT_SYMBOL_GPL(mlx4_unicast_promisc_add);
+
+int mlx4_unicast_promisc_remove(struct mlx4_dev *dev, u32 qpn, u8 port)
+{
+	if (!dev->caps.vep_mc_steering)
+		return 0;
+
+	if (mlx4_is_slave(dev))
+		return mlx4_PROMISC(dev, qpn, MLX4_UC_STEER, 0, port);
+
+	return remove_promisc_qp(dev, dev->caps.vep_num, port, MLX4_UC_STEER, qpn);
+}
+EXPORT_SYMBOL_GPL(mlx4_unicast_promisc_remove);
+
 int mlx4_init_mcg_table(struct mlx4_dev *dev)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	int err;
 
+	/* Nothing to do for slaves - mcg handling is para-virtualized */
+	if (mlx4_is_slave(dev))
+		return 0;
+
 	err = mlx4_bitmap_init(&priv->mcg_table.bitmap, dev->caps.num_amgms,
 			       dev->caps.num_amgms - 1, 0, 0);
 	if (err)
@@ -360,5 +1241,7 @@ int mlx4_init_mcg_table(struct mlx4_dev 
 
 void mlx4_cleanup_mcg_table(struct mlx4_dev *dev)
 {
+	if (mlx4_is_slave(dev))
+		return;
 	mlx4_bitmap_cleanup(&mlx4_priv(dev)->mcg_table.bitmap);
 }
diff -r c23d1fc7e422 drivers/net/mlx4/mlx4.h
--- a/drivers/net/mlx4/mlx4.h
+++ b/drivers/net/mlx4/mlx4.h
@@ -45,26 +45,31 @@
 #include <linux/mlx4/device.h>
 #include <linux/mlx4/driver.h>
 #include <linux/mlx4/doorbell.h>
+#include <linux/mlx4/cmd.h>
+#include <rdma/ib_verbs.h>
 
 #define DRV_NAME	"mlx4_core"
 #define PFX		DRV_NAME ": "
-#define DRV_VERSION	"0.01"
-#define DRV_RELDATE	"May 1, 2007"
+#define DRV_VERSION	"1.0-mlnx_ofed1.5.3"
+#define DRV_RELDATE	"November 3, 2011"
 
 enum {
 	MLX4_HCR_BASE		= 0x80680,
+	MLX4_HCR_SRIOV_BASE	= 0x4080680, /* good for SRIOV FW only */
 	MLX4_HCR_SIZE		= 0x0001c,
-	MLX4_CLR_INT_SIZE	= 0x00008
+	MLX4_CLR_INT_SIZE	= 0x00008,
+	MLX4_SLAVE_COMM_BASE	= 0x0,
+	MLX4_COMM_PAGESIZE	= 0x1000
 };
 
 enum {
-	MLX4_MGM_ENTRY_SIZE	=  0x100,
+	MLX4_MGM_ENTRY_SIZE	=  0x200,
 	MLX4_QP_PER_MGM		= 4 * (MLX4_MGM_ENTRY_SIZE / 16 - 2),
 	MLX4_MTT_ENTRY_PER_SEG	= 8
 };
 
 enum {
-	MLX4_NUM_PDS		= 1 << 15
+	MLX4_NUM_PDS		= 1 << 15,
 };
 
 enum {
@@ -80,6 +85,43 @@ enum {
 	MLX4_NUM_CMPTS		= MLX4_CMPT_NUM_TYPE << MLX4_CMPT_SHIFT
 };
 
+#define MLX4_COMM_TIME		10000
+enum {
+	MLX4_COMM_CMD_RESET,
+	MLX4_COMM_CMD_VHCR0,
+	MLX4_COMM_CMD_VHCR1,
+	MLX4_COMM_CMD_VHCR2,
+	MLX4_COMM_CMD_VHCR_EN,
+	MLX4_COMM_CMD_VHCR_POST
+};
+
+enum mlx4_resource {
+	RES_QP,
+	RES_CQ,
+	RES_SRQ,
+	RES_MPT,
+	RES_MTT,
+	RES_MAC,
+	RES_VLAN,
+	RES_MCAST,
+   /* update after adding new resource*/
+    MLX4_NUM_OF_RESOURCE_TYPE = 8
+};
+
+enum mlx4_alloc_mode {
+	ICM_RESERVE_AND_ALLOC,
+	ICM_RESERVE,
+	ICM_ALLOC,
+	ICM_MAC_VLAN,
+};
+
+enum {
+	MLX4_MFUNC_MAX		= 64,
+	MLX4_MFUNC_EQ_NUM	= 4,
+	MLX4_MFUNC_MAX_EQES     = 8,
+	MLX4_MFUNC_EQE_MASK     = (MLX4_MFUNC_MAX_EQES - 1)
+};
+
 #ifdef CONFIG_MLX4_DEBUG
 extern int mlx4_debug_level;
 #else /* CONFIG_MLX4_DEBUG */
@@ -99,12 +141,23 @@ extern int mlx4_debug_level;
 #define mlx4_warn(mdev, format, arg...) \
 	dev_warn(&mdev->pdev->dev, format, ## arg)
 
+extern int mlx4_blck_lb;
+
+#define MLX4_MAX_NUM_PF		16
+#define MLX4_MAX_NUM_VF		64
+#define MLX4_MAX_NUM_SLAVES	(MLX4_MAX_NUM_PF + MLX4_MAX_NUM_VF)
+
+#define MLX4_VF					(1 << 0)
+#define MLX4_VDEVICE(vendor, device, flags)	\
+	PCI_VDEVICE(vendor, device), (flags)
+
 struct mlx4_bitmap {
 	u32			last;
 	u32			top;
 	u32			max;
 	u32                     reserved_top;
 	u32			mask;
+	u32			avail;
 	spinlock_t		lock;
 	unsigned long	       *table;
 };
@@ -129,6 +182,67 @@ struct mlx4_icm_table {
 	struct mlx4_icm	      **icm;
 };
 
+
+struct mlx4_eqe {
+	u8			reserved1;
+	u8			type;
+	u8			reserved2;
+	u8			subtype;
+	union {
+		u32		raw[6];
+		struct {
+			__be32	cqn;
+		} __attribute__((packed)) comp;
+		struct {
+			u16	reserved1;
+			__be16	token;
+			u32	reserved2;
+			u8	reserved3[3];
+			u8	status;
+			__be64	out_param;
+		} __attribute__((packed)) cmd;
+		struct {
+			__be32	qpn;
+		} __attribute__((packed)) qp;
+		struct {
+			__be32	srqn;
+		} __attribute__((packed)) srq;
+		struct {
+			__be32	cqn;
+			u32	reserved1;
+			u8	reserved2[3];
+			u8	syndrome;
+		} __attribute__((packed)) cq_err;
+		struct {
+			u32	reserved1[2];
+			__be32	port;
+		} __attribute__((packed)) port_change;
+		struct {
+			#define COMM_CHANNEL_BIT_ARRAY_SIZE	4
+			u32 reserved;
+			u32 bit_vec[COMM_CHANNEL_BIT_ARRAY_SIZE];
+		} __attribute__((packed)) comm_channel_arm;
+		struct {
+			u8	reserved[2];
+			u8 	vep_num;
+			u8	port;
+		} __attribute__((packed)) vep_config;
+		struct {
+			u8	port;
+			u8	reserved[3];
+			__be64	mac;
+		} __attribute__((packed)) mac_update;
+		struct {
+			__be16	current_temperature;
+			__be16	warning_threshold;
+		} __attribute__((packed)) warming;
+	}			event;
+#define ALL_SLAVES 0xff
+	u8			slave_id;
+	u8			reserved3[2];
+	u8			owner;
+} __attribute__((packed));
+
 struct mlx4_eq {
 	struct mlx4_dev	       *dev;
 	void __iomem	       *doorbell;
@@ -137,10 +251,24 @@ struct mlx4_eq {
 	u16			irq;
 	u16			have_irq;
 	int			nent;
+	int			load;
 	struct mlx4_buf_list   *page_list;
 	struct mlx4_mtt		mtt;
 };
 
+struct mlx4_slave_eqe {
+	u8 type;
+	u8 port;
+	u32 param;
+};
+
+struct mlx4_slave_event_eq_info {
+	u32 eqn;
+	bool  use_int;
+	u16 token;
+	u64 event_type;
+};
+
 struct mlx4_profile {
 	int			num_qp;
 	int			rdmarc_per_qp;
@@ -154,12 +282,174 @@ struct mlx4_profile {
 struct mlx4_fw {
 	u64			clr_int_base;
 	u64			catas_offset;
+	u64			comm_base;
 	struct mlx4_icm	       *fw_icm;
 	struct mlx4_icm	       *aux_icm;
 	u32			catas_size;
 	u16			fw_pages;
 	u8			clr_int_bar;
 	u8			catas_bar;
+	u8			comm_bar;
+};
+
+struct mlx4_comm {
+	u32			slave_write;
+	u32			slave_read;
+};
+
+#define VLAN_FLTR_SIZE	128
+
+struct mlx4_vlan_fltr {
+	__be32 entry[VLAN_FLTR_SIZE];
+};
+
+#define GID_SIZE        16
+
+#define MGM_QPN_MASK       0x00FFFFFF
+#define MGM_BLCK_LB_BIT    30
+
+enum mlx4_resource_state {
+	RES_INIT = 0,
+	RES_RESERVED = 1,
+	RES_ALLOCATED = 2,
+	RES_ALLOCATED_AFTER_RESERVATION = 3,
+/*When registered mac the master reserved that qp, but the allocation should be in the slave*/
+	RES_ALLOCATED_WITH_MASTER_RESERVATION = 4
+};
+
+struct mlx_tracked_qp_mcg {
+		u8 gid[GID_SIZE];
+		enum mlx4_protocol prot;
+		struct list_head list;
+};
+
+struct mlx_tracked_vln_fltr {
+	int port;
+	struct mlx4_vlan_fltr vlan_fltr;
+};
+
+struct mlx4_tracked_resource {
+		int slave_id;
+		int res_type;
+		int resource_id;
+		/* state indicates the allocation stage,
+		   importance where there is reservation and after that allocation
+		*/
+		unsigned long state;
+		union {
+			struct list_head mcg_list ; /*for the QP res*/
+			u8 port ; /*for the MAC res and VLAN*/
+			int order;/*for the MTT object*/
+		} specific_data;
+		struct list_head list;
+};
+
+struct mlx4_resource_tracker {
+	spinlock_t lock;
+	/* tree for each resources */
+	struct radix_tree_root res_tree[MLX4_NUM_OF_RESOURCE_TYPE];
+	/* num_of_slave's lists, one per slave */
+	struct list_head *res_list;
+};
+
+struct mlx4_mcast_entry {
+	struct list_head list;
+	u64 addr;
+};
+
+struct mlx4_promisc_qp {
+	struct list_head list;
+	u32 qpn;
+};
+
+struct mlx4_steer_index {
+	struct list_head list;
+	unsigned int index;
+	struct list_head duplicates;
+};
+
+struct mlx4_vep_cfg {
+	u64 mac;
+	u8  link;
+	u8  bw_value;
+};
+
+struct mlx4_slave_state {
+	u8 comm_toggle;
+	u8 last_cmd;
+	u8 init_port_mask;
+	u8 pf_num;
+	u8 vep_num;
+	u8 port_num;
+	bool active;
+	u8 function;
+	dma_addr_t vhcr_dma;
+	u16 mtu[MLX4_MAX_PORTS + 1];
+	__be32 ib_cap_mask[MLX4_MAX_PORTS + 1];
+	struct mlx4_slave_eqe eq[MLX4_MFUNC_MAX_EQES];
+	struct list_head mcast_filters[MLX4_MAX_PORTS + 1];
+	struct mlx4_vlan_fltr *vlan_filter[MLX4_MAX_PORTS + 1];
+	struct mlx4_slave_event_eq_info event_eq;
+	struct mlx4_vep_cfg vep_cfg;
+	u16 eq_pi;
+	u16 eq_ci;
+	int sqp_start;
+	spinlock_t lock;
+};
+
+#define SLAVE_EVENT_EQ_SIZE	128
+struct mlx4_slave_event_eq {
+	u32 eqn;
+	u32 cons;
+	u32 prod;
+	struct mlx4_eqe event_eqe[SLAVE_EVENT_EQ_SIZE];
+};
+
+
+struct mlx4_mfunc_master_ctx {
+	struct mlx4_slave_state *slave_state;
+	int			init_port_ref[MLX4_MAX_PORTS + 1];
+	u16			max_mtu[MLX4_MAX_PORTS + 1];
+	int			disable_mcast_ref[MLX4_MAX_PORTS + 1];
+	u8			num_veps[MLX4_MAX_PORTS + 1];
+	struct mlx4_resource_tracker res_tracker;
+	struct workqueue_struct *comm_wq;
+	struct work_struct	comm_work;
+	struct work_struct	slave_event_work;
+	struct work_struct	vep_config_work;
+	u16			vep_config_bitmap;
+	spinlock_t		vep_config_lock;
+	u32			comm_arm_bit_vector[4];
+	struct mlx4_eqe		cmd_eqe;
+	struct mlx4_slave_event_eq slave_eq;
+};
+
+struct mlx4_vhcr {
+	u64	in_param;
+	u64	out_param;
+	u32	in_modifier;
+	u32	timeout;
+	u16	op;
+	u16	token;
+	u8	op_modifier;
+	int	errno;
+};
+
+struct mlx4_mfunc {
+	struct mlx4_comm __iomem       *comm;
+	struct mlx4_vhcr	       *vhcr;
+	dma_addr_t			vhcr_dma;
+
+	struct mlx4_mfunc_master_ctx	master;
+	u32				demux_sqp[MLX4_MFUNC_MAX];
+};
+
+struct mlx4_mgm {
+	__be32			next_gid_index;
+	__be32			members_count;
+	u32			reserved[2];
+	u8			gid[16];
+	__be32			qp[MLX4_QP_PER_MGM];
 };
 
 struct mlx4_cmd {
@@ -168,6 +458,7 @@ struct mlx4_cmd {
 	struct mutex		hcr_mutex;
 	struct semaphore	poll_sem;
 	struct semaphore	event_sem;
+	struct semaphore	slave_sem;
 	int			max_cmds;
 	spinlock_t		context_lock;
 	int			free_head;
@@ -175,6 +466,7 @@ struct mlx4_cmd {
 	u16			token_mask;
 	u8			use_events;
 	u8			toggle;
+	u8			comm_toggle;
 };
 
 struct mlx4_uar_table {
@@ -214,7 +506,6 @@ struct mlx4_eq_table {
 struct mlx4_srq_table {
 	struct mlx4_bitmap	bitmap;
 	spinlock_t		lock;
-	struct radix_tree_root	tree;
 	struct mlx4_icm_table	table;
 	struct mlx4_icm_table	cmpt_table;
 };
@@ -265,6 +556,54 @@ struct mlx4_vlan_table {
 	int			max;
 };
 
+
+#define SET_PORT_GEN_ALL_VALID		0x7
+#define SET_PORT_PROMISC_SHIFT		31
+#define SET_PORT_MC_PROMISC_SHIFT	30
+
+enum {
+	MCAST_DIRECT_ONLY	= 0,
+	MCAST_DIRECT		= 1,
+	MCAST_DEFAULT		= 2
+};
+
+struct mlx4_set_port_general_context {
+	u8 reserved[3];
+	u8 flags;
+	u16 reserved2;
+	__be16 mtu;
+	u8 pptx;
+	u8 pfctx;
+	u16 reserved3;
+	u8 pprx;
+	u8 pfcrx;
+	u16 reserved4;
+	u8 qinq;
+	u8 reserved5[3];
+};
+
+struct mlx4_set_port_rqp_calc_context {
+	__be32 base_qpn;
+	u8 rererved;
+	u8 n_mac;
+	u8 n_vlan;
+	u8 n_prio;
+	u8 reserved2[3];
+	u8 mac_miss;
+	u8 intra_no_vlan;
+	u8 no_vlan;
+	u8 intra_vlan_miss;
+	u8 vlan_miss;
+	u8 reserved3[3];
+	u8 no_vlan_prio;
+	__be32 promisc;
+	__be32 mcast;
+};
+
+struct mlx4_mac_entry {
+	u64 mac;
+};
+
 struct mlx4_port_info {
 	struct mlx4_dev	       *dev;
 	int			port;
@@ -272,7 +611,9 @@ struct mlx4_port_info {
 	struct device_attribute port_attr;
 	enum mlx4_port_type	tmp_type;
 	struct mlx4_mac_table	mac_table;
+	struct radix_tree_root	mac_tree;
 	struct mlx4_vlan_table	vlan_table;
+	int			base_qpn;
 };
 
 struct mlx4_sense {
@@ -280,6 +621,21 @@ struct mlx4_sense {
 	u8			do_sense_port[MLX4_MAX_PORTS + 1];
 	u8			sense_allowed[MLX4_MAX_PORTS + 1];
 	struct delayed_work	sense_poll;
+	struct workqueue_struct	*sense_wq;
+	u32			resched;
+};
+
+extern struct mutex drv_mutex;
+
+struct mlx4_steer {
+	struct list_head promisc_qps[MLX4_NUM_STEERS];
+	struct list_head steer_entries[MLX4_NUM_STEERS];
+	struct list_head high_prios;
+};
+
+struct mlx4_msix_ctl {
+	u64		pool_bm;
+	struct mutex	pool_lock;
 };
 
 struct mlx4_priv {
@@ -294,8 +650,10 @@ struct mlx4_priv {
 
 	struct mlx4_fw		fw;
 	struct mlx4_cmd		cmd;
+	struct mlx4_mfunc	mfunc;
 
 	struct mlx4_bitmap	pd_bitmap;
+	struct mlx4_bitmap	xrcd_bitmap;
 	struct mlx4_uar_table	uar_table;
 	struct mlx4_mr_table	mr_table;
 	struct mlx4_cq_table	cq_table;
@@ -303,6 +661,9 @@ struct mlx4_priv {
 	struct mlx4_srq_table	srq_table;
 	struct mlx4_qp_table	qp_table;
 	struct mlx4_mcg_table	mcg_table;
+	struct mlx4_bitmap	counters_bitmap;
+	struct list_head	bf_list;
+	struct mutex		bf_mutex;
 
 	struct mlx4_catas_err	catas_err;
 
@@ -311,8 +672,22 @@ struct mlx4_priv {
 	struct mlx4_uar		driver_uar;
 	void __iomem	       *kar;
 	struct mlx4_port_info	port[MLX4_MAX_PORTS + 1];
+	struct device_attribute trigger_attr;
+	int                     trig;
+	int                     changed_ports;
 	struct mlx4_sense       sense;
 	struct mutex		port_mutex;
+	int			iboe_counter_index[MLX4_MAX_PORTS];
+	struct io_mapping      *bf_mapping;
+ 	struct mlx4_steer	*steer;
+ 	bool			link_up[MLX4_MAX_PORTS + 1];
+	bool			vep_mode[MLX4_MAX_PORTS + 1];
+	struct mutex		port_ops_mutex;
+	int                     init_port_ref[MLX4_MAX_PORTS + 1];
+	atomic_t		opreq_count;
+	struct workqueue_struct *opreq_queue;
+	struct work_struct 	opreq_task;
+	struct mlx4_msix_ctl    msix_ctl;
 };
 
 static inline struct mlx4_priv *mlx4_priv(struct mlx4_dev *dev)
@@ -326,18 +701,28 @@ extern struct workqueue_struct *mlx4_wq;
 
 u32 mlx4_bitmap_alloc(struct mlx4_bitmap *bitmap);
 void mlx4_bitmap_free(struct mlx4_bitmap *bitmap, u32 obj);
-u32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt, int align);
+u32 mlx4_bitmap_alloc_range(struct mlx4_bitmap *bitmap, int cnt, int align,
+			    u32 skip_mask);
 void mlx4_bitmap_free_range(struct mlx4_bitmap *bitmap, u32 obj, int cnt);
+u32 mlx4_bitmap_avail(struct mlx4_bitmap *bitmap);
 int mlx4_bitmap_init(struct mlx4_bitmap *bitmap, u32 num, u32 mask,
 		     u32 reserved_bot, u32 resetrved_top);
+int mlx4_bitmap_init_no_mask(struct mlx4_bitmap *bitmap, u32 num,
+			     u32 reserved_bot, u32 reserved_top);
 void mlx4_bitmap_cleanup(struct mlx4_bitmap *bitmap);
 
 int mlx4_reset(struct mlx4_dev *dev);
+int mlx4_get_ownership(struct mlx4_dev *dev);
+void mlx4_free_ownership(struct mlx4_dev *dev);
 
 int mlx4_alloc_eq_table(struct mlx4_dev *dev);
 void mlx4_free_eq_table(struct mlx4_dev *dev);
+int mlx4_GET_EVENT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						 struct mlx4_cmd_mailbox *inbox,
+						 struct mlx4_cmd_mailbox *outbox);
 
 int mlx4_init_pd_table(struct mlx4_dev *dev);
+int mlx4_init_xrcd_table(struct mlx4_dev *dev);
 int mlx4_init_uar_table(struct mlx4_dev *dev);
 int mlx4_init_mr_table(struct mlx4_dev *dev);
 int mlx4_init_eq_table(struct mlx4_dev *dev);
@@ -354,6 +739,23 @@ void mlx4_cleanup_cq_table(struct mlx4_d
 void mlx4_cleanup_qp_table(struct mlx4_dev *dev);
 void mlx4_cleanup_srq_table(struct mlx4_dev *dev);
 void mlx4_cleanup_mcg_table(struct mlx4_dev *dev);
+void mlx4_cleanup_xrcd_table(struct mlx4_dev *dev);
+
+int mlx4_qp_alloc_icm(struct mlx4_dev *dev, int qpn);
+void mlx4_qp_free_icm(struct mlx4_dev *dev, int qpn);
+int mlx4_cq_alloc_icm(struct mlx4_dev *dev, int *cqn);
+void mlx4_cq_free_icm(struct mlx4_dev *dev, int cqn);
+int mlx4_srq_alloc_icm(struct mlx4_dev *dev, int *srqn);
+void mlx4_srq_free_icm(struct mlx4_dev *dev, int srqn);
+int mlx4_mr_reserve(struct mlx4_dev *dev);
+void mlx4_mr_release(struct mlx4_dev *dev, u32 index);
+int mlx4_mr_alloc_icm(struct mlx4_dev *dev, u32 index);
+void mlx4_mr_free_icm(struct mlx4_dev *dev, u32 index);
+u32 mlx4_alloc_mtt_range(struct mlx4_dev *dev, int order);
+void mlx4_free_mtt_range(struct mlx4_dev *dev, u32 first_seg, int order);
+int mlx4_WRITE_MTT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						 struct mlx4_cmd_mailbox *inbox,
+						 struct mlx4_cmd_mailbox *outbox);
 
 void mlx4_start_catas_poll(struct mlx4_dev *dev);
 void mlx4_stop_catas_poll(struct mlx4_dev *dev);
@@ -362,7 +764,8 @@ int mlx4_restart_one(struct pci_dev *pde
 int mlx4_register_device(struct mlx4_dev *dev);
 void mlx4_unregister_device(struct mlx4_dev *dev);
 void mlx4_dispatch_event(struct mlx4_dev *dev, enum mlx4_dev_event type, int port);
-
+u16 mlx4_set_interface_mtu_get_max(struct mlx4_interface *intf,
+		struct mlx4_dev *dev, int port, u16 new_mtu);
 struct mlx4_dev_cap;
 struct mlx4_init_hca_param;
 
@@ -370,13 +773,33 @@ u64 mlx4_make_profile(struct mlx4_dev *d
 		      struct mlx4_profile *request,
 		      struct mlx4_dev_cap *dev_cap,
 		      struct mlx4_init_hca_param *init_hca);
+void mlx4_master_comm_channel(struct work_struct *work);
+void mlx4_gen_slave_eqe(struct work_struct *work);
+void mlx4_update_vep_config(struct work_struct *work);
+
+int mlx4_MAP_EQ_wrapper(struct mlx4_dev *dev, int slave,
+			struct mlx4_vhcr *vhcr, struct mlx4_cmd_mailbox *inbox,
+			struct mlx4_cmd_mailbox *outbox);
+int mlx4_COMM_INT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+			  struct mlx4_cmd_mailbox *inbox,
+			  struct mlx4_cmd_mailbox *outbox);
+int mlx4_RTR2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,
+			    struct mlx4_vhcr *vhcr,
+			    struct mlx4_cmd_mailbox *inbox,
+			    struct mlx4_cmd_mailbox *outbox);
+int mlx4_GEN_EQE(struct mlx4_dev *dev, int slave, struct mlx4_eqe *eqe);
 
 int mlx4_cmd_init(struct mlx4_dev *dev);
 void mlx4_cmd_cleanup(struct mlx4_dev *dev);
+int mlx4_multi_func_init(struct mlx4_dev *dev);
+void mlx4_multi_func_cleanup(struct mlx4_dev *dev);
 void mlx4_cmd_event(struct mlx4_dev *dev, u16 token, u8 status, u64 out_param);
 int mlx4_cmd_use_events(struct mlx4_dev *dev);
 void mlx4_cmd_use_polling(struct mlx4_dev *dev);
 
+int mlx4_comm_cmd(struct mlx4_dev *dev, u8 cmd, u16 param, unsigned long timeout);
+
+
 void mlx4_cq_completion(struct mlx4_dev *dev, u32 cqn);
 void mlx4_cq_event(struct mlx4_dev *dev, u32 cqn, int event_type);
 
@@ -386,21 +809,141 @@ void mlx4_srq_event(struct mlx4_dev *dev
 
 void mlx4_handle_catas_err(struct mlx4_dev *dev);
 
+int mlx4_SENSE_PORT(struct mlx4_dev *dev, int port,
+			   enum mlx4_port_type *type);
 void mlx4_do_sense_ports(struct mlx4_dev *dev,
 			 enum mlx4_port_type *stype,
 			 enum mlx4_port_type *defaults);
 void mlx4_start_sense(struct mlx4_dev *dev);
 void mlx4_stop_sense(struct mlx4_dev *dev);
-void mlx4_sense_init(struct mlx4_dev *dev);
+int mlx4_sense_init(struct mlx4_dev *dev);
+void mlx4_sense_cleanup(struct mlx4_dev *dev);
 int mlx4_check_port_params(struct mlx4_dev *dev,
 			   enum mlx4_port_type *port_type);
 int mlx4_change_port_types(struct mlx4_dev *dev,
 			   enum mlx4_port_type *port_types);
+void mlx4_set_port_mask(struct mlx4_dev *dev, struct mlx4_caps *caps, int function);
 
 void mlx4_init_mac_table(struct mlx4_dev *dev, struct mlx4_mac_table *table);
 void mlx4_init_vlan_table(struct mlx4_dev *dev, struct mlx4_vlan_table *table);
 
+/* resource tracker functions*/
+int mlx4_init_resource_tracker(struct mlx4_dev *dev);
+
+void mlx4_free_resource_tracker(struct mlx4_dev *dev);
+
+int mlx4_get_slave_from_resource_id(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				    int resource_id, int *slave);
+
+/* the parameter "state" indicates the current status (like in qp/mtt)
+	need to reserve the renge before the allocation*/
+int mlx4_add_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				int slave_id, int resource_id, unsigned long state);
+/*The MPT index need to have a mask*/
+int mlx4_add_mpt_resource_for_slave(struct mlx4_dev *dev,
+				    enum mlx4_resource resource_type, int slave_id,
+				    int resource_id, unsigned long state);
+
+/* use this fuction when there is call for resrvation of qp/mtt */
+int mlx4_add_range_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				      int slave_id, int from, int cnt);
+
+void mlx4_delete_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+					int slave_id, int resource_id);
+
+void mlx4_delete_range_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+					int slave_id, int from, int cnt);
+
+void mlx4_delete_all_resources_for_slave(struct mlx4_dev *dev, int slave_id);
+
+int mlx4_add_mcg_to_tracked_qp(struct mlx4_dev *dev, int qpn, u8* gid, enum mlx4_protocol prot) ;
+int mlx4_remove_mcg_from_tracked_qp(struct mlx4_dev *dev, int qpn, u8* gid);
+
+int mlx4_add_port_to_tracked_mac(struct mlx4_dev *dev, int qpn, u8 port) ;
+
+int mlx4_add_vlan_fltr_to_tracked_slave(struct mlx4_dev *dev, int slave_id, int port);
+
+void mlx4_delete_specific_res_type_for_slave(struct mlx4_dev *dev, int slave_id,
+					     enum mlx4_resource resource_type);
+
+int mlx4_add_mtt_resource_for_slave(struct mlx4_dev *dev,
+				    int slave_id, int resource_id,
+				    unsigned long state, int order);
+/*Resource tracker - verification functions.*/
+
+int mlx4_verify_resource_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox);
+
+int mlx4_verify_mpt_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox);
+
+int mlx4_verify_cq_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox);
+
+int mlx4_verify_srq_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox);
+
+int mlx4_verify_qp_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox);
+
+int mlx4_verify_srq_aram(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox) ;
+
+/*Ruturns the mask according to specific bitmap allocator*/
+u32 calculate_bitmap_mask(struct mlx4_dev *dev, enum mlx4_resource resource_type);
+
 int mlx4_SET_PORT(struct mlx4_dev *dev, u8 port);
+int mlx4_SET_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							struct mlx4_cmd_mailbox *inbox,
+							struct mlx4_cmd_mailbox *outbox);
+int mlx4_INIT_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							 struct mlx4_cmd_mailbox *inbox,
+							 struct mlx4_cmd_mailbox *outbox);
+int mlx4_CLOSE_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox);
+int mlx4_QUERY_PORT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox);
 int mlx4_get_port_ib_caps(struct mlx4_dev *dev, u8 port, __be32 *caps);
+int mlx4_check_ext_port_caps(struct mlx4_dev *dev, u8 port);
+
+int mlx4_CONF_SPECIAL_QP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						       struct mlx4_cmd_mailbox *inbox,
+						       struct mlx4_cmd_mailbox *outbox);
+int mlx4_GET_SLAVE_SQP(struct mlx4_dev *dev, u32 *sqp, int num);
+int mlx4_GET_SLAVE_SQP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox);
+
+int mlx4_MCAST_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						     struct mlx4_cmd_mailbox *inbox,
+						     struct mlx4_cmd_mailbox *outbox);
+int mlx4_PROMISC_wrapper(struct mlx4_dev *dev, int slave,
+			 struct mlx4_vhcr *vhcr,
+			 struct mlx4_cmd_mailbox *inbox,
+			 struct mlx4_cmd_mailbox *outbox);
+int mlx4_SET_MCAST_FLTR_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+				struct mlx4_cmd_mailbox *inbox,
+				struct mlx4_cmd_mailbox *outbox);
+int mlx4_SET_VLAN_FLTR_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+				struct mlx4_cmd_mailbox *inbox,
+				struct mlx4_cmd_mailbox *outbox);
+int mlx4_common_set_vlan_fltr(struct mlx4_dev *dev, int function,
+				     int port, void *buf);
+int mlx4_common_dump_eth_stats(struct mlx4_dev *dev, int slave, u32 in_mod,
+				struct mlx4_cmd_mailbox *outbox);
+int mlx4_DUMP_ETH_STATS_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox);
+int MLX4_CMD_QUERY_IF_STAT_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox);
+int MLX4_CMD_SET_NODE_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox);
 
 #endif /* MLX4_H */
diff -r c23d1fc7e422 drivers/net/mlx4/mlx4_en.h
--- a/drivers/net/mlx4/mlx4_en.h
+++ b/drivers/net/mlx4/mlx4_en.h
@@ -45,13 +45,13 @@
 #include <linux/mlx4/cq.h>
 #include <linux/mlx4/srq.h>
 #include <linux/mlx4/doorbell.h>
+#include <linux/mlx4/cmd.h>
 
 #include "en_port.h"
 
 #define DRV_NAME	"mlx4_en"
-#define DRV_VERSION	"1.4.1.1"
-#define DRV_RELDATE	"June 2009"
-
+#define DRV_VERSION	"1.5.7.4"
+#define DRV_RELDATE	"March 2012"
 
 #define MLX4_EN_MSG_LEVEL	(NETIF_MSG_LINK | NETIF_MSG_IFDOWN)
 
@@ -66,15 +66,16 @@
 			(priv)->port, ## arg);			\
 	}
 
-#define en_dbg(mlevel, priv, format, arg...)			\
-	{							\
-	if (NETIF_MSG_##mlevel & priv->msg_enable)		\
-		en_print(KERN_DEBUG, priv, format, ## arg)	\
-	}
-#define en_warn(priv, format, arg...)				\
+#define en_dbg(mlevel, priv, format, arg...)	\
+	if (NETIF_MSG_##mlevel & priv->msg_enable) \
+		en_print(KERN_DEBUG, priv, format, ## arg)
+#define en_warn(priv, format, arg...) \
 	en_print(KERN_WARNING, priv, format, ## arg)
-#define en_err(priv, format, arg...)				\
+#define en_err(priv, format, arg...) \
 	en_print(KERN_ERR, priv, format, ## arg)
+#define en_info(priv, format, arg...) \
+	en_print(KERN_INFO, priv, format, ## arg)
+
 
 #define mlx4_err(mdev, format, arg...) \
 	printk(KERN_ERR "%s %s: " format , DRV_NAME ,\
@@ -93,8 +94,10 @@
 
 #define MLX4_EN_PAGE_SHIFT	12
 #define MLX4_EN_PAGE_SIZE	(1 << MLX4_EN_PAGE_SHIFT)
-#define MAX_TX_RINGS		16
+#define MAX_TX_RINGS		(MLX4_EN_NUM_HASH_RINGS + MLX4_EN_NUM_PPP_RINGS)
 #define MAX_RX_RINGS		16
+#define MIN_RX_RINGS		1
+#define MIN_DEF_RX_RINGS	4
 #define TXBB_SIZE		64
 #define HEADROOM		(2048 / TXBB_SIZE + 1)
 #define STAMP_STRIDE		64
@@ -113,15 +116,20 @@
 
 #define MLX4_EN_WATCHDOG_TIMEOUT	(15 * HZ)
 
+#ifdef CONFIG_X86_32
+#define MLX4_EN_ALLOC_ORDER	0
+#else
 #define MLX4_EN_ALLOC_ORDER	2
+#endif
 #define MLX4_EN_ALLOC_SIZE	(PAGE_SIZE << MLX4_EN_ALLOC_ORDER)
 
 #define MLX4_EN_MAX_LRO_DESCRIPTORS	32
+#define MLX4_EN_NUM_IPFRAG_SESSIONS	16
 
 /* Receive fragment sizes; we use at most 4 fragments (for 9600 byte MTU
  * and 4K allocations) */
 enum {
-	FRAG_SZ0 = 512 - NET_IP_ALIGN,
+	FRAG_SZ0 = 2048,
 	FRAG_SZ1 = 1024,
 	FRAG_SZ2 = 4096,
 	FRAG_SZ3 = MLX4_EN_ALLOC_SIZE
@@ -137,17 +145,19 @@ enum {
 #define MLX4_EN_MIN_TX_SIZE	(4096 / TXBB_SIZE)
 
 #define MLX4_EN_SMALL_PKT_SIZE		64
-#define MLX4_EN_NUM_TX_RINGS		8
+#define MLX4_EN_TX_HASH_SIZE		256
+#define MLX4_EN_TX_HASH_MASK		(MLX4_EN_TX_HASH_SIZE - 1)
+#define MLX4_EN_NUM_HASH_RINGS		16
 #define MLX4_EN_NUM_PPP_RINGS		8
-#define MLX4_EN_DEF_TX_RING_SIZE	512
+#define MLX4_EN_DEF_TX_RING_SIZE	1024
 #define MLX4_EN_DEF_RX_RING_SIZE  	1024
 
-/* Target number of packets to coalesce with interrupt moderation */
-#define MLX4_EN_RX_COAL_TARGET	44
-#define MLX4_EN_RX_COAL_TIME	0x10
+/* Target number of bytes to coalesce with interrupt moderation */
+#define MLX4_EN_RX_COAL_TARGET	0x0
+#define MLX4_EN_RX_COAL_TIME	0x0
 
-#define MLX4_EN_TX_COAL_PKTS	5
-#define MLX4_EN_TX_COAL_TIME	0x80
+#define MLX4_EN_TX_COAL_PKTS	1
+#define MLX4_EN_TX_COAL_TIME	64
 
 #define MLX4_EN_RX_RATE_LOW		400000
 #define MLX4_EN_RX_COAL_TIME_LOW	0
@@ -156,6 +166,7 @@ enum {
 #define MLX4_EN_RX_SIZE_THRESH		1024
 #define MLX4_EN_RX_RATE_THRESH		(1000000 / MLX4_EN_RX_COAL_TIME_HIGH)
 #define MLX4_EN_SAMPLE_INTERVAL		0
+#define MLX4_EN_AVG_PKT_SMALL		256
 
 #define MLX4_EN_AUTO_CONF	0xffff
 
@@ -171,10 +182,14 @@ enum {
 
 #define SMALL_PACKET_SIZE      (256 - NET_IP_ALIGN)
 #define HEADER_COPY_SIZE       (128 - NET_IP_ALIGN)
+#define MLX4_LOOPBACK_TEST_PAYLOAD (HEADER_COPY_SIZE - ETH_HLEN)
 
 #define MLX4_EN_MIN_MTU		46
 #define ETH_BCAST		0xffffffffffffULL
 
+#define MLX4_EN_LOOPBACK_RETRIES	5
+#define MLX4_EN_LOOPBACK_TIMEOUT	100
+
 #ifdef MLX4_EN_PERF_STAT
 /* Number of samples to 'average' */
 #define AVG_SIZE			128
@@ -242,11 +257,23 @@ struct mlx4_en_tx_desc {
 
 #define MLX4_EN_USE_SRQ		0x01000000
 
+#define MLX4_EN_RX_BUDGET 64
+#define MLX4_EN_RX_LIMIT 1024
+
+#define MLX4_EN_CX3_LOW_ID	0x1000
+#define MLX4_EN_CX3_HIGH_ID	0x1005
+
+#define MLX4_EN_POLL_TCP	1UL << 28
+#define MLX4_EN_POLL_UDP	1UL << 29
+
 struct mlx4_en_rx_alloc {
 	struct page *page;
 	u16 offset;
 };
 
+struct mlx4_bf;
+struct mlx4_stat_out_mbox;
+
 struct mlx4_en_tx_ring {
 	struct mlx4_hwq_resources wqres;
 	u32 size ; /* number of TXBBs */
@@ -270,7 +297,24 @@ struct mlx4_en_tx_ring {
 	struct mlx4_srq dummy;
 	unsigned long bytes;
 	unsigned long packets;
+	unsigned long tx_csum;
 	spinlock_t comp_lock;
+	struct mlx4_bf bf;
+	bool bf_enabled;
+	int numa_node;
+};
+
+struct mlx4_en_ipfrag {
+	struct sk_buff *fragments;
+	struct sk_buff *last;
+	__be32		saddr;
+	__be32		daddr;
+	__be16		id;
+	u8		protocol;
+	int		total_len;
+	u16		offset;
+	unsigned int	vlan;
+	__be16		sl_vid;
 };
 
 struct mlx4_en_rx_desc {
@@ -281,7 +325,6 @@ struct mlx4_en_rx_desc {
 struct mlx4_en_rx_ring {
 	struct mlx4_hwq_resources wqres;
 	struct mlx4_en_rx_alloc page_alloc[MLX4_EN_MAX_RX_FRAGS];
-	struct net_lro_mgr lro;
 	u32 size ;	/* number of Rx descs*/
 	u32 actual_size;
 	u32 size_mask;
@@ -291,32 +334,20 @@ struct mlx4_en_rx_ring {
 	u32 prod;
 	u32 cons;
 	u32 buf_size;
+	u32 len_red;
 	void *buf;
 	void *rx_info;
 	unsigned long bytes;
 	unsigned long packets;
+	unsigned long csum_ok;
+	unsigned long csum_none;
+	int numa_node;
 };
 
-
-static inline int mlx4_en_can_lro(__be16 status)
-{
-	return (status & cpu_to_be16(MLX4_CQE_STATUS_IPV4	|
-				     MLX4_CQE_STATUS_IPV4F	|
-				     MLX4_CQE_STATUS_IPV6	|
-				     MLX4_CQE_STATUS_IPV4OPT	|
-				     MLX4_CQE_STATUS_TCP	|
-				     MLX4_CQE_STATUS_UDP	|
-				     MLX4_CQE_STATUS_IPOK)) ==
-		cpu_to_be16(MLX4_CQE_STATUS_IPV4 |
-			    MLX4_CQE_STATUS_IPOK |
-			    MLX4_CQE_STATUS_TCP);
-}
-
 struct mlx4_en_cq {
 	struct mlx4_cq          mcq;
 	struct mlx4_hwq_resources wqres;
 	int                     ring;
-	spinlock_t              lock;
 	struct net_device      *dev;
 	struct napi_struct	napi;
 	/* Per-core Tx cq processing support */
@@ -329,6 +360,8 @@ struct mlx4_en_cq {
 	u16 moder_cnt;
 	struct mlx4_cqe *buf;
 #define MLX4_EN_OPCODE_ERROR	0x1e
+	u32 tot_rx;
+	int numa_node;
 };
 
 struct mlx4_en_port_profile {
@@ -338,14 +371,14 @@ struct mlx4_en_port_profile {
 	u32 tx_ring_size;
 	u32 rx_ring_size;
 	u8 rx_pause;
+	u8 tx_pause;
 	u8 rx_ppp;
-	u8 tx_pause;
-	u8 tx_ppp;
 };
 
 struct mlx4_en_profile {
 	int rss_xor;
-	int num_lro;
+	bool use_tx_polling;
+	bool udp_rss;
 	u8 rss_mask;
 	u32 active_ports;
 	u32 small_pkt_int;
@@ -358,6 +391,7 @@ struct mlx4_en_dev {
 	struct pci_dev		*pdev;
 	struct mutex		state_lock;
 	struct net_device       *pndev[MLX4_MAX_PORTS + 1];
+        struct mlx4_interface   *mlx4_intf;
 	u32                     port_cnt;
 	bool			device_up;
 	struct mlx4_en_profile  profile;
@@ -369,6 +403,7 @@ struct mlx4_en_dev {
 	struct mlx4_mr		mr;
 	u32                     priv_pdn;
 	spinlock_t              uar_lock;
+	u8			mac_removed[MLX4_MAX_PORTS + 1];
 };
 
 
@@ -380,13 +415,10 @@ struct mlx4_en_rss_map {
 	enum mlx4_qp_state indir_state;
 };
 
-struct mlx4_en_rss_context {
-	__be32 base_qpn;
-	__be32 default_qpn;
-	u16 reserved;
-	u8 hash_fn;
-	u8 flags;
-	__be32 rss_key[10];
+struct mlx4_en_port_state {
+	int link_state;
+	int link_speed;
+	int transciver;
 };
 
 struct mlx4_en_pkt_stats {
@@ -397,9 +429,6 @@ struct mlx4_en_pkt_stats {
 };
 
 struct mlx4_en_port_stats {
-	unsigned long lro_aggregated;
-	unsigned long lro_flushed;
-	unsigned long lro_no_desc;
 	unsigned long tso_packets;
 	unsigned long queue_stopped;
 	unsigned long wake_queue;
@@ -408,7 +437,7 @@ struct mlx4_en_port_stats {
 	unsigned long rx_chksum_good;
 	unsigned long rx_chksum_none;
 	unsigned long tx_chksum_offload;
-#define NUM_PORT_STATS		11
+#define NUM_PORT_STATS		8
 };
 
 struct mlx4_en_perf_stats {
@@ -421,13 +450,31 @@ struct mlx4_en_perf_stats {
 #define NUM_PERF_COUNTERS		6
 };
 
+enum mlx4_en_mclist_act {
+	MCLIST_NONE,
+	MCLIST_REM,
+	MCLIST_ADD,
+};
+
+struct mlx4_en_mc_list {
+	struct mlx4_en_mc_list	*next;
+	u8 			addr[ETH_ALEN];
+	enum mlx4_en_mclist_act	action;
+};
+
 struct mlx4_en_frag_info {
 	u16 frag_size;
 	u16 frag_prefix_size;
 	u16 frag_stride;
 	u16 frag_align;
 	u16 last_offset;
+};
 
+struct mlx4_en_tx_hash_entry {
+	u8 cnt;
+	unsigned int small_pkts;
+	unsigned int big_pkts;
+	unsigned int ring;
 };
 
 struct mlx4_en_priv {
@@ -435,15 +482,21 @@ struct mlx4_en_priv {
 	struct mlx4_en_port_profile *prof;
 	struct net_device *dev;
 	struct vlan_group *vlgrp;
+	bool vlgrp_modified;
+#define MLX4_VLREG_SIZE	512
+	u8 vlan_register[MLX4_VLREG_SIZE];		/* Each bit is a vlan */
+	u8 vlan_unregister[MLX4_VLREG_SIZE];		/* Each bit is a vlan */
+	spinlock_t vlan_lock;
 	struct net_device_stats stats;
 	struct net_device_stats ret_stats;
+	struct mlx4_en_port_state port_state;
 	spinlock_t stats_lock;
 
-	unsigned long last_moder_packets;
+	unsigned long last_moder_packets[MAX_RX_RINGS];
 	unsigned long last_moder_tx_packets;
-	unsigned long last_moder_bytes;
+	unsigned long last_moder_bytes[MAX_RX_RINGS];
 	unsigned long last_moder_jiffies;
-	int last_moder_time;
+	int last_moder_time[MAX_RX_RINGS];
 	u16 rx_usecs;
 	u16 rx_frames;
 	u16 tx_usecs;
@@ -455,35 +508,43 @@ struct mlx4_en_priv {
 	u16 sample_interval;
 	u16 adaptive_rx_coal;
 	u32 msg_enable;
+	u32 loopback_ok;
+	u32 validate_loopback;
 
 	struct mlx4_hwq_resources res;
 	int link_state;
 	int last_link_state;
 	bool port_up;
+	bool port_inited;
 	int port;
 	int registered;
+	bool resources_allocated;
 	int allocated;
-	int stride;
 	int rx_csum;
 	u64 mac;
 	int mac_index;
 	unsigned max_mtu;
 	int base_qpn;
+	int cqe_factor;
 
 	struct mlx4_en_rss_map rss_map;
+	u16 tx_prio_map[8];
 	u32 flags;
 #define MLX4_EN_FLAG_PROMISC	0x1
+#define MLX4_EN_FLAG_MC_PROMISC	0x2
 	u32 tx_ring_num;
 	u32 rx_ring_num;
+	u32 udp_rings;
 	u32 rx_skb_size;
 	struct mlx4_en_frag_info frag_info[MLX4_EN_MAX_RX_FRAGS];
 	u16 num_frags;
 	u16 log_rx_info;
 
-	struct mlx4_en_tx_ring tx_ring[MAX_TX_RINGS];
-	struct mlx4_en_rx_ring rx_ring[MAX_RX_RINGS];
-	struct mlx4_en_cq tx_cq[MAX_TX_RINGS];
-	struct mlx4_en_cq rx_cq[MAX_RX_RINGS];
+	struct mlx4_en_tx_ring	*tx_ring[MAX_TX_RINGS];
+	struct mlx4_en_rx_ring	*rx_ring[MAX_RX_RINGS];
+	struct mlx4_en_cq	*tx_cq[MAX_TX_RINGS];
+	struct mlx4_en_cq	*rx_cq[MAX_RX_RINGS];
+	struct mlx4_en_tx_hash_entry tx_hash[MLX4_EN_TX_HASH_SIZE];
 	struct work_struct mcast_task;
 	struct work_struct mac_task;
 	struct work_struct watchdog_task;
@@ -492,10 +553,23 @@ struct mlx4_en_priv {
 	struct mlx4_en_perf_stats pstats;
 	struct mlx4_en_pkt_stats pkstats;
 	struct mlx4_en_port_stats port_stats;
-	struct dev_mc_list *mc_list;
-	struct mlx4_en_stat_out_mbox hw_stats;
+	struct mlx4_en_mc_list *mc_list;
+	struct mlx4_en_mc_list curr_list;
+	struct mlx4_stat_out_mbox hw_stats;
+	int vids[128];
+	bool wol;
+	bool stored_mparams;
+	int polled_rx;
+	char poll_name[16];
+	struct device_attribute poll_sysfs_attr;
+	struct device *ddev;
 };
 
+enum mlx4_en_wol {
+	MLX4_EN_WOL_MAGIC = (1ULL << 61),
+	MLX4_EN_WOL_ENABLED = (1ULL << 62),
+};
+#define MLX4_EN_WOL_DO_MODIFY (1ULL << 63)
 
 void mlx4_en_destroy_netdev(struct net_device *dev);
 int mlx4_en_init_netdev(struct mlx4_en_dev *mdev, int port,
@@ -510,7 +584,7 @@ int mlx4_en_alloc_resources(struct mlx4_
 int mlx4_en_create_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq,
 		      int entries, int ring, enum cq_type mode);
 void mlx4_en_destroy_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
-int mlx4_en_activate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
+int mlx4_en_activate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq, int cq_idx);
 void mlx4_en_deactivate_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
 int mlx4_en_set_cq_moder(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
 int mlx4_en_arm_cq(struct mlx4_en_priv *priv, struct mlx4_en_cq *cq);
@@ -518,10 +592,10 @@ int mlx4_en_arm_cq(struct mlx4_en_priv *
 void mlx4_en_poll_tx_cq(unsigned long data);
 void mlx4_en_tx_irq(struct mlx4_cq *mcq);
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb);
-netdev_tx_t mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev);
+int mlx4_en_xmit(struct sk_buff *skb, struct net_device *dev);
 
 int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv, struct mlx4_en_tx_ring *ring,
-			   u32 size, u16 stride);
+			   int qpn, u32 size, u16 stride);
 void mlx4_en_destroy_tx_ring(struct mlx4_en_priv *priv, struct mlx4_en_tx_ring *ring);
 int mlx4_en_activate_tx_ring(struct mlx4_en_priv *priv,
 			     struct mlx4_en_tx_ring *ring,
@@ -530,22 +604,21 @@ void mlx4_en_deactivate_tx_ring(struct m
 				struct mlx4_en_tx_ring *ring);
 
 int mlx4_en_create_rx_ring(struct mlx4_en_priv *priv,
-			   struct mlx4_en_rx_ring *ring,
-			   u32 size, u16 stride);
+			   struct mlx4_en_rx_ring *ring, u32 size);
 void mlx4_en_destroy_rx_ring(struct mlx4_en_priv *priv,
 			     struct mlx4_en_rx_ring *ring);
 int mlx4_en_activate_rx_rings(struct mlx4_en_priv *priv);
 void mlx4_en_deactivate_rx_ring(struct mlx4_en_priv *priv,
 				struct mlx4_en_rx_ring *ring);
-int mlx4_en_process_rx_cq(struct net_device *dev,
-			  struct mlx4_en_cq *cq,
-			  int budget);
+int mlx4_en_process_rx_cq_skb(struct net_device *dev,
+			      struct mlx4_en_cq *cq,
+			      int budget);
 int mlx4_en_poll_rx_cq(struct napi_struct *napi, int budget);
 void mlx4_en_fill_qp_context(struct mlx4_en_priv *priv, int size, int stride,
 			     int is_tx, int rss, int qpn, int cqn,
 			     struct mlx4_qp_context *context);
 void mlx4_en_sqp_event(struct mlx4_qp *qp, enum mlx4_event event);
-int mlx4_en_map_buffer(struct mlx4_buf *buf);
+int mlx4_en_map_buffer(struct mlx4_buf *buf, int numa_node);
 void mlx4_en_unmap_buffer(struct mlx4_buf *buf);
 
 void mlx4_en_calc_rx_buf(struct net_device *dev);
@@ -554,15 +627,35 @@ void mlx4_en_release_rss_steer(struct ml
 int mlx4_en_free_tx_buf(struct net_device *dev, struct mlx4_en_tx_ring *ring);
 void mlx4_en_rx_irq(struct mlx4_cq *mcq);
 
-int mlx4_SET_MCAST_FLTR(struct mlx4_dev *dev, u8 port, u64 mac, u64 clear, u8 mode);
+struct sk_buff *mlx4_en_rx_skb(struct mlx4_en_priv *priv,
+			       struct mlx4_en_rx_desc *rx_desc,
+			       struct skb_frag_struct *skb_frags,
+			       struct mlx4_en_rx_alloc *page_alloc,
+			       unsigned int length);
+int mlx4_en_complete_rx_desc(struct mlx4_en_priv *priv,
+			     struct mlx4_en_rx_desc *rx_desc,
+			     struct skb_frag_struct *skb_frags,
+			     struct skb_frag_struct *skb_frags_rx,
+			     struct mlx4_en_rx_alloc *page_alloc,
+			     int length);
+
 int mlx4_SET_VLAN_FLTR(struct mlx4_dev *dev, u8 port, struct vlan_group *grp);
-int mlx4_SET_PORT_general(struct mlx4_dev *dev, u8 port, int mtu,
-			  u8 pptx, u8 pfctx, u8 pprx, u8 pfcrx);
+void mlx4_get_port_pfc(struct mlx4_dev *dev, u8 port, u8 *pfctx, u8 *pfcrx);
 int mlx4_SET_PORT_qpn_calc(struct mlx4_dev *dev, u8 port, u32 base_qpn,
 			   u8 promisc);
 
-int mlx4_en_DUMP_ETH_STATS(struct mlx4_en_dev *mdev, u8 port, u8 reset);
+int mlx4_en_QUERY_PORT(struct mlx4_en_dev *mdev, u8 port);
 
+void sys_tune_init(void);
+void sys_tune_fini(void);
+
+#define MLX4_EN_NUM_SELF_TEST	5
+void mlx4_en_ex_selftest(struct net_device *dev, u32 *flags, u64 *buf);
+u64 mlx4_en_mac_to_u64(u8 *addr);
+
+void mlx4_en_poll(struct net_device *dev, int ring_num);
+void mlx4_en_get_tcp_ring(struct net_device *dev, u8 *poll_ring, u32 saddr, u32 daddr, u16 sport, u16 dport);
+void mlx4_en_get_udp_rings(struct net_device *dev, u8 *poll_rings, u8 *num_rings);
 /*
  * Globals
  */
diff -r c23d1fc7e422 drivers/net/mlx4/mr.c
--- a/drivers/net/mlx4/mr.c
+++ b/drivers/net/mlx4/mr.c
@@ -32,7 +32,9 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/errno.h>
+#include <linux/kernel.h>
 
 #include <linux/mlx4/cmd.h>
 
@@ -51,7 +53,9 @@ struct mlx4_mpt_entry {
 	__be64 length;
 	__be32 lkey;
 	__be32 win_cnt;
-	u8	reserved1[3];
+	u8	reserved1;
+	u8	flags2;
+	u8	reserved2;
 	u8	mtt_rep;
 	__be64 mtt_seg;
 	__be32 mtt_sz;
@@ -70,6 +74,8 @@ struct mlx4_mpt_entry {
 #define MLX4_MPT_PD_FLAG_RAE	    (1 << 28)
 #define MLX4_MPT_PD_FLAG_EN_INV	    (3 << 24)
 
+#define MLX4_MPT_FLAG2_FBO_EN	     (1 <<  7)
+
 #define MLX4_MPT_STATUS_SW		0xF0
 #define MLX4_MPT_STATUS_HW		0x00
 
@@ -136,9 +142,9 @@ static int mlx4_buddy_init(struct mlx4_b
 	buddy->max_order = max_order;
 	spin_lock_init(&buddy->lock);
 
-	buddy->bits = kzalloc((buddy->max_order + 1) * sizeof (long *),
+	buddy->bits = kzalloc((buddy->max_order + 1) * sizeof(long *),
 			      GFP_KERNEL);
-	buddy->num_free = kzalloc((buddy->max_order + 1) * sizeof (int *),
+	buddy->num_free = kzalloc((buddy->max_order + 1) * sizeof(int),
 				  GFP_KERNEL);
 	if (!buddy->bits || !buddy->num_free)
 		goto err_out;
@@ -178,10 +184,26 @@ static void mlx4_buddy_cleanup(struct ml
 	kfree(buddy->num_free);
 }
 
-static u32 mlx4_alloc_mtt_range(struct mlx4_dev *dev, int order)
+u32 mlx4_alloc_mtt_range(struct mlx4_dev *dev, int order)
 {
 	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
+	u64 in_param;
+	u64 out_param;
 	u32 seg;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = order;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd_imm(dev, in_param, &out_param, RES_MTT,
+						       ICM_RESERVE_AND_ALLOC,
+						       MLX4_CMD_ALLOC_RES,
+						       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			return -1;
+		else
+			return out_param;
+	}
 
 	seg = mlx4_buddy_alloc(&mr_table->mtt_buddy, order);
 	if (seg == -1)
@@ -219,16 +241,33 @@ int mlx4_mtt_init(struct mlx4_dev *dev, 
 }
 EXPORT_SYMBOL_GPL(mlx4_mtt_init);
 
+void mlx4_free_mtt_range(struct mlx4_dev *dev, u32 first_seg, int order)
+{
+	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = first_seg;
+		*(((u32 *) &in_param) + 1) = order;
+		err = mlx4_cmd(dev, in_param, RES_MTT, ICM_RESERVE_AND_ALLOC,
+						       MLX4_CMD_FREE_RES,
+						       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed to free mtt range at:%d order:%d\n", first_seg, order);
+	} else {
+		mlx4_buddy_free(&mr_table->mtt_buddy, first_seg, order);
+		mlx4_table_put_range(dev, &mr_table->mtt_table, first_seg,
+					     first_seg + (1 << order) - 1);
+	}
+}
+
 void mlx4_mtt_cleanup(struct mlx4_dev *dev, struct mlx4_mtt *mtt)
 {
-	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
-
 	if (mtt->order < 0)
 		return;
 
-	mlx4_buddy_free(&mr_table->mtt_buddy, mtt->first_seg, mtt->order);
-	mlx4_table_put_range(dev, &mr_table->mtt_table, mtt->first_seg,
-			     mtt->first_seg + (1 << mtt->order) - 1);
+	mlx4_free_mtt_range(dev, mtt->first_seg, mtt->order);
 }
 EXPORT_SYMBOL_GPL(mlx4_mtt_cleanup);
 
@@ -251,8 +290,8 @@ static u32 key_to_hw_index(u32 key)
 static int mlx4_SW2HW_MPT(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			  int mpt_index)
 {
-	return mlx4_cmd(dev, mailbox->dma, mpt_index, 0, MLX4_CMD_SW2HW_MPT,
-			MLX4_CMD_TIME_CLASS_B);
+	return mlx4_cmd(dev, mailbox->dma | dev->caps.function , mpt_index,
+			0, MLX4_CMD_SW2HW_MPT, MLX4_CMD_TIME_CLASS_B);
 }
 
 static int mlx4_HW2SW_MPT(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
@@ -262,35 +301,163 @@ static int mlx4_HW2SW_MPT(struct mlx4_de
 			    !mailbox, MLX4_CMD_HW2SW_MPT, MLX4_CMD_TIME_CLASS_B);
 }
 
-int mlx4_mr_alloc(struct mlx4_dev *dev, u32 pd, u64 iova, u64 size, u32 access,
-		  int npages, int page_shift, struct mlx4_mr *mr)
+int mlx4_mr_reserve_range(struct mlx4_dev *dev, int cnt, int align, u32 *base_mridx)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
-	u32 index;
-	int err;
+	u32 mridx;
 
-	index = mlx4_bitmap_alloc(&priv->mr_table.mpt_bitmap);
-	if (index == -1)
+	mridx = mlx4_bitmap_alloc_range(&priv->mr_table.mpt_bitmap, cnt, align, 0);
+	if (mridx == -1)
 		return -ENOMEM;
 
+	*base_mridx = mridx;
+	return 0;
+
+}
+EXPORT_SYMBOL_GPL(mlx4_mr_reserve_range);
+
+void mlx4_mr_release_range(struct mlx4_dev *dev, u32 base_mridx, int cnt)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	mlx4_bitmap_free_range(&priv->mr_table.mpt_bitmap, base_mridx, cnt);
+}
+EXPORT_SYMBOL_GPL(mlx4_mr_release_range);
+
+int mlx4_mr_alloc_reserved(struct mlx4_dev *dev, u32 mridx, u32 pd,
+			   u64 iova, u64 size, u32 access, int npages,
+			   int page_shift, struct mlx4_mr *mr)
+{
 	mr->iova       = iova;
 	mr->size       = size;
 	mr->pd	       = pd;
 	mr->access     = access;
 	mr->enabled    = 0;
-	mr->key	       = hw_index_to_key(index);
+	mr->key	       = hw_index_to_key(mridx);
 
-	err = mlx4_mtt_init(dev, npages, page_shift, &mr->mtt);
+	return mlx4_mtt_init(dev, npages, page_shift, &mr->mtt);
+}
+EXPORT_SYMBOL_GPL(mlx4_mr_alloc_reserved);
+
+int mlx4_WRITE_MTT_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						 struct mlx4_cmd_mailbox *inbox,
+						 struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_mtt mtt;
+	u64 *page_list = inbox->buf;
+	int i;
+
+	/* Call the SW implementation of write_mtt:
+	 * - Prepare a dummy mtt struct
+	 * - Translate inbox contents to simple addresses in host endianess */
+	mtt.first_seg = 0;
+	mtt.order = 0;
+	mtt.page_shift = 0;
+	for (i = 0; i < vhcr->in_modifier; ++i)
+		page_list[i + 2] = be64_to_cpu(page_list[i + 2]) & ~1ULL;
+	vhcr->errno = mlx4_write_mtt(dev, &mtt, be64_to_cpu(page_list[0]),
+						vhcr->in_modifier,
+						page_list + 2);
+	return 0;
+}
+
+static int mlx4_WRITE_MTT(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
+			  int num_entries)
+{
+	return mlx4_cmd(dev, mailbox->dma, num_entries, 0, MLX4_CMD_WRITE_MTT,
+			MLX4_CMD_TIME_CLASS_A);
+}
+
+int mlx4_mr_reserve(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u64 out_param;
+	int err;
+	u32 ret ;
+
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd_imm(dev, 0, &out_param, RES_MPT, ICM_RESERVE,
+								MLX4_CMD_ALLOC_RES,
+								MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			return -1;
+		return out_param;
+	}
+	ret = mlx4_bitmap_alloc(&priv->mr_table.mpt_bitmap);
+	return ret ;
+}
+
+void mlx4_mr_release(struct mlx4_dev *dev, u32 index)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = index;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd(dev, in_param, RES_MPT, ICM_RESERVE,
+						       MLX4_CMD_FREE_RES,
+						       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed to release mr index:%d\n", index);
+	} else
+		mlx4_bitmap_free(&priv->mr_table.mpt_bitmap, index);
+}
+
+int mlx4_mr_alloc_icm(struct mlx4_dev *dev, u32 index)
+{
+	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
+	u64 param;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &param) = index;
+		*(((u32 *) &param) + 1) = 0;
+		return mlx4_cmd_imm(dev, param, &param, RES_MPT, ICM_ALLOC,
+							MLX4_CMD_ALLOC_RES,
+							MLX4_CMD_TIME_CLASS_A);
+	} else
+		return mlx4_table_get(dev, &mr_table->dmpt_table, index);
+}
+
+void mlx4_mr_free_icm(struct mlx4_dev *dev, u32 index)
+{
+	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = index;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd(dev, in_param, RES_MPT, ICM_ALLOC,
+						       MLX4_CMD_FREE_RES,
+						       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed to free icm of mr index:%d\n", index);
+	} else
+		mlx4_table_put(dev, &mr_table->dmpt_table, index);
+}
+
+int mlx4_mr_alloc(struct mlx4_dev *dev, u32 pd, u64 iova, u64 size, u32 access,
+		  int npages, int page_shift, struct mlx4_mr *mr)
+{
+	u32 index;
+	int err;
+
+	index = mlx4_mr_reserve(dev);
+	if (index == -1)
+		return -ENOMEM;
+
+	err = mlx4_mr_alloc_reserved(dev, index, pd, iova, size,
+				     access, npages, page_shift, mr);
 	if (err)
-		mlx4_bitmap_free(&priv->mr_table.mpt_bitmap, index);
+		mlx4_mr_release(dev, index);
 
 	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_mr_alloc);
 
-void mlx4_mr_free(struct mlx4_dev *dev, struct mlx4_mr *mr)
+void mlx4_mr_free_reserved(struct mlx4_dev *dev, struct mlx4_mr *mr)
 {
-	struct mlx4_priv *priv = mlx4_priv(dev);
 	int err;
 
 	if (mr->enabled) {
@@ -300,20 +467,25 @@ void mlx4_mr_free(struct mlx4_dev *dev, 
 		if (err)
 			mlx4_warn(dev, "HW2SW_MPT failed (%d)\n", err);
 	}
+	mlx4_mtt_cleanup(dev, &mr->mtt);
+}
+EXPORT_SYMBOL_GPL(mlx4_mr_free_reserved);
 
-	mlx4_mtt_cleanup(dev, &mr->mtt);
-	mlx4_bitmap_free(&priv->mr_table.mpt_bitmap, key_to_hw_index(mr->key));
+void mlx4_mr_free(struct mlx4_dev *dev, struct mlx4_mr *mr)
+{
+	mlx4_mr_free_reserved(dev, mr);
+	mlx4_mr_release(dev, key_to_hw_index(mr->key));
+	mlx4_mr_free_icm(dev, key_to_hw_index(mr->key));
 }
 EXPORT_SYMBOL_GPL(mlx4_mr_free);
 
 int mlx4_mr_enable(struct mlx4_dev *dev, struct mlx4_mr *mr)
 {
-	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
 	struct mlx4_cmd_mailbox *mailbox;
 	struct mlx4_mpt_entry *mpt_entry;
 	int err;
 
-	err = mlx4_table_get(dev, &mr_table->dmpt_table, key_to_hw_index(mr->key));
+	err = mlx4_mr_alloc_icm(dev, key_to_hw_index(mr->key));
 	if (err)
 		return err;
 
@@ -371,7 +543,7 @@ err_cmd:
 	mlx4_free_cmd_mailbox(dev, mailbox);
 
 err_table:
-	mlx4_table_put(dev, &mr_table->dmpt_table, key_to_hw_index(mr->key));
+	mlx4_mr_free_icm(dev, key_to_hw_index(mr->key));
 	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_mr_enable);
@@ -413,24 +585,50 @@ static int mlx4_write_mtt_chunk(struct m
 int mlx4_write_mtt(struct mlx4_dev *dev, struct mlx4_mtt *mtt,
 		   int start_index, int npages, u64 *page_list)
 {
+	struct mlx4_cmd_mailbox *mailbox = NULL;
 	int chunk;
-	int err;
+	int err = 0;
+	__be64 *inbox = NULL;
+	int i;
 
 	if (mtt->order < 0)
 		return -EINVAL;
 
+	if (mlx4_is_slave(dev)) {
+		mailbox = mlx4_alloc_cmd_mailbox(dev);
+		if (IS_ERR(mailbox))
+			return PTR_ERR(mailbox);
+		inbox = mailbox->buf;
+	}
+
 	while (npages > 0) {
-		chunk = min_t(int, PAGE_SIZE / sizeof(u64), npages);
-		err = mlx4_write_mtt_chunk(dev, mtt, start_index, chunk, page_list);
+		if (mlx4_is_slave(dev)) {
+			int s = mtt->first_seg * dev->caps.mtts_per_seg + start_index;
+			chunk = min_t(int, MLX4_MAILBOX_SIZE / sizeof(u64) - dev->caps.mtts_per_seg, npages);
+			if (s / (PAGE_SIZE / sizeof (u64)) !=
+			    (s + chunk - 1) / (PAGE_SIZE / sizeof (u64)))
+				chunk = PAGE_SIZE / sizeof (u64) - (s % (PAGE_SIZE / sizeof (u64)));
+
+			inbox[0] = cpu_to_be64(mtt->first_seg * dev->caps.mtts_per_seg + start_index);
+			inbox[1] = 0;
+			for (i = 0; i < chunk; ++i)
+				inbox[i + 2] = cpu_to_be64(page_list[i] | MLX4_MTT_FLAG_PRESENT);
+			err = mlx4_WRITE_MTT(dev, mailbox, chunk);
+		} else {
+			chunk = min_t(int, PAGE_SIZE / sizeof(u64), npages);
+			err = mlx4_write_mtt_chunk(dev, mtt, start_index, chunk, page_list);
+		}
 		if (err)
-			return err;
+			goto out;
 
 		npages      -= chunk;
 		start_index += chunk;
 		page_list   += chunk;
 	}
-
-	return 0;
+out:
+	if (mlx4_is_slave(dev))
+		mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_write_mtt);
 
@@ -442,6 +640,7 @@ int mlx4_buf_write_mtt(struct mlx4_dev *
 	int i;
 
 	page_list = kmalloc(buf->npages * sizeof *page_list, GFP_KERNEL);
+
 	if (!page_list)
 		return -ENOMEM;
 
@@ -463,6 +662,13 @@ int mlx4_init_mr_table(struct mlx4_dev *
 	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
 	int err;
 
+	if (!is_power_of_2(dev->caps.num_mpts))
+		return -EINVAL;
+
+	/* Nothing to do for slaves - all MR handling is forwarded to the master */
+	if (mlx4_is_slave(dev))
+		return 0;
+
 	err = mlx4_bitmap_init(&mr_table->mpt_bitmap, dev->caps.num_mpts,
 			       ~0, dev->caps.reserved_mrws, 0);
 	if (err)
@@ -497,6 +703,8 @@ void mlx4_cleanup_mr_table(struct mlx4_d
 {
 	struct mlx4_mr_table *mr_table = &mlx4_priv(dev)->mr_table;
 
+	if (mlx4_is_slave(dev))
+		return;
 	mlx4_buddy_cleanup(&mr_table->mtt_buddy);
 	mlx4_bitmap_cleanup(&mr_table->mpt_bitmap);
 }
@@ -528,8 +736,9 @@ static inline int mlx4_check_fmr(struct 
 	return 0;
 }
 
-int mlx4_map_phys_fmr(struct mlx4_dev *dev, struct mlx4_fmr *fmr, u64 *page_list,
-		      int npages, u64 iova, u32 *lkey, u32 *rkey)
+int mlx4_map_phys_fmr_fbo(struct mlx4_dev *dev, struct mlx4_fmr *fmr,
+			  u64 *page_list, int npages, u64 iova, u32 fbo,
+			  u32 len, u32 *lkey, u32 *rkey, int same_key)
 {
 	u32 key;
 	int i, err;
@@ -541,7 +750,8 @@ int mlx4_map_phys_fmr(struct mlx4_dev *d
 	++fmr->maps;
 
 	key = key_to_hw_index(fmr->mr.key);
-	key += dev->caps.num_mpts;
+	if (!same_key)
+		key += dev->caps.num_mpts;
 	*lkey = *rkey = fmr->mr.key = hw_index_to_key(key);
 
 	*(u8 *) fmr->mpt = MLX4_MPT_STATUS_SW;
@@ -560,8 +770,10 @@ int mlx4_map_phys_fmr(struct mlx4_dev *d
 
 	fmr->mpt->key    = cpu_to_be32(key);
 	fmr->mpt->lkey   = cpu_to_be32(key);
-	fmr->mpt->length = cpu_to_be64(npages * (1ull << fmr->page_shift));
+	fmr->mpt->length = cpu_to_be64(len);
 	fmr->mpt->start  = cpu_to_be64(iova);
+	fmr->mpt->first_byte_offset = cpu_to_be32(fbo & 0x001fffff);
+	fmr->mpt->flags2 = (fbo ? MLX4_MPT_FLAG2_FBO_EN : 0);
 
 	/* Make MTT entries are visible before setting MPT status */
 	wmb();
@@ -573,6 +785,16 @@ int mlx4_map_phys_fmr(struct mlx4_dev *d
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(mlx4_map_phys_fmr_fbo);
+
+int mlx4_map_phys_fmr(struct mlx4_dev *dev, struct mlx4_fmr *fmr, u64 *page_list,
+		      int npages, u64 iova, u32 *lkey, u32 *rkey)
+{
+	u32 len = npages * (1ull << fmr->page_shift);
+
+	return mlx4_map_phys_fmr_fbo(dev, fmr, page_list, npages, iova, 0,
+				     len, lkey, rkey, 0);
+}
 EXPORT_SYMBOL_GPL(mlx4_map_phys_fmr);
 
 int mlx4_fmr_alloc(struct mlx4_dev *dev, u32 pd, u32 access, int max_pages,
@@ -617,6 +839,49 @@ err_free:
 }
 EXPORT_SYMBOL_GPL(mlx4_fmr_alloc);
 
+int mlx4_fmr_alloc_reserved(struct mlx4_dev *dev, u32 mridx,
+			    u32 pd, u32 access, int max_pages,
+			    int max_maps, u8 page_shift, struct mlx4_fmr *fmr)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u64 mtt_seg;
+	int err = -ENOMEM;
+
+	if (page_shift < (ffs(dev->caps.page_size_cap) - 1) || page_shift >= 32)
+		return -EINVAL;
+
+	/* All MTTs must fit in the same page */
+	if (max_pages * sizeof *fmr->mtts > PAGE_SIZE)
+		return -EINVAL;
+
+	fmr->page_shift = page_shift;
+	fmr->max_pages  = max_pages;
+	fmr->max_maps   = max_maps;
+	fmr->maps = 0;
+
+	err = mlx4_mr_alloc_reserved(dev, mridx, pd, 0, 0, access, max_pages,
+				     page_shift, &fmr->mr);
+	if (err)
+		return err;
+
+	mtt_seg = fmr->mr.mtt.first_seg * dev->caps.mtt_entry_sz;
+
+	fmr->mtts = mlx4_table_find(&priv->mr_table.mtt_table,
+				    fmr->mr.mtt.first_seg,
+				    &fmr->dma_handle);
+	if (!fmr->mtts) {
+		err = -ENOMEM;
+		goto err_free;
+	}
+
+	return 0;
+
+err_free:
+	mlx4_mr_free_reserved(dev, &fmr->mr);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_fmr_alloc_reserved);
+
 int mlx4_fmr_enable(struct mlx4_dev *dev, struct mlx4_fmr *fmr)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -659,6 +924,18 @@ int mlx4_fmr_free(struct mlx4_dev *dev, 
 }
 EXPORT_SYMBOL_GPL(mlx4_fmr_free);
 
+int mlx4_fmr_free_reserved(struct mlx4_dev *dev, struct mlx4_fmr *fmr)
+{
+	if (fmr->maps)
+		return -EBUSY;
+
+	fmr->mr.enabled = 0;
+	mlx4_mr_free_reserved(dev, &fmr->mr);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mlx4_fmr_free_reserved);
+
 int mlx4_SYNC_TPT(struct mlx4_dev *dev)
 {
 	return mlx4_cmd(dev, 0, 0, 0, MLX4_CMD_SYNC_TPT, 1000);
diff -r c23d1fc7e422 drivers/net/mlx4/pd.c
--- a/drivers/net/mlx4/pd.c
+++ b/drivers/net/mlx4/pd.c
@@ -31,13 +31,21 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/errno.h>
+#include <linux/io-mapping.h>
 
 #include <asm/page.h>
 
 #include "mlx4.h"
 #include "icm.h"
 
+enum {
+	MLX4_NUM_RESERVED_UARS = 8
+};
+
+#define NOT_MASKED_PD_BITS 17
+
 int mlx4_pd_alloc(struct mlx4_dev *dev, u32 *pdn)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -45,7 +53,8 @@ int mlx4_pd_alloc(struct mlx4_dev *dev, 
 	*pdn = mlx4_bitmap_alloc(&priv->pd_bitmap);
 	if (*pdn == -1)
 		return -ENOMEM;
-
+	if (mlx4_is_mfunc(dev))
+		*pdn |= (dev->caps.function + 1) << NOT_MASKED_PD_BITS;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(mlx4_pd_alloc);
@@ -72,12 +81,18 @@ void mlx4_cleanup_pd_table(struct mlx4_d
 
 int mlx4_uar_alloc(struct mlx4_dev *dev, struct mlx4_uar *uar)
 {
+	int offset;
+
 	uar->index = mlx4_bitmap_alloc(&mlx4_priv(dev)->uar_table.bitmap);
 	if (uar->index == -1)
 		return -ENOMEM;
 
-	uar->pfn = (pci_resource_start(dev->pdev, 2) >> PAGE_SHIFT) + uar->index;
-
+	if (mlx4_is_slave(dev))
+		offset = uar->index % ((int) pci_resource_len(dev->pdev, 2) /
+				       dev->caps.uar_page_size);
+	else
+		offset = uar->index;
+	uar->pfn = (pci_resource_start(dev->pdev, 2) >> PAGE_SHIFT) + offset;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(mlx4_uar_alloc);
@@ -88,6 +103,123 @@ void mlx4_uar_free(struct mlx4_dev *dev,
 }
 EXPORT_SYMBOL_GPL(mlx4_uar_free);
 
+#ifndef CONFIG_PPC
+int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int numa_node)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_uar *uar;
+	int err = 0;
+	int idx;
+
+	if (!priv->bf_mapping)
+		return -ENOMEM;
+
+	mutex_lock(&priv->bf_mutex);
+	if (!list_empty(&priv->bf_list))
+		uar = list_entry(priv->bf_list.next, struct mlx4_uar, bf_list);
+	else {
+		if (mlx4_bitmap_avail(&priv->uar_table.bitmap) < MLX4_NUM_RESERVED_UARS) {
+			err = -ENOMEM;
+			goto out;
+		}
+
+		uar = kmalloc_node(sizeof *uar, GFP_KERNEL, numa_node);
+
+		if (!uar)
+			uar = kmalloc(sizeof *uar, GFP_KERNEL);
+
+		if (!uar) {
+			err = -ENOMEM;
+			goto out;
+		}
+		err = mlx4_uar_alloc(dev, uar);
+		if (err)
+			goto free_kmalloc;
+
+		uar->map = ioremap(uar->pfn << PAGE_SHIFT, PAGE_SIZE);
+		if (!uar->map) {
+			err = -ENOMEM;
+			goto free_uar;
+		}
+
+		uar->bf_map = io_mapping_map_wc(priv->bf_mapping, uar->index << PAGE_SHIFT);
+		if (!uar->bf_map) {
+			err = -ENOMEM;
+			goto unamp_uar;
+		}
+		uar->free_bf_bmap = 0;
+		list_add(&uar->bf_list, &priv->bf_list);
+	}
+
+	bf->uar = uar;
+	idx = ffz(uar->free_bf_bmap);
+	uar->free_bf_bmap |= 1 << idx;
+	bf->uar = uar;
+	bf->offset = 0;
+	bf->buf_size = dev->caps.bf_reg_size / 2;
+	bf->reg = uar->bf_map + idx * dev->caps.bf_reg_size;
+	if (uar->free_bf_bmap == (1 << dev->caps.bf_regs_per_page) - 1)
+		list_del_init(&uar->bf_list);
+
+	goto out;
+
+unamp_uar:
+	bf->uar = NULL;
+	iounmap(uar->map);
+
+free_uar:
+	mlx4_uar_free(dev, uar);
+
+free_kmalloc:
+	kfree(uar);
+
+out:
+	mutex_unlock(&priv->bf_mutex);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_bf_alloc);
+
+void mlx4_bf_free(struct mlx4_dev *dev, struct mlx4_bf *bf)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int idx;
+
+	if (!bf->uar || !bf->uar->bf_map)
+		return;
+
+	mutex_lock(&priv->bf_mutex);
+	idx = (bf->reg - bf->uar->bf_map) / dev->caps.bf_reg_size;
+	bf->uar->free_bf_bmap &= ~(1 << idx);
+	if (!bf->uar->free_bf_bmap) {
+		if (!list_empty(&bf->uar->bf_list))
+			list_del(&bf->uar->bf_list);
+
+		io_mapping_unmap(bf->uar->bf_map);
+		iounmap(bf->uar->map);
+		mlx4_uar_free(dev, bf->uar);
+		kfree(bf->uar);
+	} else if (list_empty(&bf->uar->bf_list))
+		list_add(&bf->uar->bf_list, &priv->bf_list);
+
+	mutex_unlock(&priv->bf_mutex);
+}
+EXPORT_SYMBOL_GPL(mlx4_bf_free);
+
+#else
+int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int numa_node)
+{
+	memset(bf, 0, sizeof *bf);
+	return -ENOSYS;
+}
+EXPORT_SYMBOL_GPL(mlx4_bf_alloc);
+
+void mlx4_bf_free(struct mlx4_dev *dev, struct mlx4_bf *bf)
+{
+	return;
+}
+EXPORT_SYMBOL_GPL(mlx4_bf_free);
+#endif
+
 int mlx4_init_uar_table(struct mlx4_dev *dev)
 {
 	if (dev->caps.num_uars <= 128) {
@@ -99,7 +231,7 @@ int mlx4_init_uar_table(struct mlx4_dev 
 
 	return mlx4_bitmap_init(&mlx4_priv(dev)->uar_table.bitmap,
 				dev->caps.num_uars, dev->caps.num_uars - 1,
-				max(128, dev->caps.reserved_uars), 0);
+				max_t(u32, 128, dev->caps.reserved_uars), 0);
 }
 
 void mlx4_cleanup_uar_table(struct mlx4_dev *dev)
diff -r c23d1fc7e422 drivers/net/mlx4/port.c
--- a/drivers/net/mlx4/port.c
+++ b/drivers/net/mlx4/port.c
@@ -32,17 +32,79 @@
 
 #include <linux/errno.h>
 #include <linux/if_ether.h>
+#include <linux/if_vlan.h>
 
 #include <linux/mlx4/cmd.h>
 
+#include <linux/mlx4/device.h>
+
 #include "mlx4.h"
+#include "en_port.h"
+
+int mlx4_ib_set_4k_mtu = 0;
+module_param_named(set_4k_mtu, mlx4_ib_set_4k_mtu, int, 0444);
+MODULE_PARM_DESC(set_4k_mtu, "attempt to set 4K MTU to all ConnectX ports");
+
+static unsigned int pfctx = 0;
+module_param(pfctx, uint, 0444);
+MODULE_PARM_DESC(pfctx, "Priority based Flow Control policy on TX[7:0]."
+		" Per priority bit mask");
+
+static unsigned int pfcrx = 0;
+module_param(pfcrx, uint, 0444);
+MODULE_PARM_DESC(pfcrx, "Priority based Flow Control policy on RX[7:0]."
+		" Per priority bit mask");
+
 
 #define MLX4_MAC_VALID		(1ull << 63)
-#define MLX4_MAC_MASK		0xffffffffffffULL
+#define MLX4_MAC_MASK		0x7fffffffffffffffULL
 
 #define MLX4_VLAN_VALID		(1u << 31)
 #define MLX4_VLAN_MASK		0xfff
 
+int mlx4_SET_MCAST_FLTR(struct mlx4_dev *dev, u8 port,
+			u64 mac, u64 clear, u8 mode)
+{
+	return mlx4_cmd(dev, (mac | (clear << 63)), port, mode,
+			MLX4_CMD_SET_MCAST_FLTR, MLX4_CMD_TIME_CLASS_B);
+}
+EXPORT_SYMBOL_GPL(mlx4_SET_MCAST_FLTR);
+
+int mlx4_SET_VLAN_FLTR(struct mlx4_dev *dev, u8 port, struct vlan_group *grp)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_set_vlan_fltr_mbox *filter;
+	int i;
+	int j;
+	int index = 0;
+	u32 entry;
+	int err = 0;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	filter = mailbox->buf;
+	if (grp) {
+		memset(filter, 0, sizeof *filter);
+		for (i = VLAN_FLTR_SIZE - 1; i >= 0; i--) {
+			entry = 0;
+			for (j = 0; j < 32; j++)
+				if (vlan_group_get_device(grp, index++))
+					entry |= 1 << j;
+			filter->entry[i] = cpu_to_be32(entry);
+		}
+	} else {
+		/* When no vlans are configured we block all vlans */
+		memset(filter, 0, sizeof(*filter));
+	}
+	err = mlx4_cmd(dev, mailbox->dma, port, 0, MLX4_CMD_SET_VLAN_FLTR,
+		       MLX4_CMD_TIME_CLASS_B);
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_SET_VLAN_FLTR);
+
 void mlx4_init_mac_table(struct mlx4_dev *dev, struct mlx4_mac_table *table)
 {
 	int i;
@@ -65,7 +127,7 @@ void mlx4_init_vlan_table(struct mlx4_de
 		table->entries[i] = 0;
 		table->refs[i]	 = 0;
 	}
-	table->max   = 1 << dev->caps.log_num_vlans;
+	table->max   = (1 << dev->caps.log_num_vlans) - 2;
 	table->total = 0;
 }
 
@@ -90,27 +152,118 @@ static int mlx4_set_port_mac_table(struc
 	return err;
 }
 
-int mlx4_register_mac(struct mlx4_dev *dev, u8 port, u64 mac, int *index)
+static int mlx4_uc_steer_add(struct mlx4_dev *dev, u8 port,
+			     u64 mac, int *qpn, u8 reserve)
 {
-	struct mlx4_mac_table *table = &mlx4_priv(dev)->port[port].mac_table;
+	struct mlx4_qp qp;
+	u8 vep_num;
+	u8 gid[16] = {0};
+	int err;
+
+	if (reserve) {
+		err = mlx4_qp_reserve_range(dev, 1, 1, qpn, 0xC0);
+		if (err) {
+			mlx4_err(dev, "Failed to reserve qp for mac registration\n");
+			return err;
+		}
+	}
+	qp.qpn = *qpn;
+
+	vep_num = ((u8) (mac >> 48));
+	mac &= 0xffffffffffffULL;
+	mac = cpu_to_be64(mac << 16);
+	memcpy(&gid[10], &mac, ETH_ALEN);
+	gid[4] = vep_num;
+	gid[5] = port;
+	gid[7] = MLX4_UC_STEER << 1;
+
+	err = mlx4_qp_attach_common(dev, &qp, gid, 0,
+				    MLX4_PROT_ETH, MLX4_UC_STEER, 0);
+	if (err && reserve)
+		mlx4_qp_release_range(dev, *qpn, 1);
+
+	return err;
+}
+
+static void mlx4_uc_steer_release(struct mlx4_dev *dev, u8 port,
+				  u64 mac, int qpn, u8 free)
+{
+	struct mlx4_qp qp;
+	u8 vep_num;
+	u8 gid[16] = {0};
+
+	qp.qpn = qpn;
+	vep_num = ((u8) (mac >> 48));
+	mac &= 0xffffffffffffULL;
+	mac = cpu_to_be64(mac << 16);
+	memcpy(&gid[10], &mac, ETH_ALEN);
+	gid[4] = vep_num;
+	gid[5] = port;
+	gid[7] = MLX4_UC_STEER << 1;
+
+	mlx4_qp_detach_common(dev, &qp, gid, MLX4_PROT_ETH, MLX4_UC_STEER, 0);
+	if (free)
+		mlx4_qp_release_range(dev, qpn, 1);
+}
+
+int mlx4_register_mac(struct mlx4_dev *dev, u8 port, u64 mac, int *qpn, u8 wrap)
+{
+	struct mlx4_port_info *info = &mlx4_priv(dev)->port[port];
+	struct mlx4_mac_table *table = &info->mac_table;
+	u64 out_param;
+	struct mlx4_mac_entry *entry;
 	int i, err = 0;
 	int free = -1;
 
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd_imm(dev, mac, &out_param, RES_MAC, port,
+				   MLX4_CMD_ALLOC_RES, MLX4_CMD_TIME_CLASS_A);
+		if (!err)
+			*qpn = out_param;
+		return err;
+	} else if (!wrap)
+		mac |= (u64) (dev->caps.vep_num) << 48;
+
+	if (dev->caps.vep_uc_steering) {
+		err = mlx4_uc_steer_add(dev, port, mac, qpn, 1);
+		if (!err) {
+			entry = kmalloc(sizeof *entry, GFP_KERNEL);
+			if (!entry) {
+				mlx4_uc_steer_release(dev, port, mac, *qpn, 1);
+				return -ENOMEM;
+			}
+			entry->mac = mac;
+			err = radix_tree_insert(&info->mac_tree, *qpn, entry);
+			if (err) {
+				mlx4_uc_steer_release(dev, port, mac, *qpn, 1);
+				kfree(entry);
+				return err;
+			}
+		}
+		else
+			return err;
+	}
+
 	mlx4_dbg(dev, "Registering MAC: 0x%llx\n", (unsigned long long) mac);
 	mutex_lock(&table->mutex);
-	for (i = 0; i < MLX4_MAX_MAC_NUM - 1; i++) {
-		if (free < 0 && !table->refs[i]) {
+	for (i = 0; i < MLX4_MAX_MAC_NUM; i++) {
+		if (free < 0 && !table->entries[i]) {
 			free = i;
 			continue;
 		}
 
 		if (mac == (MLX4_MAC_MASK & be64_to_cpu(table->entries[i]))) {
-			/* MAC already registered, increase refernce count */
-			*index = i;
-			++table->refs[i];
+			/* MAC + PF already registered, Must not have duplicates */
+			err = -EEXIST;
 			goto out;
 		}
 	}
+
+	if (free < 0) {
+		err = -ENOMEM;
+		goto out;
+	}
+
 	mlx4_dbg(dev, "Free MAC index is %d\n", free);
 
 	if (table->total == table->max) {
@@ -120,18 +273,17 @@ int mlx4_register_mac(struct mlx4_dev *d
 	}
 
 	/* Register new MAC */
-	table->refs[free] = 1;
 	table->entries[free] = cpu_to_be64(mac | MLX4_MAC_VALID);
 
 	err = mlx4_set_port_mac_table(dev, port, table->entries);
 	if (unlikely(err)) {
 		mlx4_err(dev, "Failed adding MAC: 0x%llx\n", (unsigned long long) mac);
-		table->refs[free] = 0;
 		table->entries[free] = 0;
 		goto out;
 	}
 
-	*index = free;
+	if (!dev->caps.vep_uc_steering)
+		*qpn = info->base_qpn + free;
 	++table->total;
 out:
 	mutex_unlock(&table->mutex);
@@ -139,20 +291,57 @@ out:
 }
 EXPORT_SYMBOL_GPL(mlx4_register_mac);
 
-void mlx4_unregister_mac(struct mlx4_dev *dev, u8 port, int index)
+static int validate_index(struct mlx4_dev *dev,
+			  struct mlx4_mac_table *table, int index)
 {
-	struct mlx4_mac_table *table = &mlx4_priv(dev)->port[port].mac_table;
+	int err = 0;
+
+	if (index < 0 || index >= table->max || !table->entries[index]) {
+		mlx4_warn(dev, "No valid Mac entry for the given index\n");
+		err = -EINVAL;
+	}
+	return err;
+}
+
+static int find_index(struct mlx4_dev *dev,
+		      struct mlx4_mac_table *table, u64 mac)
+{
+	int i;
+	for (i = 0; i < MLX4_MAX_MAC_NUM; i++) {
+		if (mac == (MLX4_MAC_MASK & be64_to_cpu(table->entries[i])))
+			return i;
+	}
+	/* Mac not found */
+	return -EINVAL;
+}
+
+void mlx4_unregister_mac(struct mlx4_dev *dev, u8 port, int qpn)
+{
+	struct mlx4_port_info *info = &mlx4_priv(dev)->port[port];
+	struct mlx4_mac_table *table = &info->mac_table;
+	int index = qpn - info->base_qpn;
+	struct mlx4_mac_entry *entry;
+
+	if (mlx4_is_slave(dev)) {
+		mlx4_cmd(dev, qpn, RES_MAC, port,
+			 MLX4_CMD_FREE_RES, MLX4_CMD_TIME_CLASS_A);
+		return;
+	}
+	if (dev->caps.vep_uc_steering) {
+		entry = radix_tree_lookup(&info->mac_tree, qpn);
+		if (entry) {
+			mlx4_uc_steer_release(dev, port, entry->mac, qpn, 1);
+			radix_tree_delete(&info->mac_tree, qpn);
+			index = find_index(dev, table, entry->mac);
+			kfree(entry);
+		}
+	}
 
 	mutex_lock(&table->mutex);
-	if (!table->refs[index]) {
-		mlx4_warn(dev, "No MAC entry for index %d\n", index);
+
+	if (validate_index(dev, table, index))
 		goto out;
-	}
-	if (--table->refs[index]) {
-		mlx4_warn(dev, "Have more references for index %d,"
-			  "no need to modify MAC table\n", index);
-		goto out;
-	}
+
 	table->entries[index] = 0;
 	mlx4_set_port_mac_table(dev, port, table->entries);
 	--table->total;
@@ -161,6 +350,52 @@ out:
 }
 EXPORT_SYMBOL_GPL(mlx4_unregister_mac);
 
+int mlx4_replace_mac(struct mlx4_dev *dev, u8 port, int qpn, u64 new_mac, u8 wrap)
+{
+	struct mlx4_port_info *info = &mlx4_priv(dev)->port[port];
+	struct mlx4_mac_table *table = &info->mac_table;
+	int index = qpn - info->base_qpn;
+	struct mlx4_mac_entry *entry;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd_imm(dev, new_mac, (u64 *) &qpn, RES_MAC, port,
+				   MLX4_CMD_REPLACE_RES, MLX4_CMD_TIME_CLASS_A);
+		return err;
+	} else if (!wrap)
+		new_mac |= (u64) (dev->caps.vep_num) << 48;
+
+	if (dev->caps.vep_uc_steering) {
+		entry = radix_tree_lookup(&info->mac_tree, qpn);
+		if (!entry)
+			return -EINVAL;
+		index = find_index(dev, table, entry->mac);
+		mlx4_uc_steer_release(dev, port, entry->mac, qpn, 0);
+		entry->mac = new_mac;
+		err = mlx4_uc_steer_add(dev, port, entry->mac, &qpn, 0);
+		if (err || index < 0)
+			return err;
+	}
+
+	mutex_lock(&table->mutex);
+
+	err = validate_index(dev, table, index);
+	if (err)
+		goto out;
+
+	table->entries[index] = cpu_to_be64(new_mac | MLX4_MAC_VALID);
+
+	err = mlx4_set_port_mac_table(dev, port, table->entries);
+	if (unlikely(err)) {
+		mlx4_err(dev, "Failed adding MAC: 0x%llx\n", (unsigned long long) new_mac);
+		table->entries[index] = 0;
+	}
+out:
+	mutex_unlock(&table->mutex);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_replace_mac);
+
 static int mlx4_set_port_vlan_table(struct mlx4_dev *dev, u8 port,
 				    __be32 *entries)
 {
@@ -182,13 +417,45 @@ static int mlx4_set_port_vlan_table(stru
 	return err;
 }
 
+int mlx4_find_cached_vlan(struct mlx4_dev *dev, u8 port, u16 vid, int *idx)
+{
+	struct mlx4_vlan_table *table = &mlx4_priv(dev)->port[port].vlan_table;
+	int i;
+
+	if (mlx4_is_slave(dev))
+		return -ENOENT;
+
+	for (i = 0; i < MLX4_MAX_VLAN_NUM; ++i) {
+		if (table->refs[i] &&
+		    (vid == (MLX4_VLAN_MASK &
+			      be32_to_cpu(table->entries[i])))) {
+			/* Vlan already registered, increase refernce count */
+			*idx = i;
+			return 0;
+		}
+	}
+
+	return -ENOENT;
+}
+EXPORT_SYMBOL_GPL(mlx4_find_cached_vlan);
+
 int mlx4_register_vlan(struct mlx4_dev *dev, u8 port, u16 vlan, int *index)
 {
 	struct mlx4_vlan_table *table = &mlx4_priv(dev)->port[port].vlan_table;
 	int i, err = 0;
 	int free = -1;
 
+	if (mlx4_is_slave(dev))
+		return 0;
+
 	mutex_lock(&table->mutex);
+
+	if (table->total == table->max) {
+		/* No free vlan entries */
+		err = -ENOSPC;
+		goto out;
+	}
+
 	for (i = MLX4_VLAN_REGULAR; i < MLX4_MAX_VLAN_NUM; i++) {
 		if (free < 0 && (table->refs[i] == 0)) {
 			free = i;
@@ -205,9 +472,8 @@ int mlx4_register_vlan(struct mlx4_dev *
 		}
 	}
 
-	if (table->total == table->max) {
-		/* No free vlan entries */
-		err = -ENOSPC;
+	if (free < 0) {
+		err = -ENOMEM;
 		goto out;
 	}
 
@@ -235,6 +501,9 @@ void mlx4_unregister_vlan(struct mlx4_de
 {
 	struct mlx4_vlan_table *table = &mlx4_priv(dev)->port[port].vlan_table;
 
+	if (mlx4_is_slave(dev))
+		return;
+
 	if (index < MLX4_VLAN_REGULAR) {
 		mlx4_warn(dev, "Trying to free special vlan index %d\n", index);
 		return;
@@ -294,12 +563,186 @@ int mlx4_get_port_ib_caps(struct mlx4_de
 	return err;
 }
 
+static int mlx4_common_set_port(struct mlx4_dev *dev, int slave, u32 in_mod,
+				u8 op_mod, struct mlx4_cmd_mailbox *inbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_port_info *port_info;
+	struct mlx4_mfunc_master_ctx *master = &priv->mfunc.master;
+	struct mlx4_slave_state *slave_st = &master->slave_state[slave];
+	struct mlx4_set_port_rqp_calc_context *qpn_context;
+	struct mlx4_set_port_general_context *gen_context;
+	int reset_qkey_viols;
+	int port;
+	int is_eth;
+	u32 in_modifier;
+	u32 promisc;
+	u16 mtu, prev_mtu;
+	int err;
+	int i;
+	__be32 agg_cap_mask;
+	__be32 slave_cap_mask;
+	__be32 new_cap_mask;
+
+	port = in_mod & 0xff;
+	in_modifier = in_mod >> 8;
+	is_eth = op_mod;
+	port_info = &priv->port[port];
+
+	/* All slaves can perform SET_PORT operations, just need to verify
+	 * we keep the mutual resources unchanged */
+	if (is_eth) {
+		switch (in_modifier) {
+		case MLX4_SET_PORT_RQP_CALC:
+			qpn_context = inbox->buf;
+			qpn_context->base_qpn = cpu_to_be32(port_info->base_qpn);
+			qpn_context->n_mac = 0x7;
+			promisc = be32_to_cpu(qpn_context->promisc) >>
+				SET_PORT_PROMISC_SHIFT;
+			qpn_context->promisc = cpu_to_be32(
+				promisc << SET_PORT_PROMISC_SHIFT |
+				port_info->base_qpn);
+			promisc = be32_to_cpu(qpn_context->mcast) >>
+				SET_PORT_MC_PROMISC_SHIFT;
+			qpn_context->mcast = cpu_to_be32(
+				promisc << SET_PORT_MC_PROMISC_SHIFT |
+				port_info->base_qpn);
+			break;
+		case MLX4_SET_PORT_GENERAL:
+			gen_context = inbox->buf;
+			/* Mtu is configured as the max MTU among all the
+			 * the functions on the port. */
+			mtu = be16_to_cpu(gen_context->mtu);
+			mtu = min_t(int, mtu, dev->caps.eth_mtu_cap[port]);
+			prev_mtu = slave_st->mtu[port];
+			slave_st->mtu[port] = mtu;
+			if (mtu > master->max_mtu[port])
+				master->max_mtu[port] = mtu;
+			if (mtu < prev_mtu && prev_mtu == master->max_mtu[port]) {
+				slave_st->mtu[port] = mtu;
+				master->max_mtu[port] = mtu;
+				for (i = 0; i < dev->num_slaves; i++) {
+					master->max_mtu[port] =
+						max(master->max_mtu[port],
+						    master->slave_state[i].mtu[port]);
+				}
+			}
+
+			gen_context->mtu = cpu_to_be16(master->max_mtu[port]);
+			break;
+		}
+		return mlx4_cmd(dev, inbox->dma, in_mod, op_mod,
+				MLX4_CMD_SET_PORT, MLX4_CMD_TIME_CLASS_B);
+	}
+
+	/* For IB, we only consider:
+	 * - The capability mask, which is set to the aggregate of all slave frunction
+	 *   capabilities
+	 * - The QKey violatin counter - reset according to each request.
+	 */
+
+	if (dev->flags & MLX4_FLAG_OLD_PORT_CMDS) {
+		reset_qkey_viols = (*(u8 *) inbox->buf) & 0x40;
+		new_cap_mask = ((__be32 *) inbox->buf)[2];
+	} else {
+		reset_qkey_viols = ((u8 *) inbox->buf)[3] & 0x1;
+		new_cap_mask = ((__be32 *) inbox->buf)[1];
+	}
+
+	/* only master has access to qp0 */
+	if (new_cap_mask & cpu_to_be32(IB_PORT_SM)) {
+		mlx4_warn(dev, "denying sm port capability for slave:%d\n", slave);
+		return -EINVAL;
+	}
+
+	agg_cap_mask = 0;
+	slave_cap_mask = priv->mfunc.master.slave_state[slave].ib_cap_mask[port];
+	priv->mfunc.master.slave_state[slave].ib_cap_mask[port] = new_cap_mask;
+	for (i = 0; i < dev->num_slaves; i++)
+		agg_cap_mask |= priv->mfunc.master.slave_state[slave].ib_cap_mask[port];
+
+#if 0
+	mlx4_warn(dev, "old_slave_cap:0x%x slave_cap:0x%x cap:0x%x qkey_reset:%d\n",
+			slave_cap_mask, priv->mfunc.master.slave_state[slave].ib_cap_mask[port],
+			agg_cap_mask, reset_qkey_viols);
+#endif
+
+	memset(inbox->buf, 0, 256);
+	if (dev->flags & MLX4_FLAG_OLD_PORT_CMDS) {
+		*(u8 *) inbox->buf	   = !!reset_qkey_viols << 6;
+		((__be32 *) inbox->buf)[2] = agg_cap_mask;
+	} else {
+		((u8 *) inbox->buf)[3]     = !!reset_qkey_viols;
+		((__be32 *) inbox->buf)[1] = agg_cap_mask;
+	}
+
+	err = mlx4_cmd(dev, inbox->dma, port, is_eth, MLX4_CMD_SET_PORT,
+		       MLX4_CMD_TIME_CLASS_B);
+	if (err)
+		priv->mfunc.master.slave_state[slave].ib_cap_mask[port] = slave_cap_mask;
+	return err;
+}
+
+int mlx4_SET_PORT_wrapper(struct mlx4_dev *dev, int slave,
+			  struct mlx4_vhcr *vhcr,
+			  struct mlx4_cmd_mailbox *inbox,
+			  struct mlx4_cmd_mailbox *outbox)
+{
+	return mlx4_common_set_port(dev, slave, vhcr->in_modifier,
+				    vhcr->op_modifier, inbox);
+}
+
+
+int mlx4_check_ext_port_caps(struct mlx4_dev *dev, u8 port)
+{
+	struct mlx4_cmd_mailbox *inmailbox, *outmailbox;
+	u8 *inbuf, *outbuf;
+	int err, packet_error;
+
+	inmailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(inmailbox))
+		return PTR_ERR(inmailbox);
+
+	outmailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(outmailbox)) {
+		mlx4_free_cmd_mailbox(dev, inmailbox);
+		return PTR_ERR(outmailbox);
+	}
+
+	inbuf = inmailbox->buf;
+	outbuf = outmailbox->buf;
+	memset(inbuf, 0, 256);
+	memset(outbuf, 0, 256);
+	inbuf[0] = 1;
+	inbuf[1] = 1;
+	inbuf[2] = 1;
+	inbuf[3] = 1;
+
+	*(__be16 *) (&inbuf[16]) = MLX4_ATTR_EXTENDED_PORT_INFO;
+	*(__be32 *) (&inbuf[20]) = cpu_to_be32(port);
+
+	err = mlx4_cmd_box(dev, inmailbox->dma, outmailbox->dma, port, 3,
+			   MLX4_CMD_MAD_IFC, MLX4_CMD_TIME_CLASS_C);
+
+	packet_error = be16_to_cpu(*(__be16 *) (outbuf + 4));
+
+	dev->caps.ext_port_cap[port] = (!err && !packet_error) ?
+				       MLX_EXT_PORT_CAP_FLAG_EXTENDED_PORT_INFO
+				       : 0;
+
+	mlx4_free_cmd_mailbox(dev, inmailbox);
+	mlx4_free_cmd_mailbox(dev, outmailbox);
+	return err;
+}
+
 int mlx4_SET_PORT(struct mlx4_dev *dev, u8 port)
 {
 	struct mlx4_cmd_mailbox *mailbox;
 	int err;
+	u32 in_mod;
 
-	if (dev->caps.port_type[port] == MLX4_PORT_TYPE_ETH)
+	if ((dev->caps.port_type[port] != MLX4_PORT_TYPE_IB) &&
+	    (dev->caps.port_type[port] != MLX4_PORT_TYPE_ETH))
 		return 0;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
@@ -308,10 +751,477 @@ int mlx4_SET_PORT(struct mlx4_dev *dev, 
 
 	memset(mailbox->buf, 0, 256);
 
-	((__be32 *) mailbox->buf)[1] = dev->caps.ib_port_def_cap[port];
-	err = mlx4_cmd(dev, mailbox->dma, port, 0, MLX4_CMD_SET_PORT,
-		       MLX4_CMD_TIME_CLASS_B);
+	if (dev->caps.port_type[port] == MLX4_PORT_TYPE_ETH) {
+		in_mod = MLX4_SET_PORT_GENERAL << 8 | port;
+		if (mlx4_is_master(dev))
+			err = mlx4_common_set_port(dev, dev->caps.function,
+						   in_mod, 1, mailbox);
+		else
+			err = mlx4_cmd(dev, mailbox->dma, in_mod, 1,
+				       MLX4_CMD_SET_PORT,
+				       MLX4_CMD_TIME_CLASS_B);
+	} else {
+		if (mlx4_ib_set_4k_mtu)
+			((__be32 *) mailbox->buf)[0] |= cpu_to_be32((1 << 22) |
+								    (1 << 21) |
+								    (5 << 12) |
+								    (2 << 4));
+
+		((__be32 *) mailbox->buf)[1] = dev->caps.ib_port_def_cap[port];
+
+		if (mlx4_is_master(dev))
+			err = mlx4_common_set_port(dev, dev->caps.function,
+						   port, 0, mailbox);
+		else
+			err = mlx4_cmd(dev, mailbox->dma, port, 0,
+				       MLX4_CMD_SET_PORT,
+				       MLX4_CMD_TIME_CLASS_B);
+	}
 
 	mlx4_free_cmd_mailbox(dev, mailbox);
 	return err;
 }
+
+
+int mlx4_SET_PORT_general(struct mlx4_interface *intf, struct mlx4_dev *dev,
+		u8 port, int mtu, u8 *pptx, u8 *pprx)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_set_port_general_context *context;
+	int err;
+	u32 in_mod;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	context = mailbox->buf;
+	memset(context, 0, sizeof *context);
+
+	if (*pprx && pfcrx) {
+		mlx4_warn(dev, "port %d: ignoring setting of RX global pause"
+				" since RX PFC is enabled\n", port);
+		*pprx = 0;
+	}
+	if (*pptx && pfctx) {
+		mlx4_warn(dev, "port %d: ignoring setting of TX global pause"
+				" since TX PFC is enabled\n", port);
+		*pptx = 0;
+	}
+
+	mutex_lock(&priv->port_ops_mutex);
+	context->flags = SET_PORT_GEN_ALL_VALID | (0x40 & (dev->caps.qinq << 6));
+	context->mtu = cpu_to_be16(mlx4_set_interface_mtu_get_max(intf, dev, port, mtu));
+	context->pptx = (*pptx * (!pfctx)) << 7;
+	context->pfctx = pfctx;
+	context->pprx = (*pprx * (!pfcrx)) << 7;
+	context->pfcrx = pfcrx;
+	context->qinq = (dev->caps.qinq) ? (1 << 7) : 0;
+
+	in_mod = MLX4_SET_PORT_GENERAL << 8 | port;
+	if (mlx4_is_master(dev))
+		err = mlx4_common_set_port(dev, dev->caps.function, in_mod, 1, mailbox);
+	else
+		err = mlx4_cmd(dev, mailbox->dma, in_mod, 1, MLX4_CMD_SET_PORT,
+			       MLX4_CMD_TIME_CLASS_B);
+	mutex_unlock(&priv->port_ops_mutex);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(mlx4_SET_PORT_general);
+
+void mlx4_get_port_pfc(struct mlx4_dev *dev, u8 port, u8 *r_pfctx, u8 *r_pfcrx)
+{
+	*r_pfctx = pfctx;
+	*r_pfcrx = pfcrx;
+}
+EXPORT_SYMBOL(mlx4_get_port_pfc);
+
+int mlx4_SET_PORT_qpn_calc(struct mlx4_dev *dev, u8 port, u32 base_qpn,
+			   u8 promisc)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_set_port_rqp_calc_context *context;
+	int err;
+	u32 in_mod;
+	u32 m_promisc = (dev->caps.vep_mc_steering) ? MCAST_DIRECT : MCAST_DEFAULT;
+
+	if (dev->caps.vep_mc_steering && dev->caps.vep_uc_steering)
+		return 0;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	context = mailbox->buf;
+	memset(context, 0, sizeof *context);
+
+	context->base_qpn = cpu_to_be32(base_qpn);
+	context->n_mac = dev->caps.log_num_macs;
+	context->promisc = cpu_to_be32(promisc << SET_PORT_PROMISC_SHIFT |
+				       base_qpn);
+	context->mcast = cpu_to_be32(m_promisc << SET_PORT_MC_PROMISC_SHIFT |
+				     base_qpn);
+	context->intra_no_vlan = 0;
+	context->no_vlan = MLX4_NO_VLAN_IDX;
+	context->intra_vlan_miss = 0;
+	context->vlan_miss = MLX4_VLAN_MISS_IDX;
+
+	in_mod = MLX4_SET_PORT_RQP_CALC << 8 | port;
+	if (mlx4_is_master(dev))
+		err = mlx4_common_set_port(dev, dev->caps.function, in_mod, 1, mailbox);
+	else
+		err = mlx4_cmd(dev, mailbox->dma, in_mod, 1, MLX4_CMD_SET_PORT,
+			       MLX4_CMD_TIME_CLASS_B);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(mlx4_SET_PORT_qpn_calc);
+
+static int mlx4_common_set_mcast_fltr(struct mlx4_dev *dev, int function,
+				      int port, u64 addr, u64 clear, u8 mode)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int err = 0;
+	struct mlx4_mcast_entry *entry, *tmp;
+	struct mlx4_slave_state *s_state = &priv->mfunc.master.slave_state[function];
+	int i;
+
+	switch (mode) {
+	case MLX4_MCAST_DISABLE:
+		/* The multicast filter is disabled only once,
+		 * If some other function already done it, operation
+		 * is ignored */
+		if (!(priv->mfunc.master.disable_mcast_ref[port]++))
+			err = mlx4_cmd(dev, 0, port, MLX4_MCAST_DISABLE,
+					MLX4_CMD_SET_MCAST_FLTR,
+					MLX4_CMD_TIME_CLASS_B);
+		break;
+	case MLX4_MCAST_ENABLE:
+		/* We enable the muticast filter only if all functions
+		 * have the filter enabled */
+		if (!(--priv->mfunc.master.disable_mcast_ref[port]))
+			err = mlx4_cmd(dev, 0, port, MLX4_MCAST_ENABLE,
+					MLX4_CMD_SET_MCAST_FLTR,
+					MLX4_CMD_TIME_CLASS_B);
+		break;
+	case MLX4_MCAST_CONFIG:
+		if (clear) {
+			/* Disable the muticast filter while updating it */
+			if (!priv->mfunc.master.disable_mcast_ref[port]) {
+				err = mlx4_cmd(dev, 0, port, MLX4_MCAST_DISABLE,
+						MLX4_CMD_SET_MCAST_FLTR,
+						MLX4_CMD_TIME_CLASS_B);
+				if (err) {
+					mlx4_warn(dev, "Failed to disable multicast "
+						       "filter\n");
+					goto out;
+				}
+			}
+			/* Clear the multicast filter */
+			err = mlx4_cmd(dev, clear << 63, port,
+				       MLX4_MCAST_CONFIG,
+				       MLX4_CMD_SET_MCAST_FLTR,
+				       MLX4_CMD_TIME_CLASS_B);
+			if (err) {
+				mlx4_warn(dev, "Failed clearing the multicast filter\n");
+				goto out;
+			}
+
+			/* Clear the multicast addresses for the given slave */
+			list_for_each_entry_safe(entry, tmp,
+						 &s_state->mcast_filters[port],
+						 list) {
+				list_del(&entry->list);
+				kfree(entry);
+			}
+
+			/* Assign all the multicast addresses that still exist */
+			for (i = 0; i < dev->num_slaves; i++) {
+				list_for_each_entry(entry,
+					&priv->mfunc.master.slave_state[function].mcast_filters[port],
+					list) {
+					if (mlx4_cmd(dev, entry->addr, port,
+						     MLX4_MCAST_CONFIG,
+						     MLX4_CMD_SET_MCAST_FLTR,
+						     MLX4_CMD_TIME_CLASS_B))
+						mlx4_warn(dev, "Failed to reconfigure "
+							  "multicast address: 0x%llx\n",
+							  entry->addr);
+				}
+			}
+			/* Enable the filter */
+			if (!priv->mfunc.master.disable_mcast_ref[port]) {
+				err = mlx4_cmd(dev, 0, port, MLX4_MCAST_ENABLE,
+						MLX4_CMD_SET_MCAST_FLTR,
+						MLX4_CMD_TIME_CLASS_B);
+				if (err) {
+					mlx4_warn(dev, "Failed to enable multicast "
+						       "filter\n");
+					goto out;
+				}
+			}
+		}
+		/* Add the new address if exists */
+		if (addr) {
+			entry = kzalloc(sizeof (struct mlx4_mcast_entry),
+					GFP_KERNEL);
+			if (!entry) {
+				mlx4_warn(dev, "Failed to allocate entry for "
+					       "muticast address\n");
+				err = -ENOMEM;
+				goto out;
+			}
+			INIT_LIST_HEAD(&entry->list);
+			entry->addr = addr;
+			list_add_tail(&entry->list, &s_state->mcast_filters[port]);
+			err = mlx4_cmd(dev, addr, port, MLX4_MCAST_CONFIG,
+				       MLX4_CMD_SET_MCAST_FLTR,
+				       MLX4_CMD_TIME_CLASS_B);
+			if (err)
+				mlx4_warn(dev, "Failed to add the new address:"
+					       "0x%llx\n", addr);
+		}
+		break;
+	default:
+		mlx4_warn(dev, "SET_MCAST_FILTER called with illegal modifier\n");
+		err = -EINVAL;
+	}
+out:
+	return err;
+}
+
+int mlx4_SET_MCAST_FLTR_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+				struct mlx4_cmd_mailbox *inbox,
+				struct mlx4_cmd_mailbox *outbox)
+{
+	int port = vhcr->in_modifier;
+	u64 addr = vhcr->in_param & 0xffffffffffffULL;
+	u64 clear = vhcr->in_param >> 63;
+	u8 mode = vhcr->op_modifier;
+
+	return mlx4_common_set_mcast_fltr(dev, slave, port, addr, clear, mode);
+}
+
+int mlx4_common_set_vlan_fltr(struct mlx4_dev *dev, int function,
+				     int port, void *buf)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_vlan_fltr *filter;
+	struct mlx4_slave_state *s_state = &priv->mfunc.master.slave_state[function];
+	int i, j, err;
+
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	/* Update slave's Vlan filter */
+	memcpy(s_state->vlan_filter[port]->entry, buf,
+	       sizeof(struct mlx4_vlan_fltr));
+
+	/* We configure the Vlan filter to allow the vlans of
+	 * all slaves */
+	filter = mailbox->buf;
+	memset(filter, 0, sizeof(*filter));
+	for (i = VLAN_FLTR_SIZE - 1; i >= 0; i--) {
+		for (j = 0; j < dev->num_slaves; j++) {
+			s_state = &priv->mfunc.master.slave_state[j];
+			filter->entry[i] |= s_state->vlan_filter[port]->entry[i];
+		}
+	}
+
+	err = mlx4_cmd(dev, mailbox->dma, port, 0, MLX4_CMD_SET_VLAN_FLTR,
+		       MLX4_CMD_TIME_CLASS_B);
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+int mlx4_SET_VLAN_FLTR_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+			       struct mlx4_cmd_mailbox *inbox,
+			       struct mlx4_cmd_mailbox *outbox)
+{
+	int err, port;
+
+	port = vhcr->in_modifier;
+	err =  mlx4_common_set_vlan_fltr(dev, slave, vhcr->in_modifier, inbox->buf);
+
+	if (!err)
+		err = mlx4_add_vlan_fltr_to_tracked_slave(dev, slave,  port);
+	return err;
+}
+
+int mlx4_common_dump_eth_stats(struct mlx4_dev *dev, int slave,
+				      u32 in_mod, struct mlx4_cmd_mailbox *outbox)
+{
+	return mlx4_cmd_box(dev, 0, outbox->dma, in_mod, 0,
+			   MLX4_CMD_DUMP_ETH_STATS, MLX4_CMD_TIME_CLASS_B);
+}
+
+int mlx4_DUMP_ETH_STATS_wrapper(struct mlx4_dev *dev, int slave,
+				   struct mlx4_vhcr *vhcr,
+				   struct mlx4_cmd_mailbox *inbox,
+				   struct mlx4_cmd_mailbox *outbox)
+{
+	return mlx4_common_dump_eth_stats(dev, slave,
+					  vhcr->in_modifier, outbox);
+}
+
+static void fill_port_statistics(void *statistics,
+				 struct mlx4_eth_common_counters *stats)
+{
+	struct mlx4_stat_out_mbox *mlx4_port_stats;
+	mlx4_port_stats = statistics;
+
+	stats->rx_errors = be64_to_cpu(mlx4_port_stats->PCS) +
+			   be32_to_cpu(mlx4_port_stats->RdropLength) +
+			   be32_to_cpu(mlx4_port_stats->RJBBR) +
+			   be32_to_cpu(mlx4_port_stats->RCRC) +
+			   be32_to_cpu(mlx4_port_stats->RRUNT);
+	stats->tx_errors = be32_to_cpu(mlx4_port_stats->TDROP);
+	stats->multicast = be64_to_cpu(mlx4_port_stats->MCAST_prio_0) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_1) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_2) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_3) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_4) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_5) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_6) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_prio_7) +
+			   be64_to_cpu(mlx4_port_stats->MCAST_novlan);
+	stats->rx_length_errors = be32_to_cpu(mlx4_port_stats->RdropLength);
+	stats->rx_over_errors = be32_to_cpu(mlx4_port_stats->RdropOvflw);
+	stats->rx_crc_errors = be32_to_cpu(mlx4_port_stats->RCRC);
+	stats->rx_fifo_errors = be32_to_cpu(mlx4_port_stats->RdropOvflw);
+	stats->rx_missed_errors = be32_to_cpu(mlx4_port_stats->RdropOvflw);
+	stats->broadcast = be64_to_cpu(mlx4_port_stats->RBCAST_prio_0) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_1) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_2) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_3) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_4) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_5) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_6) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_prio_7) +
+				be64_to_cpu(mlx4_port_stats->RBCAST_novlan);
+}
+
+static int read_iboe_counters(struct mlx4_dev *dev, int index, u64 counters[])
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	int err;
+	int mode;
+	struct mlx4_counters_ext *ext;
+	struct mlx4_counters *reg;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return -ENOMEM;
+
+	err = mlx4_cmd_box(dev, 0, mailbox->dma, index, 0,
+			   MLX4_CMD_QUERY_IF_STAT, MLX4_CMD_TIME_CLASS_C);
+	if (err)
+		goto out;
+
+	mode = be32_to_cpu(((struct mlx4_counters *)mailbox->buf)->counter_mode) & 0xf;
+	switch (mode) {
+	case 0:
+		reg = mailbox->buf;
+		counters[0] = be64_to_cpu(reg->rx_frames);
+		counters[1] = be64_to_cpu(reg->tx_frames);
+		counters[2] = be64_to_cpu(reg->rx_bytes);
+		counters[3] = be64_to_cpu(reg->tx_bytes);
+		break;
+	case 1:
+		ext = mailbox->buf;
+		counters[0] = be64_to_cpu(ext->rx_uni_frames);
+		counters[1] = be64_to_cpu(ext->tx_uni_frames);
+		counters[2] = be64_to_cpu(ext->rx_uni_bytes);
+		counters[3] = be64_to_cpu(ext->tx_uni_bytes);
+		break;
+	default:
+		err = -EINVAL;
+	}
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+
+static void fill_function_statistics(void *statistics,
+				     struct mlx4_eth_common_counters *stats)
+{
+	struct mlx4_func_stat_out_mbox *mlx4_function_stats;
+	mlx4_function_stats = statistics;
+
+	stats->rx_errors =
+		be64_to_cpu(mlx4_function_stats->etherStatsCRCAlignErrors) +
+		be64_to_cpu(mlx4_function_stats->etherStatsFragments) +
+		be64_to_cpu(mlx4_function_stats->etherStatsJabbers);
+	/* stats->tx_errors = */
+	stats->multicast =
+		be64_to_cpu(mlx4_function_stats->etherStatsMulticastPkts);
+	/* stats->rx_length_errors = */
+	stats->rx_over_errors =
+		be64_to_cpu(mlx4_function_stats->etherStatsDropEvents);
+	stats->rx_crc_errors =
+		be64_to_cpu(mlx4_function_stats->etherStatsCRCAlignErrors);
+	stats->rx_fifo_errors =
+		be64_to_cpu(mlx4_function_stats->etherStatsDropEvents);
+	stats->rx_missed_errors =
+		be64_to_cpu(mlx4_function_stats->etherStatsDropEvents);
+	stats->broadcast =
+		be64_to_cpu(mlx4_function_stats->etherStatsBroadcastPkts);
+}
+
+int mlx4_DUMP_ETH_STATS(struct mlx4_dev *dev, u8 port, u8 reset,
+			   struct mlx4_eth_common_counters *stats)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	void (*do_fill_statistics)(void *, struct mlx4_eth_common_counters *) = NULL;
+	u32 in_mod;
+	int err;
+	int counter;
+	u64 counters[4];
+
+	memset(counters, 0, sizeof counters);
+	counter = mlx4_get_iboe_counter(dev, port);
+	if (counter >= 0)
+		err = read_iboe_counters(dev, counter, counters);
+
+	if (stats) {
+		stats->iboe_rx_packets = (unsigned long)counters[0];
+		stats->iboe_rx_bytess = (unsigned long)counters[2];
+		stats->iboe_tx_packets = (unsigned long)counters[1];
+		stats->iboe_tx_bytess = (unsigned long)counters[3];
+	}
+
+	in_mod = (reset << 8) | ((mlx4_is_mfunc(dev)) ?
+			(MLX4_DUMP_STATS_FUNC_COUNTERS << 12 | port | dev->caps.vep_num << 16) :
+			(MLX4_DUMP_STATS_PORT_COUNTERS << 12 | port));
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	if (mlx4_is_master(dev))
+		err = mlx4_common_dump_eth_stats(dev, dev->caps.function,
+						 in_mod, mailbox);
+	else
+		err = mlx4_cmd_box(dev, 0, mailbox->dma, in_mod, 0,
+			   MLX4_CMD_DUMP_ETH_STATS, MLX4_CMD_TIME_CLASS_B);
+	if (err)
+		goto out;
+
+	do_fill_statistics = mlx4_is_mfunc(dev) ? fill_function_statistics
+						: fill_port_statistics;
+
+	if (!reset)
+		do_fill_statistics(mailbox->buf, stats);
+
+out:
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_DUMP_ETH_STATS);
diff -r c23d1fc7e422 drivers/net/mlx4/profile.c
--- a/drivers/net/mlx4/profile.c
+++ b/drivers/net/mlx4/profile.c
@@ -32,6 +32,8 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
+
 #include "mlx4.h"
 #include "fw.h"
 
@@ -105,9 +107,19 @@ u64 mlx4_make_profile(struct mlx4_dev *d
 	profile[MLX4_RES_AUXC].num    = request->num_qp;
 	profile[MLX4_RES_SRQ].num     = request->num_srq;
 	profile[MLX4_RES_CQ].num      = request->num_cq;
-	profile[MLX4_RES_EQ].num      = min_t(unsigned, dev_cap->max_eqs,
-					      dev_cap->reserved_eqs +
-					      num_possible_cpus() + 1);
+	if (mlx4_is_master(dev)) {
+		profile[MLX4_RES_EQ].num = dev_cap->reserved_eqs +
+					   MLX4_MFUNC_EQ_NUM *
+					   (dev->num_slaves + 1);
+		if (profile[MLX4_RES_EQ].num > dev_cap->max_eqs) {
+			mlx4_warn(dev, "Not enough eqs for:%ld slave functions\n", dev->num_slaves);
+			kfree(profile);
+			return -ENOMEM;
+		}
+	} else
+		profile[MLX4_RES_EQ].num = min_t(unsigned, dev_cap->max_eqs,
+						 dev_cap->reserved_eqs +
+						 (num_possible_cpus() + 1) * dev_cap->num_ports);
 	profile[MLX4_RES_DMPT].num    = request->num_mpt;
 	profile[MLX4_RES_CMPT].num    = MLX4_NUM_CMPTS;
 	profile[MLX4_RES_MTT].num     = request->num_mtt;
@@ -196,7 +208,13 @@ u64 mlx4_make_profile(struct mlx4_dev *d
 			init_hca->log_num_cqs = profile[i].log_num;
 			break;
 		case MLX4_RES_EQ:
-			dev->caps.num_eqs     = profile[i].num;
+			if (mlx4_is_master(dev)) {
+				dev->caps.num_eqs = dev_cap->reserved_eqs +
+						    min_t(unsigned,
+							  MLX4_MFUNC_EQ_NUM,
+							  num_possible_cpus() + 1);
+			} else
+				dev->caps.num_eqs     = profile[i].num;
 			init_hca->eqc_base    = profile[i].start;
 			init_hca->log_num_eqs = profile[i].log_num;
 			break;
diff -r c23d1fc7e422 drivers/net/mlx4/qp.c
--- a/drivers/net/mlx4/qp.c
+++ b/drivers/net/mlx4/qp.c
@@ -33,6 +33,8 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
+
 #include <linux/mlx4/cmd.h>
 #include <linux/mlx4/qp.h>
 
@@ -63,6 +65,23 @@ void mlx4_qp_event(struct mlx4_dev *dev,
 		complete(&qp->free);
 }
 
+int mlx4_RTR2RTS_QP_wrapper(struct mlx4_dev *dev, int slave,
+			    struct mlx4_vhcr *vhcr,
+			    struct mlx4_cmd_mailbox *inbox,
+			    struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_qp_context *context = inbox->buf + 8;
+	u8 vep_num = mlx4_priv(dev)->mfunc.master.slave_state[slave].vep_num;
+	u8 port = ((context->pri_path.sched_queue >> 6) & 1) + 1;
+
+	if (mlx4_priv(dev)->vep_mode[port])
+		context->pri_path.sched_queue = (context->pri_path.sched_queue & 0xc3 ) |
+						(vep_num << 3);
+	inbox->dma |= (u64) slave;
+	return mlx4_cmd(dev, inbox->dma, vhcr->in_modifier,
+			vhcr->op_modifier, vhcr->op, MLX4_CMD_TIME_CLASS_C);
+}
+
 int mlx4_qp_modify(struct mlx4_dev *dev, struct mlx4_mtt *mtt,
 		   enum mlx4_qp_state cur_state, enum mlx4_qp_state new_state,
 		   struct mlx4_qp_context *context, enum mlx4_qp_optpar optpar,
@@ -110,6 +129,8 @@ int mlx4_qp_modify(struct mlx4_dev *dev,
 
 	struct mlx4_cmd_mailbox *mailbox;
 	int ret = 0;
+	u8 port;
+	u8 vep_num;
 
 	if (cur_state >= MLX4_QP_NUM_STATE || new_state >= MLX4_QP_NUM_STATE ||
 	    !op[cur_state][new_state])
@@ -130,13 +151,21 @@ int mlx4_qp_modify(struct mlx4_dev *dev,
 		context->log_page_size   = mtt->page_shift - MLX4_ICM_PAGE_SHIFT;
 	}
 
+	port = ((context->pri_path.sched_queue >> 6) & 1) + 1;
+	if (dev->caps.port_type[port] == MLX4_PORT_TYPE_ETH) {
+		vep_num = dev->caps.vep_num;
+		context->pri_path.sched_queue = (context->pri_path.sched_queue & 0xc3) |
+						(vep_num << 3);
+	}
+
 	*(__be32 *) mailbox->buf = cpu_to_be32(optpar);
 	memcpy(mailbox->buf + 8, context, sizeof *context);
 
 	((struct mlx4_qp_context *) (mailbox->buf + 8))->local_qpn =
 		cpu_to_be32(qp->qpn);
 
-	ret = mlx4_cmd(dev, mailbox->dma, qp->qpn | (!!sqd_event << 31),
+	ret = mlx4_cmd(dev, mailbox->dma | dev->caps.function,
+		       qp->qpn | (!!sqd_event << 31),
 		       new_state == MLX4_QP_STATE_RST ? 2 : 0,
 		       op[cur_state][new_state], MLX4_CMD_TIME_CLASS_C);
 
@@ -145,17 +174,76 @@ int mlx4_qp_modify(struct mlx4_dev *dev,
 }
 EXPORT_SYMBOL_GPL(mlx4_qp_modify);
 
-int mlx4_qp_reserve_range(struct mlx4_dev *dev, int cnt, int align, int *base)
+u32 mlx4_get_slave_sqp(struct mlx4_dev *dev, int slave)
+{
+	if (mlx4_is_master(dev) && slave < dev->num_slaves) {
+		return mlx4_priv(dev)->mfunc.master.slave_state[slave].sqp_start;
+	}
+	if (mlx4_is_slave(dev) && slave < dev->caps.sqp_demux) {
+		return mlx4_priv(dev)->mfunc.demux_sqp[slave];
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mlx4_get_slave_sqp);
+
+int mlx4_GET_SLAVE_SQP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+							  struct mlx4_cmd_mailbox *inbox,
+							  struct mlx4_cmd_mailbox *outbox)
+{
+	u32 *slave_sqp = outbox->buf;
+	int i;
+
+	for (i = 0; i < 64; i++)
+		slave_sqp[i] = mlx4_get_slave_sqp(dev, i);
+	return 0;
+}
+
+int mlx4_GET_SLAVE_SQP(struct mlx4_dev *dev, u32 *sqp, int num)
+{
+	struct mlx4_cmd_mailbox *mailbox;
+	int err;
+
+	mailbox = mlx4_alloc_cmd_mailbox(dev);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	err = mlx4_cmd_box(dev, 0, mailbox->dma, 0, 0, MLX4_CMD_GET_SLAVE_SQP,
+			   MLX4_CMD_TIME_CLASS_A);
+	if (!err)
+		memcpy(sqp, mailbox->buf, sizeof (u32) * num);
+
+	mlx4_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL_GPL(mlx4_GET_SLAVE_SQP);
+
+int mlx4_qp_reserve_range(struct mlx4_dev *dev, int cnt, int align, int *base,
+			  u32 skip_mask)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_qp_table *qp_table = &priv->qp_table;
-	int qpn;
+	u64 in_param;
+	u64 out_param;
+	int err;
 
-	qpn = mlx4_bitmap_alloc_range(&qp_table->bitmap, cnt, align);
-	if (qpn == -1)
-		return -ENOMEM;
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = cnt;
+		*(((u32 *) &in_param) + 1) = align;
+		err = mlx4_cmd_imm(dev, in_param, &out_param, RES_QP, ICM_RESERVE,
+							      MLX4_CMD_ALLOC_RES,
+							      MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			return err;
+		*base = out_param;
+	} else {
+		int qpn;
 
-	*base = qpn;
+		qpn = mlx4_bitmap_alloc_range(&qp_table->bitmap, cnt, align, skip_mask);
+		if (qpn == -1)
+			return -ENOMEM;
+		*base = qpn;
+	}
+
 	return 0;
 }
 EXPORT_SYMBOL_GPL(mlx4_qp_reserve_range);
@@ -164,13 +252,104 @@ void mlx4_qp_release_range(struct mlx4_d
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
 	struct mlx4_qp_table *qp_table = &priv->qp_table;
-	if (base_qpn < dev->caps.sqp_start + 8)
-		return;
+	u64 in_param;
+	int err;
 
-	mlx4_bitmap_free_range(&qp_table->bitmap, base_qpn, cnt);
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = base_qpn;
+		*(((u32 *) &in_param) + 1) = cnt;
+		err = mlx4_cmd(dev, in_param, RES_QP, ICM_RESERVE,
+						      MLX4_CMD_FREE_RES,
+						      MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			mlx4_warn(dev, "Failed to release qp range base:%d cnt:%d\n",
+									base_qpn, cnt);
+		}
+	} else {
+		if (base_qpn < dev->caps.sqp_start + 8)
+			return;
+
+		mlx4_bitmap_free_range(&qp_table->bitmap, base_qpn, cnt);
+	}
 }
 EXPORT_SYMBOL_GPL(mlx4_qp_release_range);
 
+int mlx4_qp_alloc_icm(struct mlx4_dev *dev, int qpn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_qp_table *qp_table = &priv->qp_table;
+	u64 param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &param) = qpn;
+		*(((u32 *) &param) + 1) = 0;
+		return mlx4_cmd_imm(dev, param, &param, RES_QP, ICM_ALLOC,
+								MLX4_CMD_ALLOC_RES,
+								MLX4_CMD_TIME_CLASS_A);
+	}
+	err = mlx4_table_get(dev, &qp_table->qp_table, qpn);
+	if (err)
+		goto err_out;
+
+	err = mlx4_table_get(dev, &qp_table->auxc_table, qpn);
+	if (err)
+		goto err_put_qp;
+
+	err = mlx4_table_get(dev, &qp_table->altc_table, qpn);
+	if (err)
+		goto err_put_auxc;
+
+	err = mlx4_table_get(dev, &qp_table->rdmarc_table, qpn);
+	if (err)
+		goto err_put_altc;
+
+	err = mlx4_table_get(dev, &qp_table->cmpt_table, qpn);
+	if (err)
+		goto err_put_rdmarc;
+
+	return 0;
+
+err_put_rdmarc:
+	mlx4_table_put(dev, &qp_table->rdmarc_table, qpn);
+
+err_put_altc:
+	mlx4_table_put(dev, &qp_table->altc_table, qpn);
+
+err_put_auxc:
+	mlx4_table_put(dev, &qp_table->auxc_table, qpn);
+
+err_put_qp:
+	mlx4_table_put(dev, &qp_table->qp_table, qpn);
+
+err_out:
+	return err;
+}
+
+void mlx4_qp_free_icm(struct mlx4_dev *dev, int qpn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_qp_table *qp_table = &priv->qp_table;
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = qpn;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd(dev, in_param, RES_QP, ICM_ALLOC,
+						      MLX4_CMD_FREE_RES,
+						      MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed to free icm of qp:%d\n", qpn);
+	} else {
+		mlx4_table_put(dev, &qp_table->cmpt_table, qpn);
+		mlx4_table_put(dev, &qp_table->rdmarc_table, qpn);
+		mlx4_table_put(dev, &qp_table->altc_table, qpn);
+		mlx4_table_put(dev, &qp_table->auxc_table, qpn);
+		mlx4_table_put(dev, &qp_table->qp_table, qpn);
+	}
+}
+
 int mlx4_qp_alloc(struct mlx4_dev *dev, int qpn, struct mlx4_qp *qp)
 {
 	struct mlx4_priv *priv = mlx4_priv(dev);
@@ -182,57 +361,40 @@ int mlx4_qp_alloc(struct mlx4_dev *dev, 
 
 	qp->qpn = qpn;
 
-	err = mlx4_table_get(dev, &qp_table->qp_table, qp->qpn);
+	err = mlx4_qp_alloc_icm(dev, qpn);
 	if (err)
-		goto err_out;
-
-	err = mlx4_table_get(dev, &qp_table->auxc_table, qp->qpn);
-	if (err)
-		goto err_put_qp;
-
-	err = mlx4_table_get(dev, &qp_table->altc_table, qp->qpn);
-	if (err)
-		goto err_put_auxc;
-
-	err = mlx4_table_get(dev, &qp_table->rdmarc_table, qp->qpn);
-	if (err)
-		goto err_put_altc;
-
-	err = mlx4_table_get(dev, &qp_table->cmpt_table, qp->qpn);
-	if (err)
-		goto err_put_rdmarc;
+		return err;
 
 	spin_lock_irq(&qp_table->lock);
 	err = radix_tree_insert(&dev->qp_table_tree, qp->qpn & (dev->caps.num_qps - 1), qp);
 	spin_unlock_irq(&qp_table->lock);
 	if (err)
-		goto err_put_cmpt;
+		goto err_icm;
 
 	atomic_set(&qp->refcount, 1);
 	init_completion(&qp->free);
 
 	return 0;
 
-err_put_cmpt:
-	mlx4_table_put(dev, &qp_table->cmpt_table, qp->qpn);
-
-err_put_rdmarc:
-	mlx4_table_put(dev, &qp_table->rdmarc_table, qp->qpn);
-
-err_put_altc:
-	mlx4_table_put(dev, &qp_table->altc_table, qp->qpn);
-
-err_put_auxc:
-	mlx4_table_put(dev, &qp_table->auxc_table, qp->qpn);
-
-err_put_qp:
-	mlx4_table_put(dev, &qp_table->qp_table, qp->qpn);
-
-err_out:
+err_icm:
+	mlx4_qp_free_icm(dev, qpn);
 	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_qp_alloc);
 
+struct mlx4_qp *mlx4_qp_lookup_lock(struct mlx4_dev *dev, u32 qpn)
+{
+	struct mlx4_qp_table *qp_table = &mlx4_priv(dev)->qp_table;
+	unsigned long flags;
+	struct mlx4_qp *qp;
+
+	spin_lock_irqsave(&qp_table->lock, flags);
+	qp = radix_tree_lookup(&dev->qp_table_tree, qpn & (dev->caps.num_qps - 1));
+	spin_unlock_irqrestore(&qp_table->lock, flags);
+	return qp;
+}
+EXPORT_SYMBOL_GPL(mlx4_qp_lookup_lock);
+
 void mlx4_qp_remove(struct mlx4_dev *dev, struct mlx4_qp *qp)
 {
 	struct mlx4_qp_table *qp_table = &mlx4_priv(dev)->qp_table;
@@ -246,24 +408,29 @@ EXPORT_SYMBOL_GPL(mlx4_qp_remove);
 
 void mlx4_qp_free(struct mlx4_dev *dev, struct mlx4_qp *qp)
 {
-	struct mlx4_qp_table *qp_table = &mlx4_priv(dev)->qp_table;
-
 	if (atomic_dec_and_test(&qp->refcount))
 		complete(&qp->free);
 	wait_for_completion(&qp->free);
 
-	mlx4_table_put(dev, &qp_table->cmpt_table, qp->qpn);
-	mlx4_table_put(dev, &qp_table->rdmarc_table, qp->qpn);
-	mlx4_table_put(dev, &qp_table->altc_table, qp->qpn);
-	mlx4_table_put(dev, &qp_table->auxc_table, qp->qpn);
-	mlx4_table_put(dev, &qp_table->qp_table, qp->qpn);
+	mlx4_qp_free_icm(dev, qp->qpn);
 }
 EXPORT_SYMBOL_GPL(mlx4_qp_free);
 
 static int mlx4_CONF_SPECIAL_QP(struct mlx4_dev *dev, u32 base_qpn)
 {
-	return mlx4_cmd(dev, 0, base_qpn, 0, MLX4_CMD_CONF_SPECIAL_QP,
-			MLX4_CMD_TIME_CLASS_B);
+	return mlx4_cmd(dev, 0, base_qpn,
+			(dev->caps.flags & MLX4_DEV_CAP_FLAG_RAW_ETY) ? 4 : 0,
+			MLX4_CMD_CONF_SPECIAL_QP, MLX4_CMD_TIME_CLASS_B);
+}
+
+int mlx4_CONF_SPECIAL_QP_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						       struct mlx4_cmd_mailbox *inbox,
+						       struct mlx4_cmd_mailbox *outbox)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	priv->mfunc.master.slave_state[slave].sqp_start = vhcr->in_modifier & 0xffffff;
+	return 0;
 }
 
 int mlx4_init_qp_table(struct mlx4_dev *dev)
@@ -274,15 +441,37 @@ int mlx4_init_qp_table(struct mlx4_dev *
 
 	spin_lock_init(&qp_table->lock);
 	INIT_RADIX_TREE(&dev->qp_table_tree, GFP_ATOMIC);
+	if (mlx4_is_slave(dev)) {
+		/* For each slave, just allocate a normal 8-byte alligned special-QP
+		 * range intead of mlx4_init_qp_table() reservation */
+		err = mlx4_qp_reserve_range(dev, 8, 8, &dev->caps.sqp_start, 0);
+		if (err) {
+			mlx4_err(dev, "Failed to allocate special QP range\n");
+			return err;
+		}
+
+		err = mlx4_CONF_SPECIAL_QP(dev, dev->caps.sqp_start);
+		if (err) {
+			mlx4_err(dev, "Failed to configure special QP range\n");
+			mlx4_qp_release_range(dev, dev->caps.sqp_start, 8);
+			return err;
+		}
+		return 0;
+	}
 
 	/*
 	 * We reserve 2 extra QPs per port for the special QPs.  The
 	 * block of special QPs must be aligned to a multiple of 8, so
 	 * round up.
+	 * We also reserve the MSB of the 24-bit QP number to indicate
+	 * an XRC qp.
 	 */
 	dev->caps.sqp_start =
 		ALIGN(dev->caps.reserved_qps_cnt[MLX4_QP_REGION_FW], 8);
 
+	/* If multi-function is enabled, we reserve an additional QP for qp0/1 tunneling. */
+	dev->caps.tunnel_qpn = mlx4_is_master(dev) ? dev->caps.sqp_start + 8 : 0;
+
 	{
 		int sort[MLX4_NUM_QP_REGION];
 		int i, j, tmp;
@@ -312,8 +501,8 @@ int mlx4_init_qp_table(struct mlx4_dev *
 	}
 
 	err = mlx4_bitmap_init(&qp_table->bitmap, dev->caps.num_qps,
-			       (1 << 23) - 1, dev->caps.sqp_start + 8,
-			       reserved_from_top);
+			       (1 << 23) - 1, dev->caps.sqp_start + 8 +
+			       2 * !!dev->caps.tunnel_qpn, reserved_from_top);
 	if (err)
 		return err;
 
@@ -323,9 +512,26 @@ int mlx4_init_qp_table(struct mlx4_dev *
 void mlx4_cleanup_qp_table(struct mlx4_dev *dev)
 {
 	mlx4_CONF_SPECIAL_QP(dev, 0);
+	if (mlx4_is_slave(dev)) {
+		mlx4_qp_release_range(dev, dev->caps.sqp_start, 8);
+		return;
+	}
 	mlx4_bitmap_cleanup(&mlx4_priv(dev)->qp_table.bitmap);
 }
 
+int mlx4_qp_get_region(struct mlx4_dev *dev, enum mlx4_qp_region region,
+			int *base_qpn, int *cnt)
+{
+	if ((region < 0) || (region >= MLX4_NUM_QP_REGION))
+		return -EINVAL;
+
+	*base_qpn = dev->caps.reserved_qps_base[region];
+	*cnt = dev->caps.reserved_qps_cnt[region];
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mlx4_qp_get_region);
+
 int mlx4_qp_query(struct mlx4_dev *dev, struct mlx4_qp *qp,
 		  struct mlx4_qp_context *context)
 {
diff -r c23d1fc7e422 drivers/net/mlx4/reset.c
--- a/drivers/net/mlx4/reset.c
+++ b/drivers/net/mlx4/reset.c
@@ -31,6 +31,7 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
 #include <linux/errno.h>
 #include <linux/pci.h>
 #include <linux/delay.h>
@@ -39,6 +40,42 @@
 
 #include "mlx4.h"
 
+
+#define MLX4_OWNER_BASE	0x8069c
+#define MLX4_OWNER_SIZE	4
+
+int mlx4_get_ownership(struct mlx4_dev *dev)
+{
+	void __iomem *owner;
+	u32 ret;
+
+	owner = ioremap(pci_resource_start(dev->pdev, 0) + MLX4_OWNER_BASE,
+			MLX4_OWNER_SIZE);
+	if (!owner) {
+		mlx4_err(dev, "Failed to obtain ownership bit\n");
+		return -ENOMEM;
+	}
+
+	ret = readl(owner);
+	iounmap(owner);
+	return (int) !!ret;
+}
+
+void mlx4_free_ownership(struct mlx4_dev *dev)
+{
+	void __iomem *owner;
+
+	owner = ioremap(pci_resource_start(dev->pdev, 0) + MLX4_OWNER_BASE,
+			MLX4_OWNER_SIZE);
+	if (!owner) {
+		mlx4_err(dev, "Failed to obtain ownership bit\n");
+		return;
+	}
+	writel(0, owner);
+	msleep(1000);
+	iounmap(owner);
+}
+
 int mlx4_reset(struct mlx4_dev *dev)
 {
 	void __iomem *reset;
@@ -119,8 +156,8 @@ int mlx4_reset(struct mlx4_dev *dev)
 	writel(MLX4_RESET_VALUE, reset + MLX4_RESET_OFFSET);
 	iounmap(reset);
 
-	/* Docs say to wait one second before accessing device */
-	msleep(1000);
+	/* wait half a second before accessing device */
+	msleep(500);
 
 	end = jiffies + MLX4_RESET_TIMEOUT_JIFFIES;
 	do {
diff -r c23d1fc7e422 drivers/net/mlx4/resource_tracker.c
--- /dev/null
+++ b/drivers/net/mlx4/resource_tracker.c
@@ -0,0 +1,799 @@
+/*
+ * Copyright (c) 2004, 2005 Topspin Communications.  All rights reserved.
+ * Copyright (c) 2005, 2006, 2007, 2008 Mellanox Technologies. All rights reserved.
+ * Copyright (c) 2005, 2006, 2007 Cisco Systems, Inc.  All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/sched.h>
+#include <linux/pci.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <asm/io.h>
+
+#include "mlx4.h"
+#include "fw.h"
+
+/* resource tracker functions*/
+static inline int verify_rt_resource_typ(int resource_type)
+{
+	return ((resource_type < MLX4_NUM_OF_RESOURCE_TYPE) && (resource_type >= 0)) ? 0 : -1 ;
+}
+
+static inline int verify_rt_slave_id(struct mlx4_dev *dev, int slave_id)
+{
+	return ((slave_id < dev->num_slaves) && (slave_id >= 0)) ? 0 : -1 ;
+}
+
+/* For Debug uses */
+char *ResourceType(enum mlx4_resource rt)
+{
+	switch (rt) {
+	case RES_QP: return "RES_QP";
+	case RES_CQ: return "RES_CQ";
+	case RES_SRQ: return "RES_SRQ";
+	case RES_MPT: return "RES_MPT";
+	case RES_MTT: return "RES_MTT";
+	case RES_MAC: return  "RES_MAC";
+	case RES_VLAN: return "RES_VLAN";
+	case RES_MCAST: return "RES_MCAST";
+	default: return "";
+	};
+}
+
+void dump_resources(struct mlx4_dev *dev, int slave_id, enum mlx4_resource res_type)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_tracked_resource *tracked_res;
+	mlx4_dbg(dev, "\n*************************************\n"
+		      "Dump resources:%d for Salve:%d \n",
+		  res_type, slave_id);
+	list_for_each_entry(tracked_res, &priv->mfunc.master.res_tracker.res_list[slave_id], list) {
+		if (res_type == tracked_res->res_type)
+			mlx4_dbg(dev, "* resource id: %d\n", tracked_res->resource_id);
+	}
+	mlx4_dbg(dev, "\n*************************************\n");
+}
+
+int mlx4_init_resource_tracker(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int i;
+
+	priv->mfunc.master.res_tracker.res_list = kzalloc(dev->num_slaves *
+							   sizeof (struct list_head), GFP_KERNEL);
+	if (!priv->mfunc.master.res_tracker.res_list)
+		return -ENOMEM;
+
+	for (i = 0 ; i < dev->num_slaves; i++)
+		INIT_LIST_HEAD(&priv->mfunc.master.res_tracker.res_list[i]);
+
+	mlx4_dbg(dev, "Started init_resource_tracker: %ld slaves \n", dev->num_slaves);
+	for (i = 0 ; i < MLX4_NUM_OF_RESOURCE_TYPE; i++)
+		INIT_RADIX_TREE(&priv->mfunc.master.res_tracker.res_tree[i],
+	 GFP_ATOMIC|__GFP_NOWARN);
+
+	spin_lock_init(&priv->mfunc.master.res_tracker.lock);
+	return 0 ;
+}
+
+void mlx4_free_resource_tracker(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int i;
+	if (priv->mfunc.master.res_tracker.res_list) {
+		for (i = 0 ; i < dev->num_slaves; i++)
+			mlx4_delete_all_resources_for_slave(dev, i);
+
+		kfree(priv->mfunc.master.res_tracker.res_list);
+	}
+	priv->mfunc.master.res_tracker.res_list = NULL;
+}
+
+
+int mlx4_get_slave_from_resource_id(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				     int resource_id, int *slave)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_tracked_resource *rt;
+
+	if (verify_rt_resource_typ(resource_type)) {
+			mlx4_warn(dev, "mlx4_get_slave_from_resource_id,"
+				  "res_type(%d) is not valid\n",
+				   resource_type);
+			return -EINVAL;
+	}
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	rt = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[resource_type],
+			       resource_id);
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	if (NULL == rt) {
+		/*
+		mlx4_dbg(dev, "mlx4_get_slave_from_resource_id radix_tree_lookup "
+			 "(res_type:%d) FAILED for resource_id: %d\n",
+				 resource_type, resource_id);
+		*/
+		return -ENOENT;
+	}
+	*slave = rt->slave_id;
+	return 0 ;
+}
+
+/*
+There are resources that allocated in 2 steps, the first is the reservation,
+the second is the allocation.
+In order to track after the state of the resource the structure needs to
+save the reservation status,
+it should be one of the next list:
+	RES_INIT
+	RES_RESERVED
+	RES_ALLOCATED
+	RES_ALLOCATED_AFTER_RESERVATION
+The tegular resources (the kind that has one step of allocation will get RES_INIT
+when they tracked (created).
+the rest will have the relevant steps.)
+*/
+int update_resource_reservation_status(struct mlx4_dev *dev, int resource_type,
+				       int slave_id, int resource_id,
+				       enum mlx4_resource_state state)
+{
+	int ret = 0;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_tracked_resource *tracked_res;
+
+	if ((verify_rt_resource_typ(resource_type)) || verify_rt_slave_id(dev, slave_id))
+		return -EINVAL;
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[resource_type],
+					 resource_id);
+	if (!tracked_res) {
+		mlx4_err(dev, "Failed to find resource type: %d id: %d state: %d\n",
+			  resource_type, resource_id, state);
+		ret =  -ENOENT;
+		goto exit;
+	}
+	set_bit(state, &tracked_res->state) ;
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+int mlx4_add_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				 int slave_id, int resource_id, unsigned long state)
+{
+	int ret = 0;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_tracked_resource *tracked_res;
+	struct mlx4_tracked_resource *qp_tracked_res;
+	int tmp_slave_id;
+	mlx4_dbg(dev, "mlx4_add_resource_for_slave: slave:%d, res:%d res_id:%d\n",
+		  slave_id, resource_type, resource_id);
+	if ((verify_rt_resource_typ(resource_type)) || verify_rt_slave_id(dev, slave_id))
+		return -EINVAL;
+
+	/* if the resource is qp or mtt it allready was created in the reservationstage,
+	 so don't need to allocat it, only to change the status of the resrevation.
+	*/
+	if (RES_ALLOCATED_AFTER_RESERVATION == state) {
+		ret = update_resource_reservation_status(dev, resource_type, slave_id,
+							  resource_id, state);
+		/*Only for this state we return here.*/
+		return ret ;
+	}
+
+	/*Sanity check that the object does not exist*/
+	ret = mlx4_get_slave_from_resource_id(dev, resource_type, resource_id,
+					       &tmp_slave_id);
+	if (0 == ret) {
+		if (RES_QP == resource_type) {
+			/*check if it is the case where the master reserverd qp
+			 for the slave, and now the slave allocates it.*/
+			spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+			/*check the status first.*/
+			qp_tracked_res =
+				radix_tree_lookup(
+					&priv->mfunc.master.res_tracker.res_tree[resource_type],
+					 resource_id
+					);
+			spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+			if (qp_tracked_res) {
+			/*check the state:*/
+				if (test_bit(RES_ALLOCATED_WITH_MASTER_RESERVATION,
+					      &qp_tracked_res->state)) {
+					ret = update_resource_reservation_status(dev,
+										 resource_type,
+										 slave_id,
+										 resource_id,
+										 state);
+					mlx4_dbg(dev, "Resource(%d:%d) was created"
+						 "for slave:%d by master\n",
+						 resource_type, resource_id, slave_id);
+					return ret;
+				}
+			}
+		}
+		mlx4_err(dev, "mlx4_add_resource_for_slave Adding already exists object,"
+			 " slave:%d res_type:"
+			" %d res_id:%d\n", slave_id, resource_type, resource_id);
+		return -EEXIST;
+	}
+	tracked_res = kzalloc(sizeof (struct mlx4_tracked_resource), GFP_KERNEL);
+	if (!tracked_res)
+		return -ENOMEM;
+
+	tracked_res->slave_id = slave_id ;
+	tracked_res->res_type = resource_type ;
+	tracked_res->resource_id = resource_id ;
+	set_bit(state, &tracked_res->state) ;
+
+	/*if it is qp create list for the mcg */
+	if ((RES_QP == resource_type))
+		INIT_LIST_HEAD(&tracked_res->specific_data.mcg_list);
+
+	/* add it to the specific tree and to the list */
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	ret = radix_tree_insert(&priv->mfunc.master.res_tracker.res_tree[resource_type],
+				 resource_id, tracked_res);
+	if (ret) {
+		/*-EEXIST or -ENOMEM */
+		mlx4_dbg(dev, "mlx4_add_resource_for_slave: Failed to add resource"
+			 " slave:%d, res:%d res_id:%d\n",
+			  slave_id, resource_type, resource_id);
+		kfree(tracked_res);
+		goto exit;
+	}
+	list_add(&tracked_res->list, &priv->mfunc.master.res_tracker.res_list[slave_id]);
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+
+int mlx4_add_mpt_resource_for_slave(struct mlx4_dev *dev,
+				    enum mlx4_resource resource_type, int slave_id,
+				    int resource_id, unsigned long state)
+{
+	return mlx4_add_resource_for_slave(dev, resource_type, slave_id,
+					   (resource_id & (dev->caps.num_mpts - 1)),
+					    state);
+}
+/* The function cleans the hw resources that were allocated for this resource,
+   Assume that calling this function only in cases that the slave is going down
+   and have open resources allocated by it.
+*/
+void mlx4_clean_specific_hw_resource(struct mlx4_dev *dev,
+				      struct mlx4_tracked_resource *tracked_res)
+{
+	switch (tracked_res->res_type) {
+	case RES_QP: {
+			struct mlx_tracked_qp_mcg *tracked_mcg;
+			struct mlx_tracked_qp_mcg *tmp_tracked_mcg;
+			struct mlx4_qp qp; /* dummy for calling attach/detach */
+			qp.qpn = tracked_res->resource_id;
+			if (test_bit(RES_ALLOCATED_AFTER_RESERVATION, &tracked_res->state)) {
+					mlx4_qp_free_icm(dev, tracked_res->resource_id);
+				/* free all the mcg that the qp attached to*/
+				list_for_each_entry_safe(tracked_mcg,
+							  tmp_tracked_mcg,
+							  &tracked_res->specific_data.mcg_list,
+							  list) {
+					mlx4_qp_detach_common(dev, &qp, tracked_mcg->gid,
+							      tracked_mcg->prot, MLX4_MC_STEER, 0);
+				}
+			} else {
+				/*If the master reserved no need to free reservation*/
+				if (!test_bit(RES_ALLOCATED_WITH_MASTER_RESERVATION,
+					       &tracked_res->state))
+					mlx4_qp_release_range(dev, tracked_res->resource_id, 1);
+			}
+			break;
+		}
+	case RES_MPT:
+		if (test_bit(RES_ALLOCATED_AFTER_RESERVATION, &tracked_res->state)) {
+			mlx4_mr_free_icm(dev, tracked_res->resource_id);
+		} else {
+			mlx4_mr_release(dev, tracked_res->resource_id);
+		}
+		break;
+	case RES_CQ:
+		mlx4_cq_free_icm(dev, tracked_res->resource_id);
+		break;
+	case RES_SRQ:
+		mlx4_srq_free_icm(dev, tracked_res->resource_id);
+		break;
+	case RES_MTT:
+		mlx4_free_mtt_range(dev, tracked_res->resource_id,
+				     tracked_res->specific_data.order);
+		break;
+	case RES_MAC:
+		mlx4_unregister_mac(dev, tracked_res->specific_data.port,
+				     tracked_res->resource_id) ;
+		break;
+	case RES_VLAN: {
+			/*clear all the bits (vlans) of this slave*/
+			struct mlx4_vlan_fltr vlan_fltr ;
+			memset(&vlan_fltr, 0, sizeof(struct mlx4_vlan_fltr));
+			mlx4_common_set_vlan_fltr(dev, tracked_res->slave_id,
+						  tracked_res->specific_data.port, &vlan_fltr);
+		}
+		break;
+	default:
+		break;
+	}
+}
+
+/* The function removes resources from the resource_tracking database only.
+(not cleaning the from the hw)*/
+void mlx4_delete_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				    int slave_id, int resource_id)
+{
+	struct mlx4_tracked_resource *tracked_res = NULL ;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	int ret = 0 ;
+
+	if ((verify_rt_resource_typ(resource_type)) || verify_rt_slave_id(dev, slave_id)) {
+		mlx4_err(dev, "mlx4_delete_resource_for_slave, error input,"
+			      " slave:%d res_type: %d res_id:%d\n",
+			 slave_id, resource_type, resource_id);
+		return ;
+	}
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	/*check the status first.*/
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[resource_type],
+					 resource_id);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	/* if we are in qp or mtt the resource is not deleted till the release of
+	   the reservation so only needs to change the status of the allocation
+	   to became false
+	*/
+	if (test_bit(RES_ALLOCATED_AFTER_RESERVATION, &tracked_res->state)) {
+		clear_bit(RES_ALLOCATED_AFTER_RESERVATION, &tracked_res->state);
+		goto exit;
+	}
+
+	mlx4_dbg(dev, "mlx4_delete_resource_for_slave slave:%d res_type: %d res_id:%d\n",
+		 slave_id, resource_type, resource_id);
+
+	tracked_res = radix_tree_delete(&priv->mfunc.master.res_tracker.res_tree[resource_type],
+					 resource_id);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	list_del(&tracked_res->list);
+	kfree(tracked_res);
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	if (ret) {
+		mlx4_err(dev, "mlx4_delete_resource_for_slave,"
+			      " error deleting, slave:%d res_type: %d res_id:%d"
+			      " return: %d\n",
+			 slave_id, resource_type, resource_id, ret);
+	}
+}
+
+void mlx4_delete_specific_res_type_for_slave(struct mlx4_dev *dev, int slave_id,
+					     enum mlx4_resource resource_type)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	struct mlx4_tracked_resource *tracked_res;
+	struct mlx4_tracked_resource *tmp_rt;
+	mlx4_dbg(dev, "mlx4_delete_specific_res_type_for_slave: Go over"
+		      " slave:%d res_type: %d \n",
+		  slave_id, resource_type);
+/*TODO: replace the list head to avoid race on the the same list.*/
+	list_for_each_entry_safe(tracked_res, tmp_rt, &priv->mfunc.master.res_tracker.res_list[slave_id], list) {
+		mlx4_dbg(dev, "mlx4_delete_all_resources_for_slave: Go over slave:%d"
+			      " id: %d \n", tracked_res->slave_id,
+			 tracked_res->resource_id);
+		if (tracked_res->res_type == resource_type) {
+			/* clean the hw*/
+			mlx4_clean_specific_hw_resource(dev, tracked_res);
+			/*clean the resource_tracker*/
+			mlx4_delete_resource_for_slave(dev, tracked_res->res_type,
+						       tracked_res->slave_id, tracked_res->resource_id);
+		}
+	}
+}
+
+/* The function is calling when you want to clean all resources that were occupied for specific slave.
+   The function does 2 things:
+				1. cleans the resources from the hw.
+				2. cleans the resource tracker db.
+	Pay attention: the order of the releasing is like in the ibvers, e.g :
+		QP->SRQ->CQ-> MTT-> MPT
+*/
+void mlx4_delete_all_resources_for_slave(struct mlx4_dev *dev, int slave_id)
+{
+	mlx4_dbg(dev, "1. mlx4_delete_all_resources_for_slave:%d\n", slave_id);
+	if (verify_rt_slave_id(dev, slave_id)) {
+		mlx4_err(dev, "mlx4_delete_all_resources_for_slave: error input,"
+			      " slave:%d \n", slave_id);
+		return;
+	}
+	/*VLAN*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_VLAN);
+	/* MAC */
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_MAC);
+	/* QP: allocation */
+	/*dump_resources(dev, slave_id, RES_QP);*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_QP);
+	/*QP: reservation*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_QP);
+	/*SRQ*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_SRQ);
+	/*CQ*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_CQ);
+	/*MTT*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_MTT);
+	/*MPT : allocation*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_MPT);
+	/*MPT: reservation*/
+	mlx4_delete_specific_res_type_for_slave(dev, slave_id, RES_MPT);
+
+}
+
+int mlx4_add_range_resource_for_slave(struct mlx4_dev *dev, enum mlx4_resource resource_type,
+				       int slave_id, int from, int cnt)
+{
+	int i;
+	int ret = 0;
+	for (i = from; i < (from + cnt); i++) {
+		ret |= mlx4_add_resource_for_slave(dev, resource_type, slave_id, i, RES_INIT);
+	}
+	return ret ;
+}
+
+void mlx4_delete_range_resource_for_slave(struct mlx4_dev *dev,
+					  enum mlx4_resource resource_type, int slave_id, int from, int cnt)
+{
+	int i;
+	for (i = from; i < (from + cnt); i++) {
+		mlx4_delete_resource_for_slave(dev, resource_type, slave_id, i);
+	}
+}
+
+int mlx4_add_mtt_resource_for_slave(struct mlx4_dev *dev,
+				    int slave_id, int resource_id,
+				    unsigned long state, int order)
+{
+	int ret = 0 ;
+	struct mlx4_tracked_resource *tracked_res;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	ret = mlx4_add_resource_for_slave(dev, RES_MTT, slave_id,
+					   resource_id,
+					    state);
+	if (ret) {
+		mlx4_info(dev, "ERROR: mlx4_add_mtt_resource_for_slave failed: ret = %d \n",
+			ret);
+		return ret;
+	}
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_MTT], resource_id);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	tracked_res->specific_data.order = order ;
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+int mlx4_add_mcg_to_tracked_qp(struct mlx4_dev *dev, int qpn, u8* gid, enum mlx4_protocol prot)
+{
+	struct mlx_tracked_qp_mcg *new_qp_in_mcg ;
+	struct mlx4_tracked_resource *tracked_res;
+	int ret = 0 ;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	new_qp_in_mcg = kzalloc(sizeof (struct mlx_tracked_qp_mcg), GFP_KERNEL);
+	if (!new_qp_in_mcg)
+		return -ENOMEM;
+
+	memcpy(new_qp_in_mcg->gid, gid, GID_SIZE);
+	new_qp_in_mcg->prot = prot;
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_QP], qpn);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		kfree(new_qp_in_mcg);
+		goto exit;
+	}
+	mlx4_dbg(dev, "mlx4_add_mcg_to_tracked_qp: added new mcg to qpn: %d \n", qpn);
+	list_add(&new_qp_in_mcg->list, &tracked_res->specific_data.mcg_list);
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+int mlx4_remove_mcg_from_tracked_qp(struct mlx4_dev *dev, int qpn, u8* gid)
+{
+	struct mlx4_tracked_resource *tracked_res;
+	struct mlx_tracked_qp_mcg *tracked_mcg;
+	struct mlx_tracked_qp_mcg *tmp_tracked_mcg;
+	int ret = -ENXIO ;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_QP],
+					 qpn);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	mlx4_dbg(dev, "mlx4_remove_mcg_from_tracked_qp: delete"
+		      " mcg from qpn: %d \n", qpn);
+	list_for_each_entry_safe(tracked_mcg, tmp_tracked_mcg,
+				  &tracked_res->specific_data.mcg_list, list) {
+		if (!memcmp(tracked_mcg->gid, gid, GID_SIZE)) {
+			list_del(&tracked_mcg->list);
+			kfree(tracked_mcg);
+			tracked_mcg = NULL;
+			ret = 0 ;
+			break;
+		}
+	}
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+int mlx4_add_port_to_tracked_mac(struct mlx4_dev *dev, int qpn, u8 port)
+{
+	struct mlx4_tracked_resource *tracked_res;
+	int ret = 0 ;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_MAC], qpn);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	mlx4_dbg(dev, "mlx4_add_port_to_tracked_macmlx4_add_mcg_to_tracked_qp:"
+		      " added port:%d in key(qpn): %d \n",
+		 port, qpn);
+
+	tracked_res->specific_data.port = port ;
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+/* 	The function assumes that there is only one filter per slave
+	so, it tries first to remove the resource if already exists,
+	and after that add new resource,
+	and to this resource attaches the (one and only)new filter.*/
+int mlx4_add_vlan_fltr_to_tracked_slave(struct mlx4_dev *dev, int slave_id,  int port)
+{
+	struct mlx4_tracked_resource *tracked_res;
+	int ret = 0 ;
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	bool vlan_exists = false;
+
+	if (verify_rt_slave_id(dev, slave_id))
+		return -EINVAL;
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	if (radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_VLAN], slave_id))
+		vlan_exists = true;
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+
+	if (vlan_exists)
+		mlx4_delete_resource_for_slave(dev, RES_VLAN, slave_id, slave_id);
+
+	ret = mlx4_add_resource_for_slave(dev, RES_VLAN, slave_id,
+					  slave_id/*the slave is also the key*/, RES_INIT);
+	if (0 != ret) {
+		mlx4_err(dev, "mlx4_add_vlan_fltr_to_tracked_slave: Failed to "
+			      "add new resource key(slave_id): %d \n", slave_id);
+		return ret;
+	}
+
+	spin_lock_irq(&priv->mfunc.master.res_tracker.lock);
+	tracked_res = radix_tree_lookup(&priv->mfunc.master.res_tracker.res_tree[RES_VLAN], slave_id);
+	if (!tracked_res) {
+		ret =  -ENOENT;
+		goto exit;
+	}
+	mlx4_dbg(dev, "mlx4_add_vlan_fltr_to_tracked_slave: added filter"
+		      "  key(slave_id): %d \n", slave_id);
+	tracked_res->specific_data.port = port ;
+
+exit:
+	spin_unlock_irq(&priv->mfunc.master.res_tracker.lock);
+	return ret ;
+}
+
+int verify_resource_belong_to_slave(struct mlx4_dev *dev, int slave,
+				    enum mlx4_resource resource_type, int resource_id)
+{
+	int slave_from_db = -1;
+	int ret;
+	ret =  mlx4_get_slave_from_resource_id(dev, resource_type, resource_id, &slave_from_db) ;
+	if ((0 == ret) && (slave_from_db == slave)) {
+		mlx4_dbg(dev, "Verify resource:%d from type: %d,"
+			      " belong to slave: %d\n",
+			  resource_id, resource_type, slave);
+		return 0 ;
+	}
+
+	mlx4_err(dev, "Verify resource:%d from type: %d,"
+		      " DOES NOT belong to slave: %d\n",
+		  resource_id, resource_type, slave);
+	return -ENOENT;
+}
+
+int mlx4_verify_mpt_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox)
+{
+	u32 bit_mask;
+	/*dump_resources(dev, slave, RES_MPT); */
+	bit_mask = calculate_bitmap_mask(dev, RES_MPT);
+	return verify_resource_belong_to_slave(dev, slave, RES_MPT,
+					       (vhcr->in_modifier & bit_mask));
+}
+
+int mlx4_verify_cq_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox)
+{
+	return verify_resource_belong_to_slave(dev, slave, RES_CQ, vhcr->in_modifier);
+}
+
+int mlx4_verify_srq_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox)
+{
+	return verify_resource_belong_to_slave(dev, slave, RES_SRQ, vhcr->in_modifier);
+}
+
+int mlx4_verify_qp_index(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+			struct mlx4_cmd_mailbox *inbox)
+{
+	dump_resources(dev, slave, RES_QP);
+	return verify_resource_belong_to_slave(dev, slave, RES_QP, (vhcr->in_modifier &
+						0xffffff/*(dev->caps.num_qps - 1)*/));
+}
+
+int mlx4_verify_srq_aram(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+			struct mlx4_cmd_mailbox *inbox)
+{
+	if (test_bit(31, (const unsigned long *)&vhcr->in_modifier)) /*RQ*/
+		return verify_resource_belong_to_slave(dev, slave, RES_QP,
+			(vhcr->in_modifier & 0xffffff/*(dev->caps.num_qps - 1)*/));
+	/*SRQ*/
+	return verify_resource_belong_to_slave(dev, slave, RES_SRQ,
+					       (vhcr->in_modifier & 0xffffff));
+}
+/*
+	The function checks which slave can asks for each resource,
+	currently the relevant resources are QP and MTT, where there are 2 stages of allocation
+	The reservation and the icm_allocation
+*/
+int mlx4_verify_resource_wrapper(struct mlx4_dev *dev, int slave, struct mlx4_vhcr *vhcr,
+						  struct mlx4_cmd_mailbox *inbox)
+{
+	u32 param1 = *((u32 *) &vhcr->in_param);
+	u32 param2 = *(((u32 *) &vhcr->in_param) + 1);
+	int i ;
+	int ret = 0 ;
+	switch (vhcr->in_modifier) {
+	case RES_QP:
+		switch (vhcr->op_modifier) {
+		case ICM_RESERVE:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+				/*nothing to do*/
+				return 0;
+			}
+			/*check this is its qp that it wants to free */
+			for (i = 0 ; i < param2; i++) {
+				ret = verify_resource_belong_to_slave(dev,
+								       slave,
+								       RES_QP,
+								       param1 + i);
+				if (ret)
+					return ret;
+			}
+			return 0 ;
+		case ICM_ALLOC:
+			/*check this is its qp*/
+			return verify_resource_belong_to_slave(dev, slave, RES_QP, param1);
+		default:
+			mlx4_err(dev, "mlx4_verify_resource_wrapper:(QP) Got unknown"
+				"op_modifier:%d for slave:%d.\n",
+				vhcr->op_modifier, slave);
+			break;
+		}
+		break;
+	case RES_MPT:
+		switch (vhcr->op_modifier) {
+		case ICM_RESERVE:
+			if (vhcr->op == MLX4_CMD_ALLOC_RES) {
+				/*nothing to do*/
+				return 0;
+			}
+			/*check this is its mpt*/
+			return verify_resource_belong_to_slave(dev,
+							       slave,
+							       RES_MPT,
+							       param1 & calculate_bitmap_mask(dev, RES_MPT));
+
+		case ICM_ALLOC:
+			/*check this is its mpt*/
+			return verify_resource_belong_to_slave(dev,
+							       slave,
+							       RES_MPT,
+							       param1 & calculate_bitmap_mask(dev, RES_MPT));
+
+		default:
+			mlx4_err(dev, "mlx4_verify_resource_wrapper:(MPT) Got unknown"
+				"op_modifier:%d for slave:%d.\n",
+				vhcr->op_modifier, slave);
+			break;
+		}
+		break;
+	default:
+		break;
+	}
+	return 0;
+}
+/*Ruturns the mask according to specific bitmap allocator*/
+u32 calculate_bitmap_mask(struct mlx4_dev *dev, enum mlx4_resource resource_type)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+	u32 ret = ~0;
+	switch (resource_type) {
+	case RES_QP:
+		/*ret = priv->qp_table.bitmap.max + priv->qp_table.bitmap.reserved_top - 1 ;*/
+		ret = (dev->caps.num_qps - 1);
+		break;
+	case RES_MPT:
+		ret = priv->mr_table.mpt_bitmap.max + priv->mr_table.mpt_bitmap.reserved_top - 1 ;
+		break;
+	default:
+		mlx4_warn(dev, "calculate_bitmap_mask: Unknown type, check it...\n");
+		break;
+	}
+	return ret;
+}
+
diff -r c23d1fc7e422 drivers/net/mlx4/srq.c
--- a/drivers/net/mlx4/srq.c
+++ b/drivers/net/mlx4/srq.c
@@ -31,6 +31,8 @@
  * SOFTWARE.
  */
 
+#include <linux/init.h>
+
 #include <linux/mlx4/cmd.h>
 
 #include "mlx4.h"
@@ -39,20 +41,20 @@
 struct mlx4_srq_context {
 	__be32			state_logsize_srqn;
 	u8			logstride;
-	u8			reserved1[3];
-	u8			pg_offset;
-	u8			reserved2[3];
-	u32			reserved3;
+	u8			reserved1;
+	__be16			xrc_domain;
+	__be32			pg_offset_cqn;
+	u32			reserved2;
 	u8			log_page_size;
-	u8			reserved4[2];
+	u8			reserved3[2];
 	u8			mtt_base_addr_h;
 	__be32			mtt_base_addr_l;
 	__be32			pd;
 	__be16			limit_watermark;
 	__be16			wqe_cnt;
-	u16			reserved5;
+	u16			reserved4;
 	__be16			wqe_counter;
-	u32			reserved6;
+	u32			reserved5;
 	__be64			db_rec_addr;
 };
 
@@ -63,7 +65,8 @@ void mlx4_srq_event(struct mlx4_dev *dev
 
 	spin_lock(&srq_table->lock);
 
-	srq = radix_tree_lookup(&srq_table->tree, srqn & (dev->caps.num_srqs - 1));
+	srq = radix_tree_lookup(&dev->srq_table_tree,
+				srqn & (dev->caps.num_srqs - 1));
 	if (srq)
 		atomic_inc(&srq->refcount);
 
@@ -83,8 +86,8 @@ void mlx4_srq_event(struct mlx4_dev *dev
 static int mlx4_SW2HW_SRQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
 			  int srq_num)
 {
-	return mlx4_cmd(dev, mailbox->dma, srq_num, 0, MLX4_CMD_SW2HW_SRQ,
-			MLX4_CMD_TIME_CLASS_A);
+	return mlx4_cmd(dev, mailbox->dma | dev->caps.function, srq_num, 0,
+			MLX4_CMD_SW2HW_SRQ, MLX4_CMD_TIME_CLASS_A);
 }
 
 static int mlx4_HW2SW_SRQ(struct mlx4_dev *dev, struct mlx4_cmd_mailbox *mailbox,
@@ -108,8 +111,70 @@ static int mlx4_QUERY_SRQ(struct mlx4_de
 			    MLX4_CMD_TIME_CLASS_A);
 }
 
-int mlx4_srq_alloc(struct mlx4_dev *dev, u32 pdn, struct mlx4_mtt *mtt,
-		   u64 db_rec, struct mlx4_srq *srq)
+int mlx4_srq_alloc_icm(struct mlx4_dev *dev, int *srqn)
+{
+	struct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;
+	u64 out_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		err = mlx4_cmd_imm(dev, 0, &out_param, RES_SRQ,
+				   ICM_RESERVE_AND_ALLOC,
+				   MLX4_CMD_ALLOC_RES,
+				   MLX4_CMD_TIME_CLASS_A);
+		if (err) {
+			*srqn = -1;
+			return err;
+		} else {
+			*srqn = out_param;
+			return 0;
+		}
+	}
+
+	*srqn = mlx4_bitmap_alloc(&srq_table->bitmap);
+	if (*srqn == -1)
+		return -ENOMEM;
+
+	err = mlx4_table_get(dev, &srq_table->table, *srqn);
+	if (err)
+		goto err_out;
+
+	err = mlx4_table_get(dev, &srq_table->cmpt_table, *srqn);
+	if (err)
+		goto err_put;
+	return 0;
+
+err_put:
+	mlx4_table_put(dev, &srq_table->table, *srqn);
+
+err_out:
+	mlx4_bitmap_free(&srq_table->bitmap, *srqn);
+	return err;
+}
+
+void mlx4_srq_free_icm(struct mlx4_dev *dev, int srqn)
+{
+	struct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;
+	u64 in_param;
+	int err;
+
+	if (mlx4_is_slave(dev)) {
+		*((u32 *) &in_param) = srqn;
+		*(((u32 *) &in_param) + 1) = 0;
+		err = mlx4_cmd(dev, in_param, RES_SRQ, ICM_RESERVE_AND_ALLOC,
+			       MLX4_CMD_FREE_RES,
+			       MLX4_CMD_TIME_CLASS_A);
+		if (err)
+			mlx4_warn(dev, "Failed freeing cq:%d\n", srqn);
+	} else {
+		mlx4_table_put(dev, &srq_table->cmpt_table, srqn);
+		mlx4_table_put(dev, &srq_table->table, srqn);
+		mlx4_bitmap_free(&srq_table->bitmap, srqn);
+	}
+}
+
+int mlx4_srq_alloc(struct mlx4_dev *dev, u32 pdn, u32 cqn, u16 xrcd,
+		   struct mlx4_mtt *mtt, u64 db_rec, struct mlx4_srq *srq)
 {
 	struct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;
 	struct mlx4_cmd_mailbox *mailbox;
@@ -117,23 +182,15 @@ int mlx4_srq_alloc(struct mlx4_dev *dev,
 	u64 mtt_addr;
 	int err;
 
-	srq->srqn = mlx4_bitmap_alloc(&srq_table->bitmap);
-	if (srq->srqn == -1)
-		return -ENOMEM;
-
-	err = mlx4_table_get(dev, &srq_table->table, srq->srqn);
+	err = mlx4_srq_alloc_icm(dev, &srq->srqn);
 	if (err)
-		goto err_out;
-
-	err = mlx4_table_get(dev, &srq_table->cmpt_table, srq->srqn);
-	if (err)
-		goto err_put;
+		return err;
 
 	spin_lock_irq(&srq_table->lock);
-	err = radix_tree_insert(&srq_table->tree, srq->srqn, srq);
+	err = radix_tree_insert(&dev->srq_table_tree, srq->srqn, srq);
 	spin_unlock_irq(&srq_table->lock);
 	if (err)
-		goto err_cmpt_put;
+		goto err_icm;
 
 	mailbox = mlx4_alloc_cmd_mailbox(dev);
 	if (IS_ERR(mailbox)) {
@@ -147,6 +204,8 @@ int mlx4_srq_alloc(struct mlx4_dev *dev,
 	srq_context->state_logsize_srqn = cpu_to_be32((ilog2(srq->max) << 24) |
 						      srq->srqn);
 	srq_context->logstride          = srq->wqe_shift - 4;
+	srq_context->xrc_domain		= cpu_to_be16(xrcd);
+	srq_context->pg_offset_cqn	= cpu_to_be32(cqn & 0xffffff);
 	srq_context->log_page_size      = mtt->page_shift - MLX4_ICM_PAGE_SHIFT;
 
 	mtt_addr = mlx4_mtt_addr(dev, mtt);
@@ -167,41 +226,42 @@ int mlx4_srq_alloc(struct mlx4_dev *dev,
 
 err_radix:
 	spin_lock_irq(&srq_table->lock);
-	radix_tree_delete(&srq_table->tree, srq->srqn);
+	radix_tree_delete(&dev->srq_table_tree, srq->srqn);
 	spin_unlock_irq(&srq_table->lock);
 
-err_cmpt_put:
-	mlx4_table_put(dev, &srq_table->cmpt_table, srq->srqn);
-
-err_put:
-	mlx4_table_put(dev, &srq_table->table, srq->srqn);
-
-err_out:
-	mlx4_bitmap_free(&srq_table->bitmap, srq->srqn);
-
+err_icm:
+	mlx4_srq_free_icm(dev, srq->srqn);
 	return err;
 }
 EXPORT_SYMBOL_GPL(mlx4_srq_alloc);
 
-void mlx4_srq_free(struct mlx4_dev *dev, struct mlx4_srq *srq)
+void mlx4_srq_invalidate(struct mlx4_dev *dev, struct mlx4_srq *srq)
 {
-	struct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;
 	int err;
 
 	err = mlx4_HW2SW_SRQ(dev, NULL, srq->srqn);
 	if (err)
 		mlx4_warn(dev, "HW2SW_SRQ failed (%d) for SRQN %06x\n", err, srq->srqn);
+}
+EXPORT_SYMBOL_GPL(mlx4_srq_invalidate);
+
+void mlx4_srq_remove(struct mlx4_dev *dev, struct mlx4_srq *srq)
+{
+	struct mlx4_srq_table *srq_table = &mlx4_priv(dev)->srq_table;
 
 	spin_lock_irq(&srq_table->lock);
-	radix_tree_delete(&srq_table->tree, srq->srqn);
+	radix_tree_delete(&dev->srq_table_tree, srq->srqn);
 	spin_unlock_irq(&srq_table->lock);
+}
+EXPORT_SYMBOL_GPL(mlx4_srq_remove);
 
+void mlx4_srq_free(struct mlx4_dev *dev, struct mlx4_srq *srq)
+{
 	if (atomic_dec_and_test(&srq->refcount))
 		complete(&srq->free);
 	wait_for_completion(&srq->free);
 
-	mlx4_table_put(dev, &srq_table->table, srq->srqn);
-	mlx4_bitmap_free(&srq_table->bitmap, srq->srqn);
+	mlx4_srq_free_icm(dev, srq->srqn);
 }
 EXPORT_SYMBOL_GPL(mlx4_srq_free);
 
@@ -240,7 +300,9 @@ int mlx4_init_srq_table(struct mlx4_dev 
 	int err;
 
 	spin_lock_init(&srq_table->lock);
-	INIT_RADIX_TREE(&srq_table->tree, GFP_ATOMIC);
+	INIT_RADIX_TREE(&dev->srq_table_tree, GFP_ATOMIC);
+	if (mlx4_is_slave(dev))
+		return 0;
 
 	err = mlx4_bitmap_init(&srq_table->bitmap, dev->caps.num_srqs,
 			       dev->caps.num_srqs - 1, dev->caps.reserved_srqs, 0);
@@ -252,5 +314,7 @@ int mlx4_init_srq_table(struct mlx4_dev 
 
 void mlx4_cleanup_srq_table(struct mlx4_dev *dev)
 {
+	if (mlx4_is_slave(dev))
+		return;
 	mlx4_bitmap_cleanup(&mlx4_priv(dev)->srq_table.bitmap);
 }
diff -r c23d1fc7e422 drivers/net/mlx4/sys_tune.c
--- /dev/null
+++ b/drivers/net/mlx4/sys_tune.c
@@ -0,0 +1,328 @@
+/*
+ * Copyright (c) 2010 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#include <linux/sched.h>
+#include <linux/version.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/percpu.h>
+#include <linux/mutex.h>
+#include <linux/sysdev.h>
+#include <asm/atomic.h>
+#include <linux/node.h>
+#include "mlx4_en.h"
+
+#ifdef CONFIG_X86
+
+/* Each CPU is put into a group.  In most cases, the group number is
+ * equal to the CPU number of one of the CPUs in the group.  The
+ * exception is group NR_CPUS which is the default group.  This is
+ * protected by sys_tune_startup_mutex. */
+DEFINE_PER_CPU(int, idle_cpu_group) = NR_CPUS;
+
+/* For each group, a count of the number of CPUs in the group which
+ * are known to be busy.  A busy CPU might be running the busy loop
+ * below or general kernel code.  The count is decremented on entry to
+ * the old pm_idle handler and incremented on exit.  The aim is to
+ * avoid the count going to zero or negative.  This situation can
+ * occur temporarily during module unload or CPU hot-plug but
+ * normality will be restored when the affected CPUs next exit the
+ * idle loop. */
+static atomic_t busy_cpu_count[NR_CPUS+1];
+
+/* A workqueue item to be executed to cause the CPU to exit from the
+ * idle loop. */
+DEFINE_PER_CPU(struct work_struct, sys_tune_cpu_work);
+
+#define sys_tune_set_state(CPU,STATE) \
+	do { } while(0)
+
+
+/* A mutex to protect most of the module datastructures. */
+static DEFINE_MUTEX(sys_tune_startup_mutex);
+
+/* The old pm_idle handler. */
+static void (*old_pm_idle)(void) = NULL;
+
+static void sys_tune_pm_idle(void)
+{
+	atomic_t *busy_cpus_ptr;
+	int busy_cpus;
+	int cpu = smp_processor_id();
+
+	busy_cpus_ptr = &(busy_cpu_count[per_cpu(idle_cpu_group, cpu)]);
+
+	sys_tune_set_state(cpu, 2);
+
+	local_irq_enable();
+	while (!need_resched()) {
+		busy_cpus = atomic_read(busy_cpus_ptr);
+
+		/* If other CPUs in this group are busy then let this
+		 * CPU go idle.  We mustn't let the number of busy
+		 * CPUs drop below 1. */
+		if ( busy_cpus > 1 &&
+		     old_pm_idle != NULL &&
+		     ( atomic_cmpxchg(busy_cpus_ptr, busy_cpus,
+				      busy_cpus-1) == busy_cpus ) ) {
+			local_irq_disable();
+			sys_tune_set_state(cpu, 3);
+			/* This check might not be necessary, but it
+			 * seems safest to include it because there
+			 * might be a kernel version which requires
+			 * it. */
+			if (need_resched())
+				local_irq_enable();
+			else
+				old_pm_idle();
+			/* This CPU is busy again. */
+			sys_tune_set_state(cpu, 1);
+			atomic_add(1, busy_cpus_ptr);
+			return;
+		}
+
+		cpu_relax();
+	}
+	sys_tune_set_state(cpu, 0);
+}
+
+
+void sys_tune_work_func(struct work_struct *work)
+{
+	/* Do nothing.  Since this function is running in process
+	 * context, the idle thread isn't running on this CPU. */
+}
+
+
+#ifdef CONFIG_SMP
+static void sys_tune_smp_call(void *info)
+{
+	schedule_work(&get_cpu_var(sys_tune_cpu_work));
+	put_cpu_var(sys_tune_cpu_work);
+}
+#endif
+
+
+#ifdef CONFIG_SMP
+static void sys_tune_refresh(void)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,26)
+        on_each_cpu(&sys_tune_smp_call, NULL, 0, 1);
+#else
+        on_each_cpu(&sys_tune_smp_call, NULL, 1);
+#endif
+}
+#else
+static void sys_tune_refresh(void)
+{
+	/* The current thread is executing on the one and only CPU so
+	 * the idle thread isn't running. */
+}
+#endif
+
+
+
+static int sys_tune_cpu_group(int cpu)
+{
+#ifdef CONFIG_SMP
+	const cpumask_t *mask;
+	int other_cpu;
+	int group;
+
+#if defined(topology_thread_cpumask) && defined(ST_HAVE_EXPORTED_CPU_SIBLING_MAP)
+	/* Keep one hyperthread busy per core. */
+	mask = topology_thread_cpumask(cpu);
+#else
+	return cpu;
+#endif
+	for_each_cpu_mask(cpu, *(mask))	{
+		group = per_cpu(idle_cpu_group, other_cpu);
+		if (group != NR_CPUS)
+			return group;
+	}
+#endif
+
+	return cpu;
+}
+
+
+static void sys_tune_add_cpu(int cpu)
+{
+	int group;
+
+	/* Do nothing if this CPU has already been added. */
+	if (per_cpu(idle_cpu_group, cpu) != NR_CPUS)
+		return;
+
+	group = sys_tune_cpu_group(cpu);
+	per_cpu(idle_cpu_group, cpu) = group;
+	atomic_inc(&(busy_cpu_count[group]));
+
+}
+
+static void sys_tune_del_cpu(int cpu)
+{
+
+	int group;
+
+	if (per_cpu(idle_cpu_group, cpu) == NR_CPUS)
+		return;
+
+	group = per_cpu(idle_cpu_group, cpu);
+	/* If the CPU was busy, this can cause the count to drop to
+	 * zero.  To rectify this, we need to cause one of the other
+	 * CPUs in the group to exit the idle loop.  If the CPU was
+	 * not busy then this causes the contribution for this CPU to
+	 * go to -1 which can cause the overall count to drop to zero
+	 * or go negative.  To rectify this situation we need to cause
+	 * this CPU to exit the idle loop. */
+	atomic_dec(&(busy_cpu_count[group]));
+	per_cpu(idle_cpu_group, cpu) = NR_CPUS;
+
+}
+
+
+static int sys_tune_cpu_notify(struct notifier_block *self,
+			       unsigned long action, void *hcpu)
+{
+	int cpu = (long)hcpu;
+	
+	switch(action) {
+#ifdef CPU_ONLINE_FROZEN
+	case CPU_ONLINE_FROZEN:
+#endif
+	case CPU_ONLINE:
+		mutex_lock(&sys_tune_startup_mutex);
+		sys_tune_add_cpu(cpu);
+		mutex_unlock(&sys_tune_startup_mutex);
+		/* The CPU might have already entered the idle loop in
+		 * the wrong group.  Make sure it exits the idle loop
+		 * so that it picks up the correct group. */
+		sys_tune_refresh();
+		break;
+
+#ifdef CPU_DEAD_FROZEN
+	case CPU_DEAD_FROZEN:
+#endif
+	case CPU_DEAD:
+		mutex_lock(&sys_tune_startup_mutex);
+		sys_tune_del_cpu(cpu);
+		mutex_unlock(&sys_tune_startup_mutex);
+		/* The deleted CPU may have been the only busy CPU in
+		 * the group.  Make sure one of the other CPUs in the
+		 * group exits the idle loop. */
+		sys_tune_refresh();
+		break;
+	}
+	return NOTIFY_OK;
+}
+
+
+static struct notifier_block sys_tune_cpu_nb = {
+	.notifier_call = sys_tune_cpu_notify,
+};
+
+
+static void sys_tune_ensure_init(void)
+{
+	BUG_ON (old_pm_idle != NULL);
+
+	/* Atomically update pm_idle to &sys_tune_pm_idle.  The old value
+	 * is stored in old_pm_idle before installing the new
+	 * handler. */
+	do {
+		old_pm_idle = pm_idle;
+	} while (cmpxchg(&pm_idle, old_pm_idle, &sys_tune_pm_idle) !=
+		 old_pm_idle);
+}
+#endif
+
+void sys_tune_fini(void)
+{
+#ifdef CONFIG_X86
+	void (*old)(void);
+	int cpu;
+
+	unregister_cpu_notifier(&sys_tune_cpu_nb);
+
+	mutex_lock(&sys_tune_startup_mutex);
+
+
+	old = cmpxchg(&pm_idle, &sys_tune_pm_idle, old_pm_idle);
+
+	for_each_online_cpu(cpu)
+		sys_tune_del_cpu(cpu);
+
+	mutex_unlock(&sys_tune_startup_mutex);
+	
+	/* Our handler may still be executing on other CPUs.
+	 * Schedule this thread on all CPUs to make sure all
+	 * idle threads get interrupted. */
+	sys_tune_refresh();
+
+	/* Make sure the work item has finished executing on all CPUs.
+	 * This in turn ensures that all idle threads have been
+	 * interrupted. */
+	flush_scheduled_work();
+#endif /* CONFIG_X86 */
+}
+
+void sys_tune_init(void)
+{
+#ifdef CONFIG_X86
+	int cpu;
+
+	for_each_possible_cpu(cpu) {
+		INIT_WORK(&per_cpu(sys_tune_cpu_work, cpu),
+			  sys_tune_work_func);
+	}
+
+	/* Start by registering the handler to ensure we don't miss
+	 * any updates. */
+	register_cpu_notifier(&sys_tune_cpu_nb);
+
+	mutex_lock(&sys_tune_startup_mutex);
+
+	for_each_online_cpu(cpu)
+		sys_tune_add_cpu(cpu);
+
+	sys_tune_ensure_init();
+
+
+	mutex_unlock(&sys_tune_startup_mutex);
+
+	/* Ensure our idle handler starts to run. */
+	sys_tune_refresh();
+#endif
+}
+
diff -r c23d1fc7e422 drivers/net/mlx4/xrcd.c
--- /dev/null
+++ b/drivers/net/mlx4/xrcd.c
@@ -0,0 +1,70 @@
+/*
+ * Copyright (c) 2006, 2007 Cisco Systems, Inc.  All rights reserved.
+ * Copyright (c) 2007 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/init.h>
+#include <linux/errno.h>
+
+#include "mlx4.h"
+
+int mlx4_xrcd_alloc(struct mlx4_dev *dev, u32 *xrcdn)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	*xrcdn = mlx4_bitmap_alloc(&priv->xrcd_bitmap);
+	if (*xrcdn == -1)
+		return -ENOMEM;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(mlx4_xrcd_alloc);
+
+void mlx4_xrcd_free(struct mlx4_dev *dev, u32 xrcdn)
+{
+	mlx4_bitmap_free(&mlx4_priv(dev)->xrcd_bitmap, xrcdn);
+}
+EXPORT_SYMBOL_GPL(mlx4_xrcd_free);
+
+int mlx4_init_xrcd_table(struct mlx4_dev *dev)
+{
+	struct mlx4_priv *priv = mlx4_priv(dev);
+
+	return mlx4_bitmap_init(&priv->xrcd_bitmap, (1 << 16),
+				(1 << 16) - 1, dev->caps.reserved_xrcds + 1, 0);
+}
+
+void mlx4_cleanup_xrcd_table(struct mlx4_dev *dev)
+{
+	mlx4_bitmap_cleanup(&mlx4_priv(dev)->xrcd_bitmap);
+}
+
+
diff -r c23d1fc7e422 include/linux/mlx4/cmd.h
--- a/include/linux/mlx4/cmd.h
+++ b/include/linux/mlx4/cmd.h
@@ -56,13 +56,17 @@ enum {
 	MLX4_CMD_QUERY_HCA	 = 0xb,
 	MLX4_CMD_QUERY_PORT	 = 0x43,
 	MLX4_CMD_SENSE_PORT	 = 0x4d,
+	MLX4_CMD_HW_HEALTH_CHECK = 0x50,
 	MLX4_CMD_SET_PORT	 = 0xc,
+	MLX4_CMD_QUERY_FUNC	 = 0x56,
+	MLX4_CMD_SET_NODE	 = 0x5a,
 	MLX4_CMD_ACCESS_DDR	 = 0x2e,
 	MLX4_CMD_MAP_ICM	 = 0xffa,
 	MLX4_CMD_UNMAP_ICM	 = 0xff9,
 	MLX4_CMD_MAP_ICM_AUX	 = 0xffc,
 	MLX4_CMD_UNMAP_ICM_AUX	 = 0xffb,
 	MLX4_CMD_SET_ICM_SIZE	 = 0xffd,
+	MLX4_CMD_GET_OP_REQ	 = 0x59,
 
 	/* TPT commands */
 	MLX4_CMD_SW2HW_MPT	 = 0xd,
@@ -117,20 +121,47 @@ enum {
 	/* miscellaneous commands */
 	MLX4_CMD_DIAG_RPRT	 = 0x30,
 	MLX4_CMD_NOP		 = 0x31,
+	MLX4_CMD_ACCESS_MEM	 = 0x2e,
+	MLX4_CMD_SET_VEP	 = 0x52,
+
+	/* Ethernet specific commands */
+	MLX4_CMD_SET_VLAN_FLTR	 = 0x47,
+	MLX4_CMD_SET_MCAST_FLTR	 = 0x48,
+	MLX4_CMD_DUMP_ETH_STATS	 = 0x49,
+
+	/* Communication channel commands */
+	MLX4_CMD_ARM_COMM_CHANNEL = 0x57,
+	MLX4_CMD_GEN_EQE	 = 0x58,
+
+	/* virtual commands */
+	MLX4_CMD_ALLOC_RES	 = 0xf00,
+	MLX4_CMD_FREE_RES	 = 0xf01,
+	MLX4_CMD_REPLACE_RES	 = 0xf02,
+	MLX4_CMD_GET_EVENT	 = 0xf03,
+	MLX4_CMD_QUERY_SLAVE_CAP = 0xf04,
+	MLX4_CMD_MCAST_ATTACH	 = 0xf05,
+	MLX4_CMD_GET_SLAVE_SQP	 = 0xf06,
+	MLX4_CMD_COMM_INT	 = 0xf07,
+	MLX4_CMD_PROMISC	 = 0xf08,
 
 	/* debug commands */
 	MLX4_CMD_QUERY_DEBUG_MSG = 0x2a,
 	MLX4_CMD_SET_DEBUG_MSG	 = 0x2b,
+
+	/* statistics commands */
+	MLX4_CMD_QUERY_IF_STAT	 = 0X54,
+	MLX4_CMD_SET_IF_STAT	 = 0X55,
 };
 
 enum {
-	MLX4_CMD_TIME_CLASS_A	= 10000,
-	MLX4_CMD_TIME_CLASS_B	= 10000,
-	MLX4_CMD_TIME_CLASS_C	= 10000,
+	MLX4_CMD_TIME_CLASS_A	= 60000,
+	MLX4_CMD_TIME_CLASS_B	= 60000,
+	MLX4_CMD_TIME_CLASS_C	= 60000,
 };
 
 enum {
-	MLX4_MAILBOX_SIZE	=  4096
+	MLX4_MAILBOX_SIZE	= 4096,
+	MLX4_ACCESS_MEM_ALIGN	= 256,
 };
 
 enum {
@@ -140,6 +171,13 @@ enum {
 	MLX4_SET_PORT_MAC_TABLE = 0x2,
 	MLX4_SET_PORT_VLAN_TABLE = 0x3,
 	MLX4_SET_PORT_PRIO_MAP  = 0x4,
+	MLX4_SET_PORT_GID_TABLE = 0x5,
+	MLX4_SET_PORT_MODIFIERS
+};
+
+enum {
+	MLX4_DUMP_STATS_PORT_COUNTERS = 0x0,
+	MLX4_DUMP_STATS_FUNC_COUNTERS = 0x8,
 };
 
 struct mlx4_dev;
diff -r c23d1fc7e422 include/linux/mlx4/cq.h
--- a/include/linux/mlx4/cq.h
+++ b/include/linux/mlx4/cq.h
@@ -146,5 +146,5 @@ int mlx4_cq_modify(struct mlx4_dev *dev,
 		   u16 count, u16 period);
 int mlx4_cq_resize(struct mlx4_dev *dev, struct mlx4_cq *cq,
 		   int entries, struct mlx4_mtt *mtt);
-
+int mlx4_cq_ignore_overrun(struct mlx4_dev *dev, struct mlx4_cq *cq);
 #endif /* MLX4_CQ_H */
diff -r c23d1fc7e422 include/linux/mlx4/device.h
--- a/include/linux/mlx4/device.h
+++ b/include/linux/mlx4/device.h
@@ -35,13 +35,22 @@
 
 #include <linux/pci.h>
 #include <linux/completion.h>
+#include <linux/netdevice.h>
 #include <linux/radix-tree.h>
 
 #include <asm/atomic.h>
 
+#include <linux/mlx4/driver.h>
+
+#define MAX_MSIX_P_PORT 17
+
 enum {
 	MLX4_FLAG_MSI_X		= 1 << 0,
 	MLX4_FLAG_OLD_PORT_CMDS	= 1 << 1,
+	MLX4_FLAG_MASTER	= 1 << 2,
+	MLX4_FLAG_SLAVE		= 1 << 3,
+	MLX4_FLAG_SRIOV		= 1 << 4,
+	MLX4_FLAG_PF		= 1 << 5,
 };
 
 enum {
@@ -53,20 +62,37 @@ enum {
 };
 
 enum {
+	MLX4_DEV_CAP_FLAG_WOL_PORT1	= 1 << 5,
+	MLX4_DEV_CAP_FLAG_WOL_PORT2	= 1 << 6
+};
+
+enum {
 	MLX4_DEV_CAP_FLAG_RC		= 1 <<  0,
 	MLX4_DEV_CAP_FLAG_UC		= 1 <<  1,
 	MLX4_DEV_CAP_FLAG_UD		= 1 <<  2,
+	MLX4_DEV_CAP_FLAG_XRC		= 1 <<  3,
 	MLX4_DEV_CAP_FLAG_SRQ		= 1 <<  6,
 	MLX4_DEV_CAP_FLAG_IPOIB_CSUM	= 1 <<  7,
 	MLX4_DEV_CAP_FLAG_BAD_PKEY_CNTR	= 1 <<  8,
 	MLX4_DEV_CAP_FLAG_BAD_QKEY_CNTR	= 1 <<  9,
 	MLX4_DEV_CAP_FLAG_DPDP		= 1 << 12,
+	MLX4_DEV_CAP_FLAG_RAW_ETY	= 1 << 13,
+	MLX4_DEV_CAP_FLAG_BLH		= 1 << 15,
 	MLX4_DEV_CAP_FLAG_MEM_WINDOW	= 1 << 16,
 	MLX4_DEV_CAP_FLAG_APM		= 1 << 17,
 	MLX4_DEV_CAP_FLAG_ATOMIC	= 1 << 18,
 	MLX4_DEV_CAP_FLAG_RAW_MCAST	= 1 << 19,
 	MLX4_DEV_CAP_FLAG_UD_AV_PORT	= 1 << 20,
-	MLX4_DEV_CAP_FLAG_UD_MCAST	= 1 << 21
+	MLX4_DEV_CAP_FLAG_UD_MCAST	= 1 << 21,
+	MLX4_DEV_CAP_FLAG_IBOE		= 1 << 30,
+	MLX4_DEV_CAP_FLAG_FC_T11	= 1 << 31,
+	MLX4_DEV_CAP_SENSE_SUPPORT	= 1ull << 55
+};
+
+#define MLX4_ATTR_EXTENDED_PORT_INFO	cpu_to_be16(0xff90)
+
+enum {
+	MLX_EXT_PORT_CAP_FLAG_EXTENDED_PORT_INFO	= 1 <<  0
 };
 
 enum {
@@ -95,12 +121,21 @@ enum mlx4_event {
 	MLX4_EVENT_TYPE_PORT_CHANGE	   = 0x09,
 	MLX4_EVENT_TYPE_EQ_OVERFLOW	   = 0x0f,
 	MLX4_EVENT_TYPE_ECC_DETECT	   = 0x0e,
-	MLX4_EVENT_TYPE_CMD		   = 0x0a
+	MLX4_EVENT_TYPE_CMD		   = 0x0a,
+	MLX4_EVENT_TYPE_VEP_UPDATE	   = 0x19,
+	MLX4_EVENT_TYPE_COMM_CHANNEL	   = 0x18,
+	MLX4_EVENT_TYPE_MAC_UPDATE	   = 0x20,
+	MLX4_EVENT_TYPE_SQP_UPDATE	   = 0xfe,
+	MLX4_EVENT_TYPE_NONE		   = 0xff,
+	MLX4_EVENT_TYPE_OP_REQUIRED	   = 0x1a,
+	MLX4_EVENT_TYPE_FATAL_WARNING	   = 0x1b,
 };
 
 enum {
 	MLX4_PORT_CHANGE_SUBTYPE_DOWN	= 1,
-	MLX4_PORT_CHANGE_SUBTYPE_ACTIVE	= 4
+	MLX4_PORT_CHANGE_SUBTYPE_ACTIVE	= 4,
+
+	MLX4_FATAL_WARNING_SUBTYPE_WARMING = 0,
 };
 
 enum {
@@ -119,11 +154,12 @@ enum {
 	MLX4_OPCODE_SEND		= 0x0a,
 	MLX4_OPCODE_SEND_IMM		= 0x0b,
 	MLX4_OPCODE_LSO			= 0x0e,
+	MLX4_OPCODE_BIG_LSO		= 0x2e,
 	MLX4_OPCODE_RDMA_READ		= 0x10,
 	MLX4_OPCODE_ATOMIC_CS		= 0x11,
 	MLX4_OPCODE_ATOMIC_FA		= 0x12,
-	MLX4_OPCODE_ATOMIC_MASK_CS	= 0x14,
-	MLX4_OPCODE_ATOMIC_MASK_FA	= 0x15,
+	MLX4_OPCODE_MASKED_ATOMIC_CS	= 0x14,
+	MLX4_OPCODE_MASKED_ATOMIC_FA	= 0x15,
 	MLX4_OPCODE_BIND_MW		= 0x18,
 	MLX4_OPCODE_FMR			= 0x19,
 	MLX4_OPCODE_LOCAL_INVAL		= 0x1b,
@@ -142,6 +178,13 @@ enum {
 	MLX4_STAT_RATE_OFFSET	= 5
 };
 
+enum mlx4_protocol {
+	MLX4_PROT_IB_IPV6 = 0,
+	MLX4_PROT_ETH,
+	MLX4_PROT_IB_IPV4,
+	MLX4_PROT_FCOE
+};
+
 enum {
 	MLX4_MTT_FLAG_PRESENT		= 1
 };
@@ -150,11 +193,11 @@ enum mlx4_qp_region {
 	MLX4_QP_REGION_FW = 0,
 	MLX4_QP_REGION_ETH_ADDR,
 	MLX4_QP_REGION_FC_ADDR,
-	MLX4_QP_REGION_FC_EXCH,
 	MLX4_NUM_QP_REGION
 };
 
 enum mlx4_port_type {
+	MLX4_PORT_TYPE_NONE	= 0,
 	MLX4_PORT_TYPE_IB	= 1,
 	MLX4_PORT_TYPE_ETH	= 2,
 	MLX4_PORT_TYPE_AUTO	= 3
@@ -165,11 +208,26 @@ enum mlx4_special_vlan_idx {
 	MLX4_VLAN_MISS_IDX,
 	MLX4_VLAN_REGULAR
 };
+#define MLX4_LEAST_ATTACHED_VECTOR	0xffffffff
+
+enum mlx4_steer_type {
+	MLX4_MC_STEER = 0,
+	MLX4_UC_STEER,
+	MLX4_NUM_STEERS
+};
 
 enum {
-	MLX4_NUM_FEXCH          = 64 * 1024,
+	MLX4_CUNTERS_DISABLED,
+	MLX4_CUNTERS_BASIC,
+	MLX4_CUNTERS_EXT
 };
 
+enum {
+	MAX_FAST_REG_PAGES = 511,
+};
+
+#define MLX4_LEAST_ATTACHED_VECTOR	0xffffffff
+
 static inline u64 mlx4_fw_ver(u64 major, u64 minor, u64 subminor)
 {
 	return (major << 32) | (minor << 16) | subminor;
@@ -177,70 +235,102 @@ static inline u64 mlx4_fw_ver(u64 major,
 
 struct mlx4_caps {
 	u64			fw_ver;
-	int			num_ports;
-	int			vl_cap[MLX4_MAX_PORTS + 1];
-	int			ib_mtu_cap[MLX4_MAX_PORTS + 1];
+	u32			function;
+	u32			pf_num;
+	u32			vep_num;
+	u32			num_ports;
+	u32			vl_cap[MLX4_MAX_PORTS + 1];
+	u32			ib_mtu_cap[MLX4_MAX_PORTS + 1];
 	__be32			ib_port_def_cap[MLX4_MAX_PORTS + 1];
 	u64			def_mac[MLX4_MAX_PORTS + 1];
-	int			eth_mtu_cap[MLX4_MAX_PORTS + 1];
-	int			gid_table_len[MLX4_MAX_PORTS + 1];
-	int			pkey_table_len[MLX4_MAX_PORTS + 1];
-	int			local_ca_ack_delay;
-	int			num_uars;
-	int			bf_reg_size;
-	int			bf_regs_per_page;
-	int			max_sq_sg;
-	int			max_rq_sg;
-	int			num_qps;
-	int			max_wqes;
-	int			max_sq_desc_sz;
-	int			max_rq_desc_sz;
-	int			max_qp_init_rdma;
-	int			max_qp_dest_rdma;
-	int			sqp_start;
-	int			num_srqs;
-	int			max_srq_wqes;
-	int			max_srq_sge;
-	int			reserved_srqs;
-	int			num_cqs;
-	int			max_cqes;
-	int			reserved_cqs;
-	int			num_eqs;
-	int			reserved_eqs;
-	int			num_comp_vectors;
-	int			num_mpts;
-	int			num_mtt_segs;
-	int			mtts_per_seg;
-	int			fmr_reserved_mtts;
-	int			reserved_mtts;
-	int			reserved_mrws;
-	int			reserved_uars;
-	int			num_mgms;
-	int			num_amgms;
-	int			reserved_mcgs;
-	int			num_qp_per_mgm;
-	int			num_pds;
-	int			reserved_pds;
-	int			mtt_entry_sz;
+	u32			eth_mtu_cap[MLX4_MAX_PORTS + 1];
+	u32			gid_table_len[MLX4_MAX_PORTS + 1];
+	u32			pkey_table_len[MLX4_MAX_PORTS + 1];
+	u32			trans_type[MLX4_MAX_PORTS + 1];
+	u32			vendor_oui[MLX4_MAX_PORTS + 1];
+	u32			wavelength[MLX4_MAX_PORTS + 1];
+	u64			trans_code[MLX4_MAX_PORTS + 1];
+	u32			local_ca_ack_delay;
+	u32			num_uars;
+	u32			uar_page_size;
+	u32			bf_reg_size;
+	u32			bf_regs_per_page;
+	u32			max_sq_sg;
+	u32			max_rq_sg;
+	u32			num_qps;
+	u32			max_wqes;
+	u32			max_sq_desc_sz;
+	u32			max_rq_desc_sz;
+	u32			max_qp_init_rdma;
+	u32			max_qp_dest_rdma;
+	u32			sqp_start;
+	u32			tunnel_qpn;
+	u32			num_srqs;
+	u32			max_srq_wqes;
+	u32			max_srq_sge;
+	u32			reserved_srqs;
+	u32			num_cqs;
+	u32			max_cqes;
+	u32			reserved_cqs;
+	u32			num_eqs;
+	u32			reserved_eqs;
+	u32			num_comp_vectors;
+	u32			num_mpts;
+	u32			num_mtt_segs;
+	u32			mtts_per_seg;
+	u32			fmr_reserved_mtts;
+	u32			reserved_mtts;
+	u32			reserved_mrws;
+	u32			reserved_uars;
+	u32			num_mgms;
+	u32			num_amgms;
+	u32			reserved_mcgs;
+	u32			num_qp_per_mgm;
+	u32			num_pds;
+	u32			reserved_pds;
+	u32			mtt_entry_sz;
+	u32			reserved_xrcds;
+	u32			max_xrcds;
 	u32			max_msg_sz;
 	u32			page_size_cap;
-	u32			flags;
+	u64			flags;
 	u32			bmme_flags;
 	u32			reserved_lkey;
 	u16			stat_rate_support;
+	u32			udp_rss;
+	u32			loopback_support;
+	u32			vep_uc_steering;
+	u32			vep_mc_steering;
+	u32			wol;
 	u8			port_width_cap[MLX4_MAX_PORTS + 1];
-	int			max_gso_sz;
-	int                     reserved_qps_cnt[MLX4_NUM_QP_REGION];
-	int			reserved_qps;
-	int                     reserved_qps_base[MLX4_NUM_QP_REGION];
-	int                     log_num_macs;
-	int                     log_num_vlans;
-	int                     log_num_prios;
-	enum mlx4_port_type	port_type[MLX4_MAX_PORTS + 1];
+	u32			max_gso_sz;
+	u32                     reserved_qps_cnt[MLX4_NUM_QP_REGION];
+	u32			reserved_qps;
+	u32                     reserved_qps_base[MLX4_NUM_QP_REGION];
+	u32                     log_num_macs;
+	u32                     log_num_vlans;
+	u32                     log_num_prios;
+	u32	port_type[MLX4_MAX_PORTS + 1];
 	u8			supported_type[MLX4_MAX_PORTS + 1];
-	u32			port_mask;
-	enum mlx4_port_type	possible_type[MLX4_MAX_PORTS + 1];
-};
+	u8			suggested_type[MLX4_MAX_PORTS + 1];
+	u8			default_sense[MLX4_MAX_PORTS + 1];
+	u8			sqp_demux;
+	u32	port_mask[MLX4_MAX_PORTS + 1];
+	u32	possible_type[MLX4_MAX_PORTS + 1];
+	u8			ext_port_cap[MLX4_MAX_PORTS + 1];
+	u8			counters_mode;
+	u32			max_basic_counters;
+	u32			max_ext_counters;
+	u32			mc_promisc_mode;
+	u16			clp_ver;
+	u16			poolsz;
+	u32			eqe_size;
+	u32			cqe_size;
+	u8			eqe_factor;
+	u32			sync_qp;
+	u8			fast_drop;
+	u8			qinq;
+} __attribute__((packed));
 
 struct mlx4_buf_list {
 	void		       *buf;
@@ -276,6 +366,14 @@ struct mlx4_db_pgdir {
 
 struct mlx4_ib_user_db_page;
 
+struct mlx4_accl_ops {
+	void 	(*poll)(struct net_device *dev, int ring_num);
+	void 	(*get_tcp_ring)(struct net_device *dev, u8 *poll_ring,
+				u32 saddr, u32 daddr, u16 sport, u16 dport);
+	void 	(*get_udp_rings)(struct net_device *dev, u8 *poll_rings,
+				 u8 *num_rings);
+};
+
 struct mlx4_db {
 	__be32			*db;
 	union {
@@ -317,6 +415,17 @@ struct mlx4_fmr {
 struct mlx4_uar {
 	unsigned long		pfn;
 	int			index;
+	struct list_head	bf_list;
+	unsigned		free_bf_bmap;
+	void __iomem	       *map;
+	void __iomem	       *bf_map;
+};
+
+struct mlx4_bf {
+	unsigned long		offset;
+	int			buf_size;
+	struct mlx4_uar	       *uar;
+	void __iomem	       *reg;
 };
 
 struct mlx4_cq {
@@ -372,12 +481,71 @@ struct mlx4_av {
 	u8			dgid[16];
 };
 
+struct mlx4_eth_av {
+	__be32		port_pd;
+	u8		reserved1;
+	u8		smac_idx;
+	u16		reserved2;
+	u8		reserved3;
+	u8		gid_index;
+	u8		stat_rate;
+	u8		hop_limit;
+	__be32		sl_tclass_flowlabel;
+	u8		dgid[16];
+	u32		reserved4[2];
+	__be16		vlan;
+	u8		mac[6];
+};
+
+union mlx4_ext_av {
+	struct mlx4_av		ib;
+	struct mlx4_eth_av	eth;
+};
+
+struct mlx4_counters {
+	__be32	counter_mode;
+	__be32	num_ifc;
+	u32	reserved[2];
+	__be64	rx_frames;
+	__be64	rx_bytes;
+	__be64	tx_frames;
+	__be64	tx_bytes;
+};
+
+struct mlx4_counters_ext {
+	__be32	counter_mode;
+	__be32	num_ifc;
+	u32	reserved[2];
+	__be64	rx_uni_frames;
+	__be64	rx_uni_bytes;
+	__be64	rx_mcast_frames;
+	__be64	rx_mcast_bytes;
+	__be64	rx_bcast_frames;
+	__be64	rx_bcast_bytes;
+	__be64	rx_nobuf_frames;
+	__be64	rx_nobuf_bytes;
+	__be64	rx_err_frames;
+	__be64	rx_err_bytes;
+	__be64	tx_uni_frames;
+	__be64	tx_uni_bytes;
+	__be64	tx_mcast_frames;
+	__be64	tx_mcast_bytes;
+	__be64	tx_bcast_frames;
+	__be64	tx_bcast_bytes;
+	__be64	tx_nobuf_frames;
+	__be64	tx_nobuf_bytes;
+	__be64	tx_err_frames;
+	__be64	tx_err_bytes;
+};
+
 struct mlx4_dev {
 	struct pci_dev	       *pdev;
 	unsigned long		flags;
+	unsigned long		num_slaves;
 	struct mlx4_caps	caps;
 	struct radix_tree_root	qp_table_tree;
-	u32			rev_id;
+	struct radix_tree_root	srq_table_tree;
+	u8			rev_id;
 	char			board_id[MLX4_BOARD_ID_LEN];
 };
 
@@ -395,13 +563,568 @@ struct mlx4_init_port_param {
 	u64			si_guid;
 };
 
+static inline void mlx4_query_steer_cap(struct mlx4_dev *dev, int *log_mac,
+					int *log_vlan, int *log_prio)
+{
+	*log_mac = dev->caps.log_num_macs;
+	*log_vlan = dev->caps.log_num_vlans;
+	*log_prio = dev->caps.log_num_prios;
+}
+
+struct mlx4_stat_out_mbox {
+	/* Received frames with a length of 64 octets */
+	__be64 R64_prio_0;
+	__be64 R64_prio_1;
+	__be64 R64_prio_2;
+	__be64 R64_prio_3;
+	__be64 R64_prio_4;
+	__be64 R64_prio_5;
+	__be64 R64_prio_6;
+	__be64 R64_prio_7;
+	__be64 R64_novlan;
+	/* Received frames with a length of 127 octets */
+	__be64 R127_prio_0;
+	__be64 R127_prio_1;
+	__be64 R127_prio_2;
+	__be64 R127_prio_3;
+	__be64 R127_prio_4;
+	__be64 R127_prio_5;
+	__be64 R127_prio_6;
+	__be64 R127_prio_7;
+	__be64 R127_novlan;
+	/* Received frames with a length of 255 octets */
+	__be64 R255_prio_0;
+	__be64 R255_prio_1;
+	__be64 R255_prio_2;
+	__be64 R255_prio_3;
+	__be64 R255_prio_4;
+	__be64 R255_prio_5;
+	__be64 R255_prio_6;
+	__be64 R255_prio_7;
+	__be64 R255_novlan;
+	/* Received frames with a length of 511 octets */
+	__be64 R511_prio_0;
+	__be64 R511_prio_1;
+	__be64 R511_prio_2;
+	__be64 R511_prio_3;
+	__be64 R511_prio_4;
+	__be64 R511_prio_5;
+	__be64 R511_prio_6;
+	__be64 R511_prio_7;
+	__be64 R511_novlan;
+	/* Received frames with a length of 1023 octets */
+	__be64 R1023_prio_0;
+	__be64 R1023_prio_1;
+	__be64 R1023_prio_2;
+	__be64 R1023_prio_3;
+	__be64 R1023_prio_4;
+	__be64 R1023_prio_5;
+	__be64 R1023_prio_6;
+	__be64 R1023_prio_7;
+	__be64 R1023_novlan;
+	/* Received frames with a length of 1518 octets */
+	__be64 R1518_prio_0;
+	__be64 R1518_prio_1;
+	__be64 R1518_prio_2;
+	__be64 R1518_prio_3;
+	__be64 R1518_prio_4;
+	__be64 R1518_prio_5;
+	__be64 R1518_prio_6;
+	__be64 R1518_prio_7;
+	__be64 R1518_novlan;
+	/* Received frames with a length of 1522 octets */
+	__be64 R1522_prio_0;
+	__be64 R1522_prio_1;
+	__be64 R1522_prio_2;
+	__be64 R1522_prio_3;
+	__be64 R1522_prio_4;
+	__be64 R1522_prio_5;
+	__be64 R1522_prio_6;
+	__be64 R1522_prio_7;
+	__be64 R1522_novlan;
+	/* Received frames with a length of 1548 octets */
+	__be64 R1548_prio_0;
+	__be64 R1548_prio_1;
+	__be64 R1548_prio_2;
+	__be64 R1548_prio_3;
+	__be64 R1548_prio_4;
+	__be64 R1548_prio_5;
+	__be64 R1548_prio_6;
+	__be64 R1548_prio_7;
+	__be64 R1548_novlan;
+	/* Received frames with a length of 1548 < octets < MTU */
+	__be64 R2MTU_prio_0;
+	__be64 R2MTU_prio_1;
+	__be64 R2MTU_prio_2;
+	__be64 R2MTU_prio_3;
+	__be64 R2MTU_prio_4;
+	__be64 R2MTU_prio_5;
+	__be64 R2MTU_prio_6;
+	__be64 R2MTU_prio_7;
+	__be64 R2MTU_novlan;
+	/* Received frames with a length of MTU< octets and good CRC */
+	__be64 RGIANT_prio_0;
+	__be64 RGIANT_prio_1;
+	__be64 RGIANT_prio_2;
+	__be64 RGIANT_prio_3;
+	__be64 RGIANT_prio_4;
+	__be64 RGIANT_prio_5;
+	__be64 RGIANT_prio_6;
+	__be64 RGIANT_prio_7;
+	__be64 RGIANT_novlan;
+	/* Received broadcast frames with good CRC */
+	__be64 RBCAST_prio_0;
+	__be64 RBCAST_prio_1;
+	__be64 RBCAST_prio_2;
+	__be64 RBCAST_prio_3;
+	__be64 RBCAST_prio_4;
+	__be64 RBCAST_prio_5;
+	__be64 RBCAST_prio_6;
+	__be64 RBCAST_prio_7;
+	__be64 RBCAST_novlan;
+	/* Received multicast frames with good CRC */
+	__be64 MCAST_prio_0;
+	__be64 MCAST_prio_1;
+	__be64 MCAST_prio_2;
+	__be64 MCAST_prio_3;
+	__be64 MCAST_prio_4;
+	__be64 MCAST_prio_5;
+	__be64 MCAST_prio_6;
+	__be64 MCAST_prio_7;
+	__be64 MCAST_novlan;
+	/* Received unicast not short or GIANT frames with good CRC */
+	__be64 RTOTG_prio_0;
+	__be64 RTOTG_prio_1;
+	__be64 RTOTG_prio_2;
+	__be64 RTOTG_prio_3;
+	__be64 RTOTG_prio_4;
+	__be64 RTOTG_prio_5;
+	__be64 RTOTG_prio_6;
+	__be64 RTOTG_prio_7;
+	__be64 RTOTG_novlan;
+
+	/* Count of total octets of received frames, includes framing characters */
+	__be64 RTTLOCT_prio_0;
+	/* Count of total octets of received frames, not including framing
+	   characters */
+	__be64 RTTLOCT_NOFRM_prio_0;
+	/* Count of Total number of octets received
+	   (only for frames without errors) */
+	__be64 ROCT_prio_0;
+
+	__be64 RTTLOCT_prio_1;
+	__be64 RTTLOCT_NOFRM_prio_1;
+	__be64 ROCT_prio_1;
+
+	__be64 RTTLOCT_prio_2;
+	__be64 RTTLOCT_NOFRM_prio_2;
+	__be64 ROCT_prio_2;
+
+	__be64 RTTLOCT_prio_3;
+	__be64 RTTLOCT_NOFRM_prio_3;
+	__be64 ROCT_prio_3;
+
+	__be64 RTTLOCT_prio_4;
+	__be64 RTTLOCT_NOFRM_prio_4;
+	__be64 ROCT_prio_4;
+
+	__be64 RTTLOCT_prio_5;
+	__be64 RTTLOCT_NOFRM_prio_5;
+	__be64 ROCT_prio_5;
+
+	__be64 RTTLOCT_prio_6;
+	__be64 RTTLOCT_NOFRM_prio_6;
+	__be64 ROCT_prio_6;
+
+	__be64 RTTLOCT_prio_7;
+	__be64 RTTLOCT_NOFRM_prio_7;
+	__be64 ROCT_prio_7;
+
+	__be64 RTTLOCT_novlan;
+	__be64 RTTLOCT_NOFRM_novlan;
+	__be64 ROCT_novlan;
+
+	/* Count of Total received frames including bad frames */
+	__be64 RTOT_prio_0;
+	/* Count of  Total number of received frames with 802.1Q encapsulation */
+	__be64 R1Q_prio_0;
+	__be64 reserved1;
+
+	__be64 RTOT_prio_1;
+	__be64 R1Q_prio_1;
+	__be64 reserved2;
+
+	__be64 RTOT_prio_2;
+	__be64 R1Q_prio_2;
+	__be64 reserved3;
+
+	__be64 RTOT_prio_3;
+	__be64 R1Q_prio_3;
+	__be64 reserved4;
+
+	__be64 RTOT_prio_4;
+	__be64 R1Q_prio_4;
+	__be64 reserved5;
+
+	__be64 RTOT_prio_5;
+	__be64 R1Q_prio_5;
+	__be64 reserved6;
+
+	__be64 RTOT_prio_6;
+	__be64 R1Q_prio_6;
+	__be64 reserved7;
+
+	__be64 RTOT_prio_7;
+	__be64 R1Q_prio_7;
+	__be64 reserved8;
+
+	__be64 RTOT_novlan;
+	__be64 R1Q_novlan;
+	__be64 reserved9;
+
+	/* Total number of Successfully Received Control Frames */
+	__be64 RCNTL;
+	__be64 reserved10;
+	__be64 reserved11;
+	__be64 reserved12;
+	/* Count of received frames with a length/type field  value between 46
+	   (42 for VLANtagged frames) and 1500 (also 1500 for VLAN-tagged frames),
+	   inclusive */
+	__be64 RInRangeLengthErr;
+	/* Count of received frames with length/type field between 1501 and 1535
+	   decimal, inclusive */
+	__be64 ROutRangeLengthErr;
+	/* Count of received frames that are longer than max allowed size for
+	   802.3 frames (1518/1522) */
+	__be64 RFrmTooLong;
+	/* Count frames received with PCS error */
+	__be64 PCS;
+
+	/* Transmit frames with a length of 64 octets */
+	__be64 T64_prio_0;
+	__be64 T64_prio_1;
+	__be64 T64_prio_2;
+	__be64 T64_prio_3;
+	__be64 T64_prio_4;
+	__be64 T64_prio_5;
+	__be64 T64_prio_6;
+	__be64 T64_prio_7;
+	__be64 T64_novlan;
+	__be64 T64_loopbk;
+	/* Transmit frames with a length of 65 to 127 octets. */
+	__be64 T127_prio_0;
+	__be64 T127_prio_1;
+	__be64 T127_prio_2;
+	__be64 T127_prio_3;
+	__be64 T127_prio_4;
+	__be64 T127_prio_5;
+	__be64 T127_prio_6;
+	__be64 T127_prio_7;
+	__be64 T127_novlan;
+	__be64 T127_loopbk;
+	/* Transmit frames with a length of 128 to 255 octets */
+	__be64 T255_prio_0;
+	__be64 T255_prio_1;
+	__be64 T255_prio_2;
+	__be64 T255_prio_3;
+	__be64 T255_prio_4;
+	__be64 T255_prio_5;
+	__be64 T255_prio_6;
+	__be64 T255_prio_7;
+	__be64 T255_novlan;
+	__be64 T255_loopbk;
+	/* Transmit frames with a length of 256 to 511 octets */
+	__be64 T511_prio_0;
+	__be64 T511_prio_1;
+	__be64 T511_prio_2;
+	__be64 T511_prio_3;
+	__be64 T511_prio_4;
+	__be64 T511_prio_5;
+	__be64 T511_prio_6;
+	__be64 T511_prio_7;
+	__be64 T511_novlan;
+	__be64 T511_loopbk;
+	/* Transmit frames with a length of 512 to 1023 octets */
+	__be64 T1023_prio_0;
+	__be64 T1023_prio_1;
+	__be64 T1023_prio_2;
+	__be64 T1023_prio_3;
+	__be64 T1023_prio_4;
+	__be64 T1023_prio_5;
+	__be64 T1023_prio_6;
+	__be64 T1023_prio_7;
+	__be64 T1023_novlan;
+	__be64 T1023_loopbk;
+	/* Transmit frames with a length of 1024 to 1518 octets */
+	__be64 T1518_prio_0;
+	__be64 T1518_prio_1;
+	__be64 T1518_prio_2;
+	__be64 T1518_prio_3;
+	__be64 T1518_prio_4;
+	__be64 T1518_prio_5;
+	__be64 T1518_prio_6;
+	__be64 T1518_prio_7;
+	__be64 T1518_novlan;
+	__be64 T1518_loopbk;
+	/* Counts transmit frames with a length of 1519 to 1522 bytes */
+	__be64 T1522_prio_0;
+	__be64 T1522_prio_1;
+	__be64 T1522_prio_2;
+	__be64 T1522_prio_3;
+	__be64 T1522_prio_4;
+	__be64 T1522_prio_5;
+	__be64 T1522_prio_6;
+	__be64 T1522_prio_7;
+	__be64 T1522_novlan;
+	__be64 T1522_loopbk;
+	/* Transmit frames with a length of 1523 to 1548 octets */
+	__be64 T1548_prio_0;
+	__be64 T1548_prio_1;
+	__be64 T1548_prio_2;
+	__be64 T1548_prio_3;
+	__be64 T1548_prio_4;
+	__be64 T1548_prio_5;
+	__be64 T1548_prio_6;
+	__be64 T1548_prio_7;
+	__be64 T1548_novlan;
+	__be64 T1548_loopbk;
+	/* Counts transmit frames with a length of 1549 to MTU bytes */
+	__be64 T2MTU_prio_0;
+	__be64 T2MTU_prio_1;
+	__be64 T2MTU_prio_2;
+	__be64 T2MTU_prio_3;
+	__be64 T2MTU_prio_4;
+	__be64 T2MTU_prio_5;
+	__be64 T2MTU_prio_6;
+	__be64 T2MTU_prio_7;
+	__be64 T2MTU_novlan;
+	__be64 T2MTU_loopbk;
+	/* Transmit frames with a length greater than MTU octets and a good CRC. */
+	__be64 TGIANT_prio_0;
+	__be64 TGIANT_prio_1;
+	__be64 TGIANT_prio_2;
+	__be64 TGIANT_prio_3;
+	__be64 TGIANT_prio_4;
+	__be64 TGIANT_prio_5;
+	__be64 TGIANT_prio_6;
+	__be64 TGIANT_prio_7;
+	__be64 TGIANT_novlan;
+	__be64 TGIANT_loopbk;
+	/* Transmit broadcast frames with a good CRC */
+	__be64 TBCAST_prio_0;
+	__be64 TBCAST_prio_1;
+	__be64 TBCAST_prio_2;
+	__be64 TBCAST_prio_3;
+	__be64 TBCAST_prio_4;
+	__be64 TBCAST_prio_5;
+	__be64 TBCAST_prio_6;
+	__be64 TBCAST_prio_7;
+	__be64 TBCAST_novlan;
+	__be64 TBCAST_loopbk;
+	/* Transmit multicast frames with a good CRC */
+	__be64 TMCAST_prio_0;
+	__be64 TMCAST_prio_1;
+	__be64 TMCAST_prio_2;
+	__be64 TMCAST_prio_3;
+	__be64 TMCAST_prio_4;
+	__be64 TMCAST_prio_5;
+	__be64 TMCAST_prio_6;
+	__be64 TMCAST_prio_7;
+	__be64 TMCAST_novlan;
+	__be64 TMCAST_loopbk;
+	/* Transmit good frames that are neither broadcast nor multicast */
+	__be64 TTOTG_prio_0;
+	__be64 TTOTG_prio_1;
+	__be64 TTOTG_prio_2;
+	__be64 TTOTG_prio_3;
+	__be64 TTOTG_prio_4;
+	__be64 TTOTG_prio_5;
+	__be64 TTOTG_prio_6;
+	__be64 TTOTG_prio_7;
+	__be64 TTOTG_novlan;
+	__be64 TTOTG_loopbk;
+
+	/* total octets of transmitted frames, including framing characters */
+	__be64 TTTLOCT_prio_0;
+	/* total octets of transmitted frames, not including framing characters */
+	__be64 TTTLOCT_NOFRM_prio_0;
+	/* ifOutOctets */
+	__be64 TOCT_prio_0;
+
+	__be64 TTTLOCT_prio_1;
+	__be64 TTTLOCT_NOFRM_prio_1;
+	__be64 TOCT_prio_1;
+
+	__be64 TTTLOCT_prio_2;
+	__be64 TTTLOCT_NOFRM_prio_2;
+	__be64 TOCT_prio_2;
+
+	__be64 TTTLOCT_prio_3;
+	__be64 TTTLOCT_NOFRM_prio_3;
+	__be64 TOCT_prio_3;
+
+	__be64 TTTLOCT_prio_4;
+	__be64 TTTLOCT_NOFRM_prio_4;
+	__be64 TOCT_prio_4;
+
+	__be64 TTTLOCT_prio_5;
+	__be64 TTTLOCT_NOFRM_prio_5;
+	__be64 TOCT_prio_5;
+
+	__be64 TTTLOCT_prio_6;
+	__be64 TTTLOCT_NOFRM_prio_6;
+	__be64 TOCT_prio_6;
+
+	__be64 TTTLOCT_prio_7;
+	__be64 TTTLOCT_NOFRM_prio_7;
+	__be64 TOCT_prio_7;
+
+	__be64 TTTLOCT_novlan;
+	__be64 TTTLOCT_NOFRM_novlan;
+	__be64 TOCT_novlan;
+
+	__be64 TTTLOCT_loopbk;
+	__be64 TTTLOCT_NOFRM_loopbk;
+	__be64 TOCT_loopbk;
+
+	/* Total frames transmitted with a good CRC that are not aborted  */
+	__be64 TTOT_prio_0;
+	/* Total number of frames transmitted with 802.1Q encapsulation */
+	__be64 T1Q_prio_0;
+	__be64 reserved13;
+
+	__be64 TTOT_prio_1;
+	__be64 T1Q_prio_1;
+	__be64 reserved14;
+
+	__be64 TTOT_prio_2;
+	__be64 T1Q_prio_2;
+	__be64 reserved15;
+
+	__be64 TTOT_prio_3;
+	__be64 T1Q_prio_3;
+	__be64 reserved16;
+
+	__be64 TTOT_prio_4;
+	__be64 T1Q_prio_4;
+	__be64 reserved17;
+
+	__be64 TTOT_prio_5;
+	__be64 T1Q_prio_5;
+	__be64 reserved18;
+
+	__be64 TTOT_prio_6;
+	__be64 T1Q_prio_6;
+	__be64 reserved19;
+
+	__be64 TTOT_prio_7;
+	__be64 T1Q_prio_7;
+	__be64 reserved20;
+
+	__be64 TTOT_novlan;
+	__be64 T1Q_novlan;
+	__be64 reserved21;
+
+	__be64 TTOT_loopbk;
+	__be64 T1Q_loopbk;
+	__be64 reserved22;
+
+	/* Received frames with a length greater than MTU octets and a bad CRC */
+	__be32 RJBBR;
+	/* Received frames with a bad CRC that are not runts, jabbers,
+	   or alignment errors */
+	__be32 RCRC;
+	/* Received frames with SFD with a length of less than 64 octets and a
+	   bad CRC */
+	__be32 RRUNT;
+	/* Received frames with a length less than 64 octets and a good CRC */
+	__be32 RSHORT;
+	/* Total Number of Received Packets Dropped */
+	__be32 RDROP;
+	/* Drop due to overflow  */
+	__be32 RdropOvflw;
+	/* Drop due to overflow */
+	__be32 RdropLength;
+	/* Total of good frames. Does not include frames received with
+	   frame-too-long, FCS, or length errors */
+	__be32 RTOTFRMS;
+	/* Total dropped Xmited packets */
+	__be32 TDROP;
+};
+
+struct mlx4_func_stat_out_mbox {
+	__be64 etherStatsDropEvents;
+	__be64 etherStatsOctets;
+	__be64 etherStatsPkts;
+	__be64 etherStatsBroadcastPkts;
+	__be64 etherStatsMulticastPkts;
+	__be64 etherStatsCRCAlignErrors;
+	__be64 etherStatsUndersizePkts;
+	__be64 etherStatsOversizePkts;
+	__be64 etherStatsFragments;
+	__be64 etherStatsJabbers;
+	__be64 etherStatsCollisions;
+	__be64 etherStatsPkts64Octets;
+	__be64 etherStatsPkts65to127Octets;
+	__be64 etherStatsPkts128to255Octets;
+	__be64 etherStatsPkts256to511Octets;
+	__be64 etherStatsPkts512to1023Octets;
+	__be64 etherStatsPkts1024to1518Octets;
+};
+
+struct mlx4_eth_common_counters {
+	/* bad packets received		*/
+	unsigned long	rx_errors;
+	/* packet transmit problems	*/
+	unsigned long	tx_errors;
+	/* multicast packets received	*/
+	unsigned long	multicast;
+	unsigned long	rx_length_errors;
+	/* receiver ring buff overflow	*/
+	unsigned long	rx_over_errors;
+	/* recved pkt with crc error	*/
+	unsigned long	rx_crc_errors;
+	/* recv'r fifo overrun		*/
+	unsigned long	rx_fifo_errors;
+	/* receiver missed packet	*/
+	unsigned long	rx_missed_errors;
+	unsigned long	broadcast;
+
+	unsigned long	iboe_tx_packets;
+	unsigned long	iboe_rx_packets;
+	unsigned long	iboe_tx_bytess;
+	unsigned long	iboe_rx_bytess;
+};
+
+int mlx4_DUMP_ETH_STATS(struct mlx4_dev *dev, u8 port, u8 reset,
+			   struct mlx4_eth_common_counters *stats);
+
 #define mlx4_foreach_port(port, dev, type)				\
 	for ((port) = 1; (port) <= (dev)->caps.num_ports; (port)++)	\
-		if (((type) == MLX4_PORT_TYPE_IB ? (dev)->caps.port_mask : \
-		     ~(dev)->caps.port_mask) & 1 << ((port) - 1))
+		if ((type) == (dev)->caps.port_mask[(port)])
+
+#define mlx4_foreach_ib_transport_port(port, dev)                         \
+	for ((port) = 1; (port) <= (dev)->caps.num_ports; (port)++)       \
+		if (((dev)->caps.port_mask[port] == MLX4_PORT_TYPE_IB) || \
+			((dev)->caps.flags & MLX4_DEV_CAP_FLAG_IBOE))
+
+static inline int mlx4_is_slave(struct mlx4_dev *dev)
+{
+	return dev->flags & MLX4_FLAG_SLAVE;
+}
+
+static inline int mlx4_is_master(struct mlx4_dev *dev)
+{
+	return dev->flags & MLX4_FLAG_MASTER;
+}
+
+static inline int mlx4_is_mfunc(struct mlx4_dev *dev)
+{
+	return dev->flags & (MLX4_FLAG_MASTER | MLX4_FLAG_SLAVE);
+}
 
 int mlx4_buf_alloc(struct mlx4_dev *dev, int size, int max_direct,
-		   struct mlx4_buf *buf);
+		   struct mlx4_buf *buf, int numa_node);
 void mlx4_buf_free(struct mlx4_dev *dev, int size, struct mlx4_buf *buf);
 static inline void *mlx4_buf_offset(struct mlx4_buf *buf, int offset)
 {
@@ -415,16 +1138,27 @@ static inline void *mlx4_buf_offset(stru
 int mlx4_pd_alloc(struct mlx4_dev *dev, u32 *pdn);
 void mlx4_pd_free(struct mlx4_dev *dev, u32 pdn);
 
+int mlx4_xrcd_alloc(struct mlx4_dev *dev, u32 *xrcdn);
+void mlx4_xrcd_free(struct mlx4_dev *dev, u32 xrcdn);
+
 int mlx4_uar_alloc(struct mlx4_dev *dev, struct mlx4_uar *uar);
 void mlx4_uar_free(struct mlx4_dev *dev, struct mlx4_uar *uar);
+int mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf, int numa_node);
+void mlx4_bf_free(struct mlx4_dev *dev, struct mlx4_bf *bf);
 
 int mlx4_mtt_init(struct mlx4_dev *dev, int npages, int page_shift,
 		  struct mlx4_mtt *mtt);
 void mlx4_mtt_cleanup(struct mlx4_dev *dev, struct mlx4_mtt *mtt);
 u64 mlx4_mtt_addr(struct mlx4_dev *dev, struct mlx4_mtt *mtt);
 
+int mlx4_mr_reserve_range(struct mlx4_dev *dev, int cnt, int align, u32 *base_mridx);
+void mlx4_mr_release_range(struct mlx4_dev *dev, u32 base_mridx, int cnt);
+int mlx4_mr_alloc_reserved(struct mlx4_dev *dev, u32 mridx, u32 pd,
+			   u64 iova, u64 size, u32 access, int npages,
+			   int page_shift, struct mlx4_mr *mr);
 int mlx4_mr_alloc(struct mlx4_dev *dev, u32 pd, u64 iova, u64 size, u32 access,
 		  int npages, int page_shift, struct mlx4_mr *mr);
+void mlx4_mr_free_reserved(struct mlx4_dev *dev, struct mlx4_mr *mr);
 void mlx4_mr_free(struct mlx4_dev *dev, struct mlx4_mr *mr);
 int mlx4_mr_enable(struct mlx4_dev *dev, struct mlx4_mr *mr);
 int mlx4_write_mtt(struct mlx4_dev *dev, struct mlx4_mtt *mtt,
@@ -432,11 +1166,12 @@ int mlx4_write_mtt(struct mlx4_dev *dev,
 int mlx4_buf_write_mtt(struct mlx4_dev *dev, struct mlx4_mtt *mtt,
 		       struct mlx4_buf *buf);
 
-int mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db, int order);
+int mlx4_db_alloc(struct mlx4_dev *dev, struct mlx4_db *db, int order,
+		  int numa_node);
 void mlx4_db_free(struct mlx4_dev *dev, struct mlx4_db *db);
 
 int mlx4_alloc_hwq_res(struct mlx4_dev *dev, struct mlx4_hwq_resources *wqres,
-		       int size, int max_direct);
+		       int size, int max_direct, int numa_node);
 void mlx4_free_hwq_res(struct mlx4_dev *mdev, struct mlx4_hwq_resources *wqres,
 		       int size);
 
@@ -445,39 +1180,83 @@ int mlx4_cq_alloc(struct mlx4_dev *dev, 
 		  unsigned vector, int collapsed);
 void mlx4_cq_free(struct mlx4_dev *dev, struct mlx4_cq *cq);
 
-int mlx4_qp_reserve_range(struct mlx4_dev *dev, int cnt, int align, int *base);
+int mlx4_qp_reserve_range(struct mlx4_dev *dev, int cnt, int align, int *base,
+			  u32 skip_mask);
 void mlx4_qp_release_range(struct mlx4_dev *dev, int base_qpn, int cnt);
 
 int mlx4_qp_alloc(struct mlx4_dev *dev, int qpn, struct mlx4_qp *qp);
 void mlx4_qp_free(struct mlx4_dev *dev, struct mlx4_qp *qp);
+u32 mlx4_get_slave_sqp(struct mlx4_dev *dev, int vf);
 
-int mlx4_srq_alloc(struct mlx4_dev *dev, u32 pdn, struct mlx4_mtt *mtt,
-		   u64 db_rec, struct mlx4_srq *srq);
+int mlx4_srq_alloc(struct mlx4_dev *dev, u32 pdn, u32 cqn, u16 xrcd,
+		   struct mlx4_mtt *mtt, u64 db_rec, struct mlx4_srq *srq);
 void mlx4_srq_free(struct mlx4_dev *dev, struct mlx4_srq *srq);
 int mlx4_srq_arm(struct mlx4_dev *dev, struct mlx4_srq *srq, int limit_watermark);
 int mlx4_srq_query(struct mlx4_dev *dev, struct mlx4_srq *srq, int *limit_watermark);
 
+int mlx4_SET_PORT_general(struct mlx4_interface *intf, struct mlx4_dev *dev,
+		u8 port, int mtu, u8 *pptx, u8 *pprx);
+int mlx4_SET_PORT_qpn_calc(struct mlx4_dev *dev, u8 port, u32 base_qpn,
+			   u8 promisc);
+
 int mlx4_INIT_PORT(struct mlx4_dev *dev, int port);
 int mlx4_CLOSE_PORT(struct mlx4_dev *dev, int port);
 
 int mlx4_multicast_attach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
-			  int block_mcast_loopback);
-int mlx4_multicast_detach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16]);
+			  int block_mcast_loopback, enum mlx4_protocol prot,
+			  u8 high_prio);
+int mlx4_multicast_detach(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+				enum mlx4_protocol prot, u8 high_prio);
+int mlx4_multicast_promisc_add(struct mlx4_dev *dev, u32 qpn, u8 port);
+int mlx4_multicast_promisc_remove(struct mlx4_dev *dev, u32 qpn, u8 port);
+int mlx4_unicast_promisc_add(struct mlx4_dev *dev, u32 qpn, u8 port);
+int mlx4_unicast_promisc_remove(struct mlx4_dev *dev, u32 qpn, u8 port);
+int mlx4_SET_MCAST_FLTR(struct mlx4_dev *dev, u8 port, u64 mac, u64 clear, u8 mode);
 
-int mlx4_register_mac(struct mlx4_dev *dev, u8 port, u64 mac, int *index);
-void mlx4_unregister_mac(struct mlx4_dev *dev, u8 port, int index);
+int mlx4_register_mac(struct mlx4_dev *dev, u8 port, u64 mac, int *qpn, u8 wrap);
+void mlx4_unregister_mac(struct mlx4_dev *dev, u8 port, int qpn);
+int mlx4_replace_mac(struct mlx4_dev *dev, u8 port, int qpn, u64 new_mac, u8 wrap);
 
+int mlx4_find_cached_vlan(struct mlx4_dev *dev, u8 port, u16 vid, int *idx);
 int mlx4_register_vlan(struct mlx4_dev *dev, u8 port, u16 vlan, int *index);
 void mlx4_unregister_vlan(struct mlx4_dev *dev, u8 port, int index);
 
+int mlx4_qp_attach_common(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  int block_mcast_loopback, enum mlx4_protocol prot,
+			  enum mlx4_steer_type steer, u8 high_prio);
+int mlx4_qp_detach_common(struct mlx4_dev *dev, struct mlx4_qp *qp, u8 gid[16],
+			  enum mlx4_protocol prot, enum mlx4_steer_type steer,
+			  u8 high_prio);
+
+int mlx4_map_phys_fmr_fbo(struct mlx4_dev *dev, struct mlx4_fmr *fmr,
+			  u64 *page_list, int npages, u64 iova, u32 fbo,
+			  u32 len, u32 *lkey, u32 *rkey, int same_key);
 int mlx4_map_phys_fmr(struct mlx4_dev *dev, struct mlx4_fmr *fmr, u64 *page_list,
 		      int npages, u64 iova, u32 *lkey, u32 *rkey);
+int mlx4_fmr_alloc_reserved(struct mlx4_dev *dev, u32 mridx, u32 pd,
+			    u32 access, int max_pages, int max_maps,
+			    u8 page_shift, struct mlx4_fmr *fmr);
 int mlx4_fmr_alloc(struct mlx4_dev *dev, u32 pd, u32 access, int max_pages,
 		   int max_maps, u8 page_shift, struct mlx4_fmr *fmr);
 int mlx4_fmr_enable(struct mlx4_dev *dev, struct mlx4_fmr *fmr);
 void mlx4_fmr_unmap(struct mlx4_dev *dev, struct mlx4_fmr *fmr,
 		    u32 *lkey, u32 *rkey);
+int mlx4_fmr_free_reserved(struct mlx4_dev *dev, struct mlx4_fmr *fmr);
 int mlx4_fmr_free(struct mlx4_dev *dev, struct mlx4_fmr *fmr);
 int mlx4_SYNC_TPT(struct mlx4_dev *dev);
+int mlx4_query_diag_counters(struct mlx4_dev *mlx4_dev, int array_length,
+			     u8 op_modifier, u32 in_offset[], u32 counter_out[]);
+int mlx4_test_interrupts(struct mlx4_dev *dev);
+int mlx4_QUERY_PORT(struct mlx4_dev *dev, void *outbox, u8 port);
+int mlx4_assign_eq(struct mlx4_dev *dev, char* name , int* vector);
+void mlx4_release_eq(struct mlx4_dev *dev, int vec);
+
+void mlx4_get_fc_t11_settings(struct mlx4_dev *dev, int *enable_pre_t11, int *t11_supported);
+
+int mlx4_counter_alloc(struct mlx4_dev *dev, u32 *idx);
+void mlx4_counter_free(struct mlx4_dev *dev, u32 idx);
+
+int mlx4_wol_read(struct mlx4_dev *dev, u64 *config, int port);
+int mlx4_wol_write(struct mlx4_dev *dev, u64 config, int port);
 
 #endif /* MLX4_DEVICE_H */
diff -r c23d1fc7e422 include/linux/mlx4/driver.h
--- a/include/linux/mlx4/driver.h
+++ b/include/linux/mlx4/driver.h
@@ -44,15 +44,34 @@ enum mlx4_dev_event {
 	MLX4_DEV_EVENT_PORT_REINIT,
 };
 
+enum mlx4_query_reply {
+	MLX4_QUERY_NOT_MINE	= -1,
+	MLX4_QUERY_MINE_NOPORT 	= 0
+};
+
+enum mlx4_prot {
+	MLX4_PROT_IB,
+	MLX4_PROT_EN,
+};
+
 struct mlx4_interface {
 	void *			(*add)	 (struct mlx4_dev *dev);
 	void			(*remove)(struct mlx4_dev *dev, void *context);
 	void			(*event) (struct mlx4_dev *dev, void *context,
 					  enum mlx4_dev_event event, int port);
+	enum mlx4_query_reply	(*query) (void *context, void *);
+	void *			(*get_prot_dev) (struct mlx4_dev *dev,
+						 void *context, u8 port);
 	struct list_head	list;
+	enum mlx4_prot          protocol;
 };
 
 int mlx4_register_interface(struct mlx4_interface *intf);
 void mlx4_unregister_interface(struct mlx4_interface *intf);
+void *mlx4_get_prot_dev(struct mlx4_dev *dev, enum mlx4_prot proto, int port);
+
+struct mlx4_dev *mlx4_query_interface(void *, int *port);
+void mlx4_set_iboe_counter(struct mlx4_dev *dev, int index, u8 port);
+int mlx4_get_iboe_counter(struct mlx4_dev *dev, u8 port);
 
 #endif /* MLX4_DRIVER_H */
diff -r c23d1fc7e422 include/linux/mlx4/qp.h
--- a/include/linux/mlx4/qp.h
+++ b/include/linux/mlx4/qp.h
@@ -39,6 +39,15 @@
 
 #define MLX4_INVALID_LKEY	0x100
 
+enum ib_m_qp_attr_mask {
+	IB_M_EXT_CLASS_1 = 1 << 28,
+	IB_M_EXT_CLASS_2 = 1 << 29,
+	IB_M_EXT_CLASS_3 = 1 << 30,
+
+	IB_M_QP_MOD_VEND_MASK =  ( IB_M_EXT_CLASS_1 |
+        	IB_M_EXT_CLASS_2 | IB_M_EXT_CLASS_3)
+};
+
 enum mlx4_qp_optpar {
 	MLX4_QP_OPTPAR_ALT_ADDR_PATH		= 1 << 0,
 	MLX4_QP_OPTPAR_RRE			= 1 << 1,
@@ -54,7 +63,8 @@ enum mlx4_qp_optpar {
 	MLX4_QP_OPTPAR_RETRY_COUNT		= 1 << 12,
 	MLX4_QP_OPTPAR_RNR_RETRY		= 1 << 13,
 	MLX4_QP_OPTPAR_ACK_TIMEOUT		= 1 << 14,
-	MLX4_QP_OPTPAR_SCHED_QUEUE		= 1 << 16
+	MLX4_QP_OPTPAR_SCHED_QUEUE		= 1 << 16,
+	MLX4_QP_OPTPAR_COUNTER_INDEX		= 1 << 20
 };
 
 enum mlx4_qp_state {
@@ -74,6 +84,7 @@ enum {
 	MLX4_QP_ST_UC				= 0x1,
 	MLX4_QP_ST_RD				= 0x2,
 	MLX4_QP_ST_UD				= 0x3,
+	MLX4_QP_ST_XRC				= 0x6,
 	MLX4_QP_ST_MLX				= 0x7
 };
 
@@ -93,13 +104,43 @@ enum {
 	MLX4_QP_BIT_RWE				= 1 << 14,
 	MLX4_QP_BIT_RAE				= 1 << 13,
 	MLX4_QP_BIT_RIC				= 1 <<	4,
+	MLX4_QP_BIT_COLL_SYNC_RQ                = 1 <<  2,
+	MLX4_QP_BIT_COLL_SYNC_SQ                = 1 <<  1,
+	MLX4_QP_BIT_COLL_MASTER                 = 1 <<  0
+};
+
+enum {
+	MLX4_RSS_HASH_XOR			= 0,
+	MLX4_RSS_HASH_TOP			= 1,
+
+	MLX4_RSS_UDP_IPV6			= 1 << 0,
+	MLX4_RSS_UDP_IPV4			= 1 << 1,
+	MLX4_RSS_TCP_IPV6			= 1 << 2,
+	MLX4_RSS_IPV6				= 1 << 3,
+	MLX4_RSS_TCP_IPV4			= 1 << 4,
+	MLX4_RSS_IPV4				= 1 << 5,
+
+	/* offset of mlx4_rss_context within mlx4_qp_context.pri_path */
+	MLX4_RSS_OFFSET_IN_QPC_PRI_PATH		= 0x24,
+	/* offset of being RSS indirection QP within mlx4_qp_context.flags */
+	MLX4_RSS_QPC_FLAG_OFFSET		= 13,
+};
+
+struct mlx4_rss_context {
+	__be32			base_qpn;
+	__be32			default_qpn;
+	u16			reserved;
+	u8			hash_fn;
+	u8			flags;
+	__be32			rss_key[10];
+	__be32			base_qpn_udp;
 };
 
 struct mlx4_qp_path {
 	u8			fl;
 	u8			reserved1[2];
 	u8			pkey_index;
-	u8			reserved2;
+	u8			counter_index;
 	u8			grh_mylmc;
 	__be16			rlid;
 	u8			ackto;
@@ -109,10 +150,10 @@ struct mlx4_qp_path {
 	__be32			tclass_flowlabel;
 	u8			rgid[16];
 	u8			sched_queue;
-	u8			snooper_flags;
+	u8			vlan_index;
 	u8			reserved3[2];
-	u8			counter_index;
-	u8			reserved4[7];
+	u8			reserved4[2];
+	u8			dmac[6];
 };
 
 struct mlx4_qp_context {
@@ -136,7 +177,7 @@ struct mlx4_qp_context {
 	__be32			ssn;
 	__be32			params2;
 	__be32			rnr_nextrecvpsn;
-	__be32			srcd;
+	__be32			xrcd;
 	__be32			cqn_recv;
 	__be64			db_rec_addr;
 	__be32			qkey;
@@ -151,7 +192,16 @@ struct mlx4_qp_context {
 	u8			reserved4[2];
 	u8			mtt_base_addr_h;
 	__be32			mtt_base_addr_l;
-	u32			reserved5[10];
+	u8			VE;
+	u8			reserved5;
+	__be16			VFT_id_prio;
+	u8			reserved6;
+	u8			exch_size;
+	__be16			exch_base;
+	u8			VFT_hop_cnt;
+	u8			my_fc_id_idx;
+	__be16			reserved7;
+	u32			reserved8[7];
 };
 
 /* Which firmware version adds support for NEC (NoErrorCompletion) bit */
@@ -166,6 +216,7 @@ enum {
 	MLX4_WQE_CTRL_TCP_UDP_CSUM	= 1 << 5,
 	MLX4_WQE_CTRL_INS_VLAN		= 1 << 6,
 	MLX4_WQE_CTRL_STRONG_ORDER	= 1 << 7,
+	MLX4_WQE_CTRL_FORCE_LOOPBACK	= 1 << 0,
 };
 
 struct mlx4_wqe_ctrl_seg {
@@ -192,7 +243,8 @@ struct mlx4_wqe_ctrl_seg {
 
 enum {
 	MLX4_WQE_MLX_VL15	= 1 << 17,
-	MLX4_WQE_MLX_SLR	= 1 << 16
+	MLX4_WQE_MLX_SLR	= 1 << 16,
+	MLX4_WQE_MLX_ICRC	= 1 << 4
 };
 
 struct mlx4_wqe_mlx_seg {
@@ -219,7 +271,8 @@ struct mlx4_wqe_datagram_seg {
 	__be32			av[8];
 	__be32			dqpn;
 	__be32			qkey;
-	__be32			reservd[2];
+	__be16			vlan;
+	u8			mac[6];
 };
 
 struct mlx4_wqe_lso_seg {
@@ -285,6 +338,13 @@ struct mlx4_wqe_atomic_seg {
 	__be64			compare;
 };
 
+struct mlx4_wqe_masked_atomic_seg {
+	__be64			swap_add;
+	__be64			compare;
+	__be64			swap_add_mask;
+	__be64			compare_mask;
+};
+
 struct mlx4_wqe_data_seg {
 	__be32			byte_count;
 	__be32			lkey;
@@ -293,6 +353,7 @@ struct mlx4_wqe_data_seg {
 
 enum {
 	MLX4_INLINE_ALIGN	= 64,
+	MLX4_INLINE_SEG		= 1 << 31,
 };
 
 struct mlx4_wqe_inline_seg {
@@ -316,6 +377,9 @@ static inline struct mlx4_qp *__mlx4_qp_
 	return radix_tree_lookup(&dev->qp_table_tree, qpn & (dev->caps.num_qps - 1));
 }
 
+struct mlx4_qp *mlx4_qp_lookup_lock(struct mlx4_dev *dev, u32 qpn);
 void mlx4_qp_remove(struct mlx4_dev *dev, struct mlx4_qp *qp);
+int mlx4_qp_get_region(struct mlx4_dev *dev, enum mlx4_qp_region region,
+			int *base_qpn, int *cnt);
 
 #endif /* MLX4_QP_H */
diff -r c23d1fc7e422 include/linux/mlx4/srq.h
--- a/include/linux/mlx4/srq.h
+++ b/include/linux/mlx4/srq.h
@@ -33,10 +33,22 @@
 #ifndef MLX4_SRQ_H
 #define MLX4_SRQ_H
 
+#include <linux/types.h>
+#include <linux/mlx4/device.h>
+
 struct mlx4_wqe_srq_next_seg {
 	u16			reserved1;
 	__be16			next_wqe_index;
 	u32			reserved2[3];
 };
 
+void mlx4_srq_invalidate(struct mlx4_dev *dev, struct mlx4_srq *srq);
+void mlx4_srq_remove(struct mlx4_dev *dev, struct mlx4_srq *srq);
+
+static inline struct mlx4_srq *__mlx4_srq_lookup(struct mlx4_dev *dev, u32 srqn)
+{
+	return radix_tree_lookup(&dev->srq_table_tree,
+				 srqn & (dev->caps.num_srqs - 1));
+}
+
 #endif /* MLX4_SRQ_H */
