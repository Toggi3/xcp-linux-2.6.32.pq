diff -r 2e49d550e208 arch/x86/mm/fault-xen.c
--- a/arch/x86/mm/fault-xen.c	Mon Oct 24 17:33:09 2011 +0100
+++ b/arch/x86/mm/fault-xen.c	Mon Oct 24 17:36:07 2011 +0100
@@ -175,6 +175,9 @@ force_sig_info_fault(int si_signo, int s
 DEFINE_SPINLOCK(pgd_lock);
 LIST_HEAD(pgd_list);
 
+#define pgd_page_table(what, pg) \
+	spin_##what(&((struct mm_struct *)(pg)->private)->page_table_lock)
+
 #ifdef CONFIG_X86_32
 static inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)
 {
@@ -235,7 +238,13 @@ void vmalloc_sync_all(void)
 
 		spin_lock(&pgd_lock);
 		list_for_each_entry(page, &pgd_list, lru) {
-			if (!vmalloc_sync_one(page_address(page), address))
+			pmd_t *pmd;
+
+			pgd_page_table(lock, page);
+			pmd = vmalloc_sync_one(page_address(page), address);
+			pgd_page_table(unlock, page);
+
+			if (!pmd)
 				break;
 		}
 		spin_unlock(&pgd_lock);
@@ -348,10 +357,13 @@ void vmalloc_sync_all(void)
 		list_for_each_entry(page, &pgd_list, lru) {
 			pgd_t *pgd;
 			pgd = (pgd_t *)page_address(page) + pgd_index(address);
+
+			pgd_page_table(lock, page);
 			if (pgd_none(*pgd))
 				set_pgd(pgd, *pgd_ref);
 			else
 				BUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));
+			pgd_page_table(unlock, page);
 		}
 		spin_unlock(&pgd_lock);
 	}
diff -r 2e49d550e208 arch/x86/mm/pgtable-xen.c
--- a/arch/x86/mm/pgtable-xen.c	Mon Oct 24 17:33:09 2011 +0100
+++ b/arch/x86/mm/pgtable-xen.c	Mon Oct 24 17:36:07 2011 +0100
@@ -741,6 +741,9 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 	pgd_ctor(pgd);
 	pgd_prepopulate_pmd(mm, pgd, pmds);
 
+	/* Store a back link for vmalloc_sync_all(). */
+	virt_to_page(pgd)->private = (unsigned long)mm;
+
 	spin_unlock(&pgd_lock);
 
 	return pgd;
