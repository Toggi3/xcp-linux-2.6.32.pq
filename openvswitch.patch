
Upstream repository: git://openvswitch.org/openvswitch

cp -v ../openvswitch.git/include/openflow/*.h include/openflow/
cp -v ../openvswitch.git/include/openvswitch/*.h include/openvswitch/
cp -v ../openvswitch.git/datapath/*.[ch] net/openvswitch/
hg add|remove as appropriate.
* * *
* * *

diff -r 89e197c6e9d5 include/linux/compiler.h
--- a/include/linux/compiler.h
+++ b/include/linux/compiler.h
@@ -280,4 +280,12 @@
  */
 #define ACCESS_ONCE(x) (*(volatile typeof(x) *)&(x))
 
+#ifndef __percpu
+#define __percpu
+#endif
+
+#ifndef __rcu
+#define __rcu
+#endif
+
 #endif /* __LINUX_COMPILER_H */
diff -r 89e197c6e9d5 include/linux/if.h
--- a/include/linux/if.h
+++ b/include/linux/if.h
@@ -98,6 +98,8 @@
 #define IF_PROTO_FR_ETH_PVC 0x200B
 #define IF_PROTO_RAW    0x200C          /* RAW Socket                   */
 
+#define IFF_OVS_DATAPATH 0              /* no-op flag */
+
 /* RFC 2863 operational status */
 enum {
 	IF_OPER_UNKNOWN,
diff -r 89e197c6e9d5 include/linux/if_link.h
--- a/include/linux/if_link.h
+++ b/include/linux/if_link.h
@@ -1,10 +1,46 @@
 #ifndef _LINUX_IF_LINK_H
 #define _LINUX_IF_LINK_H
 
+#include <linux/version.h>
 #include <linux/types.h>
 #include <linux/netlink.h>
 
-/* The struct should be in sync with struct net_device_stats */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,35)
+/* The main device statistics structure */
+struct rtnl_link_stats64 {
+	__u64	rx_packets;		/* total packets received	*/
+	__u64	tx_packets;		/* total packets transmitted	*/
+	__u64	rx_bytes;		/* total bytes received 	*/
+	__u64	tx_bytes;		/* total bytes transmitted	*/
+	__u64	rx_errors;		/* bad packets received		*/
+	__u64	tx_errors;		/* packet transmit problems	*/
+	__u64	rx_dropped;		/* no space in linux buffers	*/
+	__u64	tx_dropped;		/* no space available in linux	*/
+	__u64	multicast;		/* multicast packets received	*/
+	__u64	collisions;
+
+	/* detailed rx_errors: */
+	__u64	rx_length_errors;
+	__u64	rx_over_errors;		/* receiver ring buff overflow	*/
+	__u64	rx_crc_errors;		/* recved pkt with crc error	*/
+	__u64	rx_frame_errors;	/* recv'd frame alignment error */
+	__u64	rx_fifo_errors;		/* recv'r fifo overrun		*/
+	__u64	rx_missed_errors;	/* receiver missed packet	*/
+
+	/* detailed tx_errors */
+	__u64	tx_aborted_errors;
+	__u64	tx_carrier_errors;
+	__u64	tx_fifo_errors;
+	__u64	tx_heartbeat_errors;
+	__u64	tx_window_errors;
+
+	/* for cslip etc */
+	__u64	rx_compressed;
+	__u64	tx_compressed;
+};
+#endif	/* linux kernel < 2.6.35 */
+
+
 struct rtnl_link_stats
 {
 	__u32	rx_packets;		/* total packets received	*/
@@ -236,3 +272,6 @@
 	__u32 tx_rate;
 };
 #endif /* _LINUX_IF_LINK_H */
+
+#include <linux/version.h>
+
diff -r 89e197c6e9d5 include/linux/if_vlan.h
--- a/include/linux/if_vlan.h
+++ b/include/linux/if_vlan.h
@@ -357,4 +357,14 @@
 	short vlan_qos;   
 };
 
+
+/* All of these were introduced in a single commit preceding 2.6.33, so
+ * presumably all of them or none of them are present. */
+#ifndef VLAN_PRIO_MASK
+#define VLAN_PRIO_MASK          0xe000 /* Priority Code Point */
+#define VLAN_PRIO_SHIFT         13
+#define VLAN_CFI_MASK           0x1000 /* Canonical Format Indicator */
+#define VLAN_TAG_PRESENT        VLAN_CFI_MASK
+#endif
+
 #endif /* !(_LINUX_IF_VLAN_H_) */
diff -r 89e197c6e9d5 include/linux/openvswitch.h
--- /dev/null
+++ b/include/linux/openvswitch.h
@@ -0,0 +1,488 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef _LINUX_OPENVSWITCH_H
+#define _LINUX_OPENVSWITCH_H 1
+
+#include <linux/types.h>
+
+/**
+ * struct ovs_header - header for OVS Generic Netlink messages.
+ * @dp_ifindex: ifindex of local port for datapath (0 to make a request not
+ * specific to a datapath).
+ *
+ * Attributes following the header are specific to a particular OVS Generic
+ * Netlink family, but all of the OVS families use this header.
+ */
+
+struct ovs_header {
+	int dp_ifindex;
+};
+
+/* Datapaths. */
+
+#define OVS_DATAPATH_FAMILY  "ovs_datapath"
+#define OVS_DATAPATH_MCGROUP "ovs_datapath"
+#define OVS_DATAPATH_VERSION 0x1
+
+enum ovs_datapath_cmd {
+	OVS_DP_CMD_UNSPEC,
+	OVS_DP_CMD_NEW,
+	OVS_DP_CMD_DEL,
+	OVS_DP_CMD_GET,
+	OVS_DP_CMD_SET
+};
+
+/**
+ * enum ovs_datapath_attr - attributes for %OVS_DP_* commands.
+ * @OVS_DP_ATTR_NAME: Name of the network device that serves as the "local
+ * port".  This is the name of the network device whose dp_ifindex is given in
+ * the &struct ovs_header.  Always present in notifications.  Required in
+ * %OVS_DP_NEW requests.  May be used as an alternative to specifying
+ * dp_ifindex in other requests (with a dp_ifindex of 0).
+ * @OVS_DP_ATTR_UPCALL_PID: The Netlink socket in userspace that is initially
+ * set on the datapath port (for OVS_ACTION_ATTR_MISS).  Only valid on
+ * %OVS_DP_CMD_NEW requests. A value of zero indicates that upcalls should
+ * not be sent.
+ * @OVS_DP_ATTR_STATS: Statistics about packets that have passed through the
+ * datapath.  Always present in notifications.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_DP_* commands.
+ */
+enum ovs_datapath_attr {
+	OVS_DP_ATTR_UNSPEC,
+	OVS_DP_ATTR_NAME,       /* name of dp_ifindex netdev */
+	OVS_DP_ATTR_UPCALL_PID, /* Netlink PID to receive upcalls */
+	OVS_DP_ATTR_STATS,      /* struct ovs_dp_stats */
+	__OVS_DP_ATTR_MAX
+};
+
+#define OVS_DP_ATTR_MAX (__OVS_DP_ATTR_MAX - 1)
+
+struct ovs_dp_stats {
+	__u64 n_hit;             /* Number of flow table matches. */
+	__u64 n_missed;          /* Number of flow table misses. */
+	__u64 n_lost;            /* Number of misses not sent to userspace. */
+	__u64 n_flows;           /* Number of flows present */
+};
+
+struct ovs_vport_stats {
+	__u64   rx_packets;		/* total packets received       */
+	__u64   tx_packets;		/* total packets transmitted    */
+	__u64   rx_bytes;		/* total bytes received         */
+	__u64   tx_bytes;		/* total bytes transmitted      */
+	__u64   rx_errors;		/* bad packets received         */
+	__u64   tx_errors;		/* packet transmit problems     */
+	__u64   rx_dropped;		/* no space in linux buffers    */
+	__u64   tx_dropped;		/* no space available in linux  */
+};
+
+/* Fixed logical ports. */
+#define OVSP_LOCAL      ((__u16)0)
+
+/* Packet transfer. */
+
+#define OVS_PACKET_FAMILY "ovs_packet"
+#define OVS_PACKET_VERSION 0x1
+
+enum ovs_packet_cmd {
+	OVS_PACKET_CMD_UNSPEC,
+
+	/* Kernel-to-user notifications. */
+	OVS_PACKET_CMD_MISS,    /* Flow table miss. */
+	OVS_PACKET_CMD_ACTION,  /* OVS_ACTION_ATTR_USERSPACE action. */
+
+	/* Userspace commands. */
+	OVS_PACKET_CMD_EXECUTE  /* Apply actions to a packet. */
+};
+
+/**
+ * enum ovs_packet_attr - attributes for %OVS_PACKET_* commands.
+ * @OVS_PACKET_ATTR_PACKET: Present for all notifications.  Contains the entire
+ * packet as received, from the start of the Ethernet header onward.  For
+ * %OVS_PACKET_CMD_ACTION, %OVS_PACKET_ATTR_PACKET reflects changes made by
+ * actions preceding %OVS_ACTION_ATTR_USERSPACE, but %OVS_PACKET_ATTR_KEY is
+ * the flow key extracted from the packet as originally received.
+ * @OVS_PACKET_ATTR_KEY: Present for all notifications.  Contains the flow key
+ * extracted from the packet as nested %OVS_KEY_ATTR_* attributes.  This allows
+ * userspace to adapt its flow setup strategy by comparing its notion of the
+ * flow key against the kernel's.
+ * @OVS_PACKET_ATTR_ACTIONS: Contains actions for the packet.  Used
+ * for %OVS_PACKET_CMD_EXECUTE.  It has nested %OVS_ACTION_ATTR_* attributes.
+ * @OVS_PACKET_ATTR_USERDATA: Present for an %OVS_PACKET_CMD_ACTION
+ * notification if the %OVS_ACTION_ATTR_USERSPACE action specified an
+ * %OVS_USERSPACE_ATTR_USERDATA attribute.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_PACKET_* commands.
+ */
+enum ovs_packet_attr {
+	OVS_PACKET_ATTR_UNSPEC,
+	OVS_PACKET_ATTR_PACKET,      /* Packet data. */
+	OVS_PACKET_ATTR_KEY,         /* Nested OVS_KEY_ATTR_* attributes. */
+	OVS_PACKET_ATTR_ACTIONS,     /* Nested OVS_ACTION_ATTR_* attributes. */
+	OVS_PACKET_ATTR_USERDATA,    /* u64 OVS_ACTION_ATTR_USERSPACE arg. */
+	__OVS_PACKET_ATTR_MAX
+};
+
+#define OVS_PACKET_ATTR_MAX (__OVS_PACKET_ATTR_MAX - 1)
+
+/* Virtual ports. */
+
+#define OVS_VPORT_FAMILY  "ovs_vport"
+#define OVS_VPORT_MCGROUP "ovs_vport"
+#define OVS_VPORT_VERSION 0x1
+
+enum ovs_vport_cmd {
+	OVS_VPORT_CMD_UNSPEC,
+	OVS_VPORT_CMD_NEW,
+	OVS_VPORT_CMD_DEL,
+	OVS_VPORT_CMD_GET,
+	OVS_VPORT_CMD_SET
+};
+
+enum ovs_vport_type {
+	OVS_VPORT_TYPE_UNSPEC,
+	OVS_VPORT_TYPE_NETDEV,   /* network device */
+	OVS_VPORT_TYPE_INTERNAL, /* network device implemented by datapath */
+	OVS_VPORT_TYPE_PATCH = 100, /* virtual tunnel connecting two vports */
+	OVS_VPORT_TYPE_GRE,      /* GRE tunnel */
+	OVS_VPORT_TYPE_CAPWAP,   /* CAPWAP tunnel */
+	__OVS_VPORT_TYPE_MAX
+};
+
+#define OVS_VPORT_TYPE_MAX (__OVS_VPORT_TYPE_MAX - 1)
+
+/**
+ * enum ovs_vport_attr - attributes for %OVS_VPORT_* commands.
+ * @OVS_VPORT_ATTR_PORT_NO: 32-bit port number within datapath.
+ * @OVS_VPORT_ATTR_TYPE: 32-bit %OVS_VPORT_TYPE_* constant describing the type
+ * of vport.
+ * @OVS_VPORT_ATTR_NAME: Name of vport.  For a vport based on a network device
+ * this is the name of the network device.  Maximum length %IFNAMSIZ-1 bytes
+ * plus a null terminator.
+ * @OVS_VPORT_ATTR_OPTIONS: Vport-specific configuration information.
+ * @OVS_VPORT_ATTR_UPCALL_PID: The Netlink socket in userspace that
+ * OVS_PACKET_CMD_MISS upcalls will be directed to for packets received on
+ * this port.  A value of zero indicates that upcalls should not be sent.
+ * @OVS_VPORT_ATTR_STATS: A &struct ovs_vport_stats giving statistics for
+ * packets sent or received through the vport.
+ * @OVS_VPORT_ATTR_ADDRESS: A 6-byte Ethernet address for the vport.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_VPORT_* commands.
+ *
+ * For %OVS_VPORT_CMD_NEW requests, the %OVS_VPORT_ATTR_TYPE and
+ * %OVS_VPORT_ATTR_NAME attributes are required.  %OVS_VPORT_ATTR_PORT_NO is
+ * optional; if not specified a free port number is automatically selected.
+ * Whether %OVS_VPORT_ATTR_OPTIONS is required or optional depends on the type
+ * of vport.  %OVS_VPORT_ATTR_STATS and %OVS_VPORT_ATTR_ADDRESS are optional,
+ * and other attributes are ignored.
+ *
+ * For other requests, if %OVS_VPORT_ATTR_NAME is specified then it is used to
+ * look up the vport to operate on; otherwise dp_idx from the &struct
+ * ovs_header plus %OVS_VPORT_ATTR_PORT_NO determine the vport.
+ */
+enum ovs_vport_attr {
+	OVS_VPORT_ATTR_UNSPEC,
+	OVS_VPORT_ATTR_PORT_NO,	/* u32 port number within datapath */
+	OVS_VPORT_ATTR_TYPE,	/* u32 OVS_VPORT_TYPE_* constant. */
+	OVS_VPORT_ATTR_NAME,	/* string name, up to IFNAMSIZ bytes long */
+	OVS_VPORT_ATTR_OPTIONS, /* nested attributes, varies by vport type */
+	OVS_VPORT_ATTR_UPCALL_PID, /* u32 Netlink PID to receive upcalls */
+	OVS_VPORT_ATTR_STATS,	/* struct ovs_vport_stats */
+	OVS_VPORT_ATTR_ADDRESS = 100, /* hardware address */
+	__OVS_VPORT_ATTR_MAX
+};
+
+#define OVS_VPORT_ATTR_MAX (__OVS_VPORT_ATTR_MAX - 1)
+
+/* OVS_VPORT_ATTR_OPTIONS attributes for patch vports. */
+enum {
+	OVS_PATCH_ATTR_UNSPEC,
+	OVS_PATCH_ATTR_PEER,	/* name of peer vport, as a string */
+	__OVS_PATCH_ATTR_MAX
+};
+
+#define OVS_PATCH_ATTR_MAX (__OVS_PATCH_ATTR_MAX - 1)
+
+/* Flows. */
+
+#define OVS_FLOW_FAMILY  "ovs_flow"
+#define OVS_FLOW_MCGROUP "ovs_flow"
+#define OVS_FLOW_VERSION 0x1
+
+enum ovs_flow_cmd {
+	OVS_FLOW_CMD_UNSPEC,
+	OVS_FLOW_CMD_NEW,
+	OVS_FLOW_CMD_DEL,
+	OVS_FLOW_CMD_GET,
+	OVS_FLOW_CMD_SET
+};
+
+struct ovs_flow_stats {
+	__u64 n_packets;         /* Number of matched packets. */
+	__u64 n_bytes;           /* Number of matched bytes. */
+};
+
+enum ovs_key_attr {
+	OVS_KEY_ATTR_UNSPEC,
+	OVS_KEY_ATTR_ENCAP,	/* Nested set of encapsulated attributes. */
+	OVS_KEY_ATTR_PRIORITY,  /* u32 skb->priority */
+	OVS_KEY_ATTR_IN_PORT,   /* u32 OVS dp port number */
+	OVS_KEY_ATTR_ETHERNET,  /* struct ovs_key_ethernet */
+	OVS_KEY_ATTR_VLAN,	/* be16 VLAN TCI */
+	OVS_KEY_ATTR_ETHERTYPE,	/* be16 Ethernet type */
+	OVS_KEY_ATTR_IPV4,      /* struct ovs_key_ipv4 */
+	OVS_KEY_ATTR_IPV6,      /* struct ovs_key_ipv6 */
+	OVS_KEY_ATTR_TCP,       /* struct ovs_key_tcp */
+	OVS_KEY_ATTR_UDP,       /* struct ovs_key_udp */
+	OVS_KEY_ATTR_ICMP,      /* struct ovs_key_icmp */
+	OVS_KEY_ATTR_ICMPV6,    /* struct ovs_key_icmpv6 */
+	OVS_KEY_ATTR_ARP,       /* struct ovs_key_arp */
+	OVS_KEY_ATTR_ND,        /* struct ovs_key_nd */
+	OVS_KEY_ATTR_TUN_ID = 63, /* be64 tunnel ID */
+	__OVS_KEY_ATTR_MAX
+};
+
+#define OVS_KEY_ATTR_MAX (__OVS_KEY_ATTR_MAX - 1)
+
+/**
+ * enum ovs_frag_type - IPv4 and IPv6 fragment type
+ * @OVS_FRAG_TYPE_NONE: Packet is not a fragment.
+ * @OVS_FRAG_TYPE_FIRST: Packet is a fragment with offset 0.
+ * @OVS_FRAG_TYPE_LATER: Packet is a fragment with nonzero offset.
+ *
+ * Used as the @ipv4_frag in &struct ovs_key_ipv4 and as @ipv6_frag &struct
+ * ovs_key_ipv6.
+ */
+enum ovs_frag_type {
+	OVS_FRAG_TYPE_NONE,
+	OVS_FRAG_TYPE_FIRST,
+	OVS_FRAG_TYPE_LATER,
+	__OVS_FRAG_TYPE_MAX
+};
+
+#define OVS_FRAG_TYPE_MAX (__OVS_FRAG_TYPE_MAX - 1)
+
+struct ovs_key_ethernet {
+	__u8	 eth_src[6];
+	__u8	 eth_dst[6];
+};
+
+struct ovs_key_ipv4 {
+	__be32 ipv4_src;
+	__be32 ipv4_dst;
+	__u8   ipv4_proto;
+	__u8   ipv4_tos;
+	__u8   ipv4_ttl;
+	__u8   ipv4_frag;	/* One of OVS_FRAG_TYPE_*. */
+};
+
+struct ovs_key_ipv6 {
+	__be32 ipv6_src[4];
+	__be32 ipv6_dst[4];
+	__be32 ipv6_label;	/* 20-bits in least-significant bits. */
+	__u8   ipv6_proto;
+	__u8   ipv6_tclass;
+	__u8   ipv6_hlimit;
+	__u8   ipv6_frag;	/* One of OVS_FRAG_TYPE_*. */
+};
+
+struct ovs_key_tcp {
+	__be16 tcp_src;
+	__be16 tcp_dst;
+};
+
+struct ovs_key_udp {
+	__be16 udp_src;
+	__be16 udp_dst;
+};
+
+struct ovs_key_icmp {
+	__u8 icmp_type;
+	__u8 icmp_code;
+};
+
+struct ovs_key_icmpv6 {
+	__u8 icmpv6_type;
+	__u8 icmpv6_code;
+};
+
+struct ovs_key_arp {
+	__be32 arp_sip;
+	__be32 arp_tip;
+	__be16 arp_op;
+	__u8   arp_sha[6];
+	__u8   arp_tha[6];
+};
+
+struct ovs_key_nd {
+	__u32 nd_target[4];
+	__u8  nd_sll[6];
+	__u8  nd_tll[6];
+};
+
+/**
+ * enum ovs_flow_attr - attributes for %OVS_FLOW_* commands.
+ * @OVS_FLOW_ATTR_KEY: Nested %OVS_KEY_ATTR_* attributes specifying the flow
+ * key.  Always present in notifications.  Required for all requests (except
+ * dumps).
+ * @OVS_FLOW_ATTR_ACTIONS: Nested %OVS_ACTION_ATTR_* attributes specifying
+ * the actions to take for packets that match the key.  Always present in
+ * notifications.  Required for %OVS_FLOW_CMD_NEW requests, optional for
+ * %OVS_FLOW_CMD_SET requests.
+ * @OVS_FLOW_ATTR_STATS: &struct ovs_flow_stats giving statistics for this
+ * flow.  Present in notifications if the stats would be nonzero.  Ignored in
+ * requests.
+ * @OVS_FLOW_ATTR_TCP_FLAGS: An 8-bit value giving the OR'd value of all of the
+ * TCP flags seen on packets in this flow.  Only present in notifications for
+ * TCP flows, and only if it would be nonzero.  Ignored in requests.
+ * @OVS_FLOW_ATTR_USED: A 64-bit integer giving the time, in milliseconds on
+ * the system monotonic clock, at which a packet was last processed for this
+ * flow.  Only present in notifications if a packet has been processed for this
+ * flow.  Ignored in requests.
+ * @OVS_FLOW_ATTR_CLEAR: If present in a %OVS_FLOW_CMD_SET request, clears the
+ * last-used time, accumulated TCP flags, and statistics for this flow.
+ * Otherwise ignored in requests.  Never present in notifications.
+ *
+ * These attributes follow the &struct ovs_header within the Generic Netlink
+ * payload for %OVS_FLOW_* commands.
+ */
+enum ovs_flow_attr {
+	OVS_FLOW_ATTR_UNSPEC,
+	OVS_FLOW_ATTR_KEY,       /* Sequence of OVS_KEY_ATTR_* attributes. */
+	OVS_FLOW_ATTR_ACTIONS,   /* Nested OVS_ACTION_ATTR_* attributes. */
+	OVS_FLOW_ATTR_STATS,     /* struct ovs_flow_stats. */
+	OVS_FLOW_ATTR_TCP_FLAGS, /* 8-bit OR'd TCP flags. */
+	OVS_FLOW_ATTR_USED,      /* u64 msecs last used in monotonic time. */
+	OVS_FLOW_ATTR_CLEAR,     /* Flag to clear stats, tcp_flags, used. */
+	__OVS_FLOW_ATTR_MAX
+};
+
+#define OVS_FLOW_ATTR_MAX (__OVS_FLOW_ATTR_MAX - 1)
+
+/**
+ * enum ovs_sample_attr - Attributes for %OVS_ACTION_ATTR_SAMPLE action.
+ * @OVS_SAMPLE_ATTR_PROBABILITY: 32-bit fraction of packets to sample with
+ * @OVS_ACTION_ATTR_SAMPLE.  A value of 0 samples no packets, a value of
+ * %UINT32_MAX samples all packets and intermediate values sample intermediate
+ * fractions of packets.
+ * @OVS_SAMPLE_ATTR_ACTIONS: Set of actions to execute in sampling event.
+ * Actions are passed as nested attributes.
+ *
+ * Executes the specified actions with the given probability on a per-packet
+ * basis.
+ */
+enum ovs_sample_attr {
+	OVS_SAMPLE_ATTR_UNSPEC,
+	OVS_SAMPLE_ATTR_PROBABILITY, /* u32 number */
+	OVS_SAMPLE_ATTR_ACTIONS,     /* Nested OVS_ACTION_ATTR_* attributes. */
+	__OVS_SAMPLE_ATTR_MAX,
+};
+
+#define OVS_SAMPLE_ATTR_MAX (__OVS_SAMPLE_ATTR_MAX - 1)
+
+/**
+ * enum ovs_userspace_attr - Attributes for %OVS_ACTION_ATTR_USERSPACE action.
+ * @OVS_USERSPACE_ATTR_PID: u32 Netlink PID to which the %OVS_PACKET_CMD_ACTION
+ * message should be sent.  Required.
+ * @OVS_USERSPACE_ATTR_USERDATA: If present, its u64 argument is copied to the
+ * %OVS_PACKET_CMD_ACTION message as %OVS_PACKET_ATTR_USERDATA,
+ */
+enum ovs_userspace_attr {
+	OVS_USERSPACE_ATTR_UNSPEC,
+	OVS_USERSPACE_ATTR_PID,	      /* u32 Netlink PID to receive upcalls. */
+	OVS_USERSPACE_ATTR_USERDATA,  /* u64 optional user-specified cookie. */
+	__OVS_USERSPACE_ATTR_MAX
+};
+
+#define OVS_USERSPACE_ATTR_MAX (__OVS_USERSPACE_ATTR_MAX - 1)
+
+/**
+ * struct ovs_action_push_vlan - %OVS_ACTION_ATTR_PUSH_VLAN action argument.
+ * @vlan_tpid: Tag protocol identifier (TPID) to push.
+ * @vlan_tci: Tag control identifier (TCI) to push.  The CFI bit must be set
+ * (but it will not be set in the 802.1Q header that is pushed).
+ *
+ * The @vlan_tpid value is typically %ETH_P_8021Q.  The only acceptable TPID
+ * values are those that the kernel module also parses as 802.1Q headers, to
+ * prevent %OVS_ACTION_ATTR_PUSH_VLAN followed by %OVS_ACTION_ATTR_POP_VLAN
+ * from having surprising results.
+ */
+struct ovs_action_push_vlan {
+	__be16 vlan_tpid;	/* 802.1Q TPID. */
+	__be16 vlan_tci;	/* 802.1Q TCI (VLAN ID and priority). */
+};
+
+/**
+ * enum ovs_action_attr - Action types.
+ *
+ * @OVS_ACTION_ATTR_OUTPUT: Output packet to port.
+ * @OVS_ACTION_ATTR_USERSPACE: Send packet to userspace according to nested
+ * %OVS_USERSPACE_ATTR_* attributes.
+ * @OVS_ACTION_ATTR_SET: Replaces the contents of an existing header.  The
+ * single nested %OVS_KEY_ATTR_* attribute specifies a header to modify and its
+ * value.
+ * @OVS_ACTION_ATTR_PUSH_VLAN: Push a new outermost 802.1Q header onto the
+ * packet.
+ * @OVS_ACTION_ATTR_POP_VLAN: Pop the outermost 802.1Q header off the packet.
+ * @OVS_ACTION_ATTR_SAMPLE: Probabilitically executes actions, as specified in
+ * the nested %OVS_SAMPLE_ATTR_* attributes.
+ *
+ * Only a single header can be set with a single %OVS_ACTION_ATTR_SET.  Not all
+ * fields within a header are modifiable, e.g. the IPv4 protocol and fragment
+ * type may not be changed.
+ */
+
+enum ovs_action_attr {
+	OVS_ACTION_ATTR_UNSPEC,
+	OVS_ACTION_ATTR_OUTPUT,	      /* u32 port number. */
+	OVS_ACTION_ATTR_USERSPACE,    /* Nested OVS_USERSPACE_ATTR_*. */
+	OVS_ACTION_ATTR_SET,          /* One nested OVS_KEY_ATTR_*. */
+	OVS_ACTION_ATTR_PUSH_VLAN,    /* struct ovs_action_push_vlan. */
+	OVS_ACTION_ATTR_POP_VLAN,     /* No argument. */
+	OVS_ACTION_ATTR_SAMPLE,       /* Nested OVS_SAMPLE_ATTR_*. */
+	__OVS_ACTION_ATTR_MAX
+};
+
+#define OVS_ACTION_ATTR_MAX (__OVS_ACTION_ATTR_MAX - 1)
+
+#endif /* _LINUX_OPENVSWITCH_H */
diff -r 89e197c6e9d5 include/linux/rtnetlink.h
--- a/include/linux/rtnetlink.h
+++ b/include/linux/rtnetlink.h
@@ -773,6 +773,54 @@
 	return table;
 }
 
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,34)
+#ifdef CONFIG_PROVE_LOCKING
+static inline int lockdep_rtnl_is_held(void)
+{
+        return 1;
+}
+#endif
+
+static inline int rcu_read_lock_held(void)
+{
+        return 1;
+}
+#endif
+
+#ifndef rcu_dereference_check
+#define rcu_dereference_check(p, c) rcu_dereference(p)
+#endif
+
+#ifndef rcu_dereference_protected
+#define rcu_dereference_protected(p, c) (p)
+#endif
+
+#ifndef rcu_dereference_rtnl
+/**
+ * rcu_dereference_rtnl - rcu_dereference with debug checking
+ * @p: The pointer to read, prior to dereferencing
+ *
+ * Do an rcu_dereference(p), but check caller either holds rcu_read_lock()
+ * or RTNL. Note : Please prefer rtnl_dereference() or rcu_dereference()
+ */
+#define rcu_dereference_rtnl(p)                                 \
+	rcu_dereference_check(p, rcu_read_lock_held() ||        \
+				 lockdep_rtnl_is_held())
+#endif
+
+#ifndef rtnl_dereference
+/**
+ * rtnl_dereference - fetch RCU pointer when updates are prevented by RTNL
+ * @p: The pointer to read, prior to dereferencing
+ *
+ * Return the value of the specified RCU-protected pointer, but omit
+ * both the smp_read_barrier_depends() and the ACCESS_ONCE(), because
+ * caller holds RTNL.
+ */
+#define rtnl_dereference(p)                                     \
+	rcu_dereference_protected(p, lockdep_rtnl_is_held())
+#endif
+
 #endif /* __KERNEL__ */
 
 
diff -r 89e197c6e9d5 include/linux/u64_stats_sync.h
--- /dev/null
+++ b/include/linux/u64_stats_sync.h
@@ -0,0 +1,147 @@
+#ifndef _LINUX_U64_STATS_SYNC_WRAPPER_H
+#define _LINUX_U64_STATS_SYNC_WRAPPER_H
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#include_next <linux/u64_stats_sync.h>
+#else
+
+/*
+ * To properly implement 64bits network statistics on 32bit and 64bit hosts,
+ * we provide a synchronization point, that is a noop on 64bit or UP kernels.
+ *
+ * Key points :
+ * 1) Use a seqcount on SMP 32bits, with low overhead.
+ * 2) Whole thing is a noop on 64bit arches or UP kernels.
+ * 3) Write side must ensure mutual exclusion or one seqcount update could
+ *    be lost, thus blocking readers forever.
+ *    If this synchronization point is not a mutex, but a spinlock or
+ *    spinlock_bh() or disable_bh() :
+ * 3.1) Write side should not sleep.
+ * 3.2) Write side should not allow preemption.
+ * 3.3) If applicable, interrupts should be disabled.
+ *
+ * 4) If reader fetches several counters, there is no guarantee the whole values
+ *    are consistent (remember point 1) : this is a noop on 64bit arches anyway)
+ *
+ * 5) readers are allowed to sleep or be preempted/interrupted : They perform
+ *    pure reads. But if they have to fetch many values, it's better to not allow
+ *    preemptions/interruptions to avoid many retries.
+ *
+ * 6) If counter might be written by an interrupt, readers should block interrupts.
+ *    (On UP, there is no seqcount_t protection, a reader allowing interrupts could
+ *     read partial values)
+ *
+ * 7) For softirq uses, readers can use u64_stats_fetch_begin_bh() and
+ *    u64_stats_fetch_retry_bh() helpers
+ *
+ * Usage :
+ *
+ * Stats producer (writer) should use following template granted it already got
+ * an exclusive access to counters (a lock is already taken, or per cpu
+ * data is used [in a non preemptable context])
+ *
+ *   spin_lock_bh(...) or other synchronization to get exclusive access
+ *   ...
+ *   u64_stats_update_begin(&stats->syncp);
+ *   stats->bytes64 += len; // non atomic operation
+ *   stats->packets64++;    // non atomic operation
+ *   u64_stats_update_end(&stats->syncp);
+ *
+ * While a consumer (reader) should use following template to get consistent
+ * snapshot for each variable (but no guarantee on several ones)
+ *
+ * u64 tbytes, tpackets;
+ * unsigned int start;
+ *
+ * do {
+ *         start = u64_stats_fetch_begin(&stats->syncp);
+ *         tbytes = stats->bytes64; // non atomic operation
+ *         tpackets = stats->packets64; // non atomic operation
+ * } while (u64_stats_fetch_retry(&stats->syncp, start));
+ *
+ *
+ * Example of use in drivers/net/loopback.c, using per_cpu containers,
+ * in BH disabled context.
+ */
+#include <linux/seqlock.h>
+
+struct u64_stats_sync {
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	seqcount_t	seq;
+#endif
+};
+
+static inline void u64_stats_update_begin(struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	write_seqcount_begin(&syncp->seq);
+#endif
+}
+
+static inline void u64_stats_update_end(struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	write_seqcount_end(&syncp->seq);
+#endif
+}
+
+static inline unsigned int u64_stats_fetch_begin(const struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_begin(&syncp->seq);
+#else
+#if BITS_PER_LONG==32
+	preempt_disable();
+#endif
+	return 0;
+#endif
+}
+
+static inline bool u64_stats_fetch_retry(const struct u64_stats_sync *syncp,
+					 unsigned int start)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_retry(&syncp->seq, start);
+#else
+#if BITS_PER_LONG==32
+	preempt_enable();
+#endif
+	return false;
+#endif
+}
+
+/*
+ * In case softirq handlers can update u64 counters, readers can use following helpers
+ * - SMP 32bit arches use seqcount protection, irq safe.
+ * - UP 32bit must disable BH.
+ * - 64bit have no problem atomically reading u64 values, irq safe.
+ */
+static inline unsigned int u64_stats_fetch_begin_bh(const struct u64_stats_sync *syncp)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_begin(&syncp->seq);
+#else
+#if BITS_PER_LONG==32
+	local_bh_disable();
+#endif
+	return 0;
+#endif
+}
+
+static inline bool u64_stats_fetch_retry_bh(const struct u64_stats_sync *syncp,
+					 unsigned int start)
+{
+#if BITS_PER_LONG==32 && defined(CONFIG_SMP)
+	return read_seqcount_retry(&syncp->seq, start);
+#else
+#if BITS_PER_LONG==32
+	local_bh_enable();
+#endif
+	return false;
+#endif
+}
+
+#endif /* Linux kernel < 2.6.36 */
+#endif /* _LINUX_U64_STATS_SYNC_WRAPPER_H */
diff -r 89e197c6e9d5 include/openflow/nicira-ext.h
--- /dev/null
+++ b/include/openflow/nicira-ext.h
@@ -0,0 +1,1789 @@
+/*
+ * Copyright (c) 2008, 2009, 2010, 2011 Nicira Networks
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef OPENFLOW_NICIRA_EXT_H
+#define OPENFLOW_NICIRA_EXT_H 1
+
+#include "openflow/openflow.h"
+#include "openvswitch/types.h"
+
+/* The following vendor extensions, proposed by Nicira Networks, are not yet
+ * standardized, so they are not included in openflow.h.  Some of them may be
+ * suitable for standardization; others we never expect to standardize. */
+
+#define NX_VENDOR_ID 0x00002320
+
+/* Nicira vendor-specific error messages extension.
+ *
+ * OpenFlow 1.0 has a set of predefined error types (OFPET_*) and codes (which
+ * are specific to each type).  It does not have any provision for
+ * vendor-specific error codes, and it does not even provide "generic" error
+ * codes that can apply to problems not anticipated by the OpenFlow
+ * specification authors.
+ *
+ * This extension attempts to address the problem by adding a generic "error
+ * vendor extension".  The extension works as follows: use NXET_VENDOR as type
+ * and NXVC_VENDOR_ERROR as code, followed by struct nx_vendor_error with
+ * vendor-specific details, followed by at least 64 bytes of the failed
+ * request.
+ *
+ * It would be better to have a type-specific vendor extension, e.g. so that
+ * OFPET_BAD_ACTION could be used with vendor-specific code values.  But
+ * OFPET_BAD_ACTION and most other standardized types already specify that
+ * their 'data' values are (the start of) the OpenFlow message being replied
+ * to, so there is no room to insert a vendor ID.
+ *
+ * Currently this extension is only implemented by Open vSwitch, but it seems
+ * like a reasonable candidate for future standardization.
+ */
+
+/* This is a random number to avoid accidental collision with any other
+ * vendor's extension. */
+#define NXET_VENDOR 0xb0c2
+
+/* ofp_error msg 'code' values for NXET_VENDOR. */
+enum nx_vendor_code {
+    NXVC_VENDOR_ERROR           /* 'data' contains struct nx_vendor_error. */
+};
+
+/* 'data' for 'type' == NXET_VENDOR, 'code' == NXVC_VENDOR_ERROR. */
+struct nx_vendor_error {
+    ovs_be32 vendor;            /* Vendor ID as in struct ofp_vendor_header. */
+    ovs_be16 type;              /* Vendor-defined type. */
+    ovs_be16 code;              /* Vendor-defined subtype. */
+    /* Followed by at least the first 64 bytes of the failed request. */
+};
+
+/* Specific Nicira extension error numbers.
+ *
+ * These are the "code" values used in nx_vendor_error.  So far, the "type"
+ * values in nx_vendor_error are the same as those in ofp_error_msg.  That is,
+ * at Nicira so far we've only needed additional vendor-specific 'code' values,
+ * so we're using the existing 'type' values to avoid having to invent new ones
+ * that duplicate the current ones' meanings. */
+
+/* Additional "code" values for OFPET_BAD_REQUEST. */
+enum nx_bad_request_code {
+/* Nicira Extended Match (NXM) errors. */
+
+    /* Generic error code used when there is an error in an NXM sent to the
+     * switch.  The switch may use one of the more specific error codes below,
+     * if there is an appropriate one, to simplify debugging, but it is not
+     * required to do so. */
+    NXBRC_NXM_INVALID = 0x100,
+
+    /* The nxm_type, or nxm_type taken in combination with nxm_hasmask or
+     * nxm_length or both, is invalid or not implemented. */
+    NXBRC_NXM_BAD_TYPE = 0x101,
+
+    /* Invalid nxm_value. */
+    NXBRC_NXM_BAD_VALUE = 0x102,
+
+    /* Invalid nxm_mask. */
+    NXBRC_NXM_BAD_MASK = 0x103,
+
+    /* A prerequisite was not met. */
+    NXBRC_NXM_BAD_PREREQ = 0x104,
+
+    /* A given nxm_type was specified more than once. */
+    NXBRC_NXM_DUP_TYPE = 0x105,
+
+/* Other errors. */
+
+    /* A request specified a nonexistent table ID.  (But NXFMFC_BAD_TABLE_ID is
+     * used instead, when it is appropriate, because that is such a special
+     * case.) */
+    NXBRC_BAD_TABLE_ID = 0x200,
+
+    /* NXT_ROLE_REQUEST specified an invalid role. */
+    NXBRC_BAD_ROLE = 0x201,
+
+    /* The in_port in an ofp_packet_out request is invalid. */
+    NXBRC_BAD_IN_PORT = 0x202
+};
+
+/* Additional "code" values for OFPET_FLOW_MOD_FAILED. */
+enum nx_flow_mod_failed_code {
+    /* Generic hardware error. */
+    NXFMFC_HARDWARE = 0x100,
+
+    /* A nonexistent table ID was specified in the "command" field of struct
+     * ofp_flow_mod, when the nxt_flow_mod_table_id extension is enabled. */
+    NXFMFC_BAD_TABLE_ID = 0x101
+};
+
+/* Nicira vendor requests and replies. */
+
+/* Header for Nicira vendor requests and replies. */
+struct nicira_header {
+    struct ofp_header header;
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be32 subtype;           /* One of NXT_* below. */
+};
+OFP_ASSERT(sizeof(struct nicira_header) == 16);
+
+/* Values for the 'subtype' member of struct nicira_header. */
+enum nicira_type {
+    /* No longer used. */
+    NXT_STATUS_REQUEST__OBSOLETE = 0,
+    NXT_STATUS_REPLY__OBSOLETE = 1,
+    NXT_ACT_SET_CONFIG__OBSOLETE = 2,
+    NXT_ACT_GET_CONFIG__OBSOLETE = 3,
+    NXT_COMMAND_REQUEST__OBSOLETE = 4,
+    NXT_COMMAND_REPLY__OBSOLETE = 5,
+    NXT_FLOW_END_CONFIG__OBSOLETE = 6,
+    NXT_FLOW_END__OBSOLETE = 7,
+    NXT_MGMT__OBSOLETE = 8,
+    NXT_TUN_ID_FROM_COOKIE__OBSOLETE = 9,
+
+    /* Controller role support.  The request body is struct nx_role_request.
+     * The reply echos the request. */
+    NXT_ROLE_REQUEST = 10,
+    NXT_ROLE_REPLY = 11,
+
+    /* Flexible flow specification (aka NXM = Nicira Extended Match). */
+    NXT_SET_FLOW_FORMAT = 12,   /* Set flow format. */
+    NXT_FLOW_MOD = 13,          /* Analogous to OFPT_FLOW_MOD. */
+    NXT_FLOW_REMOVED = 14,      /* Analogous to OFPT_FLOW_REMOVED. */
+
+    /* Use the upper 8 bits of the 'command' member in struct ofp_flow_mod to
+     * designate the table to which a flow is to be added?  See the big comment
+     * on struct nxt_flow_mod_table_id for more information. */
+    NXT_FLOW_MOD_TABLE_ID = 15
+};
+
+/* Header for Nicira vendor stats request and reply messages. */
+struct nicira_stats_msg {
+    struct ofp_vendor_stats_msg vsm; /* Vendor NX_VENDOR_ID. */
+    ovs_be32 subtype;           /* One of NXST_* below. */
+    uint8_t pad[4];             /* Align to 64-bits. */
+};
+OFP_ASSERT(sizeof(struct nicira_stats_msg) == 24);
+
+/* Values for the 'subtype' member of struct nicira_stats_msg. */
+enum nicira_stats_type {
+    /* Flexible flow specification (aka NXM = Nicira Extended Match). */
+    NXST_FLOW,                  /* Analogous to OFPST_FLOW. */
+    NXST_AGGREGATE              /* Analogous to OFPST_AGGREGATE. */
+};
+
+/* Fields to use when hashing flows. */
+enum nx_hash_fields {
+    /* Ethernet source address (NXM_OF_ETH_SRC) only. */
+    NX_HASH_FIELDS_ETH_SRC,
+
+    /* L2 through L4, symmetric across src/dst.  Specifically, each of the
+     * following fields, if present, is hashed (slashes separate symmetric
+     * pairs):
+     *
+     *  - NXM_OF_ETH_DST / NXM_OF_ETH_SRC
+     *  - NXM_OF_ETH_TYPE
+     *  - The VID bits from NXM_OF_VLAN_TCI, ignoring PCP and CFI.
+     *  - NXM_OF_IP_PROTO
+     *  - NXM_OF_IP_SRC / NXM_OF_IP_DST
+     *  - NXM_OF_TCP_SRC / NXM_OF_TCP_DST
+     */
+    NX_HASH_FIELDS_SYMMETRIC_L4
+};
+
+/* This command enables or disables an Open vSwitch extension that allows a
+ * controller to specify the OpenFlow table to which a flow should be added,
+ * instead of having the switch decide which table is most appropriate as
+ * required by OpenFlow 1.0.  By default, the extension is disabled.
+ *
+ * When this feature is enabled, Open vSwitch treats struct ofp_flow_mod's
+ * 16-bit 'command' member as two separate fields.  The upper 8 bits are used
+ * as the table ID, the lower 8 bits specify the command as usual.  A table ID
+ * of 0xff is treated like a wildcarded table ID.
+ *
+ * The specific treatment of the table ID depends on the type of flow mod:
+ *
+ *    - OFPFC_ADD: Given a specific table ID, the flow is always placed in that
+ *      table.  If an identical flow already exists in that table only, then it
+ *      is replaced.  If the flow cannot be placed in the specified table,
+ *      either because the table is full or because the table cannot support
+ *      flows of the given type, the switch replies with an
+ *      OFPFMFC_ALL_TABLES_FULL error.  (A controller can distinguish these
+ *      cases by comparing the current and maximum number of entries reported
+ *      in ofp_table_stats.)
+ *
+ *      If the table ID is wildcarded, the switch picks an appropriate table
+ *      itself.  If an identical flow already exist in the selected flow table,
+ *      then it is replaced.  The choice of table might depend on the flows
+ *      that are already in the switch; for example, if one table fills up then
+ *      the switch might fall back to another one.
+ *
+ *    - OFPFC_MODIFY, OFPFC_DELETE: Given a specific table ID, only flows
+ *      within that table are matched and modified or deleted.  If the table ID
+ *      is wildcarded, flows within any table may be matched and modified or
+ *      deleted.
+ *
+ *    - OFPFC_MODIFY_STRICT, OFPFC_DELETE_STRICT: Given a specific table ID,
+ *      only a flow within that table may be matched and modified or deleted.
+ *      If the table ID is wildcarded and exactly one flow within any table
+ *      matches, then it is modified or deleted; if flows in more than one
+ *      table match, then none is modified or deleted.
+ */
+struct nxt_flow_mod_table_id {
+    struct ofp_header header;
+    uint32_t vendor;            /* NX_VENDOR_ID. */
+    uint32_t subtype;           /* NXT_FLOW_MOD_TABLE_ID. */
+    uint8_t set;                /* Nonzero to enable, zero to disable. */
+    uint8_t pad[7];
+};
+OFP_ASSERT(sizeof(struct nxt_flow_mod_table_id) == 24);
+
+/* Configures the "role" of the sending controller.  The default role is:
+ *
+ *    - Other (NX_ROLE_OTHER), which allows the controller access to all
+ *      OpenFlow features.
+ *
+ * The other possible roles are a related pair:
+ *
+ *    - Master (NX_ROLE_MASTER) is equivalent to Other, except that there may
+ *      be at most one Master controller at a time: when a controller
+ *      configures itself as Master, any existing Master is demoted to the
+ *      Slave role.
+ *
+ *    - Slave (NX_ROLE_SLAVE) allows the controller read-only access to
+ *      OpenFlow features.  In particular attempts to modify the flow table
+ *      will be rejected with an OFPBRC_EPERM error.
+ *
+ *      Slave controllers do not receive OFPT_PACKET_IN or OFPT_FLOW_REMOVED
+ *      messages, but they do receive OFPT_PORT_STATUS messages.
+ */
+struct nx_role_request {
+    struct nicira_header nxh;
+    ovs_be32 role;              /* One of NX_ROLE_*. */
+};
+
+enum nx_role {
+    NX_ROLE_OTHER,              /* Default role, full access. */
+    NX_ROLE_MASTER,             /* Full access, at most one. */
+    NX_ROLE_SLAVE               /* Read-only access. */
+};
+
+/* Nicira vendor flow actions. */
+
+enum nx_action_subtype {
+    NXAST_SNAT__OBSOLETE,       /* No longer used. */
+    NXAST_RESUBMIT,             /* struct nx_action_resubmit */
+    NXAST_SET_TUNNEL,           /* struct nx_action_set_tunnel */
+    NXAST_DROP_SPOOFED_ARP__OBSOLETE,
+    NXAST_SET_QUEUE,            /* struct nx_action_set_queue */
+    NXAST_POP_QUEUE,            /* struct nx_action_pop_queue */
+    NXAST_REG_MOVE,             /* struct nx_action_reg_move */
+    NXAST_REG_LOAD,             /* struct nx_action_reg_load */
+    NXAST_NOTE,                 /* struct nx_action_note */
+    NXAST_SET_TUNNEL64,         /* struct nx_action_set_tunnel64 */
+    NXAST_MULTIPATH,            /* struct nx_action_multipath */
+    NXAST_AUTOPATH,             /* struct nx_action_autopath */
+    NXAST_BUNDLE,               /* struct nx_action_bundle */
+    NXAST_BUNDLE_LOAD,          /* struct nx_action_bundle */
+    NXAST_RESUBMIT_TABLE,       /* struct nx_action_resubmit */
+    NXAST_OUTPUT_REG,           /* struct nx_action_output_reg */
+    NXAST_LEARN,                /* struct nx_action_learn */
+    NXAST_EXIT                  /* struct nx_action_header */
+};
+
+/* Header for Nicira-defined actions. */
+struct nx_action_header {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_*. */
+    uint8_t pad[6];
+};
+OFP_ASSERT(sizeof(struct nx_action_header) == 16);
+
+/* Action structures for NXAST_RESUBMIT and NXAST_RESUBMIT_TABLE.
+ *
+ * These actions search one of the switch's flow tables:
+ *
+ *    - For NXAST_RESUBMIT_TABLE only, if the 'table' member is not 255, then
+ *      it specifies the table to search.
+ *
+ *    - Otherwise (for NXAST_RESUBMIT_TABLE with a 'table' of 255, or for
+ *      NXAST_RESUBMIT regardless of 'table'), it searches the current flow
+ *      table, that is, the OpenFlow flow table that contains the flow from
+ *      which this action was obtained.  If this action did not come from a
+ *      flow table (e.g. it came from an OFPT_PACKET_OUT message), then table 0
+ *      is the current table.
+ *
+ * The flow table lookup uses a flow that may be slightly modified from the
+ * original lookup:
+ *
+ *    - For NXAST_RESUBMIT, the 'in_port' member of struct nx_action_resubmit
+ *      is used as the flow's in_port.
+ *
+ *    - For NXAST_RESUBMIT_TABLE, if the 'in_port' member is not OFPP_IN_PORT,
+ *      then its value is used as the flow's in_port.  Otherwise, the original
+ *      in_port is used.
+ *
+ *    - If actions that modify the flow (e.g. OFPAT_SET_VLAN_VID) precede the
+ *      resubmit action, then the flow is updated with the new values.
+ *
+ * Following the lookup, the original in_port is restored.
+ *
+ * If the modified flow matched in the flow table, then the corresponding
+ * actions are executed.  Afterward, actions following the resubmit in the
+ * original set of actions, if any, are executed; any changes made to the
+ * packet (e.g. changes to VLAN) by secondary actions persist when those
+ * actions are executed, although the original in_port is restored.
+ *
+ * Resubmit actions may be used any number of times within a set of actions.
+ *
+ * Resubmit actions may nest to an implementation-defined depth.  Beyond this
+ * implementation-defined depth, further resubmit actions are simply ignored.
+ *
+ * NXAST_RESUBMIT ignores 'table' and 'pad'.  NXAST_RESUBMIT_TABLE requires
+ * 'pad' to be all-bits-zero.
+ *
+ * Open vSwitch 1.0.1 and earlier did not support recursion.  Open vSwitch
+ * before 1.2.90 did not support NXAST_RESUBMIT_TABLE.
+ */
+struct nx_action_resubmit {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_RESUBMIT. */
+    ovs_be16 in_port;               /* New in_port for checking flow table. */
+    uint8_t table;                  /* NXAST_RESUBMIT_TABLE: table to use. */
+    uint8_t pad[3];
+};
+OFP_ASSERT(sizeof(struct nx_action_resubmit) == 16);
+
+/* Action structure for NXAST_SET_TUNNEL.
+ *
+ * Sets the encapsulating tunnel ID to a 32-bit value.  The most-significant 32
+ * bits of the tunnel ID are set to 0. */
+struct nx_action_set_tunnel {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_SET_TUNNEL. */
+    uint8_t pad[2];
+    ovs_be32 tun_id;                /* Tunnel ID. */
+};
+OFP_ASSERT(sizeof(struct nx_action_set_tunnel) == 16);
+
+/* Action structure for NXAST_SET_TUNNEL64.
+ *
+ * Sets the encapsulating tunnel ID to a 64-bit value. */
+struct nx_action_set_tunnel64 {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_SET_TUNNEL64. */
+    uint8_t pad[6];
+    ovs_be64 tun_id;                /* Tunnel ID. */
+};
+OFP_ASSERT(sizeof(struct nx_action_set_tunnel64) == 24);
+
+/* Action structure for NXAST_SET_QUEUE.
+ *
+ * Set the queue that should be used when packets are output.  This is similar
+ * to the OpenFlow OFPAT_ENQUEUE action, but does not take the output port as
+ * an argument.  This allows the queue to be defined before the port is
+ * known. */
+struct nx_action_set_queue {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_SET_QUEUE. */
+    uint8_t pad[2];
+    ovs_be32 queue_id;              /* Where to enqueue packets. */
+};
+OFP_ASSERT(sizeof(struct nx_action_set_queue) == 16);
+
+/* Action structure for NXAST_POP_QUEUE.
+ *
+ * Restores the queue to the value it was before any NXAST_SET_QUEUE actions
+ * were used.  Only the original queue can be restored this way; no stack is
+ * maintained. */
+struct nx_action_pop_queue {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_POP_QUEUE. */
+    uint8_t pad[6];
+};
+OFP_ASSERT(sizeof(struct nx_action_pop_queue) == 16);
+
+/* Action structure for NXAST_REG_MOVE.
+ *
+ * Copies src[src_ofs:src_ofs+n_bits] to dst[dst_ofs:dst_ofs+n_bits], where
+ * a[b:c] denotes the bits within 'a' numbered 'b' through 'c' (not including
+ * bit 'c').  Bit numbering starts at 0 for the least-significant bit, 1 for
+ * the next most significant bit, and so on.
+ *
+ * 'src' and 'dst' are nxm_header values with nxm_hasmask=0.  (It doesn't make
+ * sense to use nxm_hasmask=1 because the action does not do any kind of
+ * matching; it uses the actual value of a field.)
+ *
+ * The following nxm_header values are potentially acceptable as 'src':
+ *
+ *   - NXM_OF_IN_PORT
+ *   - NXM_OF_ETH_DST
+ *   - NXM_OF_ETH_SRC
+ *   - NXM_OF_ETH_TYPE
+ *   - NXM_OF_VLAN_TCI
+ *   - NXM_OF_IP_TOS
+ *   - NXM_OF_IP_PROTO
+ *   - NXM_OF_IP_SRC
+ *   - NXM_OF_IP_DST
+ *   - NXM_OF_TCP_SRC
+ *   - NXM_OF_TCP_DST
+ *   - NXM_OF_UDP_SRC
+ *   - NXM_OF_UDP_DST
+ *   - NXM_OF_ICMP_TYPE
+ *   - NXM_OF_ICMP_CODE
+ *   - NXM_OF_ARP_OP
+ *   - NXM_OF_ARP_SPA
+ *   - NXM_OF_ARP_TPA
+ *   - NXM_NX_TUN_ID
+ *   - NXM_NX_ARP_SHA
+ *   - NXM_NX_ARP_THA
+ *   - NXM_NX_ICMPV6_TYPE
+ *   - NXM_NX_ICMPV6_CODE
+ *   - NXM_NX_ND_SLL
+ *   - NXM_NX_ND_TLL
+ *   - NXM_NX_REG(idx) for idx in the switch's accepted range.
+ *
+ * The following nxm_header values are potentially acceptable as 'dst':
+ *
+ *   - NXM_OF_ETH_DST
+ *   - NXM_OF_ETH_SRC
+ *   - NXM_OF_IP_TOS
+ *   - NXM_OF_IP_SRC
+ *   - NXM_OF_IP_DST
+ *   - NXM_OF_TCP_SRC
+ *   - NXM_OF_TCP_DST
+ *   - NXM_OF_UDP_SRC
+ *   - NXM_OF_UDP_DST
+ *     Modifying any of the above fields changes the corresponding packet
+ *     header.
+ *
+ *   - NXM_NX_REG(idx) for idx in the switch's accepted range.
+ *
+ *   - NXM_OF_VLAN_TCI.  Modifying this field's value has side effects on the
+ *     packet's 802.1Q header.  Setting a value with CFI=0 removes the 802.1Q
+ *     header (if any), ignoring the other bits.  Setting a value with CFI=1
+ *     adds or modifies the 802.1Q header appropriately, setting the TCI field
+ *     to the field's new value (with the CFI bit masked out).
+ *
+ *   - NXM_NX_TUN_ID.  Modifying this value modifies the tunnel ID used for the
+ *     packet's next tunnel encapsulation.
+ *
+ * A given nxm_header value may be used as 'src' or 'dst' only on a flow whose
+ * nx_match satisfies its prerequisites.  For example, NXM_OF_IP_TOS may be
+ * used only if the flow's nx_match includes an nxm_entry that specifies
+ * nxm_type=NXM_OF_ETH_TYPE, nxm_hasmask=0, and nxm_value=0x0800.
+ *
+ * The switch will reject actions for which src_ofs+n_bits is greater than the
+ * width of 'src' or dst_ofs+n_bits is greater than the width of 'dst' with
+ * error type OFPET_BAD_ACTION, code OFPBAC_BAD_ARGUMENT.
+ */
+struct nx_action_reg_move {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_REG_MOVE. */
+    ovs_be16 n_bits;                /* Number of bits. */
+    ovs_be16 src_ofs;               /* Starting bit offset in source. */
+    ovs_be16 dst_ofs;               /* Starting bit offset in destination. */
+    ovs_be32 src;                   /* Source register. */
+    ovs_be32 dst;                   /* Destination register. */
+};
+OFP_ASSERT(sizeof(struct nx_action_reg_move) == 24);
+
+/* Action structure for NXAST_REG_LOAD.
+ *
+ * Copies value[0:n_bits] to dst[ofs:ofs+n_bits], where a[b:c] denotes the bits
+ * within 'a' numbered 'b' through 'c' (not including bit 'c').  Bit numbering
+ * starts at 0 for the least-significant bit, 1 for the next most significant
+ * bit, and so on.
+ *
+ * 'dst' is an nxm_header with nxm_hasmask=0.  See the documentation for
+ * NXAST_REG_MOVE, above, for the permitted fields and for the side effects of
+ * loading them.
+ *
+ * The 'ofs' and 'n_bits' fields are combined into a single 'ofs_nbits' field
+ * to avoid enlarging the structure by another 8 bytes.  To allow 'n_bits' to
+ * take a value between 1 and 64 (inclusive) while taking up only 6 bits, it is
+ * also stored as one less than its true value:
+ *
+ *  15                           6 5                0
+ * +------------------------------+------------------+
+ * |              ofs             |    n_bits - 1    |
+ * +------------------------------+------------------+
+ *
+ * The switch will reject actions for which ofs+n_bits is greater than the
+ * width of 'dst', or in which any bits in 'value' with value 2**n_bits or
+ * greater are set to 1, with error type OFPET_BAD_ACTION, code
+ * OFPBAC_BAD_ARGUMENT.
+ */
+struct nx_action_reg_load {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_REG_LOAD. */
+    ovs_be16 ofs_nbits;             /* (ofs << 6) | (n_bits - 1). */
+    ovs_be32 dst;                   /* Destination register. */
+    ovs_be64 value;                 /* Immediate value. */
+};
+OFP_ASSERT(sizeof(struct nx_action_reg_load) == 24);
+
+/* Action structure for NXAST_NOTE.
+ *
+ * This action has no effect.  It is variable length.  The switch does not
+ * attempt to interpret the user-defined 'note' data in any way.  A controller
+ * can use this action to attach arbitrary metadata to a flow.
+ *
+ * This action might go away in the future.
+ */
+struct nx_action_note {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* A multiple of 8, but at least 16. */
+    ovs_be32 vendor;                /* NX_VENDOR_ID. */
+    ovs_be16 subtype;               /* NXAST_NOTE. */
+    uint8_t note[6];                /* Start of user-defined data. */
+    /* Possibly followed by additional user-defined data. */
+};
+OFP_ASSERT(sizeof(struct nx_action_note) == 16);
+
+/* Action structure for NXAST_MULTIPATH.
+ *
+ * This action performs the following steps in sequence:
+ *
+ *    1. Hashes the fields designated by 'fields', one of NX_HASH_FIELDS_*.
+ *       Refer to the definition of "enum nx_mp_fields" for details.
+ *
+ *       The 'basis' value is used as a universal hash parameter, that is,
+ *       different values of 'basis' yield different hash functions.  The
+ *       particular universal hash function used is implementation-defined.
+ *
+ *       The hashed fields' values are drawn from the current state of the
+ *       flow, including all modifications that have been made by actions up to
+ *       this point.
+ *
+ *    2. Applies the multipath link choice algorithm specified by 'algorithm',
+ *       one of NX_MP_ALG_*.  Refer to the definition of "enum nx_mp_algorithm"
+ *       for details.
+ *
+ *       The output of the algorithm is 'link', an unsigned integer less than
+ *       or equal to 'max_link'.
+ *
+ *       Some algorithms use 'arg' as an additional argument.
+ *
+ *    3. Stores 'link' in dst[ofs:ofs+n_bits].  The format and semantics of
+ *       'dst' and 'ofs_nbits' are similar to those for the NXAST_REG_LOAD
+ *       action.
+ *
+ * The switch will reject actions that have an unknown 'fields', or an unknown
+ * 'algorithm', or in which ofs+n_bits is greater than the width of 'dst', or
+ * in which 'max_link' is greater than or equal to 2**n_bits, with error type
+ * OFPET_BAD_ACTION, code OFPBAC_BAD_ARGUMENT.
+ */
+struct nx_action_multipath {
+    ovs_be16 type;              /* OFPAT_VENDOR. */
+    ovs_be16 len;               /* Length is 32. */
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be16 subtype;           /* NXAST_MULTIPATH. */
+
+    /* What fields to hash and how. */
+    ovs_be16 fields;            /* One of NX_HASH_FIELDS_*. */
+    ovs_be16 basis;             /* Universal hash parameter. */
+    ovs_be16 pad0;
+
+    /* Multipath link choice algorithm to apply to hash value. */
+    ovs_be16 algorithm;         /* One of NX_MP_ALG_*. */
+    ovs_be16 max_link;          /* Number of output links, minus 1. */
+    ovs_be32 arg;               /* Algorithm-specific argument. */
+    ovs_be16 pad1;
+
+    /* Where to store the result. */
+    ovs_be16 ofs_nbits;         /* (ofs << 6) | (n_bits - 1). */
+    ovs_be32 dst;               /* Destination. */
+};
+OFP_ASSERT(sizeof(struct nx_action_multipath) == 32);
+
+/* NXAST_MULTIPATH: Multipath link choice algorithm to apply.
+ *
+ * In the descriptions below, 'n_links' is max_link + 1. */
+enum nx_mp_algorithm {
+    /* link = hash(flow) % n_links.
+     *
+     * Redistributes all traffic when n_links changes.  O(1) performance.  See
+     * RFC 2992.
+     *
+     * Use UINT16_MAX for max_link to get a raw hash value. */
+    NX_MP_ALG_MODULO_N,
+
+    /* link = hash(flow) / (MAX_HASH / n_links).
+     *
+     * Redistributes between one-quarter and one-half of traffic when n_links
+     * changes.  O(1) performance.  See RFC 2992.
+     */
+    NX_MP_ALG_HASH_THRESHOLD,
+
+    /* for i in [0,n_links):
+     *   weights[i] = hash(flow, i)
+     * link = { i such that weights[i] >= weights[j] for all j != i }
+     *
+     * Redistributes 1/n_links of traffic when n_links changes.  O(n_links)
+     * performance.  If n_links is greater than a threshold (currently 64, but
+     * subject to change), Open vSwitch will substitute another algorithm
+     * automatically.  See RFC 2992. */
+    NX_MP_ALG_HRW,              /* Highest Random Weight. */
+
+    /* i = 0
+     * repeat:
+     *     i = i + 1
+     *     link = hash(flow, i) % arg
+     * while link > max_link
+     *
+     * Redistributes 1/n_links of traffic when n_links changes.  O(1)
+     * performance when arg/max_link is bounded by a constant.
+     *
+     * Redistributes all traffic when arg changes.
+     *
+     * arg must be greater than max_link and for best performance should be no
+     * more than approximately max_link * 2.  If arg is outside the acceptable
+     * range, Open vSwitch will automatically substitute the least power of 2
+     * greater than max_link.
+     *
+     * This algorithm is specific to Open vSwitch.
+     */
+    NX_MP_ALG_ITER_HASH         /* Iterative Hash. */
+};
+
+/* Action structure for NXAST_LEARN.
+ *
+ * This action adds or modifies a flow in an OpenFlow table, similar to
+ * OFPT_FLOW_MOD with OFPFC_MODIFY_STRICT as 'command'.  The new flow has the
+ * specified idle timeout, hard timeout, priority, cookie, and flags.  The new
+ * flow's match criteria and actions are built by applying each of the series
+ * of flow_mod_spec elements included as part of the action.
+ *
+ * A flow_mod_spec starts with a 16-bit header.  A header that is all-bits-0 is
+ * a no-op used for padding the action as a whole to a multiple of 8 bytes in
+ * length.  Otherwise, the flow_mod_spec can be thought of as copying 'n_bits'
+ * bits from a source to a destination.  In this case, the header contains
+ * multiple fields:
+ *
+ *  15  14  13 12  11 10                              0
+ * +------+---+------+---------------------------------+
+ * |   0  |src|  dst |             n_bits              |
+ * +------+---+------+---------------------------------+
+ *
+ * The meaning and format of a flow_mod_spec depends on 'src' and 'dst'.  The
+ * following table summarizes the meaning of each possible combination.
+ * Details follow the table:
+ *
+ *   src dst  meaning
+ *   --- ---  ----------------------------------------------------------
+ *    0   0   Add match criteria based on value in a field.
+ *    1   0   Add match criteria based on an immediate value.
+ *    0   1   Add NXAST_REG_LOAD action to copy field into a different field.
+ *    1   1   Add NXAST_REG_LOAD action to load immediate value into a field.
+ *    0   2   Add OFPAT_OUTPUT action to output to port from specified field.
+ *   All other combinations are undefined and not allowed.
+ *
+ * The flow_mod_spec header is followed by a source specification and a
+ * destination specification.  The format and meaning of the source
+ * specification depends on 'src':
+ *
+ *   - If 'src' is 0, the source bits are taken from a field in the flow to
+ *     which this action is attached.  (This should be a wildcarded field.  If
+ *     its value is fully specified then the source bits being copied have
+ *     constant values.)
+ *
+ *     The source specification is an ovs_be32 'field' and an ovs_be16 'ofs'.
+ *     'field' is an nxm_header with nxm_hasmask=0, and 'ofs' the starting bit
+ *     offset within that field.  The source bits are field[ofs:ofs+n_bits-1].
+ *     'field' and 'ofs' are subject to the same restrictions as the source
+ *     field in NXAST_REG_MOVE.
+ *
+ *   - If 'src' is 1, the source bits are a constant value.  The source
+ *     specification is (n_bits+15)/16*2 bytes long.  Taking those bytes as a
+ *     number in network order, the source bits are the 'n_bits'
+ *     least-significant bits.  The switch will report an error if other bits
+ *     in the constant are nonzero.
+ *
+ * The flow_mod_spec destination specification, for 'dst' of 0 or 1, is an
+ * ovs_be32 'field' and an ovs_be16 'ofs'.  'field' is an nxm_header with
+ * nxm_hasmask=0 and 'ofs' is a starting bit offset within that field.  The
+ * meaning of the flow_mod_spec depends on 'dst':
+ *
+ *   - If 'dst' is 0, the flow_mod_spec specifies match criteria for the new
+ *     flow.  The new flow matches only if bits field[ofs:ofs+n_bits-1] in a
+ *     packet equal the source bits.  'field' may be any nxm_header with
+ *     nxm_hasmask=0 that is allowed in NXT_FLOW_MOD.
+ *
+ *     Order is significant.  Earlier flow_mod_specs must satisfy any
+ *     prerequisites for matching fields specified later, by copying constant
+ *     values into prerequisite fields.
+ *
+ *     The switch will reject flow_mod_specs that do not satisfy NXM masking
+ *     restrictions.
+ *
+ *   - If 'dst' is 1, the flow_mod_spec specifies an NXAST_REG_LOAD action for
+ *     the new flow.  The new flow copies the source bits into
+ *     field[ofs:ofs+n_bits-1].  Actions are executed in the same order as the
+ *     flow_mod_specs.
+ *
+ * The flow_mod_spec destination spec for 'dst' of 2 (when 'src' is 0) is
+ * empty.  It has the following meaning:
+ *
+ *   - The flow_mod_spec specifies an OFPAT_OUTPUT action for the new flow.
+ *     The new flow outputs to the OpenFlow port specified by the source field.
+ *     Of the special output ports with value OFPP_MAX or larger, OFPP_IN_PORT,
+ *     OFPP_FLOOD, OFPP_LOCAL, and OFPP_ALL are supported.  Other special ports
+ *     may not be used.
+ *
+ * Resource Management
+ * -------------------
+ *
+ * A switch has a finite amount of flow table space available for learning.
+ * When this space is exhausted, no new learning table entries will be learned
+ * until some existing flow table entries expire.  The controller should be
+ * prepared to handle this by flooding (which can be implemented as a
+ * low-priority flow).
+ *
+ * Examples
+ * --------
+ *
+ * The following examples give a prose description of the flow_mod_specs along
+ * with informal notation for how those would be represented and a hex dump of
+ * the bytes that would be required.
+ *
+ * These examples could work with various nx_action_learn parameters.  Typical
+ * values would be idle_timeout=OFP_FLOW_PERMANENT, hard_timeout=60,
+ * priority=OFP_DEFAULT_PRIORITY, flags=0, table_id=10.
+ *
+ * 1. Learn input port based on the source MAC, with lookup into
+ *    NXM_NX_REG1[16:31] by resubmit to in_port=99:
+ *
+ *    Match on in_port=99:
+ *       ovs_be16(src=1, dst=0, n_bits=16),               20 10
+ *       ovs_be16(99),                                    00 63
+ *       ovs_be32(NXM_OF_IN_PORT), ovs_be16(0)            00 00 00 02 00 00
+ *
+ *    Match Ethernet destination on Ethernet source from packet:
+ *       ovs_be16(src=0, dst=0, n_bits=48),               00 30
+ *       ovs_be32(NXM_OF_ETH_SRC), ovs_be16(0)            00 00 04 06 00 00
+ *       ovs_be32(NXM_OF_ETH_DST), ovs_be16(0)            00 00 02 06 00 00
+ *
+ *    Set NXM_NX_REG1[16:31] to the packet's input port:
+ *       ovs_be16(src=0, dst=1, n_bits=16),               08 10
+ *       ovs_be32(NXM_OF_IN_PORT), ovs_be16(0)            00 00 00 02 00 00
+ *       ovs_be32(NXM_NX_REG1), ovs_be16(16)              00 01 02 04 00 10
+ *
+ *    Given a packet that arrived on port A with Ethernet source address B,
+ *    this would set up the flow "in_port=99, dl_dst=B,
+ *    actions=load:A->NXM_NX_REG1[16..31]".
+ *
+ *    In syntax accepted by ovs-ofctl, this action is: learn(in_port=99,
+ *    NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],
+ *    load:NXM_OF_IN_PORT[]->NXM_NX_REG1[16..31])
+ *
+ * 2. Output to input port based on the source MAC and VLAN VID, with lookup
+ *    into NXM_NX_REG1[16:31]:
+ *
+ *    Match on same VLAN ID as packet:
+ *       ovs_be16(src=0, dst=0, n_bits=12),               00 0c
+ *       ovs_be32(NXM_OF_VLAN_TCI), ovs_be16(0)           00 00 08 02 00 00
+ *       ovs_be32(NXM_OF_VLAN_TCI), ovs_be16(0)           00 00 08 02 00 00
+ *
+ *    Match Ethernet destination on Ethernet source from packet:
+ *       ovs_be16(src=0, dst=0, n_bits=48),               00 30
+ *       ovs_be32(NXM_OF_ETH_SRC), ovs_be16(0)            00 00 04 06 00 00
+ *       ovs_be32(NXM_OF_ETH_DST), ovs_be16(0)            00 00 02 06 00 00
+ *
+ *    Output to the packet's input port:
+ *       ovs_be16(src=0, dst=2, n_bits=16),               10 10
+ *       ovs_be32(NXM_OF_IN_PORT), ovs_be16(0)            00 00 00 02 00 00
+ *
+ *    Given a packet that arrived on port A with Ethernet source address B in
+ *    VLAN C, this would set up the flow "dl_dst=B, vlan_vid=C,
+ *    actions=output:A".
+ *
+ *    In syntax accepted by ovs-ofctl, this action is:
+ *    learn(NXM_OF_VLAN_TCI[0..11], NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],
+ *    output:NXM_OF_IN_PORT[])
+ *
+ * 3. Here's a recipe for a very simple-minded MAC learning switch.  It uses a
+ *    10-second MAC expiration time to make it easier to see what's going on
+ *
+ *      ovs-vsctl del-controller br0
+ *      ovs-ofctl del-flows br0
+ *      ovs-ofctl add-flow br0 "table=0 actions=learn(table=1, \
+          hard_timeout=10, NXM_OF_VLAN_TCI[0..11],             \
+          NXM_OF_ETH_DST[]=NXM_OF_ETH_SRC[],                   \
+          output:NXM_OF_IN_PORT[]), resubmit(,1)"
+ *      ovs-ofctl add-flow br0 "table=1 priority=0 actions=flood"
+ *
+ *    You can then dump the MAC learning table with:
+ *
+ *      ovs-ofctl dump-flows br0 table=1
+ *
+ * Usage Advice
+ * ------------
+ *
+ * For best performance, segregate learned flows into a table that is not used
+ * for any other flows except possibly for a lowest-priority "catch-all" flow
+ * (a flow with no match criteria).  If different learning actions specify
+ * different match criteria, use different tables for the learned flows.
+ *
+ * The meaning of 'hard_timeout' and 'idle_timeout' can be counterintuitive.
+ * These timeouts apply to the flow that is added, which means that a flow with
+ * an idle timeout will expire when no traffic has been sent *to* the learned
+ * address.  This is not usually the intent in MAC learning; instead, we want
+ * the MAC learn entry to expire when no traffic has been sent *from* the
+ * learned address.  Use a hard timeout for that.
+ */
+struct nx_action_learn {
+    ovs_be16 type;              /* OFPAT_VENDOR. */
+    ovs_be16 len;               /* At least 24. */
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be16 subtype;           /* NXAST_LEARN. */
+    ovs_be16 idle_timeout;      /* Idle time before discarding (seconds). */
+    ovs_be16 hard_timeout;      /* Max time before discarding (seconds). */
+    ovs_be16 priority;          /* Priority level of flow entry. */
+    ovs_be64 cookie;            /* Cookie for new flow. */
+    ovs_be16 flags;             /* Either 0 or OFPFF_SEND_FLOW_REM. */
+    uint8_t table_id;           /* Table to insert flow entry. */
+    uint8_t pad[5];             /* Must be zero. */
+    /* Followed by a sequence of flow_mod_spec elements, as described above,
+     * until the end of the action is reached. */
+};
+OFP_ASSERT(sizeof(struct nx_action_learn) == 32);
+
+#define NX_LEARN_N_BITS_MASK    0x3ff
+
+#define NX_LEARN_SRC_FIELD     (0 << 13) /* Copy from field. */
+#define NX_LEARN_SRC_IMMEDIATE (1 << 13) /* Copy from immediate value. */
+#define NX_LEARN_SRC_MASK      (1 << 13)
+
+#define NX_LEARN_DST_MATCH     (0 << 11) /* Add match criterion. */
+#define NX_LEARN_DST_LOAD      (1 << 11) /* Add NXAST_REG_LOAD action. */
+#define NX_LEARN_DST_OUTPUT    (2 << 11) /* Add OFPAT_OUTPUT action. */
+#define NX_LEARN_DST_RESERVED  (3 << 11) /* Not yet defined. */
+#define NX_LEARN_DST_MASK      (3 << 11)
+
+/* Action structure for NXAST_AUTOPATH.
+ *
+ * This action performs the following steps in sequence:
+ *
+ *    1. Hashes the flow using an implementation-defined hash function.
+ *
+ *       The hashed fields' values are drawn from the current state of the
+ *       flow, including all modifications that have been made by actions up to
+ *       this point.
+ *
+ *    2. Selects an OpenFlow 'port'.
+ *
+ *       'port' is selected in an implementation-defined manner, taking into
+ *       account 'id' and the hash value calculated in step 1.
+ *
+ *       Generally a switch will have been configured with a set of ports that
+ *       may be chosen given 'id'.  The switch may take into account any number
+ *       of factors when choosing 'port' from its configured set.  Factors may
+ *       include carrier, load, and the results of configuration protocols such
+ *       as LACP.
+ *
+ *    3. Stores 'port' in dst[ofs:ofs+n_bits].
+ *
+ *       The format and semantics of 'dst' and 'ofs_nbits' are similar to those
+ *       for the NXAST_REG_LOAD action.
+ *
+ * The switch will reject actions in which ofs+n_bits is greater than the width
+ * of 'dst', with error type OFPET_BAD_ACTION, code OFPBAC_BAD_ARGUMENT.
+ */
+struct nx_action_autopath {
+    ovs_be16 type;              /* OFPAT_VENDOR. */
+    ovs_be16 len;               /* Length is 20. */
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be16 subtype;           /* NXAST_AUTOPATH. */
+
+    /* Where to store the result. */
+    ovs_be16 ofs_nbits;         /* (ofs << 6) | (n_bits - 1). */
+    ovs_be32 dst;               /* Destination. */
+
+    ovs_be32 id;                /* Autopath ID. */
+    ovs_be32 pad;
+};
+OFP_ASSERT(sizeof(struct nx_action_autopath) == 24);
+
+/* Action structure for NXAST_BUNDLE and NXAST_BUNDLE_LOAD.
+ *
+ * The bundle actions choose a slave from a supplied list of options.
+ * NXAST_BUNDLE outputs to its selection.  NXAST_BUNDLE_LOAD writes its
+ * selection to a register.
+ *
+ * The list of possible slaves follows the nx_action_bundle structure. The size
+ * of each slave is governed by its type as indicated by the 'slave_type'
+ * parameter. The list of slaves should be padded at its end with zeros to make
+ * the total length of the action a multiple of 8.
+ *
+ * Switches infer from the 'slave_type' parameter the size of each slave.  All
+ * implementations must support the NXM_OF_IN_PORT 'slave_type' which indicates
+ * that the slaves are OpenFlow port numbers with NXM_LENGTH(NXM_OF_IN_PORT) ==
+ * 2 byte width.  Switches should reject actions which indicate unknown or
+ * unsupported slave types.
+ *
+ * Switches use a strategy dictated by the 'algorithm' parameter to choose a
+ * slave.  If the switch does not support the specified 'algorithm' parameter,
+ * it should reject the action.
+ *
+ * Several algorithms take into account liveness when selecting slaves.  The
+ * liveness of a slave is implementation defined (with one exception), but will
+ * generally take into account things like its carrier status and the results
+ * of any link monitoring protocols which happen to be running on it.  In order
+ * to give controllers a place-holder value, the OFPP_NONE port is always
+ * considered live.
+ *
+ * Some slave selection strategies require the use of a hash function, in which
+ * case the 'fields' and 'basis' parameters should be populated.  The 'fields'
+ * parameter (one of NX_HASH_FIELDS_*) designates which parts of the flow to
+ * hash.  Refer to the definition of "enum nx_hash_fields" for details.  The
+ * 'basis' parameter is used as a universal hash parameter.  Different values
+ * of 'basis' yield different hash results.
+ *
+ * The 'zero' parameter at the end of the action structure is reserved for
+ * future use.  Switches are required to reject actions which have nonzero
+ * bytes in the 'zero' field.
+ *
+ * NXAST_BUNDLE actions should have 'ofs_nbits' and 'dst' zeroed.  Switches
+ * should reject actions which have nonzero bytes in either of these fields.
+ *
+ * NXAST_BUNDLE_LOAD stores the OpenFlow port number of the selected slave in
+ * dst[ofs:ofs+n_bits].  The format and semantics of 'dst' and 'ofs_nbits' are
+ * similar to those for the NXAST_REG_LOAD action. */
+struct nx_action_bundle {
+    ovs_be16 type;              /* OFPAT_VENDOR. */
+    ovs_be16 len;               /* Length including slaves. */
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be16 subtype;           /* NXAST_BUNDLE. */
+
+    /* Slave choice algorithm to apply to hash value. */
+    ovs_be16 algorithm;         /* One of NX_BD_ALG_*. */
+
+    /* What fields to hash and how. */
+    ovs_be16 fields;            /* One of NX_HASH_FIELDS_*. */
+    ovs_be16 basis;             /* Universal hash parameter. */
+
+    ovs_be32 slave_type;        /* NXM_OF_IN_PORT. */
+    ovs_be16 n_slaves;          /* Number of slaves. */
+
+    ovs_be16 ofs_nbits;         /* (ofs << 6) | (n_bits - 1). */
+    ovs_be32 dst;               /* Destination. */
+
+    uint8_t zero[4];            /* Reserved. Must be zero. */
+};
+OFP_ASSERT(sizeof(struct nx_action_bundle) == 32);
+
+/* NXAST_BUNDLE: Bundle slave choice algorithm to apply.
+ *
+ * In the descriptions below, 'slaves' is the list of possible slaves in the
+ * order they appear in the OpenFlow action. */
+enum nx_bd_algorithm {
+    /* Chooses the first live slave listed in the bundle.
+     *
+     * O(n_slaves) performance. */
+    NX_BD_ALG_ACTIVE_BACKUP,
+
+    /* for i in [0,n_slaves):
+     *   weights[i] = hash(flow, i)
+     * slave = { slaves[i] such that weights[i] >= weights[j] for all j != i }
+     *
+     * Redistributes 1/n_slaves of traffic when a slave's liveness changes.
+     * O(n_slaves) performance.
+     *
+     * Uses the 'fields' and 'basis' parameters. */
+    NX_BD_ALG_HRW /* Highest Random Weight. */
+};
+
+/* Action structure for NXAST_OUTPUT_REG.
+ *
+ * Outputs to the OpenFlow port number written to src[ofs:ofs+nbits].
+ *
+ * The format and semantics of 'src' and 'ofs_nbits' are similar to those for
+ * the NXAST_REG_LOAD action.
+ *
+ * The acceptable nxm_header values for 'src' are the same as the acceptable
+ * nxm_header values for the 'src' field of NXAST_REG_MOVE.
+ *
+ * The 'max_len' field indicates the number of bytes to send when the chosen
+ * port is OFPP_CONTROLLER.  Its semantics are equivalent to the 'max_len'
+ * field of OFPAT_OUTPUT.
+ *
+ * The 'zero' field is required to be zeroed for forward compatibility. */
+struct nx_action_output_reg {
+    ovs_be16 type;              /* OFPAT_VENDOR. */
+    ovs_be16 len;               /* 24. */
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be16 subtype;           /* NXAST_OUTPUT_REG. */
+
+    ovs_be16 ofs_nbits;         /* (ofs << 6) | (n_bits - 1). */
+    ovs_be32 src;               /* Source. */
+
+    ovs_be16 max_len;           /* Max length to send to controller. */
+
+    uint8_t zero[6];            /* Reserved, must be zero. */
+};
+OFP_ASSERT(sizeof(struct nx_action_output_reg) == 24);
+
+/* NXAST_EXIT
+ *
+ * Discontinues action processing.
+ *
+ * The NXAST_EXIT action causes the switch to immediately halt processing
+ * actions for the flow.  Any actions which have already been processed are
+ * executed by the switch.  However, any further actions, including those which
+ * may be in different tables, or different levels of the NXAST_RESUBMIT
+ * hierarchy, will be ignored.
+ *
+ * Uses the nx_action_header structure. */
+
+/* Flexible flow specifications (aka NXM = Nicira Extended Match).
+ *
+ * OpenFlow 1.0 has "struct ofp_match" for specifying flow matches.  This
+ * structure is fixed-length and hence difficult to extend.  This section
+ * describes a more flexible, variable-length flow match, called "nx_match" for
+ * short, that is also supported by Open vSwitch.  This section also defines a
+ * replacement for each OpenFlow message that includes struct ofp_match.
+ *
+ *
+ * Format
+ * ======
+ *
+ * An nx_match is a sequence of zero or more "nxm_entry"s, which are
+ * type-length-value (TLV) entries, each 5 to 259 (inclusive) bytes long.
+ * "nxm_entry"s are not aligned on or padded to any multibyte boundary.  The
+ * first 4 bytes of an nxm_entry are its "header", followed by the entry's
+ * "body".
+ *
+ * An nxm_entry's header is interpreted as a 32-bit word in network byte order:
+ *
+ * |<-------------------- nxm_type ------------------>|
+ * |                                                  |
+ * |31                              16 15            9| 8 7                0
+ * +----------------------------------+---------------+--+------------------+
+ * |            nxm_vendor            |   nxm_field   |hm|    nxm_length    |
+ * +----------------------------------+---------------+--+------------------+
+ *
+ * The most-significant 23 bits of the header are collectively "nxm_type".
+ * Bits 16...31 are "nxm_vendor", one of the NXM_VENDOR_* values below.  Bits
+ * 9...15 are "nxm_field", which is a vendor-specific value.  nxm_type normally
+ * designates a protocol header, such as the Ethernet type, but it can also
+ * refer to packet metadata, such as the switch port on which a packet arrived.
+ *
+ * Bit 8 is "nxm_hasmask" (labeled "hm" above for space reasons).  The meaning
+ * of this bit is explained later.
+ *
+ * The least-significant 8 bits are "nxm_length", a positive integer.  The
+ * length of the nxm_entry, including the header, is exactly 4 + nxm_length
+ * bytes.
+ *
+ * For a given nxm_vendor, nxm_field, and nxm_hasmask value, nxm_length is a
+ * constant.  It is included only to allow software to minimally parse
+ * "nxm_entry"s of unknown types.  (Similarly, for a given nxm_vendor,
+ * nxm_field, and nxm_length, nxm_hasmask is a constant.)
+ *
+ *
+ * Semantics
+ * =========
+ *
+ * A zero-length nx_match (one with no "nxm_entry"s) matches every packet.
+ *
+ * An nxm_entry places a constraint on the packets matched by the nx_match:
+ *
+ *   - If nxm_hasmask is 0, the nxm_entry's body contains a value for the
+ *     field, called "nxm_value".  The nx_match matches only packets in which
+ *     the field equals nxm_value.
+ *
+ *   - If nxm_hasmask is 1, then the nxm_entry's body contains a value for the
+ *     field (nxm_value), followed by a bitmask of the same length as the
+ *     value, called "nxm_mask".  For each 1-bit in position J in nxm_mask, the
+ *     nx_match matches only packets for which bit J in the given field's value
+ *     matches bit J in nxm_value.  A 0-bit in nxm_mask causes the
+ *     corresponding bits in nxm_value and the field's value to be ignored.
+ *     (The sense of the nxm_mask bits is the opposite of that used by the
+ *     "wildcards" member of struct ofp_match.)
+ *
+ *     When nxm_hasmask is 1, nxm_length is always even.
+ *
+ *     An all-zero-bits nxm_mask is equivalent to omitting the nxm_entry
+ *     entirely.  An all-one-bits nxm_mask is equivalent to specifying 0 for
+ *     nxm_hasmask.
+ *
+ * When there are multiple "nxm_entry"s, all of the constraints must be met.
+ *
+ *
+ * Mask Restrictions
+ * =================
+ *
+ * Masks may be restricted:
+ *
+ *   - Some nxm_types may not support masked wildcards, that is, nxm_hasmask
+ *     must always be 0 when these fields are specified.  For example, the
+ *     field that identifies the port on which a packet was received may not be
+ *     masked.
+ *
+ *   - Some nxm_types that do support masked wildcards may only support certain
+ *     nxm_mask patterns.  For example, fields that have IPv4 address values
+ *     may be restricted to CIDR masks.
+ *
+ * These restrictions should be noted in specifications for individual fields.
+ * A switch may accept an nxm_hasmask or nxm_mask value that the specification
+ * disallows, if the switch correctly implements support for that nxm_hasmask
+ * or nxm_mask value.  A switch must reject an attempt to set up a flow that
+ * contains a nxm_hasmask or nxm_mask value that it does not support.
+ *
+ *
+ * Prerequisite Restrictions
+ * =========================
+ *
+ * The presence of an nxm_entry with a given nxm_type may be restricted based
+ * on the presence of or values of other "nxm_entry"s.  For example:
+ *
+ *   - An nxm_entry for nxm_type=NXM_OF_IP_TOS is allowed only if it is
+ *     preceded by another entry with nxm_type=NXM_OF_ETH_TYPE, nxm_hasmask=0,
+ *     and nxm_value=0x0800.  That is, matching on the IP source address is
+ *     allowed only if the Ethernet type is explicitly set to IP.
+ *
+ *   - An nxm_entry for nxm_type=NXM_OF_TCP_SRC is allowed only if it is
+ *     preceded by an entry with nxm_type=NXM_OF_ETH_TYPE, nxm_hasmask=0, and
+ *     nxm_value either 0x0800 or 0x86dd, and another with
+ *     nxm_type=NXM_OF_IP_PROTO, nxm_hasmask=0, nxm_value=6, in that order.
+ *     That is, matching on the TCP source port is allowed only if the Ethernet
+ *     type is IP or IPv6 and the IP protocol is TCP.
+ *
+ * These restrictions should be noted in specifications for individual fields.
+ * A switch may implement relaxed versions of these restrictions.  A switch
+ * must reject an attempt to set up a flow that violates its restrictions.
+ *
+ *
+ * Ordering Restrictions
+ * =====================
+ *
+ * An nxm_entry that has prerequisite restrictions must appear after the
+ * "nxm_entry"s for its prerequisites.  Ordering of "nxm_entry"s within an
+ * nx_match is not otherwise constrained.
+ *
+ * Any given nxm_type may appear in an nx_match at most once.
+ *
+ *
+ * nxm_entry Examples
+ * ==================
+ *
+ * These examples show the format of a single nxm_entry with particular
+ * nxm_hasmask and nxm_length values.  The diagrams are labeled with field
+ * numbers and byte indexes.
+ *
+ *
+ * 8-bit nxm_value, nxm_hasmask=1, nxm_length=2:
+ *
+ *  0          3  4   5
+ * +------------+---+---+
+ * |   header   | v | m |
+ * +------------+---+---+
+ *
+ *
+ * 16-bit nxm_value, nxm_hasmask=0, nxm_length=2:
+ *
+ *  0          3 4    5
+ * +------------+------+
+ * |   header   | value|
+ * +------------+------+
+ *
+ *
+ * 32-bit nxm_value, nxm_hasmask=0, nxm_length=4:
+ *
+ *  0          3 4           7
+ * +------------+-------------+
+ * |   header   |  nxm_value  |
+ * +------------+-------------+
+ *
+ *
+ * 48-bit nxm_value, nxm_hasmask=0, nxm_length=6:
+ *
+ *  0          3 4                9
+ * +------------+------------------+
+ * |   header   |     nxm_value    |
+ * +------------+------------------+
+ *
+ *
+ * 48-bit nxm_value, nxm_hasmask=1, nxm_length=12:
+ *
+ *  0          3 4                9 10              15
+ * +------------+------------------+------------------+
+ * |   header   |     nxm_value    |      nxm_mask    |
+ * +------------+------------------+------------------+
+ *
+ *
+ * Error Reporting
+ * ===============
+ *
+ * A switch should report an error in an nx_match using error type
+ * OFPET_BAD_REQUEST and one of the NXBRC_NXM_* codes.  Ideally the switch
+ * should report a specific error code, if one is assigned for the particular
+ * problem, but NXBRC_NXM_INVALID is also available to report a generic
+ * nx_match error.
+ */
+
+#define NXM_HEADER__(VENDOR, FIELD, HASMASK, LENGTH) \
+    (((VENDOR) << 16) | ((FIELD) << 9) | ((HASMASK) << 8) | (LENGTH))
+#define NXM_HEADER(VENDOR, FIELD, LENGTH) \
+    NXM_HEADER__(VENDOR, FIELD, 0, LENGTH)
+#define NXM_HEADER_W(VENDOR, FIELD, LENGTH) \
+    NXM_HEADER__(VENDOR, FIELD, 1, (LENGTH) * 2)
+#define NXM_VENDOR(HEADER) ((HEADER) >> 16)
+#define NXM_FIELD(HEADER) (((HEADER) >> 9) & 0x7f)
+#define NXM_TYPE(HEADER) (((HEADER) >> 9) & 0x7fffff)
+#define NXM_HASMASK(HEADER) (((HEADER) >> 8) & 1)
+#define NXM_LENGTH(HEADER) ((HEADER) & 0xff)
+
+#define NXM_MAKE_WILD_HEADER(HEADER) \
+        NXM_HEADER_W(NXM_VENDOR(HEADER), NXM_FIELD(HEADER), NXM_LENGTH(HEADER))
+
+/* ## ------------------------------- ## */
+/* ## OpenFlow 1.0-compatible fields. ## */
+/* ## ------------------------------- ## */
+
+/* Physical or virtual port on which the packet was received.
+ *
+ * Prereqs: None.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_IN_PORT    NXM_HEADER  (0x0000,  0, 2)
+
+/* Source or destination address in Ethernet header.
+ *
+ * Prereqs: None.
+ *
+ * Format: 48-bit Ethernet MAC address.
+ *
+ * Masking: The nxm_mask patterns 01:00:00:00:00:00 and FE:FF:FF:FF:FF:FF must
+ *   be supported for NXM_OF_ETH_DST_W (as well as the trivial patterns that
+ *   are all-0-bits or all-1-bits).  Support for other patterns and for masking
+ *   of NXM_OF_ETH_SRC is optional. */
+#define NXM_OF_ETH_DST    NXM_HEADER  (0x0000,  1, 6)
+#define NXM_OF_ETH_DST_W  NXM_HEADER_W(0x0000,  1, 6)
+#define NXM_OF_ETH_SRC    NXM_HEADER  (0x0000,  2, 6)
+
+/* Packet's Ethernet type.
+ *
+ * For an Ethernet II packet this is taken from the Ethernet header.  For an
+ * 802.2 LLC+SNAP header with OUI 00-00-00 this is taken from the SNAP header.
+ * A packet that has neither format has value 0x05ff
+ * (OFP_DL_TYPE_NOT_ETH_TYPE).
+ *
+ * For a packet with an 802.1Q header, this is the type of the encapsulated
+ * frame.
+ *
+ * Prereqs: None.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_ETH_TYPE   NXM_HEADER  (0x0000,  3, 2)
+
+/* 802.1Q TCI.
+ *
+ * For a packet with an 802.1Q header, this is the Tag Control Information
+ * (TCI) field, with the CFI bit forced to 1.  For a packet with no 802.1Q
+ * header, this has value 0.
+ *
+ * Prereqs: None.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Arbitrary masks.
+ *
+ * This field can be used in various ways:
+ *
+ *   - If it is not constrained at all, the nx_match matches packets without
+ *     an 802.1Q header or with an 802.1Q header that has any TCI value.
+ *
+ *   - Testing for an exact match with 0 matches only packets without an
+ *     802.1Q header.
+ *
+ *   - Testing for an exact match with a TCI value with CFI=1 matches packets
+ *     that have an 802.1Q header with a specified VID and PCP.
+ *
+ *   - Testing for an exact match with a nonzero TCI value with CFI=0 does
+ *     not make sense.  The switch may reject this combination.
+ *
+ *   - Testing with a specific VID and CFI=1, with nxm_mask=0x1fff, matches
+ *     packets that have an 802.1Q header with that VID (and any PCP).
+ *
+ *   - Testing with a specific PCP and CFI=1, with nxm_mask=0xf000, matches
+ *     packets that have an 802.1Q header with that PCP (and any VID).
+ *
+ *   - Testing with nxm_value=0, nxm_mask=0x0fff matches packets with no 802.1Q
+ *     header or with an 802.1Q header with a VID of 0.
+ *
+ *   - Testing with nxm_value=0, nxm_mask=0xe000 matches packets with no 802.1Q
+ *     header or with an 802.1Q header with a PCP of 0.
+ *
+ *   - Testing with nxm_value=0, nxm_mask=0xefff matches packets with no 802.1Q
+ *     header or with an 802.1Q header with both VID and PCP of 0.
+ */
+#define NXM_OF_VLAN_TCI   NXM_HEADER  (0x0000,  4, 2)
+#define NXM_OF_VLAN_TCI_W NXM_HEADER_W(0x0000,  4, 2)
+
+/* The "type of service" byte of the IP header, with the ECN bits forced to 0.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *
+ * Format: 8-bit integer with 2 least-significant bits forced to 0.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_IP_TOS     NXM_HEADER  (0x0000,  5, 1)
+
+/* The "protocol" byte in the IP header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *
+ * Format: 8-bit integer.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_IP_PROTO   NXM_HEADER  (0x0000,  6, 1)
+
+/* The source or destination address in the IP header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x0800 exactly.
+ *
+ * Format: 32-bit integer in network byte order.
+ *
+ * Masking: Only CIDR masks are allowed, that is, masks that consist of N
+ *   high-order bits set to 1 and the other 32-N bits set to 0. */
+#define NXM_OF_IP_SRC     NXM_HEADER  (0x0000,  7, 4)
+#define NXM_OF_IP_SRC_W   NXM_HEADER_W(0x0000,  7, 4)
+#define NXM_OF_IP_DST     NXM_HEADER  (0x0000,  8, 4)
+#define NXM_OF_IP_DST_W   NXM_HEADER_W(0x0000,  8, 4)
+
+/* The source or destination port in the TCP header.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *   NXM_OF_IP_PROTO must match 6 exactly.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_TCP_SRC    NXM_HEADER  (0x0000,  9, 2)
+#define NXM_OF_TCP_DST    NXM_HEADER  (0x0000, 10, 2)
+
+/* The source or destination port in the UDP header.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match either 0x0800 or 0x86dd.
+ *   NXM_OF_IP_PROTO must match 17 exactly.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_UDP_SRC    NXM_HEADER  (0x0000, 11, 2)
+#define NXM_OF_UDP_DST    NXM_HEADER  (0x0000, 12, 2)
+
+/* The type or code in the ICMP header.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match 0x0800 exactly.
+ *   NXM_OF_IP_PROTO must match 1 exactly.
+ *
+ * Format: 8-bit integer.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_ICMP_TYPE  NXM_HEADER  (0x0000, 13, 1)
+#define NXM_OF_ICMP_CODE  NXM_HEADER  (0x0000, 14, 1)
+
+/* ARP opcode.
+ *
+ * For an Ethernet+IP ARP packet, the opcode in the ARP header.  Always 0
+ * otherwise.  Only ARP opcodes between 1 and 255 should be specified for
+ * matching.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x0806 exactly.
+ *
+ * Format: 16-bit integer in network byte order.
+ *
+ * Masking: Not maskable. */
+#define NXM_OF_ARP_OP     NXM_HEADER  (0x0000, 15, 2)
+
+/* For an Ethernet+IP ARP packet, the source or target protocol address
+ * in the ARP header.  Always 0 otherwise.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x0806 exactly.
+ *
+ * Format: 32-bit integer in network byte order.
+ *
+ * Masking: Only CIDR masks are allowed, that is, masks that consist of N
+ *   high-order bits set to 1 and the other 32-N bits set to 0. */
+#define NXM_OF_ARP_SPA    NXM_HEADER  (0x0000, 16, 4)
+#define NXM_OF_ARP_SPA_W  NXM_HEADER_W(0x0000, 16, 4)
+#define NXM_OF_ARP_TPA    NXM_HEADER  (0x0000, 17, 4)
+#define NXM_OF_ARP_TPA_W  NXM_HEADER_W(0x0000, 17, 4)
+
+/* ## ------------------------ ## */
+/* ## Nicira match extensions. ## */
+/* ## ------------------------ ## */
+
+/* Metadata registers.
+ *
+ * Registers initially have value 0.  Actions allow register values to be
+ * manipulated.
+ *
+ * Prereqs: None.
+ *
+ * Format: Array of 32-bit integer registers.  Space is reserved for up to
+ *   NXM_NX_MAX_REGS registers, but switches may implement fewer.
+ *
+ * Masking: Arbitrary masks. */
+#define NXM_NX_MAX_REGS 16
+#define NXM_NX_REG(IDX)   NXM_HEADER  (0x0001, IDX, 4)
+#define NXM_NX_REG_W(IDX) NXM_HEADER_W(0x0001, IDX, 4)
+#define NXM_NX_REG_IDX(HEADER) NXM_FIELD(HEADER)
+#define NXM_IS_NX_REG(HEADER) (!((((HEADER) ^ NXM_NX_REG0)) & 0xffffe1ff))
+#define NXM_IS_NX_REG_W(HEADER) (!((((HEADER) ^ NXM_NX_REG0_W)) & 0xffffe1ff))
+#define NXM_NX_REG0       NXM_HEADER  (0x0001, 0, 4)
+#define NXM_NX_REG0_W     NXM_HEADER_W(0x0001, 0, 4)
+#define NXM_NX_REG1       NXM_HEADER  (0x0001, 1, 4)
+#define NXM_NX_REG1_W     NXM_HEADER_W(0x0001, 1, 4)
+#define NXM_NX_REG2       NXM_HEADER  (0x0001, 2, 4)
+#define NXM_NX_REG2_W     NXM_HEADER_W(0x0001, 2, 4)
+#define NXM_NX_REG3       NXM_HEADER  (0x0001, 3, 4)
+#define NXM_NX_REG3_W     NXM_HEADER_W(0x0001, 3, 4)
+#define NXM_NX_REG4       NXM_HEADER  (0x0001, 4, 4)
+#define NXM_NX_REG4_W     NXM_HEADER_W(0x0001, 4, 4)
+
+/* Tunnel ID.
+ *
+ * For a packet received via GRE tunnel including a (32-bit) key, the key is
+ * stored in the low 32-bits and the high bits are zeroed.  For other packets,
+ * the value is 0.
+ *
+ * Prereqs: None.
+ *
+ * Format: 64-bit integer in network byte order.
+ *
+ * Masking: Arbitrary masks. */
+#define NXM_NX_TUN_ID     NXM_HEADER  (0x0001, 16, 8)
+#define NXM_NX_TUN_ID_W   NXM_HEADER_W(0x0001, 16, 8)
+
+/* For an Ethernet+IP ARP packet, the source or target hardware address
+ * in the ARP header.  Always 0 otherwise.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x0806 exactly.
+ *
+ * Format: 48-bit Ethernet MAC address.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_ARP_SHA    NXM_HEADER  (0x0001, 17, 6)
+#define NXM_NX_ARP_THA    NXM_HEADER  (0x0001, 18, 6)
+
+/* The source or destination address in the IPv6 header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *
+ * Format: 128-bit IPv6 address.
+ *
+ * Masking: Only CIDR masks are allowed, that is, masks that consist of N
+ *   high-order bits set to 1 and the other 128-N bits set to 0. */
+#define NXM_NX_IPV6_SRC    NXM_HEADER  (0x0001, 19, 16)
+#define NXM_NX_IPV6_SRC_W  NXM_HEADER_W(0x0001, 19, 16)
+#define NXM_NX_IPV6_DST    NXM_HEADER  (0x0001, 20, 16)
+#define NXM_NX_IPV6_DST_W  NXM_HEADER_W(0x0001, 20, 16)
+
+/* The type or code in the ICMPv6 header.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *   NXM_OF_IP_PROTO must match 58 exactly.
+ *
+ * Format: 8-bit integer.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_ICMPV6_TYPE NXM_HEADER  (0x0001, 21, 1)
+#define NXM_NX_ICMPV6_CODE NXM_HEADER  (0x0001, 22, 1)
+
+/* The target address in an IPv6 Neighbor Discovery message.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *   NXM_OF_IP_PROTO must match 58 exactly.
+ *   NXM_OF_ICMPV6_TYPE must be either 135 or 136.
+ *
+ * Format: 128-bit IPv6 address.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_ND_TARGET   NXM_HEADER  (0x0001, 23, 16)
+
+/* The source link-layer address option in an IPv6 Neighbor Discovery
+ * message.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *   NXM_OF_IP_PROTO must match 58 exactly.
+ *   NXM_OF_ICMPV6_TYPE must be exactly 135.
+ *
+ * Format: 48-bit Ethernet MAC address.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_ND_SLL      NXM_HEADER  (0x0001, 24, 6)
+
+/* The target link-layer address option in an IPv6 Neighbor Discovery
+ * message.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *   NXM_OF_IP_PROTO must match 58 exactly.
+ *   NXM_OF_ICMPV6_TYPE must be exactly 136.
+ *
+ * Format: 48-bit Ethernet MAC address.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_ND_TLL      NXM_HEADER  (0x0001, 25, 6)
+
+/* IP fragment information.
+ *
+ * Prereqs:
+ *   NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *
+ * Format: 8-bit value with one of the values 0, 1, or 3, as described below.
+ *
+ * Masking: Fully maskable.
+ *
+ * This field has three possible values:
+ *
+ *   - A packet that is not an IP fragment has value 0.
+ *
+ *   - A packet that is an IP fragment with offset 0 (the first fragment) has
+ *     bit 0 set and thus value 1.
+ *
+ *   - A packet that is an IP fragment with nonzero offset has bits 0 and 1 set
+ *     and thus value 3.
+ *
+ * NX_IP_FRAG_ANY and NX_IP_FRAG_LATER are declared to symbolically represent
+ * the meanings of bits 0 and 1.
+ *
+ * The switch may reject matches against values that can never appear.
+ *
+ * It is important to understand how this field interacts with the OpenFlow IP
+ * fragment handling mode:
+ *
+ *   - In OFPC_FRAG_DROP mode, the OpenFlow switch drops all IP fragments
+ *     before they reach the flow table, so every packet that is available for
+ *     matching will have value 0 in this field.
+ *
+ *   - Open vSwitch does not implement OFPC_FRAG_REASM mode, but if it did then
+ *     IP fragments would be reassembled before they reached the flow table and
+ *     again every packet available for matching would always have value 0.
+ *
+ *   - In OFPC_FRAG_NORMAL mode, all three values are possible, but OpenFlow
+ *     1.0 says that fragments' transport ports are always 0, even for the
+ *     first fragment, so this does not provide much extra information.
+ *
+ *   - In OFPC_FRAG_NX_MATCH mode, all three values are possible.  For
+ *     fragments with offset 0, Open vSwitch makes L4 header information
+ *     available.
+ */
+#define NXM_NX_IP_FRAG     NXM_HEADER  (0x0001, 26, 1)
+#define NXM_NX_IP_FRAG_W   NXM_HEADER_W(0x0001, 26, 1)
+
+/* Bits in the value of NXM_NX_IP_FRAG. */
+#define NX_IP_FRAG_ANY   (1 << 0) /* Is this a fragment? */
+#define NX_IP_FRAG_LATER (1 << 1) /* Is this a fragment with nonzero offset? */
+
+/* The flow label in the IPv6 header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must match 0x86dd exactly.
+ *
+ * Format: 20-bit IPv6 flow label in least-significant bits.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_IPV6_LABEL  NXM_HEADER  (0x0001, 27, 4)
+
+/* The ECN of the IP header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *
+ * Format: ECN in the low-order 2 bits.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_IP_ECN      NXM_HEADER  (0x0001, 28, 1)
+
+/* The time-to-live/hop limit of the IP header.
+ *
+ * Prereqs: NXM_OF_ETH_TYPE must be either 0x0800 or 0x86dd.
+ *
+ * Format: 8-bit integer.
+ *
+ * Masking: Not maskable. */
+#define NXM_NX_IP_TTL      NXM_HEADER  (0x0001, 29, 1)
+
+/* ## --------------------- ## */
+/* ## Requests and replies. ## */
+/* ## --------------------- ## */
+
+enum nx_flow_format {
+    NXFF_OPENFLOW10 = 0,         /* Standard OpenFlow 1.0 compatible. */
+    NXFF_NXM = 2                 /* Nicira extended match. */
+};
+
+/* NXT_SET_FLOW_FORMAT request. */
+struct nxt_set_flow_format {
+    struct ofp_header header;
+    ovs_be32 vendor;            /* NX_VENDOR_ID. */
+    ovs_be32 subtype;           /* NXT_SET_FLOW_FORMAT. */
+    ovs_be32 format;            /* One of NXFF_*. */
+};
+OFP_ASSERT(sizeof(struct nxt_set_flow_format) == 20);
+
+/* NXT_FLOW_MOD (analogous to OFPT_FLOW_MOD). */
+struct nx_flow_mod {
+    struct nicira_header nxh;
+    ovs_be64 cookie;              /* Opaque controller-issued identifier. */
+    ovs_be16 command;             /* One of OFPFC_*. */
+    ovs_be16 idle_timeout;        /* Idle time before discarding (seconds). */
+    ovs_be16 hard_timeout;        /* Max time before discarding (seconds). */
+    ovs_be16 priority;            /* Priority level of flow entry. */
+    ovs_be32 buffer_id;           /* Buffered packet to apply to (or -1).
+                                     Not meaningful for OFPFC_DELETE*. */
+    ovs_be16 out_port;            /* For OFPFC_DELETE* commands, require
+                                     matching entries to include this as an
+                                     output port.  A value of OFPP_NONE
+                                     indicates no restriction. */
+    ovs_be16 flags;               /* One of OFPFF_*. */
+    ovs_be16 match_len;           /* Size of nx_match. */
+    uint8_t pad[6];               /* Align to 64-bits. */
+    /* Followed by:
+     *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
+     *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
+     *     all-zero bytes, then
+     *   - Actions to fill out the remainder of the message length (always a
+     *     multiple of 8).
+     */
+};
+OFP_ASSERT(sizeof(struct nx_flow_mod) == 48);
+
+/* NXT_FLOW_REMOVED (analogous to OFPT_FLOW_REMOVED). */
+struct nx_flow_removed {
+    struct nicira_header nxh;
+    ovs_be64 cookie;          /* Opaque controller-issued identifier. */
+    ovs_be16 priority;        /* Priority level of flow entry. */
+    uint8_t reason;           /* One of OFPRR_*. */
+    uint8_t pad[1];           /* Align to 32-bits. */
+    ovs_be32 duration_sec;    /* Time flow was alive in seconds. */
+    ovs_be32 duration_nsec;   /* Time flow was alive in nanoseconds beyond
+                                 duration_sec. */
+    ovs_be16 idle_timeout;    /* Idle timeout from original flow mod. */
+    ovs_be16 match_len;       /* Size of nx_match. */
+    ovs_be64 packet_count;
+    ovs_be64 byte_count;
+    /* Followed by:
+     *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
+     *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
+     *     all-zero bytes. */
+};
+OFP_ASSERT(sizeof(struct nx_flow_removed) == 56);
+
+/* Nicira vendor stats request of type NXST_FLOW (analogous to OFPST_FLOW
+ * request). */
+struct nx_flow_stats_request {
+    struct nicira_stats_msg nsm;
+    ovs_be16 out_port;        /* Require matching entries to include this
+                                 as an output port.  A value of OFPP_NONE
+                                 indicates no restriction. */
+    ovs_be16 match_len;       /* Length of nx_match. */
+    uint8_t table_id;         /* ID of table to read (from ofp_table_stats)
+                                 or 0xff for all tables. */
+    uint8_t pad[3];           /* Align to 64 bits. */
+    /* Followed by:
+     *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
+     *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
+     *     all-zero bytes, which must also exactly fill out the length of the
+     *     message.
+     */
+};
+OFP_ASSERT(sizeof(struct nx_flow_stats_request) == 32);
+
+/* Body for Nicira vendor stats reply of type NXST_FLOW (analogous to
+ * OFPST_FLOW reply). */
+struct nx_flow_stats {
+    ovs_be16 length;          /* Length of this entry. */
+    uint8_t table_id;         /* ID of table flow came from. */
+    uint8_t pad;
+    ovs_be32 duration_sec;    /* Time flow has been alive in seconds. */
+    ovs_be32 duration_nsec;   /* Time flow has been alive in nanoseconds
+                                 beyond duration_sec. */
+    ovs_be16 priority;        /* Priority of the entry. Only meaningful
+                                 when this is not an exact-match entry. */
+    ovs_be16 idle_timeout;    /* Number of seconds idle before expiration. */
+    ovs_be16 hard_timeout;    /* Number of seconds before expiration. */
+    ovs_be16 match_len;       /* Length of nx_match. */
+    uint8_t pad2[4];          /* Align to 64 bits. */
+    ovs_be64 cookie;          /* Opaque controller-issued identifier. */
+    ovs_be64 packet_count;    /* Number of packets, UINT64_MAX if unknown. */
+    ovs_be64 byte_count;      /* Number of bytes, UINT64_MAX if unknown. */
+    /* Followed by:
+     *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
+     *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
+     *     all-zero bytes, then
+     *   - Actions to fill out the remainder 'length' bytes (always a multiple
+     *     of 8).
+     */
+};
+OFP_ASSERT(sizeof(struct nx_flow_stats) == 48);
+
+/* Nicira vendor stats request of type NXST_AGGREGATE (analogous to
+ * OFPST_AGGREGATE request). */
+struct nx_aggregate_stats_request {
+    struct nicira_stats_msg nsm;
+    ovs_be16 out_port;        /* Require matching entries to include this
+                                 as an output port.  A value of OFPP_NONE
+                                 indicates no restriction. */
+    ovs_be16 match_len;       /* Length of nx_match. */
+    uint8_t table_id;         /* ID of table to read (from ofp_table_stats)
+                                 or 0xff for all tables. */
+    uint8_t pad[3];           /* Align to 64 bits. */
+    /* Followed by:
+     *   - Exactly match_len (possibly 0) bytes containing the nx_match, then
+     *   - Exactly (match_len + 7)/8*8 - match_len (between 0 and 7) bytes of
+     *     all-zero bytes, which must also exactly fill out the length of the
+     *     message.
+     */
+};
+OFP_ASSERT(sizeof(struct nx_aggregate_stats_request) == 32);
+
+/* Body for nicira_stats_msg reply of type NXST_AGGREGATE (analogous to
+ * OFPST_AGGREGATE reply). */
+struct nx_aggregate_stats_reply {
+    struct nicira_stats_msg nsm;
+    ovs_be64 packet_count;     /* Number of packets, UINT64_MAX if unknown. */
+    ovs_be64 byte_count;       /* Number of bytes, UINT64_MAX if unknown. */
+    ovs_be32 flow_count;       /* Number of flows. */
+    uint8_t pad[4];            /* Align to 64 bits. */
+};
+OFP_ASSERT(sizeof(struct nx_aggregate_stats_reply) == 48);
+
+#endif /* openflow/nicira-ext.h */
diff -r 89e197c6e9d5 include/openflow/openflow.h
--- /dev/null
+++ b/include/openflow/openflow.h
@@ -0,0 +1,896 @@
+/*
+ * Copyright (c) 2008, 2009, 2010, 2011 Nicira Networks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/* OpenFlow: protocol between controller and datapath. */
+
+#ifndef OPENFLOW_OPENFLOW_H
+#define OPENFLOW_OPENFLOW_H 1
+
+#include "openvswitch/types.h"
+
+#ifdef SWIG
+#define OFP_ASSERT(EXPR)        /* SWIG can't handle OFP_ASSERT. */
+#elif !defined(__cplusplus)
+/* Build-time assertion for use in a declaration context. */
+#define OFP_ASSERT(EXPR)                                                \
+        extern int (*build_assert(void))[ sizeof(struct {               \
+                    unsigned int build_assert_failed : (EXPR) ? 1 : -1; })]
+#else /* __cplusplus */
+#include <boost/static_assert.hpp>
+#define OFP_ASSERT BOOST_STATIC_ASSERT
+#endif /* __cplusplus */
+
+/* Version number:
+ * Non-experimental versions released: 0x01
+ * Experimental versions released: 0x81 -- 0x99
+ */
+/* The most significant bit being set in the version field indicates an
+ * experimental OpenFlow version.
+ */
+#define OFP_VERSION   0x01
+
+#define OFP_MAX_TABLE_NAME_LEN 32
+#define OFP_MAX_PORT_NAME_LEN  16
+
+#define OFP_TCP_PORT  6633
+#define OFP_SSL_PORT  6633
+
+#define OFP_ETH_ALEN 6          /* Bytes in an Ethernet address. */
+
+/* Port numbering.  Physical ports are numbered starting from 1. */
+enum ofp_port {
+    /* Maximum number of physical switch ports. */
+    OFPP_MAX = 0xff00,
+
+    /* Fake output "ports". */
+    OFPP_IN_PORT    = 0xfff8,  /* Send the packet out the input port.  This
+                                  virtual port must be explicitly used
+                                  in order to send back out of the input
+                                  port. */
+    OFPP_TABLE      = 0xfff9,  /* Perform actions in flow table.
+                                  NB: This can only be the destination
+                                  port for packet-out messages. */
+    OFPP_NORMAL     = 0xfffa,  /* Process with normal L2/L3 switching. */
+    OFPP_FLOOD      = 0xfffb,  /* All physical ports except input port and
+                                  those disabled by STP. */
+    OFPP_ALL        = 0xfffc,  /* All physical ports except input port. */
+    OFPP_CONTROLLER = 0xfffd,  /* Send to controller. */
+    OFPP_LOCAL      = 0xfffe,  /* Local openflow "port". */
+    OFPP_NONE       = 0xffff   /* Not associated with a physical port. */
+};
+
+enum ofp_type {
+    /* Immutable messages. */
+    OFPT_HELLO,               /* Symmetric message */
+    OFPT_ERROR,               /* Symmetric message */
+    OFPT_ECHO_REQUEST,        /* Symmetric message */
+    OFPT_ECHO_REPLY,          /* Symmetric message */
+    OFPT_VENDOR,              /* Symmetric message */
+
+    /* Switch configuration messages. */
+    OFPT_FEATURES_REQUEST,    /* Controller/switch message */
+    OFPT_FEATURES_REPLY,      /* Controller/switch message */
+    OFPT_GET_CONFIG_REQUEST,  /* Controller/switch message */
+    OFPT_GET_CONFIG_REPLY,    /* Controller/switch message */
+    OFPT_SET_CONFIG,          /* Controller/switch message */
+
+    /* Asynchronous messages. */
+    OFPT_PACKET_IN,           /* Async message */
+    OFPT_FLOW_REMOVED,        /* Async message */
+    OFPT_PORT_STATUS,         /* Async message */
+
+    /* Controller command messages. */
+    OFPT_PACKET_OUT,          /* Controller/switch message */
+    OFPT_FLOW_MOD,            /* Controller/switch message */
+    OFPT_PORT_MOD,            /* Controller/switch message */
+
+    /* Statistics messages. */
+    OFPT_STATS_REQUEST,       /* Controller/switch message */
+    OFPT_STATS_REPLY,         /* Controller/switch message */
+
+    /* Barrier messages. */
+    OFPT_BARRIER_REQUEST,     /* Controller/switch message */
+    OFPT_BARRIER_REPLY,       /* Controller/switch message */
+
+    /* Queue Configuration messages. */
+    OFPT_QUEUE_GET_CONFIG_REQUEST,  /* Controller/switch message */
+    OFPT_QUEUE_GET_CONFIG_REPLY     /* Controller/switch message */
+};
+
+/* Header on all OpenFlow packets. */
+struct ofp_header {
+    uint8_t version;    /* OFP_VERSION. */
+    uint8_t type;       /* One of the OFPT_ constants. */
+    ovs_be16 length;    /* Length including this ofp_header. */
+    ovs_be32 xid;       /* Transaction id associated with this packet.
+                           Replies use the same id as was in the request
+                           to facilitate pairing. */
+};
+OFP_ASSERT(sizeof(struct ofp_header) == 8);
+
+/* OFPT_HELLO.  This message has an empty body, but implementations must
+ * ignore any data included in the body, to allow for future extensions. */
+struct ofp_hello {
+    struct ofp_header header;
+};
+
+#define OFP_DEFAULT_MISS_SEND_LEN   128
+
+enum ofp_config_flags {
+    /* Handling of IP fragments. */
+    OFPC_FRAG_NORMAL   = 0,  /* No special handling for fragments. */
+    OFPC_FRAG_DROP     = 1,  /* Drop fragments. */
+    OFPC_FRAG_REASM    = 2,  /* Reassemble (only if OFPC_IP_REASM set). */
+    OFPC_FRAG_NX_MATCH = 3,  /* Make first fragments available for matching. */
+    OFPC_FRAG_MASK     = 3
+};
+
+/* Switch configuration. */
+struct ofp_switch_config {
+    struct ofp_header header;
+    ovs_be16 flags;             /* OFPC_* flags. */
+    ovs_be16 miss_send_len;     /* Max bytes of new flow that datapath should
+                                   send to the controller. */
+};
+OFP_ASSERT(sizeof(struct ofp_switch_config) == 12);
+
+/* Capabilities supported by the datapath. */
+enum ofp_capabilities {
+    OFPC_FLOW_STATS     = 1 << 0,  /* Flow statistics. */
+    OFPC_TABLE_STATS    = 1 << 1,  /* Table statistics. */
+    OFPC_PORT_STATS     = 1 << 2,  /* Port statistics. */
+    OFPC_STP            = 1 << 3,  /* 802.1d spanning tree. */
+    OFPC_RESERVED       = 1 << 4,  /* Reserved, must not be set. */
+    OFPC_IP_REASM       = 1 << 5,  /* Can reassemble IP fragments. */
+    OFPC_QUEUE_STATS    = 1 << 6,  /* Queue statistics. */
+    OFPC_ARP_MATCH_IP   = 1 << 7   /* Match IP addresses in ARP
+                                      pkts. */
+};
+
+/* Flags to indicate behavior of the physical port.  These flags are
+ * used in ofp_phy_port to describe the current configuration.  They are
+ * used in the ofp_port_mod message to configure the port's behavior.
+ */
+enum ofp_port_config {
+    OFPPC_PORT_DOWN    = 1 << 0,  /* Port is administratively down. */
+
+    OFPPC_NO_STP       = 1 << 1,  /* Disable 802.1D spanning tree on port. */
+    OFPPC_NO_RECV      = 1 << 2,  /* Drop all packets except 802.1D
+                                     spanning tree packets. */
+    OFPPC_NO_RECV_STP  = 1 << 3,  /* Drop received 802.1D STP packets. */
+    OFPPC_NO_FLOOD     = 1 << 4,  /* Do not include this port when flooding. */
+    OFPPC_NO_FWD       = 1 << 5,  /* Drop packets forwarded to port. */
+    OFPPC_NO_PACKET_IN = 1 << 6   /* Do not send packet-in msgs for port. */
+};
+
+/* Current state of the physical port.  These are not configurable from
+ * the controller.
+ */
+enum ofp_port_state {
+    OFPPS_LINK_DOWN   = 1 << 0, /* No physical link present. */
+
+    /* The OFPPS_STP_* bits have no effect on switch operation.  The
+     * controller must adjust OFPPC_NO_RECV, OFPPC_NO_FWD, and
+     * OFPPC_NO_PACKET_IN appropriately to fully implement an 802.1D spanning
+     * tree. */
+    OFPPS_STP_LISTEN  = 0 << 8, /* Not learning or relaying frames. */
+    OFPPS_STP_LEARN   = 1 << 8, /* Learning but not relaying frames. */
+    OFPPS_STP_FORWARD = 2 << 8, /* Learning and relaying frames. */
+    OFPPS_STP_BLOCK   = 3 << 8, /* Not part of spanning tree. */
+    OFPPS_STP_MASK    = 3 << 8  /* Bit mask for OFPPS_STP_* values. */
+};
+
+/* Features of physical ports available in a datapath. */
+enum ofp_port_features {
+    OFPPF_10MB_HD    = 1 << 0,  /* 10 Mb half-duplex rate support. */
+    OFPPF_10MB_FD    = 1 << 1,  /* 10 Mb full-duplex rate support. */
+    OFPPF_100MB_HD   = 1 << 2,  /* 100 Mb half-duplex rate support. */
+    OFPPF_100MB_FD   = 1 << 3,  /* 100 Mb full-duplex rate support. */
+    OFPPF_1GB_HD     = 1 << 4,  /* 1 Gb half-duplex rate support. */
+    OFPPF_1GB_FD     = 1 << 5,  /* 1 Gb full-duplex rate support. */
+    OFPPF_10GB_FD    = 1 << 6,  /* 10 Gb full-duplex rate support. */
+    OFPPF_COPPER     = 1 << 7,  /* Copper medium. */
+    OFPPF_FIBER      = 1 << 8,  /* Fiber medium. */
+    OFPPF_AUTONEG    = 1 << 9,  /* Auto-negotiation. */
+    OFPPF_PAUSE      = 1 << 10, /* Pause. */
+    OFPPF_PAUSE_ASYM = 1 << 11  /* Asymmetric pause. */
+};
+
+/* Description of a physical port */
+struct ofp_phy_port {
+    ovs_be16 port_no;
+    uint8_t hw_addr[OFP_ETH_ALEN];
+    char name[OFP_MAX_PORT_NAME_LEN]; /* Null-terminated */
+
+    ovs_be32 config;        /* Bitmap of OFPPC_* flags. */
+    ovs_be32 state;         /* Bitmap of OFPPS_* flags. */
+
+    /* Bitmaps of OFPPF_* that describe features.  All bits zeroed if
+     * unsupported or unavailable. */
+    ovs_be32 curr;          /* Current features. */
+    ovs_be32 advertised;    /* Features being advertised by the port. */
+    ovs_be32 supported;     /* Features supported by the port. */
+    ovs_be32 peer;          /* Features advertised by peer. */
+};
+OFP_ASSERT(sizeof(struct ofp_phy_port) == 48);
+
+/* Switch features. */
+struct ofp_switch_features {
+    struct ofp_header header;
+    ovs_be64 datapath_id;   /* Datapath unique ID.  The lower 48-bits are for
+                               a MAC address, while the upper 16-bits are
+                               implementer-defined. */
+
+    ovs_be32 n_buffers;     /* Max packets buffered at once. */
+
+    uint8_t n_tables;       /* Number of tables supported by datapath. */
+    uint8_t pad[3];         /* Align to 64-bits. */
+
+    /* Features. */
+    ovs_be32 capabilities;  /* Bitmap of support "ofp_capabilities". */
+    ovs_be32 actions;       /* Bitmap of supported "ofp_action_type"s. */
+
+    /* Port info.*/
+    struct ofp_phy_port ports[0];  /* Port definitions.  The number of ports
+                                      is inferred from the length field in
+                                      the header. */
+};
+OFP_ASSERT(sizeof(struct ofp_switch_features) == 32);
+
+/* What changed about the physical port */
+enum ofp_port_reason {
+    OFPPR_ADD,              /* The port was added. */
+    OFPPR_DELETE,           /* The port was removed. */
+    OFPPR_MODIFY            /* Some attribute of the port has changed. */
+};
+
+/* A physical port has changed in the datapath */
+struct ofp_port_status {
+    struct ofp_header header;
+    uint8_t reason;          /* One of OFPPR_*. */
+    uint8_t pad[7];          /* Align to 64-bits. */
+    struct ofp_phy_port desc;
+};
+OFP_ASSERT(sizeof(struct ofp_port_status) == 64);
+
+/* Modify behavior of the physical port */
+struct ofp_port_mod {
+    struct ofp_header header;
+    ovs_be16 port_no;
+    uint8_t hw_addr[OFP_ETH_ALEN]; /* The hardware address is not
+                                      configurable.  This is used to
+                                      sanity-check the request, so it must
+                                      be the same as returned in an
+                                      ofp_phy_port struct. */
+
+    ovs_be32 config;        /* Bitmap of OFPPC_* flags. */
+    ovs_be32 mask;          /* Bitmap of OFPPC_* flags to be changed. */
+
+    ovs_be32 advertise;     /* Bitmap of "ofp_port_features"s.  Zero all
+                               bits to prevent any action taking place. */
+    uint8_t pad[4];         /* Pad to 64-bits. */
+};
+OFP_ASSERT(sizeof(struct ofp_port_mod) == 32);
+
+/* Why is this packet being sent to the controller? */
+enum ofp_packet_in_reason {
+    OFPR_NO_MATCH,          /* No matching flow. */
+    OFPR_ACTION             /* Action explicitly output to controller. */
+};
+
+/* Packet received on port (datapath -> controller). */
+struct ofp_packet_in {
+    struct ofp_header header;
+    ovs_be32 buffer_id;     /* ID assigned by datapath. */
+    ovs_be16 total_len;     /* Full length of frame. */
+    ovs_be16 in_port;       /* Port on which frame was received. */
+    uint8_t reason;         /* Reason packet is being sent (one of OFPR_*) */
+    uint8_t pad;
+    uint8_t data[0];        /* Ethernet frame, halfway through 32-bit word,
+                               so the IP header is 32-bit aligned.  The
+                               amount of data is inferred from the length
+                               field in the header.  Because of padding,
+                               offsetof(struct ofp_packet_in, data) ==
+                               sizeof(struct ofp_packet_in) - 2. */
+};
+OFP_ASSERT(sizeof(struct ofp_packet_in) == 20);
+
+enum ofp_action_type {
+    OFPAT_OUTPUT,           /* Output to switch port. */
+    OFPAT_SET_VLAN_VID,     /* Set the 802.1q VLAN id. */
+    OFPAT_SET_VLAN_PCP,     /* Set the 802.1q priority. */
+    OFPAT_STRIP_VLAN,       /* Strip the 802.1q header. */
+    OFPAT_SET_DL_SRC,       /* Ethernet source address. */
+    OFPAT_SET_DL_DST,       /* Ethernet destination address. */
+    OFPAT_SET_NW_SRC,       /* IP source address. */
+    OFPAT_SET_NW_DST,       /* IP destination address. */
+    OFPAT_SET_NW_TOS,       /* IP ToS (DSCP field, 6 bits). */
+    OFPAT_SET_TP_SRC,       /* TCP/UDP source port. */
+    OFPAT_SET_TP_DST,       /* TCP/UDP destination port. */
+    OFPAT_ENQUEUE,          /* Output to queue. */
+    OFPAT_VENDOR = 0xffff
+};
+
+/* Action structure for OFPAT_OUTPUT, which sends packets out 'port'.
+ * When the 'port' is the OFPP_CONTROLLER, 'max_len' indicates the max
+ * number of bytes to send.  A 'max_len' of zero means no bytes of the
+ * packet should be sent. */
+struct ofp_action_output {
+    ovs_be16 type;                  /* OFPAT_OUTPUT. */
+    ovs_be16 len;                   /* Length is 8. */
+    ovs_be16 port;                  /* Output port. */
+    ovs_be16 max_len;               /* Max length to send to controller. */
+};
+OFP_ASSERT(sizeof(struct ofp_action_output) == 8);
+
+/* The VLAN id is 12 bits, so we can use the entire 16 bits to indicate
+ * special conditions.  All ones is used to match that no VLAN id was
+ * set. */
+#define OFP_VLAN_NONE      0xffff
+
+/* Action structure for OFPAT_SET_VLAN_VID. */
+struct ofp_action_vlan_vid {
+    ovs_be16 type;                  /* OFPAT_SET_VLAN_VID. */
+    ovs_be16 len;                   /* Length is 8. */
+    ovs_be16 vlan_vid;              /* VLAN id. */
+    uint8_t pad[2];
+};
+OFP_ASSERT(sizeof(struct ofp_action_vlan_vid) == 8);
+
+/* Action structure for OFPAT_SET_VLAN_PCP. */
+struct ofp_action_vlan_pcp {
+    ovs_be16 type;                  /* OFPAT_SET_VLAN_PCP. */
+    ovs_be16 len;                   /* Length is 8. */
+    uint8_t vlan_pcp;               /* VLAN priority. */
+    uint8_t pad[3];
+};
+OFP_ASSERT(sizeof(struct ofp_action_vlan_pcp) == 8);
+
+/* Action structure for OFPAT_SET_DL_SRC/DST. */
+struct ofp_action_dl_addr {
+    ovs_be16 type;                  /* OFPAT_SET_DL_SRC/DST. */
+    ovs_be16 len;                   /* Length is 16. */
+    uint8_t dl_addr[OFP_ETH_ALEN];  /* Ethernet address. */
+    uint8_t pad[6];
+};
+OFP_ASSERT(sizeof(struct ofp_action_dl_addr) == 16);
+
+/* Action structure for OFPAT_SET_NW_SRC/DST. */
+struct ofp_action_nw_addr {
+    ovs_be16 type;                  /* OFPAT_SET_TW_SRC/DST. */
+    ovs_be16 len;                   /* Length is 8. */
+    ovs_be32 nw_addr;               /* IP address. */
+};
+OFP_ASSERT(sizeof(struct ofp_action_nw_addr) == 8);
+
+/* Action structure for OFPAT_SET_NW_TOS. */
+struct ofp_action_nw_tos {
+    ovs_be16 type;                  /* OFPAT_SET_TW_TOS. */
+    ovs_be16 len;                   /* Length is 8. */
+    uint8_t nw_tos;                 /* IP TOS (DSCP field, 6 bits). */
+    uint8_t pad[3];
+};
+OFP_ASSERT(sizeof(struct ofp_action_nw_tos) == 8);
+
+/* Action structure for OFPAT_SET_TP_SRC/DST. */
+struct ofp_action_tp_port {
+    ovs_be16 type;                  /* OFPAT_SET_TP_SRC/DST. */
+    ovs_be16 len;                   /* Length is 8. */
+    ovs_be16 tp_port;               /* TCP/UDP port. */
+    uint8_t pad[2];
+};
+OFP_ASSERT(sizeof(struct ofp_action_tp_port) == 8);
+
+/* Action header for OFPAT_VENDOR. The rest of the body is vendor-defined. */
+struct ofp_action_vendor_header {
+    ovs_be16 type;                  /* OFPAT_VENDOR. */
+    ovs_be16 len;                   /* Length is a multiple of 8. */
+    ovs_be32 vendor;                /* Vendor ID, which takes the same form
+                                       as in "struct ofp_vendor_header". */
+};
+OFP_ASSERT(sizeof(struct ofp_action_vendor_header) == 8);
+
+/* Action header that is common to all actions.  The length includes the
+ * header and any padding used to make the action 64-bit aligned.
+ * NB: The length of an action *must* always be a multiple of eight. */
+struct ofp_action_header {
+    ovs_be16 type;                  /* One of OFPAT_*. */
+    ovs_be16 len;                   /* Length of action, including this
+                                       header.  This is the length of action,
+                                       including any padding to make it
+                                       64-bit aligned. */
+    uint8_t pad[4];
+};
+OFP_ASSERT(sizeof(struct ofp_action_header) == 8);
+
+/* OFPAT_ENQUEUE action struct: send packets to given queue on port. */
+struct ofp_action_enqueue {
+    ovs_be16 type;            /* OFPAT_ENQUEUE. */
+    ovs_be16 len;             /* Len is 16. */
+    ovs_be16 port;            /* Port that queue belongs. Should
+                                 refer to a valid physical port
+                                 (i.e. < OFPP_MAX) or OFPP_IN_PORT. */
+    uint8_t pad[6];           /* Pad for 64-bit alignment. */
+    ovs_be32 queue_id;        /* Where to enqueue the packets. */
+};
+OFP_ASSERT(sizeof(struct ofp_action_enqueue) == 16);
+
+union ofp_action {
+    ovs_be16 type;
+    struct ofp_action_header header;
+    struct ofp_action_vendor_header vendor;
+    struct ofp_action_output output;
+    struct ofp_action_vlan_vid vlan_vid;
+    struct ofp_action_vlan_pcp vlan_pcp;
+    struct ofp_action_nw_addr nw_addr;
+    struct ofp_action_nw_tos nw_tos;
+    struct ofp_action_tp_port tp_port;
+};
+OFP_ASSERT(sizeof(union ofp_action) == 8);
+
+/* Send packet (controller -> datapath). */
+struct ofp_packet_out {
+    struct ofp_header header;
+    ovs_be32 buffer_id;           /* ID assigned by datapath (-1 if none). */
+    ovs_be16 in_port;             /* Packet's input port (OFPP_NONE if none). */
+    ovs_be16 actions_len;         /* Size of action array in bytes. */
+    struct ofp_action_header actions[0]; /* Actions. */
+    /* uint8_t data[0]; */        /* Packet data.  The length is inferred
+                                     from the length field in the header.
+                                     (Only meaningful if buffer_id == -1.) */
+};
+OFP_ASSERT(sizeof(struct ofp_packet_out) == 16);
+
+enum ofp_flow_mod_command {
+    OFPFC_ADD,              /* New flow. */
+    OFPFC_MODIFY,           /* Modify all matching flows. */
+    OFPFC_MODIFY_STRICT,    /* Modify entry strictly matching wildcards */
+    OFPFC_DELETE,           /* Delete all matching flows. */
+    OFPFC_DELETE_STRICT     /* Strictly match wildcards and priority. */
+};
+
+/* Flow wildcards. */
+enum ofp_flow_wildcards {
+    OFPFW_IN_PORT    = 1 << 0,  /* Switch input port. */
+    OFPFW_DL_VLAN    = 1 << 1,  /* VLAN vid. */
+    OFPFW_DL_SRC     = 1 << 2,  /* Ethernet source address. */
+    OFPFW_DL_DST     = 1 << 3,  /* Ethernet destination address. */
+    OFPFW_DL_TYPE    = 1 << 4,  /* Ethernet frame type. */
+    OFPFW_NW_PROTO   = 1 << 5,  /* IP protocol. */
+    OFPFW_TP_SRC     = 1 << 6,  /* TCP/UDP source port. */
+    OFPFW_TP_DST     = 1 << 7,  /* TCP/UDP destination port. */
+
+    /* IP source address wildcard bit count.  0 is exact match, 1 ignores the
+     * LSB, 2 ignores the 2 least-significant bits, ..., 32 and higher wildcard
+     * the entire field.  This is the *opposite* of the usual convention where
+     * e.g. /24 indicates that 8 bits (not 24 bits) are wildcarded. */
+    OFPFW_NW_SRC_SHIFT = 8,
+    OFPFW_NW_SRC_BITS = 6,
+    OFPFW_NW_SRC_MASK = ((1 << OFPFW_NW_SRC_BITS) - 1) << OFPFW_NW_SRC_SHIFT,
+    OFPFW_NW_SRC_ALL = 32 << OFPFW_NW_SRC_SHIFT,
+
+    /* IP destination address wildcard bit count.  Same format as source. */
+    OFPFW_NW_DST_SHIFT = 14,
+    OFPFW_NW_DST_BITS = 6,
+    OFPFW_NW_DST_MASK = ((1 << OFPFW_NW_DST_BITS) - 1) << OFPFW_NW_DST_SHIFT,
+    OFPFW_NW_DST_ALL = 32 << OFPFW_NW_DST_SHIFT,
+
+    OFPFW_DL_VLAN_PCP = 1 << 20, /* VLAN priority. */
+    OFPFW_NW_TOS = 1 << 21, /* IP ToS (DSCP field, 6 bits). */
+
+    /* Wildcard all fields. */
+    OFPFW_ALL = ((1 << 22) - 1)
+};
+
+/* The wildcards for ICMP type and code fields use the transport source
+ * and destination port fields, respectively. */
+#define OFPFW_ICMP_TYPE OFPFW_TP_SRC
+#define OFPFW_ICMP_CODE OFPFW_TP_DST
+
+/* Values below this cutoff are 802.3 packets and the two bytes
+ * following MAC addresses are used as a frame length.  Otherwise, the
+ * two bytes are used as the Ethernet type.
+ */
+#define OFP_DL_TYPE_ETH2_CUTOFF   0x0600
+
+/* Value of dl_type to indicate that the frame does not include an
+ * Ethernet type.
+ */
+#define OFP_DL_TYPE_NOT_ETH_TYPE  0x05ff
+
+/* The VLAN id is 12-bits, so we can use the entire 16 bits to indicate
+ * special conditions.  All ones indicates that no VLAN id was set.
+ */
+#define OFP_VLAN_NONE      0xffff
+
+/* Fields to match against flows */
+struct ofp_match {
+    ovs_be32 wildcards;        /* Wildcard fields. */
+    ovs_be16 in_port;          /* Input switch port. */
+    uint8_t dl_src[OFP_ETH_ALEN]; /* Ethernet source address. */
+    uint8_t dl_dst[OFP_ETH_ALEN]; /* Ethernet destination address. */
+    ovs_be16 dl_vlan;          /* Input VLAN. */
+    uint8_t dl_vlan_pcp;       /* Input VLAN priority. */
+    uint8_t pad1[1];           /* Align to 64-bits. */
+    ovs_be16 dl_type;          /* Ethernet frame type. */
+    uint8_t nw_tos;            /* IP ToS (DSCP field, 6 bits). */
+    uint8_t nw_proto;          /* IP protocol or lower 8 bits of
+                                  ARP opcode. */
+    uint8_t pad2[2];           /* Align to 64-bits. */
+    ovs_be32 nw_src;           /* IP source address. */
+    ovs_be32 nw_dst;           /* IP destination address. */
+    ovs_be16 tp_src;           /* TCP/UDP source port. */
+    ovs_be16 tp_dst;           /* TCP/UDP destination port. */
+};
+OFP_ASSERT(sizeof(struct ofp_match) == 40);
+
+/* Value used in "idle_timeout" and "hard_timeout" to indicate that the entry
+ * is permanent. */
+#define OFP_FLOW_PERMANENT 0
+
+/* By default, choose a priority in the middle. */
+#define OFP_DEFAULT_PRIORITY 0x8000
+
+enum ofp_flow_mod_flags {
+    OFPFF_SEND_FLOW_REM = 1 << 0,  /* Send flow removed message when flow
+                                    * expires or is deleted. */
+    OFPFF_CHECK_OVERLAP = 1 << 1,  /* Check for overlapping entries first. */
+    OFPFF_EMERG         = 1 << 2   /* Ramark this is for emergency. */
+};
+
+/* Flow setup and teardown (controller -> datapath). */
+struct ofp_flow_mod {
+    struct ofp_header header;
+    struct ofp_match match;      /* Fields to match */
+    ovs_be64 cookie;             /* Opaque controller-issued identifier. */
+
+    /* Flow actions. */
+    ovs_be16 command;             /* One of OFPFC_*. */
+    ovs_be16 idle_timeout;        /* Idle time before discarding (seconds). */
+    ovs_be16 hard_timeout;        /* Max time before discarding (seconds). */
+    ovs_be16 priority;            /* Priority level of flow entry. */
+    ovs_be32 buffer_id;           /* Buffered packet to apply to (or -1).
+                                     Not meaningful for OFPFC_DELETE*. */
+    ovs_be16 out_port;            /* For OFPFC_DELETE* commands, require
+                                     matching entries to include this as an
+                                     output port.  A value of OFPP_NONE
+                                     indicates no restriction. */
+    ovs_be16 flags;               /* One of OFPFF_*. */
+    struct ofp_action_header actions[0]; /* The action length is inferred
+                                            from the length field in the
+                                            header. */
+};
+OFP_ASSERT(sizeof(struct ofp_flow_mod) == 72);
+
+/* Why was this flow removed? */
+enum ofp_flow_removed_reason {
+    OFPRR_IDLE_TIMEOUT,         /* Flow idle time exceeded idle_timeout. */
+    OFPRR_HARD_TIMEOUT,         /* Time exceeded hard_timeout. */
+    OFPRR_DELETE                /* Evicted by a DELETE flow mod. */
+};
+
+/* Flow removed (datapath -> controller). */
+struct ofp_flow_removed {
+    struct ofp_header header;
+    struct ofp_match match;   /* Description of fields. */
+    ovs_be64 cookie;          /* Opaque controller-issued identifier. */
+
+    ovs_be16 priority;        /* Priority level of flow entry. */
+    uint8_t reason;           /* One of OFPRR_*. */
+    uint8_t pad[1];           /* Align to 32-bits. */
+
+    ovs_be32 duration_sec;    /* Time flow was alive in seconds. */
+    ovs_be32 duration_nsec;   /* Time flow was alive in nanoseconds beyond
+                                 duration_sec. */
+    ovs_be16 idle_timeout;    /* Idle timeout from original flow mod. */
+    uint8_t pad2[2];          /* Align to 64-bits. */
+    ovs_be64 packet_count;
+    ovs_be64 byte_count;
+};
+OFP_ASSERT(sizeof(struct ofp_flow_removed) == 88);
+
+/* Values for 'type' in ofp_error_message.  These values are immutable: they
+ * will not change in future versions of the protocol (although new values may
+ * be added). */
+enum ofp_error_type {
+    OFPET_HELLO_FAILED,         /* Hello protocol failed. */
+    OFPET_BAD_REQUEST,          /* Request was not understood. */
+    OFPET_BAD_ACTION,           /* Error in action description. */
+    OFPET_FLOW_MOD_FAILED,      /* Problem modifying flow entry. */
+    OFPET_PORT_MOD_FAILED,      /* OFPT_PORT_MOD failed. */
+    OFPET_QUEUE_OP_FAILED       /* Queue operation failed. */
+};
+
+/* ofp_error_msg 'code' values for OFPET_HELLO_FAILED.  'data' contains an
+ * ASCII text string that may give failure details. */
+enum ofp_hello_failed_code {
+    OFPHFC_INCOMPATIBLE,        /* No compatible version. */
+    OFPHFC_EPERM                /* Permissions error. */
+};
+
+/* ofp_error_msg 'code' values for OFPET_BAD_REQUEST.  'data' contains at least
+ * the first 64 bytes of the failed request. */
+enum ofp_bad_request_code {
+    OFPBRC_BAD_VERSION,         /* ofp_header.version not supported. */
+    OFPBRC_BAD_TYPE,            /* ofp_header.type not supported. */
+    OFPBRC_BAD_STAT,            /* ofp_stats_msg.type not supported. */
+    OFPBRC_BAD_VENDOR,          /* Vendor not supported (in ofp_vendor_header
+                                 * or ofp_stats_msg). */
+    OFPBRC_BAD_SUBTYPE,         /* Vendor subtype not supported. */
+    OFPBRC_EPERM,               /* Permissions error. */
+    OFPBRC_BAD_LEN,             /* Wrong request length for type. */
+    OFPBRC_BUFFER_EMPTY,        /* Specified buffer has already been used. */
+    OFPBRC_BUFFER_UNKNOWN       /* Specified buffer does not exist. */
+};
+
+/* ofp_error_msg 'code' values for OFPET_BAD_ACTION.  'data' contains at least
+ * the first 64 bytes of the failed request. */
+enum ofp_bad_action_code {
+    OFPBAC_BAD_TYPE,           /* Unknown action type. */
+    OFPBAC_BAD_LEN,            /* Length problem in actions. */
+    OFPBAC_BAD_VENDOR,         /* Unknown vendor id specified. */
+    OFPBAC_BAD_VENDOR_TYPE,    /* Unknown action type for vendor id. */
+    OFPBAC_BAD_OUT_PORT,       /* Problem validating output action. */
+    OFPBAC_BAD_ARGUMENT,       /* Bad action argument. */
+    OFPBAC_EPERM,              /* Permissions error. */
+    OFPBAC_TOO_MANY,           /* Can't handle this many actions. */
+    OFPBAC_BAD_QUEUE           /* Problem validating output queue. */
+};
+
+/* ofp_error_msg 'code' values for OFPET_FLOW_MOD_FAILED.  'data' contains
+ * at least the first 64 bytes of the failed request. */
+enum ofp_flow_mod_failed_code {
+    OFPFMFC_ALL_TABLES_FULL,    /* Flow not added because of full tables. */
+    OFPFMFC_OVERLAP,            /* Attempted to add overlapping flow with
+                                 * CHECK_OVERLAP flag set. */
+    OFPFMFC_EPERM,              /* Permissions error. */
+    OFPFMFC_BAD_EMERG_TIMEOUT,  /* Flow not added because of non-zero idle/hard
+                                 * timeout. */
+    OFPFMFC_BAD_COMMAND,        /* Unknown command. */
+    OFPFMFC_UNSUPPORTED         /* Unsupported action list - cannot process in
+                                   the order specified. */
+};
+
+/* ofp_error_msg 'code' values for OFPET_PORT_MOD_FAILED.  'data' contains
+ * at least the first 64 bytes of the failed request. */
+enum ofp_port_mod_failed_code {
+    OFPPMFC_BAD_PORT,            /* Specified port does not exist. */
+    OFPPMFC_BAD_HW_ADDR,         /* Specified hardware address is wrong. */
+};
+
+/* ofp_error msg 'code' values for OFPET_QUEUE_OP_FAILED. 'data' contains
+ * at least the first 64 bytes of the failed request */
+enum ofp_queue_op_failed_code {
+    OFPQOFC_BAD_PORT,           /* Invalid port (or port does not exist). */
+    OFPQOFC_BAD_QUEUE,          /* Queue does not exist. */
+    OFPQOFC_EPERM               /* Permissions error. */
+};
+
+/* OFPT_ERROR: Error message (datapath -> controller). */
+struct ofp_error_msg {
+    struct ofp_header header;
+
+    ovs_be16 type;
+    ovs_be16 code;
+    uint8_t data[0];          /* Variable-length data.  Interpreted based
+                                 on the type and code. */
+};
+OFP_ASSERT(sizeof(struct ofp_error_msg) == 12);
+
+enum ofp_stats_types {
+    /* Description of this OpenFlow switch.
+     * The request is struct ofp_stats_msg.
+     * The reply is struct ofp_desc_stats. */
+    OFPST_DESC,
+
+    /* Individual flow statistics.
+     * The request is struct ofp_flow_stats_request.
+     * The reply body is an array of struct ofp_flow_stats. */
+    OFPST_FLOW,
+
+    /* Aggregate flow statistics.
+     * The request is struct ofp_flow_stats_request.
+     * The reply is struct ofp_aggregate_stats_reply. */
+    OFPST_AGGREGATE,
+
+    /* Flow table statistics.
+     * The request is struct ofp_stats_msg.
+     * The reply body is an array of struct ofp_table_stats. */
+    OFPST_TABLE,
+
+    /* Physical port statistics.
+     * The request is struct ofp_port_stats_request.
+     * The reply body is an array of struct ofp_port_stats. */
+    OFPST_PORT,
+
+    /* Queue statistics for a port.
+     * The request body is struct ofp_queue_stats_request.
+     * The reply body is an array of struct ofp_queue_stats. */
+    OFPST_QUEUE,
+
+    /* Vendor extension.
+     * The request and reply begin with "struct ofp_vendor_stats". */
+    OFPST_VENDOR = 0xffff
+};
+
+/* Statistics request or reply message. */
+struct ofp_stats_msg {
+    struct ofp_header header;
+    ovs_be16 type;              /* One of the OFPST_* constants. */
+    ovs_be16 flags;             /* Requests: always 0.
+                                 * Replies: 0 or OFPSF_REPLY_MORE. */
+};
+OFP_ASSERT(sizeof(struct ofp_stats_msg) == 12);
+
+enum ofp_stats_reply_flags {
+    OFPSF_REPLY_MORE  = 1 << 0  /* More replies to follow. */
+};
+
+#define DESC_STR_LEN   256
+#define SERIAL_NUM_LEN 32
+/* Reply to OFPST_DESC request.  Each entry is a NULL-terminated ASCII
+ * string. */
+struct ofp_desc_stats {
+    struct ofp_stats_msg osm;
+    char mfr_desc[DESC_STR_LEN];       /* Manufacturer description. */
+    char hw_desc[DESC_STR_LEN];        /* Hardware description. */
+    char sw_desc[DESC_STR_LEN];        /* Software description. */
+    char serial_num[SERIAL_NUM_LEN];   /* Serial number. */
+    char dp_desc[DESC_STR_LEN];        /* Human readable description of
+                                          the datapath. */
+};
+OFP_ASSERT(sizeof(struct ofp_desc_stats) == 1068);
+
+/* Stats request of type OFPST_AGGREGATE or OFPST_FLOW. */
+struct ofp_flow_stats_request {
+    struct ofp_stats_msg osm;
+    struct ofp_match match;   /* Fields to match. */
+    uint8_t table_id;         /* ID of table to read (from ofp_table_stats)
+                                 or 0xff for all tables. */
+    uint8_t pad;              /* Align to 32 bits. */
+    ovs_be16 out_port;        /* Require matching entries to include this
+                                 as an output port.  A value of OFPP_NONE
+                                 indicates no restriction. */
+};
+OFP_ASSERT(sizeof(struct ofp_flow_stats_request) == 56);
+
+/* Body of reply to OFPST_FLOW request. */
+struct ofp_flow_stats {
+    ovs_be16 length;          /* Length of this entry. */
+    uint8_t table_id;         /* ID of table flow came from. */
+    uint8_t pad;
+    struct ofp_match match;   /* Description of fields. */
+    ovs_be32 duration_sec;    /* Time flow has been alive in seconds. */
+    ovs_be32 duration_nsec;   /* Time flow has been alive in nanoseconds
+                                 beyond duration_sec. */
+    ovs_be16 priority;        /* Priority of the entry. Only meaningful
+                                 when this is not an exact-match entry. */
+    ovs_be16 idle_timeout;    /* Number of seconds idle before expiration. */
+    ovs_be16 hard_timeout;    /* Number of seconds before expiration. */
+    uint8_t pad2[6];          /* Align to 64 bits. */
+    ovs_32aligned_be64 cookie;       /* Opaque controller-issued identifier. */
+    ovs_32aligned_be64 packet_count; /* Number of packets in flow. */
+    ovs_32aligned_be64 byte_count;   /* Number of bytes in flow. */
+    struct ofp_action_header actions[0]; /* Actions. */
+};
+OFP_ASSERT(sizeof(struct ofp_flow_stats) == 88);
+
+/* Reply to OFPST_AGGREGATE request. */
+struct ofp_aggregate_stats_reply {
+    struct ofp_stats_msg osm;
+    ovs_32aligned_be64 packet_count; /* Number of packets in flows. */
+    ovs_32aligned_be64 byte_count;   /* Number of bytes in flows. */
+    ovs_be32 flow_count;      /* Number of flows. */
+    uint8_t pad[4];           /* Align to 64 bits. */
+};
+OFP_ASSERT(sizeof(struct ofp_aggregate_stats_reply) == 36);
+
+/* Body of reply to OFPST_TABLE request. */
+struct ofp_table_stats {
+    uint8_t table_id;        /* Identifier of table.  Lower numbered tables
+                                are consulted first. */
+    uint8_t pad[3];          /* Align to 32-bits. */
+    char name[OFP_MAX_TABLE_NAME_LEN];
+    ovs_be32 wildcards;      /* Bitmap of OFPFW_* wildcards that are
+                                supported by the table. */
+    ovs_be32 max_entries;    /* Max number of entries supported. */
+    ovs_be32 active_count;   /* Number of active entries. */
+    ovs_32aligned_be64 lookup_count;  /* # of packets looked up in table. */
+    ovs_32aligned_be64 matched_count; /* Number of packets that hit table. */
+};
+OFP_ASSERT(sizeof(struct ofp_table_stats) == 64);
+
+/* Stats request of type OFPST_PORT. */
+struct ofp_port_stats_request {
+    struct ofp_stats_msg osm;
+    ovs_be16 port_no;        /* OFPST_PORT message may request statistics
+                                for a single port (specified with port_no)
+                                or for all ports (port_no == OFPP_NONE). */
+    uint8_t pad[6];
+};
+OFP_ASSERT(sizeof(struct ofp_port_stats_request) == 20);
+
+/* Body of reply to OFPST_PORT request. If a counter is unsupported, set
+ * the field to all ones. */
+struct ofp_port_stats {
+    ovs_be16 port_no;
+    uint8_t pad[6];          /* Align to 64-bits. */
+    ovs_32aligned_be64 rx_packets;     /* Number of received packets. */
+    ovs_32aligned_be64 tx_packets;     /* Number of transmitted packets. */
+    ovs_32aligned_be64 rx_bytes;       /* Number of received bytes. */
+    ovs_32aligned_be64 tx_bytes;       /* Number of transmitted bytes. */
+    ovs_32aligned_be64 rx_dropped;     /* Number of packets dropped by RX. */
+    ovs_32aligned_be64 tx_dropped;     /* Number of packets dropped by TX. */
+    ovs_32aligned_be64 rx_errors; /* Number of receive errors.  This is a
+                                     super-set of receive errors and should be
+                                     great than or equal to the sum of all
+                                     rx_*_err values. */
+    ovs_32aligned_be64 tx_errors; /* Number of transmit errors.  This is a
+                                     super-set of transmit errors. */
+    ovs_32aligned_be64 rx_frame_err; /* Number of frame alignment errors. */
+    ovs_32aligned_be64 rx_over_err;  /* Number of packets with RX overrun. */
+    ovs_32aligned_be64 rx_crc_err;   /* Number of CRC errors. */
+    ovs_32aligned_be64 collisions;   /* Number of collisions. */
+};
+OFP_ASSERT(sizeof(struct ofp_port_stats) == 104);
+
+/* All ones is used to indicate all queues in a port (for stats retrieval). */
+#define OFPQ_ALL      0xffffffff
+
+/* Body for stats request of type OFPST_QUEUE. */
+struct ofp_queue_stats_request {
+    struct ofp_stats_msg osm;
+    ovs_be16 port_no;        /* All ports if OFPP_ALL. */
+    uint8_t pad[2];          /* Align to 32-bits. */
+    ovs_be32 queue_id;       /* All queues if OFPQ_ALL. */
+};
+OFP_ASSERT(sizeof(struct ofp_queue_stats_request) == 20);
+
+/* Body for stats reply of type OFPST_QUEUE consists of an array of this
+ * structure type. */
+struct ofp_queue_stats {
+    ovs_be16 port_no;
+    uint8_t pad[2];          /* Align to 32-bits. */
+    ovs_be32 queue_id;       /* Queue id. */
+    ovs_32aligned_be64 tx_bytes;   /* Number of transmitted bytes. */
+    ovs_32aligned_be64 tx_packets; /* Number of transmitted packets. */
+    ovs_32aligned_be64 tx_errors;  /* # of packets dropped due to overrun. */
+};
+OFP_ASSERT(sizeof(struct ofp_queue_stats) == 32);
+
+/* Vendor extension stats message. */
+struct ofp_vendor_stats_msg {
+    struct ofp_stats_msg osm;   /* Type OFPST_VENDOR. */
+    ovs_be32 vendor;            /* Vendor ID:
+                                 * - MSB 0: low-order bytes are IEEE OUI.
+                                 * - MSB != 0: defined by OpenFlow
+                                 *   consortium. */
+    /* Followed by vendor-defined arbitrary additional data. */
+};
+OFP_ASSERT(sizeof(struct ofp_vendor_stats_msg) == 16);
+
+/* Vendor extension. */
+struct ofp_vendor_header {
+    struct ofp_header header;   /* Type OFPT_VENDOR. */
+    ovs_be32 vendor;            /* Vendor ID:
+                                 * - MSB 0: low-order bytes are IEEE OUI.
+                                 * - MSB != 0: defined by OpenFlow
+                                 *   consortium. */
+    /* Vendor-defined arbitrary additional data. */
+};
+OFP_ASSERT(sizeof(struct ofp_vendor_header) == 12);
+
+#endif /* openflow/openflow.h */
diff -r 89e197c6e9d5 include/openvswitch/brcompat-netlink.h
--- /dev/null
+++ b/include/openvswitch/brcompat-netlink.h
@@ -0,0 +1,100 @@
+/*
+ * Copyright (c) 2008, 2009, 2011 Nicira Networks.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef OPENVSWITCH_BRCOMPAT_NETLINK_H
+#define OPENVSWITCH_BRCOMPAT_NETLINK_H 1
+
+#define BRC_GENL_FAMILY_NAME "brcompat"
+
+/* Attributes that can be attached to the datapath's netlink messages. */
+enum {
+	BRC_GENL_A_UNSPEC,
+
+	/*
+	 * "K:" attributes appear in messages from the kernel to userspace.
+	 * "U:" attributes appear in messages from userspace to the kernel.
+	 */
+
+	/* BRC_GENL_C_DP_ADD, BRC_GENL_C_DP_DEL. */
+	BRC_GENL_A_DP_NAME,		/* K: Datapath name. */
+
+	/* BRC_GENL_C_DP_ADD, BRC_GENL_C_DP_DEL,
+	   BRC_GENL_C_PORT_ADD, BRC_GENL_C_PORT_DEL. */
+	BRC_GENL_A_PORT_NAME,	/* K: Interface name. */
+
+	/* BRC_GENL_C_DP_RESULT. */
+	BRC_GENL_A_ERR_CODE,	/* U: Positive error code. */
+
+	/* BRC_GENL_C_QUERY_MC. */
+	BRC_GENL_A_MC_GROUP,	/* K: Generic netlink multicast group. */
+
+	/* BRC_GENL_C_FDB_QUERY. */
+	BRC_GENL_A_FDB_COUNT,	/* K: Number of FDB entries to read. */
+	BRC_GENL_A_FDB_SKIP,	/* K: Record offset into FDB to start reading. */
+
+	/* BRC_GENL_C_DP_RESULT. */
+	BRC_GENL_A_FDB_DATA,    /* U: FDB records. */
+	BRC_GENL_A_IFINDEXES,   /* U: "int" ifindexes of bridges or ports. */
+
+	__BRC_GENL_A_MAX,
+	BRC_GENL_A_MAX = __BRC_GENL_A_MAX - 1
+};
+
+/* Commands that can be executed on the datapath's netlink interface. */
+enum brc_genl_command {
+	BRC_GENL_C_UNSPEC,
+
+	/*
+	 * "K:" messages are sent by the kernel to userspace.
+	 * "U:" messages are sent by userspace to the kernel.
+	 */
+	BRC_GENL_C_DP_ADD,		/* K: Datapath created. */
+	BRC_GENL_C_DP_DEL,		/* K: Datapath destroyed. */
+	BRC_GENL_C_DP_RESULT,	/* U: Return code from ovs-brcompatd. */
+	BRC_GENL_C_PORT_ADD,	/* K: Port added to datapath. */
+	BRC_GENL_C_PORT_DEL,	/* K: Port removed from datapath. */
+	BRC_GENL_C_QUERY_MC,	/* U: Get multicast group for brcompat. */
+	BRC_GENL_C_FDB_QUERY,	/* K: Read records from forwarding database. */
+	BRC_GENL_C_GET_BRIDGES, /* K: Get ifindexes of all bridges. */
+	BRC_GENL_C_GET_PORTS,   /* K: Get ifindexes of all ports on a bridge. */
+
+	__BRC_GENL_C_MAX,
+	BRC_GENL_C_MAX = __BRC_GENL_C_MAX - 1
+};
+#endif /* openvswitch/brcompat-netlink.h */
diff -r 89e197c6e9d5 include/openvswitch/datapath-compat.h
--- /dev/null
+++ b/include/openvswitch/datapath-compat.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (c) 2011 Nicira Networks.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef OPENVSWITCH_DATAPATH_COMPAT_H
+#define OPENVSWITCH_DATAPATH_COMPAT_H 1
+
+#define OVS_VPORT_MCGROUP_FALLBACK_ID 33
+
+#endif /* openvswitch/datapath-compat.h */
diff -r 89e197c6e9d5 include/openvswitch/datapath-protocol.h
--- /dev/null
+++ b/include/openvswitch/datapath-protocol.h
@@ -0,0 +1,430 @@
+/*
+ * Copyright (c) 2009, 2010, 2011 Nicira Networks.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef OPENVSWITCH_DATAPATH_PROTOCOL_H
+#define OPENVSWITCH_DATAPATH_PROTOCOL_H 1
+
+#ifdef __KERNEL__
+#include <linux/types.h>
+#include <linux/socket.h>
+#define ovs_be16 __be16
+#define ovs_be32 __be32
+#define ovs_be64 __be64
+#else
+#include "openvswitch/types.h"
+#include <sys/socket.h>
+#endif
+
+#include <linux/if_link.h>
+#include <linux/netlink.h>
+
+/* datapaths. */
+
+#define ODP_DATAPATH_FAMILY  "odp_datapath"
+#define ODP_DATAPATH_MCGROUP "odp_datapath"
+
+enum odp_datapath_cmd {
+	ODP_DP_CMD_UNSPEC,
+	ODP_DP_CMD_NEW,
+	ODP_DP_CMD_DEL,
+	ODP_DP_CMD_GET,
+	ODP_DP_CMD_SET
+};
+
+/**
+ * struct odp_header - header for ODP Generic Netlink messages.
+ * @dp_ifindex: ifindex of local port for datapath (0 to make a request not
+ * specific to a datapath).
+ *
+ * Attributes following the header are specific to a particular ODP Generic
+ * Netlink family, but all of the ODP families use this header.
+ */
+struct odp_header {
+	int dp_ifindex;
+};
+
+/**
+ * enum odp_datapath_attr - attributes for %ODP_DP_* commands.
+ * @ODP_DP_ATTR_NAME: Name of the network device that serves as the "local
+ * port".  This is the name of the network device whose dp_ifindex is given in
+ * the &struct odp_header.  Always present in notifications.  Required in
+ * %ODP_DP_NEW requests.  May be used as an alternative to specifying
+ * dp_ifindex in other requests (with a dp_ifindex of 0).
+ * @ODP_DP_ATTR_STATS: Statistics about packets that have passed through the
+ * datapath.  Always present in notifications.
+ * @ODP_DP_ATTR_IPV4_FRAGS: One of %ODP_DP_FRAG_*.  Always present in
+ * notifications.  May be included in %ODP_DP_NEW or %ODP_DP_SET requests to
+ * change the fragment handling policy.
+ * @ODP_DP_ATTR_SAMPLING: 32-bit fraction of packets to sample with
+ * @ODP_PACKET_CMD_SAMPLE.  A value of 0 samples no packets, a value of
+ * %UINT32_MAX samples all packets, and intermediate values sample intermediate
+ * fractions of packets.
+ * @ODP_DP_ATTR_MCGROUPS: Nested attributes with multicast groups.  Each nested
+ * attribute has a %ODP_PACKET_CMD_* type with a 32-bit value giving the
+ * Generic Netlink multicast group number used for sending this datapath's
+ * messages with that command type up to userspace.
+ *
+ * These attributes follow the &struct odp_header within the Generic Netlink
+ * payload for %ODP_DP_* commands.
+ */
+enum odp_datapath_attr {
+	ODP_DP_ATTR_UNSPEC,
+	ODP_DP_ATTR_NAME,       /* name of dp_ifindex netdev */
+	ODP_DP_ATTR_STATS,      /* struct odp_stats */
+	ODP_DP_ATTR_IPV4_FRAGS,	/* 32-bit enum odp_frag_handling */
+	ODP_DP_ATTR_SAMPLING,   /* 32-bit fraction of packets to sample. */
+	ODP_DP_ATTR_MCGROUPS,   /* Nested attributes with multicast groups. */
+	__ODP_DP_ATTR_MAX
+};
+
+#define ODP_DP_ATTR_MAX (__ODP_DP_ATTR_MAX - 1)
+
+/**
+ * enum odp_frag_handling - policy for handling received IPv4 fragments.
+ * @ODP_DP_FRAG_ZERO: Treat IP fragments as IP protocol 0 and transport ports
+ * zero.
+ * @ODP_DP_FRAG_DROP: Drop IP fragments.  Do not pass them through the flow
+ * table or up to userspace.
+ */
+enum odp_frag_handling {
+	ODP_DP_FRAG_UNSPEC,
+	ODP_DP_FRAG_ZERO,	/* Treat IP fragments as transport port 0. */
+	ODP_DP_FRAG_DROP	/* Drop IP fragments. */
+};
+
+struct odp_stats {
+    uint64_t n_frags;           /* Number of dropped IP fragments. */
+    uint64_t n_hit;             /* Number of flow table matches. */
+    uint64_t n_missed;          /* Number of flow table misses. */
+    uint64_t n_lost;            /* Number of misses not sent to userspace. */
+};
+
+/* Logical ports. */
+#define ODPP_LOCAL      ((uint16_t)0)
+
+#define ODP_PACKET_FAMILY "odp_packet"
+
+enum odp_packet_cmd {
+        ODP_PACKET_CMD_UNSPEC,
+
+        /* Kernel-to-user notifications. */
+        ODP_PACKET_CMD_MISS,    /* Flow table miss. */
+        ODP_PACKET_CMD_ACTION,  /* ODP_ACTION_ATTR_USERSPACE action. */
+        ODP_PACKET_CMD_SAMPLE,  /* Sampled packet. */
+
+        /* User commands. */
+        ODP_PACKET_CMD_EXECUTE  /* Apply actions to a packet. */
+};
+
+/**
+ * enum odp_packet_attr - attributes for %ODP_PACKET_* commands.
+ * @ODP_PACKET_ATTR_PACKET: Present for all notifications.  Contains the entire
+ * packet as received, from the start of the Ethernet header onward.  For
+ * %ODP_PACKET_CMD_ACTION, %ODP_PACKET_ATTR_PACKET reflects changes made by
+ * actions preceding %ODP_ACTION_ATTR_USERSPACE, but %ODP_PACKET_ATTR_KEY is
+ * the flow key extracted from the packet as originally received.
+ * @ODP_PACKET_ATTR_KEY: Present for all notifications.  Contains the flow key
+ * extracted from the packet as nested %ODP_KEY_ATTR_* attributes.  This allows
+ * userspace to adapt its flow setup strategy by comparing its notion of the
+ * flow key against the kernel's.
+ * @ODP_PACKET_ATTR_USERDATA: Present for an %ODP_PACKET_CMD_ACTION
+ * notification if the %ODP_ACTION_ATTR_USERSPACE, action's argument was
+ * nonzero.
+ * @ODP_PACKET_ATTR_SAMPLE_POOL: Present for %ODP_PACKET_CMD_SAMPLE.  Contains
+ * the number of packets processed so far that were candidates for sampling.
+ * @ODP_PACKET_ATTR_ACTIONS: Present for %ODP_PACKET_CMD_SAMPLE.  Contains a
+ * copy of the actions applied to the packet, as nested %ODP_ACTION_ATTR_*
+ * attributes.
+ *
+ * These attributes follow the &struct odp_header within the Generic Netlink
+ * payload for %ODP_PACKET_* commands.
+ */
+enum odp_packet_attr {
+	ODP_PACKET_ATTR_UNSPEC,
+	ODP_PACKET_ATTR_PACKET,      /* Packet data. */
+	ODP_PACKET_ATTR_KEY,         /* Nested ODP_KEY_ATTR_* attributes. */
+	ODP_PACKET_ATTR_USERDATA,    /* u64 ODP_ACTION_ATTR_USERSPACE arg. */
+	ODP_PACKET_ATTR_SAMPLE_POOL, /* # sampling candidate packets so far. */
+	ODP_PACKET_ATTR_ACTIONS,     /* Nested ODP_ACTION_ATTR_* attributes. */
+	__ODP_PACKET_ATTR_MAX
+};
+
+#define ODP_PACKET_ATTR_MAX (__ODP_PACKET_ATTR_MAX - 1)
+
+enum odp_vport_type {
+	ODP_VPORT_TYPE_UNSPEC,
+	ODP_VPORT_TYPE_NETDEV,   /* network device */
+	ODP_VPORT_TYPE_INTERNAL, /* network device implemented by datapath */
+	ODP_VPORT_TYPE_PATCH,    /* virtual tunnel connecting two vports */
+	ODP_VPORT_TYPE_GRE,      /* GRE tunnel */
+	ODP_VPORT_TYPE_CAPWAP,   /* CAPWAP tunnel */
+	__ODP_VPORT_TYPE_MAX
+};
+
+#define ODP_VPORT_TYPE_MAX (__ODP_VPORT_TYPE_MAX - 1)
+
+#define ODP_VPORT_FAMILY  "odp_vport"
+#define ODP_VPORT_MCGROUP "odp_vport"
+
+enum odp_vport_cmd {
+	ODP_VPORT_CMD_UNSPEC,
+	ODP_VPORT_CMD_NEW,
+	ODP_VPORT_CMD_DEL,
+	ODP_VPORT_CMD_GET,
+	ODP_VPORT_CMD_SET
+};
+
+/**
+ * enum odp_vport_attr - attributes for %ODP_VPORT_* commands.
+ * @ODP_VPORT_ATTR_PORT_NO: 32-bit port number within datapath.
+ * @ODP_VPORT_ATTR_TYPE: 32-bit %ODP_VPORT_TYPE_* constant describing the type
+ * of vport.
+ * @ODP_VPORT_ATTR_NAME: Name of vport.  For a vport based on a network device
+ * this is the name of the network device.  Maximum length %IFNAMSIZ-1 bytes
+ * plus a null terminator.
+ * @ODP_VPORT_ATTR_STATS: A &struct rtnl_link_stats64 giving statistics for
+ * packets sent or received through the vport.
+ * @ODP_VPORT_ATTR_ADDRESS: A 6-byte Ethernet address for the vport.
+ * @ODP_VPORT_ATTR_MTU: MTU for the vport.  Omitted if the vport does not have
+ * an MTU as, e.g., some tunnels do not.
+ * @ODP_VPORT_ATTR_IFINDEX: ifindex of the underlying network device, if any.
+ * @ODP_VPORT_ATTR_IFLINK: ifindex of the device on which packets are sent (for
+ * tunnels), if any.
+ *
+ * These attributes follow the &struct odp_header within the Generic Netlink
+ * payload for %ODP_VPORT_* commands.
+ *
+ * All attributes applicable to a given port are present in notifications.
+ * This means that, for example, a vport that has no corresponding network
+ * device would omit %ODP_VPORT_ATTR_IFINDEX.
+ *
+ * For %ODP_VPORT_CMD_NEW requests, the %ODP_VPORT_ATTR_TYPE and
+ * %ODP_VPORT_ATTR_NAME attributes are required.  %ODP_VPORT_ATTR_PORT_NO is
+ * optional; if not specified a free port number is automatically selected.
+ * Whether %ODP_VPORT_ATTR_OPTIONS is required or optional depends on the type
+ * of vport.  %ODP_VPORT_ATTR_STATS, %ODP_VPORT_ATTR_ADDRESS, and
+ * %ODP_VPORT_ATTR_MTU are optional, and other attributes are ignored.
+ *
+ * For other requests, if %ODP_VPORT_ATTR_NAME is specified then it is used to
+ * look up the vport to operate on; otherwise dp_idx from the &struct
+ * odp_header plus %ODP_VPORT_ATTR_PORT_NO determine the vport.
+ */
+enum odp_vport_attr {
+	ODP_VPORT_ATTR_UNSPEC,
+	ODP_VPORT_ATTR_PORT_NO,	/* port number within datapath */
+	ODP_VPORT_ATTR_TYPE,	/* 32-bit ODP_VPORT_TYPE_* constant. */
+	ODP_VPORT_ATTR_NAME,	/* string name, up to IFNAMSIZ bytes long */
+	ODP_VPORT_ATTR_STATS,	/* struct rtnl_link_stats64 */
+	ODP_VPORT_ATTR_ADDRESS, /* hardware address */
+	ODP_VPORT_ATTR_MTU,	/* 32-bit maximum transmission unit */
+	ODP_VPORT_ATTR_OPTIONS, /* nested attributes, varies by vport type */
+	ODP_VPORT_ATTR_IFINDEX, /* 32-bit ifindex of backing netdev */
+	ODP_VPORT_ATTR_IFLINK,	/* 32-bit ifindex on which packets are sent */
+	__ODP_VPORT_ATTR_MAX
+};
+
+#define ODP_VPORT_ATTR_MAX (__ODP_VPORT_ATTR_MAX - 1)
+
+/* ODP_VPORT_ATTR_OPTIONS attributes for patch vports. */
+enum {
+	ODP_PATCH_ATTR_UNSPEC,
+	ODP_PATCH_ATTR_PEER,	/* name of peer vport, as a string */
+	__ODP_PATCH_ATTR_MAX
+};
+
+#define ODP_PATCH_ATTR_MAX (__ODP_PATCH_ATTR_MAX - 1)
+
+/* Flows. */
+
+#define ODP_FLOW_FAMILY  "odp_flow"
+#define ODP_FLOW_MCGROUP "odp_flow"
+
+enum odp_flow_cmd {
+	ODP_FLOW_CMD_UNSPEC,
+	ODP_FLOW_CMD_NEW,
+	ODP_FLOW_CMD_DEL,
+	ODP_FLOW_CMD_GET,
+	ODP_FLOW_CMD_SET
+};
+
+struct odp_flow_stats {
+    uint64_t n_packets;         /* Number of matched packets. */
+    uint64_t n_bytes;           /* Number of matched bytes. */
+};
+
+enum odp_key_type {
+	ODP_KEY_ATTR_UNSPEC,
+	ODP_KEY_ATTR_TUN_ID,    /* 64-bit tunnel ID */
+	ODP_KEY_ATTR_IN_PORT,   /* 32-bit ODP port number */
+	ODP_KEY_ATTR_ETHERNET,  /* struct odp_key_ethernet */
+	ODP_KEY_ATTR_8021Q,     /* struct odp_key_8021q */
+	ODP_KEY_ATTR_ETHERTYPE,	/* 16-bit Ethernet type */
+	ODP_KEY_ATTR_IPV4,      /* struct odp_key_ipv4 */
+	ODP_KEY_ATTR_IPV6,      /* struct odp_key_ipv6 */
+	ODP_KEY_ATTR_TCP,       /* struct odp_key_tcp */
+	ODP_KEY_ATTR_UDP,       /* struct odp_key_udp */
+	ODP_KEY_ATTR_ICMP,      /* struct odp_key_icmp */
+	ODP_KEY_ATTR_ICMPV6,    /* struct odp_key_icmpv6 */
+	ODP_KEY_ATTR_ARP,       /* struct odp_key_arp */
+	ODP_KEY_ATTR_ND,        /* struct odp_key_nd */
+	__ODP_KEY_ATTR_MAX
+};
+
+#define ODP_KEY_ATTR_MAX (__ODP_KEY_ATTR_MAX - 1)
+
+struct odp_key_ethernet {
+	uint8_t	 eth_src[6];
+	uint8_t	 eth_dst[6];
+};
+
+struct odp_key_8021q {
+	ovs_be16 q_tpid;
+	ovs_be16 q_tci;
+};
+
+struct odp_key_ipv4 {
+	ovs_be32 ipv4_src;
+	ovs_be32 ipv4_dst;
+	uint8_t  ipv4_proto;
+	uint8_t  ipv4_tos;
+};
+
+struct odp_key_ipv6 {
+	ovs_be32 ipv6_src[4];
+	ovs_be32 ipv6_dst[4];
+	uint8_t  ipv6_proto;
+	uint8_t  ipv6_tos;
+};
+
+struct odp_key_tcp {
+	ovs_be16 tcp_src;
+	ovs_be16 tcp_dst;
+};
+
+struct odp_key_udp {
+	ovs_be16 udp_src;
+	ovs_be16 udp_dst;
+};
+
+struct odp_key_icmp {
+	uint8_t icmp_type;
+	uint8_t icmp_code;
+};
+
+struct odp_key_icmpv6 {
+	uint8_t icmpv6_type;
+	uint8_t icmpv6_code;
+};
+
+struct odp_key_arp {
+	ovs_be32 arp_sip;
+	ovs_be32 arp_tip;
+	ovs_be16 arp_op;
+	uint8_t  arp_sha[6];
+	uint8_t  arp_tha[6];
+};
+
+struct odp_key_nd {
+	uint32_t nd_target[4];
+	uint8_t  nd_sll[6];
+	uint8_t  nd_tll[6];
+};
+
+/**
+ * enum odp_flow_attr - attributes for %ODP_FLOW_* commands.
+ * @ODP_FLOW_ATTR_KEY: Nested %ODP_KEY_ATTR_* attributes specifying the flow
+ * key.  Always present in notifications.  Required for all requests (except
+ * dumps).
+ * @ODP_FLOW_ATTR_ACTIONS: Nested %ODPAT_* attributes specifying the actions to
+ * take for packets that match the key.  Always present in notifications.
+ * Required for %ODP_FLOW_CMD_NEW requests, optional on %ODP_FLOW_CMD_SET
+ * request to change the existing actions, ignored for other requests.
+ * @ODP_FLOW_ATTR_STATS: &struct odp_flow_stats giving statistics for this
+ * flow.  Present in notifications if the stats would be nonzero.  Ignored in
+ * requests.
+ * @ODP_FLOW_ATTR_TCP_FLAGS: An 8-bit value giving the OR'd value of all of the
+ * TCP flags seen on packets in this flow.  Only present in notifications for
+ * TCP flows, and only if it would be nonzero.  Ignored in requests.
+ * @ODP_FLOW_ATTR_USED: A 64-bit integer giving the time, in milliseconds on
+ * the system monotonic clock, at which a packet was last processed for this
+ * flow.  Only present in notifications if a packet has been processed for this
+ * flow.  Ignored in requests.
+ * @ODP_FLOW_ATTR_CLEAR: If present in a %ODP_FLOW_CMD_SET request, clears the
+ * last-used time, accumulated TCP flags, and statistics for this flow.
+ * Otherwise ignored in requests.  Never present in notifications.
+ *
+ * These attributes follow the &struct odp_header within the Generic Netlink
+ * payload for %ODP_FLOW_* commands.
+ */
+enum odp_flow_attr {
+	ODP_FLOW_ATTR_UNSPEC,
+	ODP_FLOW_ATTR_KEY,       /* Sequence of ODP_KEY_ATTR_* attributes. */
+	ODP_FLOW_ATTR_ACTIONS,   /* Nested ODP_ACTION_ATTR_* attributes. */
+	ODP_FLOW_ATTR_STATS,     /* struct odp_flow_stats. */
+	ODP_FLOW_ATTR_TCP_FLAGS, /* 8-bit OR'd TCP flags. */
+	ODP_FLOW_ATTR_USED,      /* u64 msecs last used in monotonic time. */
+	ODP_FLOW_ATTR_CLEAR,     /* Flag to clear stats, tcp_flags, used. */
+	__ODP_FLOW_ATTR_MAX
+};
+
+#define ODP_FLOW_ATTR_MAX (__ODP_FLOW_ATTR_MAX - 1)
+
+/* Action types. */
+enum odp_action_type {
+	ODP_ACTION_ATTR_UNSPEC,
+	ODP_ACTION_ATTR_OUTPUT,	      /* Output to switch port. */
+	ODP_ACTION_ATTR_USERSPACE,    /* Send copy to userspace. */
+	ODP_ACTION_ATTR_SET_DL_TCI,   /* Set the 802.1q TCI value. */
+	ODP_ACTION_ATTR_STRIP_VLAN,   /* Strip the 802.1q header. */
+	ODP_ACTION_ATTR_SET_DL_SRC,   /* Ethernet source address. */
+	ODP_ACTION_ATTR_SET_DL_DST,   /* Ethernet destination address. */
+	ODP_ACTION_ATTR_SET_NW_SRC,   /* IPv4 source address. */
+	ODP_ACTION_ATTR_SET_NW_DST,   /* IPv4 destination address. */
+	ODP_ACTION_ATTR_SET_NW_TOS,   /* IP ToS/DSCP field (6 bits). */
+	ODP_ACTION_ATTR_SET_TP_SRC,   /* TCP/UDP source port. */
+	ODP_ACTION_ATTR_SET_TP_DST,   /* TCP/UDP destination port. */
+	ODP_ACTION_ATTR_SET_TUNNEL,   /* Set the encapsulating tunnel ID. */
+	ODP_ACTION_ATTR_SET_PRIORITY, /* Set skb->priority. */
+	ODP_ACTION_ATTR_POP_PRIORITY, /* Restore original skb->priority. */
+	__ODP_ACTION_ATTR_MAX
+};
+
+#define ODP_ACTION_ATTR_MAX (__ODP_ACTION_ATTR_MAX - 1)
+
+#endif  /* openvswitch/datapath-protocol.h */
diff -r 89e197c6e9d5 include/openvswitch/tunnel.h
--- /dev/null
+++ b/include/openvswitch/tunnel.h
@@ -0,0 +1,75 @@
+/*
+ * Copyright (c) 2008, 2009, 2010, 2011 Nicira Networks.
+ *
+ * This file is offered under your choice of two licenses: Apache 2.0 or GNU
+ * GPL 2.0 or later.  The permission statements for each of these licenses is
+ * given below.  You may license your modifications to this file under either
+ * of these licenses or both.  If you wish to license your modifications under
+ * only one of these licenses, delete the permission text for the other
+ * license.
+ *
+ * ----------------------------------------------------------------------
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ * ----------------------------------------------------------------------
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write to the Free Software Foundation, Inc.,
+ * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
+ * ----------------------------------------------------------------------
+ */
+
+#ifndef OPENVSWITCH_TUNNEL_H
+#define OPENVSWITCH_TUNNEL_H 1
+
+#include <linux/types.h>
+#include <linux/openvswitch.h>
+
+/* OVS_VPORT_ATTR_OPTIONS attributes for tunnels.
+ *
+ * OVS_TUNNEL_ATTR_FLAGS and OVS_TUNNEL_ATTR_DST_IPV4 are required.  All other
+ * attributes are optional.
+ */
+enum {
+	OVS_TUNNEL_ATTR_UNSPEC,
+	OVS_TUNNEL_ATTR_FLAGS,    /* 32-bit TNL_F_*. */
+	OVS_TUNNEL_ATTR_DST_IPV4, /* IPv4 destination address. */
+	OVS_TUNNEL_ATTR_SRC_IPV4, /* IPv4 source address. */
+	OVS_TUNNEL_ATTR_OUT_KEY,  /* __be64 key to use on output. */
+	OVS_TUNNEL_ATTR_IN_KEY,   /* __be64 key to match on input. */
+	OVS_TUNNEL_ATTR_TOS,      /* 8-bit TOS value. */
+	OVS_TUNNEL_ATTR_TTL,      /* 8-bit TTL value. */
+	__OVS_TUNNEL_ATTR_MAX
+};
+
+#define OVS_TUNNEL_ATTR_MAX (__OVS_TUNNEL_ATTR_MAX - 1)
+
+#define TNL_F_CSUM		(1 << 0) /* Checksum packets. */
+#define TNL_F_TOS_INHERIT	(1 << 1) /* Inherit ToS from inner packet. */
+#define TNL_F_TTL_INHERIT	(1 << 2) /* Inherit TTL from inner packet. */
+#define TNL_F_DF_INHERIT	(1 << 3) /* Inherit DF bit from inner packet. */
+#define TNL_F_DF_DEFAULT	(1 << 4) /* Set DF bit if inherit off or
+					  * not IP. */
+#define TNL_F_PMTUD		(1 << 5) /* Enable path MTU discovery. */
+#define TNL_F_HDR_CACHE		(1 << 6) /* Enable tunnel header caching. */
+#define TNL_F_IPSEC		(1 << 7) /* Traffic is IPsec encrypted. */
+
+#endif /* openvswitch/tunnel.h */
diff -r 89e197c6e9d5 include/openvswitch/types.h
--- /dev/null
+++ b/include/openvswitch/types.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (c) 2010, 2011 Nicira Networks.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at:
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#ifndef OPENVSWITCH_TYPES_H
+#define OPENVSWITCH_TYPES_H 1
+
+#include <linux/types.h>
+#include <sys/types.h>
+#include <stdint.h>
+
+#ifdef __CHECKER__
+#define OVS_BITWISE __attribute__((bitwise))
+#define OVS_FORCE __attribute__((force))
+#else
+#define OVS_BITWISE
+#define OVS_FORCE
+#endif
+
+/* The ovs_be<N> types indicate that an object is in big-endian, not
+ * native-endian, byte order.  They are otherwise equivalent to uint<N>_t.
+ *
+ * We bootstrap these from the Linux __be<N> types.  If we instead define our
+ * own independently then __be<N> and ovs_be<N> become mutually
+ * incompatible. */
+typedef __be16 ovs_be16;
+typedef __be32 ovs_be32;
+typedef __be64 ovs_be64;
+
+/* Netlink and OpenFlow both contain 64-bit values that are only guaranteed to
+ * be aligned on 32-bit boundaries.  These types help.
+ *
+ * lib/unaligned.h has helper functions for accessing these. */
+
+/* A 64-bit value, in host byte order, that is only aligned on a 32-bit
+ * boundary.  */
+typedef struct {
+#ifdef WORDS_BIGENDIAN
+	uint32_t hi, lo;
+#else
+	uint32_t lo, hi;
+#endif
+} ovs_32aligned_u64;
+
+/* A 64-bit value, in network byte order, that is only aligned on a 32-bit
+ * boundary. */
+typedef struct {
+	ovs_be32 hi, lo;
+} ovs_32aligned_be64;
+
+#endif /* openvswitch/types.h */
diff -r 89e197c6e9d5 net/openvswitch/actions.c
--- /dev/null
+++ b/net/openvswitch/actions.c
@@ -0,0 +1,465 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/skbuff.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/openvswitch.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/in6.h>
+#include <linux/if_arp.h>
+#include <linux/if_vlan.h>
+#include <net/ip.h>
+#include <net/checksum.h>
+#include <net/dsfield.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport.h"
+
+static int do_execute_actions(struct datapath *dp, struct sk_buff *skb,
+			const struct nlattr *attr, int len, bool keep_skb);
+
+static int make_writable(struct sk_buff *skb, int write_len)
+{
+	if (!skb_cloned(skb) || skb_clone_writable(skb, write_len))
+		return 0;
+
+	return pskb_expand_head(skb, 0, 0, GFP_ATOMIC);
+}
+
+/* remove VLAN header from packet and update csum accrodingly. */
+static int __pop_vlan_tci(struct sk_buff *skb, __be16 *current_tci)
+{
+	struct vlan_hdr *vhdr;
+	int err;
+
+	err = make_writable(skb, VLAN_ETH_HLEN);
+	if (unlikely(err))
+		return err;
+
+	if (get_ip_summed(skb) == OVS_CSUM_COMPLETE)
+		skb->csum = csum_sub(skb->csum, csum_partial(skb->data
+					+ ETH_HLEN, VLAN_HLEN, 0));
+
+	vhdr = (struct vlan_hdr *)(skb->data + ETH_HLEN);
+	*current_tci = vhdr->h_vlan_TCI;
+
+	memmove(skb->data + VLAN_HLEN, skb->data, 2 * ETH_ALEN);
+	__skb_pull(skb, VLAN_HLEN);
+
+	vlan_set_encap_proto(skb, vhdr);
+	skb->mac_header += VLAN_HLEN;
+	skb_reset_mac_len(skb);
+
+	return 0;
+}
+
+static int pop_vlan(struct sk_buff *skb)
+{
+	__be16 tci;
+	int err;
+
+	if (likely(vlan_tx_tag_present(skb))) {
+		vlan_set_tci(skb, 0);
+	} else {
+		if (unlikely(skb->protocol != htons(ETH_P_8021Q) ||
+			     skb->len < VLAN_ETH_HLEN))
+			return 0;
+
+		err = __pop_vlan_tci(skb, &tci);
+		if (err)
+			return err;
+	}
+	/* move next vlan tag to hw accel tag */
+	if (likely(skb->protocol != htons(ETH_P_8021Q) ||
+		   skb->len < VLAN_ETH_HLEN))
+		return 0;
+
+	err = __pop_vlan_tci(skb, &tci);
+	if (unlikely(err))
+		return err;
+
+	__vlan_hwaccel_put_tag(skb, ntohs(tci));
+	return 0;
+}
+
+static int push_vlan(struct sk_buff *skb, const struct ovs_action_push_vlan *vlan)
+{
+	if (unlikely(vlan_tx_tag_present(skb))) {
+		u16 current_tag;
+
+		/* push down current VLAN tag */
+		current_tag = vlan_tx_tag_get(skb);
+
+		if (!__vlan_put_tag(skb, current_tag))
+			return -ENOMEM;
+
+		if (get_ip_summed(skb) == OVS_CSUM_COMPLETE)
+			skb->csum = csum_add(skb->csum, csum_partial(skb->data
+					+ ETH_HLEN, VLAN_HLEN, 0));
+
+	}
+	__vlan_hwaccel_put_tag(skb, ntohs(vlan->vlan_tci) & ~VLAN_TAG_PRESENT);
+	return 0;
+}
+
+static int set_eth_addr(struct sk_buff *skb,
+			const struct ovs_key_ethernet *eth_key)
+{
+	int err;
+	err = make_writable(skb, ETH_HLEN);
+	if (unlikely(err))
+		return err;
+
+	memcpy(eth_hdr(skb)->h_source, eth_key->eth_src, ETH_ALEN);
+	memcpy(eth_hdr(skb)->h_dest, eth_key->eth_dst, ETH_ALEN);
+
+	return 0;
+}
+
+static void set_ip_addr(struct sk_buff *skb, struct iphdr *nh,
+				__be32 *addr, __be32 new_addr)
+{
+	int transport_len = skb->len - skb_transport_offset(skb);
+
+	if (nh->protocol == IPPROTO_TCP) {
+		if (likely(transport_len >= sizeof(struct tcphdr)))
+			inet_proto_csum_replace4(&tcp_hdr(skb)->check, skb,
+						 *addr, new_addr, 1);
+	} else if (nh->protocol == IPPROTO_UDP) {
+		if (likely(transport_len >= sizeof(struct udphdr)))
+			inet_proto_csum_replace4(&udp_hdr(skb)->check, skb,
+						 *addr, new_addr, 1);
+	}
+
+	csum_replace4(&nh->check, *addr, new_addr);
+	skb_clear_rxhash(skb);
+	*addr = new_addr;
+}
+
+static void set_ip_ttl(struct sk_buff *skb, struct iphdr *nh, u8 new_ttl)
+{
+	csum_replace2(&nh->check, htons(nh->ttl << 8), htons(new_ttl << 8));
+	nh->ttl = new_ttl;
+}
+
+static int set_ipv4(struct sk_buff *skb, const struct ovs_key_ipv4 *ipv4_key)
+{
+	struct iphdr *nh;
+	int err;
+
+	err = make_writable(skb, skb_network_offset(skb) +
+				 sizeof(struct iphdr));
+	if (unlikely(err))
+		return err;
+
+	nh = ip_hdr(skb);
+
+	if (ipv4_key->ipv4_src != nh->saddr)
+		set_ip_addr(skb, nh, &nh->saddr, ipv4_key->ipv4_src);
+
+	if (ipv4_key->ipv4_dst != nh->daddr)
+		set_ip_addr(skb, nh, &nh->daddr, ipv4_key->ipv4_dst);
+
+	if (ipv4_key->ipv4_tos != nh->tos)
+		ipv4_change_dsfield(nh, 0, ipv4_key->ipv4_tos);
+
+	if (ipv4_key->ipv4_ttl != nh->ttl)
+		set_ip_ttl(skb, nh, ipv4_key->ipv4_ttl);
+
+	return 0;
+}
+
+/* Must follow make_writable() since that can move the skb data. */
+static void set_tp_port(struct sk_buff *skb, __be16 *port,
+			 __be16 new_port, __sum16 *check)
+{
+	inet_proto_csum_replace2(check, skb, *port, new_port, 0);
+	*port = new_port;
+	skb_clear_rxhash(skb);
+}
+
+static int set_udp_port(struct sk_buff *skb,
+			const struct ovs_key_udp *udp_port_key)
+{
+	struct udphdr *uh;
+	int err;
+
+	err = make_writable(skb, skb_transport_offset(skb) +
+				 sizeof(struct udphdr));
+	if (unlikely(err))
+		return err;
+
+	uh = udp_hdr(skb);
+	if (udp_port_key->udp_src != uh->source)
+		set_tp_port(skb, &uh->source, udp_port_key->udp_src, &uh->check);
+
+	if (udp_port_key->udp_dst != uh->dest)
+		set_tp_port(skb, &uh->dest, udp_port_key->udp_dst, &uh->check);
+
+	return 0;
+}
+
+static int set_tcp_port(struct sk_buff *skb,
+			const struct ovs_key_tcp *tcp_port_key)
+{
+	struct tcphdr *th;
+	int err;
+
+	err = make_writable(skb, skb_transport_offset(skb) +
+				 sizeof(struct tcphdr));
+	if (unlikely(err))
+		return err;
+
+	th = tcp_hdr(skb);
+	if (tcp_port_key->tcp_src != th->source)
+		set_tp_port(skb, &th->source, tcp_port_key->tcp_src, &th->check);
+
+	if (tcp_port_key->tcp_dst != th->dest)
+		set_tp_port(skb, &th->dest, tcp_port_key->tcp_dst, &th->check);
+
+	return 0;
+}
+
+static int do_output(struct datapath *dp, struct sk_buff *skb, int out_port)
+{
+	struct vport *vport;
+
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	vport = rcu_dereference(dp->ports[out_port]);
+	if (unlikely(!vport)) {
+		kfree_skb(skb);
+		return -ENODEV;
+	}
+
+	ovs_vport_send(vport, skb);
+	return 0;
+}
+
+static int output_userspace(struct datapath *dp, struct sk_buff *skb,
+			    const struct nlattr *attr)
+{
+	struct dp_upcall_info upcall;
+	const struct nlattr *a;
+	int rem;
+
+	upcall.cmd = OVS_PACKET_CMD_ACTION;
+	upcall.key = &OVS_CB(skb)->flow->key;
+	upcall.userdata = NULL;
+	upcall.pid = 0;
+
+	for (a = nla_data(attr), rem = nla_len(attr); rem > 0;
+		 a = nla_next(a, &rem)) {
+		switch (nla_type(a)) {
+		case OVS_USERSPACE_ATTR_USERDATA:
+			upcall.userdata = a;
+			break;
+
+		case OVS_USERSPACE_ATTR_PID:
+			upcall.pid = nla_get_u32(a);
+			break;
+		}
+	}
+
+	return ovs_dp_upcall(dp, skb, &upcall);
+}
+
+static int sample(struct datapath *dp, struct sk_buff *skb,
+		  const struct nlattr *attr)
+{
+	const struct nlattr *acts_list = NULL;
+	const struct nlattr *a;
+	int rem;
+
+	for (a = nla_data(attr), rem = nla_len(attr); rem > 0;
+		 a = nla_next(a, &rem)) {
+		switch (nla_type(a)) {
+		case OVS_SAMPLE_ATTR_PROBABILITY:
+			if (net_random() >= nla_get_u32(a))
+				return 0;
+			break;
+
+		case OVS_SAMPLE_ATTR_ACTIONS:
+			acts_list = a;
+			break;
+		}
+	}
+
+	return do_execute_actions(dp, skb, nla_data(acts_list),
+						 nla_len(acts_list), true);
+}
+
+static int execute_set_action(struct sk_buff *skb,
+				 const struct nlattr *nested_attr)
+{
+	int err = 0;
+
+	switch (nla_type(nested_attr)) {
+	case OVS_KEY_ATTR_PRIORITY:
+		skb->priority = nla_get_u32(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_TUN_ID:
+		OVS_CB(skb)->tun_id = nla_get_be64(nested_attr);
+		break;
+
+	case OVS_KEY_ATTR_ETHERNET:
+		err = set_eth_addr(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_IPV4:
+		err = set_ipv4(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_TCP:
+		err = set_tcp_port(skb, nla_data(nested_attr));
+		break;
+
+	case OVS_KEY_ATTR_UDP:
+		err = set_udp_port(skb, nla_data(nested_attr));
+		break;
+	}
+
+	return err;
+}
+
+/* Execute a list of actions against 'skb'. */
+static int do_execute_actions(struct datapath *dp, struct sk_buff *skb,
+			const struct nlattr *attr, int len, bool keep_skb)
+{
+	/* Every output action needs a separate clone of 'skb', but the common
+	 * case is just a single output action, so that doing a clone and
+	 * then freeing the original skbuff is wasteful.  So the following code
+	 * is slightly obscure just to avoid that. */
+	int prev_port = -1;
+	const struct nlattr *a;
+	int rem;
+
+	for (a = attr, rem = len; rem > 0;
+	     a = nla_next(a, &rem)) {
+		int err = 0;
+
+		if (prev_port != -1) {
+			do_output(dp, skb_clone(skb, GFP_ATOMIC), prev_port);
+			prev_port = -1;
+		}
+
+		switch (nla_type(a)) {
+		case OVS_ACTION_ATTR_OUTPUT:
+			prev_port = nla_get_u32(a);
+			break;
+
+		case OVS_ACTION_ATTR_USERSPACE:
+			output_userspace(dp, skb, a);
+			break;
+
+		case OVS_ACTION_ATTR_PUSH_VLAN:
+			err = push_vlan(skb, nla_data(a));
+			if (unlikely(err)) /* skb already freed. */
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_POP_VLAN:
+			err = pop_vlan(skb);
+			break;
+
+		case OVS_ACTION_ATTR_SET:
+			err = execute_set_action(skb, nla_data(a));
+			break;
+
+		case OVS_ACTION_ATTR_SAMPLE:
+			err = sample(dp, skb, a);
+			break;
+		}
+
+		if (unlikely(err)) {
+			kfree_skb(skb);
+			return err;
+		}
+	}
+
+	if (prev_port != -1) {
+		if (keep_skb)
+			skb = skb_clone(skb, GFP_ATOMIC);
+
+		do_output(dp, skb, prev_port);
+	} else if (!keep_skb)
+		consume_skb(skb);
+
+	return 0;
+}
+
+/* We limit the number of times that we pass into execute_actions()
+ * to avoid blowing out the stack in the event that we have a loop. */
+#define MAX_LOOPS 5
+
+struct loop_counter {
+	u8 count;		/* Count. */
+	bool looping;		/* Loop detected? */
+};
+
+static DEFINE_PER_CPU(struct loop_counter, loop_counters);
+
+static int loop_suppress(struct datapath *dp, struct sw_flow_actions *actions)
+{
+	if (net_ratelimit())
+		pr_warn("%s: flow looped %d times, dropping\n",
+				ovs_dp_name(dp), MAX_LOOPS);
+	actions->actions_len = 0;
+	return -ELOOP;
+}
+
+/* Execute a list of actions against 'skb'. */
+int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb)
+{
+	struct sw_flow_actions *acts = rcu_dereference(OVS_CB(skb)->flow->sf_acts);
+	struct loop_counter *loop;
+	int error;
+
+	/* Check whether we've looped too much. */
+	loop = &__get_cpu_var(loop_counters);
+	if (unlikely(++loop->count > MAX_LOOPS))
+		loop->looping = true;
+	if (unlikely(loop->looping)) {
+		error = loop_suppress(dp, acts);
+		kfree_skb(skb);
+		goto out_loop;
+	}
+
+	OVS_CB(skb)->tun_id = 0;
+	error = do_execute_actions(dp, skb, acts->actions,
+					 acts->actions_len, false);
+
+	/* Check whether sub-actions looped too much. */
+	if (unlikely(loop->looping))
+		error = loop_suppress(dp, acts);
+
+out_loop:
+	/* Decrement loop counter. */
+	if (!--loop->count)
+		loop->looping = false;
+
+	return error;
+}
diff -r 89e197c6e9d5 net/openvswitch/actions.h
--- /dev/null
+++ b/net/openvswitch/actions.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2009, 2010, 2011 Nicira Networks.
+ * Distributed under the terms of the GNU GPL version 2.
+ *
+ * Significant portions of this file may be copied from parts of the Linux
+ * kernel, by Linus Torvalds and others.
+ */
+
+#ifndef ACTIONS_H
+#define ACTIONS_H 1
+
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+struct datapath;
+struct sk_buff;
+struct sw_flow_key;
+
+int execute_actions(struct datapath *dp, struct sk_buff *skb);
+
+static inline void skb_clear_rxhash(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,35)
+	skb->rxhash = 0;
+#endif
+}
+
+#endif /* actions.h */
diff -r 89e197c6e9d5 net/openvswitch/brcompat.c
--- /dev/null
+++ b/net/openvswitch/brcompat.c
@@ -0,0 +1,571 @@
+/*
+ * Copyright (c) 2007-2012 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/uaccess.h>
+#include <linux/completion.h>
+#include <linux/etherdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <net/genetlink.h>
+
+#include "openvswitch/brcompat-netlink.h"
+#include "datapath.h"
+
+static struct genl_family brc_genl_family;
+static struct genl_multicast_group brc_mc_group;
+
+/* Time to wait for ovs-vswitchd to respond to a datapath action, in
+ * jiffies. */
+#define BRC_TIMEOUT (HZ * 5)
+
+/* Mutex to serialize ovs-brcompatd callbacks.  (Some callbacks naturally hold
+ * br_ioctl_mutex, others hold rtnl_lock, but we can't take the former
+ * ourselves and we don't want to hold the latter over a potentially long
+ * period of time.) */
+static DEFINE_MUTEX(brc_serial);
+
+/* Userspace communication. */
+static DEFINE_SPINLOCK(brc_lock);    /* Ensure atomic access to these vars. */
+static DECLARE_COMPLETION(brc_done); /* Userspace signaled operation done? */
+static struct sk_buff *brc_reply;    /* Reply from userspace. */
+static u32 brc_seq;		     /* Sequence number for current op. */
+
+static struct sk_buff *brc_send_command(struct sk_buff *,
+					struct nlattr **attrs);
+static int brc_send_simple_command(struct sk_buff *);
+
+static struct sk_buff *brc_make_request(int op, const char *bridge,
+					const char *port)
+{
+	struct sk_buff *skb = genlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);
+	if (!skb)
+		goto error;
+
+	genlmsg_put(skb, 0, 0, &brc_genl_family, 0, op);
+	if (bridge)
+		NLA_PUT_STRING(skb, BRC_GENL_A_DP_NAME, bridge);
+	if (port)
+		NLA_PUT_STRING(skb, BRC_GENL_A_PORT_NAME, port);
+	return skb;
+
+nla_put_failure:
+	kfree_skb(skb);
+error:
+	return NULL;
+}
+
+static int brc_send_simple_command(struct sk_buff *request)
+{
+	struct nlattr *attrs[BRC_GENL_A_MAX + 1];
+	struct sk_buff *reply;
+	int error;
+
+	reply = brc_send_command(request, attrs);
+	if (IS_ERR(reply))
+		return PTR_ERR(reply);
+
+	error = nla_get_u32(attrs[BRC_GENL_A_ERR_CODE]);
+	kfree_skb(reply);
+	return -error;
+}
+
+static int brc_add_del_bridge(char __user *uname, int add)
+{
+	struct sk_buff *request;
+	char name[IFNAMSIZ];
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (copy_from_user(name, uname, IFNAMSIZ))
+		return -EFAULT;
+
+	name[IFNAMSIZ - 1] = 0;
+	request = brc_make_request(add ? BRC_GENL_C_DP_ADD : BRC_GENL_C_DP_DEL,
+				   name, NULL);
+	if (!request)
+		return -ENOMEM;
+
+	return brc_send_simple_command(request);
+}
+
+static int brc_get_indices(int op, const char *br_name,
+			   int __user *uindices, int n)
+{
+	struct nlattr *attrs[BRC_GENL_A_MAX + 1];
+	struct sk_buff *request, *reply;
+	int *indices;
+	int ret;
+	int len;
+
+	if (n < 0)
+		return -EINVAL;
+	if (n >= 2048)
+		return -ENOMEM;
+
+	request = brc_make_request(op, br_name, NULL);
+	if (!request)
+		return -ENOMEM;
+
+	reply = brc_send_command(request, attrs);
+	ret = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit;
+
+	ret = -nla_get_u32(attrs[BRC_GENL_A_ERR_CODE]);
+	if (ret < 0)
+		goto exit_free_skb;
+
+	ret = -EINVAL;
+	if (!attrs[BRC_GENL_A_IFINDEXES])
+		goto exit_free_skb;
+
+	len = nla_len(attrs[BRC_GENL_A_IFINDEXES]);
+	indices = nla_data(attrs[BRC_GENL_A_IFINDEXES]);
+	if (len % sizeof(int))
+		goto exit_free_skb;
+
+	n = min_t(int, n, len / sizeof(int));
+	ret = copy_to_user(uindices, indices, n * sizeof(int)) ? -EFAULT : n;
+
+exit_free_skb:
+	kfree_skb(reply);
+exit:
+	return ret;
+}
+
+/* Called with br_ioctl_mutex. */
+static int brc_get_bridges(int __user *uindices, int n)
+{
+	return brc_get_indices(BRC_GENL_C_GET_BRIDGES, NULL, uindices, n);
+}
+
+/* Legacy deviceless bridge ioctl's.  Called with br_ioctl_mutex. */
+static int old_deviceless(void __user *uarg)
+{
+	unsigned long args[3];
+
+	if (copy_from_user(args, uarg, sizeof(args)))
+		return -EFAULT;
+
+	switch (args[0]) {
+	case BRCTL_GET_BRIDGES:
+		return brc_get_bridges((int __user *)args[1], args[2]);
+
+	case BRCTL_ADD_BRIDGE:
+		return brc_add_del_bridge((void __user *)args[1], 1);
+	case BRCTL_DEL_BRIDGE:
+		return brc_add_del_bridge((void __user *)args[1], 0);
+	}
+
+	return -EOPNOTSUPP;
+}
+
+/* Called with the br_ioctl_mutex. */
+static int
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,23)
+brc_ioctl_deviceless_stub(unsigned int cmd, void __user *uarg)
+#else
+brc_ioctl_deviceless_stub(struct net *net, unsigned int cmd, void __user *uarg)
+#endif
+{
+	switch (cmd) {
+	case SIOCGIFBR:
+	case SIOCSIFBR:
+		return old_deviceless(uarg);
+
+	case SIOCBRADDBR:
+		return brc_add_del_bridge(uarg, 1);
+	case SIOCBRDELBR:
+		return brc_add_del_bridge(uarg, 0);
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static int brc_add_del_port(struct net_device *dev, int port_ifindex, int add)
+{
+	struct sk_buff *request;
+	struct net_device *port;
+	int err;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	port = __dev_get_by_index(&init_net, port_ifindex);
+	if (!port)
+		return -EINVAL;
+
+	/* Save name of dev and port because there's a race between the
+	 * rtnl_unlock() and the brc_send_simple_command(). */
+	request = brc_make_request(add ? BRC_GENL_C_PORT_ADD : BRC_GENL_C_PORT_DEL,
+				   dev->name, port->name);
+	if (!request)
+		return -ENOMEM;
+
+	rtnl_unlock();
+	err = brc_send_simple_command(request);
+	rtnl_lock();
+
+	return err;
+}
+
+static int brc_get_bridge_info(struct net_device *dev,
+			       struct __bridge_info __user *ub)
+{
+	struct __bridge_info b;
+
+	memset(&b, 0, sizeof(struct __bridge_info));
+
+	/* First two bytes are the priority, which we should skip.  This comes
+	 * from struct bridge_id in br_private.h, which is unavailable to us.
+	 */
+	memcpy((u8 *)&b.bridge_id + 2, dev->dev_addr, ETH_ALEN);
+	b.stp_enabled = 0;
+
+	if (copy_to_user(ub, &b, sizeof(struct __bridge_info)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int brc_get_port_list(struct net_device *dev, int __user *uindices,
+			     int num)
+{
+	int retval;
+
+	rtnl_unlock();
+	retval = brc_get_indices(BRC_GENL_C_GET_PORTS, dev->name,
+				 uindices, num);
+	rtnl_lock();
+
+	return retval;
+}
+
+/*
+ * Format up to a page worth of forwarding table entries
+ * userbuf -- where to copy result
+ * maxnum  -- maximum number of entries desired
+ *            (limited to a page for sanity)
+ * offset  -- number of records to skip
+ */
+static int brc_get_fdb_entries(struct net_device *dev, void __user *userbuf,
+			       unsigned long maxnum, unsigned long offset)
+{
+	struct nlattr *attrs[BRC_GENL_A_MAX + 1];
+	struct sk_buff *request, *reply;
+	int retval;
+	int len;
+
+	/* Clamp size to PAGE_SIZE, test maxnum to avoid overflow */
+	if (maxnum > PAGE_SIZE/sizeof(struct __fdb_entry))
+		maxnum = PAGE_SIZE/sizeof(struct __fdb_entry);
+
+	request = brc_make_request(BRC_GENL_C_FDB_QUERY, dev->name, NULL);
+	if (!request)
+		return -ENOMEM;
+	NLA_PUT_U64(request, BRC_GENL_A_FDB_COUNT, maxnum);
+	NLA_PUT_U64(request, BRC_GENL_A_FDB_SKIP, offset);
+
+	rtnl_unlock();
+	reply = brc_send_command(request, attrs);
+	retval = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit;
+
+	retval = -nla_get_u32(attrs[BRC_GENL_A_ERR_CODE]);
+	if (retval < 0)
+		goto exit_free_skb;
+
+	retval = -EINVAL;
+	if (!attrs[BRC_GENL_A_FDB_DATA])
+		goto exit_free_skb;
+	len = nla_len(attrs[BRC_GENL_A_FDB_DATA]);
+	if (len % sizeof(struct __fdb_entry) ||
+	    len / sizeof(struct __fdb_entry) > maxnum)
+		goto exit_free_skb;
+
+	retval = len / sizeof(struct __fdb_entry);
+	if (copy_to_user(userbuf, nla_data(attrs[BRC_GENL_A_FDB_DATA]), len))
+		retval = -EFAULT;
+
+exit_free_skb:
+	kfree_skb(reply);
+exit:
+	rtnl_lock();
+	return retval;
+
+nla_put_failure:
+	kfree_skb(request);
+	return -ENOMEM;
+}
+
+/* Legacy ioctl's through SIOCDEVPRIVATE.  Called with rtnl_lock. */
+static int old_dev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	unsigned long args[4];
+
+	if (copy_from_user(args, rq->ifr_data, sizeof(args)))
+		return -EFAULT;
+
+	switch (args[0]) {
+	case BRCTL_ADD_IF:
+		return brc_add_del_port(dev, args[1], 1);
+	case BRCTL_DEL_IF:
+		return brc_add_del_port(dev, args[1], 0);
+
+	case BRCTL_GET_BRIDGE_INFO:
+		return brc_get_bridge_info(dev, (struct __bridge_info __user *)args[1]);
+
+	case BRCTL_GET_PORT_LIST:
+		return brc_get_port_list(dev, (int __user *)args[1], args[2]);
+
+	case BRCTL_GET_FDB_ENTRIES:
+		return brc_get_fdb_entries(dev, (void __user *)args[1],
+					   args[2], args[3]);
+	}
+
+	return -EOPNOTSUPP;
+}
+
+/* Called with the rtnl_lock. */
+static int brc_dev_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int err;
+
+	switch (cmd) {
+	case SIOCDEVPRIVATE:
+		err = old_dev_ioctl(dev, rq, cmd);
+		break;
+
+	case SIOCBRADDIF:
+		return brc_add_del_port(dev, rq->ifr_ifindex, 1);
+	case SIOCBRDELIF:
+		return brc_add_del_port(dev, rq->ifr_ifindex, 0);
+
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+
+static struct genl_family brc_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = 0,
+	.name = BRC_GENL_FAMILY_NAME,
+	.version = 1,
+	.maxattr = BRC_GENL_A_MAX,
+};
+
+static int brc_genl_query(struct sk_buff *skb, struct genl_info *info)
+{
+	int err = -EINVAL;
+	struct sk_buff *ans_skb;
+	void *data;
+
+	ans_skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!ans_skb)
+		return -ENOMEM;
+
+	data = genlmsg_put_reply(ans_skb, info, &brc_genl_family,
+				 0, BRC_GENL_C_QUERY_MC);
+	if (data == NULL) {
+		err = -ENOMEM;
+		goto err;
+	}
+	NLA_PUT_U32(ans_skb, BRC_GENL_A_MC_GROUP, brc_mc_group.id);
+
+	genlmsg_end(ans_skb, data);
+	return genlmsg_reply(ans_skb, info);
+
+err:
+nla_put_failure:
+	kfree_skb(ans_skb);
+	return err;
+}
+
+/* Attribute policy: what each attribute may contain.  */
+static struct nla_policy brc_genl_policy[BRC_GENL_A_MAX + 1] = {
+	[BRC_GENL_A_ERR_CODE] = { .type = NLA_U32 },
+	[BRC_GENL_A_FDB_DATA] = { .type = NLA_UNSPEC },
+};
+
+static int brc_genl_dp_result(struct sk_buff *skb, struct genl_info *info)
+{
+	unsigned long int flags;
+	int err;
+
+	if (!info->attrs[BRC_GENL_A_ERR_CODE])
+		return -EINVAL;
+
+	skb = skb_clone(skb, GFP_KERNEL);
+	if (!skb)
+		return -ENOMEM;
+
+	spin_lock_irqsave(&brc_lock, flags);
+	if (brc_seq == info->snd_seq) {
+		brc_seq++;
+
+		kfree_skb(brc_reply);
+		brc_reply = skb;
+
+		complete(&brc_done);
+		err = 0;
+	} else {
+		kfree_skb(skb);
+		err = -ESTALE;
+	}
+	spin_unlock_irqrestore(&brc_lock, flags);
+
+	return err;
+}
+
+static struct genl_ops brc_genl_ops[] = {
+	{ .cmd = BRC_GENL_C_QUERY_MC,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privelege. */
+	  .policy = NULL,
+	  .doit = brc_genl_query,
+	},
+	{ .cmd = BRC_GENL_C_DP_RESULT,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privelege. */
+	  .policy = brc_genl_policy,
+	  .doit = brc_genl_dp_result,
+	},
+};
+
+static struct sk_buff *brc_send_command(struct sk_buff *request,
+					struct nlattr **attrs)
+{
+	unsigned long int flags;
+	struct sk_buff *reply;
+	int error;
+
+	mutex_lock(&brc_serial);
+
+	/* Increment sequence number first, so that we ignore any replies
+	 * to stale requests. */
+	spin_lock_irqsave(&brc_lock, flags);
+	nlmsg_hdr(request)->nlmsg_seq = ++brc_seq;
+	INIT_COMPLETION(brc_done);
+	spin_unlock_irqrestore(&brc_lock, flags);
+
+	nlmsg_end(request, nlmsg_hdr(request));
+
+	/* Send message. */
+	error = genlmsg_multicast(request, 0, brc_mc_group.id, GFP_KERNEL);
+	if (error < 0)
+		goto error;
+
+	/* Wait for reply. */
+	error = -ETIMEDOUT;
+	if (!wait_for_completion_timeout(&brc_done, BRC_TIMEOUT)) {
+		pr_warn("timed out waiting for userspace\n");
+		goto error;
+	}
+
+	/* Grab reply. */
+	spin_lock_irqsave(&brc_lock, flags);
+	reply = brc_reply;
+	brc_reply = NULL;
+	spin_unlock_irqrestore(&brc_lock, flags);
+
+	mutex_unlock(&brc_serial);
+
+	/* Re-parse message.  Can't fail, since it parsed correctly once
+	 * already. */
+	error = nlmsg_parse(nlmsg_hdr(reply), GENL_HDRLEN,
+			    attrs, BRC_GENL_A_MAX, brc_genl_policy);
+	WARN_ON(error);
+
+	return reply;
+
+error:
+	mutex_unlock(&brc_serial);
+	return ERR_PTR(error);
+}
+
+static int __init brc_init(void)
+{
+	int err;
+
+	pr_info("Open vSwitch Bridge Compatibility, built "__DATE__" "__TIME__"\n");
+
+	/* Set the bridge ioctl handler */
+	brioctl_set(brc_ioctl_deviceless_stub);
+
+	/* Set the openvswitch_mod device ioctl handler */
+	ovs_dp_ioctl_hook = brc_dev_ioctl;
+
+	/* Randomize the initial sequence number.  This is not a security
+	 * feature; it only helps avoid crossed wires between userspace and
+	 * the kernel when the module is unloaded and reloaded. */
+	brc_seq = net_random();
+
+	/* Register generic netlink family to communicate changes to
+	 * userspace. */
+	err = genl_register_family_with_ops(&brc_genl_family,
+					    brc_genl_ops, ARRAY_SIZE(brc_genl_ops));
+	if (err)
+		goto error;
+
+	strcpy(brc_mc_group.name, "brcompat");
+	err = genl_register_mc_group(&brc_genl_family, &brc_mc_group);
+	if (err < 0)
+		goto err_unregister;
+
+	return 0;
+
+err_unregister:
+	genl_unregister_family(&brc_genl_family);
+error:
+	pr_emerg("failed to install!\n");
+	return err;
+}
+
+static void brc_cleanup(void)
+{
+	/* Unregister ioctl hooks */
+	ovs_dp_ioctl_hook = NULL;
+	brioctl_set(NULL);
+
+	genl_unregister_family(&brc_genl_family);
+}
+
+module_init(brc_init);
+module_exit(brc_cleanup);
+
+MODULE_DESCRIPTION("Open vSwitch bridge compatibility");
+MODULE_AUTHOR("Nicira Networks");
+MODULE_LICENSE("GPL");
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+/*
+ * In kernels 2.6.36 and later, Open vSwitch can safely coexist with
+ * the Linux bridge module, but it does not make sense to load both bridge and
+ * brcompat_mod, so this prevents it.
+ */
+BRIDGE_MUTUAL_EXCLUSION;
+#endif
diff -r 89e197c6e9d5 net/openvswitch/checksum.c
--- /dev/null
+++ b/net/openvswitch/checksum.c
@@ -0,0 +1,271 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+
+#include "checksum.h"
+#include "datapath.h"
+
+#ifdef NEED_CSUM_NORMALIZE
+
+#if defined(CONFIG_XEN) && defined(HAVE_PROTO_DATA_VALID)
+/* This code is based on skb_checksum_setup() from Xen's net/dev/core.c.  We
+ * can't call this function directly because it isn't exported in all
+ * versions. */
+static int vswitch_skb_checksum_setup(struct sk_buff *skb)
+{
+	struct iphdr *iph;
+	unsigned char *th;
+	int err = -EPROTO;
+	__u16 csum_start, csum_offset;
+
+	if (!skb->proto_csum_blank)
+		return 0;
+
+	if (skb->protocol != htons(ETH_P_IP))
+		goto out;
+
+	if (!pskb_may_pull(skb, skb_network_header(skb) + sizeof(struct iphdr) - skb->data))
+		goto out;
+
+	iph = ip_hdr(skb);
+	th = skb_network_header(skb) + 4 * iph->ihl;
+
+	csum_start = th - skb->head;
+	switch (iph->protocol) {
+	case IPPROTO_TCP:
+		csum_offset = offsetof(struct tcphdr, check);
+		break;
+	case IPPROTO_UDP:
+		csum_offset = offsetof(struct udphdr, check);
+		break;
+	default:
+		if (net_ratelimit())
+			pr_err("Attempting to checksum a non-TCP/UDP packet, "
+			       "dropping a protocol %d packet",
+			       iph->protocol);
+		goto out;
+	}
+
+	if (!pskb_may_pull(skb, th + csum_offset + 2 - skb->data))
+		goto out;
+
+	skb->proto_csum_blank = 0;
+	set_ip_summed(skb, OVS_CSUM_PARTIAL);
+	set_skb_csum_pointers(skb, csum_start, csum_offset);
+
+	err = 0;
+
+out:
+	return err;
+}
+#else
+static int vswitch_skb_checksum_setup(struct sk_buff *skb)
+{
+	return 0;
+}
+#endif /* not Xen old style checksums */
+
+/*
+ *	compute_ip_summed - map external checksum state onto OVS representation
+ *
+ * @skb: Packet to manipulate.
+ * @xmit: Whether we were on transmit path of network stack.  For example,
+ *	  this is true for the internal dev vport because it receives skbs
+ *	  that passed through dev_queue_xmit() but false for the netdev vport
+ *	  because its packets come from netif_receive_skb().
+ *
+ * Older kernels (and various versions of Xen) were not explicit enough about
+ * checksum offload parameters and rely on a combination of context and
+ * non standard fields.  This deals with all those variations so that we
+ * can internally manipulate checksum offloads without worrying about kernel
+ * version.
+ *
+ * Types of checksums that we can receive (these all refer to L4 checksums):
+ * 1. CHECKSUM_NONE: Device that did not compute checksum, contains full
+ *	(though not verified) checksum in packet but not in skb->csum.  Packets
+ *	from the bridge local port will also have this type.
+ * 2. CHECKSUM_COMPLETE (CHECKSUM_HW): Good device that computes checksums,
+ *	also the GRE module.  This is the same as CHECKSUM_NONE, except it has
+ *	a valid skb->csum.  Importantly, both contain a full checksum (not
+ *	verified) in the packet itself.  The only difference is that if the
+ *	packet gets to L4 processing on this machine (not in DomU) we won't
+ *	have to recompute the checksum to verify.  Most hardware devices do not
+ *	produce packets with this type, even if they support receive checksum
+ *	offloading (they produce type #5).
+ * 3. CHECKSUM_PARTIAL (CHECKSUM_HW): Packet without full checksum and needs to
+ *	be computed if it is sent off box.  Unfortunately on earlier kernels,
+ *	this case is impossible to distinguish from #2, despite having opposite
+ *	meanings.  Xen adds an extra field on earlier kernels (see #4) in order
+ *	to distinguish the different states.
+ * 4. CHECKSUM_UNNECESSARY (with proto_csum_blank true): This packet was
+ *	generated locally by a Xen DomU and has a partial checksum.  If it is
+ *	handled on this machine (Dom0 or DomU), then the checksum will not be
+ *	computed.  If it goes off box, the checksum in the packet needs to be
+ *	completed.  Calling skb_checksum_setup converts this to CHECKSUM_HW
+ *	(CHECKSUM_PARTIAL) so that the checksum can be completed.  In later
+ *	kernels, this combination is replaced with CHECKSUM_PARTIAL.
+ * 5. CHECKSUM_UNNECESSARY (with proto_csum_blank false): Packet with a correct
+ *	full checksum or using a protocol without a checksum.  skb->csum is
+ *	undefined.  This is common from devices with receive checksum
+ *	offloading.  This is somewhat similar to CHECKSUM_NONE, except that
+ *	nobody will try to verify the checksum with CHECKSUM_UNNECESSARY.
+ *
+ * Note that on earlier kernels, CHECKSUM_COMPLETE and CHECKSUM_PARTIAL are
+ * both defined as CHECKSUM_HW.  Normally the meaning of CHECKSUM_HW is clear
+ * based on whether it is on the transmit or receive path.  After the datapath
+ * it will be intepreted as CHECKSUM_PARTIAL.  If the packet already has a
+ * checksum, we will panic.  Since we can receive packets with checksums, we
+ * assume that all CHECKSUM_HW packets have checksums and map them to
+ * CHECKSUM_NONE, which has a similar meaning (the it is only different if the
+ * packet is processed by the local IP stack, in which case it will need to
+ * be reverified).  If we receive a packet with CHECKSUM_HW that really means
+ * CHECKSUM_PARTIAL, it will be sent with the wrong checksum.  However, there
+ * shouldn't be any devices that do this with bridging.
+ */
+int compute_ip_summed(struct sk_buff *skb, bool xmit)
+{
+	/* For our convenience these defines change repeatedly between kernel
+	 * versions, so we can't just copy them over...
+	 */
+	switch (skb->ip_summed) {
+	case CHECKSUM_NONE:
+		set_ip_summed(skb, OVS_CSUM_NONE);
+		break;
+	case CHECKSUM_UNNECESSARY:
+		set_ip_summed(skb, OVS_CSUM_UNNECESSARY);
+		break;
+#ifdef CHECKSUM_HW
+	/* In theory this could be either CHECKSUM_PARTIAL or CHECKSUM_COMPLETE.
+	 * However, on the receive side we should only get CHECKSUM_PARTIAL
+	 * packets from Xen, which uses some special fields to represent this
+	 * (see vswitch_skb_checksum_setup()).  Since we can only make one type
+	 * work, pick the one that actually happens in practice.
+	 *
+	 * On the transmit side (basically after skb_checksum_setup()
+	 * has been run or on internal dev transmit), packets with
+	 * CHECKSUM_COMPLETE aren't generated, so assume CHECKSUM_PARTIAL.
+	 */
+	case CHECKSUM_HW:
+		if (!xmit)
+			set_ip_summed(skb, OVS_CSUM_COMPLETE);
+		else
+			set_ip_summed(skb, OVS_CSUM_PARTIAL);
+		break;
+#else
+	case CHECKSUM_COMPLETE:
+		set_ip_summed(skb, OVS_CSUM_COMPLETE);
+		break;
+	case CHECKSUM_PARTIAL:
+		set_ip_summed(skb, OVS_CSUM_PARTIAL);
+		break;
+#endif
+	}
+
+	OVS_CB(skb)->csum_start = skb_headroom(skb) + skb_transport_offset(skb);
+
+	return vswitch_skb_checksum_setup(skb);
+}
+
+/*
+ *     forward_ip_summed - map internal checksum state back onto native
+ *			   kernel fields.
+ *
+ * @skb: Packet to manipulate.
+ * @xmit: Whether we are about send on the transmit path the network stack.
+ *	  This follows the same logic as the @xmit field in compute_ip_summed().
+ *	  Generally, a given vport will have opposite values for @xmit passed to
+ *	  these two functions.
+ *
+ * When a packet is about to egress from OVS take our internal fields (including
+ * any modifications we have made) and recreate the correct representation for
+ * this kernel.  This may do things like change the transport header offset.
+ */
+void forward_ip_summed(struct sk_buff *skb, bool xmit)
+{
+	switch (get_ip_summed(skb)) {
+	case OVS_CSUM_NONE:
+		skb->ip_summed = CHECKSUM_NONE;
+		break;
+	case OVS_CSUM_UNNECESSARY:
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+#if defined(CONFIG_XEN) && defined(HAVE_PROTO_DATA_VALID)
+		skb->proto_data_valid = 1;
+#endif
+		break;
+#ifdef CHECKSUM_HW
+	case OVS_CSUM_COMPLETE:
+		if (!xmit)
+			skb->ip_summed = CHECKSUM_HW;
+		else
+			skb->ip_summed = CHECKSUM_NONE;
+		break;
+	case OVS_CSUM_PARTIAL:
+		if (!xmit) {
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+#if defined(CONFIG_XEN) && defined(HAVE_PROTO_DATA_VALID)
+			skb->proto_csum_blank = 1;
+#endif
+		} else {
+			skb->ip_summed = CHECKSUM_HW;
+		}
+		break;
+#else
+	case OVS_CSUM_COMPLETE:
+		skb->ip_summed = CHECKSUM_COMPLETE;
+		break;
+	case OVS_CSUM_PARTIAL:
+		skb->ip_summed = CHECKSUM_PARTIAL;
+		break;
+#endif
+	}
+
+	if (get_ip_summed(skb) == OVS_CSUM_PARTIAL)
+		skb_set_transport_header(skb, OVS_CB(skb)->csum_start -
+					      skb_headroom(skb));
+}
+
+u8 get_ip_summed(struct sk_buff *skb)
+{
+	return OVS_CB(skb)->ip_summed;
+}
+
+void set_ip_summed(struct sk_buff *skb, u8 ip_summed)
+{
+	OVS_CB(skb)->ip_summed = ip_summed;
+}
+
+void get_skb_csum_pointers(const struct sk_buff *skb, u16 *csum_start,
+			   u16 *csum_offset)
+{
+	*csum_start = OVS_CB(skb)->csum_start;
+	*csum_offset = skb->csum;
+}
+
+void set_skb_csum_pointers(struct sk_buff *skb, u16 csum_start,
+			   u16 csum_offset)
+{
+	OVS_CB(skb)->csum_start = csum_start;
+	skb->csum = csum_offset;
+}
+#endif /* NEED_CSUM_NORMALIZE */
diff -r 89e197c6e9d5 net/openvswitch/checksum.h
--- /dev/null
+++ b/net/openvswitch/checksum.h
@@ -0,0 +1,149 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef CHECKSUM_H
+#define CHECKSUM_H 1
+
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+#include <net/checksum.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22) || \
+	(defined(CONFIG_XEN) && defined(HAVE_PROTO_DATA_VALID))
+#define NEED_CSUM_NORMALIZE
+#endif
+
+/* These are the same values as the checksum constants in 2.6.22+. */
+enum csum_type {
+	OVS_CSUM_NONE = 0,
+	OVS_CSUM_UNNECESSARY = 1,
+	OVS_CSUM_COMPLETE = 2,
+	OVS_CSUM_PARTIAL = 3,
+};
+
+#ifdef NEED_CSUM_NORMALIZE
+int compute_ip_summed(struct sk_buff *skb, bool xmit);
+void forward_ip_summed(struct sk_buff *skb, bool xmit);
+u8 get_ip_summed(struct sk_buff *skb);
+void set_ip_summed(struct sk_buff *skb, u8 ip_summed);
+void get_skb_csum_pointers(const struct sk_buff *skb, u16 *csum_start,
+			   u16 *csum_offset);
+void set_skb_csum_pointers(struct sk_buff *skb, u16 csum_start,
+			   u16 csum_offset);
+#else
+static inline int compute_ip_summed(struct sk_buff *skb, bool xmit)
+{
+	return 0;
+}
+
+static inline void forward_ip_summed(struct sk_buff *skb, bool xmit) { }
+
+static inline u8 get_ip_summed(struct sk_buff *skb)
+{
+	return skb->ip_summed;
+}
+
+static inline void set_ip_summed(struct sk_buff *skb, u8 ip_summed)
+{
+	skb->ip_summed = ip_summed;
+}
+
+static inline void get_skb_csum_pointers(const struct sk_buff *skb,
+					 u16 *csum_start, u16 *csum_offset)
+{
+	*csum_start = skb->csum_start;
+	*csum_offset = skb->csum_offset;
+}
+
+static inline void set_skb_csum_pointers(struct sk_buff *skb, u16 csum_start,
+					 u16 csum_offset)
+{
+	skb->csum_start = csum_start;
+	skb->csum_offset = csum_offset;
+}
+#endif
+
+/* This is really compatibility code that belongs in the compat directory.
+ * However, it needs access to our normalized checksum values, so put it here.
+ */
+#if defined(NEED_CSUM_NORMALIZE) || LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+#define inet_proto_csum_replace4 rpl_inet_proto_csum_replace4
+static inline void inet_proto_csum_replace4(__sum16 *sum, struct sk_buff *skb,
+					    __be32 from, __be32 to,
+					    int pseudohdr)
+{
+	__be32 diff[] = { ~from, to };
+
+	if (get_ip_summed(skb) != OVS_CSUM_PARTIAL) {
+		*sum = csum_fold(csum_partial((char *)diff, sizeof(diff),
+				~csum_unfold(*sum)));
+		if (get_ip_summed(skb) == OVS_CSUM_COMPLETE && pseudohdr)
+			skb->csum = ~csum_partial((char *)diff, sizeof(diff),
+						~skb->csum);
+	} else if (pseudohdr)
+		*sum = ~csum_fold(csum_partial((char *)diff, sizeof(diff),
+				csum_unfold(*sum)));
+}
+#endif
+
+#ifdef NEED_CSUM_NORMALIZE
+static inline void update_csum_start(struct sk_buff *skb, int delta)
+{
+	if (get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+		u16 csum_start, csum_offset;
+
+		get_skb_csum_pointers(skb, &csum_start, &csum_offset);
+		set_skb_csum_pointers(skb, csum_start + delta, csum_offset);
+	}
+}
+
+static inline int rpl_pskb_expand_head(struct sk_buff *skb, int nhead,
+				       int ntail, gfp_t gfp_mask)
+{
+	int err;
+	int old_headroom = skb_headroom(skb);
+
+	err = pskb_expand_head(skb, nhead, ntail, gfp_mask);
+	if (unlikely(err))
+		return err;
+
+	update_csum_start(skb, skb_headroom(skb) - old_headroom);
+
+	return 0;
+}
+#define pskb_expand_head rpl_pskb_expand_head
+
+static inline unsigned char *rpl__pskb_pull_tail(struct sk_buff *skb,
+						  int delta)
+{
+	unsigned char *ret;
+	int old_headroom = skb_headroom(skb);
+
+	ret = __pskb_pull_tail(skb, delta);
+	if (unlikely(!ret))
+		return ret;
+
+	update_csum_start(skb, skb_headroom(skb) - old_headroom);
+
+	return ret;
+}
+#define __pskb_pull_tail rpl__pskb_pull_tail
+#endif
+
+#endif /* checksum.h */
diff -r 89e197c6e9d5 net/openvswitch/compat.h
--- /dev/null
+++ b/net/openvswitch/compat.h
@@ -0,0 +1,76 @@
+/*
+ * Copyright (c) 2007-2012 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef COMPAT_H
+#define COMPAT_H 1
+
+#include <linux/netlink.h>
+
+#ifndef HAVE_NLA_NUL_STRING
+static inline int CHECK_NUL_STRING(struct nlattr *attr, int maxlen)
+{
+	char *s;
+	int len;
+	if (!attr)
+		return 0;
+
+	len = nla_len(attr);
+	if (len >= maxlen)
+		return -EINVAL;
+
+	s = nla_data(attr);
+	if (s[len - 1] != '\0')
+		return -EINVAL;
+
+	return 0;
+}
+#else
+static inline int CHECK_NUL_STRING(struct nlattr *attr, int maxlen)
+{
+	return 0;
+}
+#endif  /* !HAVE_NLA_NUL_STRING */
+
+static inline void skb_clear_rxhash(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,35)
+	skb->rxhash = 0;
+#endif
+}
+
+/*
+ * Enforces, mutual exclusion with the Linux bridge module, by declaring and
+ * exporting br_should_route_hook.  Because the bridge module also exports the
+ * same symbol, the module loader will refuse to load both modules at the same
+ * time (e.g. "bridge: exports duplicate symbol br_should_route_hook (owned by
+ * openvswitch_mod)").
+ *
+ * Before Linux 2.6.36, Open vSwitch cannot safely coexist with the Linux
+ * bridge module, so openvswitch_mod uses this macro in those versions.  In
+ * Linux 2.6.36 and later, Open vSwitch can coexist with the bridge module, but
+ * it makes no sense to load both bridge and brcompat_mod, so brcompat_mod uses
+ * this macro in those versions.
+ *
+ * The use of "typeof" here avoids the need to track changes in the type of
+ * br_should_route_hook over various kernel versions.
+ */
+#define BRIDGE_MUTUAL_EXCLUSION					\
+	typeof(br_should_route_hook) br_should_route_hook;	\
+	EXPORT_SYMBOL(br_should_route_hook)
+
+#endif /* compat.h */
diff -r 89e197c6e9d5 net/openvswitch/datapath.c
--- /dev/null
+++ b/net/openvswitch/datapath.c
@@ -0,0 +1,2100 @@
+/*
+ * Copyright (c) 2007-2012 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/if_arp.h>
+#include <linux/if_vlan.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/jhash.h>
+#include <linux/delay.h>
+#include <linux/time.h>
+#include <linux/etherdevice.h>
+#include <linux/genetlink.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/rcupdate.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/version.h>
+#include <linux/ethtool.h>
+#include <linux/wait.h>
+#include <asm/system.h>
+#include <asm/div64.h>
+#include <linux/highmem.h>
+#include <linux/netfilter_bridge.h>
+#include <linux/netfilter_ipv4.h>
+#include <linux/inetdevice.h>
+#include <linux/list.h>
+#include <linux/openvswitch.h>
+#include <linux/rculist.h>
+#include <linux/dmi.h>
+#include <net/genetlink.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "flow.h"
+#include "vlan.h"
+#include "tunnel.h"
+#include "vport-internal_dev.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,18) || \
+    LINUX_VERSION_CODE >= KERNEL_VERSION(3,3,0)
+#error Kernels before 2.6.18 or after 3.2 are not supported by this version of Open vSwitch.
+#endif
+
+int (*ovs_dp_ioctl_hook)(struct net_device *dev, struct ifreq *rq, int cmd);
+EXPORT_SYMBOL(ovs_dp_ioctl_hook);
+
+/**
+ * DOC: Locking:
+ *
+ * Writes to device state (add/remove datapath, port, set operations on vports,
+ * etc.) are protected by RTNL.
+ *
+ * Writes to other state (flow table modifications, set miscellaneous datapath
+ * parameters, etc.) are protected by genl_mutex.  The RTNL lock nests inside
+ * genl_mutex.
+ *
+ * Reads are protected by RCU.
+ *
+ * There are a few special cases (mostly stats) that have their own
+ * synchronization but they nest under all of above and don't interact with
+ * each other.
+ */
+
+/* Global list of datapaths to enable dumping them all out.
+ * Protected by genl_mutex.
+ */
+static LIST_HEAD(dps);
+
+static struct vport *new_vport(const struct vport_parms *);
+static int queue_gso_packets(int dp_ifindex, struct sk_buff *,
+			     const struct dp_upcall_info *);
+static int queue_userspace_packet(int dp_ifindex, struct sk_buff *,
+				  const struct dp_upcall_info *);
+
+/* Must be called with rcu_read_lock, genl_mutex, or RTNL lock. */
+static struct datapath *get_dp(int dp_ifindex)
+{
+	struct datapath *dp = NULL;
+	struct net_device *dev;
+
+	rcu_read_lock();
+	dev = dev_get_by_index_rcu(&init_net, dp_ifindex);
+	if (dev) {
+		struct vport *vport = ovs_internal_dev_get_vport(dev);
+		if (vport)
+			dp = vport->dp;
+	}
+	rcu_read_unlock();
+
+	return dp;
+}
+
+/* Must be called with rcu_read_lock or RTNL lock. */
+const char *ovs_dp_name(const struct datapath *dp)
+{
+	struct vport *vport = rcu_dereference_rtnl(dp->ports[OVSP_LOCAL]);
+	return vport->ops->get_name(vport);
+}
+
+static int get_dpifindex(struct datapath *dp)
+{
+	struct vport *local;
+	int ifindex;
+
+	rcu_read_lock();
+
+	local = rcu_dereference(dp->ports[OVSP_LOCAL]);
+	if (local)
+		ifindex = local->ops->get_ifindex(local);
+	else
+		ifindex = 0;
+
+	rcu_read_unlock();
+
+	return ifindex;
+}
+
+static size_t br_nlmsg_size(void)
+{
+	return NLMSG_ALIGN(sizeof(struct ifinfomsg))
+	       + nla_total_size(IFNAMSIZ) /* IFLA_IFNAME */
+	       + nla_total_size(MAX_ADDR_LEN) /* IFLA_ADDRESS */
+	       + nla_total_size(4) /* IFLA_MASTER */
+	       + nla_total_size(4) /* IFLA_MTU */
+	       + nla_total_size(1); /* IFLA_OPERSTATE */
+}
+
+/* Caller must hold RTNL lock. */
+static int dp_fill_ifinfo(struct sk_buff *skb,
+			  const struct vport *port,
+			  int event, unsigned int flags)
+{
+	struct datapath *dp = port->dp;
+	struct ifinfomsg *hdr;
+	struct nlmsghdr *nlh;
+
+	if (!port->ops->get_ifindex)
+		return -ENODEV;
+
+	nlh = nlmsg_put(skb, 0, 0, event, sizeof(*hdr), flags);
+	if (nlh == NULL)
+		return -EMSGSIZE;
+
+	hdr = nlmsg_data(nlh);
+	hdr->ifi_family = AF_BRIDGE;
+	hdr->__ifi_pad = 0;
+	hdr->ifi_type = ARPHRD_ETHER;
+	hdr->ifi_index = port->ops->get_ifindex(port);
+	hdr->ifi_flags = port->ops->get_dev_flags(port);
+	hdr->ifi_change = 0;
+
+	NLA_PUT_STRING(skb, IFLA_IFNAME, port->ops->get_name(port));
+	NLA_PUT_U32(skb, IFLA_MASTER, get_dpifindex(dp));
+	NLA_PUT_U32(skb, IFLA_MTU, port->ops->get_mtu(port));
+#ifdef IFLA_OPERSTATE
+	NLA_PUT_U8(skb, IFLA_OPERSTATE,
+		   port->ops->is_running(port)
+			? port->ops->get_operstate(port)
+			: IF_OPER_DOWN);
+#endif
+
+	NLA_PUT(skb, IFLA_ADDRESS, ETH_ALEN, port->ops->get_addr(port));
+
+	return nlmsg_end(skb, nlh);
+
+nla_put_failure:
+	nlmsg_cancel(skb, nlh);
+	return -EMSGSIZE;
+}
+
+/* Caller must hold RTNL lock. */
+static void dp_ifinfo_notify(int event, struct vport *port)
+{
+	struct sk_buff *skb;
+	int err;
+
+	skb = nlmsg_new(br_nlmsg_size(), GFP_KERNEL);
+	if (!skb) {
+		err = -ENOBUFS;
+		goto err;
+	}
+
+	err = dp_fill_ifinfo(skb, port, event, 0);
+	if (err < 0) {
+		if (err == -ENODEV) {
+			goto out;
+		} else {
+			/* -EMSGSIZE implies BUG in br_nlmsg_size() */
+			WARN_ON(err == -EMSGSIZE);
+			goto err;
+		}
+	}
+
+	rtnl_notify(skb, &init_net, 0, RTNLGRP_LINK, NULL, GFP_KERNEL);
+
+	return;
+err:
+	rtnl_set_sk_err(&init_net, RTNLGRP_LINK, err);
+out:
+	kfree_skb(skb);
+}
+
+static void release_dp(struct kobject *kobj)
+{
+	struct datapath *dp = container_of(kobj, struct datapath, ifobj);
+	kfree(dp);
+}
+
+static struct kobj_type dp_ktype = {
+	.release = release_dp
+};
+
+static void destroy_dp_rcu(struct rcu_head *rcu)
+{
+	struct datapath *dp = container_of(rcu, struct datapath, rcu);
+
+	ovs_flow_tbl_destroy((__force struct flow_table *)dp->table);
+	free_percpu(dp->stats_percpu);
+	kobject_put(&dp->ifobj);
+}
+
+/* Called with RTNL lock and genl_lock. */
+static struct vport *new_vport(const struct vport_parms *parms)
+{
+	struct vport *vport;
+
+	vport = ovs_vport_add(parms);
+	if (!IS_ERR(vport)) {
+		struct datapath *dp = parms->dp;
+
+		rcu_assign_pointer(dp->ports[parms->port_no], vport);
+		list_add(&vport->node, &dp->port_list);
+
+		dp_ifinfo_notify(RTM_NEWLINK, vport);
+	}
+
+	return vport;
+}
+
+/* Called with RTNL lock. */
+void ovs_dp_detach_port(struct vport *p)
+{
+	ASSERT_RTNL();
+
+	if (p->port_no != OVSP_LOCAL)
+		ovs_dp_sysfs_del_if(p);
+	dp_ifinfo_notify(RTM_DELLINK, p);
+
+	/* First drop references to device. */
+	list_del(&p->node);
+	rcu_assign_pointer(p->dp->ports[p->port_no], NULL);
+
+	/* Then destroy it. */
+	ovs_vport_del(p);
+}
+
+/* Must be called with rcu_read_lock. */
+void ovs_dp_process_received_packet(struct vport *p, struct sk_buff *skb)
+{
+	struct datapath *dp = p->dp;
+	struct sw_flow *flow;
+	struct dp_stats_percpu *stats;
+	u64 *stats_counter;
+	int error;
+
+	stats = per_cpu_ptr(dp->stats_percpu, smp_processor_id());
+
+	if (!OVS_CB(skb)->flow) {
+		struct sw_flow_key key;
+		int key_len;
+
+		/* Extract flow from 'skb' into 'key'. */
+		error = ovs_flow_extract(skb, p->port_no, &key, &key_len);
+		if (unlikely(error)) {
+			kfree_skb(skb);
+			return;
+		}
+
+		/* Look up flow. */
+		flow = ovs_flow_tbl_lookup(rcu_dereference(dp->table),
+					   &key, key_len);
+		if (unlikely(!flow)) {
+			struct dp_upcall_info upcall;
+
+			upcall.cmd = OVS_PACKET_CMD_MISS;
+			upcall.key = &key;
+			upcall.userdata = NULL;
+			upcall.pid = p->upcall_pid;
+			ovs_dp_upcall(dp, skb, &upcall);
+			consume_skb(skb);
+			stats_counter = &stats->n_missed;
+			goto out;
+		}
+
+		OVS_CB(skb)->flow = flow;
+	}
+
+	stats_counter = &stats->n_hit;
+	ovs_flow_used(OVS_CB(skb)->flow, skb);
+	ovs_execute_actions(dp, skb);
+
+out:
+	/* Update datapath statistics. */
+	u64_stats_update_begin(&stats->sync);
+	(*stats_counter)++;
+	u64_stats_update_end(&stats->sync);
+}
+
+static struct genl_family dp_packet_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_PACKET_FAMILY,
+	.version = OVS_PACKET_VERSION,
+	.maxattr = OVS_PACKET_ATTR_MAX
+};
+
+int ovs_dp_upcall(struct datapath *dp, struct sk_buff *skb,
+		  const struct dp_upcall_info *upcall_info)
+{
+	struct dp_stats_percpu *stats;
+	int dp_ifindex;
+	int err;
+
+	if (upcall_info->pid == 0) {
+		err = -ENOTCONN;
+		goto err;
+	}
+
+	dp_ifindex = get_dpifindex(dp);
+	if (!dp_ifindex) {
+		err = -ENODEV;
+		goto err;
+	}
+
+	forward_ip_summed(skb, true);
+
+	if (!skb_is_gso(skb))
+		err = queue_userspace_packet(dp_ifindex, skb, upcall_info);
+	else
+		err = queue_gso_packets(dp_ifindex, skb, upcall_info);
+	if (err)
+		goto err;
+
+	return 0;
+
+err:
+	stats = per_cpu_ptr(dp->stats_percpu, smp_processor_id());
+
+	u64_stats_update_begin(&stats->sync);
+	stats->n_lost++;
+	u64_stats_update_end(&stats->sync);
+
+	return err;
+}
+
+static int queue_gso_packets(int dp_ifindex, struct sk_buff *skb,
+			     const struct dp_upcall_info *upcall_info)
+{
+	struct dp_upcall_info later_info;
+	struct sw_flow_key later_key;
+	struct sk_buff *segs, *nskb;
+	int err;
+
+	segs = skb_gso_segment(skb, NETIF_F_SG | NETIF_F_HW_CSUM);
+	if (IS_ERR(skb))
+		return PTR_ERR(skb);
+
+	/* Queue all of the segments. */
+	skb = segs;
+	do {
+		err = queue_userspace_packet(dp_ifindex, skb, upcall_info);
+		if (err)
+			break;
+
+		if (skb == segs && skb_shinfo(skb)->gso_type & SKB_GSO_UDP) {
+			/* The initial flow key extracted by flow_extract() in
+			 * this case is for a first fragment, so we need to
+			 * properly mark later fragments.
+			 */
+			later_key = *upcall_info->key;
+			later_key.ip.frag = OVS_FRAG_TYPE_LATER;
+
+			later_info = *upcall_info;
+			later_info.key = &later_key;
+			upcall_info = &later_info;
+		}
+	} while ((skb = skb->next));
+
+	/* Free all of the segments. */
+	skb = segs;
+	do {
+		nskb = skb->next;
+		if (err)
+			kfree_skb(skb);
+		else
+			consume_skb(skb);
+	} while ((skb = nskb));
+	return err;
+}
+
+static int queue_userspace_packet(int dp_ifindex, struct sk_buff *skb,
+				  const struct dp_upcall_info *upcall_info)
+{
+	struct ovs_header *upcall;
+	struct sk_buff *nskb = NULL;
+	struct sk_buff *user_skb; /* to be queued to userspace */
+	struct nlattr *nla;
+	unsigned int len;
+	int err;
+
+	if (vlan_tx_tag_present(skb)) {
+		nskb = skb_clone(skb, GFP_ATOMIC);
+		if (!nskb)
+			return -ENOMEM;
+		
+		err = vlan_deaccel_tag(nskb);
+		if (err)
+			return err;
+
+		skb = nskb;
+	}
+
+	if (nla_attr_size(skb->len) > USHRT_MAX) {
+		err = -EFBIG;
+		goto out;
+	}
+
+	len = sizeof(struct ovs_header);
+	len += nla_total_size(skb->len);
+	len += nla_total_size(FLOW_BUFSIZE);
+	if (upcall_info->cmd == OVS_PACKET_CMD_ACTION)
+		len += nla_total_size(8);
+
+	user_skb = genlmsg_new(len, GFP_ATOMIC);
+	if (!user_skb) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	upcall = genlmsg_put(user_skb, 0, 0, &dp_packet_genl_family,
+			     0, upcall_info->cmd);
+	upcall->dp_ifindex = dp_ifindex;
+
+	nla = nla_nest_start(user_skb, OVS_PACKET_ATTR_KEY);
+	ovs_flow_to_nlattrs(upcall_info->key, user_skb);
+	nla_nest_end(user_skb, nla);
+
+	if (upcall_info->userdata)
+		nla_put_u64(user_skb, OVS_PACKET_ATTR_USERDATA,
+			    nla_get_u64(upcall_info->userdata));
+
+	nla = __nla_reserve(user_skb, OVS_PACKET_ATTR_PACKET, skb->len);
+
+	skb_copy_and_csum_dev(skb, nla_data(nla));
+
+	err = genlmsg_unicast(&init_net, user_skb, upcall_info->pid);
+
+out:
+	kfree_skb(nskb);
+	return err;
+}
+
+/* Called with genl_mutex. */
+static int flush_flows(int dp_ifindex)
+{
+	struct flow_table *old_table;
+	struct flow_table *new_table;
+	struct datapath *dp;
+
+	dp = get_dp(dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	old_table = genl_dereference(dp->table);
+	new_table = ovs_flow_tbl_alloc(TBL_MIN_BUCKETS);
+	if (!new_table)
+		return -ENOMEM;
+
+	rcu_assign_pointer(dp->table, new_table);
+
+	ovs_flow_tbl_deferred_destroy(old_table);
+	return 0;
+}
+
+static int validate_actions(const struct nlattr *attr,
+				const struct sw_flow_key *key, int depth);
+
+static int validate_sample(const struct nlattr *attr,
+				const struct sw_flow_key *key, int depth)
+{
+	const struct nlattr *attrs[OVS_SAMPLE_ATTR_MAX + 1];
+	const struct nlattr *probability, *actions;
+	const struct nlattr *a;
+	int rem;
+
+	memset(attrs, 0, sizeof(attrs));
+	nla_for_each_nested(a, attr, rem) {
+		int type = nla_type(a);
+		if (!type || type > OVS_SAMPLE_ATTR_MAX || attrs[type])
+			return -EINVAL;
+		attrs[type] = a;
+	}
+	if (rem)
+		return -EINVAL;
+
+	probability = attrs[OVS_SAMPLE_ATTR_PROBABILITY];
+	if (!probability || nla_len(probability) != sizeof(u32))
+		return -EINVAL;
+
+	actions = attrs[OVS_SAMPLE_ATTR_ACTIONS];
+	if (!actions || (nla_len(actions) && nla_len(actions) < NLA_HDRLEN))
+		return -EINVAL;
+	return validate_actions(actions, key, depth + 1);
+}
+
+static int validate_set(const struct nlattr *a,
+			const struct sw_flow_key *flow_key)
+{
+	const struct nlattr *ovs_key = nla_data(a);
+	int key_type = nla_type(ovs_key);
+
+	/* There can be only one key in a action */
+	if (nla_total_size(nla_len(ovs_key)) != nla_len(a))
+		return -EINVAL;
+
+	if (key_type > OVS_KEY_ATTR_MAX ||
+	    nla_len(ovs_key) != ovs_key_lens[key_type])
+		return -EINVAL;
+
+	switch (key_type) {
+	const struct ovs_key_ipv4 *ipv4_key;
+
+	case OVS_KEY_ATTR_PRIORITY:
+	case OVS_KEY_ATTR_TUN_ID:
+	case OVS_KEY_ATTR_ETHERNET:
+		break;
+
+	case OVS_KEY_ATTR_IPV4:
+		if (flow_key->eth.type != htons(ETH_P_IP))
+			return -EINVAL;
+
+		if (!flow_key->ipv4.addr.src || !flow_key->ipv4.addr.dst)
+			return -EINVAL;
+
+		ipv4_key = nla_data(ovs_key);
+		if (ipv4_key->ipv4_proto != flow_key->ip.proto)
+			return -EINVAL;
+
+		if (ipv4_key->ipv4_frag != flow_key->ip.frag)
+			return -EINVAL;
+
+		break;
+
+	case OVS_KEY_ATTR_TCP:
+		if (flow_key->ip.proto != IPPROTO_TCP)
+			return -EINVAL;
+
+		if (!flow_key->ipv4.tp.src || !flow_key->ipv4.tp.dst)
+			return -EINVAL;
+
+		break;
+
+	case OVS_KEY_ATTR_UDP:
+		if (flow_key->ip.proto != IPPROTO_UDP)
+			return -EINVAL;
+
+		if (!flow_key->ipv4.tp.src || !flow_key->ipv4.tp.dst)
+			return -EINVAL;
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int validate_userspace(const struct nlattr *attr)
+{
+	static const struct nla_policy userspace_policy[OVS_USERSPACE_ATTR_MAX + 1] =	{
+		[OVS_USERSPACE_ATTR_PID] = {.type = NLA_U32 },
+		[OVS_USERSPACE_ATTR_USERDATA] = {.type = NLA_U64 },
+	};
+	struct nlattr *a[OVS_USERSPACE_ATTR_MAX + 1];
+	int error;
+
+	error = nla_parse_nested(a, OVS_USERSPACE_ATTR_MAX,
+				 attr, userspace_policy);
+	if (error)
+		return error;
+
+	if (!a[OVS_USERSPACE_ATTR_PID] ||
+	    !nla_get_u32(a[OVS_USERSPACE_ATTR_PID]))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int validate_actions(const struct nlattr *attr,
+				const struct sw_flow_key *key,  int depth)
+{
+	const struct nlattr *a;
+	int rem, err;
+
+	if (depth >= SAMPLE_ACTION_DEPTH)
+		return -EOVERFLOW;
+
+	nla_for_each_nested(a, attr, rem) {
+		/* Expected argument lengths, (u32)-1 for variable length. */
+		static const u32 action_lens[OVS_ACTION_ATTR_MAX + 1] = {
+			[OVS_ACTION_ATTR_OUTPUT] = sizeof(u32),
+			[OVS_ACTION_ATTR_USERSPACE] = (u32)-1,
+			[OVS_ACTION_ATTR_PUSH_VLAN] = sizeof(struct ovs_action_push_vlan),
+			[OVS_ACTION_ATTR_POP_VLAN] = 0,
+			[OVS_ACTION_ATTR_SET] = (u32)-1,
+			[OVS_ACTION_ATTR_SAMPLE] = (u32)-1
+		};
+		const struct ovs_action_push_vlan *vlan;
+		int type = nla_type(a);
+
+		if (type > OVS_ACTION_ATTR_MAX ||
+		    (action_lens[type] != nla_len(a) &&
+		     action_lens[type] != (u32)-1))
+			return -EINVAL;
+
+		switch (type) {
+		case OVS_ACTION_ATTR_UNSPEC:
+			return -EINVAL;
+
+		case OVS_ACTION_ATTR_USERSPACE:
+			err = validate_userspace(a);
+			if (err)
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_OUTPUT:
+			if (nla_get_u32(a) >= DP_MAX_PORTS)
+				return -EINVAL;
+			break;
+
+
+		case OVS_ACTION_ATTR_POP_VLAN:
+			break;
+
+		case OVS_ACTION_ATTR_PUSH_VLAN:
+			vlan = nla_data(a);
+			if (vlan->vlan_tpid != htons(ETH_P_8021Q))
+				return -EINVAL;
+			if (!(vlan->vlan_tci & htons(VLAN_TAG_PRESENT)))
+				return -EINVAL;
+			break;
+
+		case OVS_ACTION_ATTR_SET:
+			err = validate_set(a, key);
+			if (err)
+				return err;
+			break;
+
+		case OVS_ACTION_ATTR_SAMPLE:
+			err = validate_sample(a, key, depth);
+			if (err)
+				return err;
+			break;
+
+		default:
+			return -EINVAL;
+		}
+	}
+
+	if (rem > 0)
+		return -EINVAL;
+
+	return 0;
+}
+
+static void clear_stats(struct sw_flow *flow)
+{
+	flow->used = 0;
+	flow->tcp_flags = 0;
+	flow->packet_count = 0;
+	flow->byte_count = 0;
+}
+
+static int ovs_packet_cmd_execute(struct sk_buff *skb, struct genl_info *info)
+{
+	struct ovs_header *ovs_header = info->userhdr;
+	struct nlattr **a = info->attrs;
+	struct sw_flow_actions *acts;
+	struct sk_buff *packet;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct ethhdr *eth;
+	int len;
+	int err;
+	int key_len;
+
+	err = -EINVAL;
+	if (!a[OVS_PACKET_ATTR_PACKET] || !a[OVS_PACKET_ATTR_KEY] ||
+	    !a[OVS_PACKET_ATTR_ACTIONS] ||
+	    nla_len(a[OVS_PACKET_ATTR_PACKET]) < ETH_HLEN)
+		goto err;
+
+	len = nla_len(a[OVS_PACKET_ATTR_PACKET]);
+	packet = __dev_alloc_skb(NET_IP_ALIGN + len, GFP_KERNEL);
+	err = -ENOMEM;
+	if (!packet)
+		goto err;
+	skb_reserve(packet, NET_IP_ALIGN);
+
+	memcpy(__skb_put(packet, len), nla_data(a[OVS_PACKET_ATTR_PACKET]), len);
+
+	skb_reset_mac_header(packet);
+	eth = eth_hdr(packet);
+
+	/* Normally, setting the skb 'protocol' field would be handled by a
+	 * call to eth_type_trans(), but it assumes there's a sending
+	 * device, which we may not have. */
+	if (ntohs(eth->h_proto) >= 1536)
+		packet->protocol = eth->h_proto;
+	else
+		packet->protocol = htons(ETH_P_802_2);
+
+	/* Build an sw_flow for sending this packet. */
+	flow = ovs_flow_alloc();
+	err = PTR_ERR(flow);
+	if (IS_ERR(flow))
+		goto err_kfree_skb;
+
+	err = ovs_flow_extract(packet, -1, &flow->key, &key_len);
+	if (err)
+		goto err_flow_put;
+
+	err = ovs_flow_metadata_from_nlattrs(&flow->key.phy.priority,
+					     &flow->key.phy.in_port,
+					     &flow->key.phy.tun_id,
+					     a[OVS_PACKET_ATTR_KEY]);
+	if (err)
+		goto err_flow_put;
+
+	err = validate_actions(a[OVS_PACKET_ATTR_ACTIONS], &flow->key, 0);
+	if (err)
+		goto err_flow_put;
+
+	flow->hash = ovs_flow_hash(&flow->key, key_len);
+
+	acts = ovs_flow_actions_alloc(a[OVS_PACKET_ATTR_ACTIONS]);
+	err = PTR_ERR(acts);
+	if (IS_ERR(acts))
+		goto err_flow_put;
+	rcu_assign_pointer(flow->sf_acts, acts);
+
+	OVS_CB(packet)->flow = flow;
+	packet->priority = flow->key.phy.priority;
+
+	rcu_read_lock();
+	dp = get_dp(ovs_header->dp_ifindex);
+	err = -ENODEV;
+	if (!dp)
+		goto err_unlock;
+
+	local_bh_disable();
+	err = ovs_execute_actions(dp, packet);
+	local_bh_enable();
+	rcu_read_unlock();
+
+	ovs_flow_put(flow);
+	return err;
+
+err_unlock:
+	rcu_read_unlock();
+err_flow_put:
+	ovs_flow_put(flow);
+err_kfree_skb:
+	kfree_skb(packet);
+err:
+	return err;
+}
+
+static const struct nla_policy packet_policy[OVS_PACKET_ATTR_MAX + 1] = {
+	[OVS_PACKET_ATTR_PACKET] = { .type = NLA_UNSPEC },
+	[OVS_PACKET_ATTR_KEY] = { .type = NLA_NESTED },
+	[OVS_PACKET_ATTR_ACTIONS] = { .type = NLA_NESTED },
+};
+
+static struct genl_ops dp_packet_genl_ops[] = {
+	{ .cmd = OVS_PACKET_CMD_EXECUTE,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = packet_policy,
+	  .doit = ovs_packet_cmd_execute
+	}
+};
+
+static void get_dp_stats(struct datapath *dp, struct ovs_dp_stats *stats)
+{
+	int i;
+	struct flow_table *table = genl_dereference(dp->table);
+
+	stats->n_flows = ovs_flow_tbl_count(table);
+
+	stats->n_hit = stats->n_missed = stats->n_lost = 0;
+	for_each_possible_cpu(i) {
+		const struct dp_stats_percpu *percpu_stats;
+		struct dp_stats_percpu local_stats;
+		unsigned int start;
+
+		percpu_stats = per_cpu_ptr(dp->stats_percpu, i);
+
+		do {
+			start = u64_stats_fetch_begin_bh(&percpu_stats->sync);
+			local_stats = *percpu_stats;
+		} while (u64_stats_fetch_retry_bh(&percpu_stats->sync, start));
+
+		stats->n_hit += local_stats.n_hit;
+		stats->n_missed += local_stats.n_missed;
+		stats->n_lost += local_stats.n_lost;
+	}
+}
+
+static const struct nla_policy flow_policy[OVS_FLOW_ATTR_MAX + 1] = {
+	[OVS_FLOW_ATTR_KEY] = { .type = NLA_NESTED },
+	[OVS_FLOW_ATTR_ACTIONS] = { .type = NLA_NESTED },
+	[OVS_FLOW_ATTR_CLEAR] = { .type = NLA_FLAG },
+};
+
+static struct genl_family dp_flow_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_FLOW_FAMILY,
+	.version = OVS_FLOW_VERSION,
+	.maxattr = OVS_FLOW_ATTR_MAX
+};
+
+static struct genl_multicast_group ovs_dp_flow_multicast_group = {
+	.name = OVS_FLOW_MCGROUP
+};
+
+/* Called with genl_lock. */
+static int ovs_flow_cmd_fill_info(struct sw_flow *flow, struct datapath *dp,
+				  struct sk_buff *skb, u32 pid,
+				  u32 seq, u32 flags, u8 cmd)
+{
+	const int skb_orig_len = skb->len;
+	const struct sw_flow_actions *sf_acts;
+	struct ovs_flow_stats stats;
+	struct ovs_header *ovs_header;
+	struct nlattr *nla;
+	unsigned long used;
+	u8 tcp_flags;
+	int err;
+
+	sf_acts = rcu_dereference_protected(flow->sf_acts,
+					    lockdep_genl_is_held());
+
+	ovs_header = genlmsg_put(skb, pid, seq, &dp_flow_genl_family, flags, cmd);
+	if (!ovs_header)
+		return -EMSGSIZE;
+
+	ovs_header->dp_ifindex = get_dpifindex(dp);
+
+	nla = nla_nest_start(skb, OVS_FLOW_ATTR_KEY);
+	if (!nla)
+		goto nla_put_failure;
+	err = ovs_flow_to_nlattrs(&flow->key, skb);
+	if (err)
+		goto error;
+	nla_nest_end(skb, nla);
+
+	spin_lock_bh(&flow->lock);
+	used = flow->used;
+	stats.n_packets = flow->packet_count;
+	stats.n_bytes = flow->byte_count;
+	tcp_flags = flow->tcp_flags;
+	spin_unlock_bh(&flow->lock);
+
+	if (used)
+		NLA_PUT_U64(skb, OVS_FLOW_ATTR_USED, ovs_flow_used_time(used));
+
+	if (stats.n_packets)
+		NLA_PUT(skb, OVS_FLOW_ATTR_STATS,
+			sizeof(struct ovs_flow_stats), &stats);
+
+	if (tcp_flags)
+		NLA_PUT_U8(skb, OVS_FLOW_ATTR_TCP_FLAGS, tcp_flags);
+
+	/* If OVS_FLOW_ATTR_ACTIONS doesn't fit, skip dumping the actions if
+	 * this is the first flow to be dumped into 'skb'.  This is unusual for
+	 * Netlink but individual action lists can be longer than
+	 * NLMSG_GOODSIZE and thus entirely undumpable if we didn't do this.
+	 * The userspace caller can always fetch the actions separately if it
+	 * really wants them.  (Most userspace callers in fact don't care.)
+	 *
+	 * This can only fail for dump operations because the skb is always
+	 * properly sized for single flows.
+	 */
+	err = nla_put(skb, OVS_FLOW_ATTR_ACTIONS, sf_acts->actions_len,
+		      sf_acts->actions);
+	if (err < 0 && skb_orig_len)
+		goto error;
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	err = -EMSGSIZE;
+error:
+	genlmsg_cancel(skb, ovs_header);
+	return err;
+}
+
+static struct sk_buff *ovs_flow_cmd_alloc_info(struct sw_flow *flow)
+{
+	const struct sw_flow_actions *sf_acts;
+	int len;
+
+	sf_acts = rcu_dereference_protected(flow->sf_acts,
+					    lockdep_genl_is_held());
+
+	/* OVS_FLOW_ATTR_KEY */
+	len = nla_total_size(FLOW_BUFSIZE);
+	/* OVS_FLOW_ATTR_ACTIONS */
+	len += nla_total_size(sf_acts->actions_len);
+	/* OVS_FLOW_ATTR_STATS */
+	len += nla_total_size(sizeof(struct ovs_flow_stats));
+	/* OVS_FLOW_ATTR_TCP_FLAGS */
+	len += nla_total_size(1);
+	/* OVS_FLOW_ATTR_USED */
+	len += nla_total_size(8);
+
+	len += NLMSG_ALIGN(sizeof(struct ovs_header));
+
+	return genlmsg_new(len, GFP_KERNEL);
+}
+
+static struct sk_buff *ovs_flow_cmd_build_info(struct sw_flow *flow,
+					       struct datapath *dp,
+					       u32 pid, u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = ovs_flow_cmd_alloc_info(flow);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_flow_cmd_fill_info(flow, dp, skb, pid, seq, 0, cmd);
+	BUG_ON(retval < 0);
+	return skb;
+}
+
+static int ovs_flow_cmd_new_or_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sw_flow *flow;
+	struct sk_buff *reply;
+	struct datapath *dp;
+	struct flow_table *table;
+	int error;
+	int key_len;
+
+	/* Extract key. */
+	error = -EINVAL;
+	if (!a[OVS_FLOW_ATTR_KEY])
+		goto error;
+	error = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (error)
+		goto error;
+
+	/* Validate actions. */
+	if (a[OVS_FLOW_ATTR_ACTIONS]) {
+		error = validate_actions(a[OVS_FLOW_ATTR_ACTIONS], &key,  0);
+		if (error)
+			goto error;
+	} else if (info->genlhdr->cmd == OVS_FLOW_CMD_NEW) {
+		error = -EINVAL;
+		goto error;
+	}
+
+	dp = get_dp(ovs_header->dp_ifindex);
+	error = -ENODEV;
+	if (!dp)
+		goto error;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow) {
+		struct sw_flow_actions *acts;
+
+		/* Bail out if we're not allowed to create a new flow. */
+		error = -ENOENT;
+		if (info->genlhdr->cmd == OVS_FLOW_CMD_SET)
+			goto error;
+
+		/* Expand table, if necessary, to make room. */
+		if (ovs_flow_tbl_need_to_expand(table)) {
+			struct flow_table *new_table;
+
+			new_table = ovs_flow_tbl_expand(table);
+			if (!IS_ERR(new_table)) {
+				rcu_assign_pointer(dp->table, new_table);
+				ovs_flow_tbl_deferred_destroy(table);
+				table = genl_dereference(dp->table);
+			}
+		}
+
+		/* Allocate flow. */
+		flow = ovs_flow_alloc();
+		if (IS_ERR(flow)) {
+			error = PTR_ERR(flow);
+			goto error;
+		}
+		flow->key = key;
+		clear_stats(flow);
+
+		/* Obtain actions. */
+		acts = ovs_flow_actions_alloc(a[OVS_FLOW_ATTR_ACTIONS]);
+		error = PTR_ERR(acts);
+		if (IS_ERR(acts))
+			goto error_free_flow;
+		rcu_assign_pointer(flow->sf_acts, acts);
+
+		/* Put flow in bucket. */
+		flow->hash = ovs_flow_hash(&key, key_len);
+		ovs_flow_tbl_insert(table, flow);
+
+		reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+						info->snd_seq,
+						OVS_FLOW_CMD_NEW);
+	} else {
+		/* We found a matching flow. */
+		struct sw_flow_actions *old_acts;
+		struct nlattr *acts_attrs;
+
+		/* Bail out if we're not allowed to modify an existing flow.
+		 * We accept NLM_F_CREATE in place of the intended NLM_F_EXCL
+		 * because Generic Netlink treats the latter as a dump
+		 * request.  We also accept NLM_F_EXCL in case that bug ever
+		 * gets fixed.
+		 */
+		error = -EEXIST;
+		if (info->genlhdr->cmd == OVS_FLOW_CMD_NEW &&
+		    info->nlhdr->nlmsg_flags & (NLM_F_CREATE | NLM_F_EXCL))
+			goto error;
+
+		/* Update actions. */
+		old_acts = rcu_dereference_protected(flow->sf_acts,
+						     lockdep_genl_is_held());
+		acts_attrs = a[OVS_FLOW_ATTR_ACTIONS];
+		if (acts_attrs &&
+		   (old_acts->actions_len != nla_len(acts_attrs) ||
+		   memcmp(old_acts->actions, nla_data(acts_attrs),
+			  old_acts->actions_len))) {
+			struct sw_flow_actions *new_acts;
+
+			new_acts = ovs_flow_actions_alloc(acts_attrs);
+			error = PTR_ERR(new_acts);
+			if (IS_ERR(new_acts))
+				goto error;
+
+			rcu_assign_pointer(flow->sf_acts, new_acts);
+			ovs_flow_deferred_free_acts(old_acts);
+		}
+
+		reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+					       info->snd_seq, OVS_FLOW_CMD_NEW);
+
+		/* Clear stats. */
+		if (a[OVS_FLOW_ATTR_CLEAR]) {
+			spin_lock_bh(&flow->lock);
+			clear_stats(flow);
+			spin_unlock_bh(&flow->lock);
+		}
+	}
+
+	if (!IS_ERR(reply))
+		genl_notify(reply, genl_info_net(info), info->snd_pid,
+			   ovs_dp_flow_multicast_group.id, info->nlhdr,
+			   GFP_KERNEL);
+	else
+		netlink_set_err(INIT_NET_GENL_SOCK, 0,
+				ovs_dp_flow_multicast_group.id,
+				PTR_ERR(reply));
+	return 0;
+
+error_free_flow:
+	ovs_flow_put(flow);
+error:
+	return error;
+}
+
+static int ovs_flow_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sk_buff *reply;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct flow_table *table;
+	int err;
+	int key_len;
+
+	if (!a[OVS_FLOW_ATTR_KEY])
+		return -EINVAL;
+	err = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (err)
+		return err;
+
+	dp = get_dp(ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow)
+		return -ENOENT;
+
+	reply = ovs_flow_cmd_build_info(flow, dp, info->snd_pid,
+					info->snd_seq, OVS_FLOW_CMD_NEW);
+	if (IS_ERR(reply))
+		return PTR_ERR(reply);
+
+	return genlmsg_reply(reply, info);
+}
+
+static int ovs_flow_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sw_flow_key key;
+	struct sk_buff *reply;
+	struct sw_flow *flow;
+	struct datapath *dp;
+	struct flow_table *table;
+	int err;
+	int key_len;
+
+	if (!a[OVS_FLOW_ATTR_KEY])
+		return flush_flows(ovs_header->dp_ifindex);
+	err = ovs_flow_from_nlattrs(&key, &key_len, a[OVS_FLOW_ATTR_KEY]);
+	if (err)
+		return err;
+
+	dp = get_dp(ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	table = genl_dereference(dp->table);
+	flow = ovs_flow_tbl_lookup(table, &key, key_len);
+	if (!flow)
+		return -ENOENT;
+
+	reply = ovs_flow_cmd_alloc_info(flow);
+	if (!reply)
+		return -ENOMEM;
+
+	ovs_flow_tbl_remove(table, flow);
+
+	err = ovs_flow_cmd_fill_info(flow, dp, reply, info->snd_pid,
+				     info->snd_seq, 0, OVS_FLOW_CMD_DEL);
+	BUG_ON(err < 0);
+
+	ovs_flow_deferred_free(flow);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_flow_multicast_group.id, info->nlhdr, GFP_KERNEL);
+	return 0;
+}
+
+static int ovs_flow_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct ovs_header *ovs_header = genlmsg_data(nlmsg_data(cb->nlh));
+	struct datapath *dp;
+	struct flow_table *table;
+
+	dp = get_dp(ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	table = genl_dereference(dp->table);
+
+	for (;;) {
+		struct sw_flow *flow;
+		u32 bucket, obj;
+
+		bucket = cb->args[0];
+		obj = cb->args[1];
+		flow = ovs_flow_tbl_next(table, &bucket, &obj);
+		if (!flow)
+			break;
+
+		if (ovs_flow_cmd_fill_info(flow, dp, skb,
+					   NETLINK_CB(cb->skb).pid,
+					   cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					   OVS_FLOW_CMD_NEW) < 0)
+			break;
+
+		cb->args[0] = bucket;
+		cb->args[1] = obj;
+	}
+	return skb->len;
+}
+
+static struct genl_ops dp_flow_genl_ops[] = {
+	{ .cmd = OVS_FLOW_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_new_or_set
+	},
+	{ .cmd = OVS_FLOW_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_del
+	},
+	{ .cmd = OVS_FLOW_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_get,
+	  .dumpit = ovs_flow_cmd_dump
+	},
+	{ .cmd = OVS_FLOW_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = flow_policy,
+	  .doit = ovs_flow_cmd_new_or_set,
+	},
+};
+
+static const struct nla_policy datapath_policy[OVS_DP_ATTR_MAX + 1] = {
+#ifdef HAVE_NLA_NUL_STRING
+	[OVS_DP_ATTR_NAME] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+#endif
+	[OVS_DP_ATTR_UPCALL_PID] = { .type = NLA_U32 },
+};
+
+static struct genl_family dp_datapath_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_DATAPATH_FAMILY,
+	.version = OVS_DATAPATH_VERSION,
+	.maxattr = OVS_DP_ATTR_MAX
+};
+
+static struct genl_multicast_group ovs_dp_datapath_multicast_group = {
+	.name = OVS_DATAPATH_MCGROUP
+};
+
+static int ovs_dp_cmd_fill_info(struct datapath *dp, struct sk_buff *skb,
+				u32 pid, u32 seq, u32 flags, u8 cmd)
+{
+	struct ovs_header *ovs_header;
+	struct ovs_dp_stats dp_stats;
+	int err;
+
+	ovs_header = genlmsg_put(skb, pid, seq, &dp_datapath_genl_family,
+				   flags, cmd);
+	if (!ovs_header)
+		goto error;
+
+	ovs_header->dp_ifindex = get_dpifindex(dp);
+
+	rcu_read_lock();
+	err = nla_put_string(skb, OVS_DP_ATTR_NAME, ovs_dp_name(dp));
+	rcu_read_unlock();
+	if (err)
+		goto nla_put_failure;
+
+	get_dp_stats(dp, &dp_stats);
+	NLA_PUT(skb, OVS_DP_ATTR_STATS, sizeof(struct ovs_dp_stats), &dp_stats);
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	genlmsg_cancel(skb, ovs_header);
+error:
+	return -EMSGSIZE;
+}
+
+static struct sk_buff *ovs_dp_cmd_build_info(struct datapath *dp, u32 pid,
+					     u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_KERNEL);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_dp_cmd_fill_info(dp, skb, pid, seq, 0, cmd);
+	if (retval < 0) {
+		kfree_skb(skb);
+		return ERR_PTR(retval);
+	}
+	return skb;
+}
+
+static int ovs_dp_cmd_validate(struct nlattr *a[OVS_DP_ATTR_MAX + 1])
+{
+	return CHECK_NUL_STRING(a[OVS_DP_ATTR_NAME], IFNAMSIZ - 1);
+}
+
+/* Called with genl_mutex and optionally with RTNL lock also. */
+static struct datapath *lookup_datapath(struct ovs_header *ovs_header,
+					struct nlattr *a[OVS_DP_ATTR_MAX + 1])
+{
+	struct datapath *dp;
+
+	if (!a[OVS_DP_ATTR_NAME])
+		dp = get_dp(ovs_header->dp_ifindex);
+	else {
+		struct vport *vport;
+
+		rcu_read_lock();
+		vport = ovs_vport_locate(nla_data(a[OVS_DP_ATTR_NAME]));
+		dp = vport && vport->port_no == OVSP_LOCAL ? vport->dp : NULL;
+		rcu_read_unlock();
+	}
+	return dp ? dp : ERR_PTR(-ENODEV);
+}
+
+static int ovs_dp_cmd_new(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct vport_parms parms;
+	struct sk_buff *reply;
+	struct datapath *dp;
+	struct vport *vport;
+	int err;
+
+	err = -EINVAL;
+	if (!a[OVS_DP_ATTR_NAME] || !a[OVS_DP_ATTR_UPCALL_PID])
+		goto err;
+
+	err = ovs_dp_cmd_validate(a);
+	if (err)
+		goto err;
+
+	rtnl_lock();
+	err = -ENODEV;
+	if (!try_module_get(THIS_MODULE))
+		goto err_unlock_rtnl;
+
+	err = -ENOMEM;
+	dp = kzalloc(sizeof(*dp), GFP_KERNEL);
+	if (dp == NULL)
+		goto err_put_module;
+	INIT_LIST_HEAD(&dp->port_list);
+
+	/* Initialize kobject for bridge.  This will be added as
+	 * /sys/class/net/<devname>/brif later, if sysfs is enabled. */
+	dp->ifobj.kset = NULL;
+	kobject_init(&dp->ifobj, &dp_ktype);
+
+	/* Allocate table. */
+	err = -ENOMEM;
+	rcu_assign_pointer(dp->table, ovs_flow_tbl_alloc(TBL_MIN_BUCKETS));
+	if (!dp->table)
+		goto err_free_dp;
+
+	dp->stats_percpu = alloc_percpu(struct dp_stats_percpu);
+	if (!dp->stats_percpu) {
+		err = -ENOMEM;
+		goto err_destroy_table;
+	}
+
+	/* Set up our datapath device. */
+	parms.name = nla_data(a[OVS_DP_ATTR_NAME]);
+	parms.type = OVS_VPORT_TYPE_INTERNAL;
+	parms.options = NULL;
+	parms.dp = dp;
+	parms.port_no = OVSP_LOCAL;
+	parms.upcall_pid = nla_get_u32(a[OVS_DP_ATTR_UPCALL_PID]);
+
+	vport = new_vport(&parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		if (err == -EBUSY)
+			err = -EEXIST;
+
+		goto err_destroy_percpu;
+	}
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto err_destroy_local_port;
+
+	list_add_tail(&dp->list_node, &dps);
+	ovs_dp_sysfs_add_dp(dp);
+
+	rtnl_unlock();
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+	return 0;
+
+err_destroy_local_port:
+	ovs_dp_detach_port(rtnl_dereference(dp->ports[OVSP_LOCAL]));
+err_destroy_percpu:
+	free_percpu(dp->stats_percpu);
+err_destroy_table:
+	ovs_flow_tbl_destroy(genl_dereference(dp->table));
+err_free_dp:
+	kfree(dp);
+err_put_module:
+	module_put(THIS_MODULE);
+err_unlock_rtnl:
+	rtnl_unlock();
+err:
+	return err;
+}
+
+static int ovs_dp_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct vport *vport, *next_vport;
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	dp = lookup_datapath(info->userhdr, info->attrs);
+	err = PTR_ERR(dp);
+	if (IS_ERR(dp))
+		goto exit_unlock;
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_DEL);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit_unlock;
+
+	list_for_each_entry_safe(vport, next_vport, &dp->port_list, node)
+		if (vport->port_no != OVSP_LOCAL)
+			ovs_dp_detach_port(vport);
+
+	ovs_dp_sysfs_del_dp(dp);
+	list_del(&dp->list_node);
+	ovs_dp_detach_port(rtnl_dereference(dp->ports[OVSP_LOCAL]));
+
+	/* rtnl_unlock() will wait until all the references to devices that
+	 * are pending unregistration have been dropped.  We do it here to
+	 * ensure that any internal devices (which contain DP pointers) are
+	 * fully destroyed before freeing the datapath.
+	 */
+	rtnl_unlock();
+
+	call_rcu(&dp->rcu, destroy_dp_rcu);
+	module_put(THIS_MODULE);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+
+	return 0;
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_dp_cmd_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		return err;
+
+	dp = lookup_datapath(info->userhdr, info->attrs);
+	if (IS_ERR(dp))
+		return PTR_ERR(dp);
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	if (IS_ERR(reply)) {
+		err = PTR_ERR(reply);
+		netlink_set_err(INIT_NET_GENL_SOCK, 0,
+				ovs_dp_datapath_multicast_group.id, err);
+		return 0;
+	}
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_datapath_multicast_group.id, info->nlhdr,
+		    GFP_KERNEL);
+
+	return 0;
+}
+
+static int ovs_dp_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct sk_buff *reply;
+	struct datapath *dp;
+	int err;
+
+	err = ovs_dp_cmd_validate(info->attrs);
+	if (err)
+		return err;
+
+	dp = lookup_datapath(info->userhdr, info->attrs);
+	if (IS_ERR(dp))
+		return PTR_ERR(dp);
+
+	reply = ovs_dp_cmd_build_info(dp, info->snd_pid,
+				      info->snd_seq, OVS_DP_CMD_NEW);
+	if (IS_ERR(reply))
+		return PTR_ERR(reply);
+
+	return genlmsg_reply(reply, info);
+}
+
+static int ovs_dp_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct datapath *dp;
+	int skip = cb->args[0];
+	int i = 0;
+
+	list_for_each_entry(dp, &dps, list_node) {
+		if (i >= skip &&
+		    ovs_dp_cmd_fill_info(dp, skb, NETLINK_CB(cb->skb).pid,
+					 cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					 OVS_DP_CMD_NEW) < 0)
+			break;
+		i++;
+	}
+
+	cb->args[0] = i;
+
+	return skb->len;
+}
+
+static struct genl_ops dp_datapath_genl_ops[] = {
+	{ .cmd = OVS_DP_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_new
+	},
+	{ .cmd = OVS_DP_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_del
+	},
+	{ .cmd = OVS_DP_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_get,
+	  .dumpit = ovs_dp_cmd_dump
+	},
+	{ .cmd = OVS_DP_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = datapath_policy,
+	  .doit = ovs_dp_cmd_set,
+	},
+};
+
+static const struct nla_policy vport_policy[OVS_VPORT_ATTR_MAX + 1] = {
+#ifdef HAVE_NLA_NUL_STRING
+	[OVS_VPORT_ATTR_NAME] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+	[OVS_VPORT_ATTR_STATS] = { .len = sizeof(struct ovs_vport_stats) },
+	[OVS_VPORT_ATTR_ADDRESS] = { .len = ETH_ALEN },
+#else
+	[OVS_VPORT_ATTR_STATS] = { .minlen = sizeof(struct ovs_vport_stats) },
+	[OVS_VPORT_ATTR_ADDRESS] = { .minlen = ETH_ALEN },
+#endif
+	[OVS_VPORT_ATTR_PORT_NO] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_TYPE] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_UPCALL_PID] = { .type = NLA_U32 },
+	[OVS_VPORT_ATTR_OPTIONS] = { .type = NLA_NESTED },
+};
+
+static struct genl_family dp_vport_genl_family = {
+	.id = GENL_ID_GENERATE,
+	.hdrsize = sizeof(struct ovs_header),
+	.name = OVS_VPORT_FAMILY,
+	.version = OVS_VPORT_VERSION,
+	.maxattr = OVS_VPORT_ATTR_MAX
+};
+
+struct genl_multicast_group ovs_dp_vport_multicast_group = {
+	.name = OVS_VPORT_MCGROUP
+};
+
+/* Called with RTNL lock or RCU read lock. */
+static int ovs_vport_cmd_fill_info(struct vport *vport, struct sk_buff *skb,
+				   u32 pid, u32 seq, u32 flags, u8 cmd)
+{
+	struct ovs_header *ovs_header;
+	struct ovs_vport_stats vport_stats;
+	int err;
+
+	ovs_header = genlmsg_put(skb, pid, seq, &dp_vport_genl_family,
+				 flags, cmd);
+	if (!ovs_header)
+		return -EMSGSIZE;
+
+	ovs_header->dp_ifindex = get_dpifindex(vport->dp);
+
+	NLA_PUT_U32(skb, OVS_VPORT_ATTR_PORT_NO, vport->port_no);
+	NLA_PUT_U32(skb, OVS_VPORT_ATTR_TYPE, vport->ops->type);
+	NLA_PUT_STRING(skb, OVS_VPORT_ATTR_NAME, vport->ops->get_name(vport));
+	NLA_PUT_U32(skb, OVS_VPORT_ATTR_UPCALL_PID, vport->upcall_pid);
+
+	ovs_vport_get_stats(vport, &vport_stats);
+	NLA_PUT(skb, OVS_VPORT_ATTR_STATS, sizeof(struct ovs_vport_stats),
+		&vport_stats);
+
+	NLA_PUT(skb, OVS_VPORT_ATTR_ADDRESS, ETH_ALEN,
+		vport->ops->get_addr(vport));
+
+	err = ovs_vport_get_options(vport, skb);
+	if (err == -EMSGSIZE)
+		goto error;
+
+	return genlmsg_end(skb, ovs_header);
+
+nla_put_failure:
+	err = -EMSGSIZE;
+error:
+	genlmsg_cancel(skb, ovs_header);
+	return err;
+}
+
+/* Called with RTNL lock or RCU read lock. */
+struct sk_buff *ovs_vport_cmd_build_info(struct vport *vport, u32 pid,
+					 u32 seq, u8 cmd)
+{
+	struct sk_buff *skb;
+	int retval;
+
+	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);
+	if (!skb)
+		return ERR_PTR(-ENOMEM);
+
+	retval = ovs_vport_cmd_fill_info(vport, skb, pid, seq, 0, cmd);
+	if (retval < 0) {
+		kfree_skb(skb);
+		return ERR_PTR(retval);
+	}
+	return skb;
+}
+
+static int ovs_vport_cmd_validate(struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	return CHECK_NUL_STRING(a[OVS_VPORT_ATTR_NAME], IFNAMSIZ - 1);
+}
+
+/* Called with RTNL lock or RCU read lock. */
+static struct vport *lookup_vport(struct ovs_header *ovs_header,
+				  struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	struct datapath *dp;
+	struct vport *vport;
+
+	if (a[OVS_VPORT_ATTR_NAME]) {
+		vport = ovs_vport_locate(nla_data(a[OVS_VPORT_ATTR_NAME]));
+		if (!vport)
+			return ERR_PTR(-ENODEV);
+		return vport;
+	} else if (a[OVS_VPORT_ATTR_PORT_NO]) {
+		u32 port_no = nla_get_u32(a[OVS_VPORT_ATTR_PORT_NO]);
+
+		if (port_no >= DP_MAX_PORTS)
+			return ERR_PTR(-EFBIG);
+
+		dp = get_dp(ovs_header->dp_ifindex);
+		if (!dp)
+			return ERR_PTR(-ENODEV);
+
+		vport = rcu_dereference_rtnl(dp->ports[port_no]);
+		if (!vport)
+			return ERR_PTR(-ENOENT);
+		return vport;
+	} else
+		return ERR_PTR(-EINVAL);
+}
+
+/* Called with RTNL lock. */
+static int change_vport(struct vport *vport,
+			struct nlattr *a[OVS_VPORT_ATTR_MAX + 1])
+{
+	int err = 0;
+
+	if (a[OVS_VPORT_ATTR_STATS])
+		ovs_vport_set_stats(vport, nla_data(a[OVS_VPORT_ATTR_STATS]));
+
+	if (a[OVS_VPORT_ATTR_ADDRESS])
+		err = ovs_vport_set_addr(vport, nla_data(a[OVS_VPORT_ATTR_ADDRESS]));
+
+	return err;
+}
+
+static int ovs_vport_cmd_new(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct vport_parms parms;
+	struct sk_buff *reply;
+	struct vport *vport;
+	struct datapath *dp;
+	u32 port_no;
+	int err;
+
+	err = -EINVAL;
+	if (!a[OVS_VPORT_ATTR_NAME] || !a[OVS_VPORT_ATTR_TYPE] ||
+	    !a[OVS_VPORT_ATTR_UPCALL_PID])
+		goto exit;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	dp = get_dp(ovs_header->dp_ifindex);
+	err = -ENODEV;
+	if (!dp)
+		goto exit_unlock;
+
+	if (a[OVS_VPORT_ATTR_PORT_NO]) {
+		port_no = nla_get_u32(a[OVS_VPORT_ATTR_PORT_NO]);
+
+		err = -EFBIG;
+		if (port_no >= DP_MAX_PORTS)
+			goto exit_unlock;
+
+		vport = rtnl_dereference(dp->ports[port_no]);
+		err = -EBUSY;
+		if (vport)
+			goto exit_unlock;
+	} else {
+		for (port_no = 1; ; port_no++) {
+			if (port_no >= DP_MAX_PORTS) {
+				err = -EFBIG;
+				goto exit_unlock;
+			}
+			vport = rtnl_dereference(dp->ports[port_no]);
+			if (!vport)
+				break;
+		}
+	}
+
+	parms.name = nla_data(a[OVS_VPORT_ATTR_NAME]);
+	parms.type = nla_get_u32(a[OVS_VPORT_ATTR_TYPE]);
+	parms.options = a[OVS_VPORT_ATTR_OPTIONS];
+	parms.dp = dp;
+	parms.port_no = port_no;
+	parms.upcall_pid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]);
+
+	vport = new_vport(&parms);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	ovs_dp_sysfs_add_if(vport);
+
+	err = change_vport(vport, a);
+	if (!err) {
+		reply = ovs_vport_cmd_build_info(vport, info->snd_pid,
+						 info->snd_seq,
+						 OVS_VPORT_CMD_NEW);
+		if (IS_ERR(reply))
+			err = PTR_ERR(reply);
+	}
+	if (err) {
+		ovs_dp_detach_port(vport);
+		goto exit_unlock;
+	}
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_set(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	vport = lookup_vport(info->userhdr, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	err = 0;
+	if (a[OVS_VPORT_ATTR_TYPE] &&
+	    nla_get_u32(a[OVS_VPORT_ATTR_TYPE]) != vport->ops->type)
+		err = -EINVAL;
+
+	if (!err && a[OVS_VPORT_ATTR_OPTIONS])
+		err = ovs_vport_set_options(vport, a[OVS_VPORT_ATTR_OPTIONS]);
+	if (!err)
+		err = change_vport(vport, a);
+	if (!err && a[OVS_VPORT_ATTR_UPCALL_PID])
+		vport->upcall_pid = nla_get_u32(a[OVS_VPORT_ATTR_UPCALL_PID]);
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid, info->snd_seq,
+					 OVS_VPORT_CMD_NEW);
+	if (IS_ERR(reply)) {
+		err = PTR_ERR(reply);
+		netlink_set_err(INIT_NET_GENL_SOCK, 0,
+				ovs_dp_vport_multicast_group.id, err);
+		return 0;
+	}
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_del(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rtnl_lock();
+	vport = lookup_vport(info->userhdr, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	if (vport->port_no == OVSP_LOCAL) {
+		err = -EINVAL;
+		goto exit_unlock;
+	}
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid, info->snd_seq,
+					 OVS_VPORT_CMD_DEL);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit_unlock;
+
+	ovs_dp_detach_port(vport);
+
+	genl_notify(reply, genl_info_net(info), info->snd_pid,
+		    ovs_dp_vport_multicast_group.id, info->nlhdr, GFP_KERNEL);
+
+exit_unlock:
+	rtnl_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_get(struct sk_buff *skb, struct genl_info *info)
+{
+	struct nlattr **a = info->attrs;
+	struct ovs_header *ovs_header = info->userhdr;
+	struct sk_buff *reply;
+	struct vport *vport;
+	int err;
+
+	err = ovs_vport_cmd_validate(a);
+	if (err)
+		goto exit;
+
+	rcu_read_lock();
+	vport = lookup_vport(ovs_header, a);
+	err = PTR_ERR(vport);
+	if (IS_ERR(vport))
+		goto exit_unlock;
+
+	reply = ovs_vport_cmd_build_info(vport, info->snd_pid, info->snd_seq,
+					 OVS_VPORT_CMD_NEW);
+	err = PTR_ERR(reply);
+	if (IS_ERR(reply))
+		goto exit_unlock;
+
+	rcu_read_unlock();
+
+	return genlmsg_reply(reply, info);
+
+exit_unlock:
+	rcu_read_unlock();
+exit:
+	return err;
+}
+
+static int ovs_vport_cmd_dump(struct sk_buff *skb, struct netlink_callback *cb)
+{
+	struct ovs_header *ovs_header = genlmsg_data(nlmsg_data(cb->nlh));
+	struct datapath *dp;
+	u32 port_no;
+	int retval;
+
+	dp = get_dp(ovs_header->dp_ifindex);
+	if (!dp)
+		return -ENODEV;
+
+	rcu_read_lock();
+	for (port_no = cb->args[0]; port_no < DP_MAX_PORTS; port_no++) {
+		struct vport *vport;
+
+		vport = rcu_dereference(dp->ports[port_no]);
+		if (!vport)
+			continue;
+
+		if (ovs_vport_cmd_fill_info(vport, skb, NETLINK_CB(cb->skb).pid,
+					    cb->nlh->nlmsg_seq, NLM_F_MULTI,
+					    OVS_VPORT_CMD_NEW) < 0)
+			break;
+	}
+	rcu_read_unlock();
+
+	cb->args[0] = port_no;
+	retval = skb->len;
+
+	return retval;
+}
+
+static struct genl_ops dp_vport_genl_ops[] = {
+	{ .cmd = OVS_VPORT_CMD_NEW,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_new
+	},
+	{ .cmd = OVS_VPORT_CMD_DEL,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_del
+	},
+	{ .cmd = OVS_VPORT_CMD_GET,
+	  .flags = 0,		    /* OK for unprivileged users. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_get,
+	  .dumpit = ovs_vport_cmd_dump
+	},
+	{ .cmd = OVS_VPORT_CMD_SET,
+	  .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
+	  .policy = vport_policy,
+	  .doit = ovs_vport_cmd_set,
+	},
+};
+
+struct genl_family_and_ops {
+	struct genl_family *family;
+	struct genl_ops *ops;
+	int n_ops;
+	struct genl_multicast_group *group;
+};
+
+static const struct genl_family_and_ops dp_genl_families[] = {
+	{ &dp_datapath_genl_family,
+	  dp_datapath_genl_ops, ARRAY_SIZE(dp_datapath_genl_ops),
+	  &ovs_dp_datapath_multicast_group },
+	{ &dp_vport_genl_family,
+	  dp_vport_genl_ops, ARRAY_SIZE(dp_vport_genl_ops),
+	  &ovs_dp_vport_multicast_group },
+	{ &dp_flow_genl_family,
+	  dp_flow_genl_ops, ARRAY_SIZE(dp_flow_genl_ops),
+	  &ovs_dp_flow_multicast_group },
+	{ &dp_packet_genl_family,
+	  dp_packet_genl_ops, ARRAY_SIZE(dp_packet_genl_ops),
+	  NULL },
+};
+
+static void dp_unregister_genl(int n_families)
+{
+	int i;
+
+	for (i = 0; i < n_families; i++)
+		genl_unregister_family(dp_genl_families[i].family);
+}
+
+static int dp_register_genl(void)
+{
+	int n_registered;
+	int err;
+	int i;
+
+	n_registered = 0;
+	for (i = 0; i < ARRAY_SIZE(dp_genl_families); i++) {
+		const struct genl_family_and_ops *f = &dp_genl_families[i];
+
+		err = genl_register_family_with_ops(f->family, f->ops,
+						    f->n_ops);
+		if (err)
+			goto error;
+		n_registered++;
+
+		if (f->group) {
+			err = genl_register_mc_group(f->family, f->group);
+			if (err)
+				goto error;
+		}
+	}
+
+	return 0;
+
+error:
+	dp_unregister_genl(n_registered);
+	return err;
+}
+
+static int __init dp_init(void)
+{
+	struct sk_buff *dummy_skb;
+	int err;
+
+	BUILD_BUG_ON(sizeof(struct ovs_skb_cb) > sizeof(dummy_skb->cb));
+
+	pr_info("Open vSwitch switching datapath %s, built "__DATE__" "__TIME__"\n",
+		VERSION BUILDNR);
+
+	err = ovs_tnl_init();
+	if (err)
+		goto error;
+
+	err = ovs_flow_init();
+	if (err)
+		goto error_tnl_exit;
+
+	err = ovs_vport_init();
+	if (err)
+		goto error_flow_exit;
+
+	err = register_netdevice_notifier(&ovs_dp_device_notifier);
+	if (err)
+		goto error_vport_exit;
+
+	err = dp_register_genl();
+	if (err < 0)
+		goto error_unreg_notifier;
+
+	return 0;
+
+error_unreg_notifier:
+	unregister_netdevice_notifier(&ovs_dp_device_notifier);
+error_vport_exit:
+	ovs_vport_exit();
+error_flow_exit:
+	ovs_flow_exit();
+error_tnl_exit:
+	ovs_tnl_exit();
+error:
+	return err;
+}
+
+static void dp_cleanup(void)
+{
+	rcu_barrier();
+	dp_unregister_genl(ARRAY_SIZE(dp_genl_families));
+	unregister_netdevice_notifier(&ovs_dp_device_notifier);
+	ovs_vport_exit();
+	ovs_flow_exit();
+	ovs_tnl_exit();
+}
+
+module_init(dp_init);
+module_exit(dp_cleanup);
+
+MODULE_DESCRIPTION("Open vSwitch switching datapath");
+MODULE_LICENSE("GPL");
diff -r 89e197c6e9d5 net/openvswitch/datapath.h
--- /dev/null
+++ b/net/openvswitch/datapath.h
@@ -0,0 +1,148 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef DATAPATH_H
+#define DATAPATH_H 1
+
+#include <asm/page.h>
+#include <linux/kernel.h>
+#include <linux/mutex.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/u64_stats_sync.h>
+#include <linux/version.h>
+
+#include "checksum.h"
+#include "compat.h"
+#include "flow.h"
+#include "dp_sysfs.h"
+#include "vlan.h"
+
+struct vport;
+
+#define DP_MAX_PORTS 1024
+#define SAMPLE_ACTION_DEPTH 3
+
+/**
+ * struct dp_stats_percpu - per-cpu packet processing statistics for a given
+ * datapath.
+ * @n_hit: Number of received packets for which a matching flow was found in
+ * the flow table.
+ * @n_miss: Number of received packets that had no matching flow in the flow
+ * table.  The sum of @n_hit and @n_miss is the number of packets that have
+ * been received by the datapath.
+ * @n_lost: Number of received packets that had no matching flow in the flow
+ * table that could not be sent to userspace (normally due to an overflow in
+ * one of the datapath's queues).
+ */
+struct dp_stats_percpu {
+	u64 n_hit;
+	u64 n_missed;
+	u64 n_lost;
+	struct u64_stats_sync sync;
+};
+
+/**
+ * struct datapath - datapath for flow-based packet switching
+ * @rcu: RCU callback head for deferred destruction.
+ * @list_node: Element in global 'dps' list.
+ * @ifobj: Represents /sys/class/net/<devname>/brif.  Protected by RTNL.
+ * @n_flows: Number of flows currently in flow table.
+ * @table: Current flow table.  Protected by genl_lock and RCU.
+ * @ports: Map from port number to &struct vport.  %OVSP_LOCAL port
+ * always exists, other ports may be %NULL.  Protected by RTNL and RCU.
+ * @port_list: List of all ports in @ports in arbitrary order.  RTNL required
+ * to iterate or modify.
+ * @stats_percpu: Per-CPU datapath statistics.
+ *
+ * Context: See the comment on locking at the top of datapath.c for additional
+ * locking information.
+ */
+struct datapath {
+	struct rcu_head rcu;
+	struct list_head list_node;
+	struct kobject ifobj;
+
+	/* Flow table. */
+	struct flow_table __rcu *table;
+
+	/* Switch ports. */
+	struct vport __rcu *ports[DP_MAX_PORTS];
+	struct list_head port_list;
+
+	/* Stats. */
+	struct dp_stats_percpu __percpu *stats_percpu;
+};
+
+/**
+ * struct ovs_skb_cb - OVS data in skb CB
+ * @flow: The flow associated with this packet.  May be %NULL if no flow.
+ * @tun_id: ID of the tunnel that encapsulated this packet.  It is 0 if the
+ * @ip_summed: Consistently stores L4 checksumming status across different
+ * kernel versions.
+ * @csum_start: Stores the offset from which to start checksumming independent
+ * of the transport header on all kernel versions.
+ * packet was not received on a tunnel.
+ * @vlan_tci: Provides a substitute for the skb->vlan_tci field on kernels
+ * before 2.6.27.
+ */
+struct ovs_skb_cb {
+	struct sw_flow		*flow;
+	__be64			tun_id;
+#ifdef NEED_CSUM_NORMALIZE
+	enum csum_type		ip_summed;
+	u16			csum_start;
+#endif
+#ifdef NEED_VLAN_FIELD
+	u16			vlan_tci;
+#endif
+};
+#define OVS_CB(skb) ((struct ovs_skb_cb *)(skb)->cb)
+
+/**
+ * struct dp_upcall - metadata to include with a packet to send to userspace
+ * @cmd: One of %OVS_PACKET_CMD_*.
+ * @key: Becomes %OVS_PACKET_ATTR_KEY.  Must be nonnull.
+ * @userdata: If nonnull, its u64 value is extracted and passed to userspace as
+ * %OVS_PACKET_ATTR_USERDATA.
+ * @pid: Netlink PID to which packet should be sent.  If @pid is 0 then no
+ * packet is sent and the packet is accounted in the datapath's @n_lost
+ * counter.
+ */
+struct dp_upcall_info {
+	u8 cmd;
+	const struct sw_flow_key *key;
+	const struct nlattr *userdata;
+	u32 pid;
+};
+
+extern struct notifier_block ovs_dp_device_notifier;
+extern struct genl_multicast_group ovs_dp_vport_multicast_group;
+extern int (*ovs_dp_ioctl_hook)(struct net_device *dev, struct ifreq *rq, int cmd);
+
+void ovs_dp_process_received_packet(struct vport *, struct sk_buff *);
+void ovs_dp_detach_port(struct vport *);
+int ovs_dp_upcall(struct datapath *, struct sk_buff *,
+		  const struct dp_upcall_info *);
+
+const char *ovs_dp_name(const struct datapath *dp);
+struct sk_buff *ovs_vport_cmd_build_info(struct vport *, u32 pid, u32 seq,
+					 u8 cmd);
+
+int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb);
+#endif /* datapath.h */
diff -r 89e197c6e9d5 net/openvswitch/dp_notify.c
--- /dev/null
+++ b/net/openvswitch/dp_notify.c
@@ -0,0 +1,73 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/netdevice.h>
+#include <net/genetlink.h>
+
+#include "datapath.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+static int dp_device_event(struct notifier_block *unused, unsigned long event,
+			   void *ptr)
+{
+	struct net_device *dev = ptr;
+	struct vport *vport;
+
+	if (ovs_is_internal_dev(dev))
+		vport = ovs_internal_dev_get_vport(dev);
+	else
+		vport = ovs_netdev_get_vport(dev);
+
+	if (!vport)
+		return NOTIFY_DONE;
+
+	switch (event) {
+	case NETDEV_UNREGISTER:
+		if (!ovs_is_internal_dev(dev)) {
+			struct sk_buff *notify;
+
+			notify = ovs_vport_cmd_build_info(vport, 0, 0,
+							  OVS_VPORT_CMD_DEL);
+			ovs_dp_detach_port(vport);
+			if (IS_ERR(notify)) {
+				netlink_set_err(INIT_NET_GENL_SOCK, 0,
+						ovs_dp_vport_multicast_group.id,
+						PTR_ERR(notify));
+				break;
+			}
+
+			genlmsg_multicast(notify, 0, ovs_dp_vport_multicast_group.id,
+					  GFP_KERNEL);
+		}
+		break;
+
+	case NETDEV_CHANGENAME:
+		if (vport->port_no != OVSP_LOCAL) {
+			ovs_dp_sysfs_del_if(vport);
+			ovs_dp_sysfs_add_if(vport);
+		}
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+struct notifier_block ovs_dp_device_notifier = {
+	.notifier_call = dp_device_event
+};
diff -r 89e197c6e9d5 net/openvswitch/dp_sysfs.h
--- /dev/null
+++ b/net/openvswitch/dp_sysfs.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef DP_SYSFS_H
+#define DP_SYSFS_H 1
+
+struct datapath;
+struct vport;
+
+/* dp_sysfs_dp.c */
+int ovs_dp_sysfs_add_dp(struct datapath *dp);
+int ovs_dp_sysfs_del_dp(struct datapath *dp);
+
+/* dp_sysfs_if.c */
+int ovs_dp_sysfs_add_if(struct vport *p);
+int ovs_dp_sysfs_del_if(struct vport *p);
+
+#ifdef CONFIG_SYSFS
+extern struct sysfs_ops ovs_brport_sysfs_ops;
+#endif
+
+#endif /* dp_sysfs.h */
+
diff -r 89e197c6e9d5 net/openvswitch/dp_sysfs_dp.c
--- /dev/null
+++ b/net/openvswitch/dp_sysfs_dp.c
@@ -0,0 +1,408 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+
+/*
+ *	Sysfs attributes of bridge for Open vSwitch
+ *
+ *  This has been shamelessly copied from the kernel sources.
+ */
+
+#include <linux/capability.h>
+#include <linux/device.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/rtnetlink.h>
+#include <linux/version.h>
+
+#include "dp_sysfs.h"
+#include "datapath.h"
+#include "vport-internal_dev.h"
+
+#ifdef CONFIG_SYSFS
+
+/* Hack to attempt to build on more platforms. */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+#define INTERNAL_DEVICE_ATTR CLASS_DEVICE_ATTR
+#define DEVICE_PARAMS struct class_device *d
+#define DEVICE_ARGS d
+#define DEV_ATTR(NAME) class_device_attr_##NAME
+#else
+#define INTERNAL_DEVICE_ATTR DEVICE_ATTR
+#define DEVICE_PARAMS struct device *d, struct device_attribute *attr
+#define DEVICE_ARGS d, attr
+#define DEV_ATTR(NAME) dev_attr_##NAME
+#endif
+
+static struct datapath *sysfs_get_dp(struct net_device *netdev)
+{
+	struct vport *vport = ovs_internal_dev_get_vport(netdev);
+	return vport ? vport->dp : NULL;
+}
+/*
+ * Common code for storing bridge parameters.
+ */
+static ssize_t store_bridge_parm(DEVICE_PARAMS,
+				 const char *buf, size_t len,
+				 void (*set)(struct datapath *, unsigned long))
+{
+	char *endp;
+	unsigned long val;
+	ssize_t result = len;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf)
+		return -EINVAL;
+
+	/* xxx We use a default value of 0 for all fields.  If the caller is
+	 * xxx attempting to set the value to our default, just silently
+	 * xxx ignore the request.
+	 */
+	if (val != 0) {
+		struct datapath *dp;
+
+		rcu_read_lock();
+
+		dp = sysfs_get_dp(to_net_dev(d));
+		if (dp)
+			pr_warning("%s: xxx writing dp parms not supported yet!\n",
+			       ovs_dp_name(dp));
+		else
+			result = -ENODEV;
+
+		rcu_read_unlock();
+	}
+
+	return result;
+}
+
+
+static ssize_t show_forward_delay(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_forward_delay(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_forward_delay()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_forward_delay(DEVICE_PARAMS,
+				   const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_forward_delay);
+}
+static INTERNAL_DEVICE_ATTR(forward_delay, S_IRUGO | S_IWUSR,
+		   show_forward_delay, store_forward_delay);
+
+static ssize_t show_hello_time(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_hello_time(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_hello_time()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_hello_time(DEVICE_PARAMS,
+				const char *buf,
+				size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_hello_time);
+}
+static INTERNAL_DEVICE_ATTR(hello_time, S_IRUGO | S_IWUSR, show_hello_time,
+		   store_hello_time);
+
+static ssize_t show_max_age(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_max_age(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_max_age()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_max_age(DEVICE_PARAMS,
+			     const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_max_age);
+}
+static INTERNAL_DEVICE_ATTR(max_age, S_IRUGO | S_IWUSR, show_max_age, store_max_age);
+
+static ssize_t show_ageing_time(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_ageing_time(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_ageing_time()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_ageing_time(DEVICE_PARAMS,
+				 const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_ageing_time);
+}
+static INTERNAL_DEVICE_ATTR(ageing_time, S_IRUGO | S_IWUSR, show_ageing_time,
+		   store_ageing_time);
+
+static ssize_t show_stp_state(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+
+static ssize_t store_stp_state(DEVICE_PARAMS,
+			       const char *buf,
+			       size_t len)
+{
+	struct datapath *dp;
+	ssize_t result = len;
+
+	rcu_read_lock();
+
+	dp = sysfs_get_dp(to_net_dev(d));
+	if (dp)
+		pr_info("%s: xxx attempt to set_stp_state()\n", ovs_dp_name(dp));
+	else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+static INTERNAL_DEVICE_ATTR(stp_state, S_IRUGO | S_IWUSR, show_stp_state,
+		   store_stp_state);
+
+static ssize_t show_priority(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+
+static void set_priority(struct datapath *dp, unsigned long val)
+{
+	pr_info("%s: xxx attempt to set_priority()\n", ovs_dp_name(dp));
+}
+
+static ssize_t store_priority(DEVICE_PARAMS,
+			       const char *buf, size_t len)
+{
+	return store_bridge_parm(DEVICE_ARGS, buf, len, set_priority);
+}
+static INTERNAL_DEVICE_ATTR(priority, S_IRUGO | S_IWUSR, show_priority, store_priority);
+
+static ssize_t show_root_id(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "0000.010203040506\n");
+}
+static INTERNAL_DEVICE_ATTR(root_id, S_IRUGO, show_root_id, NULL);
+
+static ssize_t show_bridge_id(DEVICE_PARAMS, char *buf)
+{
+	struct vport *vport;
+	ssize_t result;
+
+	rcu_read_lock();
+
+	vport = ovs_internal_dev_get_vport(to_net_dev(d));
+	if (vport) {
+		const unsigned char *addr;
+
+		addr = vport->ops->get_addr(vport);
+		result = sprintf(buf, "%.2x%.2x.%.2x%.2x%.2x%.2x%.2x%.2x\n",
+				 0, 0, addr[0], addr[1], addr[2], addr[3],
+				 addr[4], addr[5]);
+	} else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+static INTERNAL_DEVICE_ATTR(bridge_id, S_IRUGO, show_bridge_id, NULL);
+
+static ssize_t show_root_port(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(root_port, S_IRUGO, show_root_port, NULL);
+
+static ssize_t show_root_path_cost(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(root_path_cost, S_IRUGO, show_root_path_cost, NULL);
+
+static ssize_t show_topology_change(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change, S_IRUGO, show_topology_change, NULL);
+
+static ssize_t show_topology_change_detected(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change_detected, S_IRUGO,
+		   show_topology_change_detected, NULL);
+
+static ssize_t show_hello_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(hello_timer, S_IRUGO, show_hello_timer, NULL);
+
+static ssize_t show_tcn_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(tcn_timer, S_IRUGO, show_tcn_timer, NULL);
+
+static ssize_t show_topology_change_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(topology_change_timer, S_IRUGO, show_topology_change_timer,
+		   NULL);
+
+static ssize_t show_gc_timer(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static INTERNAL_DEVICE_ATTR(gc_timer, S_IRUGO, show_gc_timer, NULL);
+
+static ssize_t show_group_addr(DEVICE_PARAMS, char *buf)
+{
+	return sprintf(buf, "00:01:02:03:04:05\n");
+}
+
+static ssize_t store_group_addr(DEVICE_PARAMS,
+				const char *buf, size_t len)
+{
+	struct datapath *dp;
+	ssize_t result = len;
+
+	rcu_read_lock();
+
+	dp = sysfs_get_dp(to_net_dev(d));
+	if (dp)
+		pr_info("%s: xxx attempt to store_group_addr()\n",
+		       ovs_dp_name(dp));
+	else
+		result = -ENODEV;
+
+	rcu_read_unlock();
+
+	return result;
+}
+
+static INTERNAL_DEVICE_ATTR(group_addr, S_IRUGO | S_IWUSR,
+		   show_group_addr, store_group_addr);
+
+static struct attribute *bridge_attrs[] = {
+	&DEV_ATTR(forward_delay).attr,
+	&DEV_ATTR(hello_time).attr,
+	&DEV_ATTR(max_age).attr,
+	&DEV_ATTR(ageing_time).attr,
+	&DEV_ATTR(stp_state).attr,
+	&DEV_ATTR(priority).attr,
+	&DEV_ATTR(bridge_id).attr,
+	&DEV_ATTR(root_id).attr,
+	&DEV_ATTR(root_path_cost).attr,
+	&DEV_ATTR(root_port).attr,
+	&DEV_ATTR(topology_change).attr,
+	&DEV_ATTR(topology_change_detected).attr,
+	&DEV_ATTR(hello_timer).attr,
+	&DEV_ATTR(tcn_timer).attr,
+	&DEV_ATTR(topology_change_timer).attr,
+	&DEV_ATTR(gc_timer).attr,
+	&DEV_ATTR(group_addr).attr,
+	NULL
+};
+
+static struct attribute_group bridge_group = {
+	.name = SYSFS_BRIDGE_ATTR, /* "bridge" */
+	.attrs = bridge_attrs,
+};
+
+/*
+ * Add entries in sysfs onto the existing network class device
+ * for the bridge.
+ *   Adds a attribute group "bridge" containing tuning parameters.
+ *   Sub directory to hold links to interfaces.
+ *
+ * Note: the ifobj exists only to be a subdirectory
+ *   to hold links.  The ifobj exists in the same data structure
+ *   as its parent the bridge so reference counting works.
+ */
+int ovs_dp_sysfs_add_dp(struct datapath *dp)
+{
+	struct vport *vport = rtnl_dereference(dp->ports[OVSP_LOCAL]);
+	struct kobject *kobj = vport->ops->get_kobj(vport);
+	int err;
+
+	/* Create /sys/class/net/<devname>/bridge directory. */
+	err = sysfs_create_group(kobj, &bridge_group);
+	if (err) {
+		pr_info("%s: can't create group %s/%s\n",
+			__func__, ovs_dp_name(dp), bridge_group.name);
+		goto out1;
+	}
+
+	/* Create /sys/class/net/<devname>/brif directory. */
+	err = kobject_add(&dp->ifobj, kobj, SYSFS_BRIDGE_PORT_SUBDIR);
+	if (err) {
+		pr_info("%s: can't add kobject (directory) %s/%s\n",
+			__func__, ovs_dp_name(dp), kobject_name(&dp->ifobj));
+		goto out2;
+	}
+	kobject_uevent(&dp->ifobj, KOBJ_ADD);
+	return 0;
+
+ out2:
+	sysfs_remove_group(kobj, &bridge_group);
+ out1:
+	return err;
+}
+
+int ovs_dp_sysfs_del_dp(struct datapath *dp)
+{
+	struct vport *vport = rtnl_dereference(dp->ports[OVSP_LOCAL]);
+	struct kobject *kobj = vport->ops->get_kobj(vport);
+
+	kobject_del(&dp->ifobj);
+	sysfs_remove_group(kobj, &bridge_group);
+
+	return 0;
+}
+#else /* !CONFIG_SYSFS */
+int ovs_dp_sysfs_add_dp(struct datapath *dp) { return 0; }
+int ovs_dp_sysfs_del_dp(struct datapath *dp) { return 0; }
+int dp_sysfs_add_if(struct vport *p) { return 0; }
+int dp_sysfs_del_if(struct vport *p) { return 0; }
+#endif /* !CONFIG_SYSFS */
diff -r 89e197c6e9d5 net/openvswitch/dp_sysfs_if.c
--- /dev/null
+++ b/net/openvswitch/dp_sysfs_if.c
@@ -0,0 +1,267 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/capability.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/if_bridge.h>
+#include <linux/rtnetlink.h>
+
+#include "datapath.h"
+#include "dp_sysfs.h"
+#include "vport.h"
+
+#ifdef CONFIG_SYSFS
+
+struct brport_attribute {
+	struct attribute	attr;
+	ssize_t (*show)(struct vport *, char *);
+	ssize_t (*store)(struct vport *, unsigned long);
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#define BRPORT_ATTR(_name, _mode, _show, _store)		\
+struct brport_attribute brport_attr_##_name = {		        \
+	.attr = {.name = __stringify(_name),			\
+		 .mode = _mode },				\
+	.show	= _show,					\
+	.store	= _store,					\
+};
+#else
+#define BRPORT_ATTR(_name, _mode, _show, _store)		\
+struct brport_attribute brport_attr_##_name = {			\
+	.attr = {.name = __stringify(_name),			\
+		 .mode = _mode,					\
+		 .owner = THIS_MODULE, },			\
+	.show	= _show,					\
+	.store	= _store,					\
+};
+#endif
+
+static ssize_t show_path_cost(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static ssize_t store_path_cost(struct vport *p, unsigned long v)
+{
+	return 0;
+}
+static BRPORT_ATTR(path_cost, S_IRUGO | S_IWUSR,
+		   show_path_cost, store_path_cost);
+
+static ssize_t show_priority(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static ssize_t store_priority(struct vport *p, unsigned long v)
+{
+	return 0;
+}
+static BRPORT_ATTR(priority, S_IRUGO | S_IWUSR,
+			 show_priority, store_priority);
+
+static ssize_t show_designated_root(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0000.010203040506\n");
+}
+static BRPORT_ATTR(designated_root, S_IRUGO, show_designated_root, NULL);
+
+static ssize_t show_designated_bridge(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0000.060504030201\n");
+}
+static BRPORT_ATTR(designated_bridge, S_IRUGO, show_designated_bridge, NULL);
+
+static ssize_t show_designated_port(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(designated_port, S_IRUGO, show_designated_port, NULL);
+
+static ssize_t show_designated_cost(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(designated_cost, S_IRUGO, show_designated_cost, NULL);
+
+static ssize_t show_port_id(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0x%x\n", 0);
+}
+static BRPORT_ATTR(port_id, S_IRUGO, show_port_id, NULL);
+
+static ssize_t show_port_no(struct vport *p, char *buf)
+{
+	return sprintf(buf, "0x%x\n", p->port_no);
+}
+
+static BRPORT_ATTR(port_no, S_IRUGO, show_port_no, NULL);
+
+static ssize_t show_change_ack(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(change_ack, S_IRUGO, show_change_ack, NULL);
+
+static ssize_t show_config_pending(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(config_pending, S_IRUGO, show_config_pending, NULL);
+
+static ssize_t show_port_state(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(state, S_IRUGO, show_port_state, NULL);
+
+static ssize_t show_message_age_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(message_age_timer, S_IRUGO, show_message_age_timer, NULL);
+
+static ssize_t show_forward_delay_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(forward_delay_timer, S_IRUGO, show_forward_delay_timer, NULL);
+
+static ssize_t show_hold_timer(struct vport *p, char *buf)
+{
+	return sprintf(buf, "%d\n", 0);
+}
+static BRPORT_ATTR(hold_timer, S_IRUGO, show_hold_timer, NULL);
+
+static struct brport_attribute *brport_attrs[] = {
+	&brport_attr_path_cost,
+	&brport_attr_priority,
+	&brport_attr_port_id,
+	&brport_attr_port_no,
+	&brport_attr_designated_root,
+	&brport_attr_designated_bridge,
+	&brport_attr_designated_port,
+	&brport_attr_designated_cost,
+	&brport_attr_state,
+	&brport_attr_change_ack,
+	&brport_attr_config_pending,
+	&brport_attr_message_age_timer,
+	&brport_attr_forward_delay_timer,
+	&brport_attr_hold_timer,
+	NULL
+};
+
+#define to_vport_attr(_at) container_of(_at, struct brport_attribute, attr)
+#define to_vport(obj)	container_of(obj, struct vport, kobj)
+
+static ssize_t brport_show(struct kobject *kobj,
+			   struct attribute *attr, char *buf)
+{
+	struct brport_attribute *brport_attr = to_vport_attr(attr);
+	struct vport *p = to_vport(kobj);
+
+	return brport_attr->show(p, buf);
+}
+
+static ssize_t brport_store(struct kobject *kobj,
+			    struct attribute *attr,
+			    const char *buf, size_t count)
+{
+	struct vport *p = to_vport(kobj);
+	ssize_t ret = -EINVAL;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	pr_warning("%s: xxx writing port parms not supported yet!\n",
+		   ovs_dp_name(p->dp));
+
+	return ret;
+}
+
+struct sysfs_ops ovs_brport_sysfs_ops = {
+	.show = brport_show,
+	.store = brport_store,
+};
+
+/*
+ * Add sysfs entries to ethernet device added to a bridge.
+ * Creates a brport subdirectory with bridge attributes.
+ * Puts symlink in bridge's brport subdirectory
+ */
+int ovs_dp_sysfs_add_if(struct vport *p)
+{
+	struct datapath *dp = p->dp;
+	struct vport *local_port = rtnl_dereference(dp->ports[OVSP_LOCAL]);
+	struct brport_attribute **a;
+	int err;
+
+	/* Create /sys/class/net/<devname>/brport directory. */
+	if (!p->ops->get_kobj)
+		return -ENOENT;
+
+	err = kobject_add(&p->kobj, p->ops->get_kobj(p),
+			  SYSFS_BRIDGE_PORT_ATTR);
+	if (err)
+		goto err;
+
+	/* Create symlink from /sys/class/net/<devname>/brport/bridge to
+	 * /sys/class/net/<bridgename>. */
+	err = sysfs_create_link(&p->kobj, local_port->ops->get_kobj(local_port),
+		SYSFS_BRIDGE_PORT_LINK); /* "bridge" */
+	if (err)
+		goto err_del;
+
+	/* Populate /sys/class/net/<devname>/brport directory with files. */
+	for (a = brport_attrs; *a; ++a) {
+		err = sysfs_create_file(&p->kobj, &((*a)->attr));
+		if (err)
+			goto err_del;
+	}
+
+	/* Create symlink from /sys/class/net/<bridgename>/brif/<devname> to
+	 * /sys/class/net/<devname>/brport.  */
+	err = sysfs_create_link(&dp->ifobj, &p->kobj, p->ops->get_name(p));
+	if (err)
+		goto err_del;
+	strcpy(p->linkname, p->ops->get_name(p));
+
+	kobject_uevent(&p->kobj, KOBJ_ADD);
+
+	return 0;
+
+err_del:
+	kobject_del(&p->kobj);
+err:
+	p->linkname[0] = 0;
+	return err;
+}
+
+int ovs_dp_sysfs_del_if(struct vport *p)
+{
+	if (p->linkname[0]) {
+		sysfs_remove_link(&p->dp->ifobj, p->linkname);
+		kobject_uevent(&p->kobj, KOBJ_REMOVE);
+		kobject_del(&p->kobj);
+		p->linkname[0] = '\0';
+	}
+	return 0;
+}
+#endif /* CONFIG_SYSFS */
diff -r 89e197c6e9d5 net/openvswitch/flow.c
--- /dev/null
+++ b/net/openvswitch/flow.c
@@ -0,0 +1,1409 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include "flow.h"
+#include "datapath.h"
+#include <linux/uaccess.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_ether.h>
+#include <linux/if_vlan.h>
+#include <net/llc_pdu.h>
+#include <linux/kernel.h>
+#include <linux/jhash.h>
+#include <linux/jiffies.h>
+#include <linux/llc.h>
+#include <linux/module.h>
+#include <linux/in.h>
+#include <linux/rcupdate.h>
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#include <linux/icmp.h>
+#include <linux/icmpv6.h>
+#include <linux/rculist.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/ndisc.h>
+
+#include "vlan.h"
+
+static struct kmem_cache *flow_cache;
+static unsigned int hash_seed __read_mostly;
+
+static int check_header(struct sk_buff *skb, int len)
+{
+	if (unlikely(skb->len < len))
+		return -EINVAL;
+	if (unlikely(!pskb_may_pull(skb, len)))
+		return -ENOMEM;
+	return 0;
+}
+
+static bool arphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_network_offset(skb) +
+				  sizeof(struct arp_eth_header));
+}
+
+static int check_iphdr(struct sk_buff *skb)
+{
+	unsigned int nh_ofs = skb_network_offset(skb);
+	unsigned int ip_len;
+	int err;
+
+	err = check_header(skb, nh_ofs + sizeof(struct iphdr));
+	if (unlikely(err))
+		return err;
+
+	ip_len = ip_hdrlen(skb);
+	if (unlikely(ip_len < sizeof(struct iphdr) ||
+		     skb->len < nh_ofs + ip_len))
+		return -EINVAL;
+
+	skb_set_transport_header(skb, nh_ofs + ip_len);
+	return 0;
+}
+
+static bool tcphdr_ok(struct sk_buff *skb)
+{
+	int th_ofs = skb_transport_offset(skb);
+	int tcp_len;
+
+	if (unlikely(!pskb_may_pull(skb, th_ofs + sizeof(struct tcphdr))))
+		return false;
+
+	tcp_len = tcp_hdrlen(skb);
+	if (unlikely(tcp_len < sizeof(struct tcphdr) ||
+		     skb->len < th_ofs + tcp_len))
+		return false;
+
+	return true;
+}
+
+static bool udphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct udphdr));
+}
+
+static bool icmphdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct icmphdr));
+}
+
+u64 ovs_flow_used_time(unsigned long flow_jiffies)
+{
+	struct timespec cur_ts;
+	u64 cur_ms, idle_ms;
+
+	ktime_get_ts(&cur_ts);
+	idle_ms = jiffies_to_msecs(jiffies - flow_jiffies);
+	cur_ms = (u64)cur_ts.tv_sec * MSEC_PER_SEC +
+		 cur_ts.tv_nsec / NSEC_PER_MSEC;
+
+	return cur_ms - idle_ms;
+}
+
+#define SW_FLOW_KEY_OFFSET(field)		\
+	(offsetof(struct sw_flow_key, field) +	\
+	 FIELD_SIZEOF(struct sw_flow_key, field))
+
+/**
+ * skip_exthdr - skip any IPv6 extension headers
+ * @skb: skbuff to parse
+ * @start: offset of first extension header
+ * @nexthdrp: Initially, points to the type of the extension header at @start.
+ * This function updates it to point to the extension header at the final
+ * offset.
+ * @frag: Points to the @frag member in a &struct sw_flow_key.  This
+ * function sets an appropriate %OVS_FRAG_TYPE_* value.
+ *
+ * This is based on ipv6_skip_exthdr() but adds the updates to *@frag.
+ *
+ * When there is more than one fragment header, this version reports whether
+ * the final fragment header that it examines is a first fragment.
+ *
+ * Returns the final payload offset, or -1 on error.
+ */
+static int skip_exthdr(const struct sk_buff *skb, int start, u8 *nexthdrp,
+		       u8 *frag)
+{
+	u8 nexthdr = *nexthdrp;
+
+	while (ipv6_ext_hdr(nexthdr)) {
+		struct ipv6_opt_hdr _hdr, *hp;
+		int hdrlen;
+
+		if (nexthdr == NEXTHDR_NONE)
+			return -1;
+		hp = skb_header_pointer(skb, start, sizeof(_hdr), &_hdr);
+		if (hp == NULL)
+			return -1;
+		if (nexthdr == NEXTHDR_FRAGMENT) {
+			__be16 _frag_off, *fp;
+			fp = skb_header_pointer(skb,
+						start+offsetof(struct frag_hdr,
+							       frag_off),
+						sizeof(_frag_off),
+						&_frag_off);
+			if (fp == NULL)
+				return -1;
+
+			if (ntohs(*fp) & ~0x7) {
+				*frag = OVS_FRAG_TYPE_LATER;
+				break;
+			}
+			*frag = OVS_FRAG_TYPE_FIRST;
+			hdrlen = 8;
+		} else if (nexthdr == NEXTHDR_AUTH)
+			hdrlen = (hp->hdrlen+2)<<2;
+		else
+			hdrlen = ipv6_optlen(hp);
+
+		nexthdr = hp->nexthdr;
+		start += hdrlen;
+	}
+
+	*nexthdrp = nexthdr;
+	return start;
+}
+
+static int parse_ipv6hdr(struct sk_buff *skb, struct sw_flow_key *key,
+			 int *key_lenp)
+{
+	unsigned int nh_ofs = skb_network_offset(skb);
+	unsigned int nh_len;
+	int payload_ofs;
+	struct ipv6hdr *nh;
+	uint8_t nexthdr;
+	int err;
+
+	*key_lenp = SW_FLOW_KEY_OFFSET(ipv6.label);
+
+	err = check_header(skb, nh_ofs + sizeof(*nh));
+	if (unlikely(err))
+		return err;
+
+	nh = ipv6_hdr(skb);
+	nexthdr = nh->nexthdr;
+	payload_ofs = (u8 *)(nh + 1) - skb->data;
+
+	key->ip.proto = NEXTHDR_NONE;
+	key->ip.tos = ipv6_get_dsfield(nh);
+	key->ip.ttl = nh->hop_limit;
+	key->ipv6.label = *(__be32 *)nh & htonl(IPV6_FLOWINFO_FLOWLABEL);
+	key->ipv6.addr.src = nh->saddr;
+	key->ipv6.addr.dst = nh->daddr;
+
+	payload_ofs = skip_exthdr(skb, payload_ofs, &nexthdr, &key->ip.frag);
+	if (unlikely(payload_ofs < 0))
+		return -EINVAL;
+
+	nh_len = payload_ofs - nh_ofs;
+	skb_set_transport_header(skb, nh_ofs + nh_len);
+	key->ip.proto = nexthdr;
+	return nh_len;
+}
+
+static bool icmp6hdr_ok(struct sk_buff *skb)
+{
+	return pskb_may_pull(skb, skb_transport_offset(skb) +
+				  sizeof(struct icmp6hdr));
+}
+
+#define TCP_FLAGS_OFFSET 13
+#define TCP_FLAG_MASK 0x3f
+
+void ovs_flow_used(struct sw_flow *flow, struct sk_buff *skb)
+{
+	u8 tcp_flags = 0;
+
+	if (flow->key.eth.type == htons(ETH_P_IP) &&
+	    flow->key.ip.proto == IPPROTO_TCP) {
+		u8 *tcp = (u8 *)tcp_hdr(skb);
+		tcp_flags = *(tcp + TCP_FLAGS_OFFSET) & TCP_FLAG_MASK;
+	}
+
+	spin_lock(&flow->lock);
+	flow->used = jiffies;
+	flow->packet_count++;
+	flow->byte_count += skb->len;
+	flow->tcp_flags |= tcp_flags;
+	spin_unlock(&flow->lock);
+}
+
+struct sw_flow_actions *ovs_flow_actions_alloc(const struct nlattr *actions)
+{
+	int actions_len = nla_len(actions);
+	struct sw_flow_actions *sfa;
+
+	/* At least DP_MAX_PORTS actions are required to be able to flood a
+	 * packet to every port.  Factor of 2 allows for setting VLAN tags,
+	 * etc. */
+	if (actions_len > 2 * DP_MAX_PORTS * nla_total_size(4))
+		return ERR_PTR(-EINVAL);
+
+	sfa = kmalloc(sizeof(*sfa) + actions_len, GFP_KERNEL);
+	if (!sfa)
+		return ERR_PTR(-ENOMEM);
+
+	sfa->actions_len = actions_len;
+	memcpy(sfa->actions, nla_data(actions), actions_len);
+	return sfa;
+}
+
+struct sw_flow *ovs_flow_alloc(void)
+{
+	struct sw_flow *flow;
+
+	flow = kmem_cache_alloc(flow_cache, GFP_KERNEL);
+	if (!flow)
+		return ERR_PTR(-ENOMEM);
+
+	spin_lock_init(&flow->lock);
+	atomic_set(&flow->refcnt, 1);
+	flow->sf_acts = NULL;
+	flow->dead = false;
+
+	return flow;
+}
+
+static struct hlist_head *find_bucket(struct flow_table *table, u32 hash)
+{
+	return flex_array_get(table->buckets,
+				(hash & (table->n_buckets - 1)));
+}
+
+static struct flex_array *alloc_buckets(unsigned int n_buckets)
+{
+	struct flex_array *buckets;
+	int i, err;
+
+	buckets = flex_array_alloc(sizeof(struct hlist_head *),
+				   n_buckets, GFP_KERNEL);
+	if (!buckets)
+		return NULL;
+
+	err = flex_array_prealloc(buckets, 0, n_buckets, GFP_KERNEL);
+	if (err) {
+		flex_array_free(buckets);
+		return NULL;
+	}
+
+	for (i = 0; i < n_buckets; i++)
+		INIT_HLIST_HEAD((struct hlist_head *)
+					flex_array_get(buckets, i));
+
+	return buckets;
+}
+
+static void free_buckets(struct flex_array *buckets)
+{
+	flex_array_free(buckets);
+}
+
+struct flow_table *ovs_flow_tbl_alloc(int new_size)
+{
+	struct flow_table *table = kmalloc(sizeof(*table), GFP_KERNEL);
+
+	if (!table)
+		return NULL;
+
+	table->buckets = alloc_buckets(new_size);
+
+	if (!table->buckets) {
+		kfree(table);
+		return NULL;
+	}
+	table->n_buckets = new_size;
+	table->count = 0;
+
+	return table;
+}
+
+static void flow_free(struct sw_flow *flow)
+{
+	flow->dead = true;
+	ovs_flow_put(flow);
+}
+
+void ovs_flow_tbl_destroy(struct flow_table *table)
+{
+	int i;
+
+	if (!table)
+		return;
+
+	for (i = 0; i < table->n_buckets; i++) {
+		struct sw_flow *flow;
+		struct hlist_head *head = flex_array_get(table->buckets, i);
+		struct hlist_node *node, *n;
+
+		hlist_for_each_entry_safe(flow, node, n, head, hash_node) {
+			hlist_del_init_rcu(&flow->hash_node);
+			flow_free(flow);
+		}
+	}
+
+	free_buckets(table->buckets);
+	kfree(table);
+}
+
+static void flow_tbl_destroy_rcu_cb(struct rcu_head *rcu)
+{
+	struct flow_table *table = container_of(rcu, struct flow_table, rcu);
+
+	ovs_flow_tbl_destroy(table);
+}
+
+void ovs_flow_tbl_deferred_destroy(struct flow_table *table)
+{
+	if (!table)
+		return;
+
+	call_rcu(&table->rcu, flow_tbl_destroy_rcu_cb);
+}
+
+struct sw_flow *ovs_flow_tbl_next(struct flow_table *table, u32 *bucket, u32 *last)
+{
+	struct sw_flow *flow;
+	struct hlist_head *head;
+	struct hlist_node *n;
+	int i;
+
+	while (*bucket < table->n_buckets) {
+		i = 0;
+		head = flex_array_get(table->buckets, *bucket);
+		hlist_for_each_entry_rcu(flow, n, head, hash_node) {
+			if (i < *last) {
+				i++;
+				continue;
+			}
+			*last = i + 1;
+			return flow;
+		}
+		(*bucket)++;
+		*last = 0;
+	}
+
+	return NULL;
+}
+
+struct flow_table *ovs_flow_tbl_expand(struct flow_table *table)
+{
+	struct flow_table *new_table;
+	int n_buckets = table->n_buckets * 2;
+	int i;
+
+	new_table = ovs_flow_tbl_alloc(n_buckets);
+	if (!new_table)
+		return ERR_PTR(-ENOMEM);
+
+	for (i = 0; i < table->n_buckets; i++) {
+		struct sw_flow *flow;
+		struct hlist_head *head;
+		struct hlist_node *n, *pos;
+
+		head = flex_array_get(table->buckets, i);
+
+		hlist_for_each_entry_safe(flow, n, pos, head, hash_node) {
+			hlist_del_init_rcu(&flow->hash_node);
+			ovs_flow_tbl_insert(new_table, flow);
+		}
+	}
+
+	return new_table;
+}
+
+/* RCU callback used by ovs_flow_deferred_free. */
+static void rcu_free_flow_callback(struct rcu_head *rcu)
+{
+	struct sw_flow *flow = container_of(rcu, struct sw_flow, rcu);
+
+	flow->dead = true;
+	ovs_flow_put(flow);
+}
+
+/* Schedules 'flow' to be freed after the next RCU grace period.
+ * The caller must hold rcu_read_lock for this to be sensible. */
+void ovs_flow_deferred_free(struct sw_flow *flow)
+{
+	call_rcu(&flow->rcu, rcu_free_flow_callback);
+}
+
+void ovs_flow_hold(struct sw_flow *flow)
+{
+	atomic_inc(&flow->refcnt);
+}
+
+void ovs_flow_put(struct sw_flow *flow)
+{
+	if (unlikely(!flow))
+		return;
+
+	if (atomic_dec_and_test(&flow->refcnt)) {
+		kfree((struct sf_flow_acts __force *)flow->sf_acts);
+		kmem_cache_free(flow_cache, flow);
+	}
+}
+
+/* RCU callback used by ovs_flow_deferred_free_acts. */
+static void rcu_free_acts_callback(struct rcu_head *rcu)
+{
+	struct sw_flow_actions *sf_acts = container_of(rcu,
+			struct sw_flow_actions, rcu);
+	kfree(sf_acts);
+}
+
+/* Schedules 'sf_acts' to be freed after the next RCU grace period.
+ * The caller must hold rcu_read_lock for this to be sensible. */
+void ovs_flow_deferred_free_acts(struct sw_flow_actions *sf_acts)
+{
+	call_rcu(&sf_acts->rcu, rcu_free_acts_callback);
+}
+
+static int parse_vlan(struct sk_buff *skb, struct sw_flow_key *key)
+{
+	struct qtag_prefix {
+		__be16 eth_type; /* ETH_P_8021Q */
+		__be16 tci;
+	};
+	struct qtag_prefix *qp;
+
+	if (unlikely(skb->len < sizeof(struct qtag_prefix) + sizeof(__be16)))
+		return 0;
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct qtag_prefix) +
+					 sizeof(__be16))))
+		return -ENOMEM;
+
+	qp = (struct qtag_prefix *) skb->data;
+	key->eth.tci = qp->tci | htons(VLAN_TAG_PRESENT);
+	__skb_pull(skb, sizeof(struct qtag_prefix));
+
+	return 0;
+}
+
+static __be16 parse_ethertype(struct sk_buff *skb)
+{
+	struct llc_snap_hdr {
+		u8  dsap;  /* Always 0xAA */
+		u8  ssap;  /* Always 0xAA */
+		u8  ctrl;
+		u8  oui[3];
+		__be16 ethertype;
+	};
+	struct llc_snap_hdr *llc;
+	__be16 proto;
+
+	proto = *(__be16 *) skb->data;
+	__skb_pull(skb, sizeof(__be16));
+
+	if (ntohs(proto) >= 1536)
+		return proto;
+
+	if (skb->len < sizeof(struct llc_snap_hdr))
+		return htons(ETH_P_802_2);
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct llc_snap_hdr))))
+		return htons(0);
+
+	llc = (struct llc_snap_hdr *) skb->data;
+	if (llc->dsap != LLC_SAP_SNAP ||
+	    llc->ssap != LLC_SAP_SNAP ||
+	    (llc->oui[0] | llc->oui[1] | llc->oui[2]) != 0)
+		return htons(ETH_P_802_2);
+
+	__skb_pull(skb, sizeof(struct llc_snap_hdr));
+	return llc->ethertype;
+}
+
+static int parse_icmpv6(struct sk_buff *skb, struct sw_flow_key *key,
+			int *key_lenp, int nh_len)
+{
+	struct icmp6hdr *icmp = icmp6_hdr(skb);
+	int error = 0;
+	int key_len;
+
+	/* The ICMPv6 type and code fields use the 16-bit transport port
+	 * fields, so we need to store them in 16-bit network byte order.
+	 */
+	key->ipv6.tp.src = htons(icmp->icmp6_type);
+	key->ipv6.tp.dst = htons(icmp->icmp6_code);
+	key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+
+	if (icmp->icmp6_code == 0 &&
+	    (icmp->icmp6_type == NDISC_NEIGHBOUR_SOLICITATION ||
+	     icmp->icmp6_type == NDISC_NEIGHBOUR_ADVERTISEMENT)) {
+		int icmp_len = skb->len - skb_transport_offset(skb);
+		struct nd_msg *nd;
+		int offset;
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+
+		/* In order to process neighbor discovery options, we need the
+		 * entire packet.
+		 */
+		if (unlikely(icmp_len < sizeof(*nd)))
+			goto out;
+		if (unlikely(skb_linearize(skb))) {
+			error = -ENOMEM;
+			goto out;
+		}
+
+		nd = (struct nd_msg *)skb_transport_header(skb);
+		key->ipv6.nd.target = nd->target;
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+
+		icmp_len -= sizeof(*nd);
+		offset = 0;
+		while (icmp_len >= 8) {
+			struct nd_opt_hdr *nd_opt =
+				 (struct nd_opt_hdr *)(nd->opt + offset);
+			int opt_len = nd_opt->nd_opt_len * 8;
+
+			if (unlikely(!opt_len || opt_len > icmp_len))
+				goto invalid;
+
+			/* Store the link layer address if the appropriate
+			 * option is provided.  It is considered an error if
+			 * the same link layer option is specified twice.
+			 */
+			if (nd_opt->nd_opt_type == ND_OPT_SOURCE_LL_ADDR
+			    && opt_len == 8) {
+				if (unlikely(!is_zero_ether_addr(key->ipv6.nd.sll)))
+					goto invalid;
+				memcpy(key->ipv6.nd.sll,
+				    &nd->opt[offset+sizeof(*nd_opt)], ETH_ALEN);
+			} else if (nd_opt->nd_opt_type == ND_OPT_TARGET_LL_ADDR
+				   && opt_len == 8) {
+				if (unlikely(!is_zero_ether_addr(key->ipv6.nd.tll)))
+					goto invalid;
+				memcpy(key->ipv6.nd.tll,
+				    &nd->opt[offset+sizeof(*nd_opt)], ETH_ALEN);
+			}
+
+			icmp_len -= opt_len;
+			offset += opt_len;
+		}
+	}
+
+	goto out;
+
+invalid:
+	memset(&key->ipv6.nd.target, 0, sizeof(key->ipv6.nd.target));
+	memset(key->ipv6.nd.sll, 0, sizeof(key->ipv6.nd.sll));
+	memset(key->ipv6.nd.tll, 0, sizeof(key->ipv6.nd.tll));
+
+out:
+	*key_lenp = key_len;
+	return error;
+}
+
+/**
+ * ovs_flow_extract - extracts a flow key from an Ethernet frame.
+ * @skb: sk_buff that contains the frame, with skb->data pointing to the
+ * Ethernet header
+ * @in_port: port number on which @skb was received.
+ * @key: output flow key
+ * @key_lenp: length of output flow key
+ *
+ * The caller must ensure that skb->len >= ETH_HLEN.
+ *
+ * Returns 0 if successful, otherwise a negative errno value.
+ *
+ * Initializes @skb header pointers as follows:
+ *
+ *    - skb->mac_header: the Ethernet header.
+ *
+ *    - skb->network_header: just past the Ethernet header, or just past the
+ *      VLAN header, to the first byte of the Ethernet payload.
+ *
+ *    - skb->transport_header: If key->dl_type is ETH_P_IP or ETH_P_IPV6
+ *      on output, then just past the IP header, if one is present and
+ *      of a correct length, otherwise the same as skb->network_header.
+ *      For other key->dl_type values it is left untouched.
+ */
+int ovs_flow_extract(struct sk_buff *skb, u16 in_port, struct sw_flow_key *key,
+		 int *key_lenp)
+{
+	int error = 0;
+	int key_len = SW_FLOW_KEY_OFFSET(eth);
+	struct ethhdr *eth;
+
+	memset(key, 0, sizeof(*key));
+
+	key->phy.priority = skb->priority;
+	key->phy.tun_id = OVS_CB(skb)->tun_id;
+	key->phy.in_port = in_port;
+
+	skb_reset_mac_header(skb);
+
+	/* Link layer.  We are guaranteed to have at least the 14 byte Ethernet
+	 * header in the linear data area.
+	 */
+	eth = eth_hdr(skb);
+	memcpy(key->eth.src, eth->h_source, ETH_ALEN);
+	memcpy(key->eth.dst, eth->h_dest, ETH_ALEN);
+
+	__skb_pull(skb, 2 * ETH_ALEN);
+
+	if (vlan_tx_tag_present(skb))
+		key->eth.tci = htons(vlan_get_tci(skb));
+	else if (eth->h_proto == htons(ETH_P_8021Q))
+		if (unlikely(parse_vlan(skb, key)))
+			return -ENOMEM;
+
+	key->eth.type = parse_ethertype(skb);
+	if (unlikely(key->eth.type == htons(0)))
+		return -ENOMEM;
+
+	skb_reset_network_header(skb);
+	__skb_push(skb, skb->data - skb_mac_header(skb));
+
+	/* Network layer. */
+	if (key->eth.type == htons(ETH_P_IP)) {
+		struct iphdr *nh;
+		__be16 offset;
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.addr);
+
+		error = check_iphdr(skb);
+		if (unlikely(error)) {
+			if (error == -EINVAL) {
+				skb->transport_header = skb->network_header;
+				error = 0;
+			}
+			goto out;
+		}
+
+		nh = ip_hdr(skb);
+		key->ipv4.addr.src = nh->saddr;
+		key->ipv4.addr.dst = nh->daddr;
+
+		key->ip.proto = nh->protocol;
+		key->ip.tos = nh->tos;
+		key->ip.ttl = nh->ttl;
+
+		offset = nh->frag_off & htons(IP_OFFSET);
+		if (offset) {
+			key->ip.frag = OVS_FRAG_TYPE_LATER;
+			goto out;
+		}
+		if (nh->frag_off & htons(IP_MF) ||
+			 skb_shinfo(skb)->gso_type & SKB_GSO_UDP)
+			key->ip.frag = OVS_FRAG_TYPE_FIRST;
+
+		/* Transport layer. */
+		if (key->ip.proto == IPPROTO_TCP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (tcphdr_ok(skb)) {
+				struct tcphdr *tcp = tcp_hdr(skb);
+				key->ipv4.tp.src = tcp->source;
+				key->ipv4.tp.dst = tcp->dest;
+			}
+		} else if (key->ip.proto == IPPROTO_UDP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (udphdr_ok(skb)) {
+				struct udphdr *udp = udp_hdr(skb);
+				key->ipv4.tp.src = udp->source;
+				key->ipv4.tp.dst = udp->dest;
+			}
+		} else if (key->ip.proto == IPPROTO_ICMP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+			if (icmphdr_ok(skb)) {
+				struct icmphdr *icmp = icmp_hdr(skb);
+				/* The ICMP type and code fields use the 16-bit
+				 * transport port fields, so we need to store
+				 * them in 16-bit network byte order. */
+				key->ipv4.tp.src = htons(icmp->type);
+				key->ipv4.tp.dst = htons(icmp->code);
+			}
+		}
+
+	} else if (key->eth.type == htons(ETH_P_ARP) && arphdr_ok(skb)) {
+		struct arp_eth_header *arp;
+
+		arp = (struct arp_eth_header *)skb_network_header(skb);
+
+		if (arp->ar_hrd == htons(ARPHRD_ETHER)
+				&& arp->ar_pro == htons(ETH_P_IP)
+				&& arp->ar_hln == ETH_ALEN
+				&& arp->ar_pln == 4) {
+
+			/* We only match on the lower 8 bits of the opcode. */
+			if (ntohs(arp->ar_op) <= 0xff)
+				key->ip.proto = ntohs(arp->ar_op);
+
+			if (key->ip.proto == ARPOP_REQUEST
+					|| key->ip.proto == ARPOP_REPLY) {
+				memcpy(&key->ipv4.addr.src, arp->ar_sip, sizeof(key->ipv4.addr.src));
+				memcpy(&key->ipv4.addr.dst, arp->ar_tip, sizeof(key->ipv4.addr.dst));
+				memcpy(key->ipv4.arp.sha, arp->ar_sha, ETH_ALEN);
+				memcpy(key->ipv4.arp.tha, arp->ar_tha, ETH_ALEN);
+				key_len = SW_FLOW_KEY_OFFSET(ipv4.arp);
+			}
+		}
+	} else if (key->eth.type == htons(ETH_P_IPV6)) {
+		int nh_len;             /* IPv6 Header + Extensions */
+
+		nh_len = parse_ipv6hdr(skb, key, &key_len);
+		if (unlikely(nh_len < 0)) {
+			if (nh_len == -EINVAL)
+				skb->transport_header = skb->network_header;
+			else
+				error = nh_len;
+			goto out;
+		}
+
+		if (key->ip.frag == OVS_FRAG_TYPE_LATER)
+			goto out;
+		if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP)
+			key->ip.frag = OVS_FRAG_TYPE_FIRST;
+
+		/* Transport layer. */
+		if (key->ip.proto == NEXTHDR_TCP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (tcphdr_ok(skb)) {
+				struct tcphdr *tcp = tcp_hdr(skb);
+				key->ipv6.tp.src = tcp->source;
+				key->ipv6.tp.dst = tcp->dest;
+			}
+		} else if (key->ip.proto == NEXTHDR_UDP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (udphdr_ok(skb)) {
+				struct udphdr *udp = udp_hdr(skb);
+				key->ipv6.tp.src = udp->source;
+				key->ipv6.tp.dst = udp->dest;
+			}
+		} else if (key->ip.proto == NEXTHDR_ICMP) {
+			key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+			if (icmp6hdr_ok(skb)) {
+				error = parse_icmpv6(skb, key, &key_len, nh_len);
+				if (error < 0)
+					goto out;
+			}
+		}
+	}
+
+out:
+	*key_lenp = key_len;
+	return error;
+}
+
+u32 ovs_flow_hash(const struct sw_flow_key *key, int key_len)
+{
+	return jhash2((u32 *)key, DIV_ROUND_UP(key_len, sizeof(u32)), hash_seed);
+}
+
+struct sw_flow *ovs_flow_tbl_lookup(struct flow_table *table,
+				struct sw_flow_key *key, int key_len)
+{
+	struct sw_flow *flow;
+	struct hlist_node *n;
+	struct hlist_head *head;
+	u32 hash;
+
+	hash = ovs_flow_hash(key, key_len);
+
+	head = find_bucket(table, hash);
+	hlist_for_each_entry_rcu(flow, n, head, hash_node) {
+
+		if (flow->hash == hash &&
+		    !memcmp(&flow->key, key, key_len)) {
+			return flow;
+		}
+	}
+	return NULL;
+}
+
+void ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow)
+{
+	struct hlist_head *head;
+
+	head = find_bucket(table, flow->hash);
+	hlist_add_head_rcu(&flow->hash_node, head);
+	table->count++;
+}
+
+void ovs_flow_tbl_remove(struct flow_table *table, struct sw_flow *flow)
+{
+	if (!hlist_unhashed(&flow->hash_node)) {
+		hlist_del_init_rcu(&flow->hash_node);
+		table->count--;
+		BUG_ON(table->count < 0);
+	}
+}
+
+/* The size of the argument for each %OVS_KEY_ATTR_* Netlink attribute.  */
+const int ovs_key_lens[OVS_KEY_ATTR_MAX + 1] = {
+	[OVS_KEY_ATTR_ENCAP] = -1,
+	[OVS_KEY_ATTR_PRIORITY] = sizeof(u32),
+	[OVS_KEY_ATTR_IN_PORT] = sizeof(u32),
+	[OVS_KEY_ATTR_ETHERNET] = sizeof(struct ovs_key_ethernet),
+	[OVS_KEY_ATTR_VLAN] = sizeof(__be16),
+	[OVS_KEY_ATTR_ETHERTYPE] = sizeof(__be16),
+	[OVS_KEY_ATTR_IPV4] = sizeof(struct ovs_key_ipv4),
+	[OVS_KEY_ATTR_IPV6] = sizeof(struct ovs_key_ipv6),
+	[OVS_KEY_ATTR_TCP] = sizeof(struct ovs_key_tcp),
+	[OVS_KEY_ATTR_UDP] = sizeof(struct ovs_key_udp),
+	[OVS_KEY_ATTR_ICMP] = sizeof(struct ovs_key_icmp),
+	[OVS_KEY_ATTR_ICMPV6] = sizeof(struct ovs_key_icmpv6),
+	[OVS_KEY_ATTR_ARP] = sizeof(struct ovs_key_arp),
+	[OVS_KEY_ATTR_ND] = sizeof(struct ovs_key_nd),
+
+	/* Not upstream. */
+	[OVS_KEY_ATTR_TUN_ID] = sizeof(__be64),
+};
+
+static int ipv4_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_len,
+				  const struct nlattr *a[], u64 *attrs)
+{
+	const struct ovs_key_icmp *icmp_key;
+	const struct ovs_key_tcp *tcp_key;
+	const struct ovs_key_udp *udp_key;
+
+	switch (swkey->ip.proto) {
+	case IPPROTO_TCP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_TCP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_TCP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		tcp_key = nla_data(a[OVS_KEY_ATTR_TCP]);
+		swkey->ipv4.tp.src = tcp_key->tcp_src;
+		swkey->ipv4.tp.dst = tcp_key->tcp_dst;
+		break;
+
+	case IPPROTO_UDP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_UDP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_UDP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		udp_key = nla_data(a[OVS_KEY_ATTR_UDP]);
+		swkey->ipv4.tp.src = udp_key->udp_src;
+		swkey->ipv4.tp.dst = udp_key->udp_dst;
+		break;
+
+	case IPPROTO_ICMP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_ICMP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_ICMP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv4.tp);
+		icmp_key = nla_data(a[OVS_KEY_ATTR_ICMP]);
+		swkey->ipv4.tp.src = htons(icmp_key->icmp_type);
+		swkey->ipv4.tp.dst = htons(icmp_key->icmp_code);
+		break;
+	}
+
+	return 0;
+}
+
+static int ipv6_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_len,
+				  const struct nlattr *a[], u64 *attrs)
+{
+	const struct ovs_key_icmpv6 *icmpv6_key;
+	const struct ovs_key_tcp *tcp_key;
+	const struct ovs_key_udp *udp_key;
+
+	switch (swkey->ip.proto) {
+	case IPPROTO_TCP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_TCP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_TCP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		tcp_key = nla_data(a[OVS_KEY_ATTR_TCP]);
+		swkey->ipv6.tp.src = tcp_key->tcp_src;
+		swkey->ipv6.tp.dst = tcp_key->tcp_dst;
+		break;
+
+	case IPPROTO_UDP:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_UDP)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_UDP);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		udp_key = nla_data(a[OVS_KEY_ATTR_UDP]);
+		swkey->ipv6.tp.src = udp_key->udp_src;
+		swkey->ipv6.tp.dst = udp_key->udp_dst;
+		break;
+
+	case IPPROTO_ICMPV6:
+		if (!(*attrs & (1 << OVS_KEY_ATTR_ICMPV6)))
+			return -EINVAL;
+		*attrs &= ~(1 << OVS_KEY_ATTR_ICMPV6);
+
+		*key_len = SW_FLOW_KEY_OFFSET(ipv6.tp);
+		icmpv6_key = nla_data(a[OVS_KEY_ATTR_ICMPV6]);
+		swkey->ipv6.tp.src = htons(icmpv6_key->icmpv6_type);
+		swkey->ipv6.tp.dst = htons(icmpv6_key->icmpv6_code);
+
+		if (swkey->ipv6.tp.src == htons(NDISC_NEIGHBOUR_SOLICITATION) ||
+		    swkey->ipv6.tp.src == htons(NDISC_NEIGHBOUR_ADVERTISEMENT)) {
+			const struct ovs_key_nd *nd_key;
+
+			if (!(*attrs & (1 << OVS_KEY_ATTR_ND)))
+				return -EINVAL;
+			*attrs &= ~(1 << OVS_KEY_ATTR_ND);
+
+			*key_len = SW_FLOW_KEY_OFFSET(ipv6.nd);
+			nd_key = nla_data(a[OVS_KEY_ATTR_ND]);
+			memcpy(&swkey->ipv6.nd.target, nd_key->nd_target,
+			       sizeof(swkey->ipv6.nd.target));
+			memcpy(swkey->ipv6.nd.sll, nd_key->nd_sll, ETH_ALEN);
+			memcpy(swkey->ipv6.nd.tll, nd_key->nd_tll, ETH_ALEN);
+		}
+		break;
+	}
+
+	return 0;
+}
+
+static int parse_flow_nlattrs(const struct nlattr *attr,
+			      const struct nlattr *a[], u64 *attrsp)
+{
+	const struct nlattr *nla;
+	u64 attrs;
+	int rem;
+
+	attrs = 0;
+	nla_for_each_nested(nla, attr, rem) {
+		u16 type = nla_type(nla);
+		int expected_len;
+
+		if (type > OVS_KEY_ATTR_MAX || attrs & (1ULL << type))
+			return -EINVAL;
+
+		expected_len = ovs_key_lens[type];
+		if (nla_len(nla) != expected_len && expected_len != -1)
+			return -EINVAL;
+
+		attrs |= 1ULL << type;
+		a[type] = nla;
+	}
+	if (rem)
+		return -EINVAL;
+
+	*attrsp = attrs;
+	return 0;
+}
+
+/**
+ * ovs_flow_from_nlattrs - parses Netlink attributes into a flow key.
+ * @swkey: receives the extracted flow key.
+ * @key_lenp: number of bytes used in @swkey.
+ * @attr: Netlink attribute holding nested %OVS_KEY_ATTR_* Netlink attribute
+ * sequence.
+ */
+int ovs_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_lenp,
+		      const struct nlattr *attr)
+{
+	const struct nlattr *a[OVS_KEY_ATTR_MAX + 1];
+	const struct ovs_key_ethernet *eth_key;
+	int key_len;
+	u64 attrs;
+	int err;
+
+	memset(swkey, 0, sizeof(struct sw_flow_key));
+	key_len = SW_FLOW_KEY_OFFSET(eth);
+
+	err = parse_flow_nlattrs(attr, a, &attrs);
+	if (err)
+		return err;
+
+	/* Metadata attributes. */
+	if (attrs & (1 << OVS_KEY_ATTR_PRIORITY)) {
+		swkey->phy.priority = nla_get_u32(a[OVS_KEY_ATTR_PRIORITY]);
+		attrs &= ~(1 << OVS_KEY_ATTR_PRIORITY);
+	}
+	if (attrs & (1 << OVS_KEY_ATTR_IN_PORT)) {
+		u32 in_port = nla_get_u32(a[OVS_KEY_ATTR_IN_PORT]);
+		if (in_port >= DP_MAX_PORTS)
+			return -EINVAL;
+		swkey->phy.in_port = in_port;
+		attrs &= ~(1 << OVS_KEY_ATTR_IN_PORT);
+	} else {
+		swkey->phy.in_port = USHRT_MAX;
+	}
+
+	if (attrs & (1ULL << OVS_KEY_ATTR_TUN_ID)) {
+		swkey->phy.tun_id = nla_get_be64(a[OVS_KEY_ATTR_TUN_ID]);
+		attrs &= ~(1ULL << OVS_KEY_ATTR_TUN_ID);
+	}
+
+	/* Data attributes. */
+	if (!(attrs & (1 << OVS_KEY_ATTR_ETHERNET)))
+		return -EINVAL;
+	attrs &= ~(1 << OVS_KEY_ATTR_ETHERNET);
+
+	eth_key = nla_data(a[OVS_KEY_ATTR_ETHERNET]);
+	memcpy(swkey->eth.src, eth_key->eth_src, ETH_ALEN);
+	memcpy(swkey->eth.dst, eth_key->eth_dst, ETH_ALEN);
+
+	if (attrs & (1u << OVS_KEY_ATTR_ETHERTYPE) &&
+	    nla_get_be16(a[OVS_KEY_ATTR_ETHERTYPE]) == htons(ETH_P_8021Q)) {
+		const struct nlattr *encap;
+		__be16 tci;
+
+		if (attrs != ((1 << OVS_KEY_ATTR_VLAN) |
+			      (1 << OVS_KEY_ATTR_ETHERTYPE) |
+			      (1 << OVS_KEY_ATTR_ENCAP)))
+			return -EINVAL;
+
+		encap = a[OVS_KEY_ATTR_ENCAP];
+		tci = nla_get_be16(a[OVS_KEY_ATTR_VLAN]);
+		if (tci & htons(VLAN_TAG_PRESENT)) {
+			swkey->eth.tci = tci;
+
+			err = parse_flow_nlattrs(encap, a, &attrs);
+			if (err)
+				return err;
+		} else if (!tci) {
+			/* Corner case for truncated 802.1Q header. */
+			if (nla_len(encap))
+				return -EINVAL;
+
+			swkey->eth.type = htons(ETH_P_8021Q);
+			*key_lenp = key_len;
+			return 0;
+		} else {
+			return -EINVAL;
+		}
+	}
+
+	if (attrs & (1 << OVS_KEY_ATTR_ETHERTYPE)) {
+		swkey->eth.type = nla_get_be16(a[OVS_KEY_ATTR_ETHERTYPE]);
+		if (ntohs(swkey->eth.type) < 1536)
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_ETHERTYPE);
+	} else {
+		swkey->eth.type = htons(ETH_P_802_2);
+	}
+
+	if (swkey->eth.type == htons(ETH_P_IP)) {
+		const struct ovs_key_ipv4 *ipv4_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_IPV4)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_IPV4);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.addr);
+		ipv4_key = nla_data(a[OVS_KEY_ATTR_IPV4]);
+		if (ipv4_key->ipv4_frag > OVS_FRAG_TYPE_MAX)
+			return -EINVAL;
+		swkey->ip.proto = ipv4_key->ipv4_proto;
+		swkey->ip.tos = ipv4_key->ipv4_tos;
+		swkey->ip.ttl = ipv4_key->ipv4_ttl;
+		swkey->ip.frag = ipv4_key->ipv4_frag;
+		swkey->ipv4.addr.src = ipv4_key->ipv4_src;
+		swkey->ipv4.addr.dst = ipv4_key->ipv4_dst;
+
+		if (swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+			err = ipv4_flow_from_nlattrs(swkey, &key_len, a, &attrs);
+			if (err)
+				return err;
+		}
+	} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+		const struct ovs_key_ipv6 *ipv6_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_IPV6)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_IPV6);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv6.label);
+		ipv6_key = nla_data(a[OVS_KEY_ATTR_IPV6]);
+		if (ipv6_key->ipv6_frag > OVS_FRAG_TYPE_MAX)
+			return -EINVAL;
+		swkey->ipv6.label = ipv6_key->ipv6_label;
+		swkey->ip.proto = ipv6_key->ipv6_proto;
+		swkey->ip.tos = ipv6_key->ipv6_tclass;
+		swkey->ip.ttl = ipv6_key->ipv6_hlimit;
+		swkey->ip.frag = ipv6_key->ipv6_frag;
+		memcpy(&swkey->ipv6.addr.src, ipv6_key->ipv6_src,
+		       sizeof(swkey->ipv6.addr.src));
+		memcpy(&swkey->ipv6.addr.dst, ipv6_key->ipv6_dst,
+		       sizeof(swkey->ipv6.addr.dst));
+
+		if (swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+			err = ipv6_flow_from_nlattrs(swkey, &key_len, a, &attrs);
+			if (err)
+				return err;
+		}
+	} else if (swkey->eth.type == htons(ETH_P_ARP)) {
+		const struct ovs_key_arp *arp_key;
+
+		if (!(attrs & (1 << OVS_KEY_ATTR_ARP)))
+			return -EINVAL;
+		attrs &= ~(1 << OVS_KEY_ATTR_ARP);
+
+		key_len = SW_FLOW_KEY_OFFSET(ipv4.arp);
+		arp_key = nla_data(a[OVS_KEY_ATTR_ARP]);
+		swkey->ipv4.addr.src = arp_key->arp_sip;
+		swkey->ipv4.addr.dst = arp_key->arp_tip;
+		if (arp_key->arp_op & htons(0xff00))
+			return -EINVAL;
+		swkey->ip.proto = ntohs(arp_key->arp_op);
+		memcpy(swkey->ipv4.arp.sha, arp_key->arp_sha, ETH_ALEN);
+		memcpy(swkey->ipv4.arp.tha, arp_key->arp_tha, ETH_ALEN);
+	}
+
+	if (attrs)
+		return -EINVAL;
+	*key_lenp = key_len;
+
+	return 0;
+}
+
+/**
+ * ovs_flow_metadata_from_nlattrs - parses Netlink attributes into a flow key.
+ * @in_port: receives the extracted input port.
+ * @tun_id: receives the extracted tunnel ID.
+ * @key: Netlink attribute holding nested %OVS_KEY_ATTR_* Netlink attribute
+ * sequence.
+ *
+ * This parses a series of Netlink attributes that form a flow key, which must
+ * take the same form accepted by flow_from_nlattrs(), but only enough of it to
+ * get the metadata, that is, the parts of the flow key that cannot be
+ * extracted from the packet itself.
+ */
+int ovs_flow_metadata_from_nlattrs(u32 *priority, u16 *in_port, __be64 *tun_id,
+				   const struct nlattr *attr)
+{
+	const struct nlattr *nla;
+	int rem;
+
+	*in_port = USHRT_MAX;
+	*tun_id = 0;
+	*priority = 0;
+
+	nla_for_each_nested(nla, attr, rem) {
+		int type = nla_type(nla);
+
+		if (type <= OVS_KEY_ATTR_MAX && ovs_key_lens[type] > 0) {
+			if (nla_len(nla) != ovs_key_lens[type])
+				return -EINVAL;
+
+			switch (type) {
+			case OVS_KEY_ATTR_PRIORITY:
+				*priority = nla_get_u32(nla);
+				break;
+
+			case OVS_KEY_ATTR_TUN_ID:
+				*tun_id = nla_get_be64(nla);
+				break;
+
+			case OVS_KEY_ATTR_IN_PORT:
+				if (nla_get_u32(nla) >= DP_MAX_PORTS)
+					return -EINVAL;
+				*in_port = nla_get_u32(nla);
+				break;
+			}
+		}
+	}
+	if (rem)
+		return -EINVAL;
+	return 0;
+}
+
+int ovs_flow_to_nlattrs(const struct sw_flow_key *swkey, struct sk_buff *skb)
+{
+	struct ovs_key_ethernet *eth_key;
+	struct nlattr *nla, *encap;
+
+	if (swkey->phy.priority)
+		NLA_PUT_U32(skb, OVS_KEY_ATTR_PRIORITY, swkey->phy.priority);
+
+	if (swkey->phy.tun_id != cpu_to_be64(0))
+		NLA_PUT_BE64(skb, OVS_KEY_ATTR_TUN_ID, swkey->phy.tun_id);
+
+	if (swkey->phy.in_port != USHRT_MAX)
+		NLA_PUT_U32(skb, OVS_KEY_ATTR_IN_PORT, swkey->phy.in_port);
+
+	nla = nla_reserve(skb, OVS_KEY_ATTR_ETHERNET, sizeof(*eth_key));
+	if (!nla)
+		goto nla_put_failure;
+	eth_key = nla_data(nla);
+	memcpy(eth_key->eth_src, swkey->eth.src, ETH_ALEN);
+	memcpy(eth_key->eth_dst, swkey->eth.dst, ETH_ALEN);
+
+	if (swkey->eth.tci || swkey->eth.type == htons(ETH_P_8021Q)) {
+		NLA_PUT_BE16(skb, OVS_KEY_ATTR_ETHERTYPE, htons(ETH_P_8021Q));
+		NLA_PUT_BE16(skb, OVS_KEY_ATTR_VLAN, swkey->eth.tci);
+		encap = nla_nest_start(skb, OVS_KEY_ATTR_ENCAP);
+		if (!swkey->eth.tci)
+			goto unencap;
+	} else {
+		encap = NULL;
+	}
+
+	if (swkey->eth.type == htons(ETH_P_802_2))
+		goto unencap;
+
+	NLA_PUT_BE16(skb, OVS_KEY_ATTR_ETHERTYPE, swkey->eth.type);
+
+	if (swkey->eth.type == htons(ETH_P_IP)) {
+		struct ovs_key_ipv4 *ipv4_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_IPV4, sizeof(*ipv4_key));
+		if (!nla)
+			goto nla_put_failure;
+		ipv4_key = nla_data(nla);
+		ipv4_key->ipv4_src = swkey->ipv4.addr.src;
+		ipv4_key->ipv4_dst = swkey->ipv4.addr.dst;
+		ipv4_key->ipv4_proto = swkey->ip.proto;
+		ipv4_key->ipv4_tos = swkey->ip.tos;
+		ipv4_key->ipv4_ttl = swkey->ip.ttl;
+		ipv4_key->ipv4_frag = swkey->ip.frag;
+	} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+		struct ovs_key_ipv6 *ipv6_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_IPV6, sizeof(*ipv6_key));
+		if (!nla)
+			goto nla_put_failure;
+		ipv6_key = nla_data(nla);
+		memcpy(ipv6_key->ipv6_src, &swkey->ipv6.addr.src,
+				sizeof(ipv6_key->ipv6_src));
+		memcpy(ipv6_key->ipv6_dst, &swkey->ipv6.addr.dst,
+				sizeof(ipv6_key->ipv6_dst));
+		ipv6_key->ipv6_label = swkey->ipv6.label;
+		ipv6_key->ipv6_proto = swkey->ip.proto;
+		ipv6_key->ipv6_tclass = swkey->ip.tos;
+		ipv6_key->ipv6_hlimit = swkey->ip.ttl;
+		ipv6_key->ipv6_frag = swkey->ip.frag;
+	} else if (swkey->eth.type == htons(ETH_P_ARP)) {
+		struct ovs_key_arp *arp_key;
+
+		nla = nla_reserve(skb, OVS_KEY_ATTR_ARP, sizeof(*arp_key));
+		if (!nla)
+			goto nla_put_failure;
+		arp_key = nla_data(nla);
+		memset(arp_key, 0, sizeof(struct ovs_key_arp));
+		arp_key->arp_sip = swkey->ipv4.addr.src;
+		arp_key->arp_tip = swkey->ipv4.addr.dst;
+		arp_key->arp_op = htons(swkey->ip.proto);
+		memcpy(arp_key->arp_sha, swkey->ipv4.arp.sha, ETH_ALEN);
+		memcpy(arp_key->arp_tha, swkey->ipv4.arp.tha, ETH_ALEN);
+	}
+
+	if ((swkey->eth.type == htons(ETH_P_IP) ||
+	     swkey->eth.type == htons(ETH_P_IPV6)) &&
+	     swkey->ip.frag != OVS_FRAG_TYPE_LATER) {
+
+		if (swkey->ip.proto == IPPROTO_TCP) {
+			struct ovs_key_tcp *tcp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_TCP, sizeof(*tcp_key));
+			if (!nla)
+				goto nla_put_failure;
+			tcp_key = nla_data(nla);
+			if (swkey->eth.type == htons(ETH_P_IP)) {
+				tcp_key->tcp_src = swkey->ipv4.tp.src;
+				tcp_key->tcp_dst = swkey->ipv4.tp.dst;
+			} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+				tcp_key->tcp_src = swkey->ipv6.tp.src;
+				tcp_key->tcp_dst = swkey->ipv6.tp.dst;
+			}
+		} else if (swkey->ip.proto == IPPROTO_UDP) {
+			struct ovs_key_udp *udp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_UDP, sizeof(*udp_key));
+			if (!nla)
+				goto nla_put_failure;
+			udp_key = nla_data(nla);
+			if (swkey->eth.type == htons(ETH_P_IP)) {
+				udp_key->udp_src = swkey->ipv4.tp.src;
+				udp_key->udp_dst = swkey->ipv4.tp.dst;
+			} else if (swkey->eth.type == htons(ETH_P_IPV6)) {
+				udp_key->udp_src = swkey->ipv6.tp.src;
+				udp_key->udp_dst = swkey->ipv6.tp.dst;
+			}
+		} else if (swkey->eth.type == htons(ETH_P_IP) &&
+			   swkey->ip.proto == IPPROTO_ICMP) {
+			struct ovs_key_icmp *icmp_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_ICMP, sizeof(*icmp_key));
+			if (!nla)
+				goto nla_put_failure;
+			icmp_key = nla_data(nla);
+			icmp_key->icmp_type = ntohs(swkey->ipv4.tp.src);
+			icmp_key->icmp_code = ntohs(swkey->ipv4.tp.dst);
+		} else if (swkey->eth.type == htons(ETH_P_IPV6) &&
+			   swkey->ip.proto == IPPROTO_ICMPV6) {
+			struct ovs_key_icmpv6 *icmpv6_key;
+
+			nla = nla_reserve(skb, OVS_KEY_ATTR_ICMPV6,
+						sizeof(*icmpv6_key));
+			if (!nla)
+				goto nla_put_failure;
+			icmpv6_key = nla_data(nla);
+			icmpv6_key->icmpv6_type = ntohs(swkey->ipv6.tp.src);
+			icmpv6_key->icmpv6_code = ntohs(swkey->ipv6.tp.dst);
+
+			if (icmpv6_key->icmpv6_type == NDISC_NEIGHBOUR_SOLICITATION ||
+			    icmpv6_key->icmpv6_type == NDISC_NEIGHBOUR_ADVERTISEMENT) {
+				struct ovs_key_nd *nd_key;
+
+				nla = nla_reserve(skb, OVS_KEY_ATTR_ND, sizeof(*nd_key));
+				if (!nla)
+					goto nla_put_failure;
+				nd_key = nla_data(nla);
+				memcpy(nd_key->nd_target, &swkey->ipv6.nd.target,
+							sizeof(nd_key->nd_target));
+				memcpy(nd_key->nd_sll, swkey->ipv6.nd.sll, ETH_ALEN);
+				memcpy(nd_key->nd_tll, swkey->ipv6.nd.tll, ETH_ALEN);
+			}
+		}
+	}
+
+unencap:
+	if (encap)
+		nla_nest_end(skb, encap);
+
+	return 0;
+
+nla_put_failure:
+	return -EMSGSIZE;
+}
+
+/* Initializes the flow module.
+ * Returns zero if successful or a negative error code. */
+int ovs_flow_init(void)
+{
+	flow_cache = kmem_cache_create("sw_flow", sizeof(struct sw_flow), 0,
+					0, NULL);
+	if (flow_cache == NULL)
+		return -ENOMEM;
+
+	get_random_bytes(&hash_seed, sizeof(hash_seed));
+
+	return 0;
+}
+
+/* Uninitializes the flow module. */
+void ovs_flow_exit(void)
+{
+	kmem_cache_destroy(flow_cache);
+}
diff -r 89e197c6e9d5 net/openvswitch/flow.h
--- /dev/null
+++ b/net/openvswitch/flow.h
@@ -0,0 +1,202 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef FLOW_H
+#define FLOW_H 1
+
+#include <linux/kernel.h>
+#include <linux/netlink.h>
+#include <linux/openvswitch.h>
+#include <linux/spinlock.h>
+#include <linux/types.h>
+#include <linux/rcupdate.h>
+#include <linux/if_ether.h>
+#include <linux/in6.h>
+#include <linux/jiffies.h>
+#include <linux/time.h>
+#include <linux/flex_array.h>
+#include <net/inet_ecn.h>
+
+struct sk_buff;
+
+struct sw_flow_actions {
+	struct rcu_head rcu;
+	u32 actions_len;
+	struct nlattr actions[];
+};
+
+struct sw_flow_key {
+	struct {
+		__be64	tun_id;		/* Encapsulating tunnel ID. */
+		u32	priority;	/* Packet QoS priority. */
+		u16	in_port;	/* Input switch port (or USHRT_MAX). */
+	} phy;
+	struct {
+		u8     src[ETH_ALEN];	/* Ethernet source address. */
+		u8     dst[ETH_ALEN];	/* Ethernet destination address. */
+		__be16 tci;		/* 0 if no VLAN, VLAN_TAG_PRESENT set otherwise. */
+		__be16 type;		/* Ethernet frame type. */
+	} eth;
+	struct {
+		u8     proto;		/* IP protocol or lower 8 bits of ARP opcode. */
+		u8     tos;		/* IP ToS. */
+		u8     ttl;		/* IP TTL/hop limit. */
+		u8     frag;		/* One of OVS_FRAG_TYPE_*. */
+	} ip;
+	union {
+		struct {
+			struct {
+				__be32 src;	/* IP source address. */
+				__be32 dst;	/* IP destination address. */
+			} addr;
+			union {
+				struct {
+					__be16 src;		/* TCP/UDP source port. */
+					__be16 dst;		/* TCP/UDP destination port. */
+				} tp;
+				struct {
+					u8 sha[ETH_ALEN];	/* ARP source hardware address. */
+					u8 tha[ETH_ALEN];	/* ARP target hardware address. */
+				} arp;
+			};
+		} ipv4;
+		struct {
+			struct {
+				struct in6_addr src;	/* IPv6 source address. */
+				struct in6_addr dst;	/* IPv6 destination address. */
+			} addr;
+			__be32 label;			/* IPv6 flow label. */
+			struct {
+				__be16 src;		/* TCP/UDP source port. */
+				__be16 dst;		/* TCP/UDP destination port. */
+			} tp;
+			struct {
+				struct in6_addr target;	/* ND target address. */
+				u8 sll[ETH_ALEN];	/* ND source link layer address. */
+				u8 tll[ETH_ALEN];	/* ND target link layer address. */
+			} nd;
+		} ipv6;
+	};
+};
+
+struct sw_flow {
+	struct rcu_head rcu;
+	struct hlist_node  hash_node;
+	u32 hash;
+
+	struct sw_flow_key key;
+	struct sw_flow_actions __rcu *sf_acts;
+
+	atomic_t refcnt;
+	bool dead;
+
+	spinlock_t lock;	/* Lock for values below. */
+	unsigned long used;	/* Last used time (in jiffies). */
+	u64 packet_count;	/* Number of packets matched. */
+	u64 byte_count;		/* Number of bytes matched. */
+	u8 tcp_flags;		/* Union of seen TCP flags. */
+};
+
+struct arp_eth_header {
+	__be16      ar_hrd;	/* format of hardware address   */
+	__be16      ar_pro;	/* format of protocol address   */
+	unsigned char   ar_hln;	/* length of hardware address   */
+	unsigned char   ar_pln;	/* length of protocol address   */
+	__be16      ar_op;	/* ARP opcode (command)     */
+
+	/* Ethernet+IPv4 specific members. */
+	unsigned char       ar_sha[ETH_ALEN];	/* sender hardware address  */
+	unsigned char       ar_sip[4];		/* sender IP address        */
+	unsigned char       ar_tha[ETH_ALEN];	/* target hardware address  */
+	unsigned char       ar_tip[4];		/* target IP address        */
+} __packed;
+
+int ovs_flow_init(void);
+void ovs_flow_exit(void);
+
+struct sw_flow *ovs_flow_alloc(void);
+void ovs_flow_deferred_free(struct sw_flow *);
+
+struct sw_flow_actions *ovs_flow_actions_alloc(const struct nlattr *);
+void ovs_flow_deferred_free_acts(struct sw_flow_actions *);
+
+void ovs_flow_hold(struct sw_flow *);
+void ovs_flow_put(struct sw_flow *);
+
+int ovs_flow_extract(struct sk_buff *, u16 in_port, struct sw_flow_key *,
+		     int *key_lenp);
+void ovs_flow_used(struct sw_flow *, struct sk_buff *);
+u64 ovs_flow_used_time(unsigned long flow_jiffies);
+
+/* Upper bound on the length of a nlattr-formatted flow key.  The longest
+ * nlattr-formatted flow key would be:
+ *
+ *                         struct  pad  nl hdr  total
+ *                         ------  ---  ------  -----
+ *  OVS_KEY_ATTR_PRIORITY      4    --     4      8
+ *  OVS_KEY_ATTR_TUN_ID        8    --     4     12
+ *  OVS_KEY_ATTR_IN_PORT       4    --     4      8
+ *  OVS_KEY_ATTR_ETHERNET     12    --     4     16
+ *  OVS_KEY_ATTR_8021Q         4    --     4      8
+ *  OVS_KEY_ATTR_ETHERTYPE     2     2     4      8
+ *  OVS_KEY_ATTR_IPV6         40    --     4     44
+ *  OVS_KEY_ATTR_ICMPV6        2     2     4      8
+ *  OVS_KEY_ATTR_ND           28    --     4     32
+ *  -------------------------------------------------
+ *  total                                       144
+ */
+#define FLOW_BUFSIZE 144
+
+int ovs_flow_to_nlattrs(const struct sw_flow_key *, struct sk_buff *);
+int ovs_flow_from_nlattrs(struct sw_flow_key *swkey, int *key_lenp,
+		      const struct nlattr *);
+int ovs_flow_metadata_from_nlattrs(u32 *priority, u16 *in_port, __be64 *tun_id,
+				   const struct nlattr *);
+
+#define TBL_MIN_BUCKETS		1024
+
+struct flow_table {
+	struct flex_array *buckets;
+	unsigned int count, n_buckets;
+	struct rcu_head rcu;
+};
+
+static inline int ovs_flow_tbl_count(struct flow_table *table)
+{
+	return table->count;
+}
+
+static inline int ovs_flow_tbl_need_to_expand(struct flow_table *table)
+{
+	return (table->count > table->n_buckets);
+}
+
+struct sw_flow *ovs_flow_tbl_lookup(struct flow_table *table,
+				    struct sw_flow_key *key, int len);
+void ovs_flow_tbl_destroy(struct flow_table *table);
+void ovs_flow_tbl_deferred_destroy(struct flow_table *table);
+struct flow_table *ovs_flow_tbl_alloc(int new_size);
+struct flow_table *ovs_flow_tbl_expand(struct flow_table *table);
+void ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow);
+void ovs_flow_tbl_remove(struct flow_table *table, struct sw_flow *flow);
+u32 ovs_flow_hash(const struct sw_flow_key *key, int key_len);
+
+struct sw_flow *ovs_flow_tbl_next(struct flow_table *table, u32 *bucket, u32 *idx);
+extern const int ovs_key_lens[OVS_KEY_ATTR_MAX + 1];
+
+#endif /* flow.h */
diff -r 89e197c6e9d5 net/openvswitch/tunnel.c
--- /dev/null
+++ b/net/openvswitch/tunnel.c
@@ -0,0 +1,1653 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/if_arp.h>
+#include <linux/if_ether.h>
+#include <linux/ip.h>
+#include <linux/if_vlan.h>
+#include <linux/igmp.h>
+#include <linux/in.h>
+#include <linux/in_route.h>
+#include <linux/inetdevice.h>
+#include <linux/jhash.h>
+#include <linux/list.h>
+#include <linux/kernel.h>
+#include <linux/version.h>
+#include <linux/workqueue.h>
+#include <linux/rculist.h>
+
+#include <net/dsfield.h>
+#include <net/dst.h>
+#include <net/icmp.h>
+#include <net/inet_ecn.h>
+#include <net/ip.h>
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+#include <net/ipv6.h>
+#endif
+#include <net/route.h>
+#include <net/xfrm.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "tunnel.h"
+#include "vlan.h"
+#include "vport.h"
+#include "vport-generic.h"
+#include "vport-internal_dev.h"
+
+#ifdef NEED_CACHE_TIMEOUT
+/*
+ * On kernels where we can't quickly detect changes in the rest of the system
+ * we use an expiration time to invalidate the cache.  A shorter expiration
+ * reduces the length of time that we may potentially blackhole packets while
+ * a longer time increases performance by reducing the frequency that the
+ * cache needs to be rebuilt.  A variety of factors may cause the cache to be
+ * invalidated before the expiration time but this is the maximum.  The time
+ * is expressed in jiffies.
+ */
+#define MAX_CACHE_EXP HZ
+#endif
+
+/*
+ * Interval to check for and remove caches that are no longer valid.  Caches
+ * are checked for validity before they are used for packet encapsulation and
+ * old caches are removed at that time.  However, if no packets are sent through
+ * the tunnel then the cache will never be destroyed.  Since it holds
+ * references to a number of system objects, the cache will continue to use
+ * system resources by not allowing those objects to be destroyed.  The cache
+ * cleaner is periodically run to free invalid caches.  It does not
+ * significantly affect system performance.  A lower interval will release
+ * resources faster but will itself consume resources by requiring more frequent
+ * checks.  A longer interval may result in messages being printed to the kernel
+ * message buffer about unreleased resources.  The interval is expressed in
+ * jiffies.
+ */
+#define CACHE_CLEANER_INTERVAL (5 * HZ)
+
+#define CACHE_DATA_ALIGN 16
+#define PORT_TABLE_SIZE  1024
+
+static struct hlist_head *port_table __read_mostly;
+static int port_table_count;
+
+static void cache_cleaner(struct work_struct *work);
+static DECLARE_DELAYED_WORK(cache_cleaner_wq, cache_cleaner);
+
+/*
+ * These are just used as an optimization: they don't require any kind of
+ * synchronization because we could have just as easily read the value before
+ * the port change happened.
+ */
+static unsigned int key_local_remote_ports __read_mostly;
+static unsigned int key_remote_ports __read_mostly;
+static unsigned int key_multicast_ports __read_mostly;
+static unsigned int local_remote_ports __read_mostly;
+static unsigned int remote_ports __read_mostly;
+static unsigned int multicast_ports __read_mostly;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#define rt_dst(rt) (rt->dst)
+#else
+#define rt_dst(rt) (rt->u.dst)
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
+static struct hh_cache *rt_hh(struct rtable *rt)
+{
+	struct neighbour *neigh = dst_get_neighbour(&rt->dst);
+	if (!neigh || !(neigh->nud_state & NUD_CONNECTED) ||
+			!neigh->hh.hh_len)
+		return NULL;
+	return &neigh->hh;
+}
+#else
+#define rt_hh(rt) (rt_dst(rt).hh)
+#endif
+
+static struct vport *tnl_vport_to_vport(const struct tnl_vport *tnl_vport)
+{
+	return vport_from_priv(tnl_vport);
+}
+
+/* This is analogous to rtnl_dereference for the tunnel cache.  It checks that
+ * cache_lock is held, so it is only for update side code.
+ */
+static struct tnl_cache *cache_dereference(struct tnl_vport *tnl_vport)
+{
+	return rcu_dereference_protected(tnl_vport->cache,
+				 lockdep_is_held(&tnl_vport->cache_lock));
+}
+
+static void schedule_cache_cleaner(void)
+{
+	schedule_delayed_work(&cache_cleaner_wq, CACHE_CLEANER_INTERVAL);
+}
+
+static void free_cache(struct tnl_cache *cache)
+{
+	if (!cache)
+		return;
+
+	ovs_flow_put(cache->flow);
+	ip_rt_put(cache->rt);
+	kfree(cache);
+}
+
+static void free_config_rcu(struct rcu_head *rcu)
+{
+	struct tnl_mutable_config *c = container_of(rcu, struct tnl_mutable_config, rcu);
+	kfree(c);
+}
+
+static void free_cache_rcu(struct rcu_head *rcu)
+{
+	struct tnl_cache *c = container_of(rcu, struct tnl_cache, rcu);
+	free_cache(c);
+}
+
+/* Frees the portion of 'mutable' that requires RTNL and thus can't happen
+ * within an RCU callback.  Fortunately this part doesn't require waiting for
+ * an RCU grace period.
+ */
+static void free_mutable_rtnl(struct tnl_mutable_config *mutable)
+{
+	ASSERT_RTNL();
+	if (ipv4_is_multicast(mutable->key.daddr) && mutable->mlink) {
+		struct in_device *in_dev;
+		in_dev = inetdev_by_index(&init_net, mutable->mlink);
+		if (in_dev)
+			ip_mc_dec_group(in_dev, mutable->key.daddr);
+	}
+}
+
+static void assign_config_rcu(struct vport *vport,
+			      struct tnl_mutable_config *new_config)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *old_config;
+
+	old_config = rtnl_dereference(tnl_vport->mutable);
+	rcu_assign_pointer(tnl_vport->mutable, new_config);
+
+	free_mutable_rtnl(old_config);
+	call_rcu(&old_config->rcu, free_config_rcu);
+}
+
+static void assign_cache_rcu(struct vport *vport, struct tnl_cache *new_cache)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_cache *old_cache;
+
+	old_cache = cache_dereference(tnl_vport);
+	rcu_assign_pointer(tnl_vport->cache, new_cache);
+
+	if (old_cache)
+		call_rcu(&old_cache->rcu, free_cache_rcu);
+}
+
+static unsigned int *find_port_pool(const struct tnl_mutable_config *mutable)
+{
+	bool is_multicast = ipv4_is_multicast(mutable->key.daddr);
+
+	if (mutable->flags & TNL_F_IN_KEY_MATCH) {
+		if (mutable->key.saddr)
+			return &local_remote_ports;
+		else if (is_multicast)
+			return &multicast_ports;
+		else
+			return &remote_ports;
+	} else {
+		if (mutable->key.saddr)
+			return &key_local_remote_ports;
+		else if (is_multicast)
+			return &key_multicast_ports;
+		else
+			return &key_remote_ports;
+	}
+}
+
+static u32 port_hash(const struct port_lookup_key *key)
+{
+	return jhash2((u32 *)key, (PORT_KEY_LEN / sizeof(u32)), 0);
+}
+
+static struct hlist_head *find_bucket(u32 hash)
+{
+	return &port_table[(hash & (PORT_TABLE_SIZE - 1))];
+}
+
+static void port_table_add_port(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable;
+	u32 hash;
+
+	if (port_table_count == 0)
+		schedule_cache_cleaner();
+
+	mutable = rtnl_dereference(tnl_vport->mutable);
+	hash = port_hash(&mutable->key);
+	hlist_add_head_rcu(&tnl_vport->hash_node, find_bucket(hash));
+	port_table_count++;
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))++;
+}
+
+static void port_table_move_port(struct vport *vport,
+		      struct tnl_mutable_config *new_mutable)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	u32 hash;
+
+	hash = port_hash(&new_mutable->key);
+	hlist_del_init_rcu(&tnl_vport->hash_node);
+	hlist_add_head_rcu(&tnl_vport->hash_node, find_bucket(hash));
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))--;
+	assign_config_rcu(vport, new_mutable);
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))++;
+}
+
+static void port_table_remove_port(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+
+	hlist_del_init_rcu(&tnl_vport->hash_node);
+
+	port_table_count--;
+	if (port_table_count == 0)
+		cancel_delayed_work_sync(&cache_cleaner_wq);
+
+	(*find_port_pool(rtnl_dereference(tnl_vport->mutable)))--;
+}
+
+static struct vport *port_table_lookup(struct port_lookup_key *key,
+				       const struct tnl_mutable_config **pmutable)
+{
+	struct hlist_node *n;
+	struct hlist_head *bucket;
+	u32 hash = port_hash(key);
+	struct tnl_vport *tnl_vport;
+
+	bucket = find_bucket(hash);
+
+	hlist_for_each_entry_rcu(tnl_vport, n, bucket, hash_node) {
+		struct tnl_mutable_config *mutable;
+
+		mutable = rcu_dereference_rtnl(tnl_vport->mutable);
+		if (!memcmp(&mutable->key, key, PORT_KEY_LEN)) {
+			*pmutable = mutable;
+			return tnl_vport_to_vport(tnl_vport);
+		}
+	}
+
+	return NULL;
+}
+
+struct vport *ovs_tnl_find_port(__be32 saddr, __be32 daddr, __be64 key,
+				int tunnel_type,
+				const struct tnl_mutable_config **mutable)
+{
+	struct port_lookup_key lookup;
+	struct vport *vport;
+	bool is_multicast = ipv4_is_multicast(saddr);
+
+	lookup.saddr = saddr;
+	lookup.daddr = daddr;
+
+	/* First try for exact match on in_key. */
+	lookup.in_key = key;
+	lookup.tunnel_type = tunnel_type | TNL_T_KEY_EXACT;
+	if (!is_multicast && key_local_remote_ports) {
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+	if (key_remote_ports) {
+		lookup.saddr = 0;
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+
+		lookup.saddr = saddr;
+	}
+
+	/* Then try matches that wildcard in_key. */
+	lookup.in_key = 0;
+	lookup.tunnel_type = tunnel_type | TNL_T_KEY_MATCH;
+	if (!is_multicast && local_remote_ports) {
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+	if (remote_ports) {
+		lookup.saddr = 0;
+		vport = port_table_lookup(&lookup, mutable);
+		if (vport)
+			return vport;
+	}
+
+	if (is_multicast) {
+		lookup.saddr = 0;
+		lookup.daddr = saddr;
+		if (key_multicast_ports) {
+			lookup.tunnel_type = tunnel_type | TNL_T_KEY_EXACT;
+			lookup.in_key = key;
+			vport = port_table_lookup(&lookup, mutable);
+			if (vport)
+				return vport;
+		}
+		if (multicast_ports) {
+			lookup.tunnel_type = tunnel_type | TNL_T_KEY_MATCH;
+			lookup.in_key = 0;
+			vport = port_table_lookup(&lookup, mutable);
+			if (vport)
+				return vport;
+		}
+	}
+
+	return NULL;
+}
+
+static void ecn_decapsulate(struct sk_buff *skb, u8 tos)
+{
+	if (unlikely(INET_ECN_is_ce(tos))) {
+		__be16 protocol = skb->protocol;
+
+		skb_set_network_header(skb, ETH_HLEN);
+
+		if (protocol == htons(ETH_P_8021Q)) {
+			if (unlikely(!pskb_may_pull(skb, VLAN_ETH_HLEN)))
+				return;
+
+			protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+			skb_set_network_header(skb, VLAN_ETH_HLEN);
+		}
+
+		if (protocol == htons(ETH_P_IP)) {
+			if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+			    + sizeof(struct iphdr))))
+				return;
+
+			IP_ECN_set_ce(ip_hdr(skb));
+		}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		else if (protocol == htons(ETH_P_IPV6)) {
+			if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+			    + sizeof(struct ipv6hdr))))
+				return;
+
+			IP6_ECN_set_ce(ipv6_hdr(skb));
+		}
+#endif
+	}
+}
+
+/**
+ *	ovs_tnl_rcv - ingress point for generic tunnel code
+ *
+ * @vport: port this packet was received on
+ * @skb: received packet
+ * @tos: ToS from encapsulating IP packet, used to copy ECN bits
+ *
+ * Must be called with rcu_read_lock.
+ *
+ * Packets received by this function are in the following state:
+ * - skb->data points to the inner Ethernet header.
+ * - The inner Ethernet header is in the linear data area.
+ * - skb->csum does not include the inner Ethernet header.
+ * - The layer pointers are undefined.
+ */
+void ovs_tnl_rcv(struct vport *vport, struct sk_buff *skb, u8 tos)
+{
+	struct ethhdr *eh;
+
+	skb_reset_mac_header(skb);
+	eh = eth_hdr(skb);
+
+	if (likely(ntohs(eh->h_proto) >= 1536))
+		skb->protocol = eh->h_proto;
+	else
+		skb->protocol = htons(ETH_P_802_2);
+
+	skb_dst_drop(skb);
+	nf_reset(skb);
+	skb_clear_rxhash(skb);
+	secpath_reset(skb);
+
+	ecn_decapsulate(skb, tos);
+	vlan_set_tci(skb, 0);
+
+	if (unlikely(compute_ip_summed(skb, false))) {
+		kfree_skb(skb);
+		return;
+	}
+
+	ovs_vport_receive(vport, skb);
+}
+
+static bool check_ipv4_address(__be32 addr)
+{
+	if (ipv4_is_multicast(addr) || ipv4_is_lbcast(addr)
+	    || ipv4_is_loopback(addr) || ipv4_is_zeronet(addr))
+		return false;
+
+	return true;
+}
+
+static bool ipv4_should_icmp(struct sk_buff *skb)
+{
+	struct iphdr *old_iph = ip_hdr(skb);
+
+	/* Don't respond to L2 broadcast. */
+	if (is_multicast_ether_addr(eth_hdr(skb)->h_dest))
+		return false;
+
+	/* Don't respond to L3 broadcast or invalid addresses. */
+	if (!check_ipv4_address(old_iph->daddr) ||
+	    !check_ipv4_address(old_iph->saddr))
+		return false;
+
+	/* Only respond to the first fragment. */
+	if (old_iph->frag_off & htons(IP_OFFSET))
+		return false;
+
+	/* Don't respond to ICMP error messages. */
+	if (old_iph->protocol == IPPROTO_ICMP) {
+		u8 icmp_type, *icmp_typep;
+
+		icmp_typep = skb_header_pointer(skb, (u8 *)old_iph +
+						(old_iph->ihl << 2) +
+						offsetof(struct icmphdr, type) -
+						skb->data, sizeof(icmp_type),
+						&icmp_type);
+
+		if (!icmp_typep)
+			return false;
+
+		if (*icmp_typep > NR_ICMP_TYPES
+			|| (*icmp_typep <= ICMP_PARAMETERPROB
+				&& *icmp_typep != ICMP_ECHOREPLY
+				&& *icmp_typep != ICMP_ECHO))
+			return false;
+	}
+
+	return true;
+}
+
+static void ipv4_build_icmp(struct sk_buff *skb, struct sk_buff *nskb,
+			    unsigned int mtu, unsigned int payload_length)
+{
+	struct iphdr *iph, *old_iph = ip_hdr(skb);
+	struct icmphdr *icmph;
+	u8 *payload;
+
+	iph = (struct iphdr *)skb_put(nskb, sizeof(struct iphdr));
+	icmph = (struct icmphdr *)skb_put(nskb, sizeof(struct icmphdr));
+	payload = skb_put(nskb, payload_length);
+
+	/* IP */
+	iph->version		=	4;
+	iph->ihl		=	sizeof(struct iphdr) >> 2;
+	iph->tos		=	(old_iph->tos & IPTOS_TOS_MASK) |
+					IPTOS_PREC_INTERNETCONTROL;
+	iph->tot_len		=	htons(sizeof(struct iphdr)
+					      + sizeof(struct icmphdr)
+					      + payload_length);
+	get_random_bytes(&iph->id, sizeof(iph->id));
+	iph->frag_off		=	0;
+	iph->ttl		=	IPDEFTTL;
+	iph->protocol		=	IPPROTO_ICMP;
+	iph->daddr		=	old_iph->saddr;
+	iph->saddr		=	old_iph->daddr;
+
+	ip_send_check(iph);
+
+	/* ICMP */
+	icmph->type		=	ICMP_DEST_UNREACH;
+	icmph->code		=	ICMP_FRAG_NEEDED;
+	icmph->un.gateway	=	htonl(mtu);
+	icmph->checksum		=	0;
+
+	nskb->csum = csum_partial((u8 *)icmph, sizeof(struct icmphdr), 0);
+	nskb->csum = skb_copy_and_csum_bits(skb, (u8 *)old_iph - skb->data,
+					    payload, payload_length,
+					    nskb->csum);
+	icmph->checksum = csum_fold(nskb->csum);
+}
+
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+static bool ipv6_should_icmp(struct sk_buff *skb)
+{
+	struct ipv6hdr *old_ipv6h = ipv6_hdr(skb);
+	int addr_type;
+	int payload_off = (u8 *)(old_ipv6h + 1) - skb->data;
+	u8 nexthdr = ipv6_hdr(skb)->nexthdr;
+
+	/* Check source address is valid. */
+	addr_type = ipv6_addr_type(&old_ipv6h->saddr);
+	if (addr_type & IPV6_ADDR_MULTICAST || addr_type == IPV6_ADDR_ANY)
+		return false;
+
+	/* Don't reply to unspecified addresses. */
+	if (ipv6_addr_type(&old_ipv6h->daddr) == IPV6_ADDR_ANY)
+		return false;
+
+	/* Don't respond to ICMP error messages. */
+	payload_off = ipv6_skip_exthdr(skb, payload_off, &nexthdr);
+	if (payload_off < 0)
+		return false;
+
+	if (nexthdr == NEXTHDR_ICMP) {
+		u8 icmp_type, *icmp_typep;
+
+		icmp_typep = skb_header_pointer(skb, payload_off +
+						offsetof(struct icmp6hdr,
+							icmp6_type),
+						sizeof(icmp_type), &icmp_type);
+
+		if (!icmp_typep || !(*icmp_typep & ICMPV6_INFOMSG_MASK))
+			return false;
+	}
+
+	return true;
+}
+
+static void ipv6_build_icmp(struct sk_buff *skb, struct sk_buff *nskb,
+			    unsigned int mtu, unsigned int payload_length)
+{
+	struct ipv6hdr *ipv6h, *old_ipv6h = ipv6_hdr(skb);
+	struct icmp6hdr *icmp6h;
+	u8 *payload;
+
+	ipv6h = (struct ipv6hdr *)skb_put(nskb, sizeof(struct ipv6hdr));
+	icmp6h = (struct icmp6hdr *)skb_put(nskb, sizeof(struct icmp6hdr));
+	payload = skb_put(nskb, payload_length);
+
+	/* IPv6 */
+	ipv6h->version		=	6;
+	ipv6h->priority		=	0;
+	memset(&ipv6h->flow_lbl, 0, sizeof(ipv6h->flow_lbl));
+	ipv6h->payload_len	=	htons(sizeof(struct icmp6hdr)
+					      + payload_length);
+	ipv6h->nexthdr		=	NEXTHDR_ICMP;
+	ipv6h->hop_limit	=	IPV6_DEFAULT_HOPLIMIT;
+	ipv6h->daddr		=	old_ipv6h->saddr;
+	ipv6h->saddr		=	old_ipv6h->daddr;
+
+	/* ICMPv6 */
+	icmp6h->icmp6_type	=	ICMPV6_PKT_TOOBIG;
+	icmp6h->icmp6_code	=	0;
+	icmp6h->icmp6_cksum	=	0;
+	icmp6h->icmp6_mtu	=	htonl(mtu);
+
+	nskb->csum = csum_partial((u8 *)icmp6h, sizeof(struct icmp6hdr), 0);
+	nskb->csum = skb_copy_and_csum_bits(skb, (u8 *)old_ipv6h - skb->data,
+					    payload, payload_length,
+					    nskb->csum);
+	icmp6h->icmp6_cksum = csum_ipv6_magic(&ipv6h->saddr, &ipv6h->daddr,
+						sizeof(struct icmp6hdr)
+						+ payload_length,
+						ipv6h->nexthdr, nskb->csum);
+}
+#endif /* IPv6 */
+
+bool ovs_tnl_frag_needed(struct vport *vport,
+			 const struct tnl_mutable_config *mutable,
+			 struct sk_buff *skb, unsigned int mtu, __be64 flow_key)
+{
+	unsigned int eth_hdr_len = ETH_HLEN;
+	unsigned int total_length = 0, header_length = 0, payload_length;
+	struct ethhdr *eh, *old_eh = eth_hdr(skb);
+	struct sk_buff *nskb;
+
+	/* Sanity check */
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (mtu < IP_MIN_MTU)
+			return false;
+
+		if (!ipv4_should_icmp(skb))
+			return true;
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (mtu < IPV6_MIN_MTU)
+			return false;
+
+		/*
+		 * In theory we should do PMTUD on IPv6 multicast messages but
+		 * we don't have an address to send from so just fragment.
+		 */
+		if (ipv6_addr_type(&ipv6_hdr(skb)->daddr) & IPV6_ADDR_MULTICAST)
+			return false;
+
+		if (!ipv6_should_icmp(skb))
+			return true;
+	}
+#endif
+	else
+		return false;
+
+	/* Allocate */
+	if (old_eh->h_proto == htons(ETH_P_8021Q))
+		eth_hdr_len = VLAN_ETH_HLEN;
+
+	payload_length = skb->len - eth_hdr_len;
+	if (skb->protocol == htons(ETH_P_IP)) {
+		header_length = sizeof(struct iphdr) + sizeof(struct icmphdr);
+		total_length = min_t(unsigned int, header_length +
+						   payload_length, 576);
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else {
+		header_length = sizeof(struct ipv6hdr) +
+				sizeof(struct icmp6hdr);
+		total_length = min_t(unsigned int, header_length +
+						  payload_length, IPV6_MIN_MTU);
+	}
+#endif
+
+	payload_length = total_length - header_length;
+
+	nskb = dev_alloc_skb(NET_IP_ALIGN + eth_hdr_len + header_length +
+			     payload_length);
+	if (!nskb)
+		return false;
+
+	skb_reserve(nskb, NET_IP_ALIGN);
+
+	/* Ethernet / VLAN */
+	eh = (struct ethhdr *)skb_put(nskb, eth_hdr_len);
+	memcpy(eh->h_dest, old_eh->h_source, ETH_ALEN);
+	memcpy(eh->h_source, mutable->eth_addr, ETH_ALEN);
+	nskb->protocol = eh->h_proto = old_eh->h_proto;
+	if (old_eh->h_proto == htons(ETH_P_8021Q)) {
+		struct vlan_ethhdr *vh = (struct vlan_ethhdr *)eh;
+
+		vh->h_vlan_TCI = vlan_eth_hdr(skb)->h_vlan_TCI;
+		vh->h_vlan_encapsulated_proto = skb->protocol;
+	} else
+		vlan_set_tci(nskb, vlan_get_tci(skb));
+	skb_reset_mac_header(nskb);
+
+	/* Protocol */
+	if (skb->protocol == htons(ETH_P_IP))
+		ipv4_build_icmp(skb, nskb, mtu, payload_length);
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else
+		ipv6_build_icmp(skb, nskb, mtu, payload_length);
+#endif
+
+	/*
+	 * Assume that flow based keys are symmetric with respect to input
+	 * and output and use the key that we were going to put on the
+	 * outgoing packet for the fake received packet.  If the keys are
+	 * not symmetric then PMTUD needs to be disabled since we won't have
+	 * any way of synthesizing packets.
+	 */
+	if ((mutable->flags & (TNL_F_IN_KEY_MATCH | TNL_F_OUT_KEY_ACTION)) ==
+	    (TNL_F_IN_KEY_MATCH | TNL_F_OUT_KEY_ACTION))
+		OVS_CB(nskb)->tun_id = flow_key;
+
+	if (unlikely(compute_ip_summed(nskb, false))) {
+		kfree_skb(nskb);
+		return false;
+	}
+
+	ovs_vport_receive(vport, nskb);
+
+	return true;
+}
+
+static bool check_mtu(struct sk_buff *skb,
+		      struct vport *vport,
+		      const struct tnl_mutable_config *mutable,
+		      const struct rtable *rt, __be16 *frag_offp)
+{
+	bool df_inherit = mutable->flags & TNL_F_DF_INHERIT;
+	bool pmtud = mutable->flags & TNL_F_PMTUD;
+	__be16 frag_off = mutable->flags & TNL_F_DF_DEFAULT ? htons(IP_DF) : 0;
+	int mtu = 0;
+	unsigned int packet_length = skb->len - ETH_HLEN;
+
+	/* Allow for one level of tagging in the packet length. */
+	if (!vlan_tx_tag_present(skb) &&
+	    eth_hdr(skb)->h_proto == htons(ETH_P_8021Q))
+		packet_length -= VLAN_HLEN;
+
+	if (pmtud) {
+		int vlan_header = 0;
+
+		/* The tag needs to go in packet regardless of where it
+		 * currently is, so subtract it from the MTU.
+		 */
+		if (vlan_tx_tag_present(skb) ||
+		    eth_hdr(skb)->h_proto == htons(ETH_P_8021Q))
+			vlan_header = VLAN_HLEN;
+
+		mtu = dst_mtu(&rt_dst(rt))
+			- ETH_HLEN
+			- mutable->tunnel_hlen
+			- vlan_header;
+	}
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		struct iphdr *iph = ip_hdr(skb);
+
+		if (df_inherit)
+			frag_off = iph->frag_off & htons(IP_DF);
+
+		if (pmtud && iph->frag_off & htons(IP_DF)) {
+			mtu = max(mtu, IP_MIN_MTU);
+
+			if (packet_length > mtu &&
+			    ovs_tnl_frag_needed(vport, mutable, skb, mtu,
+						OVS_CB(skb)->tun_id))
+				return false;
+		}
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		/* IPv6 requires end hosts to do fragmentation
+		 * if the packet is above the minimum MTU.
+		 */
+		if (df_inherit && packet_length > IPV6_MIN_MTU)
+			frag_off = htons(IP_DF);
+
+		if (pmtud) {
+			mtu = max(mtu, IPV6_MIN_MTU);
+
+			if (packet_length > mtu &&
+			    ovs_tnl_frag_needed(vport, mutable, skb, mtu,
+						OVS_CB(skb)->tun_id))
+				return false;
+		}
+	}
+#endif
+
+	*frag_offp = frag_off;
+	return true;
+}
+
+static void create_tunnel_header(const struct vport *vport,
+				 const struct tnl_mutable_config *mutable,
+				 const struct rtable *rt, void *header)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct iphdr *iph = header;
+
+	iph->version	= 4;
+	iph->ihl	= sizeof(struct iphdr) >> 2;
+	iph->frag_off	= htons(IP_DF);
+	iph->protocol	= tnl_vport->tnl_ops->ipproto;
+	iph->tos	= mutable->tos;
+	iph->daddr	= rt->rt_dst;
+	iph->saddr	= rt->rt_src;
+	iph->ttl	= mutable->ttl;
+	if (!iph->ttl)
+		iph->ttl = ip4_dst_hoplimit(&rt_dst(rt));
+
+	tnl_vport->tnl_ops->build_header(vport, mutable, iph + 1);
+}
+
+static void *get_cached_header(const struct tnl_cache *cache)
+{
+	return (void *)cache + ALIGN(sizeof(struct tnl_cache), CACHE_DATA_ALIGN);
+}
+
+static bool check_cache_valid(const struct tnl_cache *cache,
+			      const struct tnl_mutable_config *mutable)
+{
+	struct hh_cache *hh;
+
+	if (!cache)
+		return false;
+
+	hh = rt_hh(cache->rt);
+	return hh &&
+#ifdef NEED_CACHE_TIMEOUT
+		time_before(jiffies, cache->expiration) &&
+#endif
+#ifdef HAVE_RT_GENID
+		atomic_read(&init_net.ipv4.rt_genid) == cache->rt->rt_genid &&
+#endif
+#ifdef HAVE_HH_SEQ
+		hh->hh_lock.sequence == cache->hh_seq &&
+#endif
+		mutable->seq == cache->mutable_seq &&
+		(!ovs_is_internal_dev(rt_dst(cache->rt).dev) ||
+		(cache->flow && !cache->flow->dead));
+}
+
+static void __cache_cleaner(struct tnl_vport *tnl_vport)
+{
+	const struct tnl_mutable_config *mutable =
+			rcu_dereference(tnl_vport->mutable);
+	const struct tnl_cache *cache = rcu_dereference(tnl_vport->cache);
+
+	if (cache && !check_cache_valid(cache, mutable) &&
+	    spin_trylock_bh(&tnl_vport->cache_lock)) {
+		assign_cache_rcu(tnl_vport_to_vport(tnl_vport), NULL);
+		spin_unlock_bh(&tnl_vport->cache_lock);
+	}
+}
+
+static void cache_cleaner(struct work_struct *work)
+{
+	int i;
+
+	schedule_cache_cleaner();
+
+	rcu_read_lock();
+	for (i = 0; i < PORT_TABLE_SIZE; i++) {
+		struct hlist_node *n;
+		struct hlist_head *bucket;
+		struct tnl_vport  *tnl_vport;
+
+		bucket = &port_table[i];
+		hlist_for_each_entry_rcu(tnl_vport, n, bucket, hash_node)
+			__cache_cleaner(tnl_vport);
+	}
+	rcu_read_unlock();
+}
+
+static void create_eth_hdr(struct tnl_cache *cache, struct hh_cache *hh)
+{
+	void *cache_data = get_cached_header(cache);
+	int hh_off;
+
+#ifdef HAVE_HH_SEQ
+	unsigned hh_seq;
+
+	do {
+		hh_seq = read_seqbegin(&hh->hh_lock);
+		hh_off = HH_DATA_ALIGN(hh->hh_len) - hh->hh_len;
+		memcpy(cache_data, (void *)hh->hh_data + hh_off, hh->hh_len);
+		cache->hh_len = hh->hh_len;
+	} while (read_seqretry(&hh->hh_lock, hh_seq));
+
+	cache->hh_seq = hh_seq;
+#else
+	read_lock(&hh->hh_lock);
+	hh_off = HH_DATA_ALIGN(hh->hh_len) - hh->hh_len;
+	memcpy(cache_data, (void *)hh->hh_data + hh_off, hh->hh_len);
+	cache->hh_len = hh->hh_len;
+	read_unlock(&hh->hh_lock);
+#endif
+}
+
+static struct tnl_cache *build_cache(struct vport *vport,
+				     const struct tnl_mutable_config *mutable,
+				     struct rtable *rt)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_cache *cache;
+	void *cache_data;
+	int cache_len;
+	struct hh_cache *hh;
+
+	if (!(mutable->flags & TNL_F_HDR_CACHE))
+		return NULL;
+
+	/*
+	 * If there is no entry in the ARP cache or if this device does not
+	 * support hard header caching just fall back to the IP stack.
+	 */
+
+	hh = rt_hh(rt);
+	if (!hh)
+		return NULL;
+
+	/*
+	 * If lock is contended fall back to directly building the header.
+	 * We're not going to help performance by sitting here spinning.
+	 */
+	if (!spin_trylock(&tnl_vport->cache_lock))
+		return NULL;
+
+	cache = cache_dereference(tnl_vport);
+	if (check_cache_valid(cache, mutable))
+		goto unlock;
+	else
+		cache = NULL;
+
+	cache_len = LL_RESERVED_SPACE(rt_dst(rt).dev) + mutable->tunnel_hlen;
+
+	cache = kzalloc(ALIGN(sizeof(struct tnl_cache), CACHE_DATA_ALIGN) +
+			cache_len, GFP_ATOMIC);
+	if (!cache)
+		goto unlock;
+
+	create_eth_hdr(cache, hh);
+	cache_data = get_cached_header(cache) + cache->hh_len;
+	cache->len = cache->hh_len + mutable->tunnel_hlen;
+
+	create_tunnel_header(vport, mutable, rt, cache_data);
+
+	cache->mutable_seq = mutable->seq;
+	cache->rt = rt;
+#ifdef NEED_CACHE_TIMEOUT
+	cache->expiration = jiffies + tnl_vport->cache_exp_interval;
+#endif
+
+	if (ovs_is_internal_dev(rt_dst(rt).dev)) {
+		struct sw_flow_key flow_key;
+		struct vport *dst_vport;
+		struct sk_buff *skb;
+		int err;
+		int flow_key_len;
+		struct sw_flow *flow;
+
+		dst_vport = ovs_internal_dev_get_vport(rt_dst(rt).dev);
+		if (!dst_vport)
+			goto done;
+
+		skb = alloc_skb(cache->len, GFP_ATOMIC);
+		if (!skb)
+			goto done;
+
+		__skb_put(skb, cache->len);
+		memcpy(skb->data, get_cached_header(cache), cache->len);
+
+		err = ovs_flow_extract(skb, dst_vport->port_no, &flow_key,
+				       &flow_key_len);
+
+		consume_skb(skb);
+		if (err)
+			goto done;
+
+		flow = ovs_flow_tbl_lookup(rcu_dereference(dst_vport->dp->table),
+					   &flow_key, flow_key_len);
+		if (flow) {
+			cache->flow = flow;
+			ovs_flow_hold(flow);
+		}
+	}
+
+done:
+	assign_cache_rcu(vport, cache);
+
+unlock:
+	spin_unlock(&tnl_vport->cache_lock);
+
+	return cache;
+}
+
+static struct rtable *__find_route(const struct tnl_mutable_config *mutable,
+				   u8 ipproto, u8 tos)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+	struct flowi fl = { .nl_u = { .ip4_u = {
+					.daddr = mutable->key.daddr,
+					.saddr = mutable->key.saddr,
+					.tos = tos } },
+			    .proto = ipproto };
+	struct rtable *rt;
+
+	if (unlikely(ip_route_output_key(&init_net, &rt, &fl)))
+		return ERR_PTR(-EADDRNOTAVAIL);
+
+	return rt;
+#else
+	struct flowi4 fl = { .daddr = mutable->key.daddr,
+			     .saddr = mutable->key.saddr,
+			     .flowi4_tos = tos,
+			     .flowi4_proto = ipproto };
+
+	return ip_route_output_key(&init_net, &fl);
+#endif
+}
+
+static struct rtable *find_route(struct vport *vport,
+				 const struct tnl_mutable_config *mutable,
+				 u8 tos, struct tnl_cache **cache)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_cache *cur_cache = rcu_dereference(tnl_vport->cache);
+
+	*cache = NULL;
+	tos = RT_TOS(tos);
+
+	if (likely(tos == mutable->tos &&
+	    check_cache_valid(cur_cache, mutable))) {
+		*cache = cur_cache;
+		return cur_cache->rt;
+	} else {
+		struct rtable *rt;
+
+		rt = __find_route(mutable, tnl_vport->tnl_ops->ipproto, tos);
+		if (IS_ERR(rt))
+			return NULL;
+
+		if (likely(tos == mutable->tos))
+			*cache = build_cache(vport, mutable, rt);
+
+		return rt;
+	}
+}
+
+static bool need_linearize(const struct sk_buff *skb)
+{
+	int i;
+
+	if (unlikely(skb_shinfo(skb)->frag_list))
+		return true;
+
+	/*
+	 * Generally speaking we should linearize if there are paged frags.
+	 * However, if all of the refcounts are 1 we know nobody else can
+	 * change them from underneath us and we can skip the linearization.
+	 */
+	for (i = 0; i < skb_shinfo(skb)->nr_frags; i++)
+		if (unlikely(page_count(skb_frag_page(&skb_shinfo(skb)->frags[i])) > 1))
+			return true;
+
+	return false;
+}
+
+static struct sk_buff *handle_offloads(struct sk_buff *skb,
+				       const struct tnl_mutable_config *mutable,
+				       const struct rtable *rt)
+{
+	int min_headroom;
+	int err;
+
+	min_headroom = LL_RESERVED_SPACE(rt_dst(rt).dev) + rt_dst(rt).header_len
+			+ mutable->tunnel_hlen
+			+ (vlan_tx_tag_present(skb) ? VLAN_HLEN : 0);
+
+	if (skb_headroom(skb) < min_headroom || skb_header_cloned(skb)) {
+		int head_delta = SKB_DATA_ALIGN(min_headroom -
+						skb_headroom(skb) +
+						16);
+		err = pskb_expand_head(skb, max_t(int, head_delta, 0),
+					0, GFP_ATOMIC);
+		if (unlikely(err))
+			goto error_free;
+	}
+
+	forward_ip_summed(skb, true);
+
+	if (skb_is_gso(skb)) {
+		struct sk_buff *nskb;
+
+		nskb = skb_gso_segment(skb, 0);
+		if (IS_ERR(nskb)) {
+			kfree_skb(skb);
+			err = PTR_ERR(nskb);
+			goto error;
+		}
+
+		consume_skb(skb);
+		skb = nskb;
+	} else if (get_ip_summed(skb) == OVS_CSUM_PARTIAL) {
+		/* Pages aren't locked and could change at any time.
+		 * If this happens after we compute the checksum, the
+		 * checksum will be wrong.  We linearize now to avoid
+		 * this problem.
+		 */
+		if (unlikely(need_linearize(skb))) {
+			err = __skb_linearize(skb);
+			if (unlikely(err))
+				goto error_free;
+		}
+
+		err = skb_checksum_help(skb);
+		if (unlikely(err))
+			goto error_free;
+	}
+
+	set_ip_summed(skb, OVS_CSUM_NONE);
+
+	return skb;
+
+error_free:
+	kfree_skb(skb);
+error:
+	return ERR_PTR(err);
+}
+
+static int send_frags(struct sk_buff *skb,
+		      const struct tnl_mutable_config *mutable)
+{
+	int sent_len;
+
+	sent_len = 0;
+	while (skb) {
+		struct sk_buff *next = skb->next;
+		int frag_len = skb->len - mutable->tunnel_hlen;
+		int err;
+
+		skb->next = NULL;
+		memset(IPCB(skb), 0, sizeof(*IPCB(skb)));
+
+		err = ip_local_out(skb);
+		skb = next;
+		if (unlikely(net_xmit_eval(err)))
+			goto free_frags;
+		sent_len += frag_len;
+	}
+
+	return sent_len;
+
+free_frags:
+	/*
+	 * There's no point in continuing to send fragments once one has been
+	 * dropped so just free the rest.  This may help improve the congestion
+	 * that caused the first packet to be dropped.
+	 */
+	ovs_tnl_free_linked_skbs(skb);
+	return sent_len;
+}
+
+int ovs_tnl_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable = rcu_dereference(tnl_vport->mutable);
+
+	enum vport_err_type err = VPORT_E_TX_ERROR;
+	struct rtable *rt;
+	struct dst_entry *unattached_dst = NULL;
+	struct tnl_cache *cache;
+	int sent_len = 0;
+	__be16 frag_off = 0;
+	u8 ttl;
+	u8 inner_tos;
+	u8 tos;
+
+	/* Validate the protocol headers before we try to use them. */
+	if (skb->protocol == htons(ETH_P_8021Q) &&
+	    !vlan_tx_tag_present(skb)) {
+		if (unlikely(!pskb_may_pull(skb, VLAN_ETH_HLEN)))
+			goto error_free;
+
+		skb->protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+		skb_set_network_header(skb, VLAN_ETH_HLEN);
+	}
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+		    + sizeof(struct iphdr))))
+			skb->protocol = 0;
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (unlikely(!pskb_may_pull(skb, skb_network_offset(skb)
+		    + sizeof(struct ipv6hdr))))
+			skb->protocol = 0;
+	}
+#endif
+
+	/* ToS */
+	if (skb->protocol == htons(ETH_P_IP))
+		inner_tos = ip_hdr(skb)->tos;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6))
+		inner_tos = ipv6_get_dsfield(ipv6_hdr(skb));
+#endif
+	else
+		inner_tos = 0;
+
+	if (mutable->flags & TNL_F_TOS_INHERIT)
+		tos = inner_tos;
+	else
+		tos = mutable->tos;
+
+	tos = INET_ECN_encapsulate(tos, inner_tos);
+
+	/* Route lookup */
+	rt = find_route(vport, mutable, tos, &cache);
+	if (unlikely(!rt))
+		goto error_free;
+	if (unlikely(!cache))
+		unattached_dst = &rt_dst(rt);
+
+	/* Reset SKB */
+	nf_reset(skb);
+	secpath_reset(skb);
+	skb_dst_drop(skb);
+	skb_clear_rxhash(skb);
+
+	/* Offloading */
+	skb = handle_offloads(skb, mutable, rt);
+	if (IS_ERR(skb))
+		goto error;
+
+	/* MTU */
+	if (unlikely(!check_mtu(skb, vport, mutable, rt, &frag_off))) {
+		err = VPORT_E_TX_DROPPED;
+		goto error_free;
+	}
+
+	/*
+	 * If we are over the MTU, allow the IP stack to handle fragmentation.
+	 * Fragmentation is a slow path anyways.
+	 */
+	if (unlikely(skb->len + mutable->tunnel_hlen > dst_mtu(&rt_dst(rt)) &&
+		     cache)) {
+		unattached_dst = &rt_dst(rt);
+		dst_hold(unattached_dst);
+		cache = NULL;
+	}
+
+	/* TTL */
+	ttl = mutable->ttl;
+	if (!ttl)
+		ttl = ip4_dst_hoplimit(&rt_dst(rt));
+
+	if (mutable->flags & TNL_F_TTL_INHERIT) {
+		if (skb->protocol == htons(ETH_P_IP))
+			ttl = ip_hdr(skb)->ttl;
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+		else if (skb->protocol == htons(ETH_P_IPV6))
+			ttl = ipv6_hdr(skb)->hop_limit;
+#endif
+	}
+
+	while (skb) {
+		struct iphdr *iph;
+		struct sk_buff *next_skb = skb->next;
+		skb->next = NULL;
+
+		if (unlikely(vlan_deaccel_tag(skb)))
+			goto next;
+
+		if (likely(cache)) {
+			skb_push(skb, cache->len);
+			memcpy(skb->data, get_cached_header(cache), cache->len);
+			skb_reset_mac_header(skb);
+			skb_set_network_header(skb, cache->hh_len);
+
+		} else {
+			skb_push(skb, mutable->tunnel_hlen);
+			create_tunnel_header(vport, mutable, rt, skb->data);
+			skb_reset_network_header(skb);
+
+			if (next_skb)
+				skb_dst_set(skb, dst_clone(unattached_dst));
+			else {
+				skb_dst_set(skb, unattached_dst);
+				unattached_dst = NULL;
+			}
+		}
+		skb_set_transport_header(skb, skb_network_offset(skb) + sizeof(struct iphdr));
+
+		iph = ip_hdr(skb);
+		iph->tos = tos;
+		iph->ttl = ttl;
+		iph->frag_off = frag_off;
+		ip_select_ident(iph, &rt_dst(rt), NULL);
+
+		skb = tnl_vport->tnl_ops->update_header(vport, mutable,
+							&rt_dst(rt), skb);
+		if (unlikely(!skb))
+			goto next;
+
+		if (likely(cache)) {
+			int orig_len = skb->len - cache->len;
+			struct vport *cache_vport;
+
+			cache_vport = ovs_internal_dev_get_vport(rt_dst(rt).dev);
+			skb->protocol = htons(ETH_P_IP);
+			iph = ip_hdr(skb);
+			iph->tot_len = htons(skb->len - skb_network_offset(skb));
+			ip_send_check(iph);
+
+			if (cache_vport) {
+				if (unlikely(compute_ip_summed(skb, true))) {
+					kfree_skb(skb);
+					goto next;
+				}
+
+				OVS_CB(skb)->flow = cache->flow;
+				ovs_vport_receive(cache_vport, skb);
+				sent_len += orig_len;
+			} else {
+				int xmit_err;
+
+				skb->dev = rt_dst(rt).dev;
+				xmit_err = dev_queue_xmit(skb);
+
+				if (likely(net_xmit_eval(xmit_err) == 0))
+					sent_len += orig_len;
+			}
+		} else
+			sent_len += send_frags(skb, mutable);
+
+next:
+		skb = next_skb;
+	}
+
+	if (unlikely(sent_len == 0))
+		ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+
+	goto out;
+
+error_free:
+	ovs_tnl_free_linked_skbs(skb);
+error:
+	ovs_vport_record_error(vport, err);
+out:
+	dst_release(unattached_dst);
+	return sent_len;
+}
+
+static const struct nla_policy tnl_policy[OVS_TUNNEL_ATTR_MAX + 1] = {
+	[OVS_TUNNEL_ATTR_FLAGS]    = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_DST_IPV4] = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_SRC_IPV4] = { .type = NLA_U32 },
+	[OVS_TUNNEL_ATTR_OUT_KEY]  = { .type = NLA_U64 },
+	[OVS_TUNNEL_ATTR_IN_KEY]   = { .type = NLA_U64 },
+	[OVS_TUNNEL_ATTR_TOS]      = { .type = NLA_U8 },
+	[OVS_TUNNEL_ATTR_TTL]      = { .type = NLA_U8 },
+};
+
+/* Sets OVS_TUNNEL_ATTR_* fields in 'mutable', which must initially be
+ * zeroed. */
+static int tnl_set_config(struct nlattr *options, const struct tnl_ops *tnl_ops,
+			  const struct vport *cur_vport,
+			  struct tnl_mutable_config *mutable)
+{
+	const struct vport *old_vport;
+	const struct tnl_mutable_config *old_mutable;
+	struct nlattr *a[OVS_TUNNEL_ATTR_MAX + 1];
+	int err;
+
+	if (!options)
+		return -EINVAL;
+
+	err = nla_parse_nested(a, OVS_TUNNEL_ATTR_MAX, options, tnl_policy);
+	if (err)
+		return err;
+
+	if (!a[OVS_TUNNEL_ATTR_FLAGS] || !a[OVS_TUNNEL_ATTR_DST_IPV4])
+		return -EINVAL;
+
+	mutable->flags = nla_get_u32(a[OVS_TUNNEL_ATTR_FLAGS]) & TNL_F_PUBLIC;
+
+	mutable->key.daddr = nla_get_be32(a[OVS_TUNNEL_ATTR_DST_IPV4]);
+	if (a[OVS_TUNNEL_ATTR_SRC_IPV4]) {
+		if (ipv4_is_multicast(mutable->key.daddr))
+			return -EINVAL;
+		mutable->key.saddr = nla_get_be32(a[OVS_TUNNEL_ATTR_SRC_IPV4]);
+	}
+
+	if (a[OVS_TUNNEL_ATTR_TOS]) {
+		mutable->tos = nla_get_u8(a[OVS_TUNNEL_ATTR_TOS]);
+		if (mutable->tos != RT_TOS(mutable->tos))
+			return -EINVAL;
+	}
+
+	if (a[OVS_TUNNEL_ATTR_TTL])
+		mutable->ttl = nla_get_u8(a[OVS_TUNNEL_ATTR_TTL]);
+
+	mutable->key.tunnel_type = tnl_ops->tunnel_type;
+	if (!a[OVS_TUNNEL_ATTR_IN_KEY]) {
+		mutable->key.tunnel_type |= TNL_T_KEY_MATCH;
+		mutable->flags |= TNL_F_IN_KEY_MATCH;
+	} else {
+		mutable->key.tunnel_type |= TNL_T_KEY_EXACT;
+		mutable->key.in_key = nla_get_be64(a[OVS_TUNNEL_ATTR_IN_KEY]);
+	}
+
+	if (!a[OVS_TUNNEL_ATTR_OUT_KEY])
+		mutable->flags |= TNL_F_OUT_KEY_ACTION;
+	else
+		mutable->out_key = nla_get_be64(a[OVS_TUNNEL_ATTR_OUT_KEY]);
+
+	mutable->tunnel_hlen = tnl_ops->hdr_len(mutable);
+	if (mutable->tunnel_hlen < 0)
+		return mutable->tunnel_hlen;
+
+	mutable->tunnel_hlen += sizeof(struct iphdr);
+
+	old_vport = port_table_lookup(&mutable->key, &old_mutable);
+	if (old_vport && old_vport != cur_vport)
+		return -EEXIST;
+
+	mutable->mlink = 0;
+	if (ipv4_is_multicast(mutable->key.daddr)) {
+		struct net_device *dev;
+		struct rtable *rt;
+
+		rt = __find_route(mutable, tnl_ops->ipproto, mutable->tos);
+		if (IS_ERR(rt))
+			return -EADDRNOTAVAIL;
+		dev = rt_dst(rt).dev;
+		ip_rt_put(rt);
+		if (__in_dev_get_rtnl(dev) == NULL)
+			return -EADDRNOTAVAIL;
+		mutable->mlink = dev->ifindex;
+		ip_mc_inc_group(__in_dev_get_rtnl(dev), mutable->key.daddr);
+	}
+
+	return 0;
+}
+
+struct vport *ovs_tnl_create(const struct vport_parms *parms,
+			     const struct vport_ops *vport_ops,
+			     const struct tnl_ops *tnl_ops)
+{
+	struct vport *vport;
+	struct tnl_vport *tnl_vport;
+	struct tnl_mutable_config *mutable;
+	int initial_frag_id;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct tnl_vport), vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	tnl_vport = tnl_vport_priv(vport);
+
+	strcpy(tnl_vport->name, parms->name);
+	tnl_vport->tnl_ops = tnl_ops;
+
+	mutable = kzalloc(sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	random_ether_addr(mutable->eth_addr);
+
+	get_random_bytes(&initial_frag_id, sizeof(int));
+	atomic_set(&tnl_vport->frag_id, initial_frag_id);
+
+	err = tnl_set_config(parms->options, tnl_ops, NULL, mutable);
+	if (err)
+		goto error_free_mutable;
+
+	spin_lock_init(&tnl_vport->cache_lock);
+
+#ifdef NEED_CACHE_TIMEOUT
+	tnl_vport->cache_exp_interval = MAX_CACHE_EXP -
+				       (net_random() % (MAX_CACHE_EXP / 2));
+#endif
+
+	rcu_assign_pointer(tnl_vport->mutable, mutable);
+
+	port_table_add_port(vport);
+	return vport;
+
+error_free_mutable:
+	free_mutable_rtnl(mutable);
+	kfree(mutable);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+int ovs_tnl_set_options(struct vport *vport, struct nlattr *options)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *old_mutable;
+	struct tnl_mutable_config *mutable;
+	int err;
+
+	mutable = kzalloc(sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	/* Copy fields whose values should be retained. */
+	old_mutable = rtnl_dereference(tnl_vport->mutable);
+	mutable->seq = old_mutable->seq + 1;
+	memcpy(mutable->eth_addr, old_mutable->eth_addr, ETH_ALEN);
+
+	/* Parse the others configured by userspace. */
+	err = tnl_set_config(options, tnl_vport->tnl_ops, vport, mutable);
+	if (err)
+		goto error_free;
+
+	if (port_hash(&mutable->key) != port_hash(&old_mutable->key))
+		port_table_move_port(vport, mutable);
+	else
+		assign_config_rcu(vport, mutable);
+
+	return 0;
+
+error_free:
+	free_mutable_rtnl(mutable);
+	kfree(mutable);
+error:
+	return err;
+}
+
+int ovs_tnl_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	const struct tnl_mutable_config *mutable = rcu_dereference_rtnl(tnl_vport->mutable);
+
+	NLA_PUT_U32(skb, OVS_TUNNEL_ATTR_FLAGS, mutable->flags & TNL_F_PUBLIC);
+	NLA_PUT_BE32(skb, OVS_TUNNEL_ATTR_DST_IPV4, mutable->key.daddr);
+
+	if (!(mutable->flags & TNL_F_IN_KEY_MATCH))
+		NLA_PUT_BE64(skb, OVS_TUNNEL_ATTR_IN_KEY, mutable->key.in_key);
+	if (!(mutable->flags & TNL_F_OUT_KEY_ACTION))
+		NLA_PUT_BE64(skb, OVS_TUNNEL_ATTR_OUT_KEY, mutable->out_key);
+	if (mutable->key.saddr)
+		NLA_PUT_BE32(skb, OVS_TUNNEL_ATTR_SRC_IPV4, mutable->key.saddr);
+	if (mutable->tos)
+		NLA_PUT_U8(skb, OVS_TUNNEL_ATTR_TOS, mutable->tos);
+	if (mutable->ttl)
+		NLA_PUT_U8(skb, OVS_TUNNEL_ATTR_TTL, mutable->ttl);
+
+	return 0;
+
+nla_put_failure:
+	return -EMSGSIZE;
+}
+
+static void free_port_rcu(struct rcu_head *rcu)
+{
+	struct tnl_vport *tnl_vport = container_of(rcu,
+						   struct tnl_vport, rcu);
+
+	free_cache((struct tnl_cache __force *)tnl_vport->cache);
+	kfree((struct tnl_mutable __force *)tnl_vport->mutable);
+	ovs_vport_free(tnl_vport_to_vport(tnl_vport));
+}
+
+void ovs_tnl_destroy(struct vport *vport)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *mutable;
+
+	mutable = rtnl_dereference(tnl_vport->mutable);
+	port_table_remove_port(vport);
+	free_mutable_rtnl(mutable);
+	call_rcu(&tnl_vport->rcu, free_port_rcu);
+}
+
+int ovs_tnl_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	struct tnl_mutable_config *old_mutable, *mutable;
+
+	old_mutable = rtnl_dereference(tnl_vport->mutable);
+	mutable = kmemdup(old_mutable, sizeof(struct tnl_mutable_config), GFP_KERNEL);
+	if (!mutable)
+		return -ENOMEM;
+
+	old_mutable->mlink = 0;
+
+	memcpy(mutable->eth_addr, addr, ETH_ALEN);
+	assign_config_rcu(vport, mutable);
+
+	return 0;
+}
+
+const char *ovs_tnl_get_name(const struct vport *vport)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	return tnl_vport->name;
+}
+
+const unsigned char *ovs_tnl_get_addr(const struct vport *vport)
+{
+	const struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	return rcu_dereference_rtnl(tnl_vport->mutable)->eth_addr;
+}
+
+void ovs_tnl_free_linked_skbs(struct sk_buff *skb)
+{
+	while (skb) {
+		struct sk_buff *next = skb->next;
+		kfree_skb(skb);
+		skb = next;
+	}
+}
+
+int ovs_tnl_init(void)
+{
+	int i;
+
+	port_table = kmalloc(PORT_TABLE_SIZE * sizeof(struct hlist_head *),
+			GFP_KERNEL);
+	if (!port_table)
+		return -ENOMEM;
+
+	for (i = 0; i < PORT_TABLE_SIZE; i++)
+		INIT_HLIST_HEAD(&port_table[i]);
+
+	return 0;
+}
+
+void ovs_tnl_exit(void)
+{
+	int i;
+
+	for (i = 0; i < PORT_TABLE_SIZE; i++) {
+		struct tnl_vport *tnl_vport;
+		struct hlist_head *hash_head;
+		struct hlist_node *n;
+
+		hash_head = &port_table[i];
+		hlist_for_each_entry(tnl_vport, n, hash_head, hash_node) {
+			BUG();
+			goto out;
+		}
+	}
+out:
+	kfree(port_table);
+}
diff -r 89e197c6e9d5 net/openvswitch/tunnel.h
--- /dev/null
+++ b/net/openvswitch/tunnel.h
@@ -0,0 +1,273 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef TUNNEL_H
+#define TUNNEL_H 1
+
+#include <linux/version.h>
+
+#include "flow.h"
+#include "openvswitch/tunnel.h"
+#include "vport.h"
+
+/*
+ * The absolute minimum fragment size.  Note that there are many other
+ * definitions of the minimum MTU.
+ */
+#define IP_MIN_MTU 68
+
+/*
+ * One of these goes in struct tnl_ops and in tnl_find_port().
+ * These values are in the same namespace as other TNL_T_* values, so
+ * only the least significant 10 bits are available to define protocol
+ * identifiers.
+ */
+#define TNL_T_PROTO_GRE		0
+#define TNL_T_PROTO_CAPWAP	1
+
+/* These flags are only needed when calling tnl_find_port(). */
+#define TNL_T_KEY_EXACT		(1 << 10)
+#define TNL_T_KEY_MATCH		(1 << 11)
+
+/* Private flags not exposed to userspace in this form. */
+#define TNL_F_IN_KEY_MATCH	(1 << 16) /* Store the key in tun_id to
+					   * match in flow table. */
+#define TNL_F_OUT_KEY_ACTION	(1 << 17) /* Get the key from a SET_TUNNEL
+					   * action. */
+
+/* All public tunnel flags. */
+#define TNL_F_PUBLIC (TNL_F_CSUM | TNL_F_TOS_INHERIT | TNL_F_TTL_INHERIT | \
+		      TNL_F_DF_INHERIT | TNL_F_DF_DEFAULT | TNL_F_PMTUD | \
+		      TNL_F_HDR_CACHE | TNL_F_IPSEC)
+
+/**
+ * struct port_lookup_key - Tunnel port key, used as hash table key.
+ * @in_key: Key to match on input, 0 for wildcard.
+ * @saddr: IPv4 source address to match, 0 to accept any source address.
+ * @daddr: IPv4 destination of tunnel.
+ * @tunnel_type: Set of TNL_T_* flags that define lookup.
+ */
+struct port_lookup_key {
+	__be64 in_key;
+	__be32 saddr;
+	__be32 daddr;
+	u32    tunnel_type;
+};
+
+#define PORT_KEY_LEN	(offsetof(struct port_lookup_key, tunnel_type) + \
+			 FIELD_SIZEOF(struct port_lookup_key, tunnel_type))
+
+/**
+ * struct tnl_mutable_config - modifiable configuration for a tunnel.
+ * @key: Used as key for tunnel port.  Configured via OVS_TUNNEL_ATTR_*
+ * attributes.
+ * @rcu: RCU callback head for deferred destruction.
+ * @seq: Sequence number for distinguishing configuration versions.
+ * @tunnel_hlen: Tunnel header length.
+ * @eth_addr: Source address for packets generated by tunnel itself
+ * (e.g. ICMP fragmentation needed messages).
+ * @out_key: Key to use on output, 0 if this tunnel has no fixed output key.
+ * @flags: TNL_F_* flags.
+ * @tos: IPv4 TOS value to use for tunnel, 0 if no fixed TOS.
+ * @ttl: IPv4 TTL value to use for tunnel, 0 if no fixed TTL.
+ */
+struct tnl_mutable_config {
+	struct port_lookup_key key;
+	struct rcu_head rcu;
+
+	unsigned seq;
+
+	unsigned tunnel_hlen;
+
+	unsigned char eth_addr[ETH_ALEN];
+
+	/* Configured via OVS_TUNNEL_ATTR_* attributes. */
+	__be64	out_key;
+	u32	flags;
+	u8	tos;
+	u8	ttl;
+
+	/* Multicast configuration. */
+	int	mlink;
+};
+
+struct tnl_ops {
+	u32 tunnel_type;	/* Put the TNL_T_PROTO_* type in here. */
+	u8 ipproto;		/* The IP protocol for the tunnel. */
+
+	/*
+	 * Returns the length of the tunnel header that will be added in
+	 * build_header() (i.e. excludes the IP header).  Returns a negative
+	 * error code if the configuration is invalid.
+	 */
+	int (*hdr_len)(const struct tnl_mutable_config *);
+
+	/*
+	 * Builds the static portion of the tunnel header, which is stored in
+	 * the header cache.  In general the performance of this function is
+	 * not too important as we try to only call it when building the cache
+	 * so it is preferable to shift as much work as possible here.  However,
+	 * in some circumstances caching is disabled and this function will be
+	 * called for every packet, so try not to make it too slow.
+	 */
+	void (*build_header)(const struct vport *,
+			     const struct tnl_mutable_config *, void *header);
+
+	/*
+	 * Updates the cached header of a packet to match the actual packet
+	 * data.  Typical things that might need to be updated are length,
+	 * checksum, etc.  The IP header will have already been updated and this
+	 * is the final step before transmission.  Returns a linked list of
+	 * completed SKBs (multiple packets may be generated in the event
+	 * of fragmentation).
+	 */
+	struct sk_buff *(*update_header)(const struct vport *,
+					 const struct tnl_mutable_config *,
+					 struct dst_entry *, struct sk_buff *);
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,20)
+/*
+ * On these kernels we have a fast mechanism to tell if the ARP cache for a
+ * particular destination has changed.
+ */
+#define HAVE_HH_SEQ
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)
+/*
+ * On these kernels we have a fast mechanism to tell if the routing table
+ * has changed.
+ */
+#define HAVE_RT_GENID
+#endif
+#if !defined(HAVE_HH_SEQ) || !defined(HAVE_RT_GENID)
+/* If we can't detect all system changes directly we need to use a timeout. */
+#define NEED_CACHE_TIMEOUT
+#endif
+struct tnl_cache {
+	struct rcu_head rcu;
+
+	int len;		/* Length of data to be memcpy'd from cache. */
+	int hh_len;		/* Hardware hdr length, cached from hh_cache. */
+
+	/* Sequence number of mutable->seq from which this cache was
+	 * generated. */
+	unsigned mutable_seq;
+
+#ifdef HAVE_HH_SEQ
+	/*
+	 * The sequence number from the seqlock protecting the hardware header
+	 * cache (in the ARP cache).  Since every write increments the counter
+	 * this gives us an easy way to tell if it has changed.
+	 */
+	unsigned hh_seq;
+#endif
+
+#ifdef NEED_CACHE_TIMEOUT
+	/*
+	 * If we don't have direct mechanisms to detect all important changes in
+	 * the system fall back to an expiration time.  This expiration time
+	 * can be relatively short since at high rates there will be millions of
+	 * packets per second, so we'll still get plenty of benefit from the
+	 * cache.  Note that if something changes we may blackhole packets
+	 * until the expiration time (depending on what changed and the kernel
+	 * version we may be able to detect the change sooner).  Expiration is
+	 * expressed as a time in jiffies.
+	 */
+	unsigned long expiration;
+#endif
+
+	/*
+	 * The routing table entry that is the result of looking up the tunnel
+	 * endpoints.  It also contains a sequence number (called a generation
+	 * ID) that can be compared to a global sequence to tell if the routing
+	 * table has changed (and therefore there is a potential that this
+	 * cached route has been invalidated).
+	 */
+	struct rtable *rt;
+
+	/*
+	 * If the output device for tunnel traffic is an OVS internal device,
+	 * the flow of that datapath.  Since all tunnel traffic will have the
+	 * same headers this allows us to cache the flow lookup.  NULL if the
+	 * output device is not OVS or if there is no flow installed.
+	 */
+	struct sw_flow *flow;
+
+	/* The cached header follows after padding for alignment. */
+};
+
+struct tnl_vport {
+	struct rcu_head rcu;
+	struct hlist_node hash_node;
+
+	char name[IFNAMSIZ];
+	const struct tnl_ops *tnl_ops;
+
+	struct tnl_mutable_config __rcu *mutable;
+
+	/*
+	 * ID of last fragment sent (for tunnel protocols with direct support
+	 * fragmentation).  If the protocol relies on IP fragmentation then
+	 * this is not needed.
+	 */
+	atomic_t frag_id;
+
+	spinlock_t cache_lock;
+	struct tnl_cache __rcu *cache;	/* Protected by RCU/cache_lock. */
+
+#ifdef NEED_CACHE_TIMEOUT
+	/*
+	 * If we must rely on expiration time to invalidate the cache, this is
+	 * the interval.  It is randomized within a range (defined by
+	 * MAX_CACHE_EXP in tunnel.c) to avoid synchronized expirations caused
+	 * by creation of a large number of tunnels at a one time.
+	 */
+	unsigned long cache_exp_interval;
+#endif
+};
+
+struct vport *ovs_tnl_create(const struct vport_parms *, const struct vport_ops *,
+			     const struct tnl_ops *);
+void ovs_tnl_destroy(struct vport *);
+
+int ovs_tnl_set_options(struct vport *, struct nlattr *);
+int ovs_tnl_get_options(const struct vport *, struct sk_buff *);
+
+int ovs_tnl_set_addr(struct vport *vport, const unsigned char *addr);
+const char *ovs_tnl_get_name(const struct vport *vport);
+const unsigned char *ovs_tnl_get_addr(const struct vport *vport);
+int ovs_tnl_send(struct vport *vport, struct sk_buff *skb);
+void ovs_tnl_rcv(struct vport *vport, struct sk_buff *skb, u8 tos);
+
+struct vport *ovs_tnl_find_port(__be32 saddr, __be32 daddr, __be64 key,
+				int tunnel_type,
+				const struct tnl_mutable_config **mutable);
+bool ovs_tnl_frag_needed(struct vport *vport,
+			 const struct tnl_mutable_config *mutable,
+			 struct sk_buff *skb, unsigned int mtu, __be64 flow_key);
+void ovs_tnl_free_linked_skbs(struct sk_buff *skb);
+
+int ovs_tnl_init(void);
+void ovs_tnl_exit(void);
+static inline struct tnl_vport *tnl_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+#endif /* tunnel.h */
diff -r 89e197c6e9d5 net/openvswitch/vlan.c
--- /dev/null
+++ b/net/openvswitch/vlan.c
@@ -0,0 +1,58 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if_vlan.h>
+#include <linux/skbuff.h>
+
+#include "datapath.h"
+#include "vlan.h"
+
+#ifdef NEED_VLAN_FIELD
+void vlan_copy_skb_tci(struct sk_buff *skb)
+{
+	OVS_CB(skb)->vlan_tci = 0;
+}
+
+u16 vlan_get_tci(struct sk_buff *skb)
+{
+	return OVS_CB(skb)->vlan_tci;
+}
+
+void vlan_set_tci(struct sk_buff *skb, u16 vlan_tci)
+{
+	OVS_CB(skb)->vlan_tci = vlan_tci;
+}
+
+bool vlan_tx_tag_present(struct sk_buff *skb)
+{
+	return OVS_CB(skb)->vlan_tci & VLAN_TAG_PRESENT;
+}
+
+u16 vlan_tx_tag_get(struct sk_buff *skb)
+{
+	return OVS_CB(skb)->vlan_tci & ~VLAN_TAG_PRESENT;
+}
+
+struct sk_buff *__vlan_hwaccel_put_tag(struct sk_buff *skb, u16 vlan_tci)
+{
+	OVS_CB(skb)->vlan_tci = vlan_tci | VLAN_TAG_PRESENT;
+	return skb;
+}
+#endif /* NEED_VLAN_FIELD */
diff -r 89e197c6e9d5 net/openvswitch/vlan.h
--- /dev/null
+++ b/net/openvswitch/vlan.h
@@ -0,0 +1,100 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VLAN_H
+#define VLAN_H 1
+
+#include <linux/if_vlan.h>
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+/**
+ * DOC: VLAN tag manipulation.
+ *
+ * &struct sk_buff handling of VLAN tags has evolved over time:
+ *
+ * In 2.6.26 and earlier, VLAN tags did not have any generic representation in
+ * an skb, other than as a raw 802.1Q header inside the packet data.
+ *
+ * In 2.6.27 &struct sk_buff added a @vlan_tci member.  Between 2.6.27 and
+ * 2.6.32, its value was the raw contents of the 802.1Q TCI field, or zero if
+ * no 802.1Q header was present.  This worked OK except for the corner case of
+ * an 802.1Q header with an all-0-bits TCI, which could not be represented.
+ *
+ * In 2.6.33, @vlan_tci semantics changed.  Now, if an 802.1Q header is
+ * present, then the VLAN_TAG_PRESENT bit is always set.  This fixes the
+ * all-0-bits TCI corner case.
+ *
+ * For compatibility we emulate the 2.6.33+ behavior on earlier kernel
+ * versions.  The client must not access @vlan_tci directly.  Instead, use
+ * vlan_get_tci() to read it or vlan_set_tci() to write it, with semantics
+ * equivalent to those on 2.6.33+.
+ */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,27)
+#define NEED_VLAN_FIELD
+#endif
+
+#ifndef NEED_VLAN_FIELD
+static inline void vlan_copy_skb_tci(struct sk_buff *skb) { }
+
+static inline u16 vlan_get_tci(struct sk_buff *skb)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	if (skb->vlan_tci)
+		return skb->vlan_tci | VLAN_TAG_PRESENT;
+#endif
+	return skb->vlan_tci;
+}
+
+static inline void vlan_set_tci(struct sk_buff *skb, u16 vlan_tci)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,33)
+	vlan_tci &= ~VLAN_TAG_PRESENT;
+#endif
+	skb->vlan_tci = vlan_tci;
+}
+#else
+void vlan_copy_skb_tci(struct sk_buff *skb);
+u16 vlan_get_tci(struct sk_buff *skb);
+void vlan_set_tci(struct sk_buff *skb, u16 vlan_tci);
+
+#undef vlan_tx_tag_present
+bool vlan_tx_tag_present(struct sk_buff *skb);
+
+#undef vlan_tx_tag_get
+u16 vlan_tx_tag_get(struct sk_buff *skb);
+
+#define __vlan_hwaccel_put_tag rpl__vlan_hwaccel_put_tag
+struct sk_buff *__vlan_hwaccel_put_tag(struct sk_buff *skb, u16 vlan_tci);
+#endif /* NEED_VLAN_FIELD */
+
+static inline int vlan_deaccel_tag(struct sk_buff *skb)
+{
+	if (!vlan_tx_tag_present(skb))
+		return 0;
+
+	skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+	if (unlikely(!skb))
+		return -ENOMEM;
+
+	vlan_set_tci(skb, 0);
+	return 0;
+}
+
+#endif /* vlan.h */
diff -r 89e197c6e9d5 net/openvswitch/vport-capwap.c
--- /dev/null
+++ b/net/openvswitch/vport-capwap.c
@@ -0,0 +1,807 @@
+/*
+ * Copyright (c) 2010, 2011 Nicira Networks.
+ * Distributed under the terms of the GNU GPL version 2.
+ *
+ * Significant portions of this file may be copied from parts of the Linux
+ * kernel, by Linus Torvalds and others.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/version.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+
+#include <linux/if.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/list.h>
+#include <linux/net.h>
+
+#include <net/icmp.h>
+#include <net/inet_frag.h>
+#include <net/ip.h>
+#include <net/protocol.h>
+#include <net/udp.h>
+
+#include "tunnel.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+#define CAPWAP_SRC_PORT 58881
+#define CAPWAP_DST_PORT 58882
+
+#define CAPWAP_FRAG_TIMEOUT (30 * HZ)
+#define CAPWAP_FRAG_MAX_MEM (256 * 1024)
+#define CAPWAP_FRAG_PRUNE_MEM (192 * 1024)
+#define CAPWAP_FRAG_SECRET_INTERVAL (10 * 60 * HZ)
+
+/*
+ * The CAPWAP header is a mess, with all kinds of odd size bit fields that
+ * cross byte boundaries, which are difficult to represent correctly in
+ * various byte orderings.  Luckily we only care about a few permutations, so
+ * statically create them and we can do very fast parsing by checking all 12
+ * fields in one go.
+ */
+#define CAPWAP_PREAMBLE_MASK __cpu_to_be32(0xFF000000)
+#define CAPWAP_HLEN_SHIFT    17
+#define CAPWAP_HLEN_MASK     __cpu_to_be32(0x00F80000)
+#define CAPWAP_RID_MASK      __cpu_to_be32(0x0007C000)
+#define CAPWAP_WBID_MASK     __cpu_to_be32(0x00003E00)
+#define CAPWAP_F_MASK        __cpu_to_be32(0x000001FF)
+
+#define CAPWAP_F_FRAG        __cpu_to_be32(0x00000080)
+#define CAPWAP_F_LASTFRAG    __cpu_to_be32(0x00000040)
+#define CAPWAP_F_WSI         __cpu_to_be32(0x00000020)
+#define CAPWAP_F_RMAC        __cpu_to_be32(0x00000010)
+
+#define CAPWAP_RMAC_LEN      4
+
+/*  Standard CAPWAP looks for a WBID value of 2.
+ *  When we insert WSI field, use WBID value of 30, which has been
+ *  proposed for all "experimental" usage - users with no reserved WBID value
+ *  of their own.
+*/
+#define CAPWAP_WBID_30   __cpu_to_be32(0x00003C00)
+#define CAPWAP_WBID_2    __cpu_to_be32(0x00000200)
+
+#define FRAG_HDR (CAPWAP_F_FRAG)
+#define FRAG_LAST_HDR (FRAG_HDR | CAPWAP_F_LASTFRAG)
+
+/* Keyed packet, WBID 30, and length long enough to include WSI key */
+#define CAPWAP_KEYED (CAPWAP_WBID_30 | CAPWAP_F_WSI | htonl(20 << CAPWAP_HLEN_SHIFT))
+/* A backward-compatible packet, WBID 2 and length of 2 words (no WSI fields) */
+#define CAPWAP_NO_WSI (CAPWAP_WBID_2 | htonl(8 << CAPWAP_HLEN_SHIFT))
+
+/* Mask for all parts of header that must be 0. */
+#define CAPWAP_ZERO_MASK (CAPWAP_PREAMBLE_MASK | \
+		(CAPWAP_F_MASK ^ (CAPWAP_F_WSI | CAPWAP_F_FRAG | CAPWAP_F_LASTFRAG | CAPWAP_F_RMAC)))
+
+struct capwaphdr {
+	__be32 begin;
+	__be16 frag_id;
+	/* low 3 bits of frag_off are reserved */
+	__be16 frag_off;
+};
+
+/*
+ * We use the WSI field to hold additional tunnel data.
+ * The first eight bits store the size of the wsi data in bytes.
+ */
+struct capwaphdr_wsi {
+	u8 wsi_len;
+	u8 flags;
+	__be16 reserved_padding;
+};
+
+struct capwaphdr_wsi_key {
+	__be64 key;
+};
+
+/* Flag indicating a 64bit key is stored in WSI data field */
+#define CAPWAP_WSI_F_KEY64 0x80
+
+static struct capwaphdr *capwap_hdr(const struct sk_buff *skb)
+{
+	return (struct capwaphdr *)(udp_hdr(skb) + 1);
+}
+
+/*
+ * The fragment offset is actually the high 13 bits of the last 16 bit field,
+ * so we would normally need to right shift 3 places.  However, it stores the
+ * offset in 8 byte chunks, which would involve a 3 place left shift.  So we
+ * just mask off the last 3 bits and be done with it.
+ */
+#define FRAG_OFF_MASK (~0x7U)
+
+/*
+ * The minimum header length.  The header may be longer if the optional
+ * WSI field is used.
+ */
+#define CAPWAP_MIN_HLEN (sizeof(struct udphdr) + sizeof(struct capwaphdr))
+
+struct frag_match {
+	__be32 saddr;
+	__be32 daddr;
+	__be16 id;
+};
+
+struct frag_queue {
+	struct inet_frag_queue ifq;
+	struct frag_match match;
+};
+
+struct frag_skb_cb {
+	u16 offset;
+};
+#define FRAG_CB(skb) ((struct frag_skb_cb *)(skb)->cb)
+
+static struct sk_buff *fragment(struct sk_buff *, const struct vport *,
+				struct dst_entry *dst, unsigned int hlen);
+static void defrag_init(void);
+static void defrag_exit(void);
+static struct sk_buff *defrag(struct sk_buff *, bool frag_last);
+
+static void capwap_frag_init(struct inet_frag_queue *, void *match);
+static unsigned int capwap_frag_hash(struct inet_frag_queue *);
+static int capwap_frag_match(struct inet_frag_queue *, void *match);
+static void capwap_frag_expire(unsigned long ifq);
+
+static struct inet_frags frag_state = {
+	.constructor	= capwap_frag_init,
+	.qsize		= sizeof(struct frag_queue),
+	.hashfn		= capwap_frag_hash,
+	.match		= capwap_frag_match,
+	.frag_expire	= capwap_frag_expire,
+	.secret_interval = CAPWAP_FRAG_SECRET_INTERVAL,
+};
+static struct netns_frags frag_netns_state = {
+	.timeout	= CAPWAP_FRAG_TIMEOUT,
+	.high_thresh	= CAPWAP_FRAG_MAX_MEM,
+	.low_thresh	= CAPWAP_FRAG_PRUNE_MEM,
+};
+
+static struct socket *capwap_rcv_socket;
+
+static int capwap_hdr_len(const struct tnl_mutable_config *mutable)
+{
+	int size = CAPWAP_MIN_HLEN;
+
+	/* CAPWAP has no checksums. */
+	if (mutable->flags & TNL_F_CSUM)
+		return -EINVAL;
+
+	/* if keys are specified, then add WSI field */
+	if (mutable->out_key || (mutable->flags & TNL_F_OUT_KEY_ACTION)) {
+		size += sizeof(struct capwaphdr_wsi) +
+			sizeof(struct capwaphdr_wsi_key);
+	}
+
+	return size;
+}
+
+static void capwap_build_header(const struct vport *vport,
+				const struct tnl_mutable_config *mutable,
+				void *header)
+{
+	struct udphdr *udph = header;
+	struct capwaphdr *cwh = (struct capwaphdr *)(udph + 1);
+
+	udph->source = htons(CAPWAP_SRC_PORT);
+	udph->dest = htons(CAPWAP_DST_PORT);
+	udph->check = 0;
+
+	cwh->frag_id = 0;
+	cwh->frag_off = 0;
+
+	if (mutable->out_key || (mutable->flags & TNL_F_OUT_KEY_ACTION)) {
+		struct capwaphdr_wsi *wsi = (struct capwaphdr_wsi *)(cwh + 1);
+
+		cwh->begin = CAPWAP_KEYED;
+
+		/* -1 for wsi_len byte, not included in length as per spec */
+		wsi->wsi_len = sizeof(struct capwaphdr_wsi) - 1
+			+ sizeof(struct capwaphdr_wsi_key);
+		wsi->flags = CAPWAP_WSI_F_KEY64;
+		wsi->reserved_padding = 0;
+
+		if (mutable->out_key) {
+			struct capwaphdr_wsi_key *opt = (struct capwaphdr_wsi_key *)(wsi + 1);
+			opt->key = mutable->out_key;
+		}
+	} else {
+		/* make packet readable by old capwap code */
+		cwh->begin = CAPWAP_NO_WSI;
+	}
+}
+
+static struct sk_buff *capwap_update_header(const struct vport *vport,
+					    const struct tnl_mutable_config *mutable,
+					    struct dst_entry *dst,
+					    struct sk_buff *skb)
+{
+	struct udphdr *udph = udp_hdr(skb);
+
+	if (mutable->flags & TNL_F_OUT_KEY_ACTION) {
+		/* first field in WSI is key */
+		struct capwaphdr *cwh = (struct capwaphdr *)(udph + 1);
+		struct capwaphdr_wsi *wsi = (struct capwaphdr_wsi *)(cwh + 1);
+		struct capwaphdr_wsi_key *opt = (struct capwaphdr_wsi_key *)(wsi + 1);
+
+		opt->key = OVS_CB(skb)->tun_id;
+	}
+
+	udph->len = htons(skb->len - skb_transport_offset(skb));
+
+	if (unlikely(skb->len - skb_network_offset(skb) > dst_mtu(dst))) {
+		unsigned int hlen = skb_transport_offset(skb) + capwap_hdr_len(mutable);
+		skb = fragment(skb, vport, dst, hlen);
+	}
+
+	return skb;
+}
+
+static int process_capwap_wsi(struct sk_buff *skb, __be64 *key)
+{
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	struct capwaphdr_wsi *wsi;
+	int hdr_len;
+	int rmac_len = 0;
+	int wsi_len;
+
+	if (((cwh->begin & CAPWAP_WBID_MASK) != CAPWAP_WBID_30))
+		return 0;
+
+	if (cwh->begin & CAPWAP_F_RMAC)
+		rmac_len = CAPWAP_RMAC_LEN;
+
+	hdr_len = ntohl(cwh->begin & CAPWAP_HLEN_MASK) >> CAPWAP_HLEN_SHIFT;
+
+	if (unlikely(sizeof(struct capwaphdr) + rmac_len + sizeof(struct capwaphdr_wsi) > hdr_len))
+		return -EINVAL;
+
+	/* read wsi header to find out how big it really is */
+	wsi = (struct capwaphdr_wsi *)((u8 *)(cwh + 1) + rmac_len);
+	/* +1 for length byte not included in wsi_len */
+	wsi_len = 1 + wsi->wsi_len;
+
+	if (unlikely(sizeof(struct capwaphdr) + rmac_len + wsi_len != hdr_len))
+		return -EINVAL;
+
+	wsi_len -= sizeof(struct capwaphdr_wsi);
+
+	if (wsi->flags & CAPWAP_WSI_F_KEY64) {
+		struct capwaphdr_wsi_key *opt;
+
+		if (unlikely(wsi_len < sizeof(struct capwaphdr_wsi_key)))
+			return -EINVAL;
+
+		opt = (struct capwaphdr_wsi_key *)(wsi + 1);
+		*key = opt->key;
+	}
+
+	return 0;
+}
+
+static struct sk_buff *process_capwap_proto(struct sk_buff *skb, __be64 *key)
+{
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	int hdr_len = sizeof(struct udphdr);
+
+	if (unlikely((cwh->begin & CAPWAP_ZERO_MASK) != 0))
+		goto error;
+
+	hdr_len += ntohl(cwh->begin & CAPWAP_HLEN_MASK) >> CAPWAP_HLEN_SHIFT;
+	if (unlikely(hdr_len < CAPWAP_MIN_HLEN))
+		goto error;
+
+	if (unlikely(!pskb_may_pull(skb, hdr_len + ETH_HLEN)))
+		goto error;
+
+	cwh = capwap_hdr(skb);
+	__skb_pull(skb, hdr_len);
+	skb_postpull_rcsum(skb, skb_transport_header(skb), hdr_len + ETH_HLEN);
+
+	if (cwh->begin & CAPWAP_F_FRAG) {
+		skb = defrag(skb, (__force bool)(cwh->begin & CAPWAP_F_LASTFRAG));
+		if (!skb)
+			return NULL;
+		cwh = capwap_hdr(skb);
+	}
+
+	if ((cwh->begin & CAPWAP_F_WSI) && process_capwap_wsi(skb, key))
+		goto error;
+
+	return skb;
+error:
+	kfree_skb(skb);
+	return NULL;
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static int capwap_rcv(struct sock *sk, struct sk_buff *skb)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	struct iphdr *iph;
+	__be64 key = 0;
+
+	if (unlikely(!pskb_may_pull(skb, CAPWAP_MIN_HLEN + ETH_HLEN)))
+		goto error;
+
+	skb = process_capwap_proto(skb, &key);
+	if (unlikely(!skb))
+		goto out;
+
+	iph = ip_hdr(skb);
+	vport = ovs_tnl_find_port(iph->daddr, iph->saddr, key, TNL_T_PROTO_CAPWAP,
+				  &mutable);
+	if (unlikely(!vport)) {
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
+		goto error;
+	}
+
+	if (mutable->flags & TNL_F_IN_KEY_MATCH)
+		OVS_CB(skb)->tun_id = key;
+	else
+		OVS_CB(skb)->tun_id = 0;
+
+	ovs_tnl_rcv(vport, skb, iph->tos);
+	goto out;
+
+error:
+	kfree_skb(skb);
+out:
+	return 0;
+}
+
+static const struct tnl_ops capwap_tnl_ops = {
+	.tunnel_type	= TNL_T_PROTO_CAPWAP,
+	.ipproto	= IPPROTO_UDP,
+	.hdr_len	= capwap_hdr_len,
+	.build_header	= capwap_build_header,
+	.update_header	= capwap_update_header,
+};
+
+static struct vport *capwap_create(const struct vport_parms *parms)
+{
+	return ovs_tnl_create(parms, &ovs_capwap_vport_ops, &capwap_tnl_ops);
+}
+
+/* Random value.  Irrelevant as long as it's not 0 since we set the handler. */
+#define UDP_ENCAP_CAPWAP 10
+static int capwap_init(void)
+{
+	int err;
+	struct sockaddr_in sin;
+
+	err = sock_create(AF_INET, SOCK_DGRAM, 0, &capwap_rcv_socket);
+	if (err)
+		goto error;
+
+	sin.sin_family = AF_INET;
+	sin.sin_addr.s_addr = htonl(INADDR_ANY);
+	sin.sin_port = htons(CAPWAP_DST_PORT);
+
+	err = kernel_bind(capwap_rcv_socket, (struct sockaddr *)&sin,
+			  sizeof(struct sockaddr_in));
+	if (err)
+		goto error_sock;
+
+	udp_sk(capwap_rcv_socket->sk)->encap_type = UDP_ENCAP_CAPWAP;
+	udp_sk(capwap_rcv_socket->sk)->encap_rcv = capwap_rcv;
+
+	defrag_init();
+
+	return 0;
+
+error_sock:
+	sock_release(capwap_rcv_socket);
+error:
+	pr_warn("cannot register capwap protocol handler\n");
+	return err;
+}
+
+static void capwap_exit(void)
+{
+	defrag_exit();
+	sock_release(capwap_rcv_socket);
+}
+
+static void copy_skb_metadata(struct sk_buff *from, struct sk_buff *to)
+{
+	to->pkt_type = from->pkt_type;
+	to->priority = from->priority;
+	to->protocol = from->protocol;
+	skb_dst_set(to, dst_clone(skb_dst(from)));
+	to->dev = from->dev;
+	to->mark = from->mark;
+
+	if (from->sk)
+		skb_set_owner_w(to, from->sk);
+
+#ifdef CONFIG_NET_SCHED
+	to->tc_index = from->tc_index;
+#endif
+#if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)
+	to->ipvs_property = from->ipvs_property;
+#endif
+	skb_copy_secmark(to, from);
+}
+
+static struct sk_buff *fragment(struct sk_buff *skb, const struct vport *vport,
+				struct dst_entry *dst, unsigned int hlen)
+{
+	struct tnl_vport *tnl_vport = tnl_vport_priv(vport);
+	unsigned int headroom;
+	unsigned int max_frame_len = dst_mtu(dst) + skb_network_offset(skb);
+	struct sk_buff *result = NULL, *list_cur = NULL;
+	unsigned int remaining;
+	unsigned int offset;
+	__be16 frag_id;
+
+	if (hlen + ~FRAG_OFF_MASK + 1 > max_frame_len) {
+		if (net_ratelimit())
+			pr_warn("capwap link mtu (%d) is less than minimum packet (%d)\n",
+				dst_mtu(dst),
+				hlen - skb_network_offset(skb) + ~FRAG_OFF_MASK + 1);
+		goto error;
+	}
+
+	remaining = skb->len - hlen;
+	offset = 0;
+	frag_id = htons(atomic_inc_return(&tnl_vport->frag_id));
+
+	headroom = dst->header_len + 16;
+	if (!skb_network_offset(skb))
+		headroom += LL_RESERVED_SPACE(dst->dev);
+
+	while (remaining) {
+		struct sk_buff *skb2;
+		int frag_size;
+		struct udphdr *udph;
+		struct capwaphdr *cwh;
+
+		frag_size = min(remaining, max_frame_len - hlen);
+		if (remaining > frag_size)
+			frag_size &= FRAG_OFF_MASK;
+
+		skb2 = alloc_skb(headroom + hlen + frag_size, GFP_ATOMIC);
+		if (!skb2)
+			goto error;
+
+		skb_reserve(skb2, headroom);
+		__skb_put(skb2, hlen + frag_size);
+
+		if (skb_network_offset(skb))
+			skb_reset_mac_header(skb2);
+		skb_set_network_header(skb2, skb_network_offset(skb));
+		skb_set_transport_header(skb2, skb_transport_offset(skb));
+
+		/* Copy (Ethernet)/IP/UDP/CAPWAP header. */
+		copy_skb_metadata(skb, skb2);
+		skb_copy_from_linear_data(skb, skb2->data, hlen);
+
+		/* Copy this data chunk. */
+		if (skb_copy_bits(skb, hlen + offset, skb2->data + hlen, frag_size))
+			BUG();
+
+		udph = udp_hdr(skb2);
+		udph->len = htons(skb2->len - skb_transport_offset(skb2));
+
+		cwh = capwap_hdr(skb2);
+		if (remaining > frag_size)
+			cwh->begin |= FRAG_HDR;
+		else
+			cwh->begin |= FRAG_LAST_HDR;
+		cwh->frag_id = frag_id;
+		cwh->frag_off = htons(offset);
+
+		if (result) {
+			list_cur->next = skb2;
+			list_cur = skb2;
+		} else
+			result = list_cur = skb2;
+
+		offset += frag_size;
+		remaining -= frag_size;
+	}
+
+	consume_skb(skb);
+	return result;
+
+error:
+	ovs_tnl_free_linked_skbs(result);
+	kfree_skb(skb);
+	return NULL;
+}
+
+/* All of the following functions relate to fragmentation reassembly. */
+
+static struct frag_queue *ifq_cast(struct inet_frag_queue *ifq)
+{
+	return container_of(ifq, struct frag_queue, ifq);
+}
+
+static u32 frag_hash(struct frag_match *match)
+{
+	return jhash_3words((__force u16)match->id, (__force u32)match->saddr,
+			    (__force u32)match->daddr,
+			    frag_state.rnd) & (INETFRAGS_HASHSZ - 1);
+}
+
+static struct frag_queue *queue_find(struct frag_match *match)
+{
+	struct inet_frag_queue *ifq;
+
+	read_lock(&frag_state.lock);
+
+	ifq = inet_frag_find(&frag_netns_state, &frag_state, match, frag_hash(match));
+	if (!ifq)
+		return NULL;
+
+	/* Unlock happens inside inet_frag_find(). */
+
+	return ifq_cast(ifq);
+}
+
+static struct sk_buff *frag_reasm(struct frag_queue *fq, struct net_device *dev)
+{
+	struct sk_buff *head = fq->ifq.fragments;
+	struct sk_buff *frag;
+
+	/* Succeed or fail, we're done with this queue. */
+	inet_frag_kill(&fq->ifq, &frag_state);
+
+	if (fq->ifq.len > 65535)
+		return NULL;
+
+	/* Can't have the head be a clone. */
+	if (skb_cloned(head) && pskb_expand_head(head, 0, 0, GFP_ATOMIC))
+		return NULL;
+
+	/*
+	 * We're about to build frag list for this SKB.  If it already has a
+	 * frag list, alloc a new SKB and put the existing frag list there.
+	 */
+	if (skb_shinfo(head)->frag_list) {
+		int i;
+		int paged_len = 0;
+
+		frag = alloc_skb(0, GFP_ATOMIC);
+		if (!frag)
+			return NULL;
+
+		frag->next = head->next;
+		head->next = frag;
+		skb_shinfo(frag)->frag_list = skb_shinfo(head)->frag_list;
+		skb_shinfo(head)->frag_list = NULL;
+
+		for (i = 0; i < skb_shinfo(head)->nr_frags; i++)
+			paged_len += skb_shinfo(head)->frags[i].size;
+		frag->len = frag->data_len = head->data_len - paged_len;
+		head->data_len -= frag->len;
+		head->len -= frag->len;
+
+		frag->ip_summed = head->ip_summed;
+		atomic_add(frag->truesize, &fq->ifq.net->mem);
+	}
+
+	skb_shinfo(head)->frag_list = head->next;
+	atomic_sub(head->truesize, &fq->ifq.net->mem);
+
+	/* Properly account for data in various packets. */
+	for (frag = head->next; frag; frag = frag->next) {
+		head->data_len += frag->len;
+		head->len += frag->len;
+
+		if (head->ip_summed != frag->ip_summed)
+			head->ip_summed = CHECKSUM_NONE;
+		else if (head->ip_summed == CHECKSUM_COMPLETE)
+			head->csum = csum_add(head->csum, frag->csum);
+
+		head->truesize += frag->truesize;
+		atomic_sub(frag->truesize, &fq->ifq.net->mem);
+	}
+
+	head->next = NULL;
+	head->dev = dev;
+	head->tstamp = fq->ifq.stamp;
+	fq->ifq.fragments = NULL;
+
+	return head;
+}
+
+static struct sk_buff *frag_queue(struct frag_queue *fq, struct sk_buff *skb,
+				  u16 offset, bool frag_last)
+{
+	struct sk_buff *prev, *next;
+	struct net_device *dev;
+	int end;
+
+	if (fq->ifq.last_in & INET_FRAG_COMPLETE)
+		goto error;
+
+	if (!skb->len)
+		goto error;
+
+	end = offset + skb->len;
+
+	if (frag_last) {
+		/*
+		 * Last fragment, shouldn't already have data past our end or
+		 * have another last fragment.
+		 */
+		if (end < fq->ifq.len || fq->ifq.last_in & INET_FRAG_LAST_IN)
+			goto error;
+
+		fq->ifq.last_in |= INET_FRAG_LAST_IN;
+		fq->ifq.len = end;
+	} else {
+		/* Fragments should align to 8 byte chunks. */
+		if (end & ~FRAG_OFF_MASK)
+			goto error;
+
+		if (end > fq->ifq.len) {
+			/*
+			 * Shouldn't have data past the end, if we already
+			 * have one.
+			 */
+			if (fq->ifq.last_in & INET_FRAG_LAST_IN)
+				goto error;
+
+			fq->ifq.len = end;
+		}
+	}
+
+	/* Find where we fit in. */
+	prev = NULL;
+	for (next = fq->ifq.fragments; next != NULL; next = next->next) {
+		if (FRAG_CB(next)->offset >= offset)
+			break;
+		prev = next;
+	}
+
+	/*
+	 * Overlapping fragments aren't allowed.  We shouldn't start before
+	 * the end of the previous fragment.
+	 */
+	if (prev && FRAG_CB(prev)->offset + prev->len > offset)
+		goto error;
+
+	/* We also shouldn't end after the beginning of the next fragment. */
+	if (next && end > FRAG_CB(next)->offset)
+		goto error;
+
+	FRAG_CB(skb)->offset = offset;
+
+	/* Link into list. */
+	skb->next = next;
+	if (prev)
+		prev->next = skb;
+	else
+		fq->ifq.fragments = skb;
+
+	dev = skb->dev;
+	skb->dev = NULL;
+
+	fq->ifq.stamp = skb->tstamp;
+	fq->ifq.meat += skb->len;
+	atomic_add(skb->truesize, &fq->ifq.net->mem);
+	if (offset == 0)
+		fq->ifq.last_in |= INET_FRAG_FIRST_IN;
+
+	/* If we have all fragments do reassembly. */
+	if (fq->ifq.last_in == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&
+	    fq->ifq.meat == fq->ifq.len)
+		return frag_reasm(fq, dev);
+
+	write_lock(&frag_state.lock);
+	list_move_tail(&fq->ifq.lru_list, &fq->ifq.net->lru_list);
+	write_unlock(&frag_state.lock);
+
+	return NULL;
+
+error:
+	kfree_skb(skb);
+	return NULL;
+}
+
+static struct sk_buff *defrag(struct sk_buff *skb, bool frag_last)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	struct capwaphdr *cwh = capwap_hdr(skb);
+	struct frag_match match;
+	u16 frag_off;
+	struct frag_queue *fq;
+
+	if (atomic_read(&frag_netns_state.mem) > frag_netns_state.high_thresh)
+		inet_frag_evictor(&frag_netns_state, &frag_state);
+
+	match.daddr = iph->daddr;
+	match.saddr = iph->saddr;
+	match.id = cwh->frag_id;
+	frag_off = ntohs(cwh->frag_off) & FRAG_OFF_MASK;
+
+	fq = queue_find(&match);
+	if (fq) {
+		spin_lock(&fq->ifq.lock);
+		skb = frag_queue(fq, skb, frag_off, frag_last);
+		spin_unlock(&fq->ifq.lock);
+
+		inet_frag_put(&fq->ifq, &frag_state);
+
+		return skb;
+	}
+
+	kfree_skb(skb);
+	return NULL;
+}
+
+static void defrag_init(void)
+{
+	inet_frags_init(&frag_state);
+	inet_frags_init_net(&frag_netns_state);
+}
+
+static void defrag_exit(void)
+{
+	inet_frags_exit_net(&frag_netns_state, &frag_state);
+	inet_frags_fini(&frag_state);
+}
+
+static void capwap_frag_init(struct inet_frag_queue *ifq, void *match_)
+{
+	struct frag_match *match = match_;
+
+	ifq_cast(ifq)->match = *match;
+}
+
+static unsigned int capwap_frag_hash(struct inet_frag_queue *ifq)
+{
+	return frag_hash(&ifq_cast(ifq)->match);
+}
+
+static int capwap_frag_match(struct inet_frag_queue *ifq, void *a_)
+{
+	struct frag_match *a = a_;
+	struct frag_match *b = &ifq_cast(ifq)->match;
+
+	return a->id == b->id && a->saddr == b->saddr && a->daddr == b->daddr;
+}
+
+/* Run when the timeout for a given queue expires. */
+static void capwap_frag_expire(unsigned long ifq)
+{
+	struct frag_queue *fq;
+
+	fq = ifq_cast((struct inet_frag_queue *)ifq);
+
+	spin_lock(&fq->ifq.lock);
+
+	if (!(fq->ifq.last_in & INET_FRAG_COMPLETE))
+		inet_frag_kill(&fq->ifq, &frag_state);
+
+	spin_unlock(&fq->ifq.lock);
+	inet_frag_put(&fq->ifq, &frag_state);
+}
+
+const struct vport_ops ovs_capwap_vport_ops = {
+	.type		= OVS_VPORT_TYPE_CAPWAP,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= capwap_init,
+	.exit		= capwap_exit,
+	.create		= capwap_create,
+	.destroy	= ovs_tnl_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
+#else
+#warning CAPWAP tunneling will not be available on kernels before 2.6.26
+#endif /* Linux kernel < 2.6.26 */
diff -r 89e197c6e9d5 net/openvswitch/vport-generic.c
--- /dev/null
+++ b/net/openvswitch/vport-generic.c
@@ -0,0 +1,36 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/etherdevice.h>
+
+#include "vport-generic.h"
+
+unsigned ovs_vport_gen_get_dev_flags(const struct vport *vport)
+{
+	return IFF_UP | IFF_RUNNING | IFF_LOWER_UP;
+}
+
+int ovs_vport_gen_is_running(const struct vport *vport)
+{
+	return 1;
+}
+
+unsigned char ovs_vport_gen_get_operstate(const struct vport *vport)
+{
+	return IF_OPER_UP;
+}
diff -r 89e197c6e9d5 net/openvswitch/vport-generic.h
--- /dev/null
+++ b/net/openvswitch/vport-generic.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_GENERIC_H
+#define VPORT_GENERIC_H 1
+
+#include "vport.h"
+
+unsigned ovs_vport_gen_get_dev_flags(const struct vport *);
+int ovs_vport_gen_is_running(const struct vport *);
+unsigned char ovs_vport_gen_get_operstate(const struct vport *);
+
+#endif /* vport-generic.h */
diff -r 89e197c6e9d5 net/openvswitch/vport-gre.c
--- /dev/null
+++ b/net/openvswitch/vport-gre.c
@@ -0,0 +1,419 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if.h>
+#include <linux/skbuff.h>
+#include <linux/ip.h>
+#include <linux/if_tunnel.h>
+#include <linux/if_vlan.h>
+#include <linux/in.h>
+
+#include <net/icmp.h>
+#include <net/ip.h>
+#include <net/protocol.h>
+
+#include "tunnel.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+/*
+ * The GRE header is composed of a series of sections: a base and then a variable
+ * number of options.
+ */
+#define GRE_HEADER_SECTION 4
+
+struct gre_base_hdr {
+	__be16 flags;
+	__be16 protocol;
+};
+
+static int gre_hdr_len(const struct tnl_mutable_config *mutable)
+{
+	int len;
+
+	len = GRE_HEADER_SECTION;
+
+	if (mutable->flags & TNL_F_CSUM)
+		len += GRE_HEADER_SECTION;
+
+	if (mutable->out_key || mutable->flags & TNL_F_OUT_KEY_ACTION)
+		len += GRE_HEADER_SECTION;
+
+	return len;
+}
+
+/* Returns the least-significant 32 bits of a __be64. */
+static __be32 be64_get_low32(__be64 x)
+{
+#ifdef __BIG_ENDIAN
+	return (__force __be32)x;
+#else
+	return (__force __be32)((__force u64)x >> 32);
+#endif
+}
+
+static void gre_build_header(const struct vport *vport,
+			     const struct tnl_mutable_config *mutable,
+			     void *header)
+{
+	struct gre_base_hdr *greh = header;
+	__be32 *options = (__be32 *)(greh + 1);
+
+	greh->protocol = htons(ETH_P_TEB);
+	greh->flags = 0;
+
+	if (mutable->flags & TNL_F_CSUM) {
+		greh->flags |= GRE_CSUM;
+		*options = 0;
+		options++;
+	}
+
+	if (mutable->out_key || mutable->flags & TNL_F_OUT_KEY_ACTION)
+		greh->flags |= GRE_KEY;
+
+	if (mutable->out_key)
+		*options = be64_get_low32(mutable->out_key);
+}
+
+static struct sk_buff *gre_update_header(const struct vport *vport,
+					 const struct tnl_mutable_config *mutable,
+					 struct dst_entry *dst,
+					 struct sk_buff *skb)
+{
+	__be32 *options = (__be32 *)(skb_network_header(skb) + mutable->tunnel_hlen
+					       - GRE_HEADER_SECTION);
+
+	/* Work backwards over the options so the checksum is last. */
+	if (mutable->flags & TNL_F_OUT_KEY_ACTION)
+		*options = be64_get_low32(OVS_CB(skb)->tun_id);
+
+	if (mutable->out_key || mutable->flags & TNL_F_OUT_KEY_ACTION)
+		options--;
+
+	if (mutable->flags & TNL_F_CSUM)
+		*(__sum16 *)options = csum_fold(skb_checksum(skb,
+						skb_transport_offset(skb),
+						skb->len - skb_transport_offset(skb),
+						0));
+	/*
+	 * Allow our local IP stack to fragment the outer packet even if the
+	 * DF bit is set as a last resort.  We also need to force selection of
+	 * an IP ID here because Linux will otherwise leave it at 0 if the
+	 * packet originally had DF set.
+	 */
+	skb->local_df = 1;
+	__ip_select_ident(ip_hdr(skb), dst, 0);
+
+	return skb;
+}
+
+/* Zero-extends a __be32 into the least-significant 32 bits of a __be64. */
+static __be64 be32_extend_to_be64(__be32 x)
+{
+#ifdef __BIG_ENDIAN
+	return (__force __be64)x;
+#else
+	return (__force __be64)((__force u64)x << 32);
+#endif
+}
+
+static int parse_header(struct iphdr *iph, __be16 *flags, __be64 *key)
+{
+	/* IP and ICMP protocol handlers check that the IHL is valid. */
+	struct gre_base_hdr *greh = (struct gre_base_hdr *)((u8 *)iph + (iph->ihl << 2));
+	__be32 *options = (__be32 *)(greh + 1);
+	int hdr_len;
+
+	*flags = greh->flags;
+
+	if (unlikely(greh->flags & (GRE_VERSION | GRE_ROUTING)))
+		return -EINVAL;
+
+	if (unlikely(greh->protocol != htons(ETH_P_TEB)))
+		return -EINVAL;
+
+	hdr_len = GRE_HEADER_SECTION;
+
+	if (greh->flags & GRE_CSUM) {
+		hdr_len += GRE_HEADER_SECTION;
+		options++;
+	}
+
+	if (greh->flags & GRE_KEY) {
+		hdr_len += GRE_HEADER_SECTION;
+
+		*key = be32_extend_to_be64(*options);
+		options++;
+	} else
+		*key = 0;
+
+	if (unlikely(greh->flags & GRE_SEQ))
+		hdr_len += GRE_HEADER_SECTION;
+
+	return hdr_len;
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static void gre_err(struct sk_buff *skb, u32 info)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	const int type = icmp_hdr(skb)->type;
+	const int code = icmp_hdr(skb)->code;
+	int mtu = ntohs(icmp_hdr(skb)->un.frag.mtu);
+
+	struct iphdr *iph;
+	__be16 flags;
+	__be64 key;
+	int tunnel_hdr_len, tot_hdr_len;
+	unsigned int orig_mac_header;
+	unsigned int orig_nw_header;
+
+	if (type != ICMP_DEST_UNREACH || code != ICMP_FRAG_NEEDED)
+		return;
+
+	/*
+	 * The mimimum size packet that we would actually be able to process:
+	 * encapsulating IP header, minimum GRE header, Ethernet header,
+	 * inner IPv4 header.
+	 */
+	if (!pskb_may_pull(skb, sizeof(struct iphdr) + GRE_HEADER_SECTION +
+				ETH_HLEN + sizeof(struct iphdr)))
+		return;
+
+	iph = (struct iphdr *)skb->data;
+	if (ipv4_is_multicast(iph->daddr))
+		return;
+
+	tunnel_hdr_len = parse_header(iph, &flags, &key);
+	if (tunnel_hdr_len < 0)
+		return;
+
+	vport = ovs_tnl_find_port(iph->saddr, iph->daddr, key, TNL_T_PROTO_GRE,
+				  &mutable);
+	if (!vport)
+		return;
+
+	/*
+	 * Packets received by this function were previously sent by us, so
+	 * any comparisons should be to the output values, not the input.
+	 * However, it's not really worth it to have a hash table based on
+	 * output keys (especially since ICMP error handling of tunneled packets
+	 * isn't that reliable anyways).  Therefore, we do a lookup based on the
+	 * out key as if it were the in key and then check to see if the input
+	 * and output keys are the same.
+	 */
+	if (mutable->key.in_key != mutable->out_key)
+		return;
+
+	if (!!(mutable->flags & TNL_F_IN_KEY_MATCH) !=
+	    !!(mutable->flags & TNL_F_OUT_KEY_ACTION))
+		return;
+
+	if ((mutable->flags & TNL_F_CSUM) && !(flags & GRE_CSUM))
+		return;
+
+	tunnel_hdr_len += iph->ihl << 2;
+
+	orig_mac_header = skb_mac_header(skb) - skb->data;
+	orig_nw_header = skb_network_header(skb) - skb->data;
+	skb_set_mac_header(skb, tunnel_hdr_len);
+
+	tot_hdr_len = tunnel_hdr_len + ETH_HLEN;
+
+	skb->protocol = eth_hdr(skb)->h_proto;
+	if (skb->protocol == htons(ETH_P_8021Q)) {
+		tot_hdr_len += VLAN_HLEN;
+		skb->protocol = vlan_eth_hdr(skb)->h_vlan_encapsulated_proto;
+	}
+
+	skb_set_network_header(skb, tot_hdr_len);
+	mtu -= tot_hdr_len;
+
+	if (skb->protocol == htons(ETH_P_IP))
+		tot_hdr_len += sizeof(struct iphdr);
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6))
+		tot_hdr_len += sizeof(struct ipv6hdr);
+#endif
+	else
+		goto out;
+
+	if (!pskb_may_pull(skb, tot_hdr_len))
+		goto out;
+
+	if (skb->protocol == htons(ETH_P_IP)) {
+		if (mtu < IP_MIN_MTU) {
+			if (ntohs(ip_hdr(skb)->tot_len) >= IP_MIN_MTU)
+				mtu = IP_MIN_MTU;
+			else
+				goto out;
+		}
+
+	}
+#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
+	else if (skb->protocol == htons(ETH_P_IPV6)) {
+		if (mtu < IPV6_MIN_MTU) {
+			unsigned int packet_length = sizeof(struct ipv6hdr) +
+					      ntohs(ipv6_hdr(skb)->payload_len);
+
+			if (packet_length >= IPV6_MIN_MTU
+			    || ntohs(ipv6_hdr(skb)->payload_len) == 0)
+				mtu = IPV6_MIN_MTU;
+			else
+				goto out;
+		}
+	}
+#endif
+
+	__skb_pull(skb, tunnel_hdr_len);
+	ovs_tnl_frag_needed(vport, mutable, skb, mtu, key);
+	__skb_push(skb, tunnel_hdr_len);
+
+out:
+	skb_set_mac_header(skb, orig_mac_header);
+	skb_set_network_header(skb, orig_nw_header);
+	skb->protocol = htons(ETH_P_IP);
+}
+
+static bool check_checksum(struct sk_buff *skb)
+{
+	struct iphdr *iph = ip_hdr(skb);
+	struct gre_base_hdr *greh = (struct gre_base_hdr *)(iph + 1);
+	__sum16 csum = 0;
+
+	if (greh->flags & GRE_CSUM) {
+		switch (skb->ip_summed) {
+		case CHECKSUM_COMPLETE:
+			csum = csum_fold(skb->csum);
+
+			if (!csum)
+				break;
+			/* Fall through. */
+
+		case CHECKSUM_NONE:
+			skb->csum = 0;
+			csum = __skb_checksum_complete(skb);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+			break;
+		}
+	}
+
+	return (csum == 0);
+}
+
+/* Called with rcu_read_lock and BH disabled. */
+static int gre_rcv(struct sk_buff *skb)
+{
+	struct vport *vport;
+	const struct tnl_mutable_config *mutable;
+	int hdr_len;
+	struct iphdr *iph;
+	__be16 flags;
+	__be64 key;
+
+	if (unlikely(!pskb_may_pull(skb, sizeof(struct gre_base_hdr) + ETH_HLEN)))
+		goto error;
+
+	if (unlikely(!check_checksum(skb)))
+		goto error;
+
+	hdr_len = parse_header(ip_hdr(skb), &flags, &key);
+	if (unlikely(hdr_len < 0))
+		goto error;
+
+	if (unlikely(!pskb_may_pull(skb, hdr_len + ETH_HLEN)))
+		goto error;
+
+	iph = ip_hdr(skb);
+	vport = ovs_tnl_find_port(iph->daddr, iph->saddr, key, TNL_T_PROTO_GRE,
+				  &mutable);
+	if (unlikely(!vport)) {
+		icmp_send(skb, ICMP_DEST_UNREACH, ICMP_PORT_UNREACH, 0);
+		goto error;
+	}
+
+	if (mutable->flags & TNL_F_IN_KEY_MATCH)
+		OVS_CB(skb)->tun_id = key;
+	else
+		OVS_CB(skb)->tun_id = 0;
+
+	__skb_pull(skb, hdr_len);
+	skb_postpull_rcsum(skb, skb_transport_header(skb), hdr_len + ETH_HLEN);
+
+	ovs_tnl_rcv(vport, skb, iph->tos);
+	return 0;
+
+error:
+	kfree_skb(skb);
+	return 0;
+}
+
+static const struct tnl_ops gre_tnl_ops = {
+	.tunnel_type	= TNL_T_PROTO_GRE,
+	.ipproto	= IPPROTO_GRE,
+	.hdr_len	= gre_hdr_len,
+	.build_header	= gre_build_header,
+	.update_header	= gre_update_header,
+};
+
+static struct vport *gre_create(const struct vport_parms *parms)
+{
+	return ovs_tnl_create(parms, &ovs_gre_vport_ops, &gre_tnl_ops);
+}
+
+static const struct net_protocol gre_protocol_handlers = {
+	.handler	=	gre_rcv,
+	.err_handler	=	gre_err,
+};
+
+static int gre_init(void)
+{
+	int err;
+
+	err = inet_add_protocol(&gre_protocol_handlers, IPPROTO_GRE);
+	if (err)
+		pr_warn("cannot register gre protocol handler\n");
+
+	return err;
+}
+
+static void gre_exit(void)
+{
+	inet_del_protocol(&gre_protocol_handlers, IPPROTO_GRE);
+}
+
+const struct vport_ops ovs_gre_vport_ops = {
+	.type		= OVS_VPORT_TYPE_GRE,
+	.flags		= VPORT_F_TUN_ID,
+	.init		= gre_init,
+	.exit		= gre_exit,
+	.create		= gre_create,
+	.destroy	= ovs_tnl_destroy,
+	.set_addr	= ovs_tnl_set_addr,
+	.get_name	= ovs_tnl_get_name,
+	.get_addr	= ovs_tnl_get_addr,
+	.get_options	= ovs_tnl_get_options,
+	.set_options	= ovs_tnl_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= ovs_tnl_send,
+};
diff -r 89e197c6e9d5 net/openvswitch/vport-internal_dev.c
--- /dev/null
+++ b/net/openvswitch/vport-internal_dev.c
@@ -0,0 +1,330 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/hardirq.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/skbuff.h>
+#include <linux/version.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport-generic.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3,1,0)
+#define HAVE_NET_DEVICE_OPS
+#endif
+
+struct internal_dev {
+	struct vport *vport;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	struct net_device_stats stats;
+#endif
+};
+
+static struct internal_dev *internal_dev_priv(struct net_device *netdev)
+{
+	return netdev_priv(netdev);
+}
+
+/* This function is only called by the kernel network layer.*/
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+static struct rtnl_link_stats64 *internal_dev_get_stats(struct net_device *netdev,
+							struct rtnl_link_stats64 *stats)
+{
+#else
+static struct net_device_stats *internal_dev_sys_stats(struct net_device *netdev)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,22)
+	struct net_device_stats *stats = &internal_dev_priv(netdev)->stats;
+#else
+	struct net_device_stats *stats = &netdev->stats;
+#endif
+#endif
+	struct vport *vport = ovs_internal_dev_get_vport(netdev);
+	struct ovs_vport_stats vport_stats;
+
+	ovs_vport_get_stats(vport, &vport_stats);
+
+	/* The tx and rx stats need to be swapped because the
+	 * switch and host OS have opposite perspectives. */
+	stats->rx_packets	= vport_stats.tx_packets;
+	stats->tx_packets	= vport_stats.rx_packets;
+	stats->rx_bytes		= vport_stats.tx_bytes;
+	stats->tx_bytes		= vport_stats.rx_bytes;
+	stats->rx_errors	= vport_stats.tx_errors;
+	stats->tx_errors	= vport_stats.rx_errors;
+	stats->rx_dropped	= vport_stats.tx_dropped;
+	stats->tx_dropped	= vport_stats.rx_dropped;
+
+	return stats;
+}
+
+static int internal_dev_mac_addr(struct net_device *dev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	if (!is_valid_ether_addr(addr->sa_data))
+		return -EADDRNOTAVAIL;
+	memcpy(dev->dev_addr, addr->sa_data, dev->addr_len);
+	return 0;
+}
+
+/* Called with rcu_read_lock_bh. */
+static int internal_dev_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+	if (unlikely(compute_ip_summed(skb, true))) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	vlan_copy_skb_tci(skb);
+	OVS_CB(skb)->flow = NULL;
+
+	rcu_read_lock();
+	ovs_vport_receive(internal_dev_priv(netdev)->vport, skb);
+	rcu_read_unlock();
+	return 0;
+}
+
+static int internal_dev_open(struct net_device *netdev)
+{
+	netif_start_queue(netdev);
+	return 0;
+}
+
+static int internal_dev_stop(struct net_device *netdev)
+{
+	netif_stop_queue(netdev);
+	return 0;
+}
+
+static void internal_dev_getinfo(struct net_device *netdev,
+				 struct ethtool_drvinfo *info)
+{
+	strcpy(info->driver, "openvswitch");
+}
+
+static const struct ethtool_ops internal_dev_ethtool_ops = {
+	.get_drvinfo	= internal_dev_getinfo,
+	.get_link	= ethtool_op_get_link,
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,39)
+	.get_sg		= ethtool_op_get_sg,
+	.set_sg		= ethtool_op_set_sg,
+	.get_tx_csum	= ethtool_op_get_tx_csum,
+	.set_tx_csum	= ethtool_op_set_tx_hw_csum,
+	.get_tso	= ethtool_op_get_tso,
+	.set_tso	= ethtool_op_set_tso,
+#endif
+};
+
+static int internal_dev_change_mtu(struct net_device *netdev, int new_mtu)
+{
+	if (new_mtu < 68)
+		return -EINVAL;
+
+	netdev->mtu = new_mtu;
+	return 0;
+}
+
+static int internal_dev_do_ioctl(struct net_device *dev,
+				 struct ifreq *ifr, int cmd)
+{
+	if (ovs_dp_ioctl_hook)
+		return ovs_dp_ioctl_hook(dev, ifr, cmd);
+
+	return -EOPNOTSUPP;
+}
+
+static void internal_dev_destructor(struct net_device *dev)
+{
+	struct vport *vport = ovs_internal_dev_get_vport(dev);
+
+	ovs_vport_free(vport);
+	free_netdev(dev);
+}
+
+#ifdef HAVE_NET_DEVICE_OPS
+static const struct net_device_ops internal_dev_netdev_ops = {
+	.ndo_open = internal_dev_open,
+	.ndo_stop = internal_dev_stop,
+	.ndo_start_xmit = internal_dev_xmit,
+	.ndo_set_mac_address = internal_dev_mac_addr,
+	.ndo_do_ioctl = internal_dev_do_ioctl,
+	.ndo_change_mtu = internal_dev_change_mtu,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+	.ndo_get_stats64 = internal_dev_get_stats,
+#else
+	.ndo_get_stats = internal_dev_sys_stats,
+#endif
+};
+#endif
+
+static void do_setup(struct net_device *netdev)
+{
+	ether_setup(netdev);
+
+#ifdef HAVE_NET_DEVICE_OPS
+	netdev->netdev_ops = &internal_dev_netdev_ops;
+#else
+	netdev->do_ioctl = internal_dev_do_ioctl;
+	netdev->get_stats = internal_dev_sys_stats;
+	netdev->hard_start_xmit = internal_dev_xmit;
+	netdev->open = internal_dev_open;
+	netdev->stop = internal_dev_stop;
+	netdev->set_mac_address = internal_dev_mac_addr;
+	netdev->change_mtu = internal_dev_change_mtu;
+#endif
+
+	netdev->priv_flags &= ~IFF_TX_SKB_SHARING;
+	netdev->destructor = internal_dev_destructor;
+	SET_ETHTOOL_OPS(netdev, &internal_dev_ethtool_ops);
+	netdev->tx_queue_len = 0;
+
+	netdev->features = NETIF_F_LLTX | NETIF_F_SG | NETIF_F_FRAGLIST |
+				NETIF_F_HIGHDMA | NETIF_F_HW_CSUM | NETIF_F_TSO;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,27)
+	netdev->vlan_features = netdev->features;
+	netdev->features |= NETIF_F_HW_VLAN_TX;
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+	netdev->hw_features = netdev->features & ~NETIF_F_LLTX;
+#endif
+	random_ether_addr(netdev->dev_addr);
+}
+
+static struct vport *internal_dev_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct netdev_vport *netdev_vport;
+	struct internal_dev *internal_dev;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct netdev_vport),
+				&ovs_internal_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev = alloc_netdev(sizeof(struct internal_dev),
+					 parms->name, do_setup);
+	if (!netdev_vport->dev) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	internal_dev = internal_dev_priv(netdev_vport->dev);
+	internal_dev->vport = vport;
+
+	err = register_netdevice(netdev_vport->dev);
+	if (err)
+		goto error_free_netdev;
+
+	dev_set_promiscuity(netdev_vport->dev, 1);
+	netif_start_queue(netdev_vport->dev);
+
+	return vport;
+
+error_free_netdev:
+	free_netdev(netdev_vport->dev);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void internal_dev_destroy(struct vport *vport)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+
+	netif_stop_queue(netdev_vport->dev);
+	dev_set_promiscuity(netdev_vport->dev, -1);
+
+	/* unregister_netdevice() waits for an RCU grace period. */
+	unregister_netdevice(netdev_vport->dev);
+}
+
+static int internal_dev_recv(struct vport *vport, struct sk_buff *skb)
+{
+	struct net_device *netdev = netdev_vport_priv(vport)->dev;
+	int len;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37)
+	if (unlikely(vlan_deaccel_tag(skb)))
+		return 0;
+#endif
+
+	len = skb->len;
+	skb->dev = netdev;
+	skb->pkt_type = PACKET_HOST;
+	skb->protocol = eth_type_trans(skb, netdev);
+	forward_ip_summed(skb, false);
+
+	netif_rx(skb);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,29)
+	netdev->last_rx = jiffies;
+#endif
+
+	return len;
+}
+
+const struct vport_ops ovs_internal_vport_ops = {
+	.type		= OVS_VPORT_TYPE_INTERNAL,
+	.flags		= VPORT_F_REQUIRED | VPORT_F_FLOW,
+	.create		= internal_dev_create,
+	.destroy	= internal_dev_destroy,
+	.set_addr	= ovs_netdev_set_addr,
+	.get_name	= ovs_netdev_get_name,
+	.get_addr	= ovs_netdev_get_addr,
+	.get_kobj	= ovs_netdev_get_kobj,
+	.get_dev_flags	= ovs_netdev_get_dev_flags,
+	.is_running	= ovs_netdev_is_running,
+	.get_operstate	= ovs_netdev_get_operstate,
+	.get_ifindex	= ovs_netdev_get_ifindex,
+	.get_mtu	= ovs_netdev_get_mtu,
+	.send		= internal_dev_recv,
+};
+
+int ovs_is_internal_dev(const struct net_device *netdev)
+{
+#ifdef HAVE_NET_DEVICE_OPS
+	return netdev->netdev_ops == &internal_dev_netdev_ops;
+#else
+	return netdev->open == internal_dev_open;
+#endif
+}
+
+struct vport *ovs_internal_dev_get_vport(struct net_device *netdev)
+{
+	if (!ovs_is_internal_dev(netdev))
+		return NULL;
+
+	return internal_dev_priv(netdev)->vport;
+}
diff -r 89e197c6e9d5 net/openvswitch/vport-internal_dev.h
--- /dev/null
+++ b/net/openvswitch/vport-internal_dev.h
@@ -0,0 +1,28 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_INTERNAL_DEV_H
+#define VPORT_INTERNAL_DEV_H 1
+
+#include "datapath.h"
+#include "vport.h"
+
+int ovs_is_internal_dev(const struct net_device *);
+struct vport *ovs_internal_dev_get_vport(struct net_device *);
+
+#endif /* vport-internal_dev.h */
diff -r 89e197c6e9d5 net/openvswitch/vport-netdev.c
--- /dev/null
+++ b/net/openvswitch/vport-netdev.c
@@ -0,0 +1,429 @@
+/*
+ * Copyright (c) 2007-2012 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/if_arp.h>
+#include <linux/if_bridge.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/llc.h>
+#include <linux/rtnetlink.h>
+#include <linux/skbuff.h>
+
+#include <net/llc.h>
+
+#include "checksum.h"
+#include "datapath.h"
+#include "vlan.h"
+#include "vport-internal_dev.h"
+#include "vport-netdev.h"
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,37) && \
+	!defined(HAVE_VLAN_BUG_WORKAROUND)
+#include <linux/module.h>
+
+static int vlan_tso __read_mostly;
+module_param(vlan_tso, int, 0644);
+MODULE_PARM_DESC(vlan_tso, "Enable TSO for VLAN packets");
+#else
+#define vlan_tso true
+#endif
+
+static void netdev_port_receive(struct vport *vport, struct sk_buff *skb);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,39)
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static rx_handler_result_t netdev_frame_hook(struct sk_buff **pskb)
+{
+	struct sk_buff *skb = *pskb;
+	struct vport *vport;
+
+	if (unlikely(skb->pkt_type == PACKET_LOOPBACK))
+		return RX_HANDLER_PASS;
+
+	vport = ovs_netdev_get_vport(skb->dev);
+
+	netdev_port_receive(vport, skb);
+
+	return RX_HANDLER_CONSUMED;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static struct sk_buff *netdev_frame_hook(struct sk_buff *skb)
+{
+	struct vport *vport;
+
+	if (unlikely(skb->pkt_type == PACKET_LOOPBACK))
+		return skb;
+
+	vport = ovs_netdev_get_vport(skb->dev);
+
+	netdev_port_receive(vport, skb);
+
+	return NULL;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,22)
+/*
+ * Used as br_handle_frame_hook.  (Cannot run bridge at the same time, even on
+ * different set of devices!)
+ */
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static struct sk_buff *netdev_frame_hook(struct net_bridge_port *p,
+					 struct sk_buff *skb)
+{
+	netdev_port_receive((struct vport *)p, skb);
+	return NULL;
+}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+/*
+ * Used as br_handle_frame_hook.  (Cannot run bridge at the same time, even on
+ * different set of devices!)
+ */
+/* Called with rcu_read_lock and bottom-halves disabled. */
+static int netdev_frame_hook(struct net_bridge_port *p, struct sk_buff **pskb)
+{
+	netdev_port_receive((struct vport *)p, *pskb);
+	return 1;
+}
+#else
+#error
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+static int netdev_init(void) { return 0; }
+static void netdev_exit(void) { }
+#else
+static int netdev_init(void)
+{
+	/* Hook into callback used by the bridge to intercept packets.
+	 * Parasites we are. */
+	br_handle_frame_hook = netdev_frame_hook;
+
+	return 0;
+}
+
+static void netdev_exit(void)
+{
+	br_handle_frame_hook = NULL;
+}
+#endif
+
+static struct vport *netdev_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct netdev_vport *netdev_vport;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct netdev_vport),
+				&ovs_netdev_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev = dev_get_by_name(&init_net, parms->name);
+	if (!netdev_vport->dev) {
+		err = -ENODEV;
+		goto error_free_vport;
+	}
+
+	if (netdev_vport->dev->flags & IFF_LOOPBACK ||
+	    netdev_vport->dev->type != ARPHRD_ETHER ||
+	    ovs_is_internal_dev(netdev_vport->dev)) {
+		err = -EINVAL;
+		goto error_put;
+	}
+
+	err = netdev_rx_handler_register(netdev_vport->dev, netdev_frame_hook,
+					 vport);
+	if (err)
+		goto error_put;
+
+	dev_set_promiscuity(netdev_vport->dev, 1);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,24)
+	dev_disable_lro(netdev_vport->dev);
+#endif
+	netdev_vport->dev->priv_flags |= IFF_OVS_DATAPATH;
+
+	return vport;
+
+error_put:
+	dev_put(netdev_vport->dev);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void netdev_destroy(struct vport *vport)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+
+	netdev_vport->dev->priv_flags &= ~IFF_OVS_DATAPATH;
+	netdev_rx_handler_unregister(netdev_vport->dev);
+	dev_set_promiscuity(netdev_vport->dev, -1);
+
+	synchronize_rcu();
+
+	dev_put(netdev_vport->dev);
+	ovs_vport_free(vport);
+}
+
+int ovs_netdev_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	struct sockaddr sa;
+
+	sa.sa_family = ARPHRD_ETHER;
+	memcpy(sa.sa_data, addr, ETH_ALEN);
+
+	return dev_set_mac_address(netdev_vport->dev, &sa);
+}
+
+const char *ovs_netdev_get_name(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->name;
+}
+
+const unsigned char *ovs_netdev_get_addr(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->dev_addr;
+}
+
+struct kobject *ovs_netdev_get_kobj(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return &netdev_vport->dev->NETDEV_DEV_MEMBER.kobj;
+}
+
+unsigned ovs_netdev_get_dev_flags(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return dev_get_flags(netdev_vport->dev);
+}
+
+int ovs_netdev_is_running(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netif_running(netdev_vport->dev);
+}
+
+unsigned char ovs_netdev_get_operstate(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->operstate;
+}
+
+int ovs_netdev_get_ifindex(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->ifindex;
+}
+
+int ovs_netdev_get_mtu(const struct vport *vport)
+{
+	const struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	return netdev_vport->dev->mtu;
+}
+
+/* Must be called with rcu_read_lock. */
+static void netdev_port_receive(struct vport *vport, struct sk_buff *skb)
+{
+	if (unlikely(!vport)) {
+		kfree_skb(skb);
+		return;
+	}
+
+	/* Make our own copy of the packet.  Otherwise we will mangle the
+	 * packet for anyone who came before us (e.g. tcpdump via AF_PACKET).
+	 * (No one comes after us, since we tell handle_bridge() that we took
+	 * the packet.) */
+	skb = skb_share_check(skb, GFP_ATOMIC);
+	if (unlikely(!skb))
+		return;
+
+	skb_push(skb, ETH_HLEN);
+
+	if (unlikely(compute_ip_summed(skb, false))) {
+		kfree_skb(skb);
+		return;
+	}
+	vlan_copy_skb_tci(skb);
+
+	ovs_vport_receive(vport, skb);
+}
+
+static unsigned packet_length(const struct sk_buff *skb)
+{
+	unsigned length = skb->len - ETH_HLEN;
+
+	if (skb->protocol == htons(ETH_P_8021Q))
+		length -= VLAN_HLEN;
+
+	return length;
+}
+
+static bool dev_supports_vlan_tx(struct net_device *dev)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,37)
+	/* Software fallback means every device supports vlan_tci on TX. */
+	return true;
+#elif defined(HAVE_VLAN_BUG_WORKAROUND)
+	return dev->features & NETIF_F_HW_VLAN_TX;
+#else
+	/* Assume that the driver is buggy. */
+	return false;
+#endif
+}
+
+static int netdev_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct netdev_vport *netdev_vport = netdev_vport_priv(vport);
+	int mtu = netdev_vport->dev->mtu;
+	int len;
+
+	if (unlikely(packet_length(skb) > mtu && !skb_is_gso(skb))) {
+		if (net_ratelimit())
+			pr_warn("%s: dropped over-mtu packet: %d > %d\n",
+				ovs_dp_name(vport->dp), packet_length(skb), mtu);
+		goto error;
+	}
+
+	if (unlikely(skb_warn_if_lro(skb)))
+		goto error;
+
+	skb->dev = netdev_vport->dev;
+	forward_ip_summed(skb, true);
+
+	if (vlan_tx_tag_present(skb) && !dev_supports_vlan_tx(skb->dev)) {
+		int features;
+
+		features = netif_skb_features(skb);
+
+		if (!vlan_tso)
+			features &= ~(NETIF_F_TSO | NETIF_F_TSO6 |
+				      NETIF_F_UFO | NETIF_F_FSO);
+
+		if (netif_needs_gso(skb, features)) {
+			struct sk_buff *nskb;
+
+			nskb = skb_gso_segment(skb, features);
+			if (!nskb) {
+				if (unlikely(skb_cloned(skb) &&
+				    pskb_expand_head(skb, 0, 0, GFP_ATOMIC))) {
+					kfree_skb(skb);
+					return 0;
+				}
+
+				skb_shinfo(skb)->gso_type &= ~SKB_GSO_DODGY;
+				goto tag;
+			}
+
+			if (IS_ERR(nskb)) {
+				kfree_skb(skb);
+				return 0;
+			}
+			consume_skb(skb);
+			skb = nskb;
+
+			len = 0;
+			do {
+				nskb = skb->next;
+				skb->next = NULL;
+
+				skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+				if (likely(skb)) {
+					len += skb->len;
+					vlan_set_tci(skb, 0);
+					dev_queue_xmit(skb);
+				}
+
+				skb = nskb;
+			} while (skb);
+
+			return len;
+		}
+
+tag:
+		skb = __vlan_put_tag(skb, vlan_tx_tag_get(skb));
+		if (unlikely(!skb))
+			return 0;
+		vlan_set_tci(skb, 0);
+	}
+
+	len = skb->len;
+	dev_queue_xmit(skb);
+
+	return len;
+
+error:
+	kfree_skb(skb);
+	ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+	return 0;
+}
+
+/* Returns null if this device is not attached to a datapath. */
+struct vport *ovs_netdev_get_vport(struct net_device *dev)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,36)
+#if IFF_BRIDGE_PORT != IFF_OVS_DATAPATH
+	if (likely(dev->priv_flags & IFF_OVS_DATAPATH))
+#else
+	if (likely(rcu_access_pointer(dev->rx_handler) == netdev_frame_hook))
+#endif
+		return (struct vport *)rcu_dereference_rtnl(dev->rx_handler_data);
+	else
+		return NULL;
+#else
+	return (struct vport *)rcu_dereference_rtnl(dev->br_port);
+#endif
+}
+
+const struct vport_ops ovs_netdev_vport_ops = {
+	.type		= OVS_VPORT_TYPE_NETDEV,
+	.flags          = VPORT_F_REQUIRED,
+	.init		= netdev_init,
+	.exit		= netdev_exit,
+	.create		= netdev_create,
+	.destroy	= netdev_destroy,
+	.set_addr	= ovs_netdev_set_addr,
+	.get_name	= ovs_netdev_get_name,
+	.get_addr	= ovs_netdev_get_addr,
+	.get_kobj	= ovs_netdev_get_kobj,
+	.get_dev_flags	= ovs_netdev_get_dev_flags,
+	.is_running	= ovs_netdev_is_running,
+	.get_operstate	= ovs_netdev_get_operstate,
+	.get_ifindex	= ovs_netdev_get_ifindex,
+	.get_mtu	= ovs_netdev_get_mtu,
+	.send		= netdev_send,
+};
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,36)
+/*
+ * In kernels earlier than 2.6.36, Open vSwitch cannot safely coexist with the
+ * Linux bridge module, because there is only a single bridge hook function and
+ * only a single br_port member in struct net_device, so this prevents loading
+ * both bridge and openvswitch_mod at the same time.
+ */
+BRIDGE_MUTUAL_EXCLUSION;
+#endif
diff -r 89e197c6e9d5 net/openvswitch/vport-netdev.h
--- /dev/null
+++ b/net/openvswitch/vport-netdev.h
@@ -0,0 +1,49 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_NETDEV_H
+#define VPORT_NETDEV_H 1
+
+#include <linux/netdevice.h>
+
+#include "vport.h"
+
+struct vport *ovs_netdev_get_vport(struct net_device *dev);
+
+struct netdev_vport {
+	struct net_device *dev;
+};
+
+static inline struct netdev_vport *
+netdev_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+int ovs_netdev_set_addr(struct vport *, const unsigned char *addr);
+const char *ovs_netdev_get_name(const struct vport *);
+const unsigned char *ovs_netdev_get_addr(const struct vport *);
+const char *ovs_netdev_get_config(const struct vport *);
+struct kobject *ovs_netdev_get_kobj(const struct vport *);
+unsigned ovs_netdev_get_dev_flags(const struct vport *);
+int ovs_netdev_is_running(const struct vport *);
+unsigned char ovs_netdev_get_operstate(const struct vport *);
+int ovs_netdev_get_ifindex(const struct vport *);
+int ovs_netdev_get_mtu(const struct vport *);
+
+#endif /* vport_netdev.h */
diff -r 89e197c6e9d5 net/openvswitch/vport-patch.c
--- /dev/null
+++ b/net/openvswitch/vport-patch.c
@@ -0,0 +1,314 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/dcache.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/rtnetlink.h>
+
+#include "compat.h"
+#include "datapath.h"
+#include "vport.h"
+#include "vport-generic.h"
+
+struct patch_config {
+	struct rcu_head rcu;
+
+	char peer_name[IFNAMSIZ];
+	unsigned char eth_addr[ETH_ALEN];
+};
+
+struct patch_vport {
+	struct rcu_head rcu;
+
+	char name[IFNAMSIZ];
+
+	/* Protected by RTNL lock. */
+	struct hlist_node hash_node;
+
+	struct vport __rcu *peer;
+	struct patch_config __rcu *patchconf;
+};
+
+/* Protected by RTNL lock. */
+static struct hlist_head *peer_table;
+#define PEER_HASH_BUCKETS 256
+
+static void update_peers(const char *name, struct vport *);
+
+static struct patch_vport *patch_vport_priv(const struct vport *vport)
+{
+	return vport_priv(vport);
+}
+
+/* RCU callback. */
+static void free_config(struct rcu_head *rcu)
+{
+	struct patch_config *c = container_of(rcu, struct patch_config, rcu);
+	kfree(c);
+}
+
+static void assign_config_rcu(struct vport *vport,
+			      struct patch_config *new_config)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *old_config;
+
+	old_config = rtnl_dereference(patch_vport->patchconf);
+	rcu_assign_pointer(patch_vport->patchconf, new_config);
+	call_rcu(&old_config->rcu, free_config);
+}
+
+static struct hlist_head *hash_bucket(const char *name)
+{
+	unsigned int hash = full_name_hash(name, strlen(name));
+	return &peer_table[hash & (PEER_HASH_BUCKETS - 1)];
+}
+
+static int patch_init(void)
+{
+	peer_table = kzalloc(PEER_HASH_BUCKETS * sizeof(struct hlist_head),
+			    GFP_KERNEL);
+	if (!peer_table)
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void patch_exit(void)
+{
+	kfree(peer_table);
+}
+
+static const struct nla_policy patch_policy[OVS_PATCH_ATTR_MAX + 1] = {
+#ifdef HAVE_NLA_NUL_STRING
+	[OVS_PATCH_ATTR_PEER] = { .type = NLA_NUL_STRING, .len = IFNAMSIZ - 1 },
+#endif
+};
+
+static int patch_set_config(struct vport *vport, const struct nlattr *options,
+			    struct patch_config *patchconf)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct nlattr *a[OVS_PATCH_ATTR_MAX + 1];
+	const char *peer_name;
+	int err;
+
+	if (!options)
+		return -EINVAL;
+
+	err = nla_parse_nested(a, OVS_PATCH_ATTR_MAX, options, patch_policy);
+	if (err)
+		return err;
+
+	if (!a[OVS_PATCH_ATTR_PEER] ||
+	    CHECK_NUL_STRING(a[OVS_PATCH_ATTR_PEER], IFNAMSIZ - 1))
+		return -EINVAL;
+
+	peer_name = nla_data(a[OVS_PATCH_ATTR_PEER]);
+	if (!strcmp(patch_vport->name, peer_name))
+		return -EINVAL;
+
+	strcpy(patchconf->peer_name, peer_name);
+
+	return 0;
+}
+
+static struct vport *patch_create(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	struct patch_vport *patch_vport;
+	const char *peer_name;
+	struct patch_config *patchconf;
+	int err;
+
+	vport = ovs_vport_alloc(sizeof(struct patch_vport),
+				&ovs_patch_vport_ops, parms);
+	if (IS_ERR(vport)) {
+		err = PTR_ERR(vport);
+		goto error;
+	}
+
+	patch_vport = patch_vport_priv(vport);
+
+	strcpy(patch_vport->name, parms->name);
+
+	patchconf = kmalloc(sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf) {
+		err = -ENOMEM;
+		goto error_free_vport;
+	}
+
+	err = patch_set_config(vport, parms->options, patchconf);
+	if (err)
+		goto error_free_patchconf;
+
+	random_ether_addr(patchconf->eth_addr);
+
+	rcu_assign_pointer(patch_vport->patchconf, patchconf);
+
+	peer_name = patchconf->peer_name;
+	hlist_add_head(&patch_vport->hash_node, hash_bucket(peer_name));
+	rcu_assign_pointer(patch_vport->peer, ovs_vport_locate(peer_name));
+	update_peers(patch_vport->name, vport);
+
+	return vport;
+
+error_free_patchconf:
+	kfree(patchconf);
+error_free_vport:
+	ovs_vport_free(vport);
+error:
+	return ERR_PTR(err);
+}
+
+static void free_port_rcu(struct rcu_head *rcu)
+{
+	struct patch_vport *patch_vport = container_of(rcu,
+					  struct patch_vport, rcu);
+
+	kfree((struct patch_config __force *)patch_vport->patchconf);
+	ovs_vport_free(vport_from_priv(patch_vport));
+}
+
+static void patch_destroy(struct vport *vport)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+
+	update_peers(patch_vport->name, NULL);
+	hlist_del(&patch_vport->hash_node);
+	call_rcu(&patch_vport->rcu, free_port_rcu);
+}
+
+static int patch_set_options(struct vport *vport, struct nlattr *options)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf;
+	int err;
+
+	patchconf = kmemdup(rtnl_dereference(patch_vport->patchconf),
+			  sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	err = patch_set_config(vport, options, patchconf);
+	if (err)
+		goto error_free;
+
+	assign_config_rcu(vport, patchconf);
+
+	hlist_del(&patch_vport->hash_node);
+
+	rcu_assign_pointer(patch_vport->peer, ovs_vport_locate(patchconf->peer_name));
+	hlist_add_head(&patch_vport->hash_node, hash_bucket(patchconf->peer_name));
+
+	return 0;
+
+error_free:
+	kfree(patchconf);
+error:
+	return err;
+}
+
+static void update_peers(const char *name, struct vport *vport)
+{
+	struct hlist_head *bucket = hash_bucket(name);
+	struct patch_vport *peer_vport;
+	struct hlist_node *node;
+
+	hlist_for_each_entry(peer_vport, node, bucket, hash_node) {
+		const char *peer_name;
+
+		peer_name = rtnl_dereference(peer_vport->patchconf)->peer_name;
+		if (!strcmp(peer_name, name))
+			rcu_assign_pointer(peer_vport->peer, vport);
+	}
+}
+
+static int patch_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf;
+
+	patchconf = kmemdup(rtnl_dereference(patch_vport->patchconf),
+			  sizeof(struct patch_config), GFP_KERNEL);
+	if (!patchconf)
+		return -ENOMEM;
+
+	memcpy(patchconf->eth_addr, addr, ETH_ALEN);
+	assign_config_rcu(vport, patchconf);
+
+	return 0;
+}
+
+
+static const char *patch_get_name(const struct vport *vport)
+{
+	const struct patch_vport *patch_vport = patch_vport_priv(vport);
+	return patch_vport->name;
+}
+
+static const unsigned char *patch_get_addr(const struct vport *vport)
+{
+	const struct patch_vport *patch_vport = patch_vport_priv(vport);
+	return rcu_dereference_rtnl(patch_vport->patchconf)->eth_addr;
+}
+
+static int patch_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct patch_config *patchconf = rcu_dereference_rtnl(patch_vport->patchconf);
+
+	return nla_put_string(skb, OVS_PATCH_ATTR_PEER, patchconf->peer_name);
+}
+
+static int patch_send(struct vport *vport, struct sk_buff *skb)
+{
+	struct patch_vport *patch_vport = patch_vport_priv(vport);
+	struct vport *peer = rcu_dereference(patch_vport->peer);
+	int skb_len = skb->len;
+
+	if (!peer) {
+		kfree_skb(skb);
+		ovs_vport_record_error(vport, VPORT_E_TX_DROPPED);
+
+		return 0;
+	}
+
+	ovs_vport_receive(peer, skb);
+	return skb_len;
+}
+
+const struct vport_ops ovs_patch_vport_ops = {
+	.type		= OVS_VPORT_TYPE_PATCH,
+	.init		= patch_init,
+	.exit		= patch_exit,
+	.create		= patch_create,
+	.destroy	= patch_destroy,
+	.set_addr	= patch_set_addr,
+	.get_name	= patch_get_name,
+	.get_addr	= patch_get_addr,
+	.get_options	= patch_get_options,
+	.set_options	= patch_set_options,
+	.get_dev_flags	= ovs_vport_gen_get_dev_flags,
+	.is_running	= ovs_vport_gen_is_running,
+	.get_operstate	= ovs_vport_gen_get_operstate,
+	.send		= patch_send,
+};
diff -r 89e197c6e9d5 net/openvswitch/vport.c
--- /dev/null
+++ b/net/openvswitch/vport.c
@@ -0,0 +1,521 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#include <linux/dcache.h>
+#include <linux/etherdevice.h>
+#include <linux/if.h>
+#include <linux/if_vlan.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/percpu.h>
+#include <linux/rcupdate.h>
+#include <linux/rtnetlink.h>
+#include <linux/compat.h>
+#include <linux/version.h>
+
+#include "vport.h"
+#include "vport-internal_dev.h"
+
+/* List of statically compiled vport implementations.  Don't forget to also
+ * add yours to the list at the bottom of vport.h. */
+static const struct vport_ops *base_vport_ops_list[] = {
+	&ovs_netdev_vport_ops,
+	&ovs_internal_vport_ops,
+	&ovs_patch_vport_ops,
+	&ovs_gre_vport_ops,
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,26)
+	&ovs_capwap_vport_ops,
+#endif
+};
+
+static const struct vport_ops **vport_ops_list;
+static int n_vport_types;
+
+/* Protected by RCU read lock for reading, RTNL lock for writing. */
+static struct hlist_head *dev_table;
+#define VPORT_HASH_BUCKETS 1024
+
+/**
+ *	ovs_vport_init - initialize vport subsystem
+ *
+ * Called at module load time to initialize the vport subsystem and any
+ * compiled in vport types.
+ */
+int ovs_vport_init(void)
+{
+	int err;
+	int i;
+
+	dev_table = kzalloc(VPORT_HASH_BUCKETS * sizeof(struct hlist_head),
+			    GFP_KERNEL);
+	if (!dev_table) {
+		err = -ENOMEM;
+		goto error;
+	}
+
+	vport_ops_list = kmalloc(ARRAY_SIZE(base_vport_ops_list) *
+				 sizeof(struct vport_ops *), GFP_KERNEL);
+	if (!vport_ops_list) {
+		err = -ENOMEM;
+		goto error_dev_table;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(base_vport_ops_list); i++) {
+		const struct vport_ops *new_ops = base_vport_ops_list[i];
+
+		if (new_ops->init)
+			err = new_ops->init();
+		else
+			err = 0;
+
+		if (!err)
+			vport_ops_list[n_vport_types++] = new_ops;
+		else if (new_ops->flags & VPORT_F_REQUIRED) {
+			ovs_vport_exit();
+			goto error;
+		}
+	}
+
+	return 0;
+
+error_dev_table:
+	kfree(dev_table);
+error:
+	return err;
+}
+
+/**
+ *	ovs_vport_exit - shutdown vport subsystem
+ *
+ * Called at module exit time to shutdown the vport subsystem and any
+ * initialized vport types.
+ */
+void ovs_vport_exit(void)
+{
+	int i;
+
+	for (i = 0; i < n_vport_types; i++) {
+		if (vport_ops_list[i]->exit)
+			vport_ops_list[i]->exit();
+	}
+
+	kfree(vport_ops_list);
+	kfree(dev_table);
+}
+
+static struct hlist_head *hash_bucket(const char *name)
+{
+	unsigned int hash = full_name_hash(name, strlen(name));
+	return &dev_table[hash & (VPORT_HASH_BUCKETS - 1)];
+}
+
+/**
+ *	ovs_vport_locate - find a port that has already been created
+ *
+ * @name: name of port to find
+ *
+ * Must be called with RTNL or RCU read lock.
+ */
+struct vport *ovs_vport_locate(const char *name)
+{
+	struct hlist_head *bucket = hash_bucket(name);
+	struct vport *vport;
+	struct hlist_node *node;
+
+	hlist_for_each_entry_rcu(vport, node, bucket, hash_node)
+		if (!strcmp(name, vport->ops->get_name(vport)))
+			return vport;
+
+	return NULL;
+}
+
+static void release_vport(struct kobject *kobj)
+{
+	struct vport *p = container_of(kobj, struct vport, kobj);
+	kfree(p);
+}
+
+static struct kobj_type brport_ktype = {
+#ifdef CONFIG_SYSFS
+	.sysfs_ops = &ovs_brport_sysfs_ops,
+#endif
+	.release = release_vport
+};
+
+/**
+ *	ovs_vport_alloc - allocate and initialize new vport
+ *
+ * @priv_size: Size of private data area to allocate.
+ * @ops: vport device ops
+ *
+ * Allocate and initialize a new vport defined by @ops.  The vport will contain
+ * a private data area of size @priv_size that can be accessed using
+ * vport_priv().  vports that are no longer needed should be released with
+ * ovs_vport_free().
+ */
+struct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *ops,
+			      const struct vport_parms *parms)
+{
+	struct vport *vport;
+	size_t alloc_size;
+
+	alloc_size = sizeof(struct vport);
+	if (priv_size) {
+		alloc_size = ALIGN(alloc_size, VPORT_ALIGN);
+		alloc_size += priv_size;
+	}
+
+	vport = kzalloc(alloc_size, GFP_KERNEL);
+	if (!vport)
+		return ERR_PTR(-ENOMEM);
+
+	vport->dp = parms->dp;
+	vport->port_no = parms->port_no;
+	vport->upcall_pid = parms->upcall_pid;
+	vport->ops = ops;
+
+	/* Initialize kobject for bridge.  This will be added as
+	 * /sys/class/net/<devname>/brport later, if sysfs is enabled. */
+	vport->kobj.kset = NULL;
+	kobject_init(&vport->kobj, &brport_ktype);
+
+	vport->percpu_stats = alloc_percpu(struct vport_percpu_stats);
+	if (!vport->percpu_stats) {
+		kfree(vport);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	spin_lock_init(&vport->stats_lock);
+
+	return vport;
+}
+
+/**
+ *	ovs_vport_free - uninitialize and free vport
+ *
+ * @vport: vport to free
+ *
+ * Frees a vport allocated with ovs_vport_alloc() when it is no longer needed.
+ *
+ * The caller must ensure that an RCU grace period has passed since the last
+ * time @vport was in a datapath.
+ */
+void ovs_vport_free(struct vport *vport)
+{
+	free_percpu(vport->percpu_stats);
+
+	kobject_put(&vport->kobj);
+}
+
+/**
+ *	ovs_vport_add - add vport device (for kernel callers)
+ *
+ * @parms: Information about new vport.
+ *
+ * Creates a new vport with the specified configuration (which is dependent on
+ * device type).  RTNL lock must be held.
+ */
+struct vport *ovs_vport_add(const struct vport_parms *parms)
+{
+	struct vport *vport;
+	int err = 0;
+	int i;
+
+	ASSERT_RTNL();
+
+	for (i = 0; i < n_vport_types; i++) {
+		if (vport_ops_list[i]->type == parms->type) {
+			vport = vport_ops_list[i]->create(parms);
+			if (IS_ERR(vport)) {
+				err = PTR_ERR(vport);
+				goto out;
+			}
+
+			hlist_add_head_rcu(&vport->hash_node,
+					   hash_bucket(vport->ops->get_name(vport)));
+			return vport;
+		}
+	}
+
+	err = -EAFNOSUPPORT;
+
+out:
+	return ERR_PTR(err);
+}
+
+/**
+ *	ovs_vport_set_options - modify existing vport device (for kernel callers)
+ *
+ * @vport: vport to modify.
+ * @port: New configuration.
+ *
+ * Modifies an existing device with the specified configuration (which is
+ * dependent on device type).  RTNL lock must be held.
+ */
+int ovs_vport_set_options(struct vport *vport, struct nlattr *options)
+{
+	ASSERT_RTNL();
+
+	if (!vport->ops->set_options)
+		return -EOPNOTSUPP;
+	return vport->ops->set_options(vport, options);
+}
+
+/**
+ *	ovs_vport_del - delete existing vport device
+ *
+ * @vport: vport to delete.
+ *
+ * Detaches @vport from its datapath and destroys it.  It is possible to fail
+ * for reasons such as lack of memory.  RTNL lock must be held.
+ */
+void ovs_vport_del(struct vport *vport)
+{
+	ASSERT_RTNL();
+
+	hlist_del_rcu(&vport->hash_node);
+
+	vport->ops->destroy(vport);
+}
+
+/**
+ *	ovs_vport_set_addr - set device Ethernet address (for kernel callers)
+ *
+ * @vport: vport on which to set Ethernet address.
+ * @addr: New address.
+ *
+ * Sets the Ethernet address of the given device.  Some devices may not support
+ * setting the Ethernet address, in which case the result will always be
+ * -EOPNOTSUPP.  RTNL lock must be held.
+ */
+int ovs_vport_set_addr(struct vport *vport, const unsigned char *addr)
+{
+	ASSERT_RTNL();
+
+	if (!is_valid_ether_addr(addr))
+		return -EADDRNOTAVAIL;
+
+	if (vport->ops->set_addr)
+		return vport->ops->set_addr(vport, addr);
+	else
+		return -EOPNOTSUPP;
+}
+
+/**
+ *	ovs_vport_set_stats - sets offset device stats
+ *
+ * @vport: vport on which to set stats
+ * @stats: stats to set
+ *
+ * Provides a set of transmit, receive, and error stats to be added as an
+ * offset to the collect data when stats are retreived.  Some devices may not
+ * support setting the stats, in which case the result will always be
+ * -EOPNOTSUPP.
+ *
+ * Must be called with RTNL lock.
+ */
+void ovs_vport_set_stats(struct vport *vport, struct ovs_vport_stats *stats)
+{
+	ASSERT_RTNL();
+
+	spin_lock_bh(&vport->stats_lock);
+	vport->offset_stats = *stats;
+	spin_unlock_bh(&vport->stats_lock);
+}
+
+/**
+ *	ovs_vport_get_stats - retrieve device stats
+ *
+ * @vport: vport from which to retrieve the stats
+ * @stats: location to store stats
+ *
+ * Retrieves transmit, receive, and error stats for the given device.
+ *
+ * Must be called with RTNL lock or rcu_read_lock.
+ */
+void ovs_vport_get_stats(struct vport *vport, struct ovs_vport_stats *stats)
+{
+	int i;
+
+	/* We potentially have 3 sources of stats that need to be
+	 * combined: those we have collected (split into err_stats and
+	 * percpu_stats), offset_stats from set_stats(), and device
+	 * error stats from netdev->get_stats() (for errors that happen
+	 * downstream and therefore aren't reported through our
+	 * vport_record_error() function).
+	 * Stats from first two sources are merged and reported by ovs over
+	 * OVS_VPORT_ATTR_STATS.
+	 * netdev-stats can be directly read over netlink-ioctl.
+	 */
+
+	spin_lock_bh(&vport->stats_lock);
+
+	*stats = vport->offset_stats;
+
+	stats->rx_errors	+= vport->err_stats.rx_errors;
+	stats->tx_errors	+= vport->err_stats.tx_errors;
+	stats->tx_dropped	+= vport->err_stats.tx_dropped;
+	stats->rx_dropped	+= vport->err_stats.rx_dropped;
+
+	spin_unlock_bh(&vport->stats_lock);
+
+	for_each_possible_cpu(i) {
+		const struct vport_percpu_stats *percpu_stats;
+		struct vport_percpu_stats local_stats;
+		unsigned int start;
+
+		percpu_stats = per_cpu_ptr(vport->percpu_stats, i);
+
+		do {
+			start = u64_stats_fetch_begin_bh(&percpu_stats->sync);
+			local_stats = *percpu_stats;
+		} while (u64_stats_fetch_retry_bh(&percpu_stats->sync, start));
+
+		stats->rx_bytes		+= local_stats.rx_bytes;
+		stats->rx_packets	+= local_stats.rx_packets;
+		stats->tx_bytes		+= local_stats.tx_bytes;
+		stats->tx_packets	+= local_stats.tx_packets;
+	}
+}
+
+/**
+ *	ovs_vport_get_options - retrieve device options
+ *
+ * @vport: vport from which to retrieve the options.
+ * @skb: sk_buff where options should be appended.
+ *
+ * Retrieves the configuration of the given device, appending an
+ * %OVS_VPORT_ATTR_OPTIONS attribute that in turn contains nested
+ * vport-specific attributes to @skb.
+ *
+ * Returns 0 if successful, -EMSGSIZE if @skb has insufficient room, or another
+ * negative error code if a real error occurred.  If an error occurs, @skb is
+ * left unmodified.
+ *
+ * Must be called with RTNL lock or rcu_read_lock.
+ */
+int ovs_vport_get_options(const struct vport *vport, struct sk_buff *skb)
+{
+	struct nlattr *nla;
+
+	nla = nla_nest_start(skb, OVS_VPORT_ATTR_OPTIONS);
+	if (!nla)
+		return -EMSGSIZE;
+
+	if (vport->ops->get_options) {
+		int err = vport->ops->get_options(vport, skb);
+		if (err) {
+			nla_nest_cancel(skb, nla);
+			return err;
+		}
+	}
+
+	nla_nest_end(skb, nla);
+	return 0;
+}
+
+/**
+ *	ovs_vport_receive - pass up received packet to the datapath for processing
+ *
+ * @vport: vport that received the packet
+ * @skb: skb that was received
+ *
+ * Must be called with rcu_read_lock.  The packet cannot be shared and
+ * skb->data should point to the Ethernet header.  The caller must have already
+ * called compute_ip_summed() to initialize the checksumming fields.
+ */
+void ovs_vport_receive(struct vport *vport, struct sk_buff *skb)
+{
+	struct vport_percpu_stats *stats;
+
+	stats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());
+
+	u64_stats_update_begin(&stats->sync);
+	stats->rx_packets++;
+	stats->rx_bytes += skb->len;
+	u64_stats_update_end(&stats->sync);
+
+	if (!(vport->ops->flags & VPORT_F_FLOW))
+		OVS_CB(skb)->flow = NULL;
+
+	if (!(vport->ops->flags & VPORT_F_TUN_ID))
+		OVS_CB(skb)->tun_id = 0;
+
+	ovs_dp_process_received_packet(vport, skb);
+}
+
+/**
+ *	ovs_vport_send - send a packet on a device
+ *
+ * @vport: vport on which to send the packet
+ * @skb: skb to send
+ *
+ * Sends the given packet and returns the length of data sent.  Either RTNL
+ * lock or rcu_read_lock must be held.
+ */
+int ovs_vport_send(struct vport *vport, struct sk_buff *skb)
+{
+	int sent = vport->ops->send(vport, skb);
+
+	if (likely(sent)) {
+		struct vport_percpu_stats *stats;
+
+		stats = per_cpu_ptr(vport->percpu_stats, smp_processor_id());
+
+		u64_stats_update_begin(&stats->sync);
+		stats->tx_packets++;
+		stats->tx_bytes += sent;
+		u64_stats_update_end(&stats->sync);
+	}
+	return sent;
+}
+
+/**
+ *	ovs_vport_record_error - indicate device error to generic stats layer
+ *
+ * @vport: vport that encountered the error
+ * @err_type: one of enum vport_err_type types to indicate the error type
+ *
+ * If using the vport generic stats layer indicate that an error of the given
+ * type has occured.
+ */
+void ovs_vport_record_error(struct vport *vport, enum vport_err_type err_type)
+{
+	spin_lock(&vport->stats_lock);
+
+	switch (err_type) {
+	case VPORT_E_RX_DROPPED:
+		vport->err_stats.rx_dropped++;
+		break;
+
+	case VPORT_E_RX_ERROR:
+		vport->err_stats.rx_errors++;
+		break;
+
+	case VPORT_E_TX_DROPPED:
+		vport->err_stats.tx_dropped++;
+		break;
+
+	case VPORT_E_TX_ERROR:
+		vport->err_stats.tx_errors++;
+		break;
+	};
+
+	spin_unlock(&vport->stats_lock);
+}
diff -r 89e197c6e9d5 net/openvswitch/vport.h
--- /dev/null
+++ b/net/openvswitch/vport.h
@@ -0,0 +1,254 @@
+/*
+ * Copyright (c) 2007-2011 Nicira Networks.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of version 2 of the GNU General Public
+ * License as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
+ * 02110-1301, USA
+ */
+
+#ifndef VPORT_H
+#define VPORT_H 1
+
+#include <linux/list.h>
+#include <linux/openvswitch.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/u64_stats_sync.h>
+
+#include "datapath.h"
+
+struct vport;
+struct vport_parms;
+
+/* The following definitions are for users of the vport subsytem: */
+
+int ovs_vport_init(void);
+void ovs_vport_exit(void);
+
+struct vport *ovs_vport_add(const struct vport_parms *);
+void ovs_vport_del(struct vport *);
+
+struct vport *ovs_vport_locate(const char *name);
+
+int ovs_vport_set_addr(struct vport *, const unsigned char *);
+void ovs_vport_set_stats(struct vport *, struct ovs_vport_stats *);
+void ovs_vport_get_stats(struct vport *, struct ovs_vport_stats *);
+
+int ovs_vport_set_options(struct vport *, struct nlattr *options);
+int ovs_vport_get_options(const struct vport *, struct sk_buff *);
+
+int ovs_vport_send(struct vport *, struct sk_buff *);
+
+/* The following definitions are for implementers of vport devices: */
+
+struct vport_percpu_stats {
+	u64 rx_bytes;
+	u64 rx_packets;
+	u64 tx_bytes;
+	u64 tx_packets;
+	struct u64_stats_sync sync;
+};
+
+struct vport_err_stats {
+	u64 rx_dropped;
+	u64 rx_errors;
+	u64 tx_dropped;
+	u64 tx_errors;
+};
+
+/**
+ * struct vport - one port within a datapath
+ * @rcu: RCU callback head for deferred destruction.
+ * @port_no: Index into @dp's @ports array.
+ * @dp: Datapath to which this port belongs.
+ * @kobj: Represents /sys/class/net/<devname>/brport.
+ * @linkname: The name of the link from /sys/class/net/<datapath>/brif to this
+ * &struct vport.  (We keep this around so that we can delete it if the
+ * device gets renamed.)  Set to the null string when no link exists.
+ * @node: Element in @dp's @port_list.
+ * @upcall_pid: The Netlink port to use for packets received on this port that
+ * miss the flow table.
+ * @hash_node: Element in @dev_table hash table in vport.c.
+ * @ops: Class structure.
+ * @percpu_stats: Points to per-CPU statistics used and maintained by vport
+ * @stats_lock: Protects @err_stats and @offset_stats.
+ * @err_stats: Points to error statistics used and maintained by vport
+ * @offset_stats: Added to actual statistics as a sop to compatibility with
+ * XAPI for Citrix XenServer.  Deprecated.
+ */
+struct vport {
+	struct rcu_head rcu;
+	u16 port_no;
+	struct datapath	*dp;
+	struct kobject kobj;
+	char linkname[IFNAMSIZ];
+	struct list_head node;
+	u32 upcall_pid;
+
+	struct hlist_node hash_node;
+	const struct vport_ops *ops;
+
+	struct vport_percpu_stats __percpu *percpu_stats;
+
+	spinlock_t stats_lock;
+	struct vport_err_stats err_stats;
+	struct ovs_vport_stats offset_stats;
+};
+
+#define VPORT_F_REQUIRED	(1 << 0) /* If init fails, module loading fails. */
+#define VPORT_F_FLOW		(1 << 1) /* Sets OVS_CB(skb)->flow. */
+#define VPORT_F_TUN_ID		(1 << 2) /* Sets OVS_CB(skb)->tun_id. */
+
+/**
+ * struct vport_parms - parameters for creating a new vport
+ *
+ * @name: New vport's name.
+ * @type: New vport's type.
+ * @options: %OVS_VPORT_ATTR_OPTIONS attribute from Netlink message, %NULL if
+ * none was supplied.
+ * @dp: New vport's datapath.
+ * @port_no: New vport's port number.
+ */
+struct vport_parms {
+	const char *name;
+	enum ovs_vport_type type;
+	struct nlattr *options;
+
+	/* For ovs_vport_alloc(). */
+	struct datapath *dp;
+	u16 port_no;
+	u32 upcall_pid;
+};
+
+/**
+ * struct vport_ops - definition of a type of virtual port
+ *
+ * @type: %OVS_VPORT_TYPE_* value for this type of virtual port.
+ * @flags: Flags of type VPORT_F_* that influence how the generic vport layer
+ * handles this vport.
+ * @init: Called at module initialization.  If VPORT_F_REQUIRED is set then the
+ * failure of this function will cause the module to not load.  If the flag is
+ * not set and initialzation fails then no vports of this type can be created.
+ * @exit: Called at module unload.
+ * @create: Create a new vport configured as specified.  On success returns
+ * a new vport allocated with ovs_vport_alloc(), otherwise an ERR_PTR() value.
+ * @destroy: Destroys a vport.  Must call vport_free() on the vport but not
+ * before an RCU grace period has elapsed.
+ * @set_options: Modify the configuration of an existing vport.  May be %NULL
+ * if modification is not supported.
+ * @get_options: Appends vport-specific attributes for the configuration of an
+ * existing vport to a &struct sk_buff.  May be %NULL for a vport that does not
+ * have any configuration.
+ * @set_addr: Set the device's MAC address.  May be null if not supported.
+ * @get_name: Get the device's name.
+ * @get_addr: Get the device's MAC address.
+ * @get_config: Get the device's configuration.
+ * @get_kobj: Get the kobj associated with the device (may return null).
+ * @get_dev_flags: Get the device's flags.
+ * @is_running: Checks whether the device is running.
+ * @get_operstate: Get the device's operating state.
+ * @get_ifindex: Get the system interface index associated with the device.
+ * May be null if the device does not have an ifindex.
+ * @get_mtu: Get the device's MTU.  May be %NULL if the device does not have an
+ * MTU (as e.g. some tunnels do not).  Must be implemented if @get_ifindex is
+ * implemented.
+ * @send: Send a packet on the device.  Returns the length of the packet sent.
+ */
+struct vport_ops {
+	enum ovs_vport_type type;
+	u32 flags;
+
+	/* Called at module init and exit respectively. */
+	int (*init)(void);
+	void (*exit)(void);
+
+	/* Called with RTNL lock. */
+	struct vport *(*create)(const struct vport_parms *);
+	void (*destroy)(struct vport *);
+
+	int (*set_options)(struct vport *, struct nlattr *);
+	int (*get_options)(const struct vport *, struct sk_buff *);
+
+	int (*set_addr)(struct vport *, const unsigned char *);
+
+	/* Called with rcu_read_lock or RTNL lock. */
+	const char *(*get_name)(const struct vport *);
+	const unsigned char *(*get_addr)(const struct vport *);
+	void (*get_config)(const struct vport *, void *);
+	struct kobject *(*get_kobj)(const struct vport *);
+
+	unsigned (*get_dev_flags)(const struct vport *);
+	int (*is_running)(const struct vport *);
+	unsigned char (*get_operstate)(const struct vport *);
+
+	int (*get_ifindex)(const struct vport *);
+
+	int (*get_mtu)(const struct vport *);
+
+	int (*send)(struct vport *, struct sk_buff *);
+};
+
+enum vport_err_type {
+	VPORT_E_RX_DROPPED,
+	VPORT_E_RX_ERROR,
+	VPORT_E_TX_DROPPED,
+	VPORT_E_TX_ERROR,
+};
+
+struct vport *ovs_vport_alloc(int priv_size, const struct vport_ops *,
+			      const struct vport_parms *);
+void ovs_vport_free(struct vport *);
+
+#define VPORT_ALIGN 8
+
+/**
+ *	vport_priv - access private data area of vport
+ *
+ * @vport: vport to access
+ *
+ * If a nonzero size was passed in priv_size of vport_alloc() a private data
+ * area was allocated on creation.  This allows that area to be accessed and
+ * used for any purpose needed by the vport implementer.
+ */
+static inline void *vport_priv(const struct vport *vport)
+{
+	return (u8 *)vport + ALIGN(sizeof(struct vport), VPORT_ALIGN);
+}
+
+/**
+ *	vport_from_priv - lookup vport from private data pointer
+ *
+ * @priv: Start of private data area.
+ *
+ * It is sometimes useful to translate from a pointer to the private data
+ * area to the vport, such as in the case where the private data pointer is
+ * the result of a hash table lookup.  @priv must point to the start of the
+ * private data area.
+ */
+static inline struct vport *vport_from_priv(const void *priv)
+{
+	return (struct vport *)(priv - ALIGN(sizeof(struct vport), VPORT_ALIGN));
+}
+
+void ovs_vport_receive(struct vport *, struct sk_buff *);
+void ovs_vport_record_error(struct vport *, enum vport_err_type err_type);
+
+/* List of statically compiled vport implementations.  Don't forget to also
+ * add yours to the list at the top of vport.c. */
+extern const struct vport_ops ovs_netdev_vport_ops;
+extern const struct vport_ops ovs_internal_vport_ops;
+extern const struct vport_ops ovs_patch_vport_ops;
+extern const struct vport_ops ovs_gre_vport_ops;
+extern const struct vport_ops ovs_capwap_vport_ops;
+
+#endif /* vport.h */
