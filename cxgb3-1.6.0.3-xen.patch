diff --git a/drivers/net/cxgb3/Makefile b/drivers/net/cxgb3/Makefile
--- a/drivers/net/cxgb3/Makefile
+++ b/drivers/net/cxgb3/Makefile
@@ -4,5 +4,10 @@
 
 obj-$(CONFIG_CHELSIO_T3) += cxgb3.o
 
-cxgb3-objs := cxgb3_main.o ael1002.o vsc8211.o t3_hw.o mc5.o \
-	      xgmac.o sge.o l2t.o cxgb3_offload.o aq100x.o
+cxgb3-objs := cxgb3_main.o ael1002.o vsc8211.o t3_hw.o mc5.o tn1010.o \
+	xgmac.o sge.o l2t.o cxgb3_offload.o vsc7323.o aq100x.o mv88e1xxx.o
+
+EXTRA_CFLAGS := -DATOMIC_ADD_RETURN -DCONFIG_CHELSIO_T3_CORE \
+	-DGSO_TYPE -DHAS_EEH -DIRQF -DI_PRIVATE -DNAPI_UPDATE \
+	-DNETEVENT -DNEW_SKB_OFFSET -DPDEV_MAPPING -DSPIN_TRYLOCK_IRQSAVE \
+	-DCHELSIO_FREE_TXBUF_ASAP -DDISABLE_LRO
diff --git a/drivers/net/cxgb3/adapter.h b/drivers/net/cxgb3/adapter.h
--- a/drivers/net/cxgb3/adapter.h
+++ b/drivers/net/cxgb3/adapter.h
@@ -1,33 +1,12 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
 
 /* This file should not be included directly.  Include common.h instead. */
@@ -40,18 +19,29 @@
 #include <linux/interrupt.h>
 #include <linux/timer.h>
 #include <linux/cache.h>
-#include <linux/mutex.h>
-#include <linux/bitops.h>
 #include "t3cdev.h"
+#include <asm/bitops.h>
 #include <asm/io.h>
 
+#ifdef T3_TRACE
+# include "trace.h"
+#endif
+
 struct vlan_group;
-struct adapter;
-struct sge_qset;
 
-enum {			/* rx_offload flags */
-	T3_RX_CSUM	= 1 << 0,
-	T3_LRO		= 1 << 1,
+enum {
+	LF_NO = 0,
+	LF_MAYBE,
+	LF_YES
+};
+
+enum {
+	LOOPBACK_NONE		= 0,
+	LOOPBACK_PHY_PMA_PMD	= 1,
+	LOOPBACK_PHY_WIS	= 2,
+	LOOPBACK_PHY_PCS	= 3,
+	LOOPBACK_PHY_XS		= 4,
+	LOOPBACK_XGMAC		= 5,
 };
 
 struct port_info {
@@ -59,7 +49,9 @@
 	struct vlan_group *vlan_grp;
 	struct sge_qset *qs;
 	u8 port_id;
-	u8 rx_offload;
+	u8 tx_chan;
+	u8 txpkt_intf;
+	u8 rx_csum_offload;
 	u8 nqsets;
 	u8 first_qset;
 	struct cphy phy;
@@ -68,17 +60,28 @@
 	struct net_device_stats netstats;
 	int activity;
 	__be32 iscsi_ipv4addr;
-
-	int link_fault; /* link fault was detected */
+	int max_ofld_bw;
+	int link_fault;
+	u8 sched_max;
+	u8 sched_min;
+	int loopback;
 };
 
-enum {				/* adapter flags */
-	FULL_INIT_DONE = (1 << 0),
-	USING_MSI = (1 << 1),
-	USING_MSIX = (1 << 2),
-	QUEUES_BOUND = (1 << 3),
-	TP_PARITY_INIT = (1 << 4),
-	NAPI_INIT = (1 << 5),
+struct work_struct;
+struct dentry;
+
+enum {                                 /* adapter flags */
+	FULL_INIT_DONE     = (1 << 0),
+	USING_MSI          = (1 << 1),
+	USING_MSIX         = (1 << 2),
+	QUEUES_BOUND       = (1 << 3),
+	TP_PARITY_INIT     = (1 << 4),
+	NAPI_INIT	   = (1 << 5),
+};
+
+enum {					/* rspq flags */
+	USING_POLLING      = (1 << 0),
+	RSPQ_STARVING	   = (1 << 1),
 };
 
 struct fl_pg_chunk {
@@ -102,7 +105,7 @@
 	unsigned int gen;           /* free list generation */
 	struct fl_pg_chunk pg_chunk;/* page chunk cache */
 	unsigned int use_pages;     /* whether FL uses pages or sk_buffs */
-	unsigned int order;	    /* order of page allocations */
+	unsigned int order;         /* order of page allocations */
 	unsigned int alloc_size;    /* size of allocated buffer */
 	struct rx_desc *desc;       /* address of HW Rx descriptor ring */
 	struct rx_sw_desc *sdesc;   /* address of SW Rx descriptor ring */
@@ -112,6 +115,26 @@
 	unsigned long alloc_failed; /* # of times buffer allocation failed */
 };
 
+/* max concurrent LRO sessions per queue set */
+#define MAX_LRO_SES 8
+
+struct lro_session {
+	struct sk_buff *head;
+	struct sk_buff *tail;
+	u32 seq;
+	u16 iplen;
+	u16 mss;
+	__be16 vlan;
+	u8  npkts;
+};
+
+struct lro_state {
+	unsigned short enabled;
+	unsigned short active_idx;  /* index of most recently added session */
+	unsigned int nactive;       /* # of active sessions */
+	struct lro_session sess[MAX_LRO_SES];
+};
+
 /*
  * Bundle size for grouping offload RX packets for delivery to the stack.
  * Don't make this too big as we do prefetch on each packet in a bundle.
@@ -120,97 +143,111 @@
 
 struct rsp_desc;
 
-struct sge_rspq {		/* state for an SGE response queue */
-	unsigned int credits;	/* # of pending response credits */
-	unsigned int size;	/* capacity of response queue */
-	unsigned int cidx;	/* consumer index */
-	unsigned int gen;	/* current generation bit */
-	unsigned int polling;	/* is the queue serviced through NAPI? */
-	unsigned int holdoff_tmr;	/* interrupt holdoff timer in 100ns */
-	unsigned int next_holdoff;	/* holdoff time for next interrupt */
-	unsigned int rx_recycle_buf; /* whether recycling occurred
-					within current sop-eop */
-	struct rsp_desc *desc;	/* address of HW response ring */
-	dma_addr_t phys_addr;	/* physical address of the ring */
-	unsigned int cntxt_id;	/* SGE context id for the response q */
-	spinlock_t lock;	/* guards response processing */
-	struct sk_buff_head rx_queue; /* offload packet receive queue */
-	struct sk_buff *pg_skb; /* used to build frag list in napi handler */
-
+struct sge_rspq {                   /* state for an SGE response queue */
+	unsigned int credits;       /* # of pending response credits */
+	unsigned int size;          /* capacity of response queue */
+	unsigned int cidx;          /* consumer index */
+	unsigned int gen;           /* current generation bit */
+	unsigned long flags;       /* is the queue serviced through NAPI? */
+	unsigned int holdoff_tmr;   /* interrupt holdoff timer in 100ns */
+	unsigned int next_holdoff;  /* holdoff time for next interrupt */
+	unsigned int rx_recycle_buf; /* whether recycling occurred within current sop-eop */
+	struct rsp_desc *desc;      /* address of HW response ring */
+	dma_addr_t   phys_addr;     /* physical address of the ring */
+	unsigned int cntxt_id;      /* SGE context id for the response q */
+	spinlock_t   lock;          /* guards response processing */
+	struct sk_buff *rx_head;    /* offload packet receive queue head */
+	struct sk_buff *rx_tail;    /* offload packet receive queue tail */
+	struct sk_buff *pg_skb;     /* skb for building frag list in napi response handler */
 	unsigned long offload_pkts;
 	unsigned long offload_bundles;
-	unsigned long eth_pkts;	/* # of ethernet packets */
-	unsigned long pure_rsps;	/* # of pure (non-data) responses */
-	unsigned long imm_data;	/* responses with immediate data */
-	unsigned long rx_drops;	/* # of packets dropped due to no mem */
-	unsigned long async_notif; /* # of asynchronous notification events */
-	unsigned long empty;	/* # of times queue ran out of credits */
-	unsigned long nomem;	/* # of responses deferred due to no mem */
-	unsigned long unhandled_irqs;	/* # of spurious intrs */
+	unsigned long eth_pkts;     /* # of ethernet packets */
+	unsigned long pure_rsps;    /* # of pure (non-data) responses */
+	unsigned long imm_data;     /* responses with immediate data */
+	unsigned long rx_drops;     /* # of packets dropped due to no mem */
+	unsigned long async_notif;  /* # of asynchronous notification events */
+	unsigned long empty;        /* # of times queue ran out of credits */
+	unsigned long nomem;        /* # of responses deferred due to no mem */
+	unsigned long unhandled_irqs; /* # of spurious intrs */
 	unsigned long starved;
 	unsigned long restarted;
 };
 
 struct tx_desc;
 struct tx_sw_desc;
+struct eth_coalesce_sw_desc;
 
-struct sge_txq {		/* state for an SGE Tx queue */
-	unsigned long flags;	/* HW DMA fetch status */
-	unsigned int in_use;	/* # of in-use Tx descriptors */
-	unsigned int size;	/* # of descriptors */
-	unsigned int processed;	/* total # of descs HW has processed */
-	unsigned int cleaned;	/* total # of descs SW has reclaimed */
-	unsigned int stop_thres;	/* SW TX queue suspend threshold */
-	unsigned int cidx;	/* consumer index */
-	unsigned int pidx;	/* producer index */
-	unsigned int gen;	/* current value of generation bit */
-	unsigned int unacked;	/* Tx descriptors used since last COMPL */
-	struct tx_desc *desc;	/* address of HW Tx descriptor ring */
-	struct tx_sw_desc *sdesc;	/* address of SW Tx descriptor ring */
-	spinlock_t lock;	/* guards enqueueing of new packets */
-	unsigned int token;	/* WR token */
-	dma_addr_t phys_addr;	/* physical address of the ring */
-	struct sk_buff_head sendq;	/* List of backpressured offload packets */
-	struct tasklet_struct qresume_tsk;	/* restarts the queue */
-	unsigned int cntxt_id;	/* SGE context id for the Tx q */
-	unsigned long stops;	/* # of times q has been stopped */
-	unsigned long restarts;	/* # of queue restarts */
+struct sge_txq {                    /* state for an SGE Tx queue */
+	unsigned long flags;        /* HW DMA fetch status */
+	unsigned int  in_use;       /* # of in-use Tx descriptors */
+	unsigned int  size;         /* # of descriptors */
+	unsigned int  processed;    /* total # of descs HW has processed */
+	unsigned int  cleaned;      /* total # of descs SW has reclaimed */
+	unsigned int  stop_thres;   /* SW TX queue suspend threshold */
+	unsigned int  cidx;         /* consumer index */
+	unsigned int  pidx;         /* producer index */
+	unsigned int  gen;          /* current value of generation bit */
+	unsigned int  unacked;      /* Tx descriptors used since last COMPL */
+	struct tx_desc *desc;       /* address of HW Tx descriptor ring */
+	struct tx_sw_desc *sdesc;   /* address of SW Tx descriptor ring */
+	unsigned int eth_coalesce_idx;  /* idx of the next coalesce pkt */
+	unsigned int eth_coalesce_bytes; /* total lentgh of coalesced pkts */
+	struct eth_coalesce_sw_desc *eth_coalesce_sdesc;
+	spinlock_t    lock;         /* guards enqueueing of new packets */
+	unsigned int  token;        /* WR token */
+	dma_addr_t    phys_addr;    /* physical address of the ring */
+	struct sk_buff_head sendq;  /* List of backpressured offload packets */
+	struct tasklet_struct qresume_tsk; /* restarts the queue */
+	unsigned int  cntxt_id;     /* SGE context id for the Tx q */
+	unsigned long stops;        /* # of times q has been stopped */
+	unsigned long restarts;     /* # of queue restarts */
+	unsigned long tx_pkts;      /* # of transmitted pkts */
+	unsigned int sched_max;
 };
 
-enum {				/* per port SGE statistics */
-	SGE_PSTAT_TSO,		/* # of TSO requests */
-	SGE_PSTAT_RX_CSUM_GOOD,	/* # of successful RX csum offloads */
-	SGE_PSTAT_TX_CSUM,	/* # of TX checksum offloads */
-	SGE_PSTAT_VLANEX,	/* # of VLAN tag extractions */
-	SGE_PSTAT_VLANINS,	/* # of VLAN tag insertions */
+enum {                              /* per port SGE statistics */
+	SGE_PSTAT_TSO,              /* # of TSO requests */
+	SGE_PSTAT_RX_CSUM_GOOD,     /* # of successful RX csum offloads */
+	SGE_PSTAT_TX_CSUM,          /* # of TX checksum offloads */
+	SGE_PSTAT_VLANEX,           /* # of VLAN tag extractions */
+	SGE_PSTAT_VLANINS,          /* # of VLAN tag insertions */
+	SGE_PSTAT_TX_COALESCE_WR,   /* # of TX Coalesce Work Requests */
+	SGE_PSTAT_TX_COALESCE_PKT,  /* # of TX Coalesced packets */
+	SGE_PSTAT_LRO,              /* # of completed LRO packets */
+	SGE_PSTAT_LRO_SKB,          /* # of sk_buffs added to LRO sessions */
+	SGE_PSTAT_LRO_PG,           /* # of page chunks added to LRO sessions */
+	SGE_PSTAT_LRO_ACK,          /* # of pure ACKs fully merged by LRO */
+	SGE_PSTAT_LRO_OVFLOW,       /* # of LRO session overflows */
+	SGE_PSTAT_LRO_COLSN,        /* # of LRO hash collisions */
 
-	SGE_PSTAT_MAX		/* must be last */
+	SGE_PSTAT_MAX               /* must be last */
 };
 
-struct napi_gro_fraginfo;
-
-struct sge_qset {		/* an SGE queue set */
+struct sge_qset {                   /* an SGE queue set */
 	struct adapter *adap;
+#if defined(NAPI_UPDATE)
 	struct napi_struct napi;
+#endif
 	struct sge_rspq rspq;
-	struct sge_fl fl[SGE_RXQ_PER_SET];
-	struct sge_txq txq[SGE_TXQ_PER_SET];
-	int nomem;
-	int lro_enabled;
-	void *lro_va;
-	struct net_device *netdev;
-	struct netdev_queue *tx_q;	/* associated netdev TX queue */
-	unsigned long txq_stopped;	/* which Tx queues are stopped */
-	struct timer_list tx_reclaim_timer;	/* reclaims TX buffers */
-	struct timer_list rx_reclaim_timer;	/* reclaims RX buffers */
+	struct sge_fl   fl[SGE_RXQ_PER_SET];
+	struct lro_state lro;
+	struct sge_txq  txq[SGE_TXQ_PER_SET];
+	struct net_device *netdev;            /* associated net device */
+	struct netdev_queue *tx_q;            /* associated netdev TX queue */
+	unsigned long txq_stopped;            /* which Tx queues are stopped */
+	struct timer_list tx_reclaim_timer;   /* reclaims TX buffers */
+	struct timer_list rx_reclaim_timer;
 	unsigned long port_stats[SGE_PSTAT_MAX];
 } ____cacheline_aligned;
 
 struct sge {
 	struct sge_qset qs[SGE_QSETS];
-	spinlock_t reg_lock;	/* guards non-atomic SGE registers (eg context) */
+	unsigned int nqsets; /* # of active queue sets */
+	spinlock_t reg_lock; /* guards non-atomic SGE registers (eg context) */
 };
 
+struct filter_info;
+
 struct adapter {
 	struct t3cdev tdev;
 	struct list_head adapter_list;
@@ -224,6 +261,7 @@
 	int msg_enable;
 	unsigned int mmio_len;
 
+	struct timer_list watchdog_timer;
 	struct adapter_params params;
 	unsigned int slow_intr_mask;
 	unsigned long irq_stats[IRQ_NUM_STATS];
@@ -234,6 +272,10 @@
 		char desc[22];
 	} msix_info[SGE_QSETS + 1];
 
+#ifdef T3_TRACE
+	struct trace_buf *tb[SGE_QSETS];
+#endif
+
 	/* T3 modules */
 	struct sge sge;
 	struct mc7 pmrx;
@@ -242,84 +284,230 @@
 	struct mc5 mc5;
 
 	struct net_device *port[MAX_NPORTS];
+	u8 rxpkt_map[8];        /* maps RX_PKT interface values to port ids */
+	u8 rrss_map[SGE_QSETS]; /* reverse RSS map table */
+
+	atomic_t filter_toe_mode;	/* filter / TOE exclusion switch */
+	struct filter_info *filters;	/* software copy of hardware filters */
+
 	unsigned int check_task_cnt;
 	struct delayed_work adap_check_task;
 	struct work_struct ext_intr_handler_task;
 	struct work_struct fatal_error_handler_task;
 	struct work_struct link_fault_handler_task;
 
+	struct work_struct db_full_task;
+	struct work_struct db_empty_task;
+	struct work_struct db_drop_task;
+
+#if !defined(NAPI_UPDATE)
+	/*
+	 * Dummy netdevices are needed when using multiple receive queues with
+	 * NAPI as each netdevice can service only one queue.
+	 */
+	struct net_device *dummy_netdev[SGE_QSETS - 1];
+#endif
+	u32 t3_config_space[16]; /* For old kernels only */
+
 	struct dentry *debugfs_root;
 
-	struct mutex mdio_lock;
+	spinlock_t mdio_lock;
+	spinlock_t elmer_lock;
 	spinlock_t stats_lock;
 	spinlock_t work_lock;
-
-	struct sk_buff *nofail_skb;
 };
 
-static inline u32 t3_read_reg(struct adapter *adapter, u32 reg_addr)
+/* values for filter_toe_mode interlock */
+enum {
+	CXGB3_FTM_NONE		= 0,	/* no filters or TOE activated */
+	CXGB3_FTM_FILTER	= 1,	/* filters used */
+	CXGB3_FTM_TOE		= 2,	/* TOE activated */
+};
+
+int cxgb3_filter_toe_mode(struct adapter *, int);
+
+#include "cxgb3_compat.h"
+
+#define MDIO_LOCK(adapter) spin_lock(&(adapter)->mdio_lock)
+#define MDIO_UNLOCK(adapter) spin_unlock(&(adapter)->mdio_lock)
+
+#define ELMR_LOCK(adapter) spin_lock(&(adapter)->elmer_lock)
+#define ELMR_UNLOCK(adapter) spin_unlock(&(adapter)->elmer_lock)
+
+/**
+ * t3_read_reg - read a HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ *
+ * Returns the 32-bit value of the given HW register.
+ */
+static inline u32 t3_read_reg(adapter_t *adapter, u32 reg_addr)
 {
 	u32 val = readl(adapter->regs + reg_addr);
 
-	CH_DBG(adapter, MMIO, "read register 0x%x value 0x%x\n", reg_addr, val);
+	CH_DBG(adapter, MMIO, "read register 0x%x value 0x%x\n", reg_addr,
+	       val);
 	return val;
 }
 
-static inline void t3_write_reg(struct adapter *adapter, u32 reg_addr, u32 val)
+/**
+ * t3_write_reg - write a HW register
+ * @adapter: the adapter
+ * @reg_addr: the register address
+ * @val: the value to write
+ *
+ * Write a 32-bit value into the given HW register.
+ */
+static inline void t3_write_reg(adapter_t *adapter, u32 reg_addr, u32 val)
 {
-	CH_DBG(adapter, MMIO, "setting register 0x%x to 0x%x\n", reg_addr, val);
+	CH_DBG(adapter, MMIO, "setting register 0x%x to 0x%x\n", reg_addr,
+	       val);
 	writel(val, adapter->regs + reg_addr);
 }
 
+/**
+ * t3_os_pci_write_config_4 - 32-bit write to PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: the value to write
+ *
+ * Write a 32-bit value into the given register in PCI config space.
+ */
+static inline void t3_os_pci_write_config_4(adapter_t *adapter, int reg,
+					    u32 val)
+{
+	pci_write_config_dword(adapter->pdev, reg, val);
+}
+
+/**
+ * t3_os_pci_read_config_4 - read a 32-bit value from PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: where to store the value read
+ *
+ * Read a 32-bit value from the given register in PCI config space.
+ */
+static inline void t3_os_pci_read_config_4(adapter_t *adapter, int reg,
+					   u32 *val)
+{
+	pci_read_config_dword(adapter->pdev, reg, val);
+}
+
+/**
+ * t3_os_pci_write_config_2 - 16-bit write to PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: the value to write
+ *
+ * Write a 16-bit value into the given register in PCI config space.
+ */
+static inline void t3_os_pci_write_config_2(adapter_t *adapter, int reg,
+					    u16 val)
+{
+	pci_write_config_word(adapter->pdev, reg, val);
+}
+
+/**
+ * t3_os_pci_read_config_2 - read a 16-bit value from PCI config space
+ * @adapter: the adapter
+ * @reg: the register address
+ * @val: where to store the value read
+ *
+ * Read a 16-bit value from the given register in PCI config space.
+ */
+static inline void t3_os_pci_read_config_2(adapter_t *adapter, int reg,
+					   u16 *val)
+{
+	pci_read_config_word(adapter->pdev, reg, val);
+}
+
+/**
+ * t3_os_find_pci_capability - lookup a capability in the PCI capability list
+ * @adapter: the adapter
+ * @cap: the capability
+ *
+ * Return the address of the given capability within the PCI capability list.
+ */
+static inline int t3_os_find_pci_capability(adapter_t *adapter, int cap)
+{
+	return pci_find_capability(adapter->pdev, cap);
+}
+
+/**
+ * port_name - return the string name of a port
+ * @adapter: the adapter
+ * @port_idx: the port index
+ *
+ * Return the string name of the selected port.
+ */
+static inline const char *port_name(adapter_t *adapter, unsigned int port_idx)
+{
+	return adapter->port[port_idx]->name;
+}
+
+/**
+ * t3_os_set_hw_addr - store a port's MAC address in SW
+ * @adapter: the adapter
+ * @port_idx: the port index
+ * @hw_addr: the Ethernet address
+ *
+ * Store the Ethernet address of the given port in SW.  Called by the common
+ * code when it retrieves a port's Ethernet address from EEPROM.
+ */
+static inline void t3_os_set_hw_addr(adapter_t *adapter, int port_idx,
+				     u8 hw_addr[])
+{
+	memcpy(adapter->port[port_idx]->dev_addr, hw_addr, ETH_ALEN);
+#ifdef ETHTOOL_GPERMADDR
+	memcpy(adapter->port[port_idx]->perm_addr, hw_addr, ETH_ALEN);
+#endif
+}
+
+/**
+ * adap2pinfo - return the port_info of a port
+ * @adap: the adapter
+ * @idx: the port index
+ *
+ * Return the port_info structure for the port of the given index.
+ */
 static inline struct port_info *adap2pinfo(struct adapter *adap, int idx)
 {
 	return netdev_priv(adap->port[idx]);
 }
 
-static inline int phy2portid(struct cphy *phy)
-{
-	struct adapter *adap = phy->adapter;
-	struct port_info *port0 = adap2pinfo(adap, 0);
-
-	return &port0->phy == phy ? 0 : 1;
-}
-
 #define OFFLOAD_DEVMAP_BIT 15
 
 #define tdev2adap(d) container_of(d, struct adapter, tdev)
 
-static inline int offload_running(struct adapter *adapter)
+static inline int offload_running(adapter_t *adapter)
 {
 	return test_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map);
 }
 
 int t3_offload_tx(struct t3cdev *tdev, struct sk_buff *skb);
 
-void t3_os_ext_intr_handler(struct adapter *adapter);
-void t3_os_link_changed(struct adapter *adapter, int port_id, int link_status,
-			int speed, int duplex, int fc);
+void t3_os_ext_intr_handler(adapter_t *adapter);
+void t3_os_link_changed(adapter_t *adapter, int port_id, int link_status,
+			int speed, int duplex, int fc, int mac_was_reset);
 void t3_os_phymod_changed(struct adapter *adap, int port_id);
-void t3_os_link_fault(struct adapter *adapter, int port_id, int state);
-void t3_os_link_fault_handler(struct adapter *adapter, int port_id);
+void t3_os_link_fault_handler(adapter_t *adapter, int port_id);
 
-void t3_sge_start(struct adapter *adap);
-void t3_sge_stop(struct adapter *adap);
+void t3_sge_start(adapter_t *adap);
+void t3_sge_stop(adapter_t *adap);
 void t3_start_sge_timers(struct adapter *adap);
 void t3_stop_sge_timers(struct adapter *adap);
-void t3_free_sge_resources(struct adapter *adap);
-void t3_sge_err_intr_handler(struct adapter *adapter);
-irq_handler_t t3_intr_handler(struct adapter *adap, int polling);
-netdev_tx_t t3_eth_xmit(struct sk_buff *skb, struct net_device *dev);
-int t3_mgmt_tx(struct adapter *adap, struct sk_buff *skb);
+void t3_free_sge_resources(adapter_t *adap);
+void t3_sge_err_intr_handler(adapter_t *adapter);
+int t3_eth_xmit(struct sk_buff *skb, struct net_device *dev);
+int t3_mgmt_tx(adapter_t *adap, struct sk_buff *skb);
 void t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p);
-int t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,
-		      int irq_vec_idx, const struct qset_params *p,
-		      int ntxq, struct net_device *dev,
+int t3_sge_alloc_qset(adapter_t *adapter, unsigned int id, int nports,
+	       	      int irq_vec_idx, const struct qset_params *p,
+		      int ntxq, struct net_device *netdev,
 		      struct netdev_queue *netdevq);
 int t3_get_desc(const struct sge_qset *qs, unsigned int qnum, unsigned int idx,
 		unsigned char *data);
-irqreturn_t t3_sge_intr_msix(int irq, void *cookie);
+int t3_get_edc_fw(struct cphy *phy, int edc_idx);
+extern struct workqueue_struct *cxgb3_wq;
 
-int t3_get_edc_fw(struct cphy *phy, int edc_idx, int size);
-
-#endif				/* __T3_ADAPTER_H__ */
+#endif /* __T3_ADAPTER_H__ */
diff --git a/drivers/net/cxgb3/ael1002.c b/drivers/net/cxgb3/ael1002.c
--- a/drivers/net/cxgb3/ael1002.c
+++ b/drivers/net/cxgb3/ael1002.c
@@ -1,55 +1,54 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include "common.h"
 #include "regs.h"
 
 enum {
-	AEL100X_TX_CONFIG1 = 0xc002,
+	PMD_RSD     = 10,   /* PMA/PMD receive signal detect register */
+	PCS_STAT1_X = 24,   /* 10GBASE-X PCS status 1 register */
+	PCS_STAT1_R = 32,   /* 10GBASE-R PCS status 1 register */
+	XS_LN_STAT  = 24    /* XS lane status register */
+};
+
+enum {
+	AEL100X_TX_DISABLE  = 9,
+	AEL100X_TX_CONFIG1  = 0xc002,
+
 	AEL1002_PWR_DOWN_HI = 0xc011,
 	AEL1002_PWR_DOWN_LO = 0xc012,
-	AEL1002_XFI_EQL = 0xc015,
-	AEL1002_LB_EN = 0xc017,
-	AEL_OPT_SETTINGS = 0xc017,
-	AEL_I2C_CTRL = 0xc30a,
-	AEL_I2C_DATA = 0xc30b,
-	AEL_I2C_STAT = 0xc30c,
-	AEL2005_GPIO_CTRL = 0xc214,
-	AEL2005_GPIO_STAT = 0xc215,
+	AEL1002_XFI_EQL     = 0xc015,
+	AEL1002_LB_EN       = 0xc017,
+
+	AEL_OPT_SETTINGS    = 0xc017,
+	AEL_I2C_CTRL        = 0xc30a,
+	AEL_I2C_DATA        = 0xc30b,
+	AEL_I2C_STAT        = 0xc30c,
+
+	AEL2005_GPIO_CTRL   = 0xc214,
+	AEL2005_GPIO_STAT   = 0xc215,
 
 	AEL2020_GPIO_INTR   = 0xc103,	/* Latch High (LH) */
 	AEL2020_GPIO_CTRL   = 0xc108,	/* Store Clear (SC) */
 	AEL2020_GPIO_STAT   = 0xc10c,	/* Read Only (RO) */
 	AEL2020_GPIO_CFG    = 0xc110,	/* Read Write (RW) */
 
+	/* there are four GPIO pins per PHY channel */
+	/*
+	 * Erratum: GPIO "ports" 2 & 3 are swapped relative to the
+	 * documentation.  "port 2" was supposed to manage GPIO X_0
+	 * and "port 3" was supposed to manage GPIO X_1.  Instead
+	 * port 2 manages GPIO X_1 and port 3 manages GPIO X_0 ...
+	 */
+	/* name/function      port         default at reset */
 	AEL2020_GPIO_SDA    = 0,	/* IN: i2c serial data */
 	AEL2020_GPIO_MODDET = 1,	/* IN: Module Detect */
 	AEL2020_GPIO_0      = 3,	/* IN: unassigned */
@@ -70,7 +69,7 @@
 	phy_transtype_unknown = 0,
 	phy_transtype_sfp     = 3,
 	phy_transtype_xfp     = 6,
-};
+};		
 
 #define AEL2005_MODDET_IRQ 4
 
@@ -81,14 +80,16 @@
 	unsigned short set_bits;
 };
 
+static int ael2xxx_get_module_type(struct cphy *phy, int delay_ms);
+
 static int set_phy_regs(struct cphy *phy, const struct reg_val *rv)
 {
 	int err;
 
 	for (err = 0; rv->mmd_addr && !err; rv++) {
 		if (rv->clear_bits == 0xffff)
-			err = t3_mdio_write(phy, rv->mmd_addr, rv->reg_addr,
-					    rv->set_bits);
+			err = mdio_write(phy, rv->mmd_addr, rv->reg_addr,
+					 rv->set_bits);
 		else
 			err = t3_mdio_change_bits(phy, rv->mmd_addr,
 						  rv->reg_addr, rv->clear_bits,
@@ -99,8 +100,7 @@
 
 static void ael100x_txon(struct cphy *phy)
 {
-	int tx_on_gpio =
-		phy->mdio.prtad == 0 ? F_GPIO7_OUT_VAL : F_GPIO2_OUT_VAL;
+	int tx_on_gpio = phy->addr == 0 ? F_GPIO7_OUT_VAL : F_GPIO2_OUT_VAL;
 
 	msleep(100);
 	t3_set_reg_field(phy->adapter, A_T3DBG_GPIO_EN, 0, tx_on_gpio);
@@ -115,53 +115,148 @@
 	int i, err;
 	unsigned int stat, data;
 
-	err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL_I2C_CTRL,
-			    (dev_addr << 8) | (1 << 8) | word_addr);
+	err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL_I2C_CTRL,
+			 (dev_addr << 8) | (1 << 8) | word_addr);
 	if (err)
 		return err;
 
 	for (i = 0; i < 200; i++) {
 		msleep(1);
-		err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL_I2C_STAT, &stat);
+		err = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL_I2C_STAT, &stat);
 		if (err)
 			return err;
 		if ((stat & 3) == 1) {
-			err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL_I2C_DATA,
-					   &data);
+			err = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL_I2C_DATA,
+					&data);
 			if (err)
 				return err;
 			return data >> 8;
 		}
 	}
 	CH_WARN(phy->adapter, "PHY %u i2c read of dev.addr %#x.%#x timed out\n",
-		phy->mdio.prtad, dev_addr, word_addr);
+		phy->addr, dev_addr, word_addr);
 	return -ETIMEDOUT;
 }
 
+/*
+ * Write an 8-bit word to a device attached to the PHY's i2c bus.
+ */
+static int ael_i2c_wr(struct cphy *phy, int dev_addr, int word_addr, int data)
+{
+	int i, err;
+	unsigned int stat;
+
+	err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL_I2C_DATA, data);
+	if (err)
+		return err;
+
+	err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL_I2C_CTRL,
+			 (dev_addr << 8) | word_addr);
+	if (err)
+		return err;
+
+	for (i = 0; i < 200; i++) {
+		msleep(1);
+		err = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL_I2C_STAT, &stat);
+		if (err)
+			return err;
+		if ((stat & 3) == 1)
+			return 0;
+	}
+	CH_WARN(phy->adapter, "PHY %u i2c Write of dev.addr %#x.%#x = %#x timed out\n",
+		phy->addr, dev_addr, word_addr, data);
+	return -ETIMEDOUT;
+}
+
+static int get_phytrans_type(struct cphy *phy)
+{
+	int v;
+
+	v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 0);
+	if (v < 0)
+		return phy_transtype_unknown;
+
+	return v;
+}
+
+static int ael_laser_down(struct cphy *phy, int enable)
+{
+	int v, dev_addr;
+
+	v = get_phytrans_type(phy);
+	if (v < 0)
+		return v;
+
+	if (v == phy_transtype_sfp) {
+		/* Check SFF Soft TX disable is supported */
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 93);
+		if (v < 0)
+			return v;
+
+		v &= 0x40;
+		if (!v)
+			return v;
+
+		dev_addr = SFF_DEV_ADDR;	
+	} else if (v == phy_transtype_xfp)
+		dev_addr = MODULE_DEV_ADDR;
+	else
+		return v;
+
+	v = ael_i2c_rd(phy, dev_addr, 110);
+	if (v < 0)
+		return v;
+
+	if (enable)
+		v |= 0x40;
+	else
+		v &= ~0x40;
+
+	v = ael_i2c_wr(phy, dev_addr, 110, v);
+
+	return v;
+}
+
 static int ael1002_power_down(struct cphy *phy, int enable)
 {
 	int err;
 
-	err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, MDIO_PMA_TXDIS, !!enable);
+	err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL100X_TX_DISABLE, !!enable);
 	if (!err)
-		err = mdio_set_flag(&phy->mdio, phy->mdio.prtad,
-				    MDIO_MMD_PMAPMD, MDIO_CTRL1,
-				    MDIO_CTRL1_LPOWER, enable);
+		err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR,
+					  BMCR_PDOWN, enable ? BMCR_PDOWN : 0);
 	return err;
 }
 
+static int ael1002_get_module_type(struct cphy *phy, int delay_ms)
+{
+	int v;
+
+	if (delay_ms)
+		msleep(delay_ms);
+
+	v = ael2xxx_get_module_type(phy, delay_ms);
+
+	return (v == -ETIMEDOUT ? phy_modtype_none : v);
+}
+
 static int ael1002_reset(struct cphy *phy, int wait)
 {
 	int err;
 
 	if ((err = ael1002_power_down(phy, 0)) ||
-	    (err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL100X_TX_CONFIG1, 1)) ||
-	    (err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL1002_PWR_DOWN_HI, 0)) ||
-	    (err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL1002_PWR_DOWN_LO, 0)) ||
-	    (err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL1002_XFI_EQL, 0x18)) ||
-	    (err = t3_mdio_change_bits(phy, MDIO_MMD_PMAPMD, AEL1002_LB_EN,
+	    (err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL100X_TX_CONFIG1, 1)) ||
+	    (err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL1002_PWR_DOWN_HI, 0)) ||
+	    (err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL1002_PWR_DOWN_LO, 0)) ||
+	    (err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL1002_XFI_EQL, 0x18)) ||
+	    (err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, AEL1002_LB_EN,
 				       0, 1 << 5)))
 		return err;
+
+	err = ael1002_get_module_type(phy, 300);
+	if (err >= 0)
+		phy->modtype = err;
+
 	return 0;
 }
 
@@ -178,15 +273,12 @@
 {
 	if (link_ok) {
 		unsigned int stat0, stat1, stat2;
-		int err = t3_mdio_read(phy, MDIO_MMD_PMAPMD,
-				       MDIO_PMA_RXDET, &stat0);
+		int err = mdio_read(phy, MDIO_DEV_PMA_PMD, PMD_RSD, &stat0);
 
 		if (!err)
-			err = t3_mdio_read(phy, MDIO_MMD_PCS,
-					   MDIO_PCS_10GBRT_STAT1, &stat1);
+			err = mdio_read(phy, MDIO_DEV_PCS, PCS_STAT1_R, &stat1);
 		if (!err)
-			err = t3_mdio_read(phy, MDIO_MMD_PHYXS,
-					   MDIO_PHYXS_LNSTAT, &stat2);
+			err = mdio_read(phy, MDIO_DEV_XGXS, XS_LN_STAT, &stat2);
 		if (err)
 			return err;
 		*link_ok = (stat0 & stat1 & (stat2 >> 12)) & 1;
@@ -198,49 +290,124 @@
 	return 0;
 }
 
+#ifdef C99_NOT_SUPPORTED
 static struct cphy_ops ael1002_ops = {
-	.reset = ael1002_reset,
-	.intr_enable = ael1002_intr_noop,
-	.intr_disable = ael1002_intr_noop,
-	.intr_clear = ael1002_intr_noop,
-	.intr_handler = ael1002_intr_noop,
+	ael1002_reset,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	get_link_status_r,
+	ael1002_power_down,
+};
+#else
+static struct cphy_ops ael1002_ops = {
+	.reset           = ael1002_reset,
+	.intr_enable     = ael1002_intr_noop,
+	.intr_disable    = ael1002_intr_noop,
+	.intr_clear      = ael1002_intr_noop,
+	.intr_handler    = ael1002_intr_noop,
 	.get_link_status = get_link_status_r,
-	.power_down = ael1002_power_down,
-	.mmds = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
+	.power_down      = ael1002_power_down,
 };
+#endif
 
-int t3_ael1002_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops)
+int t3_ael1002_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops)
 {
-	cphy_init(phy, adapter, phy_addr, &ael1002_ops, mdio_ops,
+	int err;
+	struct cphy *phy = &pinfo->phy;
+
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &ael1002_ops, mdio_ops,
 		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_FIBRE,
-		   "10GBASE-R");
+		  "10GBASE-R");
 	ael100x_txon(phy);
+	ael_laser_down(phy, 0);
+
+	err = ael1002_get_module_type(phy, 0);
+	if (err >= 0)
+		phy->modtype = err;
+
 	return 0;
 }
 
 static int ael1006_reset(struct cphy *phy, int wait)
 {
-	return t3_phy_reset(phy, MDIO_MMD_PMAPMD, wait);
+	int err;
+
+	err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, wait);
+	if (err)
+		return err;
+
+	t3_set_reg_field(phy->adapter, A_T3DBG_GPIO_EN, 
+			 F_GPIO6_OUT_VAL, 0);
+
+	msleep(125);
+
+	t3_set_reg_field(phy->adapter, A_T3DBG_GPIO_EN, 
+			 F_GPIO6_OUT_VAL, F_GPIO6_OUT_VAL);
+
+	msleep(125);
+
+	err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, wait);
+	if (err)
+		return err;
+
+	msleep(125);
+
+	err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR, 1, 1);
+	if (err)
+		return err;
+	
+	msleep(125);
+
+	err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR, 1, 0);
+
+	return err;
+	   
 }
 
+#ifdef C99_NOT_SUPPORTED
 static struct cphy_ops ael1006_ops = {
-	.reset = ael1006_reset,
-	.intr_enable = t3_phy_lasi_intr_enable,
-	.intr_disable = t3_phy_lasi_intr_disable,
-	.intr_clear = t3_phy_lasi_intr_clear,
-	.intr_handler = t3_phy_lasi_intr_handler,
+	ael1006_reset,
+	t3_phy_lasi_intr_enable,
+	t3_phy_lasi_intr_disable,
+	t3_phy_lasi_intr_clear,
+	t3_phy_lasi_intr_handler,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	get_link_status_r,
+	ael1002_power_down,
+};
+#else
+static struct cphy_ops ael1006_ops = {
+	.reset           = ael1006_reset,
+	.intr_enable     = t3_phy_lasi_intr_enable,
+	.intr_disable    = t3_phy_lasi_intr_disable,
+	.intr_clear      = t3_phy_lasi_intr_clear,
+	.intr_handler    = t3_phy_lasi_intr_handler,
 	.get_link_status = get_link_status_r,
-	.power_down = ael1002_power_down,
-	.mmds = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
+	.power_down      = ael1002_power_down,
 };
+#endif
 
-int t3_ael1006_phy_prep(struct cphy *phy, struct adapter *adapter,
-			     int phy_addr, const struct mdio_ops *mdio_ops)
+int t3_ael1006_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops)
 {
-	cphy_init(phy, adapter, phy_addr, &ael1006_ops, mdio_ops,
+	struct cphy *phy = &pinfo->phy;
+
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &ael1006_ops, mdio_ops,
 		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_FIBRE,
-		   "10GBASE-SR");
+		  "10GBASE-SR");
+	phy->modtype = phy_modtype_sr;
 	ael100x_txon(phy);
 	return 0;
 }
@@ -255,35 +422,76 @@
 	if (delay_ms)
 		msleep(delay_ms);
 
-	/* see SFF-8472 for below */
-	v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 3);
-	if (v < 0)
-		return v;
+	v = get_phytrans_type(phy);
+	if (v == phy_transtype_sfp) {
+		/* SFP: see SFF-8472 for below */
 
-	if (v == 0x10)
-		return phy_modtype_sr;
-	if (v == 0x20)
-		return phy_modtype_lr;
-	if (v == 0x40)
-		return phy_modtype_lrm;
-
-	v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 6);
-	if (v < 0)
-		return v;
-	if (v != 4)
-		goto unknown;
-
-	v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 10);
-	if (v < 0)
-		return v;
-
-	if (v & 0x80) {
-		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 0x12);
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 3);
 		if (v < 0)
 			return v;
-		return v > 10 ? phy_modtype_twinax_long : phy_modtype_twinax;
+
+		if (v == 0x1)
+			goto twinax;
+		if (v == 0x10)
+			return phy_modtype_sr;
+		if (v == 0x20)
+			return phy_modtype_lr;
+		if (v == 0x40)
+			return phy_modtype_lrm;
+
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 8);
+		if (v < 0)
+			return v;
+		if (v == 4) {
+			v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 60);
+			if (v < 0)
+				return v;
+			if (v & 0x1)
+				goto twinax;
+		}
+
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 6);
+		if (v < 0)
+			return v;
+		if (v != 4)
+			return phy_modtype_unknown;
+
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 10);
+		if (v < 0)
+			return v;
+
+		if (v & 0x80) {
+twinax:
+			v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 0x12);
+			if (v < 0)
+				return v;
+			return v > 10 ? phy_modtype_twinax_long :
+			    phy_modtype_twinax;
+		}
+	} else if (v == phy_transtype_xfp) {
+		/* XFP: See INF-8077i for details. */
+
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 127);
+		if (v < 0)
+			return v;
+
+		if (v != 1) {
+			/* XXX: set page select to table 1 yourself */
+			return phy_modtype_unknown;
+		}
+
+		v = ael_i2c_rd(phy, MODULE_DEV_ADDR, 131);
+		if (v < 0)
+			return v;
+		v &= 0xf0;
+		if (v == 0x10)
+			return phy_modtype_lrm;
+		if (v == 0x40)
+			return phy_modtype_lr;
+		if (v == 0x80)
+			return phy_modtype_sr;
 	}
-unknown:
+
 	return phy_modtype_unknown;
 }
 
@@ -293,13 +501,12 @@
 static int ael2005_setup_sr_edc(struct cphy *phy)
 {
 	static struct reg_val regs[] = {
-		{ MDIO_MMD_PMAPMD, 0xc003, 0xffff, 0x181 },
-		{ MDIO_MMD_PMAPMD, 0xc010, 0xffff, 0x448a },
-		{ MDIO_MMD_PMAPMD, 0xc04a, 0xffff, 0x5200 },
+		{ MDIO_DEV_PMA_PMD, 0xc003, 0xffff, 0x181 },
+		{ MDIO_DEV_PMA_PMD, 0xc010, 0xffff, 0x448a },
+		{ MDIO_DEV_PMA_PMD, 0xc04a, 0xffff, 0x5200 },
 		{ 0, 0, 0, 0 }
 	};
-
-	int i, err;
+	int i, err, size;
 
 	err = set_phy_regs(phy, regs);
 	if (err)
@@ -308,15 +515,14 @@
 	msleep(50);
 
 	if (phy->priv != edc_sr)
-		err = t3_get_edc_fw(phy, EDC_OPT_AEL2005,
-				    EDC_OPT_AEL2005_SIZE);
-	if (err)
-		return err;
+		size = t3_get_edc_fw(phy, EDC_OPT_AEL2005);
+	if (size < 0)
+		return size;
 
-	for (i = 0; i <  EDC_OPT_AEL2005_SIZE / sizeof(u16) && !err; i += 2)
-		err = t3_mdio_write(phy, MDIO_MMD_PMAPMD,
-				    phy->phy_cache[i],
-				    phy->phy_cache[i + 1]);
+	for (i = 0; i <  size / sizeof(u16) && !err; i += 2)
+		err = mdio_write(phy, MDIO_DEV_PMA_PMD,
+				 phy->phy_cache[i],
+				 phy->phy_cache[i + 1]);
 	if (!err)
 		phy->priv = edc_sr;
 	return err;
@@ -325,15 +531,15 @@
 static int ael2005_setup_twinax_edc(struct cphy *phy, int modtype)
 {
 	static struct reg_val regs[] = {
-		{ MDIO_MMD_PMAPMD, 0xc04a, 0xffff, 0x5a00 },
+		{ MDIO_DEV_PMA_PMD, 0xc04a, 0xffff, 0x5a00 },
 		{ 0, 0, 0, 0 }
 	};
 	static struct reg_val preemphasis[] = {
-		{ MDIO_MMD_PMAPMD, 0xc014, 0xffff, 0xfe16 },
-		{ MDIO_MMD_PMAPMD, 0xc015, 0xffff, 0xa000 },
+		{ MDIO_DEV_PMA_PMD, 0xc014, 0xffff, 0xfe16 },
+		{ MDIO_DEV_PMA_PMD, 0xc015, 0xffff, 0xa000 },
 		{ 0, 0, 0, 0 }
 	};
-	int i, err;
+	int i, err, size;
 
 	err = set_phy_regs(phy, regs);
 	if (!err && modtype == phy_modtype_twinax_long)
@@ -344,15 +550,14 @@
 	msleep(50);
 
 	if (phy->priv != edc_twinax)
-		err = t3_get_edc_fw(phy, EDC_TWX_AEL2005,
-				    EDC_TWX_AEL2005_SIZE);
-	if (err)
-		return err;
+		size = t3_get_edc_fw(phy, EDC_TWX_AEL2005);
+	if (size < 0)
+		return size;
 
-	for (i = 0; i <  EDC_TWX_AEL2005_SIZE / sizeof(u16) && !err; i += 2)
-		err = t3_mdio_write(phy, MDIO_MMD_PMAPMD,
-				    phy->phy_cache[i],
-				    phy->phy_cache[i + 1]);
+	for (i = 0; i <  size / sizeof(u16) && !err; i += 2)
+		err = mdio_write(phy, MDIO_DEV_PMA_PMD,
+				 phy->phy_cache[i],
+				 phy->phy_cache[i + 1]);
 	if (!err)
 		phy->priv = edc_twinax;
 	return err;
@@ -363,7 +568,7 @@
 	int v;
 	unsigned int stat;
 
-	v = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_CTRL, &stat);
+	v = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_CTRL, &stat);
 	if (v)
 		return v;
 
@@ -375,49 +580,48 @@
 
 static int ael2005_intr_enable(struct cphy *phy)
 {
-	int err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_CTRL, 0x200);
+	int err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_CTRL, 0x200);
 	return err ? err : t3_phy_lasi_intr_enable(phy);
 }
 
 static int ael2005_intr_disable(struct cphy *phy)
 {
-	int err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_CTRL, 0x100);
+	int err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_CTRL, 0x100);
 	return err ? err : t3_phy_lasi_intr_disable(phy);
 }
 
 static int ael2005_intr_clear(struct cphy *phy)
 {
-	int err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_CTRL, 0xd00);
+	int err = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_CTRL, 0xd00);
 	return err ? err : t3_phy_lasi_intr_clear(phy);
 }
 
 static int ael2005_reset(struct cphy *phy, int wait)
 {
 	static struct reg_val regs0[] = {
-		{ MDIO_MMD_PMAPMD, 0xc001, 0, 1 << 5 },
-		{ MDIO_MMD_PMAPMD, 0xc017, 0, 1 << 5 },
-		{ MDIO_MMD_PMAPMD, 0xc013, 0xffff, 0xf341 },
-		{ MDIO_MMD_PMAPMD, 0xc210, 0xffff, 0x8000 },
-		{ MDIO_MMD_PMAPMD, 0xc210, 0xffff, 0x8100 },
-		{ MDIO_MMD_PMAPMD, 0xc210, 0xffff, 0x8000 },
-		{ MDIO_MMD_PMAPMD, 0xc210, 0xffff, 0 },
+		{ MDIO_DEV_PMA_PMD, 0xc001, 0, 1 << 5 },
+		{ MDIO_DEV_PMA_PMD, 0xc017, 0, 1 << 5 },
+		{ MDIO_DEV_PMA_PMD, 0xc013, 0xffff, 0xf341 },
+		{ MDIO_DEV_PMA_PMD, 0xc210, 0xffff, 0x8000 },
+		{ MDIO_DEV_PMA_PMD, 0xc210, 0xffff, 0x8100 },
+		{ MDIO_DEV_PMA_PMD, 0xc210, 0xffff, 0x8000 },
+		{ MDIO_DEV_PMA_PMD, 0xc210, 0xffff, 0 },
 		{ 0, 0, 0, 0 }
 	};
 	static struct reg_val regs1[] = {
-		{ MDIO_MMD_PMAPMD, 0xca00, 0xffff, 0x0080 },
-		{ MDIO_MMD_PMAPMD, 0xca12, 0xffff, 0 },
+		{ MDIO_DEV_PMA_PMD, 0xca00, 0xffff, 0x0080 },
+		{ MDIO_DEV_PMA_PMD, 0xca12, 0xffff, 0 },
 		{ 0, 0, 0, 0 }
 	};
 
 	int err;
 	unsigned int lasi_ctrl;
 
-	err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_CTRL,
-			   &lasi_ctrl);
+	err = mdio_read(phy, MDIO_DEV_PMA_PMD, LASI_CTRL, &lasi_ctrl);
 	if (err)
 		return err;
 
-	err = t3_phy_reset(phy, MDIO_MMD_PMAPMD, 0);
+	err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, 0);
 	if (err)
 		return err;
 
@@ -432,9 +636,11 @@
 	err = ael2005_get_module_type(phy, 0);
 	if (err < 0)
 		return err;
-	phy->modtype = err;
+	phy->modtype = (u8)err;
 
-	if (err == phy_modtype_twinax || err == phy_modtype_twinax_long)
+	if (err == phy_modtype_none)
+		err = 0;
+	else if (err == phy_modtype_twinax || err == phy_modtype_twinax_long)
 		err = ael2005_setup_twinax_edc(phy, err);
 	else
 		err = ael2005_setup_sr_edc(phy);
@@ -456,13 +662,13 @@
 	unsigned int stat;
 	int ret, edc_needed, cause = 0;
 
-	ret = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_STAT, &stat);
+	ret = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_STAT, &stat);
 	if (ret)
 		return ret;
 
 	if (stat & AEL2005_MODDET_IRQ) {
-		ret = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AEL2005_GPIO_CTRL,
-				    0xd00);
+		ret = mdio_write(phy, MDIO_DEV_PMA_PMD, AEL2005_GPIO_CTRL,
+				 0xd00);
 		if (ret)
 			return ret;
 
@@ -471,7 +677,7 @@
 		if (ret < 0)
 			return ret;
 
-		phy->modtype = ret;
+		phy->modtype = (u8)ret;
 		if (ret == phy_modtype_none)
 			edc_needed = phy->priv;       /* on unplug retain EDC */
 		else if (ret == phy_modtype_twinax ||
@@ -492,10 +698,26 @@
 		return ret;
 
 	ret |= cause;
-	return ret ? ret : cphy_cause_link_change;
+	if (!ret)
+		ret |= cphy_cause_link_change;
+	return ret;
 }
 
 static struct cphy_ops ael2005_ops = {
+#ifdef C99_NOT_SUPPORTED
+	ael2005_reset,
+	ael2005_intr_enable,
+	ael2005_intr_disable,
+	ael2005_intr_clear,
+	ael2005_intr_handler,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	get_link_status_r,
+	ael1002_power_down,
+#else
 	.reset           = ael2005_reset,
 	.intr_enable     = ael2005_intr_enable,
 	.intr_disable    = ael2005_intr_disable,
@@ -503,33 +725,68 @@
 	.intr_handler    = ael2005_intr_handler,
 	.get_link_status = get_link_status_r,
 	.power_down      = ael1002_power_down,
-	.mmds            = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
+#endif
 };
 
-int t3_ael2005_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops)
+int t3_ael2005_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops)
 {
-	cphy_init(phy, adapter, phy_addr, &ael2005_ops, mdio_ops,
+	int err;
+	struct cphy *phy = &pinfo->phy;
+
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &ael2005_ops, mdio_ops,
 		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_FIBRE |
 		  SUPPORTED_IRQ, "10GBASE-R");
 	msleep(125);
-	return t3_mdio_change_bits(phy, MDIO_MMD_PMAPMD, AEL_OPT_SETTINGS, 0,
+	ael_laser_down(phy, 0);
+
+	err = ael2005_get_module_type(phy, 0);
+	if (err >= 0)
+		phy->modtype = err;
+
+	return t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, AEL_OPT_SETTINGS, 0,
 				   1 << 5);
 }
 
 /*
+ * The Aeluros/NetLogic AEL2020 is nearly identical to the AEL2005.  It's
+ * basically two AEL2005's packaged up into a single die.  There are some
+ * small differences which we account for here:
+ *
+ * 1. AEL2020 Errata.  See in particular erratum #2 concerning a "CDRLOL
+ *    signal being asserted under all conditions" resulting in "PMD link
+ *    status to be down all the time."
+ *
+ * 2. The Link Status LED is controlled by the AEL2020 GPIO configuration
+ *    for AEL2020 GPIO pins 0_1 and 1_1.  See comments regarding Traffic
+ *    Indicator Mode on pages 18-20 of the AEL2020 datasheet.
+ *
+ * 3. Different GPIO logic used to enable, mask and detect interrupts,
+ *    and detect module types.
+ *
+ * 4. The EDC logic can be turned off for optical modules resulting in
+ *    significant power savings.
+ *
+ * 5. Slightly different reset logic and uP firmware.
+ *
+ * Unfortunately, despite the high degree of similarity between the AEL2005
+ * and the AEL2020 PHYs, we need to duplicate the vast majority of the code
+ * because of the differences and the small size of the code base.
+ */
+
+/*
  * Setup EDC and other parameters for operation with an optical module.
  */
 static int ael2020_setup_sr_edc(struct cphy *phy)
 {
 	static struct reg_val regs[] = {
 		/* set CDR offset to 10 */
-		{ MDIO_MMD_PMAPMD, 0xcc01, 0xffff, 0x488a },
+		{ MDIO_DEV_PMA_PMD, 0xcc01, 0xffff, 0x488a },
 
 		/* adjust 10G RX bias current */
-		{ MDIO_MMD_PMAPMD, 0xcb1b, 0xffff, 0x0200 },
-		{ MDIO_MMD_PMAPMD, 0xcb1c, 0xffff, 0x00f0 },
-		{ MDIO_MMD_PMAPMD, 0xcc06, 0xffff, 0x00e0 },
+		{ MDIO_DEV_PMA_PMD, 0xcb1b, 0xffff, 0x0200 },
+		{ MDIO_DEV_PMA_PMD, 0xcb1c, 0xffff, 0x00f0 },
+		{ MDIO_DEV_PMA_PMD, 0xcc06, 0xffff, 0x00e0 },
 
 		/* end */
 		{ 0, 0, 0, 0 }
@@ -552,24 +809,25 @@
 {
 	/* set uC to 40MHz */
 	static struct reg_val uCclock40MHz[] = {
-		{ MDIO_MMD_PMAPMD, 0xff28, 0xffff, 0x4001 },
-		{ MDIO_MMD_PMAPMD, 0xff2a, 0xffff, 0x0002 },
+		{ MDIO_DEV_PMA_PMD, 0xff28, 0xffff, 0x4001 },
+		{ MDIO_DEV_PMA_PMD, 0xff2a, 0xffff, 0x0002 },
 		{ 0, 0, 0, 0 }
 	};
 
 	/* activate uC clock */
 	static struct reg_val uCclockActivate[] = {
-		{ MDIO_MMD_PMAPMD, 0xd000, 0xffff, 0x5200 },
+		{ MDIO_DEV_PMA_PMD, 0xd000, 0xffff, 0x5200 },
 		{ 0, 0, 0, 0 }
 	};
 
 	/* set PC to start of SRAM and activate uC */
 	static struct reg_val uCactivate[] = {
-		{ MDIO_MMD_PMAPMD, 0xd080, 0xffff, 0x0100 },
-		{ MDIO_MMD_PMAPMD, 0xd092, 0xffff, 0x0000 },
+		{ MDIO_DEV_PMA_PMD, 0xd080, 0xffff, 0x0100 },
+		{ MDIO_DEV_PMA_PMD, 0xd092, 0xffff, 0x0000 },
 		{ 0, 0, 0, 0 }
 	};
-	int i, err;
+
+	int i, err, size;
 
 	/* set uC clock and activate it */
 	err = set_phy_regs(phy, uCclock40MHz);
@@ -582,15 +840,16 @@
 		return err;
 
 	if (phy->priv != edc_twinax)
-		err = t3_get_edc_fw(phy, EDC_TWX_AEL2020,
-				    EDC_TWX_AEL2020_SIZE);
+		size = t3_get_edc_fw(phy, EDC_TWX_AEL2020);
 	if (err)
 		return err;
 
-	for (i = 0; i <  EDC_TWX_AEL2020_SIZE / sizeof(u16) && !err; i += 2)
-		err = t3_mdio_write(phy, MDIO_MMD_PMAPMD,
-				    phy->phy_cache[i],
-				    phy->phy_cache[i + 1]);
+	for (i = 0; i <  size / sizeof(u16) && !err; i += 2)
+		err = mdio_write(phy, MDIO_DEV_PMA_PMD,
+				 phy->phy_cache[i],
+				 phy->phy_cache[i + 1]);
+
+
 	/* activate uC */
 	err = set_phy_regs(phy, uCactivate);
 	if (!err)
@@ -606,7 +865,7 @@
 	int v;
 	unsigned int stat;
 
-	v = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL2020_GPIO_STAT, &stat);
+	v = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL2020_GPIO_STAT, &stat);
 	if (v)
 		return v;
 
@@ -619,6 +878,15 @@
 }
 
 /*
+ * Note that on the AEL2005 putting the PHY into low power mode also asserts
+ * the LOS_OUT pin on that PHY.  On the AEL2020 the internal "LOS" signal
+ * which we route to the "link status" LED is _not_ affected by putting the
+ * PHY into low power mode.  So we change the GPIO configuration for the link
+ * status LED in the interrupt enable/disable functions in order to cause it
+ * to go off when the interface is brought down.
+ */
+
+/*
  * Enable PHY interrupts.  We enable "Module Detection" interrupts (on any
  * state transition) and then generic Link Alarm Status Interrupt (LASI).
  */
@@ -626,32 +894,33 @@
 {
 	struct reg_val regs[] = {
 		/* output Module's Loss Of Signal (LOS) to LED */
-		{ MDIO_MMD_PMAPMD, AEL2020_GPIO_CFG+AEL2020_GPIO_LSTAT,
+		{ MDIO_DEV_PMA_PMD, AEL2020_GPIO_CFG+AEL2020_GPIO_LSTAT,
 			0xffff, 0x4 },
-		{ MDIO_MMD_PMAPMD, AEL2020_GPIO_CTRL,
+		{ MDIO_DEV_PMA_PMD, AEL2020_GPIO_CTRL,
 			0xffff, 0x8 << (AEL2020_GPIO_LSTAT*4) },
 
 		 /* enable module detect status change interrupts */
-		{ MDIO_MMD_PMAPMD, AEL2020_GPIO_CTRL,
+		{ MDIO_DEV_PMA_PMD, AEL2020_GPIO_CTRL,
 			0xffff, 0x2 << (AEL2020_GPIO_MODDET*4) },
 
 		/* end */
 		{ 0, 0, 0, 0 }
 	};
-	int err, link_ok = 0;
+	int err;
 
 	/* set up "link status" LED and enable module change interrupts */
 	err = set_phy_regs(phy, regs);
 	if (err)
 		return err;
 
-	err = get_link_status_r(phy, &link_ok, NULL, NULL, NULL);
-	if (err)
-		return err;
-	if (link_ok)
-		t3_link_changed(phy->adapter,
-				phy2portid(phy));
+        /*
+	 * The AEL2020 doesn't seem to send an interrupt when a link is
+	 * already establish and you enable interrupts.  So we we schedule a
+	 * link status check.
+	 */
+	t3_os_link_fault_handler(phy->adapter, phy->pinfo->port_id);
 
+	/* enable standard Link Alarm Status Interrupts */
 	err = t3_phy_lasi_intr_enable(phy);
 	if (err)
 		return err;
@@ -666,11 +935,11 @@
 {
 	struct reg_val regs[] = {
 		/* reset "link status" LED to "off" */
-		{ MDIO_MMD_PMAPMD, AEL2020_GPIO_CTRL,
+		{ MDIO_DEV_PMA_PMD, AEL2020_GPIO_CTRL,
 			0xffff, 0xb << (AEL2020_GPIO_LSTAT*4) },
 
 		/* disable module detect status change interrupts */
-		{ MDIO_MMD_PMAPMD, AEL2020_GPIO_CTRL,
+		{ MDIO_DEV_PMA_PMD, AEL2020_GPIO_CTRL,
 			0xffff, 0x1 << (AEL2020_GPIO_MODDET*4) },
 
 		/* end */
@@ -683,6 +952,7 @@
 	if (err)
 		return err;
 
+	/* disable standard Link Alarm Status Interrupts */
 	return t3_phy_lasi_intr_disable(phy);
 }
 
@@ -697,25 +967,36 @@
 	 * Thus, we simply read the register and discard the result.
 	 */
 	unsigned int stat;
-	int err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL2020_GPIO_INTR, &stat);
+	int err = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL2020_GPIO_INTR, &stat);
 	return err ? err : t3_phy_lasi_intr_clear(phy);
 }
 
+/*
+ * Common register settings for the AEL2020 when it comes out of reset.
+ */
 static struct reg_val ael2020_reset_regs[] = {
 	/* Erratum #2: CDRLOL asserted, causing PMA link down status */
-	{ MDIO_MMD_PMAPMD, 0xc003, 0xffff, 0x3101 },
+	{ MDIO_DEV_PMA_PMD, 0xc003, 0xffff, 0x3101 },
 
 	/* force XAUI to send LF when RX_LOS is asserted */
-	{ MDIO_MMD_PMAPMD, 0xcd40, 0xffff, 0x0001 },
+	{ MDIO_DEV_PMA_PMD, 0xcd40, 0xffff, 0x0001 },
+
+	{ MDIO_DEV_PMA_PMD, 0xca12, 0xffff, 0x0100 },
+	{ MDIO_DEV_PMA_PMD, 0xca22, 0xffff, 0x0100 },
+	{ MDIO_DEV_PMA_PMD, 0xca42, 0xffff, 0x0100 },
 
 	/* allow writes to transceiver module EEPROM on i2c bus */
-	{ MDIO_MMD_PMAPMD, 0xff02, 0xffff, 0x0023 },
-	{ MDIO_MMD_PMAPMD, 0xff03, 0xffff, 0x0000 },
-	{ MDIO_MMD_PMAPMD, 0xff04, 0xffff, 0x0000 },
+	{ MDIO_DEV_PMA_PMD, 0xff02, 0xffff, 0x0023 },
+	{ MDIO_DEV_PMA_PMD, 0xff03, 0xffff, 0x0000 },
+	{ MDIO_DEV_PMA_PMD, 0xff04, 0xffff, 0x0000 },
+
+	/* initiate reset sequencer */
+	{ MDIO_DEV_PMA_PMD, 0xc20d, 0xffff, 0x0002 },
 
 	/* end */
 	{ 0, 0, 0, 0 }
 };
+
 /*
  * Reset the PHY and put it into a canonical operating state.
  */
@@ -725,12 +1006,11 @@
 	unsigned int lasi_ctrl;
 
 	/* grab current interrupt state */
-	err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_CTRL,
-			   &lasi_ctrl);
+	err = mdio_read(phy, MDIO_DEV_PMA_PMD, LASI_CTRL, &lasi_ctrl);
 	if (err)
 		return err;
 
-	err = t3_phy_reset(phy, MDIO_MMD_PMAPMD, 125);
+	err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, 125);
 	if (err)
 		return err;
 	msleep(100);
@@ -740,13 +1020,16 @@
 	err = set_phy_regs(phy, ael2020_reset_regs);
 	if (err)
 		return err;
+	msleep(100);
 
 	/* determine module type and perform appropriate initialization */
 	err = ael2020_get_module_type(phy, 0);
 	if (err < 0)
 		return err;
 	phy->modtype = (u8)err;
-	if (err == phy_modtype_twinax || err == phy_modtype_twinax_long)
+	if (err == phy_modtype_none)
+		err = 0;
+	else if (err == phy_modtype_twinax || err == phy_modtype_twinax_long)
 		err = ael2020_setup_twinax_edc(phy, err);
 	else
 		err = ael2020_setup_sr_edc(phy);
@@ -755,7 +1038,7 @@
 
 	/* reset wipes out interrupts, reenable them if they were on */
 	if (lasi_ctrl & 1)
-		err = ael2005_intr_enable(phy);
+		err = ael2020_intr_enable(phy);
 	return err;
 }
 
@@ -767,7 +1050,7 @@
 	unsigned int stat;
 	int ret, edc_needed, cause = 0;
 
-	ret = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AEL2020_GPIO_INTR, &stat);
+	ret = mdio_read(phy, MDIO_DEV_PMA_PMD, AEL2020_GPIO_INTR, &stat);
 	if (ret)
 		return ret;
 
@@ -798,10 +1081,26 @@
 		return ret;
 
 	ret |= cause;
-	return ret ? ret : cphy_cause_link_change;
+	if (!ret)
+		ret |= cphy_cause_link_change;
+	return ret;
 }
 
 static struct cphy_ops ael2020_ops = {
+#ifdef C99_NOT_SUPPORTED
+	ael2020_reset,
+	ael2020_intr_enable,
+	ael2020_intr_disable,
+	ael2020_intr_clear,
+	ael2020_intr_handler,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	get_link_status_r,
+	ael1002_power_down,
+#else
 	.reset           = ael2020_reset,
 	.intr_enable     = ael2020_intr_enable,
 	.intr_disable    = ael2020_intr_disable,
@@ -809,22 +1108,30 @@
 	.intr_handler    = ael2020_intr_handler,
 	.get_link_status = get_link_status_r,
 	.power_down      = ael1002_power_down,
-	.mmds		 = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
+#endif
 };
 
-int t3_ael2020_phy_prep(struct cphy *phy, struct adapter *adapter, int phy_addr,
+int t3_ael2020_phy_prep(pinfo_t *pinfo, int phy_addr,
 			const struct mdio_ops *mdio_ops)
 {
 	int err;
+	struct cphy *phy = &pinfo->phy;
 
-	cphy_init(phy, adapter, phy_addr, &ael2020_ops, mdio_ops,
-		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_FIBRE |
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &ael2020_ops, mdio_ops,
+		SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_FIBRE |
 		  SUPPORTED_IRQ, "10GBASE-R");
 	msleep(125);
 
 	err = set_phy_regs(phy, ael2020_reset_regs);
 	if (err)
 		return err;
+	msleep(100);
+
+	err = ael2020_get_module_type(phy, 0);
+	if (err >= 0)
+		phy->modtype = err;
+
+	ael_laser_down(phy, 0);
 	return 0;
 }
 
@@ -836,15 +1143,12 @@
 {
 	if (link_ok) {
 		unsigned int stat0, stat1, stat2;
-		int err = t3_mdio_read(phy, MDIO_MMD_PMAPMD,
-				       MDIO_PMA_RXDET, &stat0);
+		int err = mdio_read(phy, MDIO_DEV_PMA_PMD, PMD_RSD, &stat0);
 
 		if (!err)
-			err = t3_mdio_read(phy, MDIO_MMD_PCS,
-					   MDIO_PCS_10GBX_STAT1, &stat1);
+			err = mdio_read(phy, MDIO_DEV_PCS, PCS_STAT1_X, &stat1);
 		if (!err)
-			err = t3_mdio_read(phy, MDIO_MMD_PHYXS,
-					   MDIO_PHYXS_LNSTAT, &stat2);
+			err = mdio_read(phy, MDIO_DEV_XGXS, XS_LN_STAT, &stat2);
 		if (err)
 			return err;
 		*link_ok = (stat0 & (stat1 >> 12) & (stat2 >> 12)) & 1;
@@ -856,23 +1160,40 @@
 	return 0;
 }
 
+#ifdef C99_NOT_SUPPORTED
 static struct cphy_ops qt2045_ops = {
-	.reset = ael1006_reset,
-	.intr_enable = t3_phy_lasi_intr_enable,
-	.intr_disable = t3_phy_lasi_intr_disable,
-	.intr_clear = t3_phy_lasi_intr_clear,
-	.intr_handler = t3_phy_lasi_intr_handler,
+	ael1006_reset,
+	t3_phy_lasi_intr_enable,
+	t3_phy_lasi_intr_disable,
+	t3_phy_lasi_intr_clear,
+	t3_phy_lasi_intr_handler,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	get_link_status_x,
+	ael1002_power_down,
+};
+#else
+static struct cphy_ops qt2045_ops = {
+	.reset           = ael1006_reset,
+	.intr_enable     = t3_phy_lasi_intr_enable,
+	.intr_disable    = t3_phy_lasi_intr_disable,
+	.intr_clear      = t3_phy_lasi_intr_clear,
+	.intr_handler    = t3_phy_lasi_intr_handler,
 	.get_link_status = get_link_status_x,
-	.power_down = ael1002_power_down,
-	.mmds = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
+	.power_down      = ael1002_power_down,
 };
+#endif
 
-int t3_qt2045_phy_prep(struct cphy *phy, struct adapter *adapter,
-		       int phy_addr, const struct mdio_ops *mdio_ops)
+int t3_qt2045_phy_prep(pinfo_t *pinfo, int phy_addr,
+		       const struct mdio_ops *mdio_ops)
 {
 	unsigned int stat;
+	struct cphy *phy = &pinfo->phy;
 
-	cphy_init(phy, adapter, phy_addr, &qt2045_ops, mdio_ops,
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &qt2045_ops, mdio_ops,
 		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_TP,
 		  "10GBASE-CX4");
 
@@ -880,10 +1201,9 @@
 	 * Some cards where the PHY is supposed to be at address 0 actually
 	 * have it at 1.
 	 */
-	if (!phy_addr &&
-	    !t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_STAT1, &stat) &&
+	if (!phy_addr && !mdio_read(phy, MDIO_DEV_PMA_PMD, MII_BMSR, &stat) &&
 	    stat == 0xffff)
-		phy->mdio.prtad = 1;
+		phy->addr = 1;
 	return 0;
 }
 
@@ -897,16 +1217,16 @@
 {
 	if (link_ok) {
 		unsigned int status;
-		int prtad = phy->mdio.prtad;
+		adapter_t *adapter = phy->adapter;
 
-		status = t3_read_reg(phy->adapter,
-				     XGM_REG(A_XGM_SERDES_STAT0, prtad)) |
-		    t3_read_reg(phy->adapter,
-				    XGM_REG(A_XGM_SERDES_STAT1, prtad)) |
-		    t3_read_reg(phy->adapter,
-				XGM_REG(A_XGM_SERDES_STAT2, prtad)) |
-		    t3_read_reg(phy->adapter,
-				XGM_REG(A_XGM_SERDES_STAT3, prtad));
+		status = t3_read_reg(adapter,
+				     XGM_REG(A_XGM_SERDES_STAT0, phy->addr)) |
+			 t3_read_reg(adapter,
+				     XGM_REG(A_XGM_SERDES_STAT1, phy->addr)) |
+			 t3_read_reg(adapter,
+				     XGM_REG(A_XGM_SERDES_STAT2, phy->addr)) |
+			 t3_read_reg(adapter,
+				     XGM_REG(A_XGM_SERDES_STAT3, phy->addr));
 		*link_ok = !(status & F_LOWSIG0);
 	}
 	if (speed)
@@ -921,20 +1241,37 @@
 	return 0;
 }
 
+#ifdef C99_NOT_SUPPORTED
 static struct cphy_ops xaui_direct_ops = {
-	.reset = xaui_direct_reset,
-	.intr_enable = ael1002_intr_noop,
-	.intr_disable = ael1002_intr_noop,
-	.intr_clear = ael1002_intr_noop,
-	.intr_handler = ael1002_intr_noop,
+	xaui_direct_reset,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	ael1002_intr_noop,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	NULL,
+	xaui_direct_get_link_status,
+	xaui_direct_power_down,
+};
+#else
+static struct cphy_ops xaui_direct_ops = {
+	.reset           = xaui_direct_reset,
+	.intr_enable     = ael1002_intr_noop,
+	.intr_disable    = ael1002_intr_noop,
+	.intr_clear      = ael1002_intr_noop,
+	.intr_handler    = ael1002_intr_noop,
 	.get_link_status = xaui_direct_get_link_status,
-	.power_down = xaui_direct_power_down,
+	.power_down      = xaui_direct_power_down,
 };
+#endif
 
-int t3_xaui_direct_phy_prep(struct cphy *phy, struct adapter *adapter,
-			    int phy_addr, const struct mdio_ops *mdio_ops)
+int t3_xaui_direct_phy_prep(pinfo_t *pinfo, int phy_addr,
+			    const struct mdio_ops *mdio_ops)
 {
-	cphy_init(phy, adapter, MDIO_PRTAD_NONE, &xaui_direct_ops, mdio_ops,
+	cphy_init(&pinfo->phy, pinfo->adapter, pinfo, phy_addr, &xaui_direct_ops, mdio_ops,
 		  SUPPORTED_10000baseT_Full | SUPPORTED_AUI | SUPPORTED_TP,
 		  "10GBASE-CX4");
 	return 0;
diff --git a/drivers/net/cxgb3/aq100x.c b/drivers/net/cxgb3/aq100x.c
--- a/drivers/net/cxgb3/aq100x.c
+++ b/drivers/net/cxgb3/aq100x.c
@@ -1,33 +1,12 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
 
 #include "common.h"
@@ -36,121 +15,241 @@
 enum {
 	/* MDIO_DEV_PMA_PMD registers */
 	AQ_LINK_STAT	= 0xe800,
-	AQ_IMASK_PMA	= 0xf000,
 
 	/* MDIO_DEV_XGXS registers */
 	AQ_XAUI_RX_CFG	= 0xc400,
+	AQ_XAUI_KX_CFG	= 0xc440,
 	AQ_XAUI_TX_CFG	= 0xe400,
 
 	/* MDIO_DEV_ANEG registers */
+	AQ_100M_CTRL	= 0x0010,
+	AQ_10G_CTRL	= 0x0020,
 	AQ_1G_CTRL	= 0xc400,
 	AQ_ANEG_STAT	= 0xc800,
 
 	/* MDIO_DEV_VEND1 registers */
 	AQ_FW_VERSION	= 0x0020,
+	AQ_THERMAL_THR	= 0xc421,
+	AQ_THERMAL1	= 0xc820,
+	AQ_THERMAL2	= 0xc821,
 	AQ_IFLAG_GLOBAL	= 0xfc00,
 	AQ_IMASK_GLOBAL	= 0xff00,
 };
 
-enum {
-	IMASK_PMA	= 1 << 2,
-	IMASK_GLOBAL	= 1 << 15,
-	ADV_1G_FULL	= 1 << 15,
-	ADV_1G_HALF	= 1 << 14,
-	ADV_10G_FULL	= 1 << 12,
-	AQ_RESET	= (1 << 14) | (1 << 15),
-	AQ_LOWPOWER	= 1 << 12,
-};
+#define AQBIT(x)	(1 << (0x##x))
+#define ADV_1G_FULL	AQBIT(f)
+#define ADV_1G_HALF	AQBIT(e)
+#define ADV_10G_FULL	AQBIT(c)
 
-static int aq100x_reset(struct cphy *phy, int wait)
-{
-	/*
-	 * Ignore the caller specified wait time; always wait for the reset to
-	 * complete. Can take up to 3s.
-	 */
-	int err = t3_phy_reset(phy, MDIO_MMD_VEND1, 3000);
+#define AQ_WRITE_REGS(phy, regs) do { \
+	int i; \
+	for (i = 0; i < ARRAY_SIZE(regs); i++) { \
+		(void) mdio_write(phy, regs[i].mmd, regs[i].reg, regs[i].val); \
+	} \
+} while (0)
+#define AQ_READ_REGS(phy, regs) do { \
+	unsigned i, v; \
+	for (i = 0; i < ARRAY_SIZE(regs); i++) { \
+		(void) mdio_read(phy, regs[i].mmd, regs[i].reg, &v); \
+	} \
+} while (0)
 
-	if (err)
-		CH_WARN(phy->adapter, "PHY%d: reset failed (0x%x).\n",
-			phy->mdio.prtad, err);
-
-	return err;
-}
-
-static int aq100x_intr_enable(struct cphy *phy)
-{
-	int err = t3_mdio_write(phy, MDIO_MMD_PMAPMD, AQ_IMASK_PMA, IMASK_PMA);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_MMD_VEND1, AQ_IMASK_GLOBAL, IMASK_GLOBAL);
-	return err;
-}
-
-static int aq100x_intr_disable(struct cphy *phy)
-{
-	return t3_mdio_write(phy, MDIO_MMD_VEND1, AQ_IMASK_GLOBAL, 0);
-}
-
-static int aq100x_intr_clear(struct cphy *phy)
+/*
+ * Return value is temperature in celcius, 0xffff for error or don't know.
+ */
+static int
+aq100x_temperature(struct cphy *phy)
 {
 	unsigned int v;
 
-	t3_mdio_read(phy, MDIO_MMD_VEND1, AQ_IFLAG_GLOBAL, &v);
-	t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_STAT1, &v);
+	if (mdio_read(phy, MDIO_DEV_VEND1, AQ_THERMAL2, &v) ||
+	    v == 0xffff || (v & 1) != 1)
+		return (0xffff);
 
-	return 0;
+	if (mdio_read(phy, MDIO_DEV_VEND1, AQ_THERMAL1, &v))
+		return (0xffff);
+
+	return ((int)((signed char)(v >> 8)));
 }
 
-static int aq100x_intr_handler(struct cphy *phy)
+static int
+aq100x_set_defaults(struct cphy *phy)
+{
+	return mdio_write(phy, MDIO_DEV_VEND1, AQ_THERMAL_THR, 0x6c00);
+}
+
+static int
+aq100x_reset(struct cphy *phy, int wait)
+{
+	int err;
+	err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, wait);
+	if (!err)
+		err = aq100x_set_defaults(phy);
+	return (err);
+}
+
+static int
+aq100x_intr_enable(struct cphy *phy)
+{
+	struct {
+		int mmd;
+		int reg;
+		int val;
+	} imasks[] = {
+		{MDIO_DEV_VEND1, 0xd400, AQBIT(e)},
+		{MDIO_DEV_VEND1, 0xff01, AQBIT(2)},
+		{MDIO_DEV_VEND1, AQ_IMASK_GLOBAL, AQBIT(0)}
+	};
+
+	AQ_WRITE_REGS(phy, imasks);
+
+	return (0);
+}
+
+static int
+aq100x_intr_disable(struct cphy *phy)
+{
+	struct {
+		int mmd;
+		int reg;
+		int val;
+	} imasks[] = {
+		{MDIO_DEV_VEND1, 0xd400, 0},
+		{MDIO_DEV_VEND1, 0xff01, 0},
+		{MDIO_DEV_VEND1, AQ_IMASK_GLOBAL, 0}
+	};
+
+	AQ_WRITE_REGS(phy, imasks);
+
+	return (0);
+}
+
+static int
+aq100x_intr_clear(struct cphy *phy)
+{
+	struct {
+		int mmd;
+		int reg;
+	} iclr[] = {
+		{MDIO_DEV_VEND1, 0xcc00},
+		{MDIO_DEV_VEND1, AQ_IMASK_GLOBAL} /* needed? */
+	};
+
+	AQ_READ_REGS(phy, iclr);
+
+	return (0);
+}
+
+static int
+aq100x_vendor_intr(struct cphy *phy, int *rc)
 {
 	int err;
 	unsigned int cause, v;
 
-	err = t3_mdio_read(phy, MDIO_MMD_VEND1, AQ_IFLAG_GLOBAL, &cause);
+	err = mdio_read(phy, MDIO_DEV_VEND1, 0xfc01, &cause);
 	if (err)
-		return err;
+		return (err);
 
-	/* Read (and reset) the latching version of the status */
-	t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_STAT1, &v);
+	if (cause & AQBIT(2)) {
+		err = mdio_read(phy, MDIO_DEV_VEND1, 0xcc00, &v);
+		if (err)
+			return (err);
 
-	return cphy_cause_link_change;
+		if (v & AQBIT(e)) {
+			CH_WARN(phy->adapter, "PHY%d: temperature is now %dC\n",
+			    phy->addr, aq100x_temperature(phy));
+
+			t3_set_reg_field(phy->adapter, A_T3DBG_GPIO_EN,
+			    phy->addr ? F_GPIO10_OUT_VAL : F_GPIO6_OUT_VAL, 0);
+
+			*rc |= cphy_cause_alarm;
+		}
+
+		cause &= ~4;
+	}
+
+	if (cause)
+		CH_WARN(phy->adapter, "PHY%d: unhandled vendor interrupt"
+		    " (0x%x)\n", phy->addr, cause);
+
+	return (0);
+
 }
 
-static int aq100x_power_down(struct cphy *phy, int off)
+static int
+aq100x_intr_handler(struct cphy *phy)
 {
-	return mdio_set_flag(&phy->mdio, phy->mdio.prtad,
-			     MDIO_MMD_PMAPMD, MDIO_CTRL1,
-			     MDIO_CTRL1_LPOWER, off);
+	int err, rc = 0;
+	unsigned int cause;
+
+	err = mdio_read(phy, MDIO_DEV_VEND1, AQ_IFLAG_GLOBAL, &cause);
+	if (err)
+		return (err);
+
+	if (cause & AQBIT(0)) {
+		err = aq100x_vendor_intr(phy, &rc);
+		if (err)
+			return (err);
+		cause &= ~AQBIT(0);
+	}
+
+	if (cause)
+		CH_WARN(phy->adapter, "PHY%d: unhandled interrupt (0x%x)\n",
+		    phy->addr, cause);
+
+	return (rc);
 }
 
-static int aq100x_autoneg_enable(struct cphy *phy)
+static int
+aq100x_power_down(struct cphy *phy, int off)
+{
+	int err, wait = 500;
+	unsigned int v;
+
+	err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR, BMCR_PDOWN,
+	    off ? BMCR_PDOWN : 0);
+	if (err || off)
+		return (v);
+
+	msleep(300);
+	do {
+		err = mdio_read(phy, MDIO_DEV_PMA_PMD, MII_BMCR, &v);
+		if (err)
+			return (err);
+		v &= BMCR_RESET;
+		if (v)
+			msleep(10);
+	} while (v && --wait);
+	if (v) {
+		CH_WARN(phy->adapter, "PHY%d: power-up timed out (0x%x).\n",
+		    phy->addr, v);
+		return (ETIMEDOUT);
+	}
+
+	return (0);
+}
+
+static int
+aq100x_autoneg_enable(struct cphy *phy)
 {
 	int err;
 
 	err = aq100x_power_down(phy, 0);
 	if (!err)
-		err = mdio_set_flag(&phy->mdio, phy->mdio.prtad,
-				    MDIO_MMD_AN, MDIO_CTRL1,
-				    BMCR_ANENABLE | BMCR_ANRESTART, 1);
+		err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, MII_BMCR,
+		    BMCR_RESET, BMCR_ANENABLE | BMCR_ANRESTART);
 
-	return err;
+	return (err);
 }
 
-static int aq100x_autoneg_restart(struct cphy *phy)
+static int
+aq100x_autoneg_restart(struct cphy *phy)
 {
-	int err;
-
-	err = aq100x_power_down(phy, 0);
-	if (!err)
-		err = mdio_set_flag(&phy->mdio, phy->mdio.prtad,
-				    MDIO_MMD_AN, MDIO_CTRL1,
-				    BMCR_ANENABLE | BMCR_ANRESTART, 1);
-
-	return err;
+	return aq100x_autoneg_enable(phy);
 }
 
-static int aq100x_advertise(struct cphy *phy, unsigned int advertise_map)
+static int
+aq100x_advertise(struct cphy *phy, unsigned int advertise_map)
 {
 	unsigned int adv;
 	int err;
@@ -159,10 +258,10 @@
 	adv = 0;
 	if (advertise_map & ADVERTISED_10000baseT_Full)
 		adv |= ADV_10G_FULL;
-	err = t3_mdio_change_bits(phy, MDIO_MMD_AN, MDIO_AN_10GBT_CTRL,
+	err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, AQ_10G_CTRL,
 				  ADV_10G_FULL, adv);
 	if (err)
-		return err;
+		return (err);
 
 	/* 1G advertisement */
 	adv = 0;
@@ -170,10 +269,10 @@
 		adv |= ADV_1G_FULL;
 	if (advertise_map & ADVERTISED_1000baseT_Half)
 		adv |= ADV_1G_HALF;
-	err = t3_mdio_change_bits(phy, MDIO_MMD_AN, AQ_1G_CTRL,
+	err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, AQ_1G_CTRL,
 				  ADV_1G_FULL | ADV_1G_HALF, adv);
 	if (err)
-		return err;
+		return (err);
 
 	/* 100M, pause advertisement */
 	adv = 0;
@@ -185,66 +284,142 @@
 		adv |= ADVERTISE_PAUSE_CAP;
 	if (advertise_map & ADVERTISED_Asym_Pause)
 		adv |= ADVERTISE_PAUSE_ASYM;
-	err = t3_mdio_change_bits(phy, MDIO_MMD_AN, MDIO_AN_ADVERTISE,
-				  0xfe0, adv);
+	err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, AQ_100M_CTRL, 0xfe0, adv);
 
-	return err;
+	return (err);
 }
 
-static int aq100x_set_loopback(struct cphy *phy, int mmd, int dir, int enable)
+static int
+aq100x_set_loopback(struct cphy *phy, int mmd, int dir, int enable)
 {
-	return mdio_set_flag(&phy->mdio, phy->mdio.prtad,
-			     MDIO_MMD_PMAPMD, MDIO_CTRL1,
-			     BMCR_LOOPBACK, enable);
+	return t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR,
+				   BMCR_LOOPBACK, enable ? BMCR_LOOPBACK : 0);
 }
 
-static int aq100x_set_speed_duplex(struct cphy *phy, int speed, int duplex)
+static int
+aq100x_set_speed_duplex(struct cphy *phy, int speed, int duplex)
 {
-	/* no can do */
-	return -1;
+	int err, set;
+
+	if (speed == SPEED_100)
+		set = BMCR_SPEED100;
+	else if (speed == SPEED_1000)
+		set = BMCR_SPEED1000;
+	else if (speed == SPEED_10000)
+		set = BMCR_SPEED1000 | BMCR_SPEED100;
+	else
+		return (EINVAL);
+
+	if (duplex != DUPLEX_FULL)
+		return (EINVAL);
+
+	err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, MII_BMCR,
+	    BMCR_RESET | BMCR_ANENABLE | BMCR_ANRESTART, 0);
+	if (err)
+		return (err);
+
+	err = t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR,
+	    BMCR_SPEED1000 | BMCR_SPEED100, set);
+	if (err)
+		return (err);
+
+	return (0);
 }
 
-static int aq100x_get_link_status(struct cphy *phy, int *link_ok,
-				  int *speed, int *duplex, int *fc)
+static int
+aq100x_get_link_status(struct cphy *phy, int *link_ok, int *speed, int *duplex,
+		       int *fc)
 {
 	int err;
-	unsigned int v;
+	unsigned int v, link = 0;
 
-	if (link_ok) {
-		err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, AQ_LINK_STAT, &v);
+	err = mdio_read(phy, MDIO_DEV_PMA_PMD, AQ_LINK_STAT, &v);
+	if (err)
+		return (err);
+	if (v == 0xffff || !(v & 1))
+		goto done;
+
+	err = mdio_read(phy, MDIO_DEV_ANEG, MII_BMCR, &v);
+	if (err)
+		return (err);
+	if (v & 0x8000)
+		goto done;
+	if (v & BMCR_ANENABLE) {
+
+		err = mdio_read(phy, MDIO_DEV_ANEG, 1, &v);
 		if (err)
-			return err;
+			return (err);
+		if ((v & 0x20) == 0)
+			goto done;
 
-		*link_ok = v & 1;
-		if (!*link_ok)
-			return 0;
+		err = mdio_read(phy, MDIO_DEV_ANEG, AQ_ANEG_STAT, &v);
+		if (err)
+			return (err);
+
+		if (speed) {
+			switch (v & 0x6) {
+			case 0x6: *speed = SPEED_10000;
+				break;
+			case 0x4: *speed = SPEED_1000;
+				break;
+			case 0x2: *speed = SPEED_100;
+				break;
+			case 0x0: *speed = SPEED_10;
+				break;
+			}
+		}
+
+		if (duplex)
+			*duplex = v & 1 ? DUPLEX_FULL : DUPLEX_HALF;
+
+		if (fc) {
+			unsigned int lpa, adv;
+			err = mdio_read(phy, MDIO_DEV_ANEG, 0x13, &lpa);
+			if (!err)
+				err = mdio_read(phy, MDIO_DEV_ANEG,
+				    AQ_100M_CTRL, &adv);
+			if (err)
+				return err;
+
+			if (lpa & adv & ADVERTISE_PAUSE_CAP)
+				*fc = PAUSE_RX | PAUSE_TX;
+			else if (lpa & ADVERTISE_PAUSE_CAP &&
+			    lpa & ADVERTISE_PAUSE_ASYM &&
+			    adv & ADVERTISE_PAUSE_ASYM)
+				*fc = PAUSE_TX;
+			else if (lpa & ADVERTISE_PAUSE_ASYM &&
+			    adv & ADVERTISE_PAUSE_CAP)
+				*fc = PAUSE_RX;
+			else
+				*fc = 0;
+		}
+
+	} else {
+		err = mdio_read(phy, MDIO_DEV_PMA_PMD, MII_BMCR, &v);
+		if (err)
+			return (err);
+
+		v &= BMCR_SPEED1000 | BMCR_SPEED100;
+		if (speed) {
+			if (v == (BMCR_SPEED1000 | BMCR_SPEED100))
+				*speed = SPEED_10000;
+			else if (v == BMCR_SPEED1000)
+				*speed = SPEED_1000;
+			else if (v == BMCR_SPEED100)
+				*speed = SPEED_100;
+			else
+				*speed = SPEED_10;
+		}
+
+		if (duplex)
+			*duplex = DUPLEX_FULL;
 	}
 
-	err = t3_mdio_read(phy, MDIO_MMD_AN, AQ_ANEG_STAT, &v);
-	if (err)
-		return err;
-
-	if (speed) {
-		switch (v & 0x6) {
-		case 0x6:
-			*speed = SPEED_10000;
-			break;
-		case 0x4:
-			*speed = SPEED_1000;
-			break;
-		case 0x2:
-			*speed = SPEED_100;
-			break;
-		case 0x0:
-			*speed = SPEED_10;
-			break;
-		}
-	}
-
-	if (duplex)
-		*duplex = v & 1 ? DUPLEX_FULL : DUPLEX_HALF;
-
-	return 0;
+	link = 1;
+done:
+	if (link_ok)
+		*link_ok = link;
+	return (0);
 }
 
 static struct cphy_ops aq100x_ops = {
@@ -260,23 +435,24 @@
 	.set_speed_duplex  = aq100x_set_speed_duplex,
 	.get_link_status   = aq100x_get_link_status,
 	.power_down        = aq100x_power_down,
-	.mmds 		   = MDIO_DEVS_PMAPMD | MDIO_DEVS_PCS | MDIO_DEVS_PHYXS,
 };
 
-int t3_aq100x_phy_prep(struct cphy *phy, struct adapter *adapter, int phy_addr,
+int
+t3_aq100x_phy_prep(pinfo_t *pinfo, int phy_addr,
 		       const struct mdio_ops *mdio_ops)
 {
+	struct cphy *phy = &pinfo->phy;
 	unsigned int v, v2, gpio, wait;
 	int err;
+	adapter_t *adapter = pinfo->adapter;
 
-	cphy_init(phy, adapter, phy_addr, &aq100x_ops, mdio_ops,
+	cphy_init(&pinfo->phy, adapter, pinfo, phy_addr, &aq100x_ops, mdio_ops,
 		  SUPPORTED_1000baseT_Full | SUPPORTED_10000baseT_Full |
-		  SUPPORTED_TP | SUPPORTED_Autoneg | SUPPORTED_AUI,
-		  "1000/10GBASE-T");
+		  SUPPORTED_TP | SUPPORTED_Autoneg | SUPPORTED_AUI |
+		  SUPPORTED_MISC_IRQ, "1000/10GBASE-T");
 
 	/*
-	 * The PHY has been out of reset ever since the system powered up.  So
-	 * we do a hard reset over here.
+	 * Hard reset the PHY.
 	 */
 	gpio = phy_addr ? F_GPIO10_OUT_VAL : F_GPIO6_OUT_VAL;
 	t3_set_reg_field(adapter, A_T3DBG_GPIO_EN, gpio, 0);
@@ -289,7 +465,7 @@
 	msleep(1000);
 	wait = 500; /* in 10ms increments */
 	do {
-		err = t3_mdio_read(phy, MDIO_MMD_VEND1, MDIO_CTRL1, &v);
+		err = mdio_read(phy, MDIO_DEV_PMA_PMD, MII_BMCR, &v);
 		if (err || v == 0xffff) {
 
 			/* Allow prep_adapter to succeed when ffff is read */
@@ -299,7 +475,7 @@
 			goto done;
 		}
 
-		v &= AQ_RESET;
+		v &= BMCR_RESET;
 		if (v)
 			msleep(10);
 	} while (v && --wait);
@@ -310,45 +486,34 @@
 		goto done; /* let prep_adapter succeed */
 	}
 
-	/* Datasheet says 3s max but this has been observed */
-	wait = (500 - wait) * 10 + 1000;
-	if (wait > 3000)
-		CH_WARN(adapter, "PHY%d: reset took %ums\n", phy_addr, wait);
+	/* Firmware version check. */
+	(void) mdio_read(phy, MDIO_DEV_VEND1, AQ_FW_VERSION, &v);
+	if (v < 0x115)
+		CH_WARN(adapter, "PHY%d: unknown firmware %d.%d\n", phy_addr,
+		    v >> 8, v & 0xff);
 
-	/* Firmware version check. */
-	t3_mdio_read(phy, MDIO_MMD_VEND1, AQ_FW_VERSION, &v);
-	if (v != 101)
-		CH_WARN(adapter, "PHY%d: unsupported firmware %d\n",
-			phy_addr, v);
-
-	/*
-	 * The PHY should start in really-low-power mode.  Prepare it for normal
-	 * operations.
-	 */
-	err = t3_mdio_read(phy, MDIO_MMD_VEND1, MDIO_CTRL1, &v);
-	if (err)
-		return err;
-	if (v & AQ_LOWPOWER) {
-		err = t3_mdio_change_bits(phy, MDIO_MMD_VEND1, MDIO_CTRL1,
-					  AQ_LOWPOWER, 0);
-		if (err)
-			return err;
-		msleep(10);
-	} else
+	/* The PHY should start in really-low-power mode. */
+	(void) mdio_read(phy, MDIO_DEV_PMA_PMD, MII_BMCR, &v);
+	if ((v & BMCR_PDOWN) == 0)
 		CH_WARN(adapter, "PHY%d does not start in low power mode.\n",
 			phy_addr);
 
 	/*
-	 * Verify XAUI settings, but let prep succeed no matter what.
+	 * Verify XAUI and 1000-X settings, but let prep succeed no matter what.
 	 */
 	v = v2 = 0;
-	t3_mdio_read(phy, MDIO_MMD_PHYXS, AQ_XAUI_RX_CFG, &v);
-	t3_mdio_read(phy, MDIO_MMD_PHYXS, AQ_XAUI_TX_CFG, &v2);
+	(void) mdio_read(phy, MDIO_DEV_XGXS, AQ_XAUI_RX_CFG, &v);
+	(void) mdio_read(phy, MDIO_DEV_XGXS, AQ_XAUI_TX_CFG, &v2);
 	if (v != 0x1b || v2 != 0x1b)
-		CH_WARN(adapter,
-			"PHY%d: incorrect XAUI settings (0x%x, 0x%x).\n",
-			phy_addr, v, v2);
+		CH_WARN(adapter, "PHY%d: incorrect XAUI settings "
+		    "(0x%x, 0x%x).\n", phy_addr, v, v2);
+	v = 0;
+	(void) mdio_read(phy, MDIO_DEV_XGXS, AQ_XAUI_KX_CFG, &v);
+	if ((v & 0xf) != 0xf)
+		CH_WARN(adapter, "PHY%d: incorrect 1000-X settings "
+		    "(0x%x).\n", phy_addr, v);
 
+	(void) aq100x_set_defaults(phy);
 done:
-	return err;
+	return (err);
 }
diff --git a/drivers/net/cxgb3/common.h b/drivers/net/cxgb3/common.h
--- a/drivers/net/cxgb3/common.h
+++ b/drivers/net/cxgb3/common.h
@@ -1,128 +1,53 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef __CHELSIO_COMMON_H
 #define __CHELSIO_COMMON_H
 
-#include <linux/kernel.h>
-#include <linux/types.h>
-#include <linux/ctype.h>
-#include <linux/delay.h>
-#include <linux/init.h>
-#include <linux/netdevice.h>
-#include <linux/ethtool.h>
-#include <linux/mdio.h>
-#include "version.h"
-
-#define CH_ERR(adap, fmt, ...)   dev_err(&adap->pdev->dev, fmt, ## __VA_ARGS__)
-#define CH_WARN(adap, fmt, ...)  dev_warn(&adap->pdev->dev, fmt, ## __VA_ARGS__)
-#define CH_ALERT(adap, fmt, ...) \
-	dev_printk(KERN_ALERT, &adap->pdev->dev, fmt, ## __VA_ARGS__)
-
-/*
- * More powerful macro that selectively prints messages based on msg_enable.
- * For info and debugging messages.
- */
-#define CH_MSG(adapter, level, category, fmt, ...) do { \
-	if ((adapter)->msg_enable & NETIF_MSG_##category) \
-		dev_printk(KERN_##level, &adapter->pdev->dev, fmt, \
-			   ## __VA_ARGS__); \
-} while (0)
-
-#ifdef DEBUG
-# define CH_DBG(adapter, category, fmt, ...) \
-	CH_MSG(adapter, DEBUG, category, fmt, ## __VA_ARGS__)
-#else
-# define CH_DBG(adapter, category, fmt, ...)
-#endif
-
-/* Additional NETIF_MSG_* categories */
-#define NETIF_MSG_MMIO 0x8000000
-
-struct t3_rx_mode {
-	struct net_device *dev;
-	struct dev_mc_list *mclist;
-	unsigned int idx;
-};
-
-static inline void init_rx_mode(struct t3_rx_mode *p, struct net_device *dev,
-				struct dev_mc_list *mclist)
-{
-	p->dev = dev;
-	p->mclist = mclist;
-	p->idx = 0;
-}
-
-static inline u8 *t3_get_next_mcaddr(struct t3_rx_mode *rm)
-{
-	u8 *addr = NULL;
-
-	if (rm->mclist && rm->idx < rm->dev->mc_count) {
-		addr = rm->mclist->dmi_addr;
-		rm->mclist = rm->mclist->next;
-		rm->idx++;
-	}
-	return addr;
-}
+#include "osdep.h"
 
 enum {
-	MAX_NPORTS = 2,		/* max # of ports */
-	MAX_FRAME_SIZE = 10240,	/* max MAC frame size, including header + FCS */
-	EEPROMSIZE = 8192,	/* Serial EEPROM size */
+	MAX_FRAME_SIZE = 10240, /* max MAC frame size, includes header + FCS */
+	EEPROMSIZE     = 8192,  /* Serial EEPROM size */
 	SERNUM_LEN     = 16,    /* Serial # length */
-	RSS_TABLE_SIZE = 64,	/* size of RSS lookup and mapping tables */
-	TCB_SIZE = 128,		/* TCB size */
-	NMTUS = 16,		/* size of MTU table */
-	NCCTRL_WIN = 32,	/* # of congestion control windows */
-	PROTO_SRAM_LINES = 128, /* size of TP sram */
+	ECNUM_LEN      = 16,    /* EC # length */
+	RSS_TABLE_SIZE = 64,    /* size of RSS lookup and mapping tables */
+	TCB_SIZE       = 128,   /* TCB size */
+	NMTUS          = 16,    /* size of MTU table */
+	NCCTRL_WIN     = 32,    /* # of congestion control windows */
+	NTX_SCHED      = 8,     /* # of HW Tx scheduling queues */
+	PROTO_SRAM_LINES = 128, /* size of protocol sram */
+	EXACT_ADDR_FILTERS = 8,	/* # of HW exact match filters */
 };
 
 #define MAX_RX_COALESCING_LEN 12288U
 
 enum {
-	PAUSE_RX = 1 << 0,
-	PAUSE_TX = 1 << 1,
+	PAUSE_RX      = 1 << 0,
+	PAUSE_TX      = 1 << 1,
 	PAUSE_AUTONEG = 1 << 2
 };
 
 enum {
-	SUPPORTED_IRQ      = 1 << 24
+	SUPPORTED_LINK_IRQ = 1 << 24,
+	SUPPORTED_MISC_IRQ = 1 << 26,
+	SUPPORTED_IRQ      = (SUPPORTED_LINK_IRQ | SUPPORTED_MISC_IRQ)
 };
 
-enum {				/* adapter interrupt-maintained statistics */
+enum {                            /* adapter interrupt-maintained statistics */
 	STAT_ULP_CH0_PBL_OOB,
 	STAT_ULP_CH1_PBL_OOB,
 	STAT_PCI_CORR_ECC,
 
-	IRQ_NUM_STATS		/* keep last */
+	IRQ_NUM_STATS             /* keep last */
 };
 
 enum {
@@ -131,6 +56,12 @@
 	TP_VERSION_MICRO	= 0
 };
 
+enum {
+	TP_VERSION_MAJOR_T3B	= 1,
+	TP_VERSION_MINOR_T3B	= 1,
+	TP_VERSION_MICRO_T3B	= 0
+};
+
 #define S_TP_VERSION_MAJOR		16
 #define M_TP_VERSION_MAJOR		0xFF
 #define V_TP_VERSION_MAJOR(x)		((x) << S_TP_VERSION_MAJOR)
@@ -150,24 +81,47 @@
 	    (((x) >> S_TP_VERSION_MICRO) & M_TP_VERSION_MICRO)
 
 enum {
-	SGE_QSETS = 8,		/* # of SGE Tx/Rx/RspQ sets */
-	SGE_RXQ_PER_SET = 2,	/* # of Rx queues per set */
-	SGE_TXQ_PER_SET = 3	/* # of Tx queues per set */
+	FW_VERSION_MAJOR = 7,
+	FW_VERSION_MINOR = 12,
+	FW_VERSION_MICRO = 0
 };
 
-enum sge_context_type {		/* SGE egress context types */
+enum {
+	LA_CTRL = 0x80,
+	LA_DATA = 0x84,
+	LA_ENTRIES = 512
+};
+
+enum {
+	IOQ_ENTRIES = 7
+};
+
+struct t3_ioq_entry {
+	u32 ioq_cp;
+	u32 ioq_pp;
+	u32 ioq_alen;
+	u32 ioq_stats;
+};
+	
+enum {
+	SGE_QSETS = 8,            /* # of SGE Tx/Rx/RspQ sets */
+	SGE_RXQ_PER_SET = 2,      /* # of Rx queues per set */
+	SGE_TXQ_PER_SET = 3       /* # of Tx queues per set */
+};
+
+enum sge_context_type {           /* SGE egress context types */
 	SGE_CNTXT_RDMA = 0,
-	SGE_CNTXT_ETH = 2,
+	SGE_CNTXT_ETH  = 2,
 	SGE_CNTXT_OFLD = 4,
 	SGE_CNTXT_CTRL = 5
 };
 
 enum {
-	AN_PKT_SIZE = 32,	/* async notification packet size */
-	IMMED_PKT_SIZE = 48	/* packet size for immediate data */
+	AN_PKT_SIZE    = 32,      /* async notification packet size */
+	IMMED_PKT_SIZE = 48       /* packet size for immediate data */
 };
 
-struct sg_ent {			/* SGE scatter/gather entry */
+struct sg_ent {                   /* SGE scatter/gather entry */
 	__be32 len[2];
 	__be64 addr[2];
 };
@@ -180,26 +134,26 @@
 #define TX_DESC_FLITS 16U
 #define WR_FLITS (TX_DESC_FLITS + 1 - SGE_NUM_GENBITS)
 
+#define MAX_PHYINTRS 4
+
 struct cphy;
-struct adapter;
 
 struct mdio_ops {
-	int (*read)(struct net_device *dev, int phy_addr, int mmd_addr,
-		    u16 reg_addr);
-	int (*write)(struct net_device *dev, int phy_addr, int mmd_addr,
-		     u16 reg_addr, u16 val);
-	unsigned mode_support;
+	int  (*read)(adapter_t *adapter, int phy_addr, int mmd_addr,
+		     int reg_addr, unsigned int *val);
+	int  (*write)(adapter_t *adapter, int phy_addr, int mmd_addr,
+		      int reg_addr, unsigned int val);
 };
 
 struct adapter_info {
-	unsigned char nports0;        /* # of ports on channel 0 */
-	unsigned char nports1;        /* # of ports on channel 1 */
-	unsigned char phy_base_addr;	/* MDIO PHY base address */
-	unsigned int gpio_out;	/* GPIO output settings */
-	unsigned char gpio_intr[MAX_NPORTS]; /* GPIO PHY IRQ pins */
-	unsigned long caps;	/* adapter capabilities */
-	const struct mdio_ops *mdio_ops;	/* MDIO operations */
-	const char *desc;	/* product description */
+	unsigned char          nports0;        /* # of ports on channel 0 */
+	unsigned char          nports1;        /* # of ports on channel 1 */
+	unsigned char          phy_base_addr;  /* MDIO PHY base address */
+	unsigned int           gpio_out;       /* GPIO output settings */
+	unsigned char gpio_intr[MAX_PHYINTRS]; /* GPIO PHY IRQ pins */
+	unsigned long          caps;           /* adapter capabilities */
+	const struct mdio_ops *mdio_ops;       /* MDIO operations */
+	const char            *desc;           /* product description */
 };
 
 struct mc5_stats {
@@ -220,23 +174,23 @@
 };
 
 struct mac_stats {
-	u64 tx_octets;		/* total # of octets in good frames */
-	u64 tx_octets_bad;	/* total # of octets in error frames */
-	u64 tx_frames;		/* all good frames */
-	u64 tx_mcast_frames;	/* good multicast frames */
-	u64 tx_bcast_frames;	/* good broadcast frames */
-	u64 tx_pause;		/* # of transmitted pause frames */
-	u64 tx_deferred;	/* frames with deferred transmissions */
-	u64 tx_late_collisions;	/* # of late collisions */
-	u64 tx_total_collisions;	/* # of total collisions */
-	u64 tx_excess_collisions;	/* frame errors from excessive collissions */
-	u64 tx_underrun;	/* # of Tx FIFO underruns */
-	u64 tx_len_errs;	/* # of Tx length errors */
-	u64 tx_mac_internal_errs;	/* # of internal MAC errors on Tx */
-	u64 tx_excess_deferral;	/* # of frames with excessive deferral */
-	u64 tx_fcs_errs;	/* # of frames with bad FCS */
+	u64 tx_octets;            /* total # of octets in good frames */
+	u64 tx_octets_bad;        /* total # of octets in error frames */
+	u64 tx_frames;            /* all good frames */
+	u64 tx_mcast_frames;      /* good multicast frames */
+	u64 tx_bcast_frames;      /* good broadcast frames */
+	u64 tx_pause;             /* # of transmitted pause frames */
+	u64 tx_deferred;          /* frames with deferred transmissions */
+	u64 tx_late_collisions;   /* # of late collisions */
+	u64 tx_total_collisions;  /* # of total collisions */
+	u64 tx_excess_collisions; /* frame errors from excessive collissions */
+	u64 tx_underrun;          /* # of Tx FIFO underruns */
+	u64 tx_len_errs;          /* # of Tx length errors */
+	u64 tx_mac_internal_errs; /* # of internal MAC errors on Tx */
+	u64 tx_excess_deferral;   /* # of frames with excessive deferral */
+	u64 tx_fcs_errs;          /* # of frames with bad FCS */
 
-	u64 tx_frames_64;	/* # of Tx frames in a particular range */
+	u64 tx_frames_64;         /* # of Tx frames in a particular range */
 	u64 tx_frames_65_127;
 	u64 tx_frames_128_255;
 	u64 tx_frames_256_511;
@@ -244,24 +198,24 @@
 	u64 tx_frames_1024_1518;
 	u64 tx_frames_1519_max;
 
-	u64 rx_octets;		/* total # of octets in good frames */
-	u64 rx_octets_bad;	/* total # of octets in error frames */
-	u64 rx_frames;		/* all good frames */
-	u64 rx_mcast_frames;	/* good multicast frames */
-	u64 rx_bcast_frames;	/* good broadcast frames */
-	u64 rx_pause;		/* # of received pause frames */
-	u64 rx_fcs_errs;	/* # of received frames with bad FCS */
-	u64 rx_align_errs;	/* alignment errors */
-	u64 rx_symbol_errs;	/* symbol errors */
-	u64 rx_data_errs;	/* data errors */
-	u64 rx_sequence_errs;	/* sequence errors */
-	u64 rx_runt;		/* # of runt frames */
-	u64 rx_jabber;		/* # of jabber frames */
-	u64 rx_short;		/* # of short frames */
-	u64 rx_too_long;	/* # of oversized frames */
-	u64 rx_mac_internal_errs;	/* # of internal MAC errors on Rx */
+	u64 rx_octets;            /* total # of octets in good frames */
+	u64 rx_octets_bad;        /* total # of octets in error frames */
+	u64 rx_frames;            /* all good frames */
+	u64 rx_mcast_frames;      /* good multicast frames */
+	u64 rx_bcast_frames;      /* good broadcast frames */
+	u64 rx_pause;             /* # of received pause frames */
+	u64 rx_fcs_errs;          /* # of received frames with bad FCS */
+	u64 rx_align_errs;        /* alignment errors */
+	u64 rx_symbol_errs;       /* symbol errors */
+	u64 rx_data_errs;         /* data errors */
+	u64 rx_sequence_errs;     /* sequence errors */
+	u64 rx_runt;              /* # of runt frames */
+	u64 rx_jabber;            /* # of jabber frames */
+	u64 rx_short;             /* # of short frames */
+	u64 rx_too_long;          /* # of oversized frames */
+	u64 rx_mac_internal_errs; /* # of internal MAC errors on Rx */
 
-	u64 rx_frames_64;	/* # of Rx frames in a particular range */
+	u64 rx_frames_64;         /* # of Rx frames in a particular range */
 	u64 rx_frames_65_127;
 	u64 rx_frames_128_255;
 	u64 rx_frames_256_511;
@@ -269,7 +223,7 @@
 	u64 rx_frames_1024_1518;
 	u64 rx_frames_1519_max;
 
-	u64 rx_cong_drops;	/* # of Rx drops due to SGE congestion */
+	u64 rx_cong_drops;        /* # of Rx drops due to SGE congestion */
 
 	unsigned long tx_fifo_parity_err;
 	unsigned long rx_fifo_parity_err;
@@ -330,41 +284,43 @@
 };
 
 struct tp_params {
-	unsigned int nchan;	/* # of channels */
-	unsigned int pmrx_size;	/* total PMRX capacity */
-	unsigned int pmtx_size;	/* total PMTX capacity */
-	unsigned int cm_size;	/* total CM capacity */
-	unsigned int chan_rx_size;	/* per channel Rx size */
-	unsigned int chan_tx_size;	/* per channel Tx size */
-	unsigned int rx_pg_size;	/* Rx page size */
-	unsigned int tx_pg_size;	/* Tx page size */
-	unsigned int rx_num_pgs;	/* # of Rx pages */
-	unsigned int tx_num_pgs;	/* # of Tx pages */
-	unsigned int ntimer_qs;	/* # of timer queues */
+	unsigned int nchan;          /* # of channels */
+	unsigned int pmrx_size;      /* total PMRX capacity */
+	unsigned int pmtx_size;      /* total PMTX capacity */
+	unsigned int cm_size;        /* total CM capacity */
+	unsigned int chan_rx_size;   /* per channel Rx size */
+	unsigned int chan_tx_size;   /* per channel Tx size */
+	unsigned int rx_pg_size;     /* Rx page size */
+	unsigned int tx_pg_size;     /* Tx page size */
+	unsigned int rx_num_pgs;     /* # of Rx pages */
+	unsigned int tx_num_pgs;     /* # of Tx pages */
+	unsigned int ntimer_qs;      /* # of timer queues */
+	unsigned int tre;            /* log2 of core clocks per TP tick */
+	unsigned int dack_re;        /* DACK timer resolution */
 };
 
-struct qset_params {		/* SGE queue set parameters */
-	unsigned int polling;	/* polling/interrupt service for rspq */
-	unsigned int lro;	/* large receive offload */
-	unsigned int coalesce_usecs;	/* irq coalescing timer */
-	unsigned int rspq_size;	/* # of entries in response queue */
-	unsigned int fl_size;	/* # of entries in regular free list */
-	unsigned int jumbo_size;	/* # of entries in jumbo free list */
-	unsigned int txq_size[SGE_TXQ_PER_SET];	/* Tx queue sizes */
-	unsigned int cong_thres;	/* FL congestion threshold */
-	unsigned int vector;		/* Interrupt (line or vector) number */
+struct qset_params {                   /* SGE queue set parameters */
+	unsigned int polling;          /* polling/interrupt service for rspq */
+	unsigned int lro;              /* large receive offload */
+	unsigned int coalesce_usecs;   /* irq coalescing timer */
+	unsigned int rspq_size;        /* # of entries in response queue */
+	unsigned int fl_size;          /* # of entries in regular free list */
+	unsigned int jumbo_size;       /* # of entries in jumbo free list */
+	unsigned int txq_size[SGE_TXQ_PER_SET];  /* Tx queue sizes */
+	unsigned int cong_thres;       /* FL congestion threshold */
+	unsigned int vector;           /* Interrupt (line or vector) number */
 };
 
 struct sge_params {
-	unsigned int max_pkt_size;	/* max offload pkt size */
+	unsigned int max_pkt_size;     /* max offload pkt size */
 	struct qset_params qset[SGE_QSETS];
 };
 
 struct mc5_params {
-	unsigned int mode;	/* selects MC5 width */
-	unsigned int nservers;	/* size of server region */
-	unsigned int nfilters;	/* size of filter region */
-	unsigned int nroutes;	/* size of routing region */
+	unsigned int mode;       /* selects MC5 width */
+	unsigned int nservers;   /* size of server region */
+	unsigned int nfilters;   /* size of filter region */
+	unsigned int nroutes;    /* size of routing region */
 };
 
 /* Default MC5 region sizes */
@@ -376,30 +332,41 @@
 /* MC5 modes, these must be non-0 */
 enum {
 	MC5_MODE_144_BIT = 1,
-	MC5_MODE_72_BIT = 2
+	MC5_MODE_72_BIT  = 2
 };
 
 /* MC5 min active region size */
 enum { MC5_MIN_TIDS = 16 };
 
 struct vpd_params {
+	unsigned int rlen;
 	unsigned int cclk;
 	unsigned int mclk;
 	unsigned int uclk;
 	unsigned int mdc;
 	unsigned int mem_timing;
 	u8 sn[SERNUM_LEN + 1];
+	u8 ec[ECNUM_LEN + 1];
 	u8 eth_base[6];
 	u8 port_type[MAX_NPORTS];
 	unsigned short xauicfg[2];
+	unsigned int csum;
 };
 
+struct generic_vpd {
+	u32 offset;
+	u32 len;
+	u8 *data;
+};
+
+enum { MAX_VPD_BYTES = 32000 };
+
 struct pci_params {
-	unsigned int vpd_cap_addr;
-	unsigned int pcie_cap_addr;
+	unsigned int   vpd_cap_addr;
+	unsigned int   pcie_cap_addr;
 	unsigned short speed;
-	unsigned char width;
-	unsigned char variant;
+	unsigned char  width;
+	unsigned char  variant;
 };
 
 enum {
@@ -413,22 +380,23 @@
 struct adapter_params {
 	struct sge_params sge;
 	struct mc5_params mc5;
-	struct tp_params tp;
+	struct tp_params  tp;
 	struct vpd_params vpd;
 	struct pci_params pci;
 
 	const struct adapter_info *info;
 
+#ifdef CONFIG_CHELSIO_T3_CORE
 	unsigned short mtus[NMTUS];
 	unsigned short a_wnd[NCCTRL_WIN];
 	unsigned short b_wnd[NCCTRL_WIN];
-
-	unsigned int nports;	/* # of ethernet ports */
-	unsigned int chan_map;  /* bitmap of in-use Tx channels */
-	unsigned int stats_update_period;	/* MAC stats accumulation period */
-	unsigned int linkpoll_period;	/* link poll period in 0.1s */
-	unsigned int rev;	/* chip revision */
-	unsigned int offload;
+#endif
+	unsigned int   nports;              /* # of ethernet ports */
+	unsigned int   chan_map;            /* bitmap of in-use Tx channels */
+	unsigned int   stats_update_period; /* MAC stats accumulation period */
+	unsigned int   linkpoll_period;     /* link poll period in 0.1s */
+	unsigned int   rev;                 /* chip revision */
+	unsigned int   offload;
 };
 
 enum {					    /* chip revisions */
@@ -451,28 +419,28 @@
 	u32 vlan_mask:12;
 	u32 intf:4;
 	u32 intf_mask:4;
-	u8 proto;
-	u8 proto_mask;
+	u8  proto;
+	u8  proto_mask;
 };
 
 struct link_config {
-	unsigned int supported;	/* link capabilities */
-	unsigned int advertising;	/* advertised capabilities */
-	unsigned short requested_speed;	/* speed user has requested */
-	unsigned short speed;	/* actual link speed */
-	unsigned char requested_duplex;	/* duplex user has requested */
-	unsigned char duplex;	/* actual link duplex */
-	unsigned char requested_fc;	/* flow control user has requested */
-	unsigned char fc;	/* actual link flow control */
-	unsigned char autoneg;	/* autonegotiating? */
-	unsigned int link_ok;	/* link up? */
+	unsigned int   supported;        /* link capabilities */
+	unsigned int   advertising;      /* advertised capabilities */
+	unsigned short requested_speed;  /* speed user has requested */
+	unsigned short speed;            /* actual link speed */
+	unsigned char  requested_duplex; /* duplex user has requested */
+	unsigned char  duplex;           /* actual link duplex */
+	unsigned char  requested_fc;     /* flow control user has requested */
+	unsigned char  fc;               /* actual link flow control */
+	unsigned char  autoneg;          /* autonegotiating? */
+	unsigned int   link_ok;          /* link up? */
 };
 
 #define SPEED_INVALID   0xffff
 #define DUPLEX_INVALID  0xff
 
 struct mc5 {
-	struct adapter *adapter;
+	adapter_t *adapter;
 	unsigned int tcam_size;
 	unsigned char part_type;
 	unsigned char parity_enabled;
@@ -486,12 +454,12 @@
 }
 
 struct mc7 {
-	struct adapter *adapter;	/* backpointer to adapter */
-	unsigned int size;	/* memory size in bytes */
-	unsigned int width;	/* MC7 interface width */
-	unsigned int offset;	/* register address offset for MC7 instance */
-	const char *name;	/* name of MC7 instance */
-	struct mc7_stats stats;	/* MC7 statistics */
+	adapter_t *adapter;     /* backpointer to adapter */
+	unsigned int size;      /* memory size in bytes */
+	unsigned int width;     /* MC7 interface width */
+	unsigned int offset;    /* register address offset for MC7 instance */
+	const char *name;       /* name of MC7 instance */
+	struct mc7_stats stats; /* MC7 statistics */
 };
 
 static inline unsigned int t3_mc7_size(const struct mc7 *p)
@@ -500,9 +468,12 @@
 }
 
 struct cmac {
-	struct adapter *adapter;
+	adapter_t *adapter;
 	unsigned int offset;
-	unsigned int nucast;	/* # of address filters for unicast MACs */
+	unsigned char nucast;    /* # of address filters for unicast MACs */
+	unsigned char multiport; /* multiple ports connected to this MAC */
+	unsigned char ext_port;  /* external MAC port */
+	unsigned char promisc_map;  /* which external ports are promiscuous */
 	unsigned int tx_tcnt;
 	unsigned int tx_xcnt;
 	u64 tx_mcnt;
@@ -511,6 +482,7 @@
 	u64 rx_mcnt;
 	unsigned int toggle_cnt;
 	unsigned int txen;
+	unsigned int was_reset;
 	u64 rx_pause;
 	struct mac_stats stats;
 };
@@ -518,7 +490,28 @@
 enum {
 	MAC_DIRECTION_RX = 1,
 	MAC_DIRECTION_TX = 2,
-	MAC_RXFIFO_SIZE = 32768
+	MAC_RXFIFO_SIZE  = 32768
+};
+
+/* IEEE 802.3 specified MDIO devices */
+enum {
+	MDIO_DEV_PMA_PMD = 1,
+	MDIO_DEV_WIS     = 2,
+	MDIO_DEV_PCS     = 3,
+	MDIO_DEV_XGXS    = 4,
+	MDIO_DEV_ANEG    = 7,
+	MDIO_DEV_VEND1   = 30,
+	MDIO_DEV_VEND2   = 31
+};
+
+/* LASI control and status registers */
+enum {
+	RX_ALARM_CTRL = 0x9000,
+	TX_ALARM_CTRL = 0x9001,
+	LASI_CTRL     = 0x9002,
+	RX_ALARM_STAT = 0x9003,
+	TX_ALARM_STAT = 0x9004,
+	LASI_STAT     = 0x9005
 };
 
 /* PHY loopback direction */
@@ -532,6 +525,7 @@
 	cphy_cause_link_change = 1,
 	cphy_cause_fifo_error = 2,
 	cphy_cause_module_change = 4,
+	cphy_cause_alarm = 8,
 };
 
 /* PHY module types */
@@ -563,70 +557,69 @@
 	int (*get_link_status)(struct cphy *phy, int *link_ok, int *speed,
 			       int *duplex, int *fc);
 	int (*power_down)(struct cphy *phy, int enable);
-
-	u32 mmds;
 };
 enum {
-	EDC_OPT_AEL2005 = 0,
-	EDC_OPT_AEL2005_SIZE = 1084,
-	EDC_TWX_AEL2005 = 1,
-	EDC_TWX_AEL2005_SIZE = 1464,
-	EDC_TWX_AEL2020 = 2,
-	EDC_TWX_AEL2020_SIZE = 1628,
-	EDC_MAX_SIZE = EDC_TWX_AEL2020_SIZE, /* Max cache size */
+        EDC_OPT_AEL2005 = 0,
+        EDC_TWX_AEL2005 = 1,
+        EDC_TWX_AEL2020 = 2,
+	EDC_MAX_SIZE = 2048, /* Max PHY EDC cache size */
 };
 
 /* A PHY instance */
 struct cphy {
-	u8 modtype;			/* PHY module type */
-	short priv;			/* scratch pad */
-	unsigned int caps;		/* PHY capabilities */
-	struct adapter *adapter;	/* associated adapter */
-	const char *desc;		/* PHY description */
-	unsigned long fifo_errors;	/* FIFO over/under-flows */
-	const struct cphy_ops *ops;	/* PHY operations */
-	struct mdio_if_info mdio;
-	u16 phy_cache[EDC_MAX_SIZE];	/* EDC cache */
+	u8 addr;                             /* PHY address */
+	u8 modtype;                          /* PHY module type */
+	unsigned int priv;                   /* scratch pad */
+	unsigned int caps;                   /* PHY capabilities */
+	adapter_t *adapter;                  /* associated adapter */
+	pinfo_t *pinfo;                      /* associated port */
+	const char *desc;                    /* PHY description */
+	unsigned long fifo_errors;           /* FIFO over/under-flows */
+	const struct cphy_ops *ops;          /* PHY operations */
+	u16 phy_cache[EDC_MAX_SIZE];         /* EDC cache */
+	int (*mdio_read)(adapter_t *adapter, int phy_addr, int mmd_addr,
+			 int reg_addr, unsigned int *val);
+	int (*mdio_write)(adapter_t *adapter, int phy_addr, int mmd_addr,
+			  int reg_addr, unsigned int val);
 };
 
 /* Convenience MDIO read/write wrappers */
-static inline int t3_mdio_read(struct cphy *phy, int mmd, int reg,
-			       unsigned int *valp)
+static inline int mdio_read(struct cphy *phy, int mmd, int reg,
+			    unsigned int *valp)
 {
-	int rc = phy->mdio.mdio_read(phy->mdio.dev, phy->mdio.prtad, mmd, reg);
-	*valp = (rc >= 0) ? rc : -1;
-	return (rc >= 0) ? 0 : rc;
+	return phy->mdio_read(phy->adapter, phy->addr, mmd, reg, valp);
 }
 
-static inline int t3_mdio_write(struct cphy *phy, int mmd, int reg,
-				unsigned int val)
+static inline int mdio_write(struct cphy *phy, int mmd, int reg,
+			     unsigned int val)
 {
-	return phy->mdio.mdio_write(phy->mdio.dev, phy->mdio.prtad, mmd,
-				    reg, val);
+	return phy->mdio_write(phy->adapter, phy->addr, mmd, reg, val);
 }
 
 /* Convenience initializer */
-static inline void cphy_init(struct cphy *phy, struct adapter *adapter,
+static inline void cphy_init(struct cphy *phy, adapter_t *adapter, pinfo_t *pinfo,
 			     int phy_addr, struct cphy_ops *phy_ops,
-			     const struct mdio_ops *mdio_ops,
-			      unsigned int caps, const char *desc)
+			     const struct mdio_ops *mdio_ops, unsigned int caps,
+			     const char *desc)
 {
-	phy->caps = caps;
+	phy->addr    = (u8)phy_addr;
+	phy->caps    = caps;
 	phy->adapter = adapter;
-	phy->desc = desc;
-	phy->ops = phy_ops;
+	phy->pinfo   = pinfo;
+	phy->desc    = desc;
+	phy->ops     = phy_ops;
 	if (mdio_ops) {
-		phy->mdio.prtad = phy_addr;
-		phy->mdio.mmds = phy_ops->mmds;
-		phy->mdio.mode_support = mdio_ops->mode_support;
-		phy->mdio.mdio_read = mdio_ops->read;
-		phy->mdio.mdio_write = mdio_ops->write;
+		phy->mdio_read  = mdio_ops->read;
+		phy->mdio_write = mdio_ops->write;
 	}
 }
 
 /* Accumulate MAC statistics every 180 seconds.  For 1G we multiply by 10. */
 #define MAC_STATS_ACCUM_SECS 180
 
+/* The external MAC needs accumulation every 30 seconds */
+#define VSC_STATS_ACCUM_SECS 30
+
 #define XGM_REG(reg_addr, idx) \
 	((reg_addr) + (idx) * (XGMAC0_1_BASE_ADDR - XGMAC0_0_BASE_ADDR))
 
@@ -646,43 +639,54 @@
 
 #define adapter_info(adap) ((adap)->params.info)
 
-static inline int uses_xaui(const struct adapter *adap)
+static inline int uses_xaui(const adapter_t *adap)
 {
 	return adapter_info(adap)->caps & SUPPORTED_AUI;
 }
 
-static inline int is_10G(const struct adapter *adap)
+static inline int is_10G(const adapter_t *adap)
 {
 	return adapter_info(adap)->caps & SUPPORTED_10000baseT_Full;
 }
 
-static inline int is_offload(const struct adapter *adap)
+static inline int is_offload(const adapter_t *adap)
 {
+#if defined(CONFIG_CHELSIO_T3_CORE)
 	return adap->params.offload;
+#else
+	return 0;
+#endif
 }
 
-static inline unsigned int core_ticks_per_usec(const struct adapter *adap)
+static inline unsigned int core_ticks_per_usec(const adapter_t *adap)
 {
 	return adap->params.vpd.cclk / 1000;
 }
 
-static inline unsigned int is_pcie(const struct adapter *adap)
+static inline unsigned int dack_ticks_to_usec(const adapter_t *adap,
+					      unsigned int ticks)
+{
+	return (ticks << adap->params.tp.dack_re) / core_ticks_per_usec(adap);
+}
+
+static inline unsigned int is_pcie(const adapter_t *adap)
 {
 	return adap->params.pci.variant == PCI_VARIANT_PCIE;
 }
 
-void t3_set_reg_field(struct adapter *adap, unsigned int addr, u32 mask,
-		      u32 val);
-void t3_write_regs(struct adapter *adapter, const struct addr_val_pair *p,
-		   int n, unsigned int offset);
-int t3_wait_op_done_val(struct adapter *adapter, int reg, u32 mask,
-			int polarity, int attempts, int delay, u32 *valp);
-static inline int t3_wait_op_done(struct adapter *adapter, int reg, u32 mask,
+void t3_set_reg_field(adapter_t *adap, unsigned int addr, u32 mask, u32 val);
+void t3_write_regs(adapter_t *adapter, const struct addr_val_pair *p, int n,
+		   unsigned int offset);
+int t3_wait_op_done_val(adapter_t *adapter, int reg, u32 mask, int polarity,
+			int attempts, int delay, u32 *valp);
+
+static inline int t3_wait_op_done(adapter_t *adapter, int reg, u32 mask,
 				  int polarity, int attempts, int delay)
 {
 	return t3_wait_op_done_val(adapter, reg, mask, polarity, attempts,
 				   delay, NULL);
 }
+
 int t3_mdio_change_bits(struct cphy *phy, int mmd, int reg, unsigned int clear,
 			unsigned int set);
 int t3_phy_reset(struct cphy *phy, int mmd, int wait);
@@ -694,55 +698,64 @@
 int t3_phy_lasi_intr_clear(struct cphy *phy);
 int t3_phy_lasi_intr_handler(struct cphy *phy);
 
-void t3_intr_enable(struct adapter *adapter);
-void t3_intr_disable(struct adapter *adapter);
-void t3_intr_clear(struct adapter *adapter);
-void t3_xgm_intr_enable(struct adapter *adapter, int idx);
-void t3_xgm_intr_disable(struct adapter *adapter, int idx);
-void t3_port_intr_enable(struct adapter *adapter, int idx);
-void t3_port_intr_disable(struct adapter *adapter, int idx);
-void t3_port_intr_clear(struct adapter *adapter, int idx);
-int t3_slow_intr_handler(struct adapter *adapter);
-int t3_phy_intr_handler(struct adapter *adapter);
+void t3_intr_enable(adapter_t *adapter);
+void t3_intr_disable(adapter_t *adapter);
+void t3_intr_clear(adapter_t *adapter);
+void t3_xgm_intr_enable(adapter_t *adapter, int idx);
+void t3_xgm_intr_disable(adapter_t *adapter, int idx);
+void t3_port_intr_enable(adapter_t *adapter, int idx);
+void t3_port_intr_disable(adapter_t *adapter, int idx);
+void t3_port_intr_clear(adapter_t *adapter, int idx);
+int t3_slow_intr_handler(adapter_t *adapter);
+int t3_phy_intr_handler(adapter_t *adapter);
 
-void t3_link_changed(struct adapter *adapter, int port_id);
-void t3_link_fault(struct adapter *adapter, int port_id);
+void t3_link_changed(adapter_t *adapter, int port_id);
 int t3_link_start(struct cphy *phy, struct cmac *mac, struct link_config *lc);
 const struct adapter_info *t3_get_adapter_info(unsigned int board_id);
-int t3_seeprom_read(struct adapter *adapter, u32 addr, __le32 *data);
-int t3_seeprom_write(struct adapter *adapter, u32 addr, __le32 data);
-int t3_seeprom_wp(struct adapter *adapter, int enable);
-int t3_get_tp_version(struct adapter *adapter, u32 *vers);
-int t3_check_tpsram_version(struct adapter *adapter);
-int t3_check_tpsram(struct adapter *adapter, const u8 *tp_ram,
-		    unsigned int size);
-int t3_set_proto_sram(struct adapter *adap, const u8 *data);
-int t3_read_flash(struct adapter *adapter, unsigned int addr,
-		  unsigned int nwords, u32 *data, int byte_oriented);
-int t3_load_fw(struct adapter *adapter, const u8 * fw_data, unsigned int size);
-int t3_get_fw_version(struct adapter *adapter, u32 *vers);
-int t3_check_fw_version(struct adapter *adapter);
-int t3_init_hw(struct adapter *adapter, u32 fw_params);
-void mac_prep(struct cmac *mac, struct adapter *adapter, int index);
-void early_hw_init(struct adapter *adapter, const struct adapter_info *ai);
-int t3_reset_adapter(struct adapter *adapter);
-int t3_prep_adapter(struct adapter *adapter, const struct adapter_info *ai,
-		    int reset);
-int t3_replay_prep_adapter(struct adapter *adapter);
-void t3_led_ready(struct adapter *adapter);
-void t3_fatal_err(struct adapter *adapter);
-void t3_set_vlan_accel(struct adapter *adapter, unsigned int ports, int on);
-void t3_config_rss(struct adapter *adapter, unsigned int rss_config,
-		   const u8 * cpus, const u16 *rspq);
-int t3_read_rss(struct adapter *adapter, u8 * lkup, u16 *map);
-int t3_mps_set_active_ports(struct adapter *adap, unsigned int port_mask);
-int t3_cim_ctl_blk_read(struct adapter *adap, unsigned int addr,
-			unsigned int n, unsigned int *valp);
+int t3_seeprom_read(adapter_t *adapter, u32 addr, u32 *data);
+int t3_seeprom_write(adapter_t *adapter, u32 addr, u32 data);
+int t3_seeprom_wp(adapter_t *adapter, int enable);
+int t3_get_vpd_len(adapter_t *adapter, struct generic_vpd *vpd);
+int t3_read_vpd(adapter_t *adapter, struct generic_vpd *vpd);
+int t3_read_flash(adapter_t *adapter, unsigned int addr, unsigned int nwords,
+		  u32 *data, int byte_oriented);
+int t3_get_tp_version(adapter_t *adapter, u32 *vers);
+int t3_check_tpsram_version(adapter_t *adapter);
+int t3_check_tpsram(adapter_t *adapter, const u8 *tp_ram, unsigned int size);
+int t3_load_fw(adapter_t *adapter, const u8 *fw_data, unsigned int size);
+int t3_get_fw_version(adapter_t *adapter, u32 *vers);
+int t3_check_fw_version(adapter_t *adapter);
+int t3_load_boot(adapter_t *adapter, const u8 *fw_data, unsigned int size);
+int t3_init_hw(adapter_t *adapter, u32 fw_params);
+void mac_prep(struct cmac *mac, adapter_t *adapter, int index);
+void early_hw_init(adapter_t *adapter, const struct adapter_info *ai);
+int t3_reset_adapter(adapter_t *adapter);
+int t3_prep_adapter(adapter_t *adapter, const struct adapter_info *ai, int reset);
+int t3_reinit_adapter(adapter_t *adap);
+void t3_led_ready(adapter_t *adapter);
+void t3_fatal_err(adapter_t *adapter);
+void t3_set_vlan_accel(adapter_t *adapter, unsigned int ports, int on);
+void t3_enable_filters(adapter_t *adap);
+void t3_disable_filters(adapter_t *adap);
+void t3_tp_set_offload_mode(adapter_t *adap, int enable);
+void t3_config_rss(adapter_t *adapter, unsigned int rss_config, const u8 *cpus,
+		   const u16 *rspq);
+int t3_read_rss(adapter_t *adapter, u8 *lkup, u16 *map);
+int t3_set_proto_sram(adapter_t *adap, const u8 *data);
+int t3_mps_set_active_ports(adapter_t *adap, unsigned int port_mask);
+void t3_port_failover(adapter_t *adapter, int port);
+void t3_failover_done(adapter_t *adapter, int port);
+void t3_failover_clear(adapter_t *adapter);
+int t3_cim_ctl_blk_read(adapter_t *adap, unsigned int addr, unsigned int n,
+			unsigned int *valp);
+int t3_cim_hac_read(adapter_t *adapter, u32 addr, u32 *val);
+int t3_cim_hac_write(adapter_t *adapter, u32 addr, u32 val);
 int t3_mc7_bd_read(struct mc7 *mc7, unsigned int start, unsigned int n,
 		   u64 *buf);
 
-int t3_mac_reset(struct cmac *mac);
+int t3_mac_init(struct cmac *mac);
 void t3b_pcs_reset(struct cmac *mac);
+void t3c_pcs_force_los(struct cmac *mac);
 void t3_mac_disable_exact_filters(struct cmac *mac);
 void t3_mac_enable_exact_filters(struct cmac *mac);
 int t3_mac_enable(struct cmac *mac, int which);
@@ -750,74 +763,115 @@
 int t3_mac_set_mtu(struct cmac *mac, unsigned int mtu);
 int t3_mac_set_rx_mode(struct cmac *mac, struct t3_rx_mode *rm);
 int t3_mac_set_address(struct cmac *mac, unsigned int idx, u8 addr[6]);
-int t3_mac_set_num_ucast(struct cmac *mac, int n);
+int t3_mac_set_num_ucast(struct cmac *mac, unsigned char n);
 const struct mac_stats *t3_mac_update_stats(struct cmac *mac);
-int t3_mac_set_speed_duplex_fc(struct cmac *mac, int speed, int duplex, int fc);
+int t3_mac_set_speed_duplex_fc(struct cmac *mac, int speed, int duplex,
+			       int fc);
 int t3b2_mac_watchdog_task(struct cmac *mac);
 
-void t3_mc5_prep(struct adapter *adapter, struct mc5 *mc5, int mode);
+void t3_mc5_prep(adapter_t *adapter, struct mc5 *mc5, int mode);
 int t3_mc5_init(struct mc5 *mc5, unsigned int nservers, unsigned int nfilters,
 		unsigned int nroutes);
 void t3_mc5_intr_handler(struct mc5 *mc5);
 int t3_read_mc5_range(const struct mc5 *mc5, unsigned int start, unsigned int n,
 		      u32 *buf);
 
-int t3_tp_set_coalescing_size(struct adapter *adap, unsigned int size, int psh);
-void t3_tp_set_max_rxsize(struct adapter *adap, unsigned int size);
-void t3_tp_set_offload_mode(struct adapter *adap, int enable);
-void t3_tp_get_mib_stats(struct adapter *adap, struct tp_mib_stats *tps);
-void t3_load_mtus(struct adapter *adap, unsigned short mtus[NMTUS],
+#ifdef CONFIG_CHELSIO_T3_CORE
+int t3_tp_set_coalescing_size(adapter_t *adap, unsigned int size, int psh);
+void t3_tp_set_max_rxsize(adapter_t *adap, unsigned int size);
+void t3_tp_get_mib_stats(adapter_t *adap, struct tp_mib_stats *tps);
+void t3_load_mtus(adapter_t *adap, unsigned short mtus[NMTUS],
 		  unsigned short alpha[NCCTRL_WIN],
 		  unsigned short beta[NCCTRL_WIN], unsigned short mtu_cap);
-void t3_read_hw_mtus(struct adapter *adap, unsigned short mtus[NMTUS]);
-void t3_get_cong_cntl_tab(struct adapter *adap,
+void t3_read_hw_mtus(adapter_t *adap, unsigned short mtus[NMTUS]);
+void t3_get_cong_cntl_tab(adapter_t *adap,
 			  unsigned short incr[NMTUS][NCCTRL_WIN]);
-void t3_config_trace_filter(struct adapter *adapter,
-			    const struct trace_params *tp, int filter_index,
-			    int invert, int enable);
-int t3_config_sched(struct adapter *adap, unsigned int kbps, int sched);
+void t3_config_trace_filter(adapter_t *adapter, const struct trace_params *tp,
+			    int filter_index, int invert, int enable);
+void t3_query_trace_filter(adapter_t *adapter, struct trace_params *tp,
+			   int filter_index, int *inverted, int *enabled);
+int t3_config_sched(adapter_t *adap, unsigned int kbps, int sched);
+int t3_set_sched_ipg(adapter_t *adap, int sched, unsigned int ipg);
+void t3_get_tx_sched(adapter_t *adap, unsigned int sched, unsigned int *kbps,
+		     unsigned int *ipg);
+void t3_read_pace_tbl(adapter_t *adap, unsigned int pace_vals[NTX_SCHED]);
+void t3_set_pace_tbl(adapter_t *adap, unsigned int *pace_vals,
+		     unsigned int start, unsigned int n);
+#endif
 
-void t3_sge_prep(struct adapter *adap, struct sge_params *p);
-void t3_sge_init(struct adapter *adap, struct sge_params *p);
-int t3_sge_init_ecntxt(struct adapter *adapter, unsigned int id, int gts_enable,
+int t3_get_up_la(adapter_t *adapter, u32 *stopped, u32 *index,
+		 u32 *size, void *data);
+int t3_get_up_ioqs(adapter_t *adapter, u32 *size, void *data);
+
+void t3_sge_prep(adapter_t *adap, struct sge_params *p);
+void t3_sge_init(adapter_t *adap, struct sge_params *p);
+int t3_sge_init_ecntxt(adapter_t *adapter, unsigned int id, int gts_enable,
 		       enum sge_context_type type, int respq, u64 base_addr,
 		       unsigned int size, unsigned int token, int gen,
 		       unsigned int cidx);
-int t3_sge_init_flcntxt(struct adapter *adapter, unsigned int id,
-			int gts_enable, u64 base_addr, unsigned int size,
-			unsigned int esize, unsigned int cong_thres, int gen,
-			unsigned int cidx);
-int t3_sge_init_rspcntxt(struct adapter *adapter, unsigned int id,
-			 int irq_vec_idx, u64 base_addr, unsigned int size,
+int t3_sge_init_flcntxt(adapter_t *adapter, unsigned int id, int gts_enable,
+			u64 base_addr, unsigned int size, unsigned int esize,
+			unsigned int cong_thres, int gen, unsigned int cidx);
+int t3_sge_init_rspcntxt(adapter_t *adapter, unsigned int id, int irq_vec_idx,
+			 u64 base_addr, unsigned int size,
 			 unsigned int fl_thres, int gen, unsigned int cidx);
-int t3_sge_init_cqcntxt(struct adapter *adapter, unsigned int id, u64 base_addr,
-			unsigned int size, int rspq, int ovfl_mode,
+int t3_sge_init_cqcntxt(adapter_t *adapter, unsigned int id, u64 base_addr,
+ 			unsigned int size, int rspq, int ovfl_mode,
 			unsigned int credits, unsigned int credit_thres);
-int t3_sge_enable_ecntxt(struct adapter *adapter, unsigned int id, int enable);
-int t3_sge_disable_fl(struct adapter *adapter, unsigned int id);
-int t3_sge_disable_rspcntxt(struct adapter *adapter, unsigned int id);
-int t3_sge_disable_cqcntxt(struct adapter *adapter, unsigned int id);
-int t3_sge_read_ecntxt(struct adapter *adapter, unsigned int id, u32 data[4]);
-int t3_sge_read_fl(struct adapter *adapter, unsigned int id, u32 data[4]);
-int t3_sge_read_cq(struct adapter *adapter, unsigned int id, u32 data[4]);
-int t3_sge_read_rspq(struct adapter *adapter, unsigned int id, u32 data[4]);
-int t3_sge_cqcntxt_op(struct adapter *adapter, unsigned int id, unsigned int op,
+int t3_sge_enable_ecntxt(adapter_t *adapter, unsigned int id, int enable);
+int t3_sge_disable_fl(adapter_t *adapter, unsigned int id);
+int t3_sge_disable_rspcntxt(adapter_t *adapter, unsigned int id);
+int t3_sge_disable_cqcntxt(adapter_t *adapter, unsigned int id);
+int t3_sge_read_ecntxt(adapter_t *adapter, unsigned int id, u32 data[4]);
+int t3_sge_read_fl(adapter_t *adapter, unsigned int id, u32 data[4]);
+int t3_sge_read_cq(adapter_t *adapter, unsigned int id, u32 data[4]);
+int t3_sge_read_rspq(adapter_t *adapter, unsigned int id, u32 data[4]);
+int t3_sge_cqcntxt_op(adapter_t *adapter, unsigned int id, unsigned int op,
 		      unsigned int credits);
 
-int t3_vsc8211_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_ael1002_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_ael1006_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_ael2005_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_ael2020_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_qt2045_phy_prep(struct cphy *phy, struct adapter *adapter, int phy_addr,
+int t3_elmr_blk_write(adapter_t *adap, int start, const u32 *vals, int n);
+int t3_elmr_blk_read(adapter_t *adap, int start, u32 *vals, int n);
+int t3_vsc7323_init(adapter_t *adap, int nports);
+int t3_vsc7323_set_speed_fc(adapter_t *adap, int speed, int fc, int port);
+int t3_vsc7323_set_mtu(adapter_t *adap, unsigned int mtu, int port);
+int t3_vsc7323_set_addr(adapter_t *adap, u8 addr[6], int port);
+int t3_vsc7323_enable(adapter_t *adap, int port, int which);
+int t3_vsc7323_disable(adapter_t *adap, int port, int which);
+const struct mac_stats *t3_vsc7323_update_stats(struct cmac *mac);
+
+int t3_mi1_read(adapter_t *adapter, int phy_addr, int mmd_addr, int reg_addr,
+		unsigned int *valp);
+int t3_mi1_write(adapter_t *adapter, int phy_addr, int mmd_addr, int reg_addr,
+		 unsigned int val);
+
+int t3_mv88e1xxx_phy_prep(pinfo_t *pinfo, int phy_addr,
+			  const struct mdio_ops *mdio_ops);
+int t3_vsc8211_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops);
+int t3_vsc8211_fifo_depth(adapter_t *adap, unsigned int mtu, int port);
+int t3_ael1002_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops);
+int t3_ael1006_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops);
+int t3_ael2005_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops);
+int t3_ael2020_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops);
+int t3_qt2045_phy_prep(pinfo_t *pinfo, int phy_addr,
 		       const struct mdio_ops *mdio_ops);
-int t3_xaui_direct_phy_prep(struct cphy *phy, struct adapter *adapter,
-			    int phy_addr, const struct mdio_ops *mdio_ops);
-int t3_aq100x_phy_prep(struct cphy *phy, struct adapter *adapter,
-			    int phy_addr, const struct mdio_ops *mdio_ops);
-#endif				/* __CHELSIO_COMMON_H */
+int t3_tn1010_phy_prep(pinfo_t *pinfo, int phy_addr,
+		       const struct mdio_ops *mdio_ops);
+int t3_xaui_direct_phy_prep(pinfo_t *pinfo, int phy_addr,
+			    const struct mdio_ops *mdio_ops);
+int t3_aq100x_phy_prep(pinfo_t *pinfo, int phy_addr,
+		       const struct mdio_ops *mdio_ops);
+
+int is_demo_bt(adapter_t *adap);
+
+int t3_check_vpd_checksum(adapter_t *adapter);
+void t3_gate_rx_traffic(struct cmac *mac, u32 *rx_cfg,
+			u32 *rx_hash_high, u32 *rx_hash_low);
+void t3_open_rx_traffic(struct cmac *mac, u32 rx_cfg,
+			u32 rx_hash_high, u32 rx_hash_low);
+
+#endif /* __CHELSIO_COMMON_H */
diff --git a/drivers/net/cxgb3/cxgb3_compat.h b/drivers/net/cxgb3/cxgb3_compat.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/cxgb3_compat.h
@@ -0,0 +1,853 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+#ifndef __CXGB3_COMPAT_H
+#define __CXGB3_COMPAT_H
+
+#include <linux/version.h>
+#include "common.h"
+#include <linux/pci.h>
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,21)
+#include <asm/kdebug.h>
+
+#ifdef RHEL_RELEASE_CODE
+
+#if RHEL_RELEASE_CODE <= RHEL_RELEASE_VERSION(4, 8)
+#define unregister_die_notifier(a)
+#define register_die_notifier(a)
+#endif
+
+#else
+
+#define unregister_die_notifier(a)
+#define register_die_notifier(a)
+
+#endif
+
+#ifdef SLE_VERSION
+
+#if SLE_VERSION_CODE <= SLE_VERSION(10,3,0)
+#define unregister_die_notifier(a)
+#define register_die_notifier(a)
+#endif
+
+#endif
+
+#else
+#include <linux/kdebug.h>
+#endif
+
+/* XXX Verify OS version */
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,13) && \
+    LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,5)
+
+#if LINUX_VERSION_CODE == KERNEL_VERSION(2,6,5)
+
+struct msix_entry {
+	u16 	vector;	/* kernel uses to write allocated vector */
+	u16	entry;	/* driver uses to specify entry, OS writes */
+};
+
+static inline void pci_disable_msi(struct pci_dev *dev)
+{}
+
+static inline int pci_enable_msix(struct pci_dev* dev, struct msix_entry *entries,
+			          int nvec)
+{
+	return -1;
+}
+
+static inline void pci_disable_msix(struct pci_dev* dev)
+{}
+
+static inline struct mii_ioctl_data *if_mii(struct ifreq *rq)
+
+{
+	return (struct mii_ioctl_data *) &rq->ifr_ifru;
+}
+
+#define _spin_trylock spin_trylock
+
+#endif /* KERNEL_VERSION(2.6.5) */
+
+#ifndef ATOMIC_ADD_RETURN
+#if defined(CONFIG_X86_64)
+static __inline__ int atomic_add_return(int i, atomic_t *v)
+{
+	int __i = i;
+	__asm__ __volatile__(
+		LOCK "xaddl %0, %1;"
+		:"=r"(i)
+		:"m"(v->counter), "0"(i));
+	return i + __i;
+}
+
+#elif defined(CONFIG_X86)
+static __inline__ int atomic_add_return(int i, atomic_t *v)
+{
+	int __i;
+#ifdef CONFIG_M386
+	if(unlikely(boot_cpu_data.x86==3))
+		goto no_xadd;
+#endif
+	/* Modern 486+ processor */
+	__i = i;
+	__asm__ __volatile__(
+		LOCK "xaddl %0, %1;"
+		:"=r"(i)
+		:"m"(v->counter), "0"(i));
+	return i + __i;
+
+#ifdef CONFIG_M386
+no_xadd: /* Legacy 386 processor */
+	local_irq_disable();
+	__i = atomic_read(v);
+	atomic_set(v, i + __i);
+	local_irq_enable();
+	return i + __i;
+#endif
+}
+
+#elif defined(CONFIG_IA64)
+#define atomic_add_return(i,v)						\
+({									\
+	int __ia64_aar_i = (i);						\
+	(__builtin_constant_p(i)					\
+	 && (   (__ia64_aar_i ==  1) || (__ia64_aar_i ==   4)		\
+	     || (__ia64_aar_i ==  8) || (__ia64_aar_i ==  16)		\
+	     || (__ia64_aar_i == -1) || (__ia64_aar_i ==  -4)		\
+	     || (__ia64_aar_i == -8) || (__ia64_aar_i == -16)))		\
+		? ia64_fetch_and_add(__ia64_aar_i, &(v)->counter)	\
+		: ia64_atomic_add(__ia64_aar_i, v);			\
+})
+
+#elif defined(CONFIG_PPC64)
+static __inline__ int atomic_add_return(int a, atomic_t *v)
+{
+	int t;
+
+	__asm__ __volatile__(
+	EIEIO_ON_SMP
+"1:	lwarx	%0,0,%2		# atomic_add_return\n\
+	add	%0,%1,%0\n\
+	stwcx.	%0,0,%2\n\
+	bne-	1b"
+	ISYNC_ON_SMP
+	: "=&r" (t)
+	: "r" (a), "r" (&v->counter)
+	: "cc", "memory");
+
+	return t;
+}
+
+#elif defined(CONFIG_PPC)
+static __inline__ int atomic_add_return(int a, atomic_t *v)
+{
+	int t;
+
+	__asm__ __volatile__(
+"1:	lwarx	%0,0,%2		# atomic_add_return\n\
+	add	%0,%1,%0\n"
+	PPC405_ERR77(0,%2)
+"	stwcx.	%0,0,%2 \n\
+	bne-	1b"
+	SMP_ISYNC
+	: "=&r" (t)
+	: "r" (a), "r" (&v->counter)
+	: "cc", "memory");
+
+	return t;
+}
+#endif
+#endif /* ATOMIC_ADD_RETURN */
+
+#ifndef SPIN_TRYLOCK_IRQSAVE
+#define spin_trylock_irqsave(lock, flags) \
+({ \
+	local_irq_save(flags); \
+	_spin_trylock(lock) ? \
+	1 : ({ local_irq_restore(flags); 0; }); \
+})
+#endif
+
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,11)
+static inline int t3_os_pci_save_state(struct adapter *adapter)
+{
+	return pci_save_state(adapter->pdev, adapter->t3_config_space);
+}
+
+static inline int t3_os_pci_restore_state(struct adapter *adapter)
+{
+	return pci_restore_state(adapter->pdev, adapter->t3_config_space);
+}
+
+static
+inline void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
+					      struct work_struct *work)
+{
+	while (!cancel_delayed_work(work))
+		flush_workqueue(wq);
+}
+
+#else
+static inline int t3_os_pci_save_state(adapter_t *adapter)
+{
+	return pci_save_state(adapter->pdev);
+}
+
+static inline int t3_os_pci_restore_state(adapter_t *adapter)
+{
+	return pci_restore_state(adapter->pdev);
+}
+#endif
+
+static inline int __netif_rx_schedule_prep(struct net_device *dev)
+{
+	return !test_and_set_bit(__LINK_STATE_RX_SCHED, &dev->state);
+}
+
+#ifndef CONFIG_DEBUG_FS
+#include <linux/err.h>
+/* Adapted from debugfs.h */
+static inline struct dentry *debugfs_create_dir(const char *name,
+						struct dentry *parent)
+{
+	return ERR_PTR(-ENODEV);
+}
+
+static inline void debugfs_remove(struct dentry *dentry)
+{}
+#else
+#include <linux/debugfs.h>
+#endif
+
+static inline void setup_timer(struct timer_list * timer,
+				void (*function)(unsigned long),
+				unsigned long data)
+{
+	timer->function = function;
+	timer->data = data;
+	init_timer(timer);
+}
+
+#define DEFINE_MUTEX DECLARE_MUTEX
+#define mutex_lock down
+#define mutex_unlock up
+
+#undef DEFINE_RWLOCK /* broken RH4u3 definition, rw_lock_t does not exist */
+#define DEFINE_RWLOCK(x)	rwlock_t x = RW_LOCK_UNLOCKED
+
+#define gfp_t unsigned
+
+/* 2.6.14 and above */
+#elif LINUX_VERSION_CODE > KERNEL_VERSION(2,6,13)
+#include <linux/debugfs.h>
+
+static inline int t3_os_pci_save_state(adapter_t *adapter)
+{
+	return pci_save_state(adapter->pdev);
+}
+
+static inline int t3_os_pci_restore_state(adapter_t *adapter)
+{
+	return pci_restore_state(adapter->pdev);
+}
+
+#endif /* LINUX_VERSION_CODE */
+
+#if !defined(NETEVENT)
+struct notifier_block;
+
+static inline void register_netevent_notifier(struct notifier_block *nb)
+{}
+
+static inline void unregister_netevent_notifier(struct notifier_block *nb)
+{}
+
+#if defined(CONFIG_TCP_OFFLOAD_MODULE) && defined(CONFIG_X86)
+#define OFLD_USE_KPROBES
+#endif
+
+#else
+extern int netdev_nit;
+#endif
+
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,19)
+
+typedef irqreturn_t (*intr_handler_t)(int, void *, struct pt_regs *);
+#define DECLARE_INTR_HANDLER(handler, irq, cookie, regs) \
+	static irqreturn_t handler(int irq, void *cookie, struct pt_regs *regs)
+
+intr_handler_t t3_intr_handler(struct adapter *adap, int polling);
+static inline void t3_poll_handler(struct adapter *adapter,
+				   struct sge_qset *qs)
+{
+	t3_intr_handler(adapter, qs->rspq.flags & USING_POLLING) (0,
+		(adapter->flags & USING_MSIX) ? (void *)qs : (void *)adapter,
+		NULL);
+}
+
+#define CHECKSUM_PARTIAL CHECKSUM_HW
+#define CHECKSUM_COMPLETE CHECKSUM_HW
+
+#ifndef I_PRIVATE
+#define i_private u.generic_ip
+#endif
+
+#else /* 2.6.19 */
+typedef irqreturn_t (*intr_handler_t)(int, void *);
+#define DECLARE_INTR_HANDLER(handler, irq, cookie, regs) \
+	static irqreturn_t handler(int irq, void *cookie)
+
+intr_handler_t t3_intr_handler(struct adapter *adap, int polling);
+static inline void t3_poll_handler(struct adapter *adapter,
+		 		   struct sge_qset *qs)
+{
+	t3_intr_handler(adapter, qs->rspq.flags & USING_POLLING) (0,
+		(adapter->flags & USING_MSIX) ? (void *)qs : (void *)adapter);
+}
+
+#endif /* 2.6.19 */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+#define DECLARE_TASK_FUNC(task, task_param) \
+	static void task(void *task_param)
+
+#define WORK2ADAP(task_param, task) task_param
+#define DELWORK2ADAP(task_param, task) task_param
+#define WORK2T3CDATA(task_param, task) task_param
+
+#define delayed_work work_struct
+
+#define T3_INIT_WORK INIT_WORK
+#define T3_INIT_DELAYED_WORK INIT_WORK
+
+#else /* 2.6.20 */
+
+#define DECLARE_TASK_FUNC(task, task_param) \
+	static void task(struct work_struct *task_param)
+
+#define WORK2ADAP(task_param, task) \
+	container_of(task_param, struct adapter, task)
+
+#define DELWORK2ADAP(task_param, task) \
+	container_of(task_param, struct adapter, task.work)
+
+#define WORK2T3CDATA(task_param, task) \
+	container_of(task_param, struct t3c_data, task)
+
+#define T3_INIT_WORK(task_handler, task, adapter) \
+	INIT_WORK(task_handler, task)
+
+#define T3_INIT_DELAYED_WORK(task_handler, task, adapter) \
+	INIT_DELAYED_WORK(task_handler, task)
+
+#endif /* 2.6.20 */
+
+#if defined(CONFIG_FW_LOADER) || defined(CONFIG_FW_LOADER_MODULE)
+#include <linux/firmware.h>
+#else
+struct firmware {
+	size_t size;
+	u8 *data;
+};
+
+struct device;
+
+static inline int request_firmware(const struct firmware **firmware_p,
+				   char *name,
+				   struct device *device)
+{
+	printk(KERN_WARNING
+	       "FW_LOADER not set in this kernel. FW upgrade aborted.\n");
+	return -1;
+}
+
+static inline void release_firmware(const struct firmware *fw)
+{}
+#endif /* FW_LOADER */
+
+/* CONFIG_CRASH_DUMP is defined in -kdump kernel of XenServer */
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(RTNL_TRYLOCK)
+#include <linux/rtnetlink.h>
+static inline int rtnl_trylock(void)
+{
+	return !rtnl_shlock_nowait();
+}
+#endif /* RTNL_TRYLOCK */
+
+#if  LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+#ifndef KZALLOC
+static inline void *kzalloc(size_t size, int flags)
+{
+	void *ret = kmalloc(size, flags);
+	if (ret)
+		memset(ret, 0, size);
+	return ret;
+}
+#endif /* KZALLOC */
+#endif
+
+#ifndef GSO_SIZE
+#define gso_size tso_size
+#endif /* GSO_SIZE */
+#endif /* !CONFIG_XEN  && !CONFIG_CRASH_DUMP */
+
+#ifndef NIPQUAD_FMT
+#define NIPQUAD_FMT "%u.%u.%u.%u"
+#endif
+
+/* sysfs compatibility */
+#if  LINUX_VERSION_CODE < KERNEL_VERSION(2,6,21)
+
+#define to_net_dev(class) container_of(class, struct net_device, class_dev)
+
+#define cxgb3_compat_device class_device
+
+#define CXGB3_SHOW_FUNC(func, d, attr, buf)			\
+	static ssize_t func(struct cxgb3_compat_device *d,	\
+			    char *buf)				\
+
+#define CXGB3_STORE_FUNC(func, d, attr, buf, len)		\
+	static ssize_t func(struct cxgb3_compat_device *d,	\
+			    const char *buf,			\
+			    size_t len)
+
+#ifndef  __ATTR
+#define __ATTR(_name,_mode,_show,_store) { \
+	.attr = {.name = __stringify(_name), .mode = _mode, .owner = THIS_MODULE },	\
+	.show	= _show,					\
+	.store	= _store,					\
+}
+#endif
+
+#define CXGB3_DEVICE_ATTR(_name,_mode,_show,_store)		\
+struct class_device_attribute dev_attr_##_name = 		\
+	__ATTR(_name,_mode,_show,_store)
+
+#ifndef LINUX_2_4
+static inline struct kobject *net2kobj(struct net_device *dev)
+{
+	return &dev->class_dev.kobj;
+}
+#endif
+
+#else /* sysfs compatibility */
+
+#define cxgb3_compat_device device
+
+#define CXGB3_SHOW_FUNC(func, d, attr, buf)			\
+	static ssize_t func(struct cxgb3_compat_device *d,	\
+			    struct device_attribute *attr,	\
+			    char *buf)				\
+
+#define CXGB3_STORE_FUNC(func, d, attr, buf, len)		\
+	static ssize_t func(struct cxgb3_compat_device *d,	\
+			    struct device_attribute *attr,	\
+			    const char *buf,			\
+			    size_t len)
+
+#define CXGB3_DEVICE_ATTR DEVICE_ATTR
+
+static inline struct kobject *net2kobj(struct net_device *dev)
+{
+	return &dev->dev.kobj;
+}
+
+#endif /* sysfs compatibility */
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(IRQF)
+#define IRQF_SHARED SA_SHIRQ
+#endif /* IRQF */
+
+#if !defined(VLANGRP)
+#include <linux/if_vlan.h>
+static inline struct net_device *vlan_group_get_device(struct vlan_group *vg,
+						       int vlan_id)
+{
+	return vg->vlan_devices[vlan_id];
+}
+#endif /* VLANGRP */
+#endif /* !CONFIG_XEN  && !CONFIG_CRASH_DUMP */
+
+#if !defined(for_each_netdev)
+#define for_each_netdev(d) \
+	for (d = dev_base; d; d = d->next)
+#endif
+
+#include <linux/ip.h>
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(NEW_SKB_COPY)
+static inline void skb_copy_from_linear_data(const struct sk_buff *skb,
+					     void *to,
+					     const unsigned int len)
+{
+	memcpy(to, skb->data, len);
+}
+
+static inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,
+						    const int offset, void *to,
+						    const unsigned int len)
+{
+	memcpy(to, skb->data + offset, len);
+}
+
+static inline void skb_copy_to_linear_data(struct sk_buff *skb,
+					   const void *from,
+					   const unsigned int len)
+{
+	memcpy(skb->data, from, len);
+}
+
+static inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,
+						  const int offset,
+						  const void *from,
+						  const unsigned int len)
+{
+	memcpy(skb->data + offset, from, len);
+}
+
+#endif
+
+#if defined(NEW_SKB_OFFSET)
+static inline void cxgb3_set_skb_header(struct sk_buff *skb,
+					struct iphdr *ip_hdr,
+					int offset)
+{
+	skb_set_network_header(skb, offset);
+}
+
+#else /* NEW_SKB_OFFSET */
+static inline int skb_network_offset(struct sk_buff *skb)
+{
+	return skb->nh.raw - skb->data;
+}
+
+static inline unsigned char *skb_transport_header(const struct sk_buff *skb)
+{
+	return skb->h.raw;
+}
+
+#if !defined(T3_SKB_TRANSPORT_OFFSET)
+static inline int skb_transport_offset(const struct sk_buff *skb)
+{
+	return skb->h.raw - skb->data;
+}
+#endif
+
+#if !defined(T3_IP_HDR)
+static inline struct iphdr *ip_hdr(const struct sk_buff *skb)
+{
+	return skb->nh.iph;
+}
+#endif
+
+#if !defined(T3_TCP_HDR)
+static inline struct tcphdr *tcp_hdr(const struct sk_buff *skb)
+{
+	return skb->h.th;
+}
+#endif
+
+#if !defined(T3_RESET_MAC_HEADER)
+static inline void skb_reset_mac_header(struct sk_buff *skb)
+{
+	skb->mac.raw = skb->data;
+}
+#endif
+
+#if !defined(T3_MAC_HEADER)
+static inline unsigned char *skb_mac_header(struct sk_buff *skb)
+{
+	return skb->mac.raw;
+}
+#endif
+
+static inline void skb_reset_network_header(struct sk_buff *skb)
+{
+	skb->nh.raw = skb->data;
+}
+
+static inline void skb_reset_transport_header(struct sk_buff *skb)
+{
+	skb->h.raw = skb->data;
+}
+
+static inline void cxgb3_set_skb_header(struct sk_buff *skb,
+					struct iphdr *ip_hdr,
+					int offset)
+{
+	skb->nh.iph = ip_hdr;
+}
+
+#endif /* NEW_SKB_OFFSET */
+
+#if !defined(ARP_HDR)
+static inline struct arphdr *arp_hdr(const struct sk_buff *skb)
+{
+        return (struct arphdr *)skb->nh.arph;
+}
+#endif /* !ARP_HDR */
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#if defined(CONFIG_XEN) || defined(CONFIG_CRASH_DUMP)
+static inline void cxgb3_set_skb_header(struct sk_buff *skb,
+					struct iphdr *ip_hdr,
+					int offset)
+{
+	skb_set_network_header(skb, offset);
+}
+#endif /* CONFIG_XEN || CONFIG_CRASH_DUMP */
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,23)
+#if defined(ETHTOOL_GPERMADDR)
+#define CXGB3_ETHTOOL_GPERMADDR ETHTOOL_GPERMADDR
+#endif /* ETHTOOL_GPERMADDR */
+#endif /* KERNEL_VERSION(2, 6, 23) */
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(TRANSPORT_HEADER)
+#define transport_header h.raw
+#endif
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,24)
+#define SET_MODULE_OWNER(module)
+#define INET_PROC_DIR init_net.proc_net
+#else
+#define INET_PROC_DIR proc_net
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,30)
+#define SET_PROC_NODE_OWNER(_p, _owner) \
+	do { (_p)->owner = (_owner); } while (0)
+#else
+#define SET_PROC_NODE_OWNER(_p, _owner) \
+	do { } while (0)
+#endif
+
+#if defined(NAPI_UPDATE)
+#define SGE_GET_OFLD_QS(napi, dev) \
+	container_of(napi, struct sge_qset, napi)
+
+#define DECLARE_OFLD_POLL(napi, dev, budget) \
+	static int ofld_poll(struct napi_struct *napi, int budget)
+
+#define DECLARE_NAPI_RX_HANDLER(napi, dev, budget) \
+	static int napi_rx_handler(struct napi_struct *napi, int budget)
+
+#else
+#define SGE_GET_OFLD_QS(napi, dev) \
+	((struct port_info *)netdev_priv(dev))->qs
+
+#define DECLARE_OFLD_POLL(napi, dev, budget) \
+	static int ofld_poll(struct net_device *dev, int *budget)
+
+#define DECLARE_NAPI_RX_HANDLER(napi, dev, budget) \
+	static int napi_rx_handler(struct net_device *dev, int *budget)
+
+#endif /* NAPI_UPDATE */
+
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(VLAN_DEV_API)
+#include <linux/if_vlan.h>
+#if defined(VLAN_DEV_INFO)
+static inline struct vlan_dev_info *vlan_dev_info(const struct net_device *dev)
+{
+	return VLAN_DEV_INFO(dev);
+}
+#endif /* VLAN_DEV_INFO */
+
+static inline u16 vlan_dev_vlan_id(const struct net_device *dev)
+{
+	return vlan_dev_info(dev)->vlan_id;
+}
+
+static inline struct net_device *vlan_dev_real_dev(const struct net_device *dev)
+{
+	return vlan_dev_info(dev)->real_dev;
+}
+#else /* VLAN_DEV_API */
+#if defined(RHEL_RELEASE_CODE)
+#if RHEL_RELEASE_CODE == RHEL_RELEASE_VERSION(5,7)
+#include <linux/if_vlan.h>
+static inline u16 vlan_dev_vlan_id(const struct net_device *dev)
+{
+	return VLAN_DEV_INFO(dev)->vlan_id;
+}
+#endif
+#endif /* RHEL_RELEASE_CODE */
+#endif /* VLAN_DEV_API */
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#if defined(PDEV_MAPPING)
+static inline int t3_pci_dma_mapping_error(struct pci_dev *pdev,
+					   dma_addr_t dma_addr)
+{
+	return pci_dma_mapping_error(pdev, dma_addr);
+}
+#else
+static inline int t3_pci_dma_mapping_error(struct pci_dev *pdev,
+					   dma_addr_t dma_addr)
+{
+	return pci_dma_mapping_error(dma_addr);
+}
+#endif
+
+#ifndef DMA_BIT_MASK
+#define DMA_BIT_MASK(n)	(((n) == 64) ? ~0ULL : ((1ULL<<(n))-1))
+#endif
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(MQ_TX)
+struct netdev_queue {};
+
+static inline void t3_compat_set_num_tx_queues(struct net_device *dev,
+					       int n)
+{}
+
+static inline struct netdev_queue * netdev_get_tx_queue(struct net_device *dev,
+				 			int qidx)
+{
+	return NULL;
+}
+
+#define netif_tx_start_all_queues netif_start_queue
+#define netif_tx_stop_all_queues netif_stop_queue
+
+static inline void t3_netif_tx_stop_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_stop_queue(dev);
+}
+
+static inline void t3_netif_tx_wake_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_wake_queue(dev);
+}
+
+static inline int t3_netif_tx_queue_stopped(struct net_device *dev,
+					    struct netdev_queue *txq)
+{
+	return netif_queue_stopped(dev);
+}
+
+#ifndef ALLOC_ETHERDEV_MQ_DEF
+#include <linux/etherdevice.h>
+static inline struct net_device * alloc_etherdev_mq(int sizeof_priv,
+						    int n_txq)
+{
+	return alloc_etherdev(sizeof_priv);
+}
+#else
+#if defined(RHEL_RELEASE_CODE)
+/* RHEL 5.6 (and above) expects number of queues to be 1 (hardcoded) */
+#if RHEL_RELEASE_CODE >= RHEL_RELEASE_VERSION(5, 6)
+#define alloc_etherdev_mq(sizeof_priv, n_txq)	alloc_etherdev(sizeof_priv)
+#endif /* RHEL_RELEASE_VERSION(5, 6) */
+#endif /* RHEL_RELEASE_CODE */
+#endif /* ALLOC_ETHERDEV_MQ_DEF */
+	
+#else /* MQ_TX - The stack supports TX multiqueues */
+static inline void t3_compat_set_num_tx_queues(struct net_device *dev,
+					       int n)
+{
+	dev->real_num_tx_queues = n;
+}
+
+static inline void t3_netif_tx_stop_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_tx_stop_queue(txq);
+}
+
+static inline void t3_netif_tx_wake_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_tx_wake_queue(txq);
+}
+
+static inline int t3_netif_tx_queue_stopped(struct net_device *dev,
+					    struct netdev_queue *txq)
+{
+	return netif_tx_queue_stopped(txq);
+}
+
+#endif
+
+#else /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+/* Dummy function to avoid many #if in the main code. */
+static inline void t3_compat_set_num_tx_queues(struct net_device *dev,
+					       int n)
+{}
+static inline void t3_netif_tx_stop_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_tx_stop_queue(txq);
+}
+static inline int t3_netif_tx_queue_stopped(struct net_device *dev,
+					    struct netdev_queue *txq)
+{
+	return netif_tx_queue_stopped(txq);
+}
+static inline void t3_netif_tx_wake_queue(struct net_device *dev,
+					  struct netdev_queue *txq)
+{
+	netif_tx_wake_queue(txq);
+}
+#define netif_tx_start_all_queues netif_start_queue
+#define netif_tx_stop_all_queues netif_stop_queue
+
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(SKB_RECORD_RX_QUEUE)
+static inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)
+{}
+#endif
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#if !defined(CXGB3_NIPQUAD)
+#define NIPQUAD(addr) \
+	((unsigned char *)&addr)[0], \
+	((unsigned char *)&addr)[1], \
+	((unsigned char *)&addr)[2], \
+	((unsigned char *)&addr)[3]
+#define NIPQUAD_FMT "%u.%u.%u.%u"
+#endif
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#if !defined(USECS_TO_JIFFIES)
+static inline unsigned long usecs_to_jiffies(const unsigned int u)
+{
+	if (u > jiffies_to_usecs(MAX_JIFFY_OFFSET))
+		return MAX_JIFFY_OFFSET;
+#if HZ <= USEC_PER_SEC && !(USEC_PER_SEC % HZ)
+	return (u + (USEC_PER_SEC / HZ) - 1) / (USEC_PER_SEC / HZ);
+#elif HZ > USEC_PER_SEC && !(HZ % USEC_PER_SEC)
+	return u * (HZ / USEC_PER_SEC);
+#else
+	return (USEC_TO_HZ_MUL32 * u + USEC_TO_HZ_ADJ32)
+		>> USEC_TO_HZ_SHR32;
+#endif
+}
+#endif /* !USECS_TO_JIFFIES */
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
+#endif
diff --git a/drivers/net/cxgb3/cxgb3_ctl_defs.h b/drivers/net/cxgb3/cxgb3_ctl_defs.h
--- a/drivers/net/cxgb3/cxgb3_ctl_defs.h
+++ b/drivers/net/cxgb3/cxgb3_ctl_defs.h
@@ -1,39 +1,19 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef _CXGB3_OFFLOAD_CTL_DEFS_H
 #define _CXGB3_OFFLOAD_CTL_DEFS_H
 
+#include <linux/compiler.h>
+
 enum {
-	GET_MAX_OUTSTANDING_WR 	= 0,
+	GET_MAX_OUTSTANDING_WR	= 0,
 	GET_TX_MAX_CHUNK	= 1,
 	GET_TID_RANGE		= 2,
 	GET_STID_RANGE		= 3,
@@ -56,26 +36,41 @@
 	RDMA_GET_MEM		= 18,
 	RDMA_GET_MIB		= 19,
 
+	FAILOVER		= 30,
+	FAILOVER_DONE		= 31,
+	FAILOVER_CLEAR		= 32,
+	FAILOVER_ACTIVE_SLAVE	= 33,
+	FAILOVER_PORT_DOWN	= 34,
+	FAILOVER_PORT_UP	= 35,
+	FAILOVER_PORT_RELEASE	= 36,
+
+	GET_CPUIDX_OF_QSET	= 40,
+	GET_PORT_SCHED          = 41,
+	GET_PORT_ARRAY          = 42,
+	GET_NUM_QUEUES          = 43,
+
 	GET_RX_PAGE_INFO	= 50,
+
 	GET_ISCSI_IPV4ADDR	= 51,
+	SET_ISCSI_IPV4ADDR	= 52,
 
-	GET_EMBEDDED_INFO	= 70,
+	GET_EMBEDDED_INFO       = 70,
 };
 
 /*
  * Structure used to describe a TID range.  Valid TIDs are [base, base+num).
  */
 struct tid_range {
-	unsigned int base;	/* first TID */
-	unsigned int num;	/* number of TIDs in range */
+	unsigned int base;   /* first TID */
+	unsigned int num;    /* number of TIDs in range */
 };
 
 /*
  * Structure used to request the size and contents of the MTU table.
  */
 struct mtutab {
-	unsigned int size;	/* # of entries in the MTU table */
-	const unsigned short *mtus;	/* the MTU table values */
+	unsigned int size;          /* # of entries in the MTU table */
+	const unsigned short *mtus; /* the MTU table values */
 };
 
 struct net_device;
@@ -84,15 +79,21 @@
  * Structure used to request the adapter net_device owning a given MAC address.
  */
 struct iff_mac {
-	struct net_device *dev;	/* the net_device */
-	const unsigned char *mac_addr;	/* MAC address to lookup */
+	struct net_device *dev;          /* the net_device */
+	const unsigned char *mac_addr;   /* MAC address to lookup */
 	u16 vlan_tag;
 };
 
+/* Structure used to request a port's offload scheduler */
+struct port_sched {
+	struct net_device *dev;          /* the net_device */
+	int sched;                       /* associated scheduler */
+};
+
 /* Structure used to request a port's iSCSI IPv4 address */
 struct iscsi_ipv4addr {
-	struct net_device *dev;	/* the net_device */
-	__be32 ipv4addr;	/* the return iSCSI IPv4 address */
+	struct net_device *dev;		/* the net_device */
+	__be32 ipv4addr;		/* the returned iSCSI IPv4 address */
 };
 
 struct pci_dev;
@@ -101,45 +102,64 @@
  * Structure used to request the TCP DDP parameters.
  */
 struct ddp_params {
-	unsigned int llimit;	/* TDDP region start address */
-	unsigned int ulimit;	/* TDDP region end address */
-	unsigned int tag_mask;	/* TDDP tag mask */
+	unsigned int llimit;     /* TDDP region start address */
+	unsigned int ulimit;     /* TDDP region end address */
+	unsigned int tag_mask;   /* TDDP tag mask */
 	struct pci_dev *pdev;
 };
 
 struct adap_ports {
-	unsigned int nports;	/* number of ports on this adapter */
-	struct net_device *lldevs[2];
+	unsigned int nports;	      /* number of ports on this adapter */
+	struct net_device *lldevs[4]; /* Max number of ports is 4 */
+};
+
+struct port_array {
+	unsigned int nports;          /* number of ports on this adapter */
+	struct net_device **lldevs;   /* points to array of net_devices */
+};
+
+struct bond_ports {
+	unsigned int port;	
+	unsigned int nports;		/* number of ports on this adapter */
+	unsigned int ports[4];		/* Max number of ports is 4 */
 };
 
 /*
  * Structure used to return information to the iscsi layer.
  */
 struct ulp_iscsi_info {
-	unsigned int offset;
-	unsigned int llimit;
-	unsigned int ulimit;
-	unsigned int tagmask;
-	u8 pgsz_factor[4];
-	unsigned int max_rxsz;
-	unsigned int max_txsz;
-	struct pci_dev *pdev;
+	unsigned int	offset;
+	unsigned int	llimit;
+	unsigned int	ulimit;
+	unsigned int	tagmask;
+	u8		pgsz_factor[4];
+	unsigned int	max_rxsz;
+	unsigned int	max_txsz;
+	struct pci_dev	*pdev;
+};
+
+/*
+ * Offload TX/RX page information.
+ */
+struct ofld_page_info {
+	unsigned int page_size;  /* Page size, should be a power of 2 */
+	unsigned int num;        /* Number of pages */
 };
 
 /*
  * Structure used to return information to the RDMA layer.
  */
 struct rdma_info {
-	unsigned int tpt_base;	/* TPT base address */
-	unsigned int tpt_top;	/* TPT last entry address */
-	unsigned int pbl_base;	/* PBL base address */
-	unsigned int pbl_top;	/* PBL last entry address */
-	unsigned int rqt_base;	/* RQT base address */
-	unsigned int rqt_top;	/* RQT last entry address */
-	unsigned int udbell_len;	/* user doorbell region length */
-	unsigned long udbell_physbase;	/* user doorbell physical start addr */
-	void __iomem *kdb_addr;	/* kernel doorbell register address */
-	struct pci_dev *pdev;	/* associated PCI device */
+	unsigned int tpt_base;   /* TPT base address */
+	unsigned int tpt_top;	 /* TPT last entry address */
+	unsigned int pbl_base;   /* PBL base address */
+	unsigned int pbl_top;	 /* PBL last entry address */
+	unsigned int rqt_base;   /* RQT base address */
+	unsigned int rqt_top;	 /* RQT last entry address */
+	unsigned int udbell_len; /* user doorbell region length */
+	unsigned long udbell_physbase;  /* user doorbell physical start addr */
+	void __iomem *kdb_addr;  /* kernel doorbell register address */
+	struct pci_dev *pdev;    /* associated PCI device */
 };
 
 /*
@@ -172,18 +192,10 @@
 };
 
 /*
- * Offload TX/RX page information.
- */
-struct ofld_page_info {
-	unsigned int page_size;  /* Page size, should be a power of 2 */
-	unsigned int num;        /* Number of pages */
-};
-
-/*
  * Structure used to get firmware and protocol engine versions.
  */
 struct ch_embedded_info {
 	u32 fw_vers;
 	u32 tp_vers;
 };
-#endif				/* _CXGB3_OFFLOAD_CTL_DEFS_H */
+#endif /* _CXGB3_OFFLOAD_CTL_DEFS_H */
diff --git a/drivers/net/cxgb3/cxgb3_defs.h b/drivers/net/cxgb3/cxgb3_defs.h
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/cxgb3_defs.h
+++ b/drivers/net/cxgb3/cxgb3_defs.h
@@ -1,33 +1,11 @@
 /*
- * Copyright (c) 2006-2008 Chelsio, Inc. All rights reserved.
+ * Copyright (c) 2005-2009 Chelsio, Inc. All rights reserved.
+ * Copyright (c) 2005-2009 Open Grid Computing, Inc. All rights reserved.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
 #ifndef _CHELSIO_DEFS_H
 #define _CHELSIO_DEFS_H
@@ -45,6 +23,13 @@
 void cxgb_free_mem(void *addr);
 void cxgb_neigh_update(struct neighbour *neigh);
 void cxgb_redirect(struct dst_entry *old, struct dst_entry *new);
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#ifndef LINUX_2_4
+int req_set_offload_policy(struct net_device *,
+			   const struct ofld_policy_file *,
+			   size_t);
+#endif
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
 
 /*
  * Map an ATID or STID to their entries in the corresponding TID tables.
@@ -55,6 +40,7 @@
 	return &t->atid_tab[atid - t->atid_base];
 }
 
+
 static inline union listen_entry *stid2entry(const struct tid_info *t,
 					     unsigned int stid)
 {
@@ -68,7 +54,7 @@
 					       unsigned int tid)
 {
 	struct t3c_tid_entry *t3c_tid = tid < t->ntids ?
-	    &(t->tid_tab[tid]) : NULL;
+					&(t->tid_tab[tid]) : NULL;
 
 	return (t3c_tid && t3c_tid->client) ? t3c_tid : NULL;
 }
diff --git a/drivers/net/cxgb3/cxgb3_ioctl.h b/drivers/net/cxgb3/cxgb3_ioctl.h
--- a/drivers/net/cxgb3/cxgb3_ioctl.h
+++ b/drivers/net/cxgb3/cxgb3_ioctl.h
@@ -1,54 +1,70 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef __CHIOCTL_H__
 #define __CHIOCTL_H__
 
+#ifndef AUTOCONF_INCLUDED
+#include <linux/autoconf.h>
+#endif
+
 /*
  * Ioctl commands specific to this driver.
  */
 enum {
+	CHELSIO_SETREG			= 1024,
+	CHELSIO_GETREG 			= 1025,
+	CHELSIO_SETTPI 			= 1026,
+	CHELSIO_GETTPI 			= 1027,
+	CHELSIO_DEVUP 			= 1028,
 	CHELSIO_GETMTUTAB 		= 1029,
 	CHELSIO_SETMTUTAB 		= 1030,
+	CHELSIO_GETMTU 			= 1031,
 	CHELSIO_SET_PM 			= 1032,
 	CHELSIO_GET_PM			= 1033,
+	CHELSIO_GET_TCAM		= 1034,
+	CHELSIO_SET_TCAM		= 1035,
+	CHELSIO_GET_TCB			= 1036,
+	CHELSIO_READ_TCAM_WORD		= 1037,
 	CHELSIO_GET_MEM			= 1038,
+	CHELSIO_GET_SGE_CONTEXT		= 1039,
+	CHELSIO_GET_SGE_DESC		= 1040,
 	CHELSIO_LOAD_FW			= 1041,
+	CHELSIO_GET_PROTO		= 1042,
+	CHELSIO_SET_PROTO		= 1043,
 	CHELSIO_SET_TRACE_FILTER	= 1044,
 	CHELSIO_SET_QSET_PARAMS		= 1045,
 	CHELSIO_GET_QSET_PARAMS		= 1046,
 	CHELSIO_SET_QSET_NUM		= 1047,
 	CHELSIO_GET_QSET_NUM		= 1048,
+	CHELSIO_SET_PKTSCHED		= 1049,
+	CHELSIO_SET_HW_SCHED		= 1051,
+	CHELSIO_LOAD_BOOT		= 1054,
+	CHELSIO_CLEAR_STATS             = 1055,
+	CHELSIO_GET_UP_LA		= 1056,
+	CHELSIO_GET_UP_IOQS		= 1057,
+	CHELSIO_GET_TRACE_FILTER	= 1058,
+
+	CHELSIO_SET_FILTER		= 1060,
+	CHELSIO_DEL_FILTER		= 1061,
+	CHELSIO_SET_OFLD_POLICY		= 1062,
+	CHELSIO_GET_PKTSCHED            = 1065,
 };
 
+/* statistics categories */
+enum {
+	STATS_PORT  = 1 << 1,
+	STATS_QUEUE = 1 << 2,
+};
+ 
 struct ch_reg {
 	uint32_t cmd;
 	uint32_t addr;
@@ -70,7 +86,7 @@
 	uint32_t queue_num;
 	uint32_t idx;
 	uint32_t size;
-	uint8_t data[128];
+	uint8_t  data[128];
 };
 
 struct ch_mem_range {
@@ -79,30 +95,71 @@
 	uint32_t addr;
 	uint32_t len;
 	uint32_t version;
-	uint8_t buf[0];
+	uint8_t  buf[0];
 };
 
+enum { MEM_CM, MEM_PMRX, MEM_PMTX };   /* ch_mem_range.mem_id values */
+
 struct ch_qset_params {
 	uint32_t cmd;
 	uint32_t qset_idx;
-	int32_t txq_size[3];
-	int32_t rspq_size;
-	int32_t fl_size[2];
-	int32_t intr_lat;
-	int32_t polling;
-	int32_t lro;
-	int32_t cong_thres;
+	int32_t  txq_size[3];
+	int32_t  rspq_size;
+	int32_t  fl_size[2];
+	int32_t  intr_lat;
+	int32_t  polling;
+	int32_t  lro;
+	int32_t  cong_thres;
 	int32_t  vector;
 	int32_t  qnum;
 };
 
 struct ch_pktsched_params {
 	uint32_t cmd;
-	uint8_t sched;
-	uint8_t idx;
-	uint8_t min;
-	uint8_t max;
-	uint8_t binding;
+	uint8_t  sched;
+	uint8_t  idx;
+	uint8_t  min;
+	uint8_t  max;
+	uint8_t  binding;
+};
+
+enum {
+	PKTSCHED_PORT = 0,
+	PKTSCHED_TUNNELQ =1,
+};
+
+struct ch_hw_sched {
+	uint32_t cmd;
+	uint8_t  sched;
+	int8_t   mode;
+	int8_t   channel;
+	int32_t  kbps;        /* rate in Kbps */
+	int32_t  class_ipg;   /* tenths of nanoseconds */
+	int32_t  flow_ipg;    /* usec */
+};
+
+struct ch_filter_tuple {
+	uint32_t sip;
+	uint32_t dip;
+	uint16_t sport;
+	uint16_t dport;
+	uint16_t vlan:12;
+	uint16_t vlan_prio:3;
+};
+
+struct ch_filter {
+	uint32_t cmd;
+	uint32_t filter_id;
+	struct ch_filter_tuple val;
+	struct ch_filter_tuple mask;
+	uint16_t mac_addr_idx;
+	uint8_t mac_hit:1;
+	uint8_t proto:2;
+
+	uint8_t want_filter_id:1; /* report filter TID instead of RSS hash */
+	uint8_t pass:1;           /* whether to pass or drop packets */
+	uint8_t rss:1;            /* use RSS or specified qset */
+	uint8_t qset;
 };
 
 #ifndef TCB_SIZE
@@ -112,8 +169,6 @@
 /* TCB size in 32-bit words */
 #define TCB_WORDS (TCB_SIZE / 4)
 
-enum { MEM_CM, MEM_PMRX, MEM_PMTX };	/* ch_mem_range.mem_id values */
-
 struct ch_mtus {
 	uint32_t cmd;
 	uint32_t nmtus;
@@ -163,13 +218,31 @@
 	uint32_t vlan_mask:12;
 	uint32_t intf:4;
 	uint32_t intf_mask:4;
-	uint8_t proto;
-	uint8_t proto_mask;
-	uint8_t invert_match:1;
-	uint8_t config_tx:1;
-	uint8_t config_rx:1;
-	uint8_t trace_tx:1;
-	uint8_t trace_rx:1;
+	uint8_t  proto;
+	uint8_t  proto_mask;
+	uint8_t  invert_match:1;
+	uint8_t  config_tx:1;
+	uint8_t  config_rx:1;
+	uint8_t  trace_tx:1;
+	uint8_t  trace_rx:1;
+};
+
+struct ch_up_la {
+	uint32_t cmd;
+	uint32_t stopped;
+	uint32_t idx;
+	uint32_t bufsize;
+	u8 *data;
+};
+
+struct ch_up_ioqs {
+	uint32_t cmd;
+	uint32_t ioq_rx_enable;
+	uint32_t ioq_tx_enable;
+	uint32_t ioq_rx_status;
+	uint32_t ioq_tx_status;
+	uint32_t bufsize;
+	u8 *data;
 };
 
 #define SIOCCHIOCTL SIOCDEVPRIVATE
diff --git a/drivers/net/cxgb3/cxgb3_main.c b/drivers/net/cxgb3/cxgb3_main.c
--- a/drivers/net/cxgb3/cxgb3_main.c
+++ b/drivers/net/cxgb3/cxgb3_main.c
@@ -1,50 +1,38 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include <linux/module.h>
+#ifndef	LINUX_2_4
 #include <linux/moduleparam.h>
+#endif	/* LINUX_2_4 */
 #include <linux/init.h>
 #include <linux/pci.h>
+#ifndef	LINUX_2_4
 #include <linux/dma-mapping.h>
+#endif	/* LINUX_2_4 */
+#include <linux/inet.h>
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
 #include <linux/if_vlan.h>
-#include <linux/mdio.h>
+#include <linux/mii.h>
 #include <linux/sockios.h>
 #include <linux/workqueue.h>
 #include <linux/proc_fs.h>
+#include <linux/seq_file.h>
 #include <linux/rtnetlink.h>
-#include <linux/firmware.h>
-#include <linux/log2.h>
+#include <linux/highmem.h>
+#include <linux/vmalloc.h>
+#include <linux/sched.h>
 #include <asm/uaccess.h>
+#include <linux/notifier.h>
 
 #include "common.h"
 #include "cxgb3_ioctl.h"
@@ -52,22 +40,54 @@
 #include "cxgb3_offload.h"
 #include "version.h"
 
+#include "cxgb3_defs.h"
 #include "cxgb3_ctl_defs.h"
 #include "t3_cpl.h"
+#include "t3_firmware.h"
 #include "firmware_exports.h"
 
 enum {
-	MAX_TXQ_ENTRIES = 16384,
+	MAX_TXQ_ENTRIES      = 16384,
 	MAX_CTRL_TXQ_ENTRIES = 1024,
-	MAX_RSPQ_ENTRIES = 16384,
-	MAX_RX_BUFFERS = 16384,
+	MAX_RSPQ_ENTRIES     = 16384,
+	MAX_RX_BUFFERS       = 16384,
 	MAX_RX_JUMBO_BUFFERS = 16384,
-	MIN_TXQ_ENTRIES = 4,
+	MIN_TXQ_ENTRIES      = 4,
 	MIN_CTRL_TXQ_ENTRIES = 4,
-	MIN_RSPQ_ENTRIES = 32,
-	MIN_FL_ENTRIES = 32
+	MIN_RSPQ_ENTRIES     = 32,
+	MIN_FL_ENTRIES       = 32,
+	MIN_FL_JUMBO_ENTRIES = 32
 };
 
+/*
+ * Local host copy of filter information.  This is used to program the
+ * hardware filters.  In general, non-zero fields indicate that the associated
+ * packet element should be compared for a match with the value.
+ */
+struct filter_info {
+	u32 sip;		/* Source IP address */
+	u32 sip_mask;		/* Source IP mask */
+	u32 dip;		/* Destination IP address */
+	u16 sport;		/* Source port */
+	u16 dport;		/* Desination port */
+	u32 vlan:12;		/* VLAN ID */
+	u32 vlan_prio:3;	/* VLAN Priority: FILTER_NO_VLAN_PRI => none */
+	u32 mac_hit:1;		/* Match MAC address at MAC Index */
+	u32 mac_idx:4;		/* Index of Exact MAC Address entry */
+				/*   (Port ID << 3) | MAC Index */
+	u32 mac_vld:1;		/* Port ID and MAC Index are valid */
+	u32 pkt_type:2;		/* Packet type: */
+				/*   {0..3} => {Any, TCP, UDP, IP Fragment} */
+	u32 report_filter_id:1;	/* Report filter ID in CPL Response Message */
+	u32 pass:1;		/* Pass packet: 0 => drop, 1 => pass */
+	u32 rss:1;		/* Use RSS: 0 => use Qset, 1 => RSS */
+	u32 qset:3;		/* Qset to which packet should be appended */
+	u32 locked:1;		/* filter used by software; unavailable to user */
+	u32 valid:1;		/* filter is valid */
+};
+
+enum { FILTER_NO_VLAN_PRI = 7 };
+
 #define PORT_MASK ((1 << MAX_NPORTS) - 1)
 
 #define DFLT_MSG_ENABLE (NETIF_MSG_DRV | NETIF_MSG_PROBE | NETIF_MSG_LINK | \
@@ -77,29 +97,36 @@
 #define EEPROM_MAGIC 0x38E2F10C
 
 #define CH_DEVICE(devid, idx) \
-	{ PCI_VENDOR_ID_CHELSIO, devid, PCI_ANY_ID, PCI_ANY_ID, 0, 0, idx }
-
-static const struct pci_device_id cxgb3_pci_tbl[] = {
-	CH_DEVICE(0x20, 0),	/* PE9000 */
-	CH_DEVICE(0x21, 1),	/* T302E */
-	CH_DEVICE(0x22, 2),	/* T310E */
-	CH_DEVICE(0x23, 3),	/* T320X */
-	CH_DEVICE(0x24, 1),	/* T302X */
-	CH_DEVICE(0x25, 3),	/* T320E */
-	CH_DEVICE(0x26, 2),	/* T310X */
-	CH_DEVICE(0x30, 2),	/* T3B10 */
-	CH_DEVICE(0x31, 3),	/* T3B20 */
-	CH_DEVICE(0x32, 1),	/* T3B02 */
-	CH_DEVICE(0x35, 6),	/* T3C20-derived T3C10 */
-	CH_DEVICE(0x36, 3),	/* S320E-CR */
-	CH_DEVICE(0x37, 7),	/* N320E-G2 */
-	{0,}
+	{ \
+		.vendor = PCI_VENDOR_ID_CHELSIO, \
+		.device = (devid), \
+		.subvendor = PCI_ANY_ID, \
+		.subdevice = PCI_ANY_ID, \
+		.driver_data = (idx) \
+	}
+
+static struct pci_device_id cxgb3_pci_tbl[] = {
+	CH_DEVICE(0x20, 0),  /* PE9000 */
+	CH_DEVICE(0x21, 1),  /* T302E */
+	CH_DEVICE(0x22, 2),  /* T310E */
+	CH_DEVICE(0x23, 3),  /* T320X */
+	CH_DEVICE(0x24, 1),  /* T302X */
+	CH_DEVICE(0x25, 3),  /* T320E */
+	CH_DEVICE(0x26, 2),  /* T310X */
+	CH_DEVICE(0x30, 2),  /* T3B10 */
+	CH_DEVICE(0x31, 3),  /* T3B20 */
+	CH_DEVICE(0x32, 1),  /* T3B02 */
+	CH_DEVICE(0x33, 4),  /* T3B04 */
+	CH_DEVICE(0x35, 6),  /* T3C20-derived T3C10 */
+	CH_DEVICE(0x36, 3),  /* S320E-CR */
+	CH_DEVICE(0x37, 7),  /* N320E-G2 */
+	{ 0, }
 };
 
-MODULE_DESCRIPTION(DRV_DESC);
+MODULE_DESCRIPTION(DRIVER_DESC);
 MODULE_AUTHOR("Chelsio Communications");
-MODULE_LICENSE("Dual BSD/GPL");
-MODULE_VERSION(DRV_VERSION);
+MODULE_LICENSE("GPL");
+MODULE_VERSION(DRIVER_VERSION);
 MODULE_DEVICE_TABLE(pci, cxgb3_pci_tbl);
 
 static int dflt_msg_enable = DFLT_MSG_ENABLE;
@@ -107,6 +134,22 @@
 module_param(dflt_msg_enable, int, 0644);
 MODULE_PARM_DESC(dflt_msg_enable, "Chelsio T3 default message enable bitmap");
 
+static int drv_wd_en = 0;
+
+module_param(drv_wd_en, int, 0644);
+MODULE_PARM_DESC(drv_wd_en, "Enable driver watchdog");
+
+static int drv_wd_ac = 1;
+
+module_param(drv_wd_ac, int, 0644);
+MODULE_PARM_DESC(drv_wd_ac, "Action to take if driver watchdog fires. Bring "
+		"down PHY's(1), PCIE linkdown(2) or FW exception(3)");
+
+static int fw_wd_en = 0;
+
+module_param(fw_wd_en, int, 0644);
+MODULE_PARM_DESC(fw_wd_en, "Enable firmware watchdog");
+
 /*
  * The driver uses the best interrupt scheme available on a platform in the
  * order MSI-X, MSI, legacy pin interrupts.  This parameter determines which
@@ -119,7 +162,7 @@
 static int msi = 2;
 
 module_param(msi, int, 0644);
-MODULE_PARM_DESC(msi, "whether to use MSI or MSI-X");
+MODULE_PARM_DESC(msi, "whether to use MSI-X (2), MSI (1) or Legacy INTx (0)");
 
 /*
  * The driver enables offload as a default.
@@ -132,6 +175,16 @@
 MODULE_PARM_DESC(ofld_disable, "whether to enable offload at init time or not");
 
 /*
+ * The driver uses an auto-queue algorithm by default.
+ * To disable it and force a single queue-set per port, use singleq = 1.
+ */
+
+static int singleq = 0;
+
+module_param(singleq, int, 0644);
+MODULE_PARM_DESC(singleq, "use a single queue-set per port");
+
+/*
  * We have work elements that we need to cancel when an interface is taken
  * down.  Normally the work elements would be executed by keventd but that
  * can deadlock because of linkwatch.  If our close method takes the rtnl
@@ -139,11 +192,34 @@
  * will block keventd as it needs the rtnl lock, and we'll deadlock waiting
  * for our work to complete.  Get our own work queue to solve this.
  */
-static struct workqueue_struct *cxgb3_wq;
+struct workqueue_struct *cxgb3_wq;
+
+#ifndef	LINUX_2_4
+static struct dentry *cxgb3_debugfs_root;
+#endif	/* LINUX_2_4 */
+
+static void cxgb_set_rxmode(struct net_device *dev);
+
+DEFINE_RWLOCK(adapter_list_lock);
+LIST_HEAD(adapter_list);
+
+static inline void add_adapter(adapter_t *adap)
+{
+	write_lock_bh(&adapter_list_lock);
+	list_add_tail(&adap->adapter_list, &adapter_list);
+	write_unlock_bh(&adapter_list_lock);
+}
+
+static inline void remove_adapter(adapter_t *adap)
+{
+	write_lock_bh(&adapter_list_lock);
+	list_del(&adap->adapter_list);
+	write_unlock_bh(&adapter_list_lock);
+}
 
 /**
  *	link_report - show link status and link speed/duplex
- *	@p: the port whose settings are to be reported
+ *	@dev: the port whose settings are to be reported
  *
  *	Shows the link status, speed, and duplex of a port.
  */
@@ -152,6 +228,8 @@
 	if (!netif_carrier_ok(dev))
 		printk(KERN_INFO "%s: link down\n", dev->name);
 	else {
+		static const char *fc[] = { "no", "Rx", "Tx", "Tx/Rx" };
+
 		const char *s = "10Mbps";
 		const struct port_info *p = netdev_priv(dev);
 
@@ -167,71 +245,17 @@
 			break;
 		}
 
-		printk(KERN_INFO "%s: link up, %s, %s-duplex\n", dev->name, s,
-		       p->link_config.duplex == DUPLEX_FULL ? "full" : "half");
+		printk(KERN_INFO "%s: link up, %s, %s-duplex, %s PAUSE\n",
+		       dev->name, s,
+		       p->link_config.duplex == DUPLEX_FULL ? "full" : "half",
+		       fc[p->link_config.fc]);
 	}
 }
 
-static void enable_tx_fifo_drain(struct adapter *adapter,
-				 struct port_info *pi)
-{
-	t3_set_reg_field(adapter, A_XGM_TXFIFO_CFG + pi->mac.offset, 0,
-			 F_ENDROPPKT);
-	t3_write_reg(adapter, A_XGM_RX_CTRL + pi->mac.offset, 0);
-	t3_write_reg(adapter, A_XGM_TX_CTRL + pi->mac.offset, F_TXEN);
-	t3_write_reg(adapter, A_XGM_RX_CTRL + pi->mac.offset, F_RXEN);
-}
-
-static void disable_tx_fifo_drain(struct adapter *adapter,
-				  struct port_info *pi)
-{
-	t3_set_reg_field(adapter, A_XGM_TXFIFO_CFG + pi->mac.offset,
-			 F_ENDROPPKT, 0);
-}
-
-void t3_os_link_fault(struct adapter *adap, int port_id, int state)
-{
-	struct net_device *dev = adap->port[port_id];
-	struct port_info *pi = netdev_priv(dev);
-
-	if (state == netif_carrier_ok(dev))
-		return;
-
-	if (state) {
-		struct cmac *mac = &pi->mac;
-
-		netif_carrier_on(dev);
-
-		disable_tx_fifo_drain(adap, pi);
-
-		/* Clear local faults */
-		t3_xgm_intr_disable(adap, pi->port_id);
-		t3_read_reg(adap, A_XGM_INT_STATUS +
-				    pi->mac.offset);
-		t3_write_reg(adap,
-			     A_XGM_INT_CAUSE + pi->mac.offset,
-			     F_XGM_INT);
-
-		t3_set_reg_field(adap,
-				 A_XGM_INT_ENABLE +
-				 pi->mac.offset,
-				 F_XGM_INT, F_XGM_INT);
-		t3_xgm_intr_enable(adap, pi->port_id);
-
-		t3_mac_enable(mac, MAC_DIRECTION_TX);
-	} else {
-		netif_carrier_off(dev);
-
-		/* Flush TX FIFO */
-		enable_tx_fifo_drain(adap, pi);
-	}
-	link_report(dev);
-}
-
 /**
  *	t3_os_link_changed - handle link status changes
  *	@adapter: the adapter associated with the link change
- *	@port_id: the port index whose limk status has changed
+ *	@port_id: the port index whose link status has changed
  *	@link_stat: the new status of the link
  *	@speed: the new speed setting
  *	@duplex: the new duplex setting
@@ -242,56 +266,30 @@
  *	then calls this handler for any OS-specific processing.
  */
 void t3_os_link_changed(struct adapter *adapter, int port_id, int link_stat,
-			int speed, int duplex, int pause)
+			int speed, int duplex, int pause, int mac_was_reset)
 {
 	struct net_device *dev = adapter->port[port_id];
 	struct port_info *pi = netdev_priv(dev);
-	struct cmac *mac = &pi->mac;
+
+	if (mac_was_reset) {
+		struct cmac *mac = &pi->mac;
+		rtnl_lock();
+		t3_mac_set_mtu(mac, dev->mtu);
+		t3_mac_set_address(mac, 0, dev->dev_addr);
+		cxgb_set_rxmode(dev);
+		rtnl_unlock();
+	}
 
 	/* Skip changes from disabled ports. */
 	if (!netif_running(dev))
 		return;
 
 	if (link_stat != netif_carrier_ok(dev)) {
-		if (link_stat) {
-			disable_tx_fifo_drain(adapter, pi);
-
-			t3_mac_enable(mac, MAC_DIRECTION_RX);
-
-			/* Clear local faults */
-			t3_xgm_intr_disable(adapter, pi->port_id);
-			t3_read_reg(adapter, A_XGM_INT_STATUS +
-				    pi->mac.offset);
-			t3_write_reg(adapter,
-				     A_XGM_INT_CAUSE + pi->mac.offset,
-				     F_XGM_INT);
-
-			t3_set_reg_field(adapter,
-					 A_XGM_INT_ENABLE + pi->mac.offset,
-					 F_XGM_INT, F_XGM_INT);
-			t3_xgm_intr_enable(adapter, pi->port_id);
-
+		if (link_stat)
 			netif_carrier_on(dev);
-		} else {
+		else
 			netif_carrier_off(dev);
 
-			t3_xgm_intr_disable(adapter, pi->port_id);
-			t3_read_reg(adapter, A_XGM_INT_STATUS + pi->mac.offset);
-			t3_set_reg_field(adapter,
-					 A_XGM_INT_ENABLE + pi->mac.offset,
-					 F_XGM_INT, 0);
-
-			if (is_10G(adapter))
-				pi->phy.ops->power_down(&pi->phy, 1);
-
-			t3_read_reg(adapter, A_XGM_INT_STATUS + pi->mac.offset);
-			t3_mac_disable(mac, MAC_DIRECTION_RX);
-			t3_link_start(&pi->phy, mac, &pi->link_config);
-
-			/* Flush TX FIFO */
-			enable_tx_fifo_drain(adapter, pi);
-		}
-
 		link_report(dev);
 	}
 }
@@ -311,8 +309,8 @@
 		NULL, "SR", "LR", "LRM", "TWINAX", "TWINAX", "unknown"
 	};
 
-	const struct net_device *dev = adap->port[port_id];
-	const struct port_info *pi = netdev_priv(dev);
+	struct net_device *dev = adap->port[port_id];
+	struct port_info *pi = netdev_priv(dev);
 
 	if (pi->phy.modtype == phy_modtype_none)
 		printk(KERN_INFO "%s: PHY module unplugged\n", dev->name);
@@ -321,37 +319,56 @@
 		       mod_str[pi->phy.modtype]);
 }
 
+#ifndef LINUX_2_4
+static ssize_t cxgb_set_nfilters(struct net_device *dev, unsigned int nfilters)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adap = pi->adapter;
+	int min_tids = is_offload(adap) ? MC5_MIN_TIDS : 0;
+
+	if (adap->flags & FULL_INIT_DONE)
+		return -EBUSY;
+	if (nfilters && adap->params.rev == 0)
+		return -EINVAL;
+	if (nfilters > t3_mc5_size(&adap->mc5) - adap->params.mc5.nservers -
+	    min_tids)
+		return -EINVAL;
+	adap->params.mc5.nfilters = nfilters;
+	return 0;
+}
+#endif
+
 static void cxgb_set_rxmode(struct net_device *dev)
 {
 	struct t3_rx_mode rm;
 	struct port_info *pi = netdev_priv(dev);
-
-	init_rx_mode(&rm, dev, dev->mc_list);
-	t3_mac_set_rx_mode(&pi->mac, &rm);
+	struct cmac *mac = &pi->mac;
+
+	init_rx_mode(&rm, dev);
+	t3_mac_set_rx_mode(mac, &rm);
+
 }
 
 /**
  *	link_start - enable a port
- *	@dev: the device to enable
+ *	@dev: the port to enable
  *
  *	Performs the MAC and PHY actions needed to enable a port.
  */
 static void link_start(struct net_device *dev)
 {
-	struct t3_rx_mode rm;
 	struct port_info *pi = netdev_priv(dev);
 	struct cmac *mac = &pi->mac;
 
-	init_rx_mode(&rm, dev, dev->mc_list);
-	t3_mac_reset(mac);
+	if (!mac->multiport)
+		t3_mac_init(mac);
 	t3_mac_set_mtu(mac, dev->mtu);
 	t3_mac_set_address(mac, 0, dev->dev_addr);
-	t3_mac_set_rx_mode(mac, &rm);
+	cxgb_set_rxmode(dev);
 	t3_link_start(&pi->phy, mac, &pi->link_config);
-	t3_mac_enable(mac, MAC_DIRECTION_RX | MAC_DIRECTION_TX);
-}
-
-static inline void cxgb_disable_msi(struct adapter *adapter)
+}
+
+static void cxgb_disable_msi(struct adapter *adapter)
 {
 	if (adapter->flags & USING_MSIX) {
 		pci_disable_msix(adapter->pdev);
@@ -365,7 +382,7 @@
 /*
  * Interrupt handler for asynchronous events used with MSI-X.
  */
-static irqreturn_t t3_async_intr_handler(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_async_intr_handler, irq, cookie, regs)
 {
 	t3_slow_intr_handler(cookie);
 	return IRQ_HANDLED;
@@ -387,33 +404,28 @@
 
 		for (i = 0; i < pi->nqsets; i++, msi_idx++) {
 			snprintf(adap->msix_info[msi_idx].desc, n,
-				 "%s-%d", d->name, pi->first_qset + i);
+				 "%s (queue %d)", d->name,
+				 pi->first_qset + i);
 			adap->msix_info[msi_idx].desc[n] = 0;
 		}
-	}
-}
-
-static int request_msix_data_irqs(struct adapter *adap)
-{
-	int i, j, err, qidx = 0;
-
-	for_each_port(adap, i) {
-		int nqsets = adap2pinfo(adap, i)->nqsets;
-
-		for (j = 0; j < nqsets; ++j) {
-			err = request_irq(adap->msix_info[qidx + 1].vec,
-					  t3_intr_handler(adap,
-							  adap->sge.qs[qidx].
-							  rspq.polling), 0,
-					  adap->msix_info[qidx + 1].desc,
-					  &adap->sge.qs[qidx]);
-			if (err) {
-				while (--qidx >= 0)
-					free_irq(adap->msix_info[qidx + 1].vec,
-						 &adap->sge.qs[qidx]);
-				return err;
-			}
-			qidx++;
+ 	}
+}
+
+static int request_msix_data_irqs(adapter_t *adap)
+{
+	int err, qidx;
+
+	for (qidx = 0; qidx < adap->sge.nqsets; ++qidx) {
+		err = request_irq(adap->msix_info[qidx + 1].vec,
+				  t3_intr_handler(adap,
+					adap->sge.qs[qidx].rspq.flags & USING_POLLING),
+				  0, adap->msix_info[qidx + 1].desc,
+				  &adap->sge.qs[qidx]);
+		if (err) {
+			while (--qidx >= 0)
+				free_irq(adap->msix_info[qidx + 1].vec,
+					 &adap->sge.qs[qidx]);
+			return err;
 		}
 	}
 	return 0;
@@ -422,13 +434,10 @@
 static void free_irq_resources(struct adapter *adapter)
 {
 	if (adapter->flags & USING_MSIX) {
-		int i, n = 0;
+		int i;
 
 		free_irq(adapter->msix_info[0].vec, adapter);
-		for_each_port(adapter, i)
-			n += adap2pinfo(adapter, i)->nqsets;
-
-		for (i = 0; i < n; ++i)
+		for (i = 0; i < adapter->sge.nqsets; ++i)
 			free_irq(adapter->msix_info[i + 1].vec,
 				 &adapter->sge.qs[i]);
 	} else
@@ -438,7 +447,7 @@
 static int await_mgmt_replies(struct adapter *adap, unsigned long init_cnt,
 			      unsigned long n)
 {
-	int attempts = 20;
+	int attempts = 5;
 
 	while (adap->sge.qs[0].rspq.offload_pkts < init_cnt + n) {
 		if (!--attempts)
@@ -460,12 +469,7 @@
 	for (i = 0; i < 16; i++) {
 		struct cpl_smt_write_req *req;
 
-		skb = alloc_skb(sizeof(*req), GFP_KERNEL);
-		if (!skb)
-			skb = adap->nofail_skb;
-		if (!skb)
-			goto alloc_skb_fail;
-
+		skb = alloc_skb(sizeof(*req), GFP_KERNEL | __GFP_NOFAIL);
 		req = (struct cpl_smt_write_req *)__skb_put(skb, sizeof(*req));
 		memset(req, 0, sizeof(*req));
 		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
@@ -473,66 +477,33 @@
 		req->mtu_idx = NMTUS - 1;
 		req->iff = i;
 		t3_mgmt_tx(adap, skb);
-		if (skb == adap->nofail_skb) {
-			await_mgmt_replies(adap, cnt, i + 1);
-			adap->nofail_skb = alloc_skb(sizeof(*greq), GFP_KERNEL);
-			if (!adap->nofail_skb)
-				goto alloc_skb_fail;
-		}
 	}
 
 	for (i = 0; i < 2048; i++) {
 		struct cpl_l2t_write_req *req;
 
-		skb = alloc_skb(sizeof(*req), GFP_KERNEL);
-		if (!skb)
-			skb = adap->nofail_skb;
-		if (!skb)
-			goto alloc_skb_fail;
-
+		skb = alloc_skb(sizeof(*req), GFP_KERNEL | __GFP_NOFAIL);
 		req = (struct cpl_l2t_write_req *)__skb_put(skb, sizeof(*req));
 		memset(req, 0, sizeof(*req));
 		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
 		OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_L2T_WRITE_REQ, i));
 		req->params = htonl(V_L2T_W_IDX(i));
 		t3_mgmt_tx(adap, skb);
-		if (skb == adap->nofail_skb) {
-			await_mgmt_replies(adap, cnt, 16 + i + 1);
-			adap->nofail_skb = alloc_skb(sizeof(*greq), GFP_KERNEL);
-			if (!adap->nofail_skb)
-				goto alloc_skb_fail;
-		}
 	}
 
 	for (i = 0; i < 2048; i++) {
 		struct cpl_rte_write_req *req;
 
-		skb = alloc_skb(sizeof(*req), GFP_KERNEL);
-		if (!skb)
-			skb = adap->nofail_skb;
-		if (!skb)
-			goto alloc_skb_fail;
-
+		skb = alloc_skb(sizeof(*req), GFP_KERNEL | __GFP_NOFAIL);
 		req = (struct cpl_rte_write_req *)__skb_put(skb, sizeof(*req));
 		memset(req, 0, sizeof(*req));
 		req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
 		OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_RTE_WRITE_REQ, i));
 		req->l2t_idx = htonl(V_L2T_W_IDX(i));
 		t3_mgmt_tx(adap, skb);
-		if (skb == adap->nofail_skb) {
-			await_mgmt_replies(adap, cnt, 16 + 2048 + i + 1);
-			adap->nofail_skb = alloc_skb(sizeof(*greq), GFP_KERNEL);
-			if (!adap->nofail_skb)
-				goto alloc_skb_fail;
-		}
 	}
 
-	skb = alloc_skb(sizeof(*greq), GFP_KERNEL);
-	if (!skb)
-		skb = adap->nofail_skb;
-	if (!skb)
-		goto alloc_skb_fail;
-
+	skb = alloc_skb(sizeof(*greq), GFP_KERNEL | __GFP_NOFAIL);
 	greq = (struct cpl_set_tcb_field *)__skb_put(skb, sizeof(*greq));
 	memset(greq, 0, sizeof(*greq));
 	greq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
@@ -541,17 +512,8 @@
 	t3_mgmt_tx(adap, skb);
 
 	i = await_mgmt_replies(adap, cnt, 16 + 2048 + 2048 + 1);
-	if (skb == adap->nofail_skb) {
-		i = await_mgmt_replies(adap, cnt, 16 + 2048 + 2048 + 1);
-		adap->nofail_skb = alloc_skb(sizeof(*greq), GFP_KERNEL);
-	}
-
 	t3_tp_set_offload_mode(adap, 0);
 	return i;
-
-alloc_skb_fail:
-	t3_tp_set_offload_mode(adap, 0);
-	return -ENOMEM;
 }
 
 /**
@@ -565,28 +527,103 @@
  *	We always configure the RSS mapping for two ports since the mapping
  *	table has plenty of entries.
  */
-static void setup_rss(struct adapter *adap)
+static void setup_rss(adapter_t *adap)
 {
 	int i;
-	unsigned int nq0 = adap2pinfo(adap, 0)->nqsets;
-	unsigned int nq1 = adap->port[1] ? adap2pinfo(adap, 1)->nqsets : 1;
+	unsigned int nq[2];
 	u8 cpus[SGE_QSETS + 1];
 	u16 rspq_map[RSS_TABLE_SIZE];
 
 	for (i = 0; i < SGE_QSETS; ++i)
 		cpus[i] = i;
-	cpus[SGE_QSETS] = 0xff;	/* terminator */
+	cpus[SGE_QSETS] = 0xff;                     /* terminator */
+
+	nq[0] = nq[1] = 0;
+	for_each_port(adap, i) {
+		const struct port_info *pi = adap2pinfo(adap, i);
+
+		nq[pi->tx_chan] += pi->nqsets;
+	}
 
 	for (i = 0; i < RSS_TABLE_SIZE / 2; ++i) {
-		rspq_map[i] = i % nq0;
-		rspq_map[i + RSS_TABLE_SIZE / 2] = (i % nq1) + nq0;
+		rspq_map[i] = nq[0] ? i % nq[0] : 0;
+		rspq_map[i + RSS_TABLE_SIZE / 2] = nq[1] ? i % nq[1] + nq[0] : 0;
 	}
 
+	/* Calculate the reverse RSS map table */
+	for (i = 0; i < RSS_TABLE_SIZE; ++i)
+		if (adap->rrss_map[rspq_map[i]] == 0xff)
+			adap->rrss_map[rspq_map[i]] = i;
+
 	t3_config_rss(adap, F_RQFEEDBACKENABLE | F_TNLLKPEN | F_TNLMAPEN |
-		      F_TNLPRTEN | F_TNL2TUPEN | F_TNL4TUPEN |
-		      V_RRCPLCPUSIZE(6) | F_HASHTOEPLITZ, cpus, rspq_map);
-}
-
+		      F_TNLPRTEN | F_TNL2TUPEN | F_TNL4TUPEN | F_OFDMAPEN |
+		      F_RRCPLMAPEN | V_RRCPLCPUSIZE(6) | F_HASHTOEPLITZ,
+		      cpus, rspq_map);
+}
+
+static void ring_dbs(struct adapter *adap)
+{
+	int i, j;
+
+	for (i = 0; i < SGE_QSETS; i++) {
+		struct sge_qset *qs = &adap->sge.qs[i];
+
+		if (qs->adap)
+			for (j = 0; j < SGE_TXQ_PER_SET; j++)
+				t3_write_reg(adap, A_SG_KDOORBELL,
+					     F_SELEGRCNTX |
+					     V_EGRCNTX(qs->txq[j].cntxt_id));
+	}
+}
+
+#if !defined(NAPI_UPDATE)
+/*
+ * If we have multiple receive queues per port serviced by NAPI we need one
+ * netdevice per queue as NAPI operates on netdevices.  We already have one
+ * netdevice, namely the one associated with the interface, so we use dummy
+ * ones for any additional queues.  Note that these netdevices exist purely
+ * so that NAPI has something to work with, they do not represent network
+ * ports and are not registered.
+ */
+static int init_dummy_netdevs(struct adapter *adap)
+{
+	int i, j, dummy_idx = 0;
+	struct net_device *nd;
+
+	for_each_port(adap, i) {
+		struct net_device *dev = adap->port[i];
+		const struct port_info *pi = netdev_priv(dev);
+
+		for (j = 0; j < pi->nqsets - 1; j++) {
+			if (!adap->dummy_netdev[dummy_idx]) {
+				struct port_info *p;
+
+				nd = alloc_netdev(sizeof(*p), "", ether_setup);
+				if (!nd)
+					goto free_all;
+
+				p = netdev_priv(nd);
+				p->adapter = adap;
+				nd->weight = 64;
+				set_bit(__LINK_STATE_START, &nd->state);
+				adap->dummy_netdev[dummy_idx] = nd;
+			}
+			strcpy(adap->dummy_netdev[dummy_idx]->name, dev->name);
+			dummy_idx++;
+		}
+	}
+	return 0;
+
+free_all:
+	while (--dummy_idx >= 0) {
+		free_netdev(adap->dummy_netdev[dummy_idx]);
+		adap->dummy_netdev[dummy_idx] = NULL;
+	}
+	return -ENOMEM;
+}
+#endif
+
+#if defined(NAPI_UPDATE)
 static void init_napi(struct adapter *adap)
 {
 	int i;
@@ -604,50 +641,248 @@
 	 * adds each new napi_struct to a list.  Be careful not to call it a
 	 * second time, e.g., during EEH recovery, by making a note of it.
 	 */
-	adap->flags |= NAPI_INIT;
-}
+        adap->flags |= NAPI_INIT;
+
+}
+#endif
 
 /*
  * Wait until all NAPI handlers are descheduled.  This includes the handlers of
  * both netdevices representing interfaces and the dummy ones for the extra
  * queues.
  */
-static void quiesce_rx(struct adapter *adap)
+static void quiesce_rx(adapter_t *adap)
 {
 	int i;
 
-	for (i = 0; i < SGE_QSETS; i++)
-		if (adap->sge.qs[i].adap)
-			napi_disable(&adap->sge.qs[i].napi);
+#if defined(NAPI_UPDATE)
+	for (i = 0; i < SGE_QSETS; i++) {
+		struct sge_qset *qs = &adap->sge.qs[i];
+
+		if (qs->adap)
+			napi_disable(&qs->napi);
+	}
+#else
+	struct net_device *dev;
+
+	for_each_port(adap, i) {
+		dev = adap->port[i];
+		while (test_bit(__LINK_STATE_RX_SCHED, &dev->state))
+			msleep(1);
+	}
+
+	for (i = 0; i < ARRAY_SIZE(adap->dummy_netdev); i++) {
+		dev = adap->dummy_netdev[i];
+		if (dev)
+			while (test_bit(__LINK_STATE_RX_SCHED, &dev->state))
+				msleep(1);
+	}
+#endif
 }
 
 static void enable_all_napi(struct adapter *adap)
 {
+#if defined(NAPI_UPDATE)
 	int i;
 	for (i = 0; i < SGE_QSETS; i++)
 		if (adap->sge.qs[i].adap)
 			napi_enable(&adap->sge.qs[i].napi);
-}
-
-/**
- *	set_qset_lro - Turn a queue set's LRO capability on and off
- *	@dev: the device the qset is attached to
- *	@qset_idx: the queue set index
- *	@val: the LRO switch
- *
- *	Sets LRO on or off for a particular queue set.
- *	the device's features flag is updated to reflect the LRO
- *	capability when all queues belonging to the device are
- *	in the same state.
+#endif
+}
+
+/*
+ * Allocate a chunk of memory using kmalloc or, if that fails, vmalloc.
+ * The allocated memory is cleared.
  */
-static void set_qset_lro(struct net_device *dev, int qset_idx, int val)
-{
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
-
-	adapter->params.sge.qset[qset_idx].lro = !!val;
-	adapter->sge.qs[qset_idx].lro_enabled = !!val;
-}
+static void *alloc_mem(unsigned long size)
+{
+	void *p = kmalloc(size, GFP_KERNEL);
+
+	if (!p)
+		p = vmalloc(size);
+	if (p)
+		memset(p, 0, size);
+	return p;
+}
+
+/*
+ * Free memory allocated through alloc_mem().
+ */
+static void free_mem(void *addr)
+{
+	unsigned long p = (unsigned long) addr;
+
+	if (p >= VMALLOC_START && p < VMALLOC_END)
+		vfree(addr);
+	else
+		kfree(addr);
+}
+
+static int alloc_filters(struct adapter *adap)
+{
+	struct filter_info *p;
+
+	if (!adap->params.mc5.nfilters)     /* no filters requested */
+		return 0;
+
+	adap->filters = alloc_mem(adap->params.mc5.nfilters * sizeof(*p));
+	if (!adap->filters)
+		return -ENOMEM;
+
+	/* Set the default filters, only need to set non-0 fields here. */
+	p = &adap->filters[adap->params.mc5.nfilters - 1];
+	p->vlan = 0xfff;
+	p->vlan_prio = FILTER_NO_VLAN_PRI;
+	p->pass = p->rss = p->valid = p->locked = 1;
+
+	return 0;
+}
+
+static void mk_set_tcb_field(struct cpl_set_tcb_field *req, unsigned int tid,
+			     unsigned int word, u64 mask, u64 val)
+{
+	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));
+	req->reply = V_NO_REPLY(1);
+	req->cpu_idx = 0;
+	req->word = htons(word);
+	req->mask = cpu_to_be64(mask);
+	req->val = cpu_to_be64(val);
+}
+
+static inline void set_tcb_field_ulp(struct cpl_set_tcb_field *req,
+				     unsigned int tid, unsigned int word,
+				     u64 mask, u64 val)
+{
+	struct ulp_txpkt *txpkt = (struct ulp_txpkt *)req;
+
+	txpkt->cmd_dest = htonl(V_ULPTX_CMD(ULP_TXPKT));
+	txpkt->len = htonl(V_ULPTX_NFLITS(sizeof(*req) / 8));
+	mk_set_tcb_field(req, tid, word, mask, val);
+}
+
+static int set_filter(struct adapter *adap, int id, const struct filter_info *f)
+{
+	int len;
+	struct sk_buff *skb;
+	struct ulp_txpkt *txpkt;
+	struct work_request_hdr *wr;
+	struct cpl_pass_open_req *oreq;
+	struct cpl_set_tcb_field *sreq;
+
+	len = sizeof(*wr) + sizeof(*oreq) + 2 * sizeof(*sreq);
+	id += t3_mc5_size(&adap->mc5) - adap->params.mc5.nroutes -
+	      adap->params.mc5.nfilters;
+
+	skb = alloc_skb(len, GFP_KERNEL | __GFP_NOFAIL);
+
+	wr = (struct work_request_hdr *)__skb_put(skb, len);
+	wr->wr_hi = htonl(V_WR_OP(FW_WROPCODE_BYPASS) | F_WR_ATOMIC);
+
+	oreq = (struct cpl_pass_open_req *)(wr + 1);
+	txpkt = (struct ulp_txpkt *)oreq;
+	txpkt->cmd_dest = htonl(V_ULPTX_CMD(ULP_TXPKT));
+	txpkt->len = htonl(V_ULPTX_NFLITS(sizeof(*oreq) / 8));
+	OPCODE_TID(oreq) = htonl(MK_OPCODE_TID(CPL_PASS_OPEN_REQ, id));
+	oreq->local_port = htons(f->dport);
+	oreq->peer_port = htons(f->sport);
+	oreq->local_ip = htonl(f->dip);
+	oreq->peer_ip = htonl(f->sip);
+	oreq->peer_netmask = htonl(f->sip_mask);
+	oreq->opt0h = 0;
+	oreq->opt0l = htonl(F_NO_OFFLOAD);
+	oreq->opt1 = htonl(V_MAC_MATCH_VALID(f->mac_vld) |
+			 V_CONN_POLICY(CPL_CONN_POLICY_FILTER) |
+			 V_VLAN_PRI(f->vlan_prio >> 1) |
+			 V_VLAN_PRI_VALID(f->vlan_prio != FILTER_NO_VLAN_PRI) |
+			 V_PKT_TYPE(f->pkt_type) | V_OPT1_VLAN(f->vlan) |
+			 V_MAC_MATCH(f->mac_idx | (f->mac_hit << 4)));
+
+	sreq = (struct cpl_set_tcb_field *)(oreq + 1);
+	set_tcb_field_ulp(sreq, id, 1, 0x1800808000ULL,
+			  (f->report_filter_id << 15) | (1 << 23) |
+			  ((u64)f->pass << 35) | ((u64)!f->rss << 36));
+	set_tcb_field_ulp(sreq + 1, id, 0, 0xffffffff, (2 << 19) | 1);
+	t3_mgmt_tx(adap, skb);
+
+	if (f->pass && !f->rss) {
+		len = sizeof(*sreq);
+		skb = alloc_skb(len, GFP_KERNEL | __GFP_NOFAIL);
+
+		sreq = (struct cpl_set_tcb_field *)__skb_put(skb, len);
+		sreq->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
+		mk_set_tcb_field(sreq, id, 25, 0x3f80000,
+				 (u64)adap->rrss_map[f->qset] << 19);
+		t3_mgmt_tx(adap, skb);
+	}
+	return 0;
+}
+
+static int setup_hw_filters(struct adapter *adap)
+{
+	int i, err = 0;
+
+	if (!adap->filters ||
+	    atomic_read(&adap->filter_toe_mode) == CXGB3_FTM_TOE)
+		return 0;
+
+	t3_enable_filters(adap);
+
+	for (i = err = 0; i < adap->params.mc5.nfilters && !err; i++)
+		if (adap->filters[i].locked) {
+			int ret = set_filter(adap, i, &adap->filters[i]);
+			if (ret)
+				err = ret;
+		}
+	return err;
+}
+
+/*
+ * Atomically determine/set the filter/TOE mode exclusion switch to the
+ * desired mode and return the success state.  The first time this is called
+ * the filter/TOE mode of the adapter will be set permanently to the selected
+ * mode.
+ */
+int cxgb3_filter_toe_mode(struct adapter *adapter, int mode)
+{
+	static spinlock_t cxgb3_filter_toe_lock = SPIN_LOCK_UNLOCKED;
+	int cur_mode;
+
+	/*
+	 * It would be much easier to do all of this if we could use
+	 * atomic_cmpxchg() but that primitive isn't available on all
+	 * platforms so we're essentially faking it here via a spinlock.  We
+	 * do an optimization here of reading the interlock without taking the
+	 * spinlock and returning success/failure if the interlock has already
+	 * been set.  We can do this because the interlock is one-shot and
+	 * once set is never changed.  With this optimization, a single global
+	 * spinlock is fine for protecting the critical section.
+	 */
+	cur_mode = atomic_read(&adapter->filter_toe_mode);
+	if (cur_mode != CXGB3_FTM_NONE)
+		return cur_mode == mode;
+
+	spin_lock(&cxgb3_filter_toe_lock);
+
+	cur_mode = atomic_read(&adapter->filter_toe_mode);
+	if (cur_mode != CXGB3_FTM_NONE) {
+		/* got changed while we were taking the lock ... */
+		spin_unlock(&cxgb3_filter_toe_lock);
+		return cur_mode == mode;
+	}
+
+	/*
+	 * If we're successfully setting TOE mode for the adapter, disable
+	 * the adapter's filter capabilities.
+	 */
+	if (mode == CXGB3_FTM_TOE)
+		t3_disable_filters(adapter);
+
+	atomic_set(&adapter->filter_toe_mode, mode);
+	spin_unlock(&cxgb3_filter_toe_lock);
+
+	return 1;
+}
+
 
 /**
  *	setup_sge_qsets - configure SGE Tx/Rx/response queues
@@ -659,24 +894,34 @@
  */
 static int setup_sge_qsets(struct adapter *adap)
 {
-	int i, j, err, irq_idx = 0, qset_idx = 0;
+	int i, j, err, irq_idx = 0, qset_idx = 0, dummy_dev_idx;
 	unsigned int ntxq = SGE_TXQ_PER_SET;
 
 	if (adap->params.rev > 0 && !(adap->flags & USING_MSI))
 		irq_idx = -1;
 
+	dummy_dev_idx = 0;
 	for_each_port(adap, i) {
 		struct net_device *dev = adap->port[i];
 		struct port_info *pi = netdev_priv(dev);
 
 		pi->qs = &adap->sge.qs[pi->first_qset];
 		for (j = 0; j < pi->nqsets; ++j, ++qset_idx) {
-			set_qset_lro(dev, qset_idx, pi->rx_offload & T3_LRO);
+			if (!pi->rx_csum_offload)
+				adap->params.sge.qset[qset_idx].lro = 0;
 			err = t3_sge_alloc_qset(adap, qset_idx, 1,
 				(adap->flags & USING_MSIX) ? qset_idx + 1 :
 							     irq_idx,
-				&adap->params.sge.qset[qset_idx], ntxq, dev,
+				&adap->params.sge.qset[qset_idx], ntxq,
+#if defined(NAPI_UPDATE)
+				dev,
 				netdev_get_tx_queue(dev, j));
+#else
+				j == 0 ? dev :
+					 adap->dummy_netdev[dummy_dev_idx++],
+				NULL);
+#endif
+
 			if (err) {
 				t3_free_sge_resources(adap);
 				return err;
@@ -687,21 +932,22 @@
 	return 0;
 }
 
-static ssize_t attr_show(struct device *d, char *buf,
-			 ssize_t(*format) (struct net_device *, char *))
+#ifndef	LINUX_2_4
+static ssize_t attr_show(struct cxgb3_compat_device *d, char *buf,
+			 ssize_t (*format)(struct net_device *, char *))
 {
 	ssize_t len;
 
 	/* Synchronize with ioctls that may shut down the device */
 	rtnl_lock();
-	len = (*format) (to_net_dev(d), buf);
+	len = (*format)(to_net_dev(d), buf);
 	rtnl_unlock();
 	return len;
 }
 
-static ssize_t attr_store(struct device *d,
+static ssize_t attr_store(struct cxgb3_compat_device *d,
 			  const char *buf, size_t len,
-			  ssize_t(*set) (struct net_device *, unsigned int),
+			  ssize_t (*set)(struct net_device *, unsigned int),
 			  unsigned int min_val, unsigned int max_val)
 {
 	char *endp;
@@ -716,7 +962,7 @@
 		return -EINVAL;
 
 	rtnl_lock();
-	ret = (*set) (to_net_dev(d), val);
+	ret = (*set)(to_net_dev(d), val);
 	if (!ret)
 		ret = len;
 	rtnl_unlock();
@@ -730,31 +976,17 @@
 	struct adapter *adap = pi->adapter; \
 	return sprintf(buf, "%u\n", val_expr); \
 } \
-static ssize_t show_##name(struct device *d, struct device_attribute *attr, \
-			   char *buf) \
+CXGB3_SHOW_FUNC(show_##name, d, attr, buf) \
 { \
 	return attr_show(d, buf, format_##name); \
 }
 
 static ssize_t set_nfilters(struct net_device *dev, unsigned int val)
 {
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adap = pi->adapter;
-	int min_tids = is_offload(adap) ? MC5_MIN_TIDS : 0;
-
-	if (adap->flags & FULL_INIT_DONE)
-		return -EBUSY;
-	if (val && adap->params.rev == 0)
-		return -EINVAL;
-	if (val > t3_mc5_size(&adap->mc5) - adap->params.mc5.nservers -
-	    min_tids)
-		return -EINVAL;
-	adap->params.mc5.nfilters = val;
-	return 0;
-}
-
-static ssize_t store_nfilters(struct device *d, struct device_attribute *attr,
-			      const char *buf, size_t len)
+	return cxgb_set_nfilters(dev, val);
+}
+
+CXGB3_STORE_FUNC(store_nfilters, d, attr, buf, len)
 {
 	return attr_store(d, buf, len, set_nfilters, 0, ~0);
 }
@@ -773,19 +1005,18 @@
 	return 0;
 }
 
-static ssize_t store_nservers(struct device *d, struct device_attribute *attr,
-			      const char *buf, size_t len)
+CXGB3_STORE_FUNC(store_nservers, d, attr, buf, len)
 {
 	return attr_store(d, buf, len, set_nservers, 0, ~0);
 }
 
 #define CXGB3_ATTR_R(name, val_expr) \
 CXGB3_SHOW(name, val_expr) \
-static DEVICE_ATTR(name, S_IRUGO, show_##name, NULL)
+static CXGB3_DEVICE_ATTR(name, S_IRUGO, show_##name, NULL)
 
 #define CXGB3_ATTR_RW(name, val_expr, store_method) \
 CXGB3_SHOW(name, val_expr) \
-static DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_method)
+static CXGB3_DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_method)
 
 CXGB3_ATTR_R(cam_size, t3_mc5_size(&adap->mc5));
 CXGB3_ATTR_RW(nfilters, adap->params.mc5.nfilters, store_nfilters);
@@ -798,42 +1029,159 @@
 	NULL
 };
 
-static struct attribute_group cxgb3_attr_group = {.attrs = cxgb3_attrs };
-
-static ssize_t tm_attr_show(struct device *d,
-			    char *buf, int sched)
+static struct attribute_group cxgb3_attr_group = { .attrs = cxgb3_attrs };
+
+static ssize_t reg_attr_show(struct cxgb3_compat_device *d, char *buf, int reg,
+			     int shift, unsigned int mask)
 {
 	struct port_info *pi = netdev_priv(to_net_dev(d));
 	struct adapter *adap = pi->adapter;
-	unsigned int v, addr, bpt, cpt;
 	ssize_t len;
-
-	addr = A_TP_TX_MOD_Q1_Q0_RATE_LIMIT - sched / 2;
+	unsigned int v;
+
+	/* Synchronize with ioctls that may shut down the device */
 	rtnl_lock();
-	t3_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
-	v = t3_read_reg(adap, A_TP_TM_PIO_DATA);
-	if (sched & 1)
-		v >>= 16;
-	bpt = (v >> 8) & 0xff;
-	cpt = v & 0xff;
-	if (!cpt)
-		len = sprintf(buf, "disabled\n");
-	else {
-		v = (adap->params.vpd.cclk * 1000) / cpt;
-		len = sprintf(buf, "%u Kbps\n", (v * bpt) / 125);
-	}
+	v = t3_read_reg(adap, reg);
+	len = sprintf(buf, "%u\n", (v >> shift) & mask);
 	rtnl_unlock();
 	return len;
 }
 
-static ssize_t tm_attr_store(struct device *d,
-			     const char *buf, size_t len, int sched)
+static ssize_t reg_attr_store(struct cxgb3_compat_device *d, const char *buf,
+			      size_t len, int reg, int shift,
+			      unsigned int mask, unsigned int min_val,
+			      unsigned int max_val)
 {
 	struct port_info *pi = netdev_priv(to_net_dev(d));
 	struct adapter *adap = pi->adapter;
+	char *endp;
 	unsigned int val;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf || val < min_val || val > max_val)
+		return -EINVAL;
+
+	rtnl_lock();
+	t3_set_reg_field(adap, reg, mask << shift,
+			 val << shift);
+	rtnl_unlock();
+	return len;
+}
+
+#define T3_REG_SHOW(name, reg, shift, mask) \
+CXGB3_SHOW_FUNC(show_##name, d, attr, buf) \
+{ \
+	return reg_attr_show(d, buf, reg, shift, mask); \
+}
+
+#define T3_REG_STORE(name, reg, shift, mask, min_val, max_val) \
+CXGB3_STORE_FUNC(store_##name, d, attr, buf, len) \
+{ \
+	return reg_attr_store(d, buf, len, reg, shift, mask, min_val, max_val); \
+}
+
+#define T3_ATTR(name, reg, shift, mask, min_val, max_val) \
+T3_REG_SHOW(name, reg, shift, mask) \
+T3_REG_STORE(name, reg, shift, mask, min_val, max_val) \
+static CXGB3_DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+T3_ATTR(tcp_retries1, A_TP_SHIFT_CNT, S_RXTSHIFTMAXR1, M_RXTSHIFTMAXR1, 3, 15);
+T3_ATTR(tcp_retries2, A_TP_SHIFT_CNT, S_RXTSHIFTMAXR2, M_RXTSHIFTMAXR2, 0, 15);
+T3_ATTR(tcp_syn_retries, A_TP_SHIFT_CNT, S_SYNSHIFTMAX, M_SYNSHIFTMAX, 0, 15);
+T3_ATTR(tcp_keepalive_probes, A_TP_SHIFT_CNT, S_KEEPALIVEMAX, M_KEEPALIVEMAX,
+	1, 15);
+T3_ATTR(tcp_sack, A_TP_TCP_OPTIONS, S_SACKMODE, M_SACKMODE, 0, 1);
+T3_ATTR(tcp_timestamps, A_TP_TCP_OPTIONS, S_TIMESTAMPSMODE, M_TIMESTAMPSMODE,
+	0, 1);
+
+static ssize_t timer_attr_show(struct cxgb3_compat_device *d, char *buf, int reg)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+	struct adapter *adap = pi->adapter;
+	unsigned int v, tps;
+	ssize_t len;
+
+	/* Synchronize with ioctls that may shut down the device */
+	rtnl_lock();
+	v = t3_read_reg(adap, reg);
+	tps = (adap->params.vpd.cclk * 1000) >> adap->params.tp.tre;
+	len = sprintf(buf, "%u\n", v / tps);
+	rtnl_unlock();
+	return len;
+}
+
+static ssize_t timer_attr_store(struct cxgb3_compat_device *d, const char *buf,
+				size_t len, int reg, unsigned int min_val,
+				unsigned int max_val)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+	struct adapter *adap = pi->adapter;
+	char *endp;
+	unsigned int val, tps;
+
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	tps = (adap->params.vpd.cclk * 1000) >> adap->params.tp.tre;
+	val = simple_strtoul(buf, &endp, 0);
+	if (endp == buf || val * tps < min_val || val * tps > max_val)
+		return -EINVAL;
+
+	rtnl_lock();
+	t3_write_reg(adap, reg, val * tps);
+	rtnl_unlock();
+	return len;
+}
+
+#define T3_TIMER_REG_SHOW(name, reg) \
+CXGB3_SHOW_FUNC(show_##name, d, attr, buf) \
+{ \
+	return timer_attr_show(d, buf, reg); \
+}
+
+#define T3_TIMER_REG_STORE(name, reg, min_val, max_val) \
+CXGB3_STORE_FUNC(store_##name, d, attr, buf, len) \
+{ \
+	return timer_attr_store(d, buf, len, reg, min_val, max_val); \
+}
+
+#define T3_TIMER_ATTR(name, reg, min_val, max_val) \
+T3_TIMER_REG_SHOW(name, reg) \
+T3_TIMER_REG_STORE(name, reg, min_val, max_val) \
+static CXGB3_DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+T3_TIMER_ATTR(tcp_keepalive_time, A_TP_KEEP_IDLE, 0, M_KEEPALIVEIDLE);
+T3_TIMER_ATTR(tcp_keepalive_intvl, A_TP_KEEP_INTVL, 0, M_KEEPALIVEINTVL);
+T3_TIMER_ATTR(tcp_finwait2_timeout, A_TP_FINWAIT2_TIMER, 0, M_FINWAIT2TIME);
+
+static ssize_t tm_attr_show(struct cxgb3_compat_device *d, char *buf, int sched)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+	struct adapter *adap = pi->adapter;
+	ssize_t len;
+	unsigned int rate;
+
+	rtnl_lock();
+	t3_get_tx_sched(adap, sched, &rate, NULL);
+	if (!rate)
+		len = sprintf(buf, "disabled\n");
+	else
+		len = sprintf(buf, "%u Kbps\n", rate);
+	rtnl_unlock();
+	return len;
+}
+
+static ssize_t tm_attr_store(struct cxgb3_compat_device *d, const char *buf,
+			     size_t len, int sched)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+	struct adapter *adap = pi->adapter;
 	char *endp;
 	ssize_t ret;
+	unsigned int val;
 
 	if (!capable(CAP_NET_ADMIN))
 		return -EPERM;
@@ -851,17 +1199,15 @@
 }
 
 #define TM_ATTR(name, sched) \
-static ssize_t show_##name(struct device *d, struct device_attribute *attr, \
-			   char *buf) \
+CXGB3_SHOW_FUNC(show_##name, d, attr, buf) \
 { \
 	return tm_attr_show(d, buf, sched); \
 } \
-static ssize_t store_##name(struct device *d, struct device_attribute *attr, \
-			    const char *buf, size_t len) \
+CXGB3_STORE_FUNC(store_##name, d, attr, buf, len) \
 { \
 	return tm_attr_store(d, buf, len, sched); \
 } \
-static DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+static CXGB3_DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
 
 TM_ATTR(sched0, 0);
 TM_ATTR(sched1, 1);
@@ -873,6 +1219,15 @@
 TM_ATTR(sched7, 7);
 
 static struct attribute *offload_attrs[] = {
+	&dev_attr_tcp_retries1.attr,
+	&dev_attr_tcp_retries2.attr,
+	&dev_attr_tcp_syn_retries.attr,
+	&dev_attr_tcp_keepalive_probes.attr,
+	&dev_attr_tcp_sack.attr,
+	&dev_attr_tcp_timestamps.attr,
+	&dev_attr_tcp_keepalive_time.attr,
+	&dev_attr_tcp_keepalive_intvl.attr,
+	&dev_attr_tcp_finwait2_timeout.attr,
 	&dev_attr_sched0.attr,
 	&dev_attr_sched1.attr,
 	&dev_attr_sched2.attr,
@@ -884,7 +1239,47 @@
 	NULL
 };
 
-static struct attribute_group offload_attr_group = {.attrs = offload_attrs };
+static struct attribute_group offload_attr_group = { .attrs = offload_attrs };
+
+static ssize_t iscsi_ipv4addr_attr_show(struct cxgb3_compat_device *d, char *buf)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+	__be32 a = pi->iscsi_ipv4addr;
+
+	return sprintf(buf, NIPQUAD_FMT "\n", NIPQUAD(a));
+}
+
+static ssize_t iscsi_ipv4addr_attr_store(struct cxgb3_compat_device *d,
+				       const char *buf, size_t len)
+{
+	struct port_info *pi = netdev_priv(to_net_dev(d));
+
+	pi->iscsi_ipv4addr = in_aton(buf);
+	return len;
+}
+
+#define ISCSI_IPADDR_ATTR(name) \
+CXGB3_SHOW_FUNC(show_##name, d, attr, buf) \
+{ \
+	return iscsi_ipv4addr_attr_show(d, buf); \
+} \
+CXGB3_STORE_FUNC(store_##name, d, attr, buf, len) \
+{ \
+	return iscsi_ipv4addr_attr_store(d, buf, len); \
+} \
+static CXGB3_DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+ISCSI_IPADDR_ATTR(iscsi_ipv4addr);
+
+static struct attribute *iscsi_offload_attrs[] = {
+	&dev_attr_iscsi_ipv4addr.attr,
+	NULL
+};
+
+static struct attribute_group iscsi_offload_attr_group = {
+	.attrs = iscsi_offload_attrs
+};
+#endif	/* ! LINUX_2_4 */
 
 /*
  * Sends an sk_buff to an offload queue driver
@@ -905,13 +1300,12 @@
 	struct cpl_smt_write_req *req;
 	struct sk_buff *skb = alloc_skb(sizeof(*req), GFP_KERNEL);
 
-	if (!skb)
-		return -ENOMEM;
+	if (!skb) return -ENOMEM;
 
 	req = (struct cpl_smt_write_req *)__skb_put(skb, sizeof(*req));
 	req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
 	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SMT_WRITE_REQ, idx));
-	req->mtu_idx = NMTUS - 1;	/* should be 0 but there's a T3 bug */
+	req->mtu_idx = NMTUS - 1;  /* should be 0 but there's a T3 bug */
 	req->iff = idx;
 	memset(req->src_mac1, 0, sizeof(req->src_mac1));
 	memcpy(req->src_mac0, adapter->port[idx]->dev_addr, ETH_ALEN);
@@ -925,7 +1319,7 @@
 	int i;
 
 	for_each_port(adapter, i)
-	    write_smt_entry(adapter, i);
+		write_smt_entry(adapter, i);
 	return 0;
 }
 
@@ -943,14 +1337,8 @@
 {
 	struct sk_buff *skb;
 	struct mngt_pktsched_wr *req;
-	int ret;
-
-	skb = alloc_skb(sizeof(*req), GFP_KERNEL);
-	if (!skb)
-		skb = adap->nofail_skb;
-	if (!skb)
-		return -ENOMEM;
-
+
+	skb = alloc_skb(sizeof(*req), GFP_KERNEL | __GFP_NOFAIL);
 	req = (struct mngt_pktsched_wr *)skb_put(skb, sizeof(*req));
 	req->wr_hi = htonl(V_WR_OP(FW_WROPCODE_MNGT));
 	req->mngt_opcode = FW_MNGTOPCODE_PKTSCHED_SET;
@@ -959,14 +1347,28 @@
 	req->min = lo;
 	req->max = hi;
 	req->binding = port;
-	ret = t3_mgmt_tx(adap, skb);
-	if (skb == adap->nofail_skb) {
-		adap->nofail_skb = alloc_skb(sizeof(struct cpl_set_tcb_field),
-					     GFP_KERNEL);
-		if (!adap->nofail_skb)
-			ret = -ENOMEM;
-	}
-
+	t3_mgmt_tx(adap, skb);
+
+	return 0;
+}
+
+static int send_watchdog_cmd(struct adapter *adap, int wd, __u16 en,
+      		__u16 ac, __u32 tval)
+{
+  	struct sk_buff *skb;
+ 	struct mngt_watchdog_wr *req;
+ 	int ret;
+ 
+	skb = alloc_skb(sizeof(*req), GFP_KERNEL | __GFP_NOFAIL);
+ 	req = (struct mngt_watchdog_wr *)skb_put(skb, sizeof(*req));
+ 	memset(req, 0, sizeof(*req));
+ 	req->wr_hi = htonl(V_WR_OP(FW_WROPCODE_MNGT));
+ 	req->mngt_opcode = (__u8)wd;
+ 	req->enable = htons(en ? V_FW_WR_WD_EN(1) : V_FW_WR_WD_EN(0));
+ 	req->ac_or_rsvd1 = htons(ac);
+	req->tval_or_rsvd2 = htonl(tval);
+ 	ret = t3_mgmt_tx(adap, skb);
+ 
 	return ret;
 }
 
@@ -979,8 +1381,8 @@
 
 		for (j = 0; j < pi->nqsets; ++j) {
 			int ret = send_pktsched_cmd(adap, 1,
-						    pi->first_qset + j, -1,
-						    -1, i);
+						    pi->first_qset + j, -1, -1,
+						    pi->tx_chan);
 			if (ret)
 				err = ret;
 		}
@@ -989,8 +1391,28 @@
 	return err;
 }
 
+static void t3_release_firmware(const struct firmware *fw)
+{
+	if (!t3_local_firmware_free(fw))
+		return;
+	release_firmware(fw);
+}	
+
+static int t3_request_firmware(const struct firmware **firmware, const char *name,
+                 struct device *dev)
+{
+	/* first check if there is firmware on the filesystem */
+	if (!request_firmware(firmware, name, dev)) {
+		return 0;
+	}
+
+	return t3_local_firmware_load(firmware, name);
+}
+
+#if !defined(LINUX_2_4)
 #define FW_FNAME "cxgb3/t3fw-%d.%d.%d.bin"
-#define TPSRAM_NAME "cxgb3/t3%c_psram-%d.%d.%d.bin"
+#define TPEEPROM_NAME "cxgb3/t3%c_tp_eeprom-%d.%d.%d.bin"
+#define TPSRAM_NAME "cxgb3/t3%c_protocol_sram-%d.%d.%d.bin"
 #define AEL2005_OPT_EDC_NAME "cxgb3/ael2005_opt_edc.bin"
 #define AEL2005_TWX_EDC_NAME "cxgb3/ael2005_twx_edc.bin"
 #define AEL2020_TWX_EDC_NAME "cxgb3/ael2020_twx_edc.bin"
@@ -1013,7 +1435,17 @@
 	return fw_name;
 }
 
-int t3_get_edc_fw(struct cphy *phy, int edc_idx, int size)
+/**
+ *	t3_get_edc_fw - load specified PHY EDC code
+ *	@phy: pointer to PHY state (and associated EDC cache)
+ *	@edc_idx: ID of EDC code to load
+ *
+ *	Load the PHY Electronic Dispersion Control (EDC) Firmware indicated
+ *	by edc_index into the PHY's EDC Cache.  If no errors occur, then the
+ *	EDC size (in bytes) will be returned.  Otherwise a standard negative
+ *	error number with be returned.
+ */
+int t3_get_edc_fw(struct cphy *phy, int edc_idx)
 {
 	struct adapter *adapter = phy->adapter;
 	const struct firmware *fw;
@@ -1025,7 +1457,7 @@
 
 	snprintf(buf, sizeof(buf), get_edc_fw_name(edc_idx));
 
-	ret = request_firmware(&fw, buf, &adapter->pdev->dev);
+	ret = t3_request_firmware(&fw, buf, &adapter->pdev->dev);
 	if (ret < 0) {
 		dev_err(&adapter->pdev->dev,
 			"could not upgrade firmware: unable to load %s\n",
@@ -1034,9 +1466,10 @@
 	}
 
 	/* check size, take checksum in account */
-	if (fw->size > size + 4) {
-		CH_ERR(adapter, "firmware image too large %u, expected %d\n",
-		       (unsigned int)fw->size, size + 4);
+	if (fw->size > sizeof(phy->phy_cache) + 4) {
+		CH_ERR(adapter, "firmware image too large %u, max supported %u\n",
+		       (unsigned int)fw->size-4,
+		       (unsigned int)sizeof(phy->phy_cache));
 		ret = -EINVAL;
 	}
 
@@ -1047,16 +1480,17 @@
 
 	if (csum != 0xffffffff) {
 		CH_ERR(adapter, "corrupted firmware image, checksum %u\n",
-		       csum);
+			csum);
 		ret = -EINVAL;
 	}
 
-	for (i = 0; i < size / 4 ; i++) {
+	for (i = 0; i < fw->size / 4 ; i++) {
 		*cache++ = (be32_to_cpu(p[i]) & 0xffff0000) >> 16;
 		*cache++ = be32_to_cpu(p[i]) & 0xffff;
 	}
-
-	release_firmware(fw);
+	ret = fw->size;
+
+	t3_release_firmware(fw);
 
 	return ret;
 }
@@ -1080,8 +1514,8 @@
 	release_firmware(fw);
 
 	if (ret == 0)
-		dev_info(dev, "successful upgrade to firmware %d.%d.%d\n",
-			 FW_VERSION_MAJOR, FW_VERSION_MINOR, FW_VERSION_MICRO);
+		dev_warn(dev, "successful upgrade to firmware %d.%d.%d\n",
+			FW_VERSION_MAJOR, FW_VERSION_MINOR, FW_VERSION_MICRO);
 	else
 		dev_err(dev, "failed to upgrade to firmware %d.%d.%d\n",
 			FW_VERSION_MAJOR, FW_VERSION_MINOR, FW_VERSION_MICRO);
@@ -1089,11 +1523,17 @@
 	return ret;
 }
 
+static int set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom,
+		      u8 *data);
+
 static inline char t3rev2char(struct adapter *adapter)
 {
-	char rev = 0;
+	char rev = 'z';
 
 	switch(adapter->params.rev) {
+	case T3_REV_A:
+		rev = 'a';
+		break;
 	case T3_REV_B:
 	case T3_REV_B2:
 		rev = 'b';
@@ -1110,15 +1550,24 @@
 	const struct firmware *tpsram;
 	char buf[64];
 	struct device *dev = &adap->pdev->dev;
-	int ret;
+	int ret, major, minor, micro;
 	char rev;
 
 	rev = t3rev2char(adap);
 	if (!rev)
 		return 0;
 
-	snprintf(buf, sizeof(buf), TPSRAM_NAME, rev,
-		 TP_VERSION_MAJOR, TP_VERSION_MINOR, TP_VERSION_MICRO);
+	if (rev == 'c') {
+		major = TP_VERSION_MAJOR;
+		minor = TP_VERSION_MINOR;
+		micro = TP_VERSION_MICRO;
+	} else {
+		major = TP_VERSION_MAJOR_T3B;
+		minor = TP_VERSION_MINOR_T3B;
+		micro = TP_VERSION_MICRO_T3B;
+	}
+
+	snprintf(buf, sizeof(buf), TPSRAM_NAME, rev, major, minor, micro);
 
 	ret = request_firmware(&tpsram, buf, dev);
 	if (ret < 0) {
@@ -1133,13 +1582,12 @@
 
 	ret = t3_set_proto_sram(adap, tpsram->data);
 	if (ret == 0)
-		dev_info(dev,
+		dev_warn(dev,
 			 "successful update of protocol engine "
-			 "to %d.%d.%d\n",
-			 TP_VERSION_MAJOR, TP_VERSION_MINOR, TP_VERSION_MICRO);
+			 "to %d.%d.%d\n", major, minor, micro);
 	else
 		dev_err(dev, "failed to update of protocol engine %d.%d.%d\n",
-			TP_VERSION_MAJOR, TP_VERSION_MINOR, TP_VERSION_MICRO);
+			major, minor, micro);
 	if (ret)
 		dev_err(dev, "loading protocol SRAM failed\n");
 
@@ -1148,10 +1596,96 @@
 
 	return ret;
 }
+#endif /* ! LINUX_2_4 */
+
+static inline int is_in_filter_mode(struct adapter *adapter)
+{
+	return adapter->params.mc5.nfilters;
+}
+
+static void kick_watchdog_timer(unsigned long data)
+{
+	struct adapter *adap = (struct adapter *)data;
+	u32 glbtimer;
+
+	t3_cim_hac_read(adap, (A_CIM_CTL_BASE + A_CIM_CTL_GLB_TIMER),
+			&glbtimer);
+	t3_cim_hac_write(adap, (A_CIM_CTL_BASE + A_CIM_CTL_TIMER0),
+			(glbtimer + (10 * adap->params.vpd.cclk * 1000)));
+	setup_timer(&adap->watchdog_timer, kick_watchdog_timer,
+			(unsigned long)adap);
+	mod_timer(&adap->watchdog_timer, jiffies + msecs_to_jiffies(1000));
+}
+
+/*
+ *	fw_supports_watchdog - Check whether the firmware supports watchdog
+ *	feature or not.
+ *	@adap - Adapter whose firmware is to be checked for watchdog
+ *	feature support.
+ *
+ *	Read adapter firmware version to find out whether it supports
+ *	watchdog feature or not. T3 firmware v7.12.0 onwards includes
+ *	support for watchdogs.
+ */
+static int fw_supports_watchdog(struct adapter *adap)
+{
+	u32 fw_ver;
+	t3_get_fw_version(adap, &fw_ver);
+
+	return fw_ver >= (G_FW_VERSION_MAJOR(7) | G_FW_VERSION_MINOR(12));
+}
+
+static int setup_watchdog(struct adapter *adap, int wd, __u16 en,
+		__u16 ac, __u32 tval)
+{
+	u32 val;
+	__u16 action = 0;
+
+	/*
+	 * Check whether firmware supports watchdogs.
+	 */
+	if (!fw_supports_watchdog(adap)) {
+		printk(KERN_INFO "cxgb3: Firmware doesn't support watchdogs\n");
+		return 1;
+	}
+
+	if (wd == FW_MNGTOPCODE_DRIVERWATCHDOG) {
+		if ((ac == 0) || (ac == 1))
+			action = GPIO_EN_0;
+		else if (ac == 2)
+			action = PCIE_LINKDOWN;
+		else if (ac == 3)
+			action = FW_EXCEPTION;
+		else {
+			printk(KERN_WARNING "cxgb3: Invalid action for driver "
+					"watchdog\n");
+			return 1;
+		}
+
+		t3_cim_hac_read(adap, (A_CIM_CTL_BASE + A_CIM_CTL_GLB_TIMER),
+				&val);
+		t3_cim_hac_write(adap, (A_CIM_CTL_BASE + A_CIM_CTL_TIMER0),
+				(val + (10 * adap->params.vpd.cclk * 1000)));
+		setup_timer(&adap->watchdog_timer, kick_watchdog_timer,
+				(unsigned long)adap);
+		mod_timer(&adap->watchdog_timer, 
+				jiffies + msecs_to_jiffies(1000));
+	} else if (wd == FW_MNGTOPCODE_FIRMWAREWATCHDOG) {
+		val = t3_read_reg(adap, A_CIM_HOST_INT_ENABLE);
+		val = val | (1 << 15);
+		t3_write_reg(adap, A_CIM_HOST_INT_ENABLE, val);
+
+		val = t3_read_reg(adap, A_CIM_HOST_INT_ENABLE);
+		val = val & ~(1 << 15);
+		t3_write_reg(adap, A_CIM_HOST_INT_CAUSE, val);
+	}
+
+	return send_watchdog_cmd(adap, wd, en, action, tval);
+}
 
 /**
  *	cxgb_up - enable the adapter
- *	@adapter: adapter being enabled
+ *	@adap: adapter being enabled
  *
  *	Called when the first port is enabled, this function performs the
  *	actions necessary to make an adapter operational, such as completing
@@ -1161,25 +1695,47 @@
  */
 static int cxgb_up(struct adapter *adap)
 {
-	int err;
+	int err = 0;
 
 	if (!(adap->flags & FULL_INIT_DONE)) {
 		err = t3_check_fw_version(adap);
+#if !defined(LINUX_2_4)
 		if (err == -EINVAL) {
 			err = upgrade_fw(adap);
 			CH_WARN(adap, "FW upgrade to %d.%d.%d %s\n",
 				FW_VERSION_MAJOR, FW_VERSION_MINOR,
 				FW_VERSION_MICRO, err ? "failed" : "succeeded");
 		}
+#endif
 
 		err = t3_check_tpsram_version(adap);
+#if !defined(LINUX_2_4)
 		if (err == -EINVAL) {
 			err = update_tpsram(adap);
-			CH_WARN(adap, "TP upgrade to %d.%d.%d %s\n",
-				TP_VERSION_MAJOR, TP_VERSION_MINOR,
-				TP_VERSION_MICRO, err ? "failed" : "succeeded");
+			if (adap->params.rev == T3_REV_C) {
+				CH_WARN(adap, "TP upgrade to %d.%d.%d %s\n",
+					TP_VERSION_MAJOR, TP_VERSION_MINOR,
+					TP_VERSION_MICRO,
+					err ? "failed" :"succeeded");
+			} else {
+				CH_WARN(adap, "TP upgrade to %d.%d.%d %s\n",
+					TP_VERSION_MAJOR_T3B,
+					TP_VERSION_MINOR_T3B,
+					TP_VERSION_MICRO_T3B,
+					err ? "failed" : "succeeded");
+			}
 		}
-
+#endif
+
+		/* PR 6487. TOE and filtering are mutually exclusive */
+		cxgb3_filter_toe_mode(adap, is_in_filter_mode(adap) ?
+				      CXGB3_FTM_FILTER : CXGB3_FTM_TOE);
+
+#if !defined(NAPI_UPDATE)
+		err = init_dummy_netdevs(adap);
+		if (err)
+			goto out;
+#endif
 		/*
 		 * Clear interrupts now to catch errors if t3_init_hw fails.
 		 * We clear them again later as initialization may trigger
@@ -1192,17 +1748,46 @@
 			goto out;
 
 		t3_set_reg_field(adap, A_TP_PARA_REG5, 0, F_RXDDPOFFINIT);
+
+/* T3 has a lookup table for 4 different page sizes with the default 4K page size using HPZ0
+ * Use the upper register HPZ3 for TCP DPP to support huge pages
+ */
+
+#if defined(CONFIG_T3_ZCOPY_HUGEPAGES) && defined(CONFIG_HUGETLB_PAGE)
+#define T3_HPAGE_SHIFT (HPAGE_SHIFT > 27 ? 27 : HPAGE_SHIFT)
+		t3_write_reg(adap, A_ULPRX_TDDP_PSZ,
+			     V_HPZ0(PAGE_SHIFT - 12) |
+			     V_HPZ3(T3_HPAGE_SHIFT - 12));
+#else
 		t3_write_reg(adap, A_ULPRX_TDDP_PSZ, V_HPZ0(PAGE_SHIFT - 12));
+#endif
 
 		err = setup_sge_qsets(adap);
 		if (err)
 			goto out;
 
+		alloc_filters(adap);
 		setup_rss(adap);
+#if defined(NAPI_UPDATE)
 		if (!(adap->flags & NAPI_INIT))
 			init_napi(adap);
-
+#endif
 		t3_start_sge_timers(adap);
+		
+		if (drv_wd_en) {
+			if (setup_watchdog(adap, FW_MNGTOPCODE_DRIVERWATCHDOG,
+						1, drv_wd_ac, 0))
+				printk(KERN_WARNING "cxgb3: Failure to setup "
+						"driver watchdog\n");
+		}
+
+		if (fw_wd_en) {
+			if (setup_watchdog(adap, FW_MNGTOPCODE_FIRMWAREWATCHDOG, 1,
+					0, (10 * adap->params.vpd.cclk * 1000)))
+				printk(KERN_WARNING "cxgb3: Failure to setup "
+						"firmware watchdog");
+		}
+
 		adap->flags |= FULL_INIT_DONE;
 	}
 
@@ -1222,27 +1807,32 @@
 			goto irq_err;
 		}
 	} else if ((err = request_irq(adap->pdev->irq,
-				      t3_intr_handler(adap,
-						      adap->sge.qs[0].rspq.
-						      polling),
-				      (adap->flags & USING_MSI) ?
-				       0 : IRQF_SHARED,
-				      adap->name, adap)))
+				t3_intr_handler(adap,
+						adap->sge.qs[0].rspq.flags & USING_POLLING),
+				(adap->flags & USING_MSI) ? 0 : IRQF_SHARED,
+				adap->name, adap)))
 		goto irq_err;
 
 	enable_all_napi(adap);
 	t3_sge_start(adap);
 	t3_intr_enable(adap);
 
-	if (adap->params.rev >= T3_REV_C && !(adap->flags & TP_PARITY_INIT) &&
-	    is_offload(adap) && init_tp_parity(adap) == 0)
-		adap->flags |= TP_PARITY_INIT;
+	if (adap->params.rev >= T3_REV_C && is_offload(adap) &&
+	    !(adap->flags & TP_PARITY_INIT)) {
+		t3_set_reg_field(adap, A_PCIE_CFG,
+			F_PCIE_DMASTOPEN, V_PCIE_DMASTOPEN(0));
+
+		if (init_tp_parity(adap) == 0)
+			adap->flags |= TP_PARITY_INIT;
+	}
 
 	if (adap->flags & TP_PARITY_INIT) {
 		t3_write_reg(adap, A_TP_INT_CAUSE,
-			     F_CMCACHEPERR | F_ARPLUTPERR);
+				F_CMCACHEPERR | F_ARPLUTPERR);
 		t3_write_reg(adap, A_TP_INT_ENABLE, 0x7fbfffff);
 	}
+	t3_set_reg_field(adap, A_PCIE_CFG,
+			F_PCIE_DMASTOPEN, F_PCIE_DMASTOPEN);
 
 	if (!(adap->flags & QUEUES_BOUND)) {
 		err = bind_qsets(adap);
@@ -1252,9 +1842,9 @@
 			free_irq_resources(adap);
 			goto out;
 		}
+		setup_hw_filters(adap);
 		adap->flags |= QUEUES_BOUND;
 	}
-
 out:
 	return err;
 irq_err:
@@ -1265,17 +1855,21 @@
 /*
  * Release resources when all the ports and offloading have been stopped.
  */
-static void cxgb_down(struct adapter *adapter)
-{
+static void cxgb_down(struct adapter *adapter, int on_wq)
+{
+	unsigned long flags;
+
 	t3_sge_stop(adapter);
-	spin_lock_irq(&adapter->work_lock);	/* sync with PHY intr task */
+
+	/* sync with PHY intr task */
+	spin_lock_irqsave(&adapter->work_lock, flags);
 	t3_intr_disable(adapter);
-	spin_unlock_irq(&adapter->work_lock);
+	spin_unlock_irqrestore(&adapter->work_lock, flags);
 
 	free_irq_resources(adapter);
+	if (!on_wq)
+		flush_workqueue(cxgb3_wq);/* wait for external IRQ handler */
 	quiesce_rx(adapter);
-	t3_sge_stop(adapter);
-	flush_workqueue(cxgb3_wq);	/* wait for external IRQ handler */
 }
 
 static void schedule_chk_task(struct adapter *adap)
@@ -1283,8 +1877,8 @@
 	unsigned int timeo;
 
 	timeo = adap->params.linkpoll_period ?
-	    (HZ * adap->params.linkpoll_period) / 10 :
-	    adap->params.stats_update_period * HZ;
+		(HZ * adap->params.linkpoll_period) / 10 :
+		adap->params.stats_update_period * HZ;
 	if (timeo)
 		queue_delayed_work(cxgb3_wq, &adap->adap_check_task, timeo);
 }
@@ -1295,11 +1889,19 @@
 	struct adapter *adapter = pi->adapter;
 	struct t3cdev *tdev = dev2t3cdev(dev);
 	int adap_up = adapter->open_device_map & PORT_MASK;
-	int err;
+	int err = 0;
 
 	if (test_and_set_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map))
 		return 0;
 
+	/* PR 6487. Filtering and TOE mutually exclusive */
+	if (!cxgb3_filter_toe_mode(adapter, CXGB3_FTM_TOE)) {
+		printk(KERN_WARNING
+		       "%s: filtering on. Offload disabled\n", dev->name);
+		err = -1;
+		goto out;
+	}
+
 	if (!adap_up && (err = cxgb_up(adapter)) < 0)
 		goto out;
 
@@ -1313,11 +1915,16 @@
 	t3_load_mtus(adapter, adapter->params.mtus, adapter->params.a_wnd,
 		     adapter->params.b_wnd,
 		     adapter->params.rev == 0 ?
-		     adapter->port[0]->mtu : 0xffff);
+		       adapter->port[0]->mtu : 0xffff);
 	init_smt(adapter);
 
-	if (sysfs_create_group(&tdev->lldev->dev.kobj, &offload_attr_group))
-		dev_dbg(&dev->dev, "cannot create sysfs group\n");
+#ifndef	LINUX_2_4
+	/* Never mind if the next step fails */
+	if (sysfs_create_group(net2kobj(tdev->lldev), &offload_attr_group))
+		printk(KERN_INFO
+		       "%s: cannot create sysfs offload_attr_group\n",
+		       dev->name);
+#endif	/* LINUX_2_4 */
 
 	/* Call back all registered clients */
 	cxgb3_add_clients(tdev);
@@ -1341,19 +1948,16 @@
 
 	/* Call back all registered clients */
 	cxgb3_remove_clients(tdev);
-
-	sysfs_remove_group(&tdev->lldev->dev.kobj, &offload_attr_group);
-
-	/* Flush work scheduled while releasing TIDs */
-	flush_scheduled_work();
-
+#ifndef LINUX_2_4
+	sysfs_remove_group(net2kobj(tdev->lldev), &offload_attr_group);
+#endif
 	tdev->lldev = NULL;
 	cxgb3_set_dummy_ops(tdev);
 	t3_tp_set_offload_mode(adapter, 0);
 	clear_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map);
 
 	if (!adapter->open_device_map)
-		cxgb_down(adapter);
+		cxgb_down(adapter, 0);
 
 	cxgb3_offload_deactivate(adapter);
 	return 0;
@@ -1370,67 +1974,98 @@
 		return err;
 
 	set_bit(pi->port_id, &adapter->open_device_map);
+
 	if (is_offload(adapter) && !ofld_disable) {
 		err = offload_open(dev);
 		if (err)
 			printk(KERN_WARNING
 			       "Could not initialize offload capabilities\n");
+#ifndef LINUX_2_4
+		if (sysfs_create_group(net2kobj(dev), &iscsi_offload_attr_group))
+			printk(KERN_INFO
+			       "%s: cannot create sysfs iscsi_offload_attr_group\n",
+			       dev->name);
+#endif
 	}
 
-	dev->real_num_tx_queues = pi->nqsets;
+	t3_compat_set_num_tx_queues(dev, pi->nqsets);
 	link_start(dev);
 	t3_port_intr_enable(adapter, pi->port_id);
 	netif_tx_start_all_queues(dev);
 	if (!other_ports)
 		schedule_chk_task(adapter);
 
-	cxgb3_event_notify(&adapter->tdev, OFFLOAD_PORT_UP, pi->port_id);
 	return 0;
 }
 
-static int cxgb_close(struct net_device *dev)
+static int __cxgb_close(struct net_device *dev, int on_wq)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
 
-	
-	if (!adapter->open_device_map)
-		return 0;
-
 	/* Stop link fault interrupts */
 	t3_xgm_intr_disable(adapter, pi->port_id);
 	t3_read_reg(adapter, A_XGM_INT_STATUS + pi->mac.offset);
 
 	t3_port_intr_disable(adapter, pi->port_id);
 	netif_tx_stop_all_queues(dev);
-	pi->phy.ops->power_down(&pi->phy, 1);
 	netif_carrier_off(dev);
-	t3_mac_disable(&pi->mac, MAC_DIRECTION_TX | MAC_DIRECTION_RX);
+
+	/* disable pause frames */
+	t3_set_reg_field(adapter, A_XGM_TX_CFG + pi->mac.offset,
+			 F_TXPAUSEEN, 0);
+
+	/* Reset RX FIFO HWM */
+        t3_set_reg_field(adapter, A_XGM_RXFIFO_CFG +  pi->mac.offset,
+			 V_RXFIFOPAUSEHWM(M_RXFIFOPAUSEHWM), 0);
+
+#ifndef LINUX_2_4
+	if (is_offload(adapter) && !ofld_disable)
+		sysfs_remove_group(net2kobj(dev), &iscsi_offload_attr_group);
+#endif
 
 	spin_lock_irq(&adapter->work_lock);	/* sync with update task */
 	clear_bit(pi->port_id, &adapter->open_device_map);
 	spin_unlock_irq(&adapter->work_lock);
 
 	if (!(adapter->open_device_map & PORT_MASK))
-		cancel_delayed_work_sync(&adapter->adap_check_task);
+		cancel_rearming_delayed_workqueue(cxgb3_wq,
+						  &adapter->adap_check_task);
 
 	if (!adapter->open_device_map)
-		cxgb_down(adapter);
-
-	cxgb3_event_notify(&adapter->tdev, OFFLOAD_PORT_DOWN, pi->port_id);
+		cxgb_down(adapter, on_wq);
+
+	msleep(100);
+
+	/* Wait for TXFIFO empty */
+	t3_wait_op_done(adapter, A_XGM_TXFIFO_CFG + pi->mac.offset,
+			F_TXFIFO_EMPTY, 1, 20, 5);
+
+	msleep(100);
+	t3_mac_disable(&pi->mac, MAC_DIRECTION_RX);
+
+	pi->phy.ops->power_down(&pi->phy, 1);
+
 	return 0;
 }
 
+static int cxgb_close(struct net_device *dev)
+{
+	return __cxgb_close(dev, 0);
+}
+
 static struct net_device_stats *cxgb_get_stats(struct net_device *dev)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
 	struct net_device_stats *ns = &pi->netstats;
-	const struct mac_stats *pstats;
-
-	spin_lock(&adapter->stats_lock);
-	pstats = t3_mac_update_stats(&pi->mac);
-	spin_unlock(&adapter->stats_lock);
+	const struct mac_stats *pstats = &pi->mac.stats;
+
+	if (adapter->flags & FULL_INIT_DONE) {
+		spin_lock(&adapter->stats_lock);
+		t3_mac_update_stats(&pi->mac);
+		spin_unlock(&adapter->stats_lock);
+	}
 
 	ns->tx_bytes = pstats->tx_octets;
 	ns->tx_packets = pstats->tx_frames;
@@ -1440,8 +2075,8 @@
 
 	ns->tx_errors = pstats->tx_underrun;
 	ns->rx_errors = pstats->rx_symbol_errs + pstats->rx_fcs_errs +
-	    pstats->rx_too_long + pstats->rx_jabber + pstats->rx_short +
-	    pstats->rx_fifo_ovfl;
+		pstats->rx_too_long + pstats->rx_jabber + pstats->rx_short +
+		pstats->rx_fifo_ovfl;
 
 	/* detailed rx_errors */
 	ns->rx_length_errors = pstats->rx_jabber + pstats->rx_too_long;
@@ -1476,6 +2111,14 @@
 	adapter->msg_enable = val;
 }
 
+static const char test_strings[][ETH_GSTRING_LEN] = {
+	"Register test         (offline)",
+	"Interrupt test        (offline)",
+	"PMA/PMD loopback test (offline)",
+	"PCS loopback test     (offline)",
+	"Link test             (online)"
+};
+
 static char stats_strings[][ETH_GSTRING_LEN] = {
 	"TxOctetsOK         ",
 	"TxFramesOK         ",
@@ -1518,11 +2161,14 @@
 	"VLANextractions    ",
 	"VLANinsertions     ",
 	"TxCsumOffload      ",
+	"TXCoalesceWR       ",
+	"TXCoalescePkt      ",
 	"RxCsumGood         ",
-	"LroAggregated      ",
+	"RxDrops            ",
+
+	"LroQueued          ",
 	"LroFlushed         ",
-	"LroNoDesc          ",
-	"RxDrops            ",
+	"LroExceededSessions",
 
 	"CheckTXEnToggled   ",
 	"CheckResets        ",
@@ -1530,15 +2176,29 @@
 	"LinkFaults         ",
 };
 
-static int get_sset_count(struct net_device *dev, int sset)
+#if defined(GET_STATS_COUNT)
+static int get_stats_count(struct net_device *dev)
+{
+	return ARRAY_SIZE(stats_strings);
+}
+
+static int self_test_count(struct net_device *dev)
+{
+	return ARRAY_SIZE(test_strings);
+}
+#else
+static int get_sset_count(struct net_device *netdev, int sset)
 {
 	switch (sset) {
+	case ETH_SS_TEST:
+		return ARRAY_SIZE(test_strings);
 	case ETH_SS_STATS:
 		return ARRAY_SIZE(stats_strings);
 	default:
 		return -EOPNOTSUPP;
 	}
 }
+#endif
 
 #define T3_REGMAP_SIZE (3 * 1024)
 
@@ -1547,25 +2207,26 @@
 	return T3_REGMAP_SIZE;
 }
 
+#ifndef	LINUX_2_4
 static int get_eeprom_len(struct net_device *dev)
 {
 	return EEPROMSIZE;
 }
+#endif	/* LINUX_2_4 */
 
 static void get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
-	u32 fw_vers = 0;
-	u32 tp_vers = 0;
+	u32 fw_vers = 0, tp_vers = 0;
 
 	spin_lock(&adapter->stats_lock);
 	t3_get_fw_version(adapter, &fw_vers);
 	t3_get_tp_version(adapter, &tp_vers);
 	spin_unlock(&adapter->stats_lock);
 
-	strcpy(info->driver, DRV_NAME);
-	strcpy(info->version, DRV_VERSION);
+	strcpy(info->driver, DRIVER_NAME);
+	strcpy(info->version, DRIVER_VERSION);
 	strcpy(info->bus_info, pci_name(adapter->pdev));
 	if (!fw_vers)
 		strcpy(info->fw_version, "N/A");
@@ -1582,10 +2243,12 @@
 	}
 }
 
-static void get_strings(struct net_device *dev, u32 stringset, u8 * data)
+static void get_strings(struct net_device *dev, u32 stringset, u8 *data)
 {
 	if (stringset == ETH_SS_STATS)
 		memcpy(data, stats_strings, sizeof(stats_strings));
+	else
+		memcpy(data, test_strings, sizeof(test_strings));
 }
 
 static unsigned long collect_sge_port_stats(struct adapter *adapter,
@@ -1599,16 +2262,27 @@
 	return tot;
 }
 
+static void clear_sge_port_stats(struct adapter *adapter, struct port_info *p)
+{
+	int i;
+	struct sge_qset *qs = &adapter->sge.qs[p->first_qset];
+
+	for (i = 0; i < p->nqsets; i++, qs++)
+		memset(qs->port_stats, 0, sizeof(qs->port_stats));
+}
+
 static void get_stats(struct net_device *dev, struct ethtool_stats *stats,
 		      u64 *data)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
-	const struct mac_stats *s;
-
-	spin_lock(&adapter->stats_lock);
-	s = t3_mac_update_stats(&pi->mac);
-	spin_unlock(&adapter->stats_lock);
+	const struct mac_stats *s = &pi->mac.stats;
+
+	if (adapter->flags & FULL_INIT_DONE) {
+		spin_lock(&adapter->stats_lock);
+		t3_mac_update_stats(&pi->mac);
+		spin_unlock(&adapter->stats_lock);
+	}
 
 	*data++ = s->tx_octets;
 	*data++ = s->tx_frames;
@@ -1652,11 +2326,15 @@
 	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_VLANEX);
 	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_VLANINS);
 	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_TX_CSUM);
+	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_TX_COALESCE_WR);
+        *data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_TX_COALESCE_PKT);
 	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_RX_CSUM_GOOD);
-	*data++ = 0;
-	*data++ = 0;
-	*data++ = 0;
 	*data++ = s->rx_cong_drops;
+	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_LRO_SKB) +
+		  collect_sge_port_stats(adapter, pi, SGE_PSTAT_LRO_PG) +
+		  collect_sge_port_stats(adapter, pi, SGE_PSTAT_LRO_ACK);
+	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_LRO);
+	*data++ = collect_sge_port_stats(adapter, pi, SGE_PSTAT_LRO_OVFLOW);
 
 	*data++ = s->num_toggled;
 	*data++ = s->num_resets;
@@ -1664,12 +2342,311 @@
 	*data++ = s->link_faults;
 }
 
+static int test_regs(struct net_device *dev, struct ethtool_test *eth_test)
+{
+	struct port_info *pi = netdev_priv(dev);
+	int val;
+
+	CH_WARN(pi->adapter, "register self test\n");
+
+	t3_write_reg(pi->adapter, A_CIM_HOST_ACC_DATA, 0xdeadbeef);
+
+	val = t3_read_reg(pi->adapter, A_CIM_HOST_ACC_DATA);
+	return val == 0xdeadbeef ? 0 : -1;
+}
+
+/*
+ * Interrupt handler used to check if MSI/MSI-X works on this platform.
+ */
+DECLARE_INTR_HANDLER(check_intr_handler, irq, adap, regs)
+{
+	t3_set_reg_field(adap, A_PL_INT_ENABLE0, F_MI1, 0);
+	return IRQ_HANDLED;
+}
+
+static int check_intr(struct adapter *adap)
+{
+	int mi1, ret;
+
+	ret = t3_read_reg(adap, A_PL_INT_CAUSE0) & F_MI1;
+	if (!ret)
+		return !ret;
+
+	free_irq(adap->pdev->irq, adap);
+	ret = request_irq(adap->pdev->irq, check_intr_handler,
+			  IRQF_SHARED, adap->name, adap);
+	if (ret)
+		return ret;
+
+	t3_set_reg_field(adap, A_PL_INT_ENABLE0, 0, F_MI1);
+	msleep(100);
+	mi1 = t3_read_reg(adap, A_PL_INT_ENABLE0) & F_MI1;
+	if (mi1)
+		t3_set_reg_field(adap, A_PL_INT_ENABLE0, F_MI1, 0);
+	free_irq(adap->pdev->irq, adap);
+	ret = request_irq(adap->pdev->irq,
+			  t3_intr_handler(adap,
+					  adap->sge.qs[0].rspq.flags &
+					  USING_POLLING),
+			  IRQF_SHARED,
+			  adap->name, adap);
+	if (ret)
+		return ret;
+	
+	return mi1;
+}
+
+static int test_intr(struct net_device *dev, struct ethtool_test *eth_test)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct adapter *adapter = pi->adapter;
+	int ret = 0;
+
+	CH_WARN(adapter, "interrupt self test\n");
+	
+	/*
+	 * The driver checks MSI/MSI-X interrupts at load time.
+	 * Take care of line intrs only here.
+	 */
+	
+	if (!(adapter->flags & (USING_MSIX | USING_MSI)))
+		ret = check_intr(adapter);
+
+	if (ret)
+		CH_ERR(adapter, "Interrupt test failed\n");
+		
+	return ret;
+}
+
+static struct sk_buff * loopback_fill(struct net_device *dev)
+{
+	struct sk_buff *skb;
+	struct ethhdr *ethhdr;
+	const char str[] = "cxgb3 loopback test";
+	char *p;
+	int len;
+
+	len = sizeof(*ethhdr) + sizeof(str);
+	skb = alloc_skb(len, GFP_KERNEL);
+	if (!skb)
+		return NULL;
+
+	skb_reset_mac_header(skb);
+	ethhdr = (struct ethhdr *)skb_mac_header(skb);
+	skb_put(skb, sizeof(*ethhdr));
+
+	memcpy(ethhdr->h_dest, dev->dev_addr, ETH_ALEN);
+	memcpy(ethhdr->h_source, dev->dev_addr, ETH_ALEN);
+	ethhdr->h_proto = htons(ETH_P_LOOP);
+
+	skb_put(skb, sizeof(str));
+	p = (char *)(ethhdr + 1);
+	memcpy(p, str, sizeof(str));
+
+	return skb;
+}
+
+static int get_rx_eth_pkts(struct port_info *pi)
+{
+	struct sge_qset *qs = pi->qs;
+	int i, n = 0;
+
+	for (i = pi->first_qset; i < pi->nqsets; i++) {
+		n += qs->rspq.eth_pkts;
+		qs++;
+	}
+
+	return n;
+}
+
+typedef void (*t3_set_loopback_t)(struct port_info *);
+typedef void (*t3_reset_loopback_t)(struct port_info *);
+
+static int test_loopback(struct net_device *dev, int loopback_mode,
+			 t3_set_loopback_t set_loopback,
+			 t3_reset_loopback_t reset_loopback)
+{
+	struct port_info *pi = netdev_priv(dev);
+	struct cmac *mac = &pi->mac;
+	struct adapter *adapter = pi->adapter;
+	struct sk_buff *skb;
+	int i, npkts = 10, rx_before, rx_after;
+	u32 rx_cfg, rx_hash_high, rx_hash_low;
+	
+	
+	if (!test_bit(pi->port_id, &adapter->open_device_map)) {
+		CH_WARN(adapter, "Port not fully initialized, "
+			"loopback test will fail\n");
+		return -ENOTSUPP;
+	}
+
+	pi->loopback = loopback_mode;
+
+	/* set the phy in loopback mode */
+	netif_carrier_off(dev);
+	
+	/* Filter Rx traffic */
+	t3_gate_rx_traffic(mac, &rx_cfg, &rx_hash_high, &rx_hash_low);
+	t3_mac_enable_exact_filters(mac);
+	
+	set_loopback(pi);
+	msleep(100);
+
+	rx_before = get_rx_eth_pkts(pi);
+	
+	skb = loopback_fill(dev);
+	for (i = 0; i < npkts; i++) {
+		skb_get(skb);
+		t3_eth_xmit(skb, dev);
+	}
+
+	msleep(100);
+	rx_after = get_rx_eth_pkts(pi);
+
+	/* Reset loopback mode */
+	reset_loopback(pi);
+	msleep(100);
+
+	t3_open_rx_traffic(mac, rx_cfg, rx_hash_high, rx_hash_low);
+	
+	pi->loopback = LOOPBACK_NONE;
+
+	/* Re-start the link */
+	link_start(dev);
+
+	return rx_after - rx_before != npkts;
+}
+
+/* Set spcified MMD block in loopback mode */
+static void t3_set_loopback_mmd(struct port_info *pi,
+				int mmd_addr, int reg_addr, u32 bit)
+{
+	struct cphy *phy = &pi->phy;
+	u32 val;
+	
+	/* reset the phy */
+	phy->ops->reset(phy, 0);
+	mdio_read(phy, mmd_addr, reg_addr, &val);
+	val |= (1 << bit);
+	mdio_write(phy, mmd_addr, reg_addr, val);
+}	
+	
+/* Set spcified MMD block out of loopback mode */
+static void t3_reset_loopback_mmd(struct port_info *pi,
+				  int mmd_addr, int reg_addr, u32 bit)
+{
+	struct cphy *phy = &pi->phy;
+	u32 val;
+	
+	mdio_read(phy, mmd_addr, reg_addr, &val);
+	val &= ~(1 << bit);
+	mdio_write(phy, mmd_addr, reg_addr, val);
+	phy->ops->reset(phy, 0);
+}	
+
+static void t3_set_loopback_pma_pmd(struct port_info *pi)
+{
+	t3_set_loopback_mmd(pi, MDIO_DEV_PMA_PMD, 0, 0);	
+}	
+
+static void t3_reset_loopback_pma_pmd(struct port_info *pi)
+{
+	t3_reset_loopback_mmd(pi, MDIO_DEV_PMA_PMD, 0, 0);	
+}
+
+static int test_loopback_pma_pmd(struct net_device *dev,
+				 struct ethtool_test *eth_test)
+{
+	struct port_info *pi = netdev_priv(dev);
+
+	CH_WARN(pi->adapter, "PHY PMA/PMD loopback test\n");
+	return test_loopback(dev, LOOPBACK_PHY_PMA_PMD,
+			     t3_set_loopback_pma_pmd,
+			     t3_reset_loopback_pma_pmd);
+}	
+
+static void t3_set_loopback_pcs(struct port_info *pi)
+{
+	t3_set_loopback_mmd(pi, MDIO_DEV_PCS, 0, 14);	
+}	
+
+static void t3_reset_loopback_pcs(struct port_info *pi)
+{
+	t3_reset_loopback_mmd(pi, MDIO_DEV_PCS, 0, 14);	
+}	
+
+static int test_loopback_pcs(struct net_device *dev,
+			     struct ethtool_test *eth_test)
+{
+	struct port_info *pi = netdev_priv(dev);
+
+	CH_WARN(pi->adapter, "PHY PCS loopback test\n");
+	return test_loopback(dev, LOOPBACK_PHY_PCS,
+			     t3_set_loopback_pcs,
+			     t3_reset_loopback_pcs);
+}
+
+static int test_link(struct net_device *dev, struct ethtool_test *eth_test)
+{
+	struct port_info *pi = netdev_priv(dev);
+	int link_ok, speed, duplex, fc;
+	struct cphy *phy = &pi->phy;
+	struct link_config *lc = &pi->link_config;
+
+	CH_WARN(pi->adapter, "link self test\n");
+
+	link_ok = lc->link_ok;
+	speed = lc->speed;
+	duplex = lc->duplex;
+	fc = lc->fc;
+	phy->ops->get_link_status(phy, &link_ok, &speed, &duplex, &fc);
+
+	return !link_ok;
+}
+
+typedef int (*t3_diag_func)(struct net_device *dev,
+			    struct ethtool_test *eth_test);
+
+enum {
+	ONLINE = 0,
+	OFFLINE,
+};
+
+struct t3_test_info {
+	t3_diag_func test;
+	int type;
+};
+
+static void self_tests(struct net_device *dev, struct ethtool_test *eth_test,
+		       u64 *data)
+{
+	int offline = eth_test->flags & ETH_TEST_FL_OFFLINE, i;
+
+	static const struct t3_test_info diags_matrix[] = {
+		{ test_regs,			OFFLINE },
+ 		{ test_intr,			OFFLINE },
+		{ test_loopback_pma_pmd,	OFFLINE },
+		{ test_loopback_pcs,		OFFLINE },
+		{ test_link,			 ONLINE }
+	};
+	
+	for (i = 0; i < ARRAY_SIZE(diags_matrix); i++) {
+		const struct t3_test_info *info = diags_matrix + i;
+		int run = info->type == offline, val;
+
+		val = run ? info->test(dev, eth_test) : 0;
+		*data++ = val;
+		if (val)
+			eth_test->flags |= ETH_TEST_FL_FAILED;
+	}
+}
+
 static inline void reg_block_dump(struct adapter *ap, void *buf,
 				  unsigned int start, unsigned int end)
 {
 	u32 *p = buf + start;
 
-	for (; start <= end; start += sizeof(u32))
+	for ( ; start <= end; start += sizeof(u32))
 		*p++ = t3_read_reg(ap, start);
 }
 
@@ -1678,7 +2655,6 @@
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *ap = pi->adapter;
-
 	/*
 	 * Version scheme:
 	 * bits 0..9: chip version
@@ -1752,7 +2728,7 @@
 	}
 
 	cmd->port = (cmd->supported & SUPPORTED_TP) ? PORT_TP : PORT_FIBRE;
-	cmd->phy_address = p->phy.mdio.prtad;
+	cmd->phy_address = p->phy.addr;
 	cmd->transceiver = XCVR_EXTERNAL;
 	cmd->autoneg = p->link_config.autoneg;
 	cmd->maxtxpkt = 0;
@@ -1797,6 +2773,7 @@
 
 static int set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
 {
+	int cap;
 	struct port_info *p = netdev_priv(dev);
 	struct link_config *lc = &p->link_config;
 
@@ -1806,7 +2783,7 @@
 		 * being requested.
 		 */
 		if (cmd->autoneg == AUTONEG_DISABLE) {
-			int cap = speed_duplex_to_caps(cmd->speed, cmd->duplex);
+			cap = speed_duplex_to_caps(cmd->speed, cmd->duplex);
 			if (lc->supported & cap)
 				return 0;
 		}
@@ -1814,9 +2791,10 @@
 	}
 
 	if (cmd->autoneg == AUTONEG_DISABLE) {
-		int cap = speed_duplex_to_caps(cmd->speed, cmd->duplex);
-
-		if (!(lc->supported & cap) || cmd->speed == SPEED_1000)
+		cap = speed_duplex_to_caps(cmd->speed, cmd->duplex);
+
+		if (!(lc->supported & cap) || cmd->speed == SPEED_1000 ||
+		    cmd->speed == SPEED_10000)
 			return -EINVAL;
 		lc->requested_speed = cmd->speed;
 		lc->requested_duplex = cmd->duplex;
@@ -1878,29 +2856,30 @@
 {
 	struct port_info *p = netdev_priv(dev);
 
-	return p->rx_offload & T3_RX_CSUM;
+	return p->rx_csum_offload;
 }
 
 static int set_rx_csum(struct net_device *dev, u32 data)
 {
 	struct port_info *p = netdev_priv(dev);
 
-	if (data) {
-		p->rx_offload |= T3_RX_CSUM;
-	} else {
+	p->rx_csum_offload = data;
+	if (!data) {
+		struct adapter *adap = p->adapter;
 		int i;
 
-		p->rx_offload &= ~(T3_RX_CSUM | T3_LRO);
-		for (i = p->first_qset; i < p->first_qset + p->nqsets; i++)
-			set_qset_lro(dev, i, 0);
+		for (i = p->first_qset; i < p->first_qset + p->nqsets; i++) {
+			adap->params.sge.qset[i].lro = 0;
+			adap->sge.qs[i].lro.enabled = 0;
+		}
 	}
 	return 0;
 }
 
 static void get_sge_param(struct net_device *dev, struct ethtool_ringparam *e)
 {
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
+	const struct port_info *pi = netdev_priv(dev);
+	const struct adapter *adapter = pi->adapter;
 	const struct qset_params *q = &adapter->params.sge.qset[pi->first_qset];
 
 	e->rx_max_pending = MAX_RX_BUFFERS;
@@ -1927,7 +2906,7 @@
 	    e->rx_mini_pending > MAX_RSPQ_ENTRIES ||
 	    e->rx_mini_pending < MIN_RSPQ_ENTRIES ||
 	    e->rx_pending < MIN_FL_ENTRIES ||
-	    e->rx_jumbo_pending < MIN_FL_ENTRIES ||
+	    e->rx_jumbo_pending < MIN_FL_JUMBO_ENTRIES ||
 	    e->tx_pending < adapter->params.nports * MIN_TXQ_ENTRIES)
 		return -EINVAL;
 
@@ -1972,7 +2951,7 @@
 }
 
 static int get_eeprom(struct net_device *dev, struct ethtool_eeprom *e,
-		      u8 * data)
+		      u8 *data)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
@@ -1984,7 +2963,7 @@
 
 	e->magic = EEPROM_MAGIC;
 	for (i = e->offset & ~3; !err && i < e->offset + e->len; i += 4)
-		err = t3_seeprom_read(adapter, i, (__le32 *) & buf[i]);
+		err = t3_seeprom_read(adapter, i, (u32 *)&buf[i]);
 
 	if (!err)
 		memcpy(data, buf + e->offset, e->len);
@@ -1993,14 +2972,14 @@
 }
 
 static int set_eeprom(struct net_device *dev, struct ethtool_eeprom *eeprom,
-		      u8 * data)
+		      u8 *data)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
-	u32 aligned_offset, aligned_len;
-	__le32 *p;
+	u32 aligned_offset, aligned_len, *p;
 	u8 *buf;
-	int err;
+	int err = 0;
+
 
 	if (eeprom->magic != EEPROM_MAGIC)
 		return -EINVAL;
@@ -2012,11 +2991,11 @@
 		buf = kmalloc(aligned_len, GFP_KERNEL);
 		if (!buf)
 			return -ENOMEM;
-		err = t3_seeprom_read(adapter, aligned_offset, (__le32 *) buf);
+		err = t3_seeprom_read(adapter, aligned_offset, (u32 *)buf);
 		if (!err && aligned_len > 4)
 			err = t3_seeprom_read(adapter,
 					      aligned_offset + aligned_len - 4,
-					      (__le32 *) & buf[aligned_len - 4]);
+					      (u32 *)&buf[aligned_len - 4]);
 		if (err)
 			goto out;
 		memcpy(buf + (eeprom->offset & 3), data, eeprom->len);
@@ -2027,7 +3006,7 @@
 	if (err)
 		goto out;
 
-	for (p = (__le32 *) buf; !err && aligned_len; aligned_len -= 4, p++) {
+	for (p = (u32 *)buf; !err && aligned_len; aligned_len -= 4, p++) {
 		err = t3_seeprom_write(adapter, aligned_offset, *p);
 		aligned_offset += 4;
 	}
@@ -2047,37 +3026,648 @@
 	memset(&wol->sopass, 0, sizeof(wol->sopass));
 }
 
-static const struct ethtool_ops cxgb_ethtool_ops = {
-	.get_settings = get_settings,
-	.set_settings = set_settings,
-	.get_drvinfo = get_drvinfo,
-	.get_msglevel = get_msglevel,
-	.set_msglevel = set_msglevel,
-	.get_ringparam = get_sge_param,
-	.set_ringparam = set_sge_param,
-	.get_coalesce = get_coalesce,
-	.set_coalesce = set_coalesce,
-	.get_eeprom_len = get_eeprom_len,
-	.get_eeprom = get_eeprom,
-	.set_eeprom = set_eeprom,
-	.get_pauseparam = get_pauseparam,
-	.set_pauseparam = set_pauseparam,
-	.get_rx_csum = get_rx_csum,
-	.set_rx_csum = set_rx_csum,
-	.set_tx_csum = ethtool_op_set_tx_csum,
-	.set_sg = ethtool_op_set_sg,
-	.get_link = ethtool_op_get_link,
-	.get_strings = get_strings,
-	.phys_id = cxgb3_phys_id,
-	.nway_reset = restart_autoneg,
-	.get_sset_count = get_sset_count,
+static struct ethtool_ops cxgb_ethtool_ops = {
+	.get_settings      = get_settings,
+	.set_settings      = set_settings,
+	.get_drvinfo       = get_drvinfo,
+	.get_msglevel      = get_msglevel,
+	.set_msglevel      = set_msglevel,
+	.get_ringparam     = get_sge_param,
+	.set_ringparam     = set_sge_param,
+	.get_coalesce      = get_coalesce,
+	.set_coalesce      = set_coalesce,
+#ifndef	LINUX_2_4
+	.get_eeprom_len    = get_eeprom_len,
+#endif	/* LINUX_2_4 */
+	.get_eeprom        = get_eeprom,
+	.set_eeprom        = set_eeprom,
+	.get_pauseparam    = get_pauseparam,
+	.set_pauseparam    = set_pauseparam,
+	.get_rx_csum       = get_rx_csum,
+	.set_rx_csum       = set_rx_csum,
+	.get_tx_csum       = ethtool_op_get_tx_csum,
+#ifndef	LINUX_2_4
+	.set_tx_csum       = ethtool_op_set_tx_csum,
+#endif	/* LINUX_2_4 */
+	.get_sg            = ethtool_op_get_sg,
+	.set_sg            = ethtool_op_set_sg,
+	.get_link          = ethtool_op_get_link,
+	.get_strings       = get_strings,
+	.phys_id           = cxgb3_phys_id,
+	.nway_reset        = restart_autoneg,
+#if defined(GET_STATS_COUNT)
+	.get_stats_count   = get_stats_count,
+	.self_test_count   = self_test_count,
+#else
+	.get_sset_count    = get_sset_count,
+#endif
 	.get_ethtool_stats = get_stats,
-	.get_regs_len = get_regs_len,
-	.get_regs = get_regs,
-	.get_wol = get_wol,
-	.set_tso = ethtool_op_set_tso,
+	.get_regs_len      = get_regs_len,
+	.get_regs          = get_regs,
+	.get_wol           = get_wol,
+#ifndef	LINUX_2_4
+	.get_tso           = ethtool_op_get_tso,
+	.set_tso           = ethtool_op_set_tso,
+#endif	/* LINUX_2_4 */
+#ifdef CXGB3_ETHTOOL_GPERMADDR
+	.get_perm_addr     = ethtool_op_get_perm_addr,
+#endif
+	.self_test         = self_tests,
 };
 
+
+#define adjust_proc_metrics() \
+	if (len <= offset + count) *eof = 1; \
+	*start = buf + offset; \
+	len -= offset; \
+	if (len > count) len = count; \
+	if (len < 0) len = 0;
+
+static int snmp_read_proc(char *buf, char **start, off_t offset, int count,
+			  int *eof, void *data)
+{
+	struct adapter *adapter = data;
+	struct tp_mib_stats m;
+	int len = 0;
+
+	spin_lock(&adapter->stats_lock);
+	t3_tp_get_mib_stats(adapter, &m);
+	spin_unlock(&adapter->stats_lock);
+
+#define MIB32(s, field) len += sprintf(buf + len, "%-18s %u\n", s, m.field)
+#define MIB64(s, hi, lo) \
+	len += sprintf(buf + len, "%-18s %llu\n", s, \
+		       ((unsigned long long)m.hi << 32) + m.lo)
+
+	MIB64("IPInReceives:", ipInReceive_hi, ipInReceive_lo);
+	MIB64("IPInHdrErrors:", ipInHdrErrors_hi, ipInHdrErrors_lo);
+	MIB64("IPInAddrErrors:", ipInAddrErrors_hi, ipInAddrErrors_lo);
+	MIB64("IPInUnknownProtos:", ipInUnknownProtos_hi,
+	      ipInUnknownProtos_lo);
+	MIB64("IPInDiscards:", ipInDiscards_hi, ipInDiscards_lo);
+	MIB64("IPInDelivers:", ipInDelivers_hi, ipInDelivers_lo);
+	MIB64("IPOutRequests:", ipOutRequests_hi, ipOutRequests_lo);
+	MIB64("IPOutDiscards:", ipOutDiscards_hi, ipOutDiscards_lo);
+	MIB64("IPOutNoRoutes:", ipOutNoRoutes_hi, ipOutNoRoutes_lo);
+	MIB32("IPReasmTimeout:", ipReasmTimeout);
+	MIB32("IPReasmReqds:", ipReasmReqds);
+	MIB32("IPReasmOKs:", ipReasmOKs);
+	MIB32("IPReasmFails:", ipReasmFails);
+	MIB32("TCPActiveOpens:", tcpActiveOpens);
+	MIB32("TCPPassiveOpens:", tcpPassiveOpens);
+	MIB32("TCPAttemptFails:", tcpAttemptFails);
+	MIB32("TCPEstabResets:", tcpEstabResets);
+	MIB32("TCPOutRsts:", tcpOutRsts);
+	MIB32("TCPCurrEstab:", tcpCurrEstab);
+	MIB64("TCPInSegs:", tcpInSegs_hi, tcpInSegs_lo);
+	MIB64("TCPOutSegs:", tcpOutSegs_hi, tcpOutSegs_lo);
+	MIB64("TCPRetransSeg:", tcpRetransSeg_hi, tcpRetransSeg_lo);
+	MIB64("TCPInErrs:", tcpInErrs_hi, tcpInErrs_lo);
+	MIB32("TCPRtoMin:", tcpRtoMin);
+	MIB32("TCPRtoMax:", tcpRtoMax);
+
+#undef MIB32
+#undef MIB64
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static int mtus_read_proc(char *buf, char **start, off_t offset, int count,
+			  int *eof, void *data)
+{
+	struct adapter *adapter = data;
+	unsigned short hw_mtus[NMTUS];
+	int i, len = 0;
+
+	spin_lock(&adapter->stats_lock);
+	t3_read_hw_mtus(adapter, hw_mtus);
+	spin_unlock(&adapter->stats_lock);
+
+	len += sprintf(buf, "Soft MTU\tEffective MTU\n");
+	for (i = 0; i < NMTUS; ++i)
+		len += sprintf(buf + len, "%8u\t\t%5u\n",
+			       adapter->params.mtus[i], hw_mtus[i]);
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static int cong_ctrl_read_proc(char *buf, char **start, off_t offset,
+			       int count, int *eof, void *data)
+{
+	static const char *dec_fac[] = {
+		"0.5", "0.5625", "0.625", "0.6875", "0.75", "0.8125", "0.875",
+		"0.9375" };
+
+	unsigned short incr[NMTUS][NCCTRL_WIN];
+	struct adapter *adapter = data;
+	int i, len = 0;
+
+	t3_get_cong_cntl_tab(adapter, incr);
+
+	for (i = 0; i < NCCTRL_WIN; ++i) {
+		int j;
+
+		for (j = 0; j < NMTUS; ++j)
+			len += sprintf(buf + len, "%5u ", incr[j][i]);
+
+		len += sprintf(buf + len, "%5u %s\n", adapter->params.a_wnd[i],
+			       dec_fac[adapter->params.b_wnd[i]]);
+	}
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static int rss_read_proc(char *buf, char **start, off_t offset, int count,
+			 int *eof, void *data)
+{
+	u8 lkup_tab[2 * RSS_TABLE_SIZE];
+	u16 map_tab[RSS_TABLE_SIZE];
+	struct adapter *adapter = data;
+	int i, len;
+
+	i = t3_read_rss(adapter, lkup_tab, map_tab);
+	if (i < 0)
+		return i;
+
+	len = sprintf(buf, "Idx\tLookup\tMap\n");
+	for (i = 0; i < RSS_TABLE_SIZE; ++i)
+		len += sprintf(buf + len, "%3u\t %3u\t %u\n", i, lkup_tab[i],
+			       map_tab[i]);
+	for (; i < 2 * RSS_TABLE_SIZE; ++i)
+		len += sprintf(buf + len, "%3u\t %3u\n", i, lkup_tab[i]);
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static int sched_read_proc(char *buf, char **start, off_t offset, int count,
+			   int *eof, void *data)
+{
+	int i, len;
+	unsigned int map, kbps, ipg;
+	unsigned int pace_tab[NTX_SCHED];
+	struct adapter *adap = data;
+
+	map = t3_read_reg(adap, A_TP_TX_MOD_QUEUE_REQ_MAP);
+	t3_read_pace_tbl(adap, pace_tab);
+
+	len = sprintf(buf, "Scheduler  Mode   Channel  Rate (Kbps)   "
+		      "Class IPG (0.1 ns)   Flow IPG (us)\n");
+	for (i = 0; i < NTX_SCHED; ++i) {
+		t3_get_tx_sched(adap, i, &kbps, &ipg);
+		len += sprintf(buf + len, "    %u      %-5s     %u     ", i,
+			       (map & (1 << (S_TX_MOD_TIMER_MODE + i))) ?
+				"flow" : "class", !!(map & (1 << i)));
+		if (kbps)
+			len += sprintf(buf + len, "%9u     ", kbps);
+		else
+			len += sprintf(buf + len, " disabled     ");
+
+		if (ipg)
+			len += sprintf(buf + len, "%13u        ", ipg);
+		else
+			len += sprintf(buf + len, "     disabled        ");
+
+		if (pace_tab[i])
+			len += sprintf(buf + len, "%10u\n", pace_tab[i] / 1000);
+		else
+			len += sprintf(buf + len, "  disabled\n");
+	}
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static int stats_read_proc(char *buf, char **start, off_t offset,
+			   int count, int *eof, void *data)
+{
+	int i, len = 0;
+	struct adapter *adapter = data;
+
+	len += sprintf(buf + len, "Interface:        ");
+	for (i = 0; i < SGE_QSETS; ++i)
+		len += sprintf(buf + len, " %10s",
+			       adapter->sge.qs[i].netdev ?
+			           adapter->sge.qs[i].netdev->name : "N/A");
+
+#define C(s, v) \
+	len += sprintf(buf + len, "\n%-18s", s); \
+	for (i = 0; i < SGE_QSETS; ++i) \
+		len += sprintf(buf + len, " %10lu", adapter->sge.qs[i].v); \
+
+	C("RspQEmpty:", rspq.empty);
+	C("FL0Empty:", fl[0].empty);
+	C("FL0AllocFailed:", fl[0].alloc_failed);
+	C("FL1Empty:", fl[1].empty);
+	C("FL1AllocFailed:", fl[1].alloc_failed);
+	C("TxQ0TunnelPkts:", txq[0].tx_pkts);
+	C("TxQ0Full:", txq[0].stops);
+	C("TxQ0Restarts:", txq[0].restarts);
+	C("TxQ1OffloadPkts:", txq[1].tx_pkts);
+	C("TxQ1Full:", txq[1].stops);
+	C("TxQ1Restarts:", txq[1].restarts);
+	C("TxQ2Full:", txq[2].stops);
+	C("TxQ2Restarts:", txq[2].restarts);
+	C("RxEthPackets:", rspq.eth_pkts);
+	C("TXCoalesceWR:", port_stats[SGE_PSTAT_TX_COALESCE_WR]);
+	C("TXCoalescePkt:", port_stats[SGE_PSTAT_TX_COALESCE_PKT]);
+	C("LROcompleted:", port_stats[SGE_PSTAT_LRO]);
+	C("LROpages:", port_stats[SGE_PSTAT_LRO_PG]);
+	C("LROpackets:", port_stats[SGE_PSTAT_LRO_SKB]);
+	C("LROmergedACKs:", port_stats[SGE_PSTAT_LRO_ACK]);
+	C("LROoverflow:", port_stats[SGE_PSTAT_LRO_OVFLOW]);
+	C("LROcollisions:", port_stats[SGE_PSTAT_LRO_COLSN]);
+	C("RxOffloadPackets:", rspq.offload_pkts);
+	C("RxOffloadBundles:", rspq.offload_bundles);
+	C("PureRepsonses:", rspq.pure_rsps);
+	C("RxImmediateData:", rspq.imm_data);
+	C("ANE:", rspq.async_notif);
+	C("RxDrops:", rspq.rx_drops);
+	C("RspDeferred:", rspq.nomem);
+	C("UnhandledIntr:", rspq.unhandled_irqs);
+	C("RspStarved:", rspq.starved);
+	C("RspRestarted:", rspq.restarted);
+#undef C
+
+	len += sprintf(buf + len, "\n%-18s %lu\n", "RxCorrectableErr:",
+		       adapter->pmrx.stats.corr_err);
+	len += sprintf(buf + len, "%-18s %lu\n", "TxCorrectableErr:",
+		       adapter->pmtx.stats.corr_err);
+	len += sprintf(buf + len, "%-18s %lu\n", "CMCorrectableErr:",
+		       adapter->cm.stats.corr_err);
+
+	len += sprintf(buf + len, "\n%-18s %lu\n", "ActiveRegionFull:",
+		       adapter->mc5.stats.active_rgn_full);
+	len += sprintf(buf + len, "%-18s %lu\n", "NFASearchErr:",
+		       adapter->mc5.stats.nfa_srch_err);
+	len += sprintf(buf + len, "%-18s %lu\n", "MC5UnknownCmd:",
+		       adapter->mc5.stats.unknown_cmd);
+	len += sprintf(buf + len, "%-18s %lu\n", "MC5DelActEmpty:",
+		       adapter->mc5.stats.del_act_empty);
+
+	len += sprintf(buf + len, "\n%-18s %lu\n", "ULPCh0PBLOOB:",
+		       adapter->irq_stats[STAT_ULP_CH0_PBL_OOB]);
+	len += sprintf(buf + len, "%-18s %lu\n", "ULPCh1PBLOOB:",
+		       adapter->irq_stats[STAT_ULP_CH1_PBL_OOB]);
+	len += sprintf(buf + len, "%-18s %lu\n", "PCICorrectableErr:",
+		       adapter->irq_stats[STAT_PCI_CORR_ECC]);
+
+	adjust_proc_metrics();
+	return len;
+}
+
+static void *filter_get_idx(struct seq_file *seq, loff_t pos)
+{
+	int i;
+	struct adapter *adap = seq->private;
+	struct filter_info *p = adap->filters;
+
+	if (!p)
+		return NULL;
+
+	for (i = 0; i < adap->params.mc5.nfilters; i++, p++)
+		if (p->valid) {
+			if (!pos)
+				return p;
+			pos--;
+		}
+	return NULL;
+}
+
+static void *filter_get_nxt_idx(struct seq_file *seq, struct filter_info *p)
+{
+	struct adapter *adap = seq->private;
+	struct filter_info *end = &adap->filters[adap->params.mc5.nfilters];
+
+	while (++p < end && !p->valid)
+		;
+	return p < end ? p : NULL;
+}
+
+static void *filter_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	return *pos ? filter_get_idx(seq, *pos - 1) : SEQ_START_TOKEN;
+}
+
+static void *filter_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	v = *pos ? filter_get_nxt_idx(seq, v) : filter_get_idx(seq, 0);
+	if (v)
+		++*pos;
+	return v;
+}
+
+static void filter_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int filter_seq_show(struct seq_file *seq, void *v)
+{
+	static const char *pkt_type[] = { "*", "tcp", "udp", "frag" };
+
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "index         SIP                DIP     sport "
+			      "dport VLAN PRI P/MAC type Q\n");
+	else {
+		char sip[20], dip[20];
+		struct filter_info *f = v;
+		struct adapter *adap = seq->private;
+		u32 nsip = htonl(f->sip);
+		u32 ndip = htonl(f->dip);
+
+		sprintf(sip, NIPQUAD_FMT "/%-2u", NIPQUAD(nsip),
+			f->sip_mask ? 33 - ffs(f->sip_mask) : 0);
+		sprintf(dip, NIPQUAD_FMT, NIPQUAD(ndip));
+		seq_printf(seq, "%5zu %18s %15s ", f - adap->filters, sip, dip);
+		seq_printf(seq, f->sport ? "%5u " : "    * ", f->sport);
+		seq_printf(seq, f->dport ? "%5u " : "    * ", f->dport);
+		seq_printf(seq, f->vlan != 0xfff ? "%4u " : "   * ", f->vlan);
+		seq_printf(seq, f->vlan_prio == FILTER_NO_VLAN_PRI ?
+			   "  * " : "%1u/%1u ", f->vlan_prio, f->vlan_prio | 1);
+		if (!f->mac_vld)
+			seq_printf(seq, "*/*   ");
+		else if (f->mac_hit)
+			seq_printf(seq, "%1u/%3u ",
+				       (f->mac_idx >> 3) & 0x1,
+				       (f->mac_idx) & 0x7);
+		else
+			seq_printf(seq, "%1u/  * ",
+				       (f->mac_idx >> 3) & 0x1);
+		seq_printf(seq, "%4s ", pkt_type[f->pkt_type]);
+		if (!f->pass)
+			seq_printf(seq, "-\n");
+		else if (f->rss)
+			seq_printf(seq, "*\n");
+		else
+			seq_printf(seq, "%1u\n", f->qset);
+	}
+	return 0;
+}
+
+static struct seq_operations filter_seq_ops = {
+	.start = filter_seq_start,
+	.next = filter_seq_next,
+	.stop = filter_seq_stop,
+	.show = filter_seq_show
+};
+
+static int filter_seq_open(struct inode *inode, struct file *file)
+{
+	int rc = seq_open(file, &filter_seq_ops);
+
+	if (!rc) {
+		struct proc_dir_entry *dp = PDE(inode);
+		struct seq_file *seq = file->private_data;
+
+		seq->private = dp->data;
+	}
+	return rc;
+}
+
+static struct file_operations filter_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = filter_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release
+};
+
+struct cxgb3_proc_entry {
+	const char *name;
+	read_proc_t *fn;
+};
+
+static struct cxgb3_proc_entry proc_files[] = {
+	{ "snmp", snmp_read_proc },
+	{ "congestion_control", cong_ctrl_read_proc },
+	{ "mtus", mtus_read_proc },
+	{ "rss", rss_read_proc },
+	{ "sched", sched_read_proc },
+	{ "stats", stats_read_proc },
+};
+
+static int __devinit cxgb_proc_setup(struct adapter *adapter,
+				     struct proc_dir_entry *dir)
+{
+	int i, created;
+	struct proc_dir_entry *p;
+
+	if (!dir)
+		return -EINVAL;
+
+	/* If we can create any of the entries we do. */
+	for (created = i = 0; i < ARRAY_SIZE(proc_files); ++i) {
+		p = create_proc_read_entry(proc_files[i].name, 0, dir,
+					   proc_files[i].fn, adapter);
+		if (p) {
+			SET_PROC_NODE_OWNER(p, THIS_MODULE);
+			created++;
+		}
+	}
+	p = create_proc_entry("filters", S_IRUGO, dir);
+	if (p) {
+		p->proc_fops = &filter_seq_fops;
+		p->data = adapter;
+		created++;
+	}
+
+	return created ? 0 : -ENOMEM;
+}
+
+static void cxgb_proc_cleanup(struct proc_dir_entry *dir)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(proc_files); ++i)
+		remove_proc_entry(proc_files[i].name, dir);
+	remove_proc_entry("filters", dir);
+}
+
+static void clear_qset_stats(struct sge_qset *qs)
+{
+	qs->rspq.empty = 0;
+	qs->fl[0].empty = 0;
+	qs->fl[1].empty = 0;
+	qs->txq[0].stops = 0;
+	qs->txq[0].restarts = 0;
+	qs->txq[1].stops = 0;
+	qs->txq[1].restarts = 0;
+	qs->txq[2].stops = 0;
+	qs->txq[2].restarts = 0;
+	qs->rspq.eth_pkts = 0;
+	qs->port_stats[SGE_PSTAT_TX_COALESCE_WR] = 0;
+	qs->port_stats[SGE_PSTAT_TX_COALESCE_PKT] = 0;
+	qs->port_stats[SGE_PSTAT_LRO] = 0;
+	qs->port_stats[SGE_PSTAT_LRO_PG] = 0;
+	qs->port_stats[SGE_PSTAT_LRO_SKB] = 0;
+	qs->port_stats[SGE_PSTAT_LRO_ACK] = 0;
+	qs->port_stats[SGE_PSTAT_LRO_OVFLOW] = 0;
+	qs->port_stats[SGE_PSTAT_LRO_COLSN] = 0;
+	qs->rspq.offload_pkts = 0;
+	qs->rspq.offload_bundles = 0;
+	qs->rspq.pure_rsps = 0;
+	qs->rspq.imm_data = 0;
+	qs->rspq.async_notif = 0;
+	qs->rspq.rx_drops = 0;
+	qs->rspq.nomem = 0;
+	qs->fl[0].alloc_failed = 0;
+	qs->fl[1].alloc_failed = 0;
+	qs->rspq.unhandled_irqs = 0;
+	qs->rspq.starved = 0;
+	qs->rspq.restarted = 0;
+}
+
+static void clear_port_qset_stats(struct adapter *adap,
+				  const struct port_info *pi)
+{
+	int i;
+	struct sge_qset *qs = &adap->sge.qs[pi->first_qset];
+
+	for (i = 0; i < pi->nqsets; i++)
+		clear_qset_stats(qs++);
+}
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#ifndef LINUX_2_4
+
+#define ERR(fmt, ...) do {\
+	printk(KERN_ERR "%s: " fmt "\n", dev->name, ## __VA_ARGS__); \
+	return -EINVAL; \
+} while (0)
+
+/*
+ * Perform device independent validation of offload policy.
+ */
+static int validate_offload_policy(const struct net_device *dev,
+				   const struct ofld_policy_file *f,
+				   size_t len)
+{
+	int i, inst;
+	const u32 *p;
+	const struct ofld_prog_inst *pi;
+
+	/*
+	 * We validate the following:
+	 * - Program sizes match what's in the header
+	 * - Branch targets are within the program
+	 * - Offsets do not step outside struct offload_req
+	 * - Outputs are valid
+	 */
+	printk(KERN_DEBUG "version %u, program length %zu bytes, alternate "
+	       "program length %zu bytes\n", f->vers,
+	       f->prog_size * sizeof(*pi), f->opt_prog_size * sizeof(*p));
+
+	if (sizeof(*f) + (f->nrules + 1) * sizeof(struct offload_settings) +
+	    f->prog_size * sizeof(*pi) + f->opt_prog_size * sizeof(*p) != len)
+		ERR("bad offload policy length %zu", len);
+
+	if (f->output_everything >= 0 && f->output_everything > f->nrules)
+		ERR("illegal output_everything %d in header",
+		    f->output_everything);
+
+	pi = f->prog;
+
+	for (i = 0; i < f->prog_size; i++, pi++) {
+		if (pi->offset < 0 ||
+		    pi->offset >= sizeof(struct offload_req) / 4)
+			ERR("illegal offset %d at instruction %d", pi->offset,
+			    i);
+		if (pi->next[0] < 0 && -pi->next[0] > f->nrules)
+			ERR("illegal output %d at instruction %d",
+			    -pi->next[0], i);
+		if (pi->next[1] < 0 && -pi->next[1] > f->nrules)
+			ERR("illegal output %d at instruction %d",
+			    -pi->next[1], i);
+		if (pi->next[0] > 0 && pi->next[0] >= f->prog_size)
+			ERR("illegal branch target %d at instruction %d",
+			    pi->next[0], i);
+		if (pi->next[1] > 0 && pi->next[1] >= f->prog_size)
+			ERR("illegal branch target %d at instruction %d",
+			    pi->next[1], i);
+	}
+
+	p = (const u32 *)pi;
+
+	for (inst = i = 0; i < f->opt_prog_size; inst++) {
+		unsigned int off = *p & 0xffff, nvals = *p >> 16;
+
+		if (off >= sizeof(struct offload_req) / 4)
+			ERR("illegal offset %u at opt instruction %d",
+			    off, inst);
+		if ((int32_t)p[1] < 0 && -p[1] > f->nrules)
+			ERR("illegal output %d at opt instruction %d",
+			    -p[1], inst);
+		if ((int32_t)p[2] < 0 && -p[2] > f->nrules)
+			ERR("illegal output %d at opt instruction %d",
+			    -p[2], inst);
+		if ((int32_t)p[1] > 0 && p[1] >= f->opt_prog_size)
+			ERR("illegal branch target %d at opt instruction %d",
+			    p[1], inst);
+		if ((int32_t)p[2] > 0 && p[2] >= f->opt_prog_size)
+			ERR("illegal branch target %d at opt instruction %d",
+			    p[2], inst);
+		p += 4 + nvals;
+		i += 4 + nvals;
+		if (i > f->opt_prog_size)
+			ERR("too many values %u for opt instruction %d",
+			    nvals, inst);
+	}
+
+	return 0;
+}
+
+#undef ERR
+
+/*
+ * Perform T3-specific validation of offload policy settings.
+ */
+static int validate_policy_settings(const struct net_device *dev,
+				    struct adapter *adap,
+				    const struct ofld_policy_file *f)
+{
+	int i, nqsets = 0, nclasses = 8 / adap->params.tp.nchan;
+	const u32 *op = (const u32 *)&f->prog[f->prog_size];
+	const struct offload_settings *s = (void *)&op[f->opt_prog_size];
+
+	for_each_port(adap, i)
+		nqsets += adap2pinfo(adap, i)->nqsets;
+
+	for (i = 0; i <= f->nrules; i++, s++) {
+		if (s->cong_algo > 3) {
+			printk(KERN_ERR "%s: illegal congestion algorithm %d\n",
+			       dev->name, s->cong_algo);
+			return -EINVAL;
+		}
+		if (s->rssq >= nqsets) {
+			printk(KERN_ERR "%s: illegal RSS queue %d\n", dev->name,
+			       s->rssq);
+			return -EINVAL;
+		}
+		if (s->sched_class >= nclasses) {
+			printk(KERN_ERR "%s: illegal scheduling class %d\n",
+			       dev->name, s->sched_class);
+			return -EINVAL;
+		}
+		if (s->tstamp >= 0) {
+			printk(KERN_ERR "%s: policy rules specifying timestamps"
+			       " not supported\n", dev->name);
+			return -EINVAL;
+		}
+		if (s->sack >= 0) {
+			printk(KERN_ERR "%s: policy rules specifying SACK not "
+			       "supported\n", dev->name);
+			return -EINVAL;
+		}
+	}
+	return 0;
+}
+
+#endif /* !LINUX_2_4 */
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
 static int in_range(int val, int lo, int hi)
 {
 	return val < 0 || (val <= hi && val >= lo);
@@ -2094,7 +3684,103 @@
 		return -EFAULT;
 
 	switch (cmd) {
-	case CHELSIO_SET_QSET_PARAMS:{
+	case CHELSIO_SETREG: {
+		struct ch_reg edata;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.addr & 3) != 0 || edata.addr >= adapter->mmio_len)
+			return -EINVAL;
+		writel(edata.val, adapter->regs + edata.addr);
+		break;
+	}
+	case CHELSIO_GETREG: {
+		struct ch_reg edata;
+
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.addr & 3) != 0 || edata.addr >= adapter->mmio_len)
+			return -EINVAL;
+		edata.val = readl(adapter->regs + edata.addr);
+		if (copy_to_user(useraddr, &edata, sizeof(edata)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_GETTPI: {
+		struct ch_reg edata;
+
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		ret = t3_elmr_blk_read(adapter, edata.addr, &edata.val, 1);
+		if (ret)
+			return ret;
+		if (copy_to_user(useraddr, &edata, sizeof(edata)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_SETTPI: {
+		struct ch_reg edata;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		ret = t3_elmr_blk_write(adapter, edata.addr, &edata.val, 1);
+		if (ret)
+			return ret;
+		break;
+	}
+	case CHELSIO_GET_SGE_CONTEXT: {
+		struct ch_cntxt ecntxt;
+
+		if (copy_from_user(&ecntxt, useraddr, sizeof(ecntxt)))
+			return -EFAULT;
+
+		spin_lock_irq(&adapter->sge.reg_lock);
+		if (ecntxt.cntxt_type == CNTXT_TYPE_EGRESS)
+			ret = t3_sge_read_ecntxt(adapter, ecntxt.cntxt_id,
+						 ecntxt.data);
+		else if (ecntxt.cntxt_type == CNTXT_TYPE_FL)
+			ret = t3_sge_read_fl(adapter, ecntxt.cntxt_id,
+					     ecntxt.data);
+		else if (ecntxt.cntxt_type == CNTXT_TYPE_RSP)
+			ret = t3_sge_read_rspq(adapter, ecntxt.cntxt_id,
+					       ecntxt.data);
+		else if (ecntxt.cntxt_type == CNTXT_TYPE_CQ)
+			ret = t3_sge_read_cq(adapter, ecntxt.cntxt_id,
+					     ecntxt.data);
+		else
+			ret = -EINVAL;
+		spin_unlock_irq(&adapter->sge.reg_lock);
+
+		if (ret)
+			return ret;
+		if (copy_to_user(useraddr, &ecntxt, sizeof(ecntxt)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_GET_SGE_DESC: {
+		struct ch_desc edesc;
+
+		if (copy_from_user(&edesc, useraddr, sizeof(edesc)))
+			return -EFAULT;
+
+		if (edesc.queue_num >= SGE_QSETS * 6)
+			return -EINVAL;
+
+		ret = t3_get_desc(&adapter->sge.qs[edesc.queue_num / 6],
+				  edesc.queue_num % 6, edesc.idx, edesc.data);
+		if (ret < 0)
+			return ret;
+		edesc.size = ret;
+
+		if (copy_to_user(useraddr, &edesc, sizeof(edesc)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_SET_QSET_PARAMS: {
 		int i;
 		struct qset_params *q;
 		struct ch_qset_params t;
@@ -2108,35 +3794,33 @@
 		if (t.qset_idx >= SGE_QSETS)
 			return -EINVAL;
 		if (!in_range(t.intr_lat, 0, M_NEWTIMER) ||
-			!in_range(t.cong_thres, 0, 255) ||
-			!in_range(t.txq_size[0], MIN_TXQ_ENTRIES,
-				MAX_TXQ_ENTRIES) ||
-			!in_range(t.txq_size[1], MIN_TXQ_ENTRIES,
-				MAX_TXQ_ENTRIES) ||
-			!in_range(t.txq_size[2], MIN_CTRL_TXQ_ENTRIES,
-				MAX_CTRL_TXQ_ENTRIES) ||
-			!in_range(t.fl_size[0], MIN_FL_ENTRIES,
-				MAX_RX_BUFFERS)
-			|| !in_range(t.fl_size[1], MIN_FL_ENTRIES,
-					MAX_RX_JUMBO_BUFFERS)
-			|| !in_range(t.rspq_size, MIN_RSPQ_ENTRIES,
-					MAX_RSPQ_ENTRIES))
-			return -EINVAL;
+		    !in_range(t.cong_thres, 0, 255) ||
+		    !in_range(t.txq_size[0], MIN_TXQ_ENTRIES,
+			      MAX_TXQ_ENTRIES) ||
+		    !in_range(t.txq_size[1], MIN_TXQ_ENTRIES,
+			      MAX_TXQ_ENTRIES) ||
+		    !in_range(t.txq_size[2], MIN_CTRL_TXQ_ENTRIES,
+			      MAX_CTRL_TXQ_ENTRIES) ||
+		    !in_range(t.fl_size[0], MIN_FL_ENTRIES, MAX_RX_BUFFERS) ||
+		    !in_range(t.fl_size[1], MIN_FL_ENTRIES,
+			      MAX_RX_JUMBO_BUFFERS) ||
+		    !in_range(t.rspq_size, MIN_RSPQ_ENTRIES, MAX_RSPQ_ENTRIES))
+		       return -EINVAL;
 
 		if ((adapter->flags & FULL_INIT_DONE) && t.lro > 0)
 			for_each_port(adapter, i) {
 				pi = adap2pinfo(adapter, i);
 				if (t.qset_idx >= pi->first_qset &&
 				    t.qset_idx < pi->first_qset + pi->nqsets &&
-				    !(pi->rx_offload & T3_RX_CSUM))
+				    !pi->rx_csum_offload)
 					return -EINVAL;
 			}
 
 		if ((adapter->flags & FULL_INIT_DONE) &&
-			(t.rspq_size >= 0 || t.fl_size[0] >= 0 ||
-			t.fl_size[1] >= 0 || t.txq_size[0] >= 0 ||
-			t.txq_size[1] >= 0 || t.txq_size[2] >= 0 ||
-			t.polling >= 0 || t.cong_thres >= 0))
+		    (t.rspq_size >= 0 || t.fl_size[0] >= 0 ||
+		     t.fl_size[1] >= 0 || t.txq_size[0] >= 0 ||
+		     t.txq_size[1] >= 0 || t.txq_size[2] >= 0 ||
+		     t.polling >= 0 || t.cong_thres >= 0))
 			return -EBUSY;
 
 		/* Allow setting of any available qset when offload enabled */
@@ -2144,7 +3828,7 @@
 			q1 = 0;
 			for_each_port(adapter, i) {
 				pi = adap2pinfo(adapter, i);
-				nqsets += pi->first_qset + pi->nqsets;
+				nqsets = pi->first_qset + pi->nqsets;
 			}
 		}
 
@@ -2170,8 +3854,7 @@
 		if (t.cong_thres >= 0)
 			q->cong_thres = t.cong_thres;
 		if (t.intr_lat >= 0) {
-			struct sge_qset *qs =
-				&adapter->sge.qs[t.qset_idx];
+			struct sge_qset *qs = &adapter->sge.qs[t.qset_idx];
 
 			q->coalesce_usecs = t.intr_lat;
 			t3_update_qset_coalesce(qs, q);
@@ -2182,22 +3865,24 @@
 			else {
 				/* No polling with INTx for T3A */
 				if (adapter->params.rev == 0 &&
-					!(adapter->flags & USING_MSI))
+				    !(adapter->flags & USING_MSI))
 					t.polling = 0;
 
 				for (i = 0; i < SGE_QSETS; i++) {
-					q = &adapter->params.sge.
-						qset[i];
+					q = &adapter->params.sge.qset[i];
 					q->polling = t.polling;
 				}
 			}
 		}
-		if (t.lro >= 0)
-			set_qset_lro(dev, t.qset_idx, t.lro);
-
+		if (t.lro >= 0) {
+			struct sge_qset *qs = &adapter->sge.qs[t.qset_idx];
+
+			q->lro = t.lro;
+			qs->lro.enabled = t.lro;
+		}
 		break;
 	}
-	case CHELSIO_GET_QSET_PARAMS:{
+	case CHELSIO_GET_QSET_PARAMS: {
 		struct qset_params *q;
 		struct ch_qset_params t;
 		int q1 = pi->first_qset;
@@ -2220,17 +3905,17 @@
 			return -EINVAL;
 
 		q = &adapter->params.sge.qset[q1 + t.qset_idx];
-		t.rspq_size = q->rspq_size;
+		t.rspq_size   = q->rspq_size;
 		t.txq_size[0] = q->txq_size[0];
 		t.txq_size[1] = q->txq_size[1];
 		t.txq_size[2] = q->txq_size[2];
-		t.fl_size[0] = q->fl_size;
-		t.fl_size[1] = q->jumbo_size;
-		t.polling = q->polling;
-		t.lro = q->lro;
-		t.intr_lat = q->coalesce_usecs;
-		t.cong_thres = q->cong_thres;
-		t.qnum = q1;
+		t.fl_size[0]  = q->fl_size;
+		t.fl_size[1]  = q->jumbo_size;
+		t.polling     = q->polling;
+		t.lro         = q->lro;
+		t.intr_lat    = q->coalesce_usecs;
+		t.cong_thres  = q->cong_thres;
+		t.qnum        = q1;
 
 		if (adapter->flags & USING_MSIX)
 			t.vector = adapter->msix_info[q1 + t.qset_idx + 1].vec;
@@ -2241,9 +3926,9 @@
 			return -EFAULT;
 		break;
 	}
-	case CHELSIO_SET_QSET_NUM:{
+	case CHELSIO_SET_QSET_NUM: {
 		struct ch_reg edata;
-		unsigned int i, first_qset = 0, other_qsets = 0;
+		unsigned int i, first_qset = 0, other_qsets;
 
 		if (!capable(CAP_NET_ADMIN))
 			return -EPERM;
@@ -2252,17 +3937,16 @@
 		if (copy_from_user(&edata, useraddr, sizeof(edata)))
 			return -EFAULT;
 		if (edata.val < 1 ||
-			(edata.val > 1 && !(adapter->flags & USING_MSIX)))
+		    (edata.val > 1 && !(adapter->flags & USING_MSIX)))
 			return -EINVAL;
 
-		for_each_port(adapter, i)
-			if (adapter->port[i] && adapter->port[i] != dev)
-				other_qsets += adap2pinfo(adapter, i)->nqsets;
-
+		other_qsets = adapter->sge.nqsets - pi->nqsets;
 		if (edata.val + other_qsets > SGE_QSETS)
 			return -EINVAL;
 
 		pi->nqsets = edata.val;
+		t3_compat_set_num_tx_queues(dev, edata.val);
+		adapter->sge.nqsets = other_qsets + pi->nqsets;
 
 		for_each_port(adapter, i)
 			if (adapter->port[i]) {
@@ -2272,7 +3956,7 @@
 			}
 		break;
 	}
-	case CHELSIO_GET_QSET_NUM:{
+	case CHELSIO_GET_QSET_NUM: {
 		struct ch_reg edata;
 
 		edata.cmd = CHELSIO_GET_QSET_NUM;
@@ -2281,21 +3965,22 @@
 			return -EFAULT;
 		break;
 	}
-	case CHELSIO_LOAD_FW:{
+	case CHELSIO_LOAD_FW: {
 		u8 *fw_data;
 		struct ch_mem_range t;
 
-		if (!capable(CAP_SYS_RAWIO))
+		if (!capable(CAP_NET_ADMIN))
 			return -EPERM;
 		if (copy_from_user(&t, useraddr, sizeof(t)))
 			return -EFAULT;
-		/* Check t.len sanity ? */
+		if (!t.len)
+			return -EINVAL;
+
 		fw_data = kmalloc(t.len, GFP_KERNEL);
 		if (!fw_data)
 			return -ENOMEM;
 
-		if (copy_from_user
-			(fw_data, useraddr + sizeof(t), t.len)) {
+		if (copy_from_user(fw_data, useraddr + sizeof(t), t.len)) {
 			kfree(fw_data);
 			return -EFAULT;
 		}
@@ -2306,7 +3991,157 @@
 			return ret;
 		break;
 	}
-	case CHELSIO_SETMTUTAB:{
+	case CHELSIO_LOAD_BOOT: {
+		u8 *boot_data;
+		struct ch_mem_range t;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		boot_data = kmalloc(t.len, GFP_KERNEL);
+		if (!boot_data)
+			return -ENOMEM;
+
+		if (copy_from_user(boot_data, useraddr + sizeof(t), t.len)) {
+			kfree(boot_data);
+			return -EFAULT;
+		}
+
+		ret = t3_load_boot(adapter, boot_data, t.len);
+		kfree(boot_data);
+		if (ret)
+			return ret;
+		break;
+	}
+	case CHELSIO_SET_FILTER: {
+		struct ch_filter f;
+		struct filter_info *p;
+
+		if (!adapter->params.mc5.nfilters)
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;  /* can still change nfilters */
+		if (!adapter->filters)
+			return -ENOMEM;
+		if (!cxgb3_filter_toe_mode(adapter, CXGB3_FTM_FILTER))
+			return -EBUSY;
+		if (copy_from_user(&f, useraddr, sizeof(f)))
+			return -EFAULT;
+
+		if (f.filter_id >= adapter->params.mc5.nfilters ||
+		    (f.val.dip && f.mask.dip != 0xffffffff) ||
+		    (f.val.sport && f.mask.sport != 0xffff) ||
+		    (f.val.dport && f.mask.dport != 0xffff) ||
+		    (f.mask.vlan && f.mask.vlan != 0xfff) ||
+		    (f.mask.vlan_prio && f.mask.vlan_prio != FILTER_NO_VLAN_PRI) ||
+		    (f.mac_addr_idx != 0xffff && f.mac_addr_idx > 15) ||
+		    f.qset >= SGE_QSETS ||
+		    adapter->rrss_map[f.qset] >= RSS_TABLE_SIZE)
+			return -EINVAL;
+
+		p = &adapter->filters[f.filter_id];
+		if (p->locked)
+			return -EPERM;
+
+		p->sip = f.val.sip;
+		p->sip_mask = f.mask.sip;
+		p->dip = f.val.dip;
+		p->sport = f.val.sport;
+		p->dport = f.val.dport;
+		p->vlan = f.mask.vlan ? f.val.vlan : 0xfff;
+		p->vlan_prio = f.mask.vlan_prio ? (f.val.vlan_prio & 6) :
+						  FILTER_NO_VLAN_PRI;
+		p->mac_hit = f.mac_hit;
+		p->mac_vld = f.mac_addr_idx != 0xffff;
+		p->mac_idx = f.mac_addr_idx;
+		p->pkt_type = f.proto;
+		p->report_filter_id = f.want_filter_id;
+		p->pass = f.pass;
+		p->rss = f.rss;
+		p->qset = f.qset;
+
+		ret = set_filter(adapter, f.filter_id, p);
+		if (ret)
+			return ret;
+		p->valid = 1;
+		break;
+	}
+	case CHELSIO_DEL_FILTER: {
+		struct ch_filter f;
+		struct filter_info *p;
+
+		if (!adapter->params.mc5.nfilters)
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;  /* can still change nfilters */
+		if (!adapter->filters)
+			return -ENOMEM;
+		if (!cxgb3_filter_toe_mode(adapter, CXGB3_FTM_FILTER))
+			return -EBUSY;
+		if (copy_from_user(&f, useraddr, sizeof(f)))
+			return -EFAULT;
+		if (f.filter_id >= adapter->params.mc5.nfilters)
+		       return -EINVAL;
+
+		p = &adapter->filters[f.filter_id];
+		if (p->locked)
+			return -EPERM;
+		memset(p, 0, sizeof(*p));
+		p->sip = p->sip_mask = 0xffffffff;
+		p->vlan = 0xfff;
+		p->vlan_prio = FILTER_NO_VLAN_PRI;
+		p->pkt_type = 1;
+		return set_filter(adapter, f.filter_id, p);
+	}
+
+	case CHELSIO_CLEAR_STATS: {
+		struct ch_reg edata;
+		struct port_info *pi = netdev_priv(dev);
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;
+		if (copy_from_user(&edata, useraddr, sizeof(edata)))
+			return -EFAULT;
+		if ((edata.val & STATS_QUEUE) && edata.addr != -1 &&
+		    edata.addr >= pi->nqsets)
+			return -EINVAL;
+		if (edata.val & STATS_PORT) {
+			spin_lock(&adapter->stats_lock);
+			t3_mac_update_stats(&pi->mac);
+			spin_unlock(&adapter->stats_lock);
+			memset(&pi->mac.stats, 0, sizeof(pi->mac.stats));
+			clear_sge_port_stats(adapter, pi);
+		}
+		if (edata.val & STATS_QUEUE) {
+			if (edata.addr == -1)
+				clear_port_qset_stats(adapter, pi);
+			else
+				clear_qset_stats(&adapter->sge.qs[edata.addr +
+							      pi->first_qset]);
+		}
+		break;
+	}
+
+	case CHELSIO_DEVUP:
+		if (!is_offload(adapter))
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		ret = offload_open(dev);
+		if (ret)
+			return ret;
+
+		break;
+#ifdef CONFIG_CHELSIO_T3_CORE
+	case CHELSIO_SETMTUTAB: {
 		struct ch_mtus m;
 		int i;
 
@@ -2320,34 +4155,49 @@
 			return -EFAULT;
 		if (m.nmtus != NMTUS)
 			return -EINVAL;
-		if (m.mtus[0] < 81)	/* accommodate SACK */
+		if (m.mtus[0] < 81)         /* accommodate SACK */
 			return -EINVAL;
 
-		/* MTUs must be in ascending order */
+		// MTUs must be in ascending order
 		for (i = 1; i < NMTUS; ++i)
 			if (m.mtus[i] < m.mtus[i - 1])
 				return -EINVAL;
 
 		memcpy(adapter->params.mtus, m.mtus,
-			sizeof(adapter->params.mtus));
+		       sizeof(adapter->params.mtus));
 		break;
 	}
-	case CHELSIO_GET_PM:{
-		struct tp_params *p = &adapter->params.tp;
-		struct ch_pm m = {.cmd = CHELSIO_GET_PM };
+	case CHELSIO_GETMTUTAB: {
+		struct ch_mtus m;
 
 		if (!is_offload(adapter))
 			return -EOPNOTSUPP;
-		m.tx_pg_sz = p->tx_pg_size;
-		m.tx_num_pg = p->tx_num_pgs;
-		m.rx_pg_sz = p->rx_pg_size;
-		m.rx_num_pg = p->rx_num_pgs;
-		m.pm_total = p->pmtx_size + p->chan_rx_size * p->nchan;
+
+		memcpy(m.mtus, adapter->params.mtus, sizeof(m.mtus));
+		m.nmtus = NMTUS;
+
 		if (copy_to_user(useraddr, &m, sizeof(m)))
 			return -EFAULT;
 		break;
 	}
-	case CHELSIO_SET_PM:{
+#endif /* CONFIG_CHELSIO_T3_CORE */
+
+	case CHELSIO_GET_PM: {
+		struct tp_params *p = &adapter->params.tp;
+		struct ch_pm m = { .cmd = CHELSIO_GET_PM };
+
+		if (!is_offload(adapter))
+			return -EOPNOTSUPP;
+		m.tx_pg_sz  = p->tx_pg_size;
+		m.tx_num_pg = p->tx_num_pgs;
+		m.rx_pg_sz  = p->rx_pg_size;
+		m.rx_num_pg = p->rx_num_pgs;
+		m.pm_total  = p->pmtx_size + p->chan_rx_size * p->nchan;
+		if (copy_to_user(useraddr, &m, sizeof(m)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_SET_PM: {
 		struct ch_pm m;
 		struct tp_params *p = &adapter->params.tp;
 
@@ -2359,11 +4209,11 @@
 			return -EBUSY;
 		if (copy_from_user(&m, useraddr, sizeof(m)))
 			return -EFAULT;
-		if (!is_power_of_2(m.rx_pg_sz) ||
-			!is_power_of_2(m.tx_pg_sz))
-			return -EINVAL;	/* not power of 2 */
+		if (!m.rx_pg_sz || (m.rx_pg_sz & (m.rx_pg_sz - 1)) ||
+		    !m.tx_pg_sz || (m.tx_pg_sz & (m.tx_pg_sz - 1)))
+			return -EINVAL;      /* not power of 2 */
 		if (!(m.rx_pg_sz & 0x14000))
-			return -EINVAL;	/* not 16KB or 64KB */
+			return -EINVAL;      /* not 16KB or 64KB */
 		if (!(m.tx_pg_sz & 0x1554000))
 			return -EINVAL;
 		if (m.tx_num_pg == -1)
@@ -2373,7 +4223,7 @@
 		if (m.tx_num_pg % 24 || m.rx_num_pg % 24)
 			return -EINVAL;
 		if (m.rx_num_pg * m.rx_pg_sz > p->chan_rx_size ||
-			m.tx_num_pg * m.tx_pg_sz > p->chan_tx_size)
+		    m.tx_num_pg * m.tx_pg_sz > p->chan_tx_size)
 			return -EINVAL;
 		p->rx_pg_size = m.rx_pg_sz;
 		p->tx_pg_size = m.tx_pg_sz;
@@ -2381,7 +4231,23 @@
 		p->tx_num_pgs = m.tx_num_pg;
 		break;
 	}
-	case CHELSIO_GET_MEM:{
+	case CHELSIO_READ_TCAM_WORD: {
+		struct ch_tcam_word t;
+
+		if (!is_offload(adapter))
+			return -EOPNOTSUPP;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EIO;         /* need MC5 */
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		ret = t3_read_mc5_range(&adapter->mc5, t.addr, 1, t.buf);
+		if (ret)
+			return ret;
+		if (copy_to_user(useraddr, &t, sizeof(t)))
+			return -EFAULT;
+		break;
+	}
+	case CHELSIO_GET_MEM: {
 		struct ch_mem_range t;
 		struct mc7 *mem;
 		u64 buf[32];
@@ -2389,7 +4255,7 @@
 		if (!is_offload(adapter))
 			return -EOPNOTSUPP;
 		if (!(adapter->flags & FULL_INIT_DONE))
-			return -EIO;	/* need the memory controllers */
+			return -EIO;         /* need the memory controllers */
 		if (copy_from_user(&t, useraddr, sizeof(t)))
 			return -EFAULT;
 		if ((t.addr & 7) || (t.len & 7))
@@ -2416,14 +4282,12 @@
 		 * Read 256 bytes at a time as len can be large and we don't
 		 * want to use huge intermediate buffers.
 		 */
-		useraddr += sizeof(t);	/* advance to start of buffer */
+		useraddr += sizeof(t);   /* advance to start of buffer */
 		while (t.len) {
-			unsigned int chunk =
-				min_t(unsigned int, t.len, sizeof(buf));
-
-			ret =
-				t3_mc7_bd_read(mem, t.addr / 8, chunk / 8,
-						buf);
+			unsigned int chunk = min_t(unsigned int, t.len,
+						   sizeof(buf));
+
+			ret = t3_mc7_bd_read(mem, t.addr / 8, chunk / 8, buf);
 			if (ret)
 				return ret;
 			if (copy_to_user(useraddr, buf, chunk))
@@ -2434,7 +4298,8 @@
 		}
 		break;
 	}
-	case CHELSIO_SET_TRACE_FILTER:{
+#ifdef CONFIG_CHELSIO_T3_CORE
+	case CHELSIO_SET_TRACE_FILTER: {
 		struct ch_trace t;
 		const struct trace_params *tp;
 
@@ -2447,15 +4312,300 @@
 
 		tp = (const struct trace_params *)&t.sip;
 		if (t.config_tx)
-			t3_config_trace_filter(adapter, tp, 0,
-						t.invert_match,
-						t.trace_tx);
+			t3_config_trace_filter(adapter, tp, 0, t.invert_match,
+					       t.trace_tx);
 		if (t.config_rx)
-			t3_config_trace_filter(adapter, tp, 1,
-						t.invert_match,
-						t.trace_rx);
+			t3_config_trace_filter(adapter, tp, 1, t.invert_match,
+					       t.trace_rx);
 		break;
 	}
+	case CHELSIO_GET_TRACE_FILTER: {
+		struct ch_trace t;
+		struct ch_trace t1;
+		struct trace_params *tp;
+		int    inverted=0, enabled=0;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!offload_running(adapter))
+			return -EAGAIN;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		/*
+		 * read the filters into t.
+		 */
+		tp = (struct trace_params *)&t.sip;
+		if (t.config_tx) {
+			t3_query_trace_filter(adapter, tp, 0, &inverted, &enabled);
+			if (inverted)
+				t.invert_match = 1;
+			if (enabled)
+				t.trace_tx = 1;
+		}
+		if (t.config_rx) {
+			if (enabled)
+				tp = (struct trace_params *)&t1.sip;
+			t3_query_trace_filter(adapter, tp, 1, &inverted, &enabled);
+			if (inverted)
+				t.invert_match = 1;
+			if (enabled)
+				t.trace_rx = 1;
+		}
+		if (copy_to_user(useraddr, &t, sizeof(t)))
+			return -EFAULT;
+		break;
+	}
+#endif
+	case CHELSIO_SET_PKTSCHED: {
+		struct ch_pktsched_params p;
+		int port;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!adapter->open_device_map)
+			return -EAGAIN;        /* uP and SGE must be running */
+		if (copy_from_user(&p, useraddr, sizeof(p)))
+			return -EFAULT;
+		if (p.sched == PKTSCHED_TUNNELQ && !in_range(p.idx, 0,
+							     SGE_QSETS-1))
+			return -EINVAL;
+		if (p.sched == PKTSCHED_PORT && !in_range(p.idx, 0,
+					      adapter->params.nports -1))
+			return -EINVAL;
+
+		if (p.sched == PKTSCHED_PORT) {
+			struct port_info *pi;
+			pi = netdev_priv(adapter->port[p.idx]);
+			ret = send_pktsched_cmd(adapter, p.sched,
+						p.idx, p.min, p.max,
+						pi->tx_chan);
+			if (ret)
+				return -EINVAL;
+			pi->sched_min = p.min;
+			pi->sched_max = p.max;
+
+			return 0;
+		}
+
+		/*
+		 * Find the port corresponding to the queue set so we can
+		 * determine the right transmit channel to use for the
+		 * schedule binding.
+		 */
+		for_each_port(adapter, port) {
+			struct port_info *pi;
+			pi = netdev_priv(adapter->port[port]);
+			if (p.idx >= pi->first_qset &&
+			    p.idx < pi->first_qset + pi->nqsets)
+				break;
+		}
+
+		ret = send_pktsched_cmd(adapter, p.sched,
+					p.idx, p.min, p.max,
+					pi->tx_chan);
+
+		if (ret)
+			return -EINVAL;
+
+		(adapter->sge.qs[p.idx].txq)->sched_max = p.max;
+
+		return 0;
+	}
+
+	case CHELSIO_GET_PKTSCHED: {
+		struct ch_pktsched_params p;
+		struct port_info *pi;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!adapter->open_device_map)
+			return -EAGAIN;        /* uP and SGE must be running */
+		if (copy_from_user(&p, useraddr, sizeof(p)))
+			return -EFAULT;
+		if (p.sched == PKTSCHED_TUNNELQ && !in_range(p.idx, 0,
+							     SGE_QSETS-1))
+			return -EINVAL;
+		if (p.sched == PKTSCHED_PORT && !in_range(p.idx, 0,
+					      adapter->params.nports -1))
+			return -EINVAL;
+
+
+		if (p.sched == PKTSCHED_PORT) {
+			pi = netdev_priv(adapter->port[p.idx]);
+			p.min = pi->sched_min;
+			p.max = pi->sched_max;
+		} else if (p.sched == PKTSCHED_TUNNELQ)
+			p.max = (adapter->sge.qs[p.idx].txq)->sched_max;
+				
+		if (copy_to_user(useraddr, &p, sizeof(p)))
+			return -EFAULT;
+
+		return 0;
+	}
+
+#ifdef CONFIG_CHELSIO_T3_CORE
+	case CHELSIO_SET_HW_SCHED: {
+		struct ch_hw_sched t;
+		unsigned int ticks_per_usec = core_ticks_per_usec(adapter);
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!(adapter->flags & FULL_INIT_DONE))
+			return -EAGAIN;       /* need TP to be initialized */
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+		if (t.sched >= NTX_SCHED || !in_range(t.mode, 0, 1) ||
+		    !in_range(t.channel, 0, 1) ||
+		    !in_range(t.kbps, 0, 10000000) ||
+		    !in_range(t.class_ipg, 0, 10000 * 65535 / ticks_per_usec) ||
+		    !in_range(t.flow_ipg, 0,
+			      dack_ticks_to_usec(adapter, 0x7ff)))
+			return -EINVAL;
+
+		if ((t.mode == 0 && t.flow_ipg >= 0) ||
+		    (t.mode == 1 && t.kbps >= 0))
+			return -EOPNOTSUPP;
+
+		if (t.kbps >= 0) {
+			ret = t3_config_sched(adapter, t.kbps, t.sched);
+			if (ret < 0)
+				return ret;
+		}
+		if (t.class_ipg >= 0)
+			t3_set_sched_ipg(adapter, t.sched, t.class_ipg);
+		if (t.flow_ipg >= 0) {
+			t.flow_ipg *= 1000;     /* us -> ns */
+			t3_set_pace_tbl(adapter, &t.flow_ipg, t.sched, 1);
+		}
+		if (t.mode >= 0) {
+			int bit = 1 << (S_TX_MOD_TIMER_MODE + t.sched);
+
+			t3_set_reg_field(adapter, A_TP_TX_MOD_QUEUE_REQ_MAP,
+					 bit, t.mode ? bit : 0);
+		}
+		if (t.channel >= 0)
+			t3_set_reg_field(adapter, A_TP_TX_MOD_QUEUE_REQ_MAP,
+					 1 << t.sched, t.channel << t.sched);
+		break;
+	}
+#endif /* CONFIG_CHELSIO_T3_CORE */
+	case CHELSIO_GET_UP_LA: {
+		struct ch_up_la t;
+		int bufsize = LA_ENTRIES * 4;
+		void *labuf;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		labuf = kmalloc(bufsize, GFP_USER);
+		if (!labuf)
+			return -ENOMEM;
+
+		ret = t3_get_up_la(adapter, &t.stopped, &t.idx,
+				   &t.bufsize, labuf);
+		if (ret)
+			goto out_la;
+
+		ret = -EFAULT;
+		if (copy_to_user(useraddr, &t, sizeof(t)))
+			goto out_la;
+		useraddr += offsetof(struct ch_up_la, data);
+		if (copy_to_user(useraddr, labuf, bufsize))
+			goto out_la;
+		ret = 0;
+out_la:
+		kfree(labuf);
+		if (ret)
+			return ret;
+
+		break;
+	}
+	case CHELSIO_GET_UP_IOQS: {
+		struct ch_up_ioqs t;
+		int bufsize = IOQ_ENTRIES * sizeof(struct t3_ioq_entry);
+		void *ioqbuf;
+		u32 *v;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		bufsize += 4 * 4; /* add room for rx/tx enable/status */
+		ioqbuf = kmalloc(bufsize, GFP_USER);
+		if (!ioqbuf)
+			return -ENOMEM;
+
+		ret = t3_get_up_ioqs(adapter, &t.bufsize, ioqbuf);
+		if (ret)
+			goto out_ioq;
+
+		v = ioqbuf;
+		t.ioq_rx_enable = *v++;
+		t.ioq_tx_enable = *v++;
+		t.ioq_rx_status = *v++;
+		t.ioq_tx_status = *v++;
+
+		ret = -EFAULT;
+		if (copy_to_user(useraddr, &t, sizeof(t)))
+			goto out_ioq;
+		useraddr += offsetof(struct ch_up_ioqs, data);
+		bufsize -= 4 * 4;
+		if (copy_to_user(useraddr, v, bufsize))
+			goto out_ioq;
+		ret = 0;
+out_ioq:
+		kfree(ioqbuf);
+		if (ret)
+			return ret;
+
+		break;
+	}
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+	case CHELSIO_SET_OFLD_POLICY: {
+#ifdef LINUX_2_4
+		return -EOPNOTSUPP;
+#else
+		struct ch_mem_range t;
+		struct ofld_policy_file *opf;
+
+		if (!test_bit(OFFLOAD_DEVMAP_BIT,
+			      &adapter->registered_device_map))
+			return -EOPNOTSUPP;
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (copy_from_user(&t, useraddr, sizeof(t)))
+			return -EFAULT;
+
+		/* len == 0 removes any existing policy */
+		if (t.len == 0) {
+			req_set_offload_policy(dev, NULL, 0);
+			break;
+		}
+
+		opf = kmalloc(t.len, GFP_KERNEL);
+		if (!opf)
+			return -ENOMEM;
+
+		if (copy_from_user(opf, useraddr + sizeof(t), t.len)) {
+			kfree(opf);
+			return -EFAULT;
+		}
+
+		ret = validate_offload_policy(dev, opf, t.len);
+		if (!ret) {
+			ret = validate_policy_settings(dev, adapter, opf);
+			if (!ret)
+				ret = req_set_offload_policy(dev, opf, t.len);
+		}
+		kfree(opf);
+		return ret;
+#endif
+	}
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -2467,43 +4617,82 @@
 	struct mii_ioctl_data *data = if_mii(req);
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
+	int ret, mmd;
 
 	switch (cmd) {
-	case SIOCGMIIREG:
-	case SIOCSMIIREG:
-		/* Convert phy_id from older PRTAD/DEVAD format */
-		if (is_10G(adapter) &&
-		    !mdio_phy_id_is_c45(data->phy_id) &&
-		    (data->phy_id & 0x1f00) &&
-		    !(data->phy_id & 0xe0e0))
-			data->phy_id = mdio_phy_id_c45(data->phy_id >> 8,
-						       data->phy_id & 0x1f);
+	case SIOCGMIIPHY:
+		data->phy_id = pi->phy.addr;
 		/* FALLTHRU */
-	case SIOCGMIIPHY:
-		return mdio_mii_ioctl(&pi->phy.mdio, data, cmd);
+	case SIOCGMIIREG: {
+		u32 val;
+		struct cphy *phy = &pi->phy;
+
+		if (!phy->mdio_read)
+			return -EOPNOTSUPP;
+		if (is_10G(adapter)) {
+			mmd = data->phy_id >> 8;
+			if (!mmd)
+				mmd = MDIO_DEV_PCS;
+			else if (mmd > MDIO_DEV_VEND2)
+				return -EINVAL;
+
+			ret = phy->mdio_read(adapter, data->phy_id & 0x1f, mmd,
+					     data->reg_num, &val);
+		} else
+			ret = phy->mdio_read(adapter, data->phy_id & 0x1f, 0,
+					     data->reg_num & 0x1f, &val);
+		if (!ret)
+			data->val_out = val;
+		break;
+	}
+	case SIOCSMIIREG: {
+		struct cphy *phy = &pi->phy;
+
+		if (!capable(CAP_NET_ADMIN))
+			return -EPERM;
+		if (!phy->mdio_write)
+			return -EOPNOTSUPP;
+		if (is_10G(adapter)) {
+			mmd = data->phy_id >> 8;
+			if (!mmd)
+				mmd = MDIO_DEV_PCS;
+			else if (mmd > MDIO_DEV_VEND2)
+				return -EINVAL;
+
+			ret = phy->mdio_write(adapter, data->phy_id & 0x1f,
+					      mmd, data->reg_num, data->val_in);
+		} else
+			ret = phy->mdio_write(adapter, data->phy_id & 0x1f, 0,
+					      data->reg_num & 0x1f,
+					      data->val_in);
+		break;
+	}
 	case SIOCCHIOCTL:
-		return cxgb_extension_ioctl(dev, req->ifr_data);
+		return cxgb_extension_ioctl(dev, (void *)req->ifr_data);
 	default:
 		return -EOPNOTSUPP;
 	}
+	return ret;
 }
 
 static int cxgb_change_mtu(struct net_device *dev, int new_mtu)
 {
-	struct port_info *pi = netdev_priv(dev);
+ 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
 	int ret;
 
-	if (new_mtu < 81)	/* accommodate SACK */
+	if (new_mtu < 81)         /* accommodate SACK */
 		return -EINVAL;
 	if ((ret = t3_mac_set_mtu(&pi->mac, new_mtu)))
 		return ret;
+
 	dev->mtu = new_mtu;
 	init_port_mtus(adapter);
 	if (adapter->params.rev == 0 && offload_running(adapter))
 		t3_load_mtus(adapter, adapter->params.mtus,
 			     adapter->params.a_wnd, adapter->params.b_wnd,
 			     adapter->port[0]->mtu);
+
 	return 0;
 }
 
@@ -2520,6 +4709,7 @@
 	t3_mac_set_address(&pi->mac, 0, dev->dev_addr);
 	if (offload_running(adapter))
 		write_smt_entry(adapter, pi->port_id);
+
 	return 0;
 }
 
@@ -2538,9 +4728,10 @@
 
 	for (i = p->first_qset; i < p->first_qset + p->nqsets; i++) {
 		struct sge_rspq *q = &adap->sge.qs[i].rspq;
-
-		spin_lock_irq(&q->lock);
-		spin_unlock_irq(&q->lock);
+		unsigned long flags;
+
+		spin_lock_irqsave(&q->lock, flags);
+		spin_unlock_irqrestore(&q->lock, flags);
 	}
 }
 
@@ -2550,8 +4741,9 @@
 	struct adapter *adapter = pi->adapter;
 
 	pi->vlan_grp = grp;
+
 	if (adapter->params.rev > 0)
-		t3_set_vlan_accel(adapter, 1 << pi->port_id, grp != NULL);
+		t3_set_vlan_accel(adapter, 1 << pi->tx_chan, grp != NULL);
 	else {
 		/* single control for all ports */
 		unsigned int i, have_vlans = 0;
@@ -2563,30 +4755,32 @@
 	t3_synchronize_rx(adapter, pi);
 }
 
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+static void vlan_rx_kill_vid(struct net_device *dev, unsigned short vid)
+{
+	/* nothing */
+}
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
+
 #ifdef CONFIG_NET_POLL_CONTROLLER
 static void cxgb_netpoll(struct net_device *dev)
 {
 	struct port_info *pi = netdev_priv(dev);
 	struct adapter *adapter = pi->adapter;
+	unsigned long flags;
 	int qidx;
 
-	for (qidx = pi->first_qset; qidx < pi->first_qset + pi->nqsets; qidx++) {
-		struct sge_qset *qs = &adapter->sge.qs[qidx];
-		void *source;
-
-		if (adapter->flags & USING_MSIX)
-			source = qs;
-		else
-			source = adapter;
-
-		t3_intr_handler(adapter, qs->rspq.polling) (0, source);
-	}
+	local_irq_save(flags);
+	for (qidx = pi->first_qset; qidx < pi->first_qset + pi->nqsets; qidx++)
+		t3_poll_handler(adapter, &adapter->sge.qs[qidx]);
+	local_irq_restore(flags);
 }
 #endif
 
 /*
  * Periodic accumulation of MAC statistics.
  */
+
 static void mac_stats_update(struct adapter *adapter)
 {
 	int i;
@@ -2616,17 +4810,18 @@
 		link_fault = p->link_fault;
 		spin_unlock_irq(&adapter->work_lock);
 
-		if (link_fault) {
-			t3_link_fault(adapter, i);
-			continue;
-		}
-
-		if (!(p->phy.caps & SUPPORTED_IRQ) && netif_running(dev)) {
+		if ((link_fault || !(p->phy.caps & SUPPORTED_LINK_IRQ)) &&
+		    netif_running(dev)) {
+			/*
+			 * Disable interrupt so p->link_fault can't change out
+			 * from under us ...
+			 */
 			t3_xgm_intr_disable(adapter, i);
 			t3_read_reg(adapter, A_XGM_INT_STATUS + p->mac.offset);
 
 			t3_link_changed(adapter, i);
 			t3_xgm_intr_enable(adapter, i);
+
 		}
 	}
 }
@@ -2635,7 +4830,7 @@
 {
 	int i;
 
-	if (!rtnl_trylock())	/* synchronize with ifdown */
+	if (!rtnl_trylock())       /* synchronize with ifdown */
 		return;
 
 	for_each_port(adapter, i) {
@@ -2666,14 +4861,14 @@
 	rtnl_unlock();
 }
 
-
-static void t3_adap_check_task(struct work_struct *work)
-{
-	struct adapter *adapter = container_of(work, struct adapter,
-					       adap_check_task.work);
+extern void check_rspq_fl_status(adapter_t *adapter);
+
+DECLARE_TASK_FUNC(t3_adap_check_task, task_param)
+{
+	struct adapter *adapter = DELWORK2ADAP(task_param, adap_check_task);
 	const struct adapter_params *p = &adapter->params;
 	int port;
-	unsigned int v, status, reset;
+	unsigned int reset;
 
 	adapter->check_task_cnt++;
 
@@ -2682,18 +4877,18 @@
 	/* Accumulate MAC stats if needed */
 	if (!p->linkpoll_period ||
 	    (adapter->check_task_cnt * p->linkpoll_period) / 10 >=
-	    p->stats_update_period) {
+	     p->stats_update_period) {
 		mac_stats_update(adapter);
 		adapter->check_task_cnt = 0;
 	}
 
-	if (p->rev == T3_REV_B2)
+	if (p->rev == T3_REV_B2 && p->nports < 4)
 		check_t3b2_mac(adapter);
 
 	/*
 	 * Scan the XGMAC's to check for various conditions which we want to
 	 * monitor in a periodic polling manner rather than via an interrupt
-	 * condition.  This is used for conditions which would otherwise flood
+	 * condition.  This is used for condions which would otherwise flood
 	 * the system with interrupts and we only really need to know that the
 	 * conditions are "happening" ...  For each condition we count the
 	 * detection of the condition and reset it for the next polling loop.
@@ -2702,6 +4897,9 @@
 		struct cmac *mac =  &adap2pinfo(adapter, port)->mac;
 		u32 cause;
 
+		if (mac->multiport)
+			continue;
+
 		cause = t3_read_reg(adapter, A_XGM_INT_CAUSE + mac->offset);
 		reset = 0;
 		if (cause & F_RXFIFO_OVERFLOW) {
@@ -2712,31 +4910,7 @@
 		t3_write_reg(adapter, A_XGM_INT_CAUSE + mac->offset, reset);
 	}
 
-	/*
-	 * We do the same as above for FL_EMPTY interrupts.
-	 */
-	status = t3_read_reg(adapter, A_SG_INT_CAUSE);
-	reset = 0;
-
-	if (status & F_FLEMPTY) {
-		struct sge_qset *qs = &adapter->sge.qs[0];
-		int i = 0;
-
-		reset |= F_FLEMPTY;
-
-		v = (t3_read_reg(adapter, A_SG_RSPQ_FL_STATUS) >> S_FL0EMPTY) &
-		    0xffff;
-
-		while (v) {
-			qs->fl[i].empty += (v & 1);
-			if (i)
-				qs++;
-			i ^= 1;
-			v >>= 1;
-		}
-	}
-
-	t3_write_reg(adapter, A_SG_INT_CAUSE, reset);
+	check_rspq_fl_status(adapter);
 
 	/* Schedule the next check update if any port is active. */
 	spin_lock_irq(&adapter->work_lock);
@@ -2745,39 +4919,75 @@
 	spin_unlock_irq(&adapter->work_lock);
 }
 
+DECLARE_TASK_FUNC(db_full_task, task_param)
+{
+	struct adapter *adapter = WORK2ADAP(task_param, db_full_task);
+
+	cxgb3_err_notify(&adapter->tdev, OFFLOAD_DB_FULL, 0);
+}
+
+DECLARE_TASK_FUNC(db_empty_task, task_param)
+{
+	struct adapter *adapter = WORK2ADAP(task_param, db_empty_task);
+
+	cxgb3_err_notify(&adapter->tdev, OFFLOAD_DB_EMPTY, 0);
+}
+
+DECLARE_TASK_FUNC(db_drop_task, task_param)
+{
+	struct adapter *adapter = WORK2ADAP(task_param, db_drop_task);
+	unsigned long delay = 1000;
+	unsigned short r;
+
+	cxgb3_err_notify(&adapter->tdev, OFFLOAD_DB_DROP, 0);
+
+	/*
+	 * Sleep a while before ringing the driver qset dbs.
+	 * The delay is between 1000-2023 usecs.
+	 */
+	get_random_bytes(&r, 2);
+	delay += r & 1023;
+	set_current_state(TASK_UNINTERRUPTIBLE);
+	schedule_timeout(usecs_to_jiffies(delay));
+	ring_dbs(adapter);
+}
+
 /*
  * Processes external (PHY) interrupts in process context.
  */
-static void ext_intr_task(struct work_struct *work)
-{
-	struct adapter *adapter = container_of(work, struct adapter,
-					       ext_intr_handler_task);
+DECLARE_TASK_FUNC(ext_intr_task, task_param)
+{
+	struct adapter *adapter = WORK2ADAP(task_param, ext_intr_handler_task);
+	unsigned long flags;
 	int i;
-
+	
 	/* Disable link fault interrupts */
-	for_each_port(adapter, i) {
-		struct net_device *dev = adapter->port[i];
-		struct port_info *p = netdev_priv(dev);
-
-		t3_xgm_intr_disable(adapter, i);
-		t3_read_reg(adapter, A_XGM_INT_STATUS + p->mac.offset);
+	if (adapter->params.nports < 4) {
+		for_each_port(adapter, i) {
+			struct net_device *dev = adapter->port[i];
+			struct port_info *p = netdev_priv(dev);
+
+			t3_xgm_intr_disable(adapter, i);
+			t3_read_reg(adapter, A_XGM_INT_STATUS + p->mac.offset);
+		}
 	}
+	t3_phy_intr_handler(adapter);
 
 	/* Re-enable link fault interrupts */
-	t3_phy_intr_handler(adapter);
-
-	for_each_port(adapter, i)
-		t3_xgm_intr_enable(adapter, i);
+	if (adapter->params.nports < 4) {
+		for_each_port(adapter, i)
+			t3_xgm_intr_enable(adapter, i);
+	}
 
 	/* Now reenable external interrupts */
-	spin_lock_irq(&adapter->work_lock);
+	spin_lock_irqsave(&adapter->work_lock, flags);
 	if (adapter->slow_intr_mask) {
 		adapter->slow_intr_mask |= F_T3DBG;
 		t3_write_reg(adapter, A_PL_INT_CAUSE0, F_T3DBG);
 		t3_write_reg(adapter, A_PL_INT_ENABLE0,
 			     adapter->slow_intr_mask);
 	}
-	spin_unlock_irq(&adapter->work_lock);
+	spin_unlock_irqrestore(&adapter->work_lock, flags);
 }
 
 /*
@@ -2807,17 +5017,17 @@
 	struct port_info *pi = netdev_priv(netdev);
 
 	spin_lock(&adapter->work_lock);
-	pi->link_fault = 1;
+	pi->link_fault = LF_MAYBE;
 	spin_unlock(&adapter->work_lock);
 }
 
-static int t3_adapter_error(struct adapter *adapter, int reset)
+static int t3_adapter_error(struct adapter *adapter, int reset, int on_wq)
 {
 	int i, ret = 0;
 
 	if (is_offload(adapter) &&
 	    test_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map)) {
-		cxgb3_event_notify(&adapter->tdev, OFFLOAD_STATUS_DOWN, 0);
+		cxgb3_err_notify(&adapter->tdev, OFFLOAD_STATUS_DOWN, 0);
 		offload_close(&adapter->tdev);
 	}
 
@@ -2826,7 +5036,7 @@
 		struct net_device *netdev = adapter->port[i];
 
 		if (netif_running(netdev))
-			cxgb_close(netdev);
+			__cxgb_close(netdev, on_wq);
 	}
 
 	/* Stop SGE timers */
@@ -2850,13 +5060,12 @@
 		goto err;
 	}
 	pci_set_master(adapter->pdev);
-	pci_restore_state(adapter->pdev);
-	pci_save_state(adapter->pdev);
+	t3_os_pci_restore_state(adapter);
 
 	/* Free sge resources */
 	t3_free_sge_resources(adapter);
 
-	if (t3_replay_prep_adapter(adapter))
+	if (t3_reinit_adapter(adapter))
 		goto err;
 
 	return 0;
@@ -2883,33 +5092,30 @@
 	}
 
 	if (is_offload(adapter) && !ofld_disable)
-		cxgb3_event_notify(&adapter->tdev, OFFLOAD_STATUS_UP, 0);
-}
-
-/*
- * processes a fatal error.
- * Bring the ports down, reset the chip, bring the ports back up.
- */
-static void fatal_error_task(struct work_struct *work)
-{
-	struct adapter *adapter = container_of(work, struct adapter,
-					       fatal_error_handler_task);
+		cxgb3_err_notify(&adapter->tdev, OFFLOAD_STATUS_UP, 0);
+}
+
+DECLARE_TASK_FUNC(fatal_error_task, task_param)
+{
+        struct adapter *adapter = WORK2ADAP(task_param, fatal_error_handler_task);
 	int err = 0;
 
 	rtnl_lock();
-	err = t3_adapter_error(adapter, 1);
-	if (!err)
-		err = t3_reenable_adapter(adapter);
-	if (!err)
-		t3_resume_ports(adapter);
+	if (t3_adapter_error(adapter, 1, 1))
+		err = 1;
+	else if (t3_reenable_adapter(adapter))
+		err = 1;
+	else t3_resume_ports(adapter);
 
 	CH_ALERT(adapter, "adapter reset %s\n", err ? "failed" : "succeeded");
 	rtnl_unlock();
+
 }
 
 void t3_fatal_err(struct adapter *adapter)
 {
 	unsigned int fw_status[4];
+	static int retries = 0;
 
 	if (adapter->flags & FULL_INIT_DONE) {
 		t3_sge_stop(adapter);
@@ -2920,16 +5126,20 @@
 
 		spin_lock(&adapter->work_lock);
 		t3_intr_disable(adapter);
-		queue_work(cxgb3_wq, &adapter->fatal_error_handler_task);
+
+		if (++retries < 5)
+			queue_work(cxgb3_wq, &adapter->fatal_error_handler_task);
+
 		spin_unlock(&adapter->work_lock);
 	}
-	CH_ALERT(adapter, "encountered fatal error, operation suspended\n");
+	CH_ALERT(adapter, "encountered fatal error #%d, operation suspended\n", retries);
 	if (!t3_cim_ctl_blk_read(adapter, 0xa0, 4, fw_status))
 		CH_ALERT(adapter, "FW status: 0x%x, 0x%x, 0x%x, 0x%x\n",
 			 fw_status[0], fw_status[1],
 			 fw_status[2], fw_status[3]);
 }
 
+#if defined(HAS_EEH)
 /**
  * t3_io_error_detected - called when PCI error is detected
  * @pdev: Pointer to PCI device
@@ -2944,10 +5154,7 @@
 	struct adapter *adapter = pci_get_drvdata(pdev);
 	int ret;
 
-	if (state == pci_channel_io_perm_failure)
-		return PCI_ERS_RESULT_DISCONNECT;
-
-	ret = t3_adapter_error(adapter, 0);
+	ret = t3_adapter_error(adapter, 0, 0);
 
 	/* Request a slot reset. */
 	return PCI_ERS_RESULT_NEED_RESET;
@@ -2980,9 +5187,6 @@
 {
 	struct adapter *adapter = pci_get_drvdata(pdev);
 
-	CH_ALERT(adapter, "adapter recovering, PEX ERR 0x%x\n",
-		 t3_read_reg(adapter, A_PCIE_PEX_ERR));
-
 	t3_resume_ports(adapter);
 }
 
@@ -2991,27 +5195,30 @@
 	.slot_reset = t3_io_slot_reset,
 	.resume = t3_io_resume,
 };
-
-/*
- * Set the number of qsets based on the number of CPUs and the number of ports,
+#endif
+
+/* Set the number of qsets based on the number of CPUs and the number of ports,
  * not to exceed the number of available qsets, assuming there are enough qsets
  * per port in HW.
  */
-static void set_nqsets(struct adapter *adap)
+static inline void set_nqsets(struct adapter *adap)
 {
 	int i, j = 0;
 	int num_cpus = num_online_cpus();
 	int hwports = adap->params.nports;
 	int nqsets = adap->msix_nvectors - 1;
 
-	if (adap->params.rev > 0 && adap->flags & USING_MSIX) {
+	if (!(adap->flags & USING_MSIX)) {
+		/* for now, only support 1 queue set/port in non-MSIX mode */
+		nqsets = 1;
+	} else if (adap->params.rev > 0 && !singleq && hwports <= 2) {
 		if (hwports == 2 &&
 		    (hwports * nqsets > SGE_QSETS ||
-		     num_cpus >= nqsets / hwports))
+		     num_cpus >= nqsets/hwports))
 			nqsets /= hwports;
 		if (nqsets > num_cpus)
 			nqsets = num_cpus;
-		if (nqsets < 1 || hwports == 4)
+		if (nqsets < 1)
 			nqsets = 1;
 	} else
 		nqsets = 1;
@@ -3021,11 +5228,42 @@
 
 		pi->first_qset = j;
 		pi->nqsets = nqsets;
-		j = pi->first_qset + nqsets;
+		j += nqsets;
 
 		dev_info(&adap->pdev->dev,
 			 "Port %d using %d queue sets.\n", i, nqsets);
 	}
+
+	adap->sge.nqsets = j;
+}
+
+static void __devinit check_msi(struct adapter *adap)
+{
+	int vec, mi1;
+
+	if (!(t3_read_reg(adap, A_PL_INT_CAUSE0) & F_MI1))
+		return;
+
+	vec = (adap->flags & USING_MSI) ? adap->pdev->irq :
+					  adap->msix_info[0].vec;
+
+	if (request_irq(vec, check_intr_handler, 0, adap->name, adap))
+		return;
+
+	t3_set_reg_field(adap, A_PL_INT_ENABLE0, 0, F_MI1);
+	msleep(10);
+	mi1 = t3_read_reg(adap, A_PL_INT_ENABLE0) & F_MI1;
+	if (mi1)
+		t3_set_reg_field(adap, A_PL_INT_ENABLE0, F_MI1, 0);
+	free_irq(vec, adap);
+
+	if (mi1) {
+		cxgb_disable_msi(adap);
+		dev_info(&adap->pdev->dev,
+			 "the kernel believes that MSI is available on this "
+			 "platform\nbut the driver's MSI test has failed.  "
+			 "Proceeding with INTx interrupts.\n");
+	}
 }
 
 static int __devinit cxgb_enable_msix(struct adapter *adap)
@@ -3058,7 +5296,31 @@
 	return err;
 }
 
-static void __devinit print_port_info(struct adapter *adap,
+#ifdef T3_TRACE
+static void __devinit alloc_trace_bufs(adapter_t *adap)
+{
+	int i;
+	char s[32];
+
+	for (i = 0; i < SGE_QSETS; ++i) {
+		sprintf(s, "sge_q%d", i);
+		adap->tb[i] = t3_trace_alloc(adap->debugfs_root, s, 512);
+	}
+}
+
+static void free_trace_bufs(adapter_t *adap)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(adap->tb); ++i)
+		t3_trace_free(adap->tb[i]);
+}
+#else
+# define alloc_trace_bufs(adapter)
+# define free_trace_bufs(adapter)
+#endif
+
+static void __devinit print_port_info(adapter_t *adap,
 				      const struct adapter_info *ai)
 {
 	static const char *pci_variant[] = {
@@ -3082,37 +5344,124 @@
 		const struct port_info *pi = netdev_priv(dev);
 
 		if (!test_bit(i, &adap->registered_device_map))
-			continue;
+		       continue;
 		printk(KERN_INFO "%s: %s %s %sNIC (rev %d) %s%s\n",
 		       dev->name, ai->desc, pi->phy.desc,
 		       is_offload(adap) ? "R" : "", adap->params.rev, buf,
 		       (adap->flags & USING_MSIX) ? " MSI-X" :
 		       (adap->flags & USING_MSI) ? " MSI" : "");
-		if (adap->name == dev->name && adap->params.vpd.mclk)
+		if (adap->name == dev->name && adap->params.vpd.mclk) {
 			printk(KERN_INFO
-			       "%s: %uMB CM, %uMB PMTX, %uMB PMRX, S/N: %s\n",
+			       "%s: %uMB CM, %uMB PMTX, %uMB PMRX\n",
 			       adap->name, t3_mc7_size(&adap->cm) >> 20,
 			       t3_mc7_size(&adap->pmtx) >> 20,
-			       t3_mc7_size(&adap->pmrx) >> 20,
-			       adap->params.vpd.sn);
+			       t3_mc7_size(&adap->pmrx) >> 20);
+			printk(KERN_INFO
+			       "%s: S/N: %s E/C: %s\n",
+			       adap->name, adap->params.vpd.sn,
+			       adap->params.vpd.ec);
+			}
 	}
 }
 
+static void touch_bars(struct pci_dev *pdev)
+{
+#if BITS_PER_LONG < 64
+	u32 v;
+
+	pci_read_config_dword(pdev, PCI_BASE_ADDRESS_1, &v);
+	pci_write_config_dword(pdev, PCI_BASE_ADDRESS_1, v);
+	pci_read_config_dword(pdev, PCI_BASE_ADDRESS_3, &v);
+	pci_write_config_dword(pdev, PCI_BASE_ADDRESS_3, v);
+	pci_read_config_dword(pdev, PCI_BASE_ADDRESS_5, &v);
+	pci_write_config_dword(pdev, PCI_BASE_ADDRESS_5, v);
+#endif
+}
+
+#define VLAN_FEAT (NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO | NETIF_F_TSO6 |\
+		   NETIF_F_IPV6_CSUM | NETIF_F_HIGHDMA)
+
+#if defined(HAVE_NET_DEVICE_OPS)
 static const struct net_device_ops cxgb_netdev_ops = {
-	.ndo_open		= cxgb_open,
-	.ndo_stop		= cxgb_close,
-	.ndo_start_xmit		= t3_eth_xmit,
-	.ndo_get_stats		= cxgb_get_stats,
-	.ndo_validate_addr	= eth_validate_addr,
-	.ndo_set_multicast_list	= cxgb_set_rxmode,
-	.ndo_do_ioctl		= cxgb_ioctl,
-	.ndo_change_mtu		= cxgb_change_mtu,
-	.ndo_set_mac_address	= cxgb_set_mac_addr,
-	.ndo_vlan_rx_register	= vlan_rx_register,
+        .ndo_open               = cxgb_open,
+        .ndo_stop               = cxgb_close,
+        .ndo_start_xmit         = t3_eth_xmit,
+        .ndo_get_stats          = cxgb_get_stats,
+        .ndo_validate_addr      = eth_validate_addr,
+        .ndo_set_multicast_list = cxgb_set_rxmode,
+        .ndo_do_ioctl           = cxgb_ioctl,
+        .ndo_change_mtu         = cxgb_change_mtu,
+        .ndo_set_mac_address    = cxgb_set_mac_addr,
+        .ndo_vlan_rx_register   = vlan_rx_register,
 #ifdef CONFIG_NET_POLL_CONTROLLER
-	.ndo_poll_controller	= cxgb_netpoll,
+        .ndo_poll_controller    = cxgb_netpoll,
 #endif
 };
+#endif /* HAVE_NET_DEVICE_OPS */
+
+static struct proc_dir_entry *cxgb3_proc_root;
+
+static void cxgb_proc_remove(void)
+{
+	remove_proc_entry("devices", cxgb3_proc_root);
+	remove_proc_entry("cxgb3", INET_PROC_DIR);
+	cxgb3_proc_root = NULL;
+}
+
+static int cxgb_proc_init(void)
+{
+	struct proc_dir_entry *d;
+
+	cxgb3_proc_root = proc_mkdir("cxgb3", INET_PROC_DIR);
+	if (!cxgb3_proc_root)
+		return -ENOMEM;
+	SET_PROC_NODE_OWNER(cxgb3_proc_root, THIS_MODULE);
+
+	d = create_proc_read_entry("devices", 0, cxgb3_proc_root,
+				   offload_devices_read_proc, NULL);
+
+	if (!d)
+		goto cleanup;
+	SET_PROC_NODE_OWNER(d, THIS_MODULE);
+	return 0;
+
+cleanup:
+	cxgb_proc_remove();
+	return -ENOMEM;
+}
+
+static int oflddev_idx = 0, nicdev_idx = 0;
+
+static void __devinit cxgb_proc_dev_init(struct adapter *adapter)
+{
+	struct t3cdev *tdev = &adapter->tdev;
+
+	if (!cxgb3_proc_root) {
+		printk("%s: root proc dir is null\n", __func__);
+		return;
+	}
+
+	if (is_offload(adapter))
+		snprintf(tdev->name, sizeof(tdev->name), "ofld_dev%d", 
+			 oflddev_idx++);
+	else
+		snprintf(tdev->name, sizeof(tdev->name), "nic_dev%d",
+			 nicdev_idx++);
+
+	tdev->proc_dir = proc_mkdir(tdev->name, cxgb3_proc_root);
+	if (!tdev->proc_dir) {
+		printk(KERN_WARNING "Unable to create /proc/net/cxgb3/%s dir\n",
+		       tdev->name);
+		return;
+	}
+	SET_PROC_NODE_OWNER(tdev->proc_dir, THIS_MODULE);
+}
+
+static void __devexit cxgb_proc_dev_exit(struct t3cdev *tdev)
+{
+	remove_proc_entry(tdev->name, cxgb3_proc_root);
+	tdev->proc_dir = NULL;
+}
 
 static int __devinit init_one(struct pci_dev *pdev,
 			      const struct pci_device_id *ent)
@@ -3120,36 +5469,45 @@
 	static int version_printed;
 
 	int i, err, pci_using_dac = 0;
-	resource_size_t mmio_start, mmio_len;
+	unsigned long mmio_start, mmio_len;
 	const struct adapter_info *ai;
 	struct adapter *adapter = NULL;
-	struct port_info *pi;
 
 	if (!version_printed) {
-		printk(KERN_INFO "%s - version %s\n", DRV_DESC, DRV_VERSION);
+		printk(KERN_INFO "%s - version %s\n", DRIVER_DESC, DRIVER_VERSION);
 		++version_printed;
 	}
 
 	if (!cxgb3_wq) {
-		cxgb3_wq = create_singlethread_workqueue(DRV_NAME);
+		cxgb3_wq = create_singlethread_workqueue(DRIVER_NAME);
 		if (!cxgb3_wq) {
-			printk(KERN_ERR DRV_NAME
+			printk(KERN_ERR DRIVER_NAME
 			       ": cannot initialize work queue\n");
 			return -ENOMEM;
 		}
 	}
 
-	err = pci_request_regions(pdev, DRV_NAME);
-	if (err) {
-		/* Just info, some other driver may have claimed the device. */
-		dev_info(&pdev->dev, "cannot obtain PCI resources\n");
-		return err;
-	}
-
 	err = pci_enable_device(pdev);
 	if (err) {
 		dev_err(&pdev->dev, "cannot enable PCI device\n");
-		goto out_release_regions;
+		return err;
+	}
+
+	/*
+	 * Can't use pci_request_regions() here because some kernels want to
+	 * request the MSI-X BAR in pci_enable_msix.  Also no need to request
+	 * the doorbell BAR if we are not doing user-space RDMA.
+	 * So only request BAR0.
+	 */
+	err = pci_request_region(pdev, 0, DRIVER_NAME);
+	if (err) {
+		/*
+		 * Some other driver may have already claimed the device.
+		 * Report the event but do not disable the device.
+		 */
+		printk(KERN_INFO "%s: cannot obtain PCI resources\n",
+		       pci_name(pdev));
+		return err;
 	}
 
 	if (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {
@@ -3158,15 +5516,15 @@
 		if (err) {
 			dev_err(&pdev->dev, "unable to obtain 64-bit DMA for "
 			       "coherent allocations\n");
-			goto out_disable_device;
+			goto out_release_regions;
 		}
 	} else if ((err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) != 0) {
 		dev_err(&pdev->dev, "no usable DMA configuration\n");
-		goto out_disable_device;
+		goto out_release_regions;
 	}
 
+	touch_bars(pdev);
 	pci_set_master(pdev);
-	pci_save_state(pdev);
 
 	mmio_start = pci_resource_start(pdev, 0);
 	mmio_len = pci_resource_len(pdev, 0);
@@ -3175,68 +5533,116 @@
 	adapter = kzalloc(sizeof(*adapter), GFP_KERNEL);
 	if (!adapter) {
 		err = -ENOMEM;
-		goto out_disable_device;
+		goto out_release_regions;
 	}
 
-	adapter->nofail_skb =
-		alloc_skb(sizeof(struct cpl_set_tcb_field), GFP_KERNEL);
-	if (!adapter->nofail_skb) {
-		dev_err(&pdev->dev, "cannot allocate nofail buffer\n");
+	adapter->pdev = pdev;
+	t3_os_pci_save_state(adapter);
+
+	adapter->regs = ioremap_nocache(mmio_start, mmio_len);
+	if (!adapter->regs) {
+		dev_err(&pdev->dev,
+			"cannot map device registers\n");
 		err = -ENOMEM;
 		goto out_free_adapter;
 	}
 
-	adapter->regs = ioremap_nocache(mmio_start, mmio_len);
-	if (!adapter->regs) {
-		dev_err(&pdev->dev, "cannot map device registers\n");
-		err = -ENOMEM;
-		goto out_free_adapter;
-	}
-
-	adapter->pdev = pdev;
 	adapter->name = pci_name(pdev);
 	adapter->msg_enable = dflt_msg_enable;
 	adapter->mmio_len = mmio_len;
-
-	mutex_init(&adapter->mdio_lock);
+	atomic_set(&adapter->filter_toe_mode, CXGB3_FTM_NONE);
+	memset(adapter->rrss_map, 0xff, sizeof(adapter->rrss_map));
+	INIT_LIST_HEAD(&adapter->adapter_list);
+	spin_lock_init(&adapter->mdio_lock);
+	spin_lock_init(&adapter->elmer_lock);
 	spin_lock_init(&adapter->work_lock);
 	spin_lock_init(&adapter->stats_lock);
 
-	INIT_LIST_HEAD(&adapter->adapter_list);
-	INIT_WORK(&adapter->ext_intr_handler_task, ext_intr_task);
-	INIT_WORK(&adapter->fatal_error_handler_task, fatal_error_task);
-	INIT_DELAYED_WORK(&adapter->adap_check_task, t3_adap_check_task);
+	T3_INIT_WORK(&adapter->ext_intr_handler_task,
+			ext_intr_task, adapter);
+        T3_INIT_WORK(&adapter->fatal_error_handler_task,
+                        fatal_error_task, adapter);
+
+	T3_INIT_WORK(&adapter->db_full_task, db_full_task, adapter);
+	T3_INIT_WORK(&adapter->db_empty_task, db_empty_task, adapter);
+	T3_INIT_WORK(&adapter->db_drop_task, db_drop_task, adapter);
+
+	T3_INIT_DELAYED_WORK(&adapter->adap_check_task,
+				t3_adap_check_task,
+				adapter);
+	init_timer(&adapter->watchdog_timer);
 
 	for (i = 0; i < ai->nports0 + ai->nports1; ++i) {
 		struct net_device *netdev;
-
-		netdev = alloc_etherdev_mq(sizeof(struct port_info), SGE_QSETS);
+		struct port_info *pi;
+
+		netdev = alloc_etherdev_mq(sizeof(struct port_info),
+					   SGE_QSETS);
 		if (!netdev) {
 			err = -ENOMEM;
 			goto out_free_dev;
 		}
 
+		SET_MODULE_OWNER(netdev);
 		SET_NETDEV_DEV(netdev, &pdev->dev);
 
 		adapter->port[i] = netdev;
 		pi = netdev_priv(netdev);
 		pi->adapter = adapter;
-		pi->rx_offload = T3_RX_CSUM | T3_LRO;
+		pi->rx_csum_offload = 1;
 		pi->port_id = i;
+		pi->tx_chan = i >= ai->nports0;
+		pi->txpkt_intf = pi->tx_chan ? 2 * (i - ai->nports0) + 1 :
+					       2 * i;
+		pi->iscsi_ipv4addr = 0;
+		pi->sched_min = 50;
+		pi->sched_max = 100;
+		adapter->rxpkt_map[pi->txpkt_intf] = i;
 		netif_carrier_off(netdev);
-		netif_tx_stop_all_queues(netdev);
 		netdev->irq = pdev->irq;
 		netdev->mem_start = mmio_start;
 		netdev->mem_end = mmio_start + mmio_len - 1;
-		netdev->features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO;
-		netdev->features |= NETIF_F_GRO;
+		netdev->features |= NETIF_F_SG | NETIF_F_IP_CSUM;
+		netdev->features |= NETIF_F_LLTX;
 		if (pci_using_dac)
 			netdev->features |= NETIF_F_HIGHDMA;
 
+		if (ai->nports0 + ai->nports1 <= 2)	// disable TSO on T304
+			netdev->features |= NETIF_F_TSO;
+
 		netdev->features |= NETIF_F_HW_VLAN_TX | NETIF_F_HW_VLAN_RX;
+#if defined(HAVE_NET_DEVICE_OPS)
 		netdev->netdev_ops = &cxgb_netdev_ops;
+#else
+		netdev->vlan_rx_register = vlan_rx_register;
+		netdev->vlan_rx_kill_vid = vlan_rx_kill_vid;
+
+		netdev->open = cxgb_open;
+		netdev->stop = cxgb_close;
+		netdev->hard_start_xmit = t3_eth_xmit;
+		netdev->tx_queue_len = 10000;
+		netdev->get_stats = cxgb_get_stats;
+		netdev->set_multicast_list = cxgb_set_rxmode;
+		netdev->do_ioctl = cxgb_ioctl;
+		netdev->change_mtu = cxgb_change_mtu;
+		netdev->set_mac_address = cxgb_set_mac_addr;
+#ifdef CONFIG_NET_POLL_CONTROLLER
+		netdev->poll_controller = cxgb_netpoll;
+#endif
+#endif /* HAVE_NET_DEVICE_OPS */
+
+#if !defined(NAPI_UPDATE)
+		netdev->weight = 64;
+#endif
 		SET_ETHTOOL_OPS(netdev, &cxgb_ethtool_ops);
+
+#ifdef GSO_MAX_SIZE
+		netdev->vlan_features = netdev->features & VLAN_FEAT;
+		if (adapter->params.nports > 2)
+			netif_set_gso_max_size(netdev, 32768);
+#endif
 	}
+	adapter->sge.nqsets = i;
 
 	pci_set_drvdata(pdev, adapter);
 	if (t3_prep_adapter(adapter, ai, 1) < 0) {
@@ -3244,6 +5650,21 @@
 		goto out_free_dev;
 	}
 
+	/* See what interrupts we'll be using */
+	if (msi > 1 && cxgb_enable_msix(adapter) == 0)
+		adapter->flags |= USING_MSIX;
+	else if (msi > 0 && pci_enable_msi(pdev) == 0)
+		adapter->flags |= USING_MSI;
+	if (adapter->flags & (USING_MSIX | USING_MSI))
+		check_msi(adapter);
+
+	/*
+	 * We need to determine how many queues we're planning on using (by
+	 * default) before we register the network devices.  These can be
+	 * changed later via our CHELSIO_SET_QSET_NUM ioctl() ...
+	 */
+	set_nqsets(adapter);
+
 	/*
 	 * The card is now ready to go.  If any errors occur during device
 	 * registration we do not fail the whole card but rather proceed only
@@ -3251,6 +5672,11 @@
 	 * register at least one net device.
 	 */
 	for_each_port(adapter, i) {
+		struct net_device *netdev = adapter->port[i];
+		struct port_info *pi = netdev_priv(netdev);
+
+		t3_compat_set_num_tx_queues(netdev, pi->nqsets);
+
 		err = register_netdev(adapter->port[i]);
 		if (err)
 			dev_warn(&pdev->dev,
@@ -3275,23 +5701,30 @@
 	/* Driver's ready. Reflect it on LEDs */
 	t3_led_ready(adapter);
 
+#ifndef	LINUX_2_4
+	if (cxgb3_debugfs_root) {
+		adapter->debugfs_root = debugfs_create_dir(adapter->name,
+							   cxgb3_debugfs_root);
+		if (adapter->debugfs_root)
+			alloc_trace_bufs(adapter);
+	}
+#endif	/* LINUX_2_4 */
+	cxgb_proc_dev_init(adapter);
+	cxgb_proc_setup(adapter, adapter->tdev.proc_dir);
+
 	if (is_offload(adapter)) {
 		__set_bit(OFFLOAD_DEVMAP_BIT, &adapter->registered_device_map);
 		cxgb3_adapter_ofld(adapter);
 	}
 
-	/* See what interrupts we'll be using */
-	if (msi > 1 && cxgb_enable_msix(adapter) == 0)
-		adapter->flags |= USING_MSIX;
-	else if (msi > 0 && pci_enable_msi(pdev) == 0)
-		adapter->flags |= USING_MSI;
-
-	set_nqsets(adapter);
-
-	err = sysfs_create_group(&adapter->port[0]->dev.kobj,
-				 &cxgb3_attr_group);
+#ifndef	LINUX_2_4
+	if (sysfs_create_group(net2kobj(adapter->port[0]), &cxgb3_attr_group))
+		printk(KERN_INFO
+		       "%s: cannot create sysfs cxgb3_attr_group", __func__);
+#endif	/* LINUX_2_4 */
 
 	print_port_info(adapter, ai);
+	add_adapter(adapter);
 	return 0;
 
 out_free_dev:
@@ -3303,10 +5736,9 @@
 out_free_adapter:
 	kfree(adapter);
 
-out_disable_device:
+ out_release_regions:
+	pci_release_region(pdev, 0);
 	pci_disable_device(pdev);
-out_release_regions:
-	pci_release_regions(pdev);
 	pci_set_drvdata(pdev, NULL);
 	return err;
 }
@@ -3319,61 +5751,159 @@
 		int i;
 
 		t3_sge_stop(adapter);
-		sysfs_remove_group(&adapter->port[0]->dev.kobj,
+#ifndef	LINUX_2_4
+		sysfs_remove_group(net2kobj(adapter->port[0]),
 				   &cxgb3_attr_group);
+#endif	/* LINUX_2_4 */
+		cxgb_proc_cleanup(adapter->tdev.proc_dir);
 
 		if (is_offload(adapter)) {
-			cxgb3_adapter_unofld(adapter);
 			if (test_bit(OFFLOAD_DEVMAP_BIT,
 				     &adapter->open_device_map))
 				offload_close(&adapter->tdev);
+			cxgb3_adapter_unofld(adapter);
 		}
 
-		for_each_port(adapter, i)
-		    if (test_bit(i, &adapter->registered_device_map))
-			unregister_netdev(adapter->port[i]);
+		cxgb_proc_dev_exit(&adapter->tdev);
+
+		for_each_port(adapter, i) {
+			if (test_bit(i, &adapter->registered_device_map)) {
+				unregister_netdev(adapter->port[i]);
+			}
+		}
 
 		t3_stop_sge_timers(adapter);
 		t3_free_sge_resources(adapter);
+		if (adapter->filters)
+			free_mem(adapter->filters);
 		cxgb_disable_msi(adapter);
 
+		if (adapter->debugfs_root) {
+			free_trace_bufs(adapter);
+#ifndef	LINUX_2_4
+			debugfs_remove(adapter->debugfs_root);
+#endif	/* LINUX_2_4 */
+		}
+
+#if !defined(NAPI_UPDATE)
+		for (i = 0; i < ARRAY_SIZE(adapter->dummy_netdev); i++)
+			if (adapter->dummy_netdev[i]) {
+				free_netdev(adapter->dummy_netdev[i]);
+				adapter->dummy_netdev[i] = NULL;
+			}
+#endif
 		for_each_port(adapter, i)
 			if (adapter->port[i])
 				free_netdev(adapter->port[i]);
 
 		iounmap(adapter->regs);
-		if (adapter->nofail_skb)
-			kfree_skb(adapter->nofail_skb);
+		remove_adapter(adapter);
+		del_timer(&adapter->watchdog_timer);
 		kfree(adapter);
-		pci_release_regions(pdev);
+		pci_release_region(pdev, 0);
 		pci_disable_device(pdev);
 		pci_set_drvdata(pdev, NULL);
 	}
 }
 
+/*
+ * cxgb3_die_notifier_cb - Bring down the link in case of a kernel panic.
+ * @nb pointer to notifier block registered by the driver.
+ * @event event that caused the callback to be invoked.
+ * @p pointer to event related data.
+ *
+ * We need to bring down the link on each port in case of a kernel panic as 
+ * the chip sends out pause frames at 8MB/sec which causes some switches to
+ * stop working. This was observed at a customer site (bug 6661).
+ *
+ * NOTE: This function has to be atomic as we are registering an atomic
+ * notifier callback.
+ */
+static int
+cxgb3_die_notifier_cb(struct notifier_block *nb, unsigned long event,
+		                void *p)
+{
+	struct adapter *adapter;
+	int i;
+
+	if ((event == DIE_OOPS) || (event == DIE_PANIC)) {
+		list_for_each_entry(adapter, &adapter_list, adapter_list) {
+			for_each_port(adapter, i) {
+				struct net_device *netdev = adapter->port[i];
+				struct port_info *pi = netdev_priv(netdev);
+				/* Disable pause frames for all T3 ports */
+				t3_set_reg_field(adapter,
+						A_XGM_TX_CFG + pi->mac.offset,
+						F_TXPAUSEEN, 0);
+			}
+			t3_write_reg(adapter, A_T3DBG_GPIO_EN, 0);
+		}
+	}
+	return NOTIFY_OK;
+}
+
+/*
+ * Notifier block to notify cxgb3 driver of a kernel panic so that
+ * the it can take appropriate action.
+ */
+static struct notifier_block die_notifier = {
+	.notifier_call = cxgb3_die_notifier_cb,
+	.priority = 0
+};
+
+
 static struct pci_driver driver = {
-	.name = DRV_NAME,
+	.name     = DRIVER_NAME,
 	.id_table = cxgb3_pci_tbl,
-	.probe = init_one,
-	.remove = __devexit_p(remove_one),
+	.probe    = init_one,
+	.remove   = __devexit_p(remove_one),
+#if defined(HAS_EEH)
 	.err_handler = &t3_err_handler,
+#endif
+
 };
 
 static int __init cxgb3_init_module(void)
 {
 	int ret;
 
+#ifndef	LINUX_2_4
+	/* Debugfs support is optional, just warn if this fails */
+	cxgb3_debugfs_root = debugfs_create_dir(DRIVER_NAME, NULL);
+	if (!cxgb3_debugfs_root)
+		printk(KERN_WARNING DRIVER_NAME
+		       ": could not create debugfs entry, continuing\n");
+#endif	/* LINUX_2_4 */
+
 	cxgb3_offload_init();
-
+	cxgb_proc_init();
+
+	register_die_notifier(&die_notifier);
 	ret = pci_register_driver(&driver);
+
+#ifndef	LINUX_2_4
+	if (ret < 0)
+		debugfs_remove(cxgb3_debugfs_root);
+#else
+	if (ret > 0)
+		ret = 0;
+#endif	/* LINUX_2_4 */
 	return ret;
 }
 
 static void __exit cxgb3_cleanup_module(void)
 {
+	unregister_die_notifier(&die_notifier);
 	pci_unregister_driver(&driver);
-	if (cxgb3_wq)
+	if (cxgb3_wq) {
 		destroy_workqueue(cxgb3_wq);
+		cxgb3_wq = NULL;
+	}
+#ifndef	LINUX_2_4
+	debugfs_remove(cxgb3_debugfs_root);  /* NULL ok */
+#endif	/* LINUX_2_4 */
+	cxgb3_offload_exit();
+	cxgb_proc_remove();
 }
 
 module_init(cxgb3_init_module);
diff --git a/drivers/net/cxgb3/cxgb3_offload.c b/drivers/net/cxgb3/cxgb3_offload.c
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/cxgb3_offload.c
+++ b/drivers/net/cxgb3/cxgb3_offload.c
@@ -1,44 +1,27 @@
 /*
- * Copyright (c) 2006-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
 
 #include <linux/list.h>
-#include <net/neighbour.h>
 #include <linux/notifier.h>
 #include <asm/atomic.h>
 #include <linux/proc_fs.h>
 #include <linux/if_vlan.h>
-#include <net/netevent.h>
 #include <linux/highmem.h>
 #include <linux/vmalloc.h>
+#include <linux/netdevice.h>
+#include <net/neighbour.h>
+
+#if defined(CONFIG_XEN) && defined(CONFIG_XEN_TOE)
+#include <net/bridge/br_private.h>
+#endif
 
 #include "common.h"
 #include "regs.h"
@@ -49,23 +32,95 @@
 #include "firmware_exports.h"
 #include "cxgb3_offload.h"
 
+#include "cxgb3_compat.h"
+#if defined(NETEVENT)
+#include <net/netevent.h>
+#endif
+
+
+#if defined(CONFIG_TCP_OFFLOAD_MODULE)
+#if defined(BOND_SUPPORT)
+#include <drivers/net/bonding/bonding.h>
+#endif
+#include <linux/toedev.h>
+#endif
+
 static LIST_HEAD(client_list);
 static LIST_HEAD(ofld_dev_list);
 static DEFINE_MUTEX(cxgb3_db_lock);
 
-static DEFINE_RWLOCK(adapter_list_lock);
-static LIST_HEAD(adapter_list);
+/* Track # of adapters registered for offload */
+static atomic_t registered_ofld_adapters = ATOMIC_INIT(0);
 
+#ifndef RAW_NOTIFIER_HEAD
+# define RAW_NOTIFIER_HEAD(name) struct notifier_block *name
+# define raw_notifier_call_chain notifier_call_chain
+# define raw_notifier_chain_register notifier_chain_register
+# define raw_notifier_chain_unregister notifier_chain_unregister
+#endif
+
+static RAW_NOTIFIER_HEAD(offload_error_notify_list);
+static DEFINE_MUTEX(notify_mutex);
+
+int register_offload_error_notifier(struct notifier_block *nb)
+{
+        int err;
+
+        mutex_lock(&notify_mutex);
+        err = raw_notifier_chain_register(&offload_error_notify_list, nb);
+        mutex_unlock(&notify_mutex);
+        return err;
+}
+EXPORT_SYMBOL(register_offload_error_notifier);
+
+int unregister_offload_error_notifier(struct notifier_block *nb)
+{
+        int err;
+
+        mutex_lock(&notify_mutex);
+        err = raw_notifier_chain_unregister(&offload_error_notify_list, nb);
+        mutex_unlock(&notify_mutex);
+        return err;
+}
+EXPORT_SYMBOL(unregister_offload_error_notifier);
+
+#ifdef  LINUX_2_4
+static unsigned int MAX_ATIDS = 64 * 1024;
+#else
 static const unsigned int MAX_ATIDS = 64 * 1024;
+#endif  /* LINUX_2_4 */
 static const unsigned int ATID_BASE = 0x10000;
 
 static inline int offload_activated(struct t3cdev *tdev)
 {
-	const struct adapter *adapter = tdev2adap(tdev);
+	struct adapter *adapter = tdev2adap(tdev);
 
+	if (!cxgb3_filter_toe_mode(adapter, CXGB3_FTM_TOE)) {
+		int i;
+		printk(KERN_WARNING "Offload services disabled for adapter %s:"
+		       " filters in use; ports:\n", tdev->name);
+		for_each_port(adapter, i) {
+			struct net_device *dev = adapter->port[i];
+			printk(KERN_WARNING "    %d: %s\n", i, dev->name);
+		}
+		return 0;
+	}
 	return (test_bit(OFFLOAD_DEVMAP_BIT, &adapter->open_device_map));
 }
 
+int offload_error_notification(struct net_device *netdev, unsigned long error)
+{
+        struct t3cdev *tdev = dev2t3cdev(netdev);
+
+        if (offload_activated(tdev)) {
+                mutex_lock(&notify_mutex);
+                raw_notifier_call_chain(&offload_error_notify_list, error, tdev);
+                mutex_unlock(&notify_mutex);
+        }
+        return 0;
+}
+EXPORT_SYMBOL(offload_error_notification);
+
 /**
  *	cxgb3_register_client - register an offload client
  *	@client: the client
@@ -88,7 +143,6 @@
 	}
 	mutex_unlock(&cxgb3_db_lock);
 }
-
 EXPORT_SYMBOL(cxgb3_register_client);
 
 /**
@@ -113,11 +167,19 @@
 	}
 	mutex_unlock(&cxgb3_db_lock);
 }
-
 EXPORT_SYMBOL(cxgb3_unregister_client);
 
+/* Get the t3cdev associated with a net_device */
+struct t3cdev *dev2t3cdev(struct net_device *dev)
+{
+	const struct port_info *pi = netdev_priv(dev);
+
+	return (struct t3cdev *)pi->adapter;
+}
+EXPORT_SYMBOL(dev2t3cdev);
+
 /**
- *	cxgb3_add_clients - activate registered clients for an offload device
+ *	cxgb3_add_clients - activate register clients for an offload device
  *	@tdev: the offload device
  *
  *	Call backs all registered clients once a offload device is activated
@@ -135,8 +197,7 @@
 }
 
 /**
- *	cxgb3_remove_clients - deactivates registered clients
- *			       for an offload device
+ *	cxgb3_remove_clients - activate register clients for an offload device
  *	@tdev: the offload device
  *
  *	Call backs all registered clients once a offload device is deactivated
@@ -153,19 +214,155 @@
 	mutex_unlock(&cxgb3_db_lock);
 }
 
-void cxgb3_event_notify(struct t3cdev *tdev, u32 event, u32 port)
+/**
+ *	cxgb3_err_notify - notifies a device failure to the registered clients
+ *	@tdev: the offload device
+ *	@status: H/W status: up or down
+ *	@error: error identifier
+ *
+ *	Call backs all registered clients if the ASIC gets reset on a fatal error
+ */
+void cxgb3_err_notify(struct t3cdev *tdev, u32 status, u32 error)
 {
 	struct cxgb3_client *client;
 
 	mutex_lock(&cxgb3_db_lock);
 	list_for_each_entry(client, &client_list, client_list) {
-		if (client->event_handler)
-			client->event_handler(tdev, event, port);
+		/*
+		 * restricted to TOM at this point,
+		 * until iSCSI and iWARP catch up
+		 */
+		if (client->name && strcmp(client->name, "tom_cxgb3") == 0 &&
+		    client->event_handler)
+			client->event_handler(tdev, status, error);
 	}
 	mutex_unlock(&cxgb3_db_lock);
 }
 
-static struct net_device *get_iff_from_mac(struct adapter *adapter,
+#if defined(CONFIG_XEN) && defined(CONFIG_XEN_TOE)
+/**
+ *	is_vif - return TRUE if a device is a Xen virtual interface (VIF)
+ *	@dev: the device to test for VIF status ...
+ *
+ *	N.B. Xen virtual interfaces (VIFs) have a few distinguishing
+ *	features that we can use to try to determine whether we're
+ *	looking at one.  Unfortunately there's noting _really_ defined
+ *	for them so this is just a hueristic and we probably ought to
+ *	think about a better predicate.  For right now we look for a
+ *	name of "vif*" and a MAC address of fe:ff:ff:ff:ff:ff ...
+ */
+static int is_vif(struct net_device *dev)
+{
+	const char vifname[3] = "vif";
+	const char vifmac[ETH_ALEN] = { 0xfe, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	return (memcmp(dev->name, vifname, sizeof(vifname)) == 0 &&
+		memcmp(dev->dev_addr, vifmac, ETH_ALEN) == 0);
+}
+
+/**
+ *	is_xenbrpif - return TRUE if we have the pysical interface (PIF)
+ *	for a Xen bridge (XENBR)
+ *
+ *	@xenbr: the Xen bridge net device
+ *	@pif: the physical interface net device
+ *
+ *	Search a Xen bridge's port interface list for the specified
+ *	physical interface (PIF).  Return TRUE if found, FALSE
+ *	otherwise.  There should be only a single PIF in a Xen bridge;
+ *	if we find more than one we're not looking at a standard Xen
+ *	bridge used to proxy for a PIF and we return FALSE.
+ */
+static int is_xenbrpif(struct net_device *xenbr,
+		       struct net_device *pif)
+{
+	struct net_bridge *br = netdev_priv(xenbr);
+	struct net_bridge_port *port;
+	
+	list_for_each_entry(port, &br->port_list, list) {
+		struct net_device *portdev = port->dev;
+		if (!is_vif(portdev))
+			return (portdev == pif);
+	}
+	return 0;
+}
+
+struct net_device *get_xenbrpif(struct net_device *xenbr) {
+
+	struct net_bridge *br = netdev_priv(xenbr);
+	struct net_device *pif = NULL;
+	struct net_bridge_port *port;
+	
+	list_for_each_entry(port, &br->port_list, list) {
+		struct net_device *portdev = port->dev;
+		if (!is_vif(portdev)) {
+			if (pif)
+				return NULL;
+			pif = portdev;
+		}
+	}
+	return pif;
+}
+#endif
+
+#if defined(NETEVENT) || defined(OFLD_USE_KPROBES)
+static struct t3cdev * dev2tdev(struct net_device *root_dev)
+{
+#if defined(CONFIG_TCP_OFFLOAD_MODULE)
+	struct adapter *adapter;
+#if defined(BOND_SUPPORT)
+	struct bonding *bond;
+#endif
+	int port;
+
+	if (!root_dev)
+		return NULL;
+
+	while (root_dev) {
+		if (root_dev->priv_flags & IFF_802_1Q_VLAN)
+			root_dev = vlan_dev_real_dev(root_dev);
+#if defined(BOND_SUPPORT)
+		else if (root_dev->flags & IFF_MASTER) {
+			bond = (struct bonding *)netdev_priv(root_dev);
+			/* We select the first child since we can only bond
+			 * offload devices belonging to the same adapter.
+			 */
+			read_lock(&bond->lock);
+			if (bond->first_slave)
+				root_dev = bond->first_slave->dev;
+			else
+				root_dev = NULL;
+			read_unlock(&bond->lock);
+		}
+#endif
+#if defined(CONFIG_XEN) && defined(CONFIG_XEN_TOE)
+		else if (root_dev->priv_flags & IFF_EBRIDGE)
+			root_dev = get_xenbrdpif(root_dev);
+#endif
+		else
+			break;
+	}
+
+	read_lock(&adapter_list_lock);
+	list_for_each_entry(adapter, &adapter_list, adapter_list) {
+		if (!is_offload(adapter))
+			continue;
+		for_each_port(adapter, port)
+			if (root_dev == adapter->port[port]) {
+				read_unlock(&adapter_list_lock);
+				return dev2t3cdev(root_dev);
+			}
+	}
+	read_unlock(&adapter_list_lock);
+
+	return NULL;
+#else
+	return NULL;
+#endif
+}
+#endif
+
+static struct net_device *get_iff_from_mac(adapter_t *adapter,
 					   const unsigned char *mac,
 					   unsigned int vlan)
 {
@@ -179,10 +376,14 @@
 		if (!memcmp(dev->dev_addr, mac, ETH_ALEN)) {
 			if (vlan && vlan != VLAN_VID_MASK) {
 				grp = p->vlan_grp;
-				dev = NULL;
-				if (grp)
-					dev = vlan_group_get_device(grp, vlan);
-			} else
+				dev = grp ? vlan_group_get_device(grp, vlan) :
+					    NULL;
+			}
+#if defined(CONFIG_XEN) && defined(CONFIG_XEN_TOE)
+			else if (dev->br_port)
+				dev = dev->br_port->br->dev;
+#endif
+			else
 				while (dev->master)
 					dev = dev->master;
 			return dev;
@@ -191,8 +392,123 @@
 	return NULL;
 }
 
-static int cxgb_ulp_iscsi_ctl(struct adapter *adapter, unsigned int req,
-			      void *data)
+static inline void failover_fixup(adapter_t *adapter, int port)
+{
+	struct net_device *dev = adapter->port[port];
+	struct port_info *p = netdev_priv(dev);
+	struct cmac *mac = &p->mac;
+
+	if (!netif_running(dev)) {
+		/* Failover triggered by the interface ifdown */
+		t3_write_reg(adapter, A_XGM_TX_CTRL + mac->offset,
+			     F_TXEN);
+		t3_read_reg(adapter, A_XGM_TX_CTRL + mac->offset);
+	} else {
+		/* Failover triggered by the interface link down */
+		t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset, 0);
+		t3_read_reg(adapter, A_XGM_RX_CTRL + mac->offset);
+		t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset,
+			     F_RXEN);
+	}
+}
+
+static inline int in_bond(int port, struct bond_ports *bond_ports)
+{
+	int i;
+
+	for (i = 0; i < bond_ports->nports; i++)
+		if (port ==  bond_ports->ports[i])
+			break;
+	
+	return (i < bond_ports->nports);
+}
+
+static int t3_4ports_failover(struct adapter *adapter, int event,
+			      struct bond_ports *bond_ports)
+{
+	int port = bond_ports->port;
+	struct t3cdev *tdev = &adapter->tdev;
+	struct l2t_data *d = L2DATA(tdev);
+	struct l2t_entry *e, *end;
+	int nports = 0, port_idx;
+
+	/* Reassign L2T entries */
+	switch (event) {
+	case FAILOVER_PORT_RELEASE:
+	case FAILOVER_PORT_DOWN:
+		read_lock_bh(&d->lock);
+		port_idx = 0;
+		nports = bond_ports->nports;
+		for (e = &d->l2tab[1], end = d->rover;
+		     e != end; ++e) {
+			int newport;
+
+			if (e->smt_idx == port) {
+				newport = bond_ports->ports[port_idx];
+				spin_lock_bh(&e->lock);
+				e->smt_idx = newport;
+				if (e->state == L2T_STATE_VALID)
+					t3_l2t_update_l2e(tdev, e);
+				spin_unlock_bh(&e->lock);
+				port_idx = port_idx < nports ?
+					   port_idx + 1 : 0;
+			}
+			/*
+			 * If the port is released, update orig_smt_idx
+			 * to failed over port.
+			 * There are 2 situations:
+			 * 1. Port X is the original port and is released.
+			 * {orig_smt_idx, smt_idx} follows these steps.
+			 * {X, X} -> {X, Y} -> {Y, Y}
+			 * 2. Port Z is released, a failover from port X
+			 * had happened previously.
+			 * {orig_smt_idx, smt_idx} follows these steps:
+			 * {X, Z} -> {Z, Z}
+			 */
+			if (event == FAILOVER_PORT_RELEASE &&
+			    e->orig_smt_idx == port) {
+				spin_lock_bh(&e->lock);
+				e->orig_smt_idx = e->smt_idx;
+				spin_unlock_bh(&e->lock);
+			}
+		}
+		read_unlock_bh(&d->lock);
+		break;
+	case FAILOVER_PORT_UP:
+		read_lock_bh(&d->lock);
+		for (e = &d->l2tab[1], end = d->rover;
+		     e != end; ++e) {
+			if (e->orig_smt_idx == port &&
+			    in_bond(e->smt_idx, bond_ports)) {
+				spin_lock_bh(&e->lock);
+				e->smt_idx = port;
+				if (e->state == L2T_STATE_VALID)
+					t3_l2t_update_l2e(tdev, e);
+				spin_unlock_bh(&e->lock);
+			}
+		}
+		read_unlock_bh(&d->lock);
+		break;
+	case FAILOVER_ACTIVE_SLAVE:
+		read_lock_bh(&d->lock);
+		for (e = &d->l2tab[1], end = d->rover;
+		     e != end; ++e) {
+			if (e->smt_idx != port &&
+			    in_bond(e->smt_idx, bond_ports)) {
+				spin_lock_bh(&e->lock);
+				e->smt_idx = port;
+				if (e->state == L2T_STATE_VALID)
+					t3_l2t_update_l2e(tdev, e);
+				spin_unlock_bh(&e->lock);
+			}
+		}
+		read_unlock_bh(&d->lock);
+		break;
+	}
+	return 0;
+}
+
+static int cxgb_ulp_iscsi_ctl(adapter_t *adapter, unsigned int req, void *data)
 {
 	int i;
 	int ret = 0;
@@ -214,6 +530,7 @@
 		uiip->max_txsz =
 		uiip->max_rxsz = min((val >> S_PMMAXXFERLEN0)&M_PMMAXXFERLEN0,
 				     (val >> S_PMMAXXFERLEN1)&M_PMMAXXFERLEN1);
+
 		/*
 		 * On tx, the iscsi pdu has to be <= tx page size and has to
 		 * fit into the Tx PM FIFO.
@@ -222,7 +539,7 @@
 			  t3_read_reg(adapter, A_PM1_TX_CFG) >> 17);
 		uiip->max_txsz = min(val, uiip->max_txsz);
 
-		/* set MaxRxData to 16224 */
+		/* set max. pdu size (MaxRxData) to 16224 */
 		val = t3_read_reg(adapter, A_TP_PARA_REG2);
 		if ((val >> S_MAXRXDATA) != 0x3f60) {
 			val &= (M_RXCOALESCESIZE << S_RXCOALESCESIZE);
@@ -245,11 +562,11 @@
 	case ULP_ISCSI_SET_PARAMS:
 		t3_write_reg(adapter, A_ULPRX_ISCSI_TAGMASK, uiip->tagmask);
 		/* program the ddp page sizes */
-		for (i = 0; i < 4; i++)
+		for (val = 0, i = 0; i < 4; i++)
 			val |= (uiip->pgsz_factor[i] & 0xF) << (8 * i);
 		if (val && (val != t3_read_reg(adapter, A_ULPRX_ISCSI_PSZ))) {
 			printk(KERN_INFO
-				"%s, setting iscsi pgsz 0x%x, %u,%u,%u,%u.\n",
+			       "%s, setting iscsi pgsz 0x%x, %u,%u,%u,%u.\n",
 				adapter->name, val, uiip->pgsz_factor[0],
 				uiip->pgsz_factor[1], uiip->pgsz_factor[2],
 				uiip->pgsz_factor[3]);
@@ -265,41 +582,39 @@
 /* Response queue used for RDMA events. */
 #define ASYNC_NOTIF_RSPQ 0
 
-static int cxgb_rdma_ctl(struct adapter *adapter, unsigned int req, void *data)
+static int cxgb_rdma_ctl(adapter_t *adapter, unsigned int req, void *data)
 {
 	int ret = 0;
 
 	switch (req) {
 	case RDMA_GET_PARAMS: {
-		struct rdma_info *rdma = data;
+		struct rdma_info *req = data;
 		struct pci_dev *pdev = adapter->pdev;
 
-		rdma->udbell_physbase = pci_resource_start(pdev, 2);
-		rdma->udbell_len = pci_resource_len(pdev, 2);
-		rdma->tpt_base =
-			t3_read_reg(adapter, A_ULPTX_TPT_LLIMIT);
-		rdma->tpt_top = t3_read_reg(adapter, A_ULPTX_TPT_ULIMIT);
-		rdma->pbl_base =
-			t3_read_reg(adapter, A_ULPTX_PBL_LLIMIT);
-		rdma->pbl_top = t3_read_reg(adapter, A_ULPTX_PBL_ULIMIT);
-		rdma->rqt_base = t3_read_reg(adapter, A_ULPRX_RQ_LLIMIT);
-		rdma->rqt_top = t3_read_reg(adapter, A_ULPRX_RQ_ULIMIT);
-		rdma->kdb_addr = adapter->regs + A_SG_KDOORBELL;
-		rdma->pdev = pdev;
+		req->udbell_physbase = pci_resource_start(pdev, 2);
+		req->udbell_len = pci_resource_len(pdev, 2);
+		req->tpt_base = t3_read_reg(adapter, A_ULPTX_TPT_LLIMIT);
+		req->tpt_top  = t3_read_reg(adapter, A_ULPTX_TPT_ULIMIT);
+		req->pbl_base = t3_read_reg(adapter, A_ULPTX_PBL_LLIMIT);
+		req->pbl_top  = t3_read_reg(adapter, A_ULPTX_PBL_ULIMIT);
+		req->rqt_base = t3_read_reg(adapter, A_ULPRX_RQ_LLIMIT);
+		req->rqt_top  = t3_read_reg(adapter, A_ULPRX_RQ_ULIMIT);
+		req->kdb_addr = adapter->regs + A_SG_KDOORBELL;
+		req->pdev     = pdev;
 		break;
 	}
-	case RDMA_CQ_OP:{
+	case RDMA_CQ_OP: {
 		unsigned long flags;
-		struct rdma_cq_op *rdma = data;
+		struct rdma_cq_op *req = data;
 
 		/* may be called in any context */
 		spin_lock_irqsave(&adapter->sge.reg_lock, flags);
-		ret = t3_sge_cqcntxt_op(adapter, rdma->id, rdma->op,
-					rdma->credits);
+		ret = t3_sge_cqcntxt_op(adapter, req->id, req->op,
+					req->credits);
 		spin_unlock_irqrestore(&adapter->sge.reg_lock, flags);
 		break;
 	}
-	case RDMA_GET_MEM:{
+	case RDMA_GET_MEM: {
 		struct ch_mem_range *t = data;
 		struct mc7 *mem;
 
@@ -314,41 +629,41 @@
 		else
 			return -EINVAL;
 
-		ret =
-			t3_mc7_bd_read(mem, t->addr / 8, t->len / 8,
-					(u64 *) t->buf);
+		ret = t3_mc7_bd_read(mem, t->addr/8, t->len/8, (u64 *)t->buf);
 		if (ret)
 			return ret;
 		break;
 	}
-	case RDMA_CQ_SETUP:{
-		struct rdma_cq_setup *rdma = data;
+	case RDMA_CQ_SETUP: {
+		struct rdma_cq_setup *req = data;
+		unsigned long flags;
 
-		spin_lock_irq(&adapter->sge.reg_lock);
-		ret =
-			t3_sge_init_cqcntxt(adapter, rdma->id,
-					rdma->base_addr, rdma->size,
-					ASYNC_NOTIF_RSPQ,
-					rdma->ovfl_mode, rdma->credits,
-					rdma->credit_thres);
-		spin_unlock_irq(&adapter->sge.reg_lock);
+		spin_lock_irqsave(&adapter->sge.reg_lock, flags);
+		ret = t3_sge_init_cqcntxt(adapter, req->id, req->base_addr,
+					  req->size, ASYNC_NOTIF_RSPQ,
+					  req->ovfl_mode, req->credits,
+					  req->credit_thres);
+		spin_unlock_irqrestore(&adapter->sge.reg_lock, flags);
 		break;
 	}
-	case RDMA_CQ_DISABLE:
-		spin_lock_irq(&adapter->sge.reg_lock);
+	case RDMA_CQ_DISABLE: {
+		unsigned long flags;
+
+		spin_lock_irqsave(&adapter->sge.reg_lock, flags);
 		ret = t3_sge_disable_cqcntxt(adapter, *(unsigned int *)data);
-		spin_unlock_irq(&adapter->sge.reg_lock);
+		spin_unlock_irqrestore(&adapter->sge.reg_lock, flags);
 		break;
-	case RDMA_CTRL_QP_SETUP:{
-		struct rdma_ctrlqp_setup *rdma = data;
+	}
+	case RDMA_CTRL_QP_SETUP: {
+		struct rdma_ctrlqp_setup *req = data;
+		unsigned long flags;
 
-		spin_lock_irq(&adapter->sge.reg_lock);
+		spin_lock_irqsave(&adapter->sge.reg_lock, flags);
 		ret = t3_sge_init_ecntxt(adapter, FW_RI_SGEEC_START, 0,
-						SGE_CNTXT_RDMA,
-						ASYNC_NOTIF_RSPQ,
-						rdma->base_addr, rdma->size,
-						FW_RI_TID_START, 1, 0);
-		spin_unlock_irq(&adapter->sge.reg_lock);
+					 SGE_CNTXT_RDMA, ASYNC_NOTIF_RSPQ,
+					 req->base_addr, req->size,
+					 FW_RI_TID_START, 1, 0);
+		spin_unlock_irqrestore(&adapter->sge.reg_lock, flags);
 		break;
 	}
 	case RDMA_GET_MIB: {
@@ -371,9 +686,11 @@
 	struct iff_mac *iffmacp;
 	struct ddp_params *ddpp;
 	struct adap_ports *ports;
+	struct port_array *pap;
 	struct ofld_page_info *rx_page_info;
 	struct tp_params *tp = &adapter->params.tp;
-	int i;
+	struct bond_ports *bond_ports;
+	int port;
 
 	switch (req) {
 	case GET_MAX_OUTSTANDING_WR:
@@ -383,24 +700,48 @@
 		*(unsigned int *)data = WR_FLITS;
 		break;
 	case GET_TX_MAX_CHUNK:
-		*(unsigned int *)data = 1 << 20;	/* 1MB */
+		*(unsigned int *)data = 1 << 20;  /* 1MB */
 		break;
 	case GET_TID_RANGE:
 		tid = data;
 		tid->num = t3_mc5_size(&adapter->mc5) -
-		    adapter->params.mc5.nroutes -
-		    adapter->params.mc5.nfilters - adapter->params.mc5.nservers;
+			adapter->params.mc5.nroutes -
+			adapter->params.mc5.nfilters -
+			adapter->params.mc5.nservers;
 		tid->base = 0;
 		break;
 	case GET_STID_RANGE:
 		tid = data;
 		tid->num = adapter->params.mc5.nservers;
 		tid->base = t3_mc5_size(&adapter->mc5) - tid->num -
-		    adapter->params.mc5.nfilters - adapter->params.mc5.nroutes;
+			adapter->params.mc5.nfilters -
+			adapter->params.mc5.nroutes;
 		break;
 	case GET_L2T_CAPACITY:
 		*(unsigned int *)data = 2048;
 		break;
+	case GET_CPUIDX_OF_QSET: {
+		unsigned int qset = *(unsigned int *)data;
+
+		if (qset >= SGE_QSETS ||
+		    adapter->rrss_map[qset] >= RSS_TABLE_SIZE)
+			return -EINVAL;
+		*(unsigned int *)data = adapter->rrss_map[qset];
+		break;
+	}
+	case GET_PORT_SCHED: {
+		struct port_sched *p = data;
+
+		if (adapter->params.nports > 2) {
+			const struct port_info *pi = netdev_priv(p->dev);
+			p->sched = pi->port_id;
+		} else
+			p->sched = -1;
+		break;
+	}
+	case GET_NUM_QUEUES:
+		*(unsigned int *)data = adapter->sge.nqsets;
+		break;
 	case GET_MTUS:
 		mtup = data;
 		mtup->size = NMTUS;
@@ -409,21 +750,62 @@
 	case GET_IFF_FROM_MAC:
 		iffmacp = data;
 		iffmacp->dev = get_iff_from_mac(adapter, iffmacp->mac_addr,
-						iffmacp->vlan_tag &
-						VLAN_VID_MASK);
+					  iffmacp->vlan_tag & VLAN_VID_MASK);
 		break;
 	case GET_DDP_PARAMS:
 		ddpp = data;
 		ddpp->llimit = t3_read_reg(adapter, A_ULPRX_TDDP_LLIMIT);
 		ddpp->ulimit = t3_read_reg(adapter, A_ULPRX_TDDP_ULIMIT);
 		ddpp->tag_mask = t3_read_reg(adapter, A_ULPRX_TDDP_TAGMASK);
+		ddpp->pdev = adapter->pdev;
 		break;
 	case GET_PORTS:
 		ports = data;
-		ports->nports = adapter->params.nports;
-		for_each_port(adapter, i)
-			ports->lldevs[i] = adapter->port[i];
+		ports->nports   = adapter->params.nports;
+		for_each_port(adapter, port)
+			ports->lldevs[port] = adapter->port[port];
 		break;
+	case GET_PORT_ARRAY:
+		pap = data;
+		pap->nports = adapter->params.nports;
+		pap->lldevs = adapter->port;
+		break;
+	case FAILOVER:
+		port = *(int *)data;
+		t3_port_failover(adapter, port);
+		failover_fixup(adapter, !port);
+		break;
+	case FAILOVER_DONE:
+		port = *(int *)data;
+		t3_failover_done(adapter, port);
+		break;
+	case FAILOVER_CLEAR:
+		t3_failover_clear(adapter);
+		break;
+	case FAILOVER_ACTIVE_SLAVE:
+	case FAILOVER_PORT_DOWN:
+	case FAILOVER_PORT_UP:
+	case FAILOVER_PORT_RELEASE:
+		bond_ports = data;
+		t3_4ports_failover(adapter, req, bond_ports);
+		break;	
+	case GET_RX_PAGE_INFO:
+		rx_page_info = data;
+		rx_page_info->page_size = tp->rx_pg_size;
+		rx_page_info->num = tp->rx_num_pgs;
+		break;
+	case GET_ISCSI_IPV4ADDR: {
+		struct iscsi_ipv4addr *p = data;
+		struct port_info *pi = netdev_priv(p->dev);
+		p->ipv4addr = pi->iscsi_ipv4addr;
+		break;
+	}
+	case SET_ISCSI_IPV4ADDR: {
+		struct iscsi_ipv4addr *p = data;
+		struct port_info *pi = netdev_priv(p->dev);
+		pi->iscsi_ipv4addr = p->ipv4addr;
+		break;
+	}
 	case ULP_ISCSI_GET_PARAMS:
 	case ULP_ISCSI_SET_PARAMS:
 		if (!offload_running(adapter))
@@ -439,26 +821,16 @@
 		if (!offload_running(adapter))
 			return -EAGAIN;
 		return cxgb_rdma_ctl(adapter, req, data);
-	case GET_RX_PAGE_INFO:
-		rx_page_info = data;
-		rx_page_info->page_size = tp->rx_pg_size;
-		rx_page_info->num = tp->rx_num_pgs;
-		break;
-	case GET_ISCSI_IPV4ADDR: {
-		struct iscsi_ipv4addr *p = data;
-		struct port_info *pi = netdev_priv(p->dev);
-		p->ipv4addr = pi->iscsi_ipv4addr;
-		break;
-	}
 	case GET_EMBEDDED_INFO: {
-		struct ch_embedded_info *e = data;
+	struct ch_embedded_info *e = data;
 
 		spin_lock(&adapter->stats_lock);
 		t3_get_fw_version(adapter, &e->fw_vers);
 		t3_get_tp_version(adapter, &e->tp_vers);
 		spin_unlock(&adapter->stats_lock);
+
 		break;
-	}
+}
 	default:
 		return -EOPNOTSUPP;
 	}
@@ -474,7 +846,7 @@
 				int n)
 {
 	while (n--)
-		dev_kfree_skb_any(skbs[n]);
+		kfree_skb(skbs[n]);
 	return 0;
 }
 
@@ -484,7 +856,7 @@
 
 void cxgb3_set_dummy_ops(struct t3cdev *dev)
 {
-	dev->recv = rx_offload_blackhole;
+	dev->recv         = rx_offload_blackhole;
 	dev->neigh_update = dummy_neigh_update;
 }
 
@@ -498,6 +870,8 @@
 	void *ctx = p->t3c_tid.ctx;
 
 	spin_lock_bh(&t->atid_lock);
+	p->t3c_tid.ctx = NULL;
+	p->t3c_tid.client = NULL;
 	p->next = t->afree;
 	t->afree = p;
 	t->atids_in_use--;
@@ -505,7 +879,6 @@
 
 	return ctx;
 }
-
 EXPORT_SYMBOL(cxgb3_free_atid);
 
 /*
@@ -517,16 +890,17 @@
 	union listen_entry *p = stid2entry(t, stid);
 
 	spin_lock_bh(&t->stid_lock);
+	p->t3c_tid.ctx = NULL;
+	p->t3c_tid.client = NULL;
 	p->next = t->sfree;
 	t->sfree = p;
 	t->stids_in_use--;
 	spin_unlock_bh(&t->stid_lock);
 }
-
 EXPORT_SYMBOL(cxgb3_free_stid);
 
 void cxgb3_insert_tid(struct t3cdev *tdev, struct cxgb3_client *client,
-		      void *ctx, unsigned int tid)
+	void *ctx, unsigned int tid)
 {
 	struct tid_info *t = &(T3C_DATA(tdev))->tid_maps;
 
@@ -534,7 +908,6 @@
 	t->tid_tab[tid].ctx = ctx;
 	atomic_inc(&t->tids_in_use);
 }
-
 EXPORT_SYMBOL(cxgb3_insert_tid);
 
 /*
@@ -550,14 +923,12 @@
 	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_TID_RELEASE, tid));
 }
 
-static void t3_process_tid_release_list(struct work_struct *work)
+DECLARE_TASK_FUNC(t3_process_tid_release_list, task_param)
 {
-	struct t3c_data *td = container_of(work, struct t3c_data,
-					   tid_release_task);
 	struct sk_buff *skb;
+	struct t3c_data *td = WORK2T3CDATA(task_param, tid_release_task);
 	struct t3cdev *tdev = td->dev;
 
-
 	spin_lock_bh(&td->tid_release_lock);
 	while (td->tid_release_list) {
 		struct t3c_tid_entry *p = td->tid_release_list;
@@ -566,31 +937,14 @@
 		spin_unlock_bh(&td->tid_release_lock);
 
 		skb = alloc_skb(sizeof(struct cpl_tid_release),
-				GFP_KERNEL);
-		if (!skb)
-			skb = td->nofail_skb;
-		if (!skb) {
-			spin_lock_bh(&td->tid_release_lock);
-			p->ctx = (void *)td->tid_release_list;
-			td->tid_release_list = (struct t3c_tid_entry *)p;
-			break;
-		}
+				GFP_KERNEL | __GFP_NOFAIL);
+
 		mk_tid_release(skb, p - td->tid_maps.tid_tab);
 		cxgb3_ofld_send(tdev, skb);
 		p->ctx = NULL;
-		if (skb == td->nofail_skb)
-			td->nofail_skb =
-				alloc_skb(sizeof(struct cpl_tid_release),
-					GFP_KERNEL);
 		spin_lock_bh(&td->tid_release_lock);
 	}
-	td->release_list_incomplete = (td->tid_release_list == NULL) ? 0 : 1;
 	spin_unlock_bh(&td->tid_release_lock);
-
-	if (!td->nofail_skb)
-		td->nofail_skb =
-			alloc_skb(sizeof(struct cpl_tid_release),
-				GFP_KERNEL);
 }
 
 /* use ctx as a next pointer in the tid release list */
@@ -603,11 +957,10 @@
 	p->ctx = (void *)td->tid_release_list;
 	p->client = NULL;
 	td->tid_release_list = p;
-	if (!p->ctx || td->release_list_incomplete)
+	if (!p->ctx)
 		schedule_work(&td->tid_release_task);
 	spin_unlock_bh(&td->tid_release_lock);
 }
-
 EXPORT_SYMBOL(cxgb3_queue_tid_release);
 
 /*
@@ -628,7 +981,7 @@
 		struct sk_buff *skb;
 
 		skb = alloc_skb(sizeof(struct cpl_tid_release), GFP_ATOMIC);
-		if (likely(skb)) {
+		if (likely(skb != NULL)) {
 			mk_tid_release(skb, tid);
 			cxgb3_ofld_send(tdev, skb);
 			t->tid_tab[tid].ctx = NULL;
@@ -637,7 +990,6 @@
 	}
 	atomic_dec(&t->tids_in_use);
 }
-
 EXPORT_SYMBOL(cxgb3_remove_tid);
 
 int cxgb3_alloc_atid(struct t3cdev *tdev, struct cxgb3_client *client,
@@ -661,7 +1013,6 @@
 	spin_unlock_bh(&t->atid_lock);
 	return atid;
 }
-
 EXPORT_SYMBOL(cxgb3_alloc_atid);
 
 int cxgb3_alloc_stid(struct t3cdev *tdev, struct cxgb3_client *client,
@@ -683,19 +1034,8 @@
 	spin_unlock_bh(&t->stid_lock);
 	return stid;
 }
-
 EXPORT_SYMBOL(cxgb3_alloc_stid);
 
-/* Get the t3cdev associated with a net_device */
-struct t3cdev *dev2t3cdev(struct net_device *dev)
-{
-	const struct port_info *pi = netdev_priv(dev);
-
-	return (struct t3cdev *)pi->adapter;
-}
-
-EXPORT_SYMBOL(dev2t3cdev);
-
 static int do_smt_write_rpl(struct t3cdev *dev, struct sk_buff *skb)
 {
 	struct cpl_smt_write_rpl *rpl = cplhdr(skb);
@@ -743,11 +1083,11 @@
 	    t3c_tid->client->handlers &&
 	    t3c_tid->client->handlers[CPL_ACT_OPEN_RPL]) {
 		return t3c_tid->client->handlers[CPL_ACT_OPEN_RPL] (dev, skb,
-								    t3c_tid->
-								    ctx);
+			t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, CPL_ACT_OPEN_RPL);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, CPL_ACT_OPEN_RPL);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
@@ -757,15 +1097,23 @@
 	union opcode_tid *p = cplhdr(skb);
 	unsigned int stid = G_TID(ntohl(p->opcode_tid));
 	struct t3c_tid_entry *t3c_tid;
+	const struct tid_info *t = &(T3C_DATA(dev))->tid_maps;
 
-	t3c_tid = lookup_stid(&(T3C_DATA(dev))->tid_maps, stid);
+	/*
+	 * We get these messages also when setting up HW filters.  Throw
+	 * those away silently.
+	 */
+	if (stid >= t->stid_base + t->nstids)
+		return CPL_RET_BUF_DONE;
+
+	t3c_tid = lookup_stid(t, stid);
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
 	    t3c_tid->client->handlers[p->opcode]) {
-		return t3c_tid->client->handlers[p->opcode] (dev, skb,
-							     t3c_tid->ctx);
+		return t3c_tid->client->handlers[p->opcode] (dev, skb, t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, p->opcode);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, p->opcode);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
@@ -778,12 +1126,13 @@
 
 	t3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
-	    t3c_tid->client->handlers[p->opcode]) {
+		t3c_tid->client->handlers[p->opcode]) {
 		return t3c_tid->client->handlers[p->opcode]
-		    (dev, skb, t3c_tid->ctx);
+						(dev, skb, t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, p->opcode);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, p->opcode);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
@@ -807,10 +1156,11 @@
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
 	    t3c_tid->client->handlers[CPL_PASS_ACCEPT_REQ]) {
 		return t3c_tid->client->handlers[CPL_PASS_ACCEPT_REQ]
-		    (dev, skb, t3c_tid->ctx);
+						(dev, skb, t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, CPL_PASS_ACCEPT_REQ);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, CPL_PASS_ACCEPT_REQ);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
@@ -823,7 +1173,7 @@
  * the buffer.
  */
 static struct sk_buff *cxgb3_get_cpl_reply_skb(struct sk_buff *skb, size_t len,
-					       gfp_t gfp)
+					       int gfp)
 {
 	if (likely(!skb_cloned(skb))) {
 		BUG_ON(skb->len < len);
@@ -845,9 +1195,9 @@
 
 	t3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
-	    t3c_tid->client->handlers[p->opcode]) {
+		t3c_tid->client->handlers[p->opcode]) {
 		return t3c_tid->client->handlers[p->opcode]
-		    (dev, skb, t3c_tid->ctx);
+						(dev, skb, t3c_tid->ctx);
 	} else {
 		struct cpl_abort_req_rss *req = cplhdr(skb);
 		struct cpl_abort_rpl *rpl;
@@ -855,13 +1205,14 @@
 		unsigned int tid = GET_TID(req);
 		u8 cmd = req->status;
 
+		WARN_ON(dev->type == T3B);
+
 		if (req->status == CPL_ERR_RTX_NEG_ADVICE ||
 		    req->status == CPL_ERR_PERSIST_NEG_ADVICE)
 			goto out;
 
 		reply_skb = cxgb3_get_cpl_reply_skb(skb,
-						    sizeof(struct
-							   cpl_abort_rpl),
+						    sizeof(struct cpl_abort_rpl),
 						    GFP_ATOMIC);
 
 		if (!reply_skb) {
@@ -869,15 +1220,15 @@
 			goto out;
 		}
 		reply_skb->priority = CPL_PRIORITY_DATA;
-		__skb_put(reply_skb, sizeof(struct cpl_abort_rpl));
 		rpl = cplhdr(reply_skb);
 		rpl->wr.wr_hi =
-		    htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));
+			htonl(V_WR_OP(FW_WROPCODE_OFLD_HOST_ABORT_CON_RPL));
 		rpl->wr.wr_lo = htonl(V_WR_TID(tid));
-		OPCODE_TID(rpl) = htonl(MK_OPCODE_TID(CPL_ABORT_RPL, tid));
+		OPCODE_TID(rpl) =
+			htonl(MK_OPCODE_TID(CPL_ABORT_RPL, tid));
 		rpl->cmd = cmd;
 		cxgb3_ofld_send(dev, reply_skb);
-out:
+ out:
 		return CPL_RET_BUF_DONE;
 	}
 }
@@ -901,10 +1252,11 @@
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
 	    t3c_tid->client->handlers[CPL_ACT_ESTABLISH]) {
 		return t3c_tid->client->handlers[CPL_ACT_ESTABLISH]
-		    (dev, skb, t3c_tid->ctx);
+						(dev, skb, t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, CPL_ACT_ESTABLISH);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, CPL_ACT_ESTABLISH);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
@@ -912,74 +1264,279 @@
 static int do_trace(struct t3cdev *dev, struct sk_buff *skb)
 {
 	struct cpl_trace_pkt *p = cplhdr(skb);
+	struct adapter *adapter = tdev2adap(dev);
 
 	skb->protocol = htons(0xffff);
 	skb->dev = dev->lldev;
-	skb_pull(skb, sizeof(*p));
+	if (adapter->params.nports > 2)
+		skb_pull(skb, sizeof(*p) + 8); /* pull CPL + preamble */
+	else
+		skb_pull(skb, sizeof(*p));     /* pull CPL */
 	skb_reset_mac_header(skb);
 	netif_receive_skb(skb);
 	return 0;
 }
 
-/*
- * That skb would better have come from process_responses() where we abuse
- * ->priority and ->csum to carry our data.  NB: if we get to per-arch
- * ->csum, the things might get really interesting here.
- */
-
-static inline u32 get_hwtid(struct sk_buff *skb)
-{
-	return ntohl((__force __be32)skb->priority) >> 8 & 0xfffff;
-}
-
-static inline u32 get_opcode(struct sk_buff *skb)
-{
-	return G_OPCODE(ntohl((__force __be32)skb->csum));
-}
-
 static int do_term(struct t3cdev *dev, struct sk_buff *skb)
 {
-	unsigned int hwtid = get_hwtid(skb);
-	unsigned int opcode = get_opcode(skb);
+	unsigned int hwtid = ntohl(skb->priority) >> 8 & 0xfffff;
+	unsigned int opcode = G_OPCODE(ntohl(skb->csum));
 	struct t3c_tid_entry *t3c_tid;
 
 	t3c_tid = lookup_tid(&(T3C_DATA(dev))->tid_maps, hwtid);
 	if (t3c_tid && t3c_tid->ctx && t3c_tid->client->handlers &&
-	    t3c_tid->client->handlers[opcode]) {
-		return t3c_tid->client->handlers[opcode] (dev, skb,
-							  t3c_tid->ctx);
+		t3c_tid->client->handlers[opcode]) {
+		return t3c_tid->client->handlers[opcode](dev,skb,t3c_tid->ctx);
 	} else {
-		printk(KERN_ERR "%s: received clientless CPL command 0x%x\n",
-		       dev->name, opcode);
+		CH_MSG(tdev2adap(dev), DEBUG, OFLD,
+		       "%s: received clientless CPL command 0x%x\n",
+			dev->name, opcode);
 		return CPL_RET_BUF_DONE | CPL_RET_BAD_MSG;
 	}
 }
 
+#if defined(NETEVENT)
 static int nb_callback(struct notifier_block *self, unsigned long event,
-		       void *ctx)
+	void *ctx)
 {
 	switch (event) {
-	case (NETEVENT_NEIGH_UPDATE):{
-		cxgb_neigh_update((struct neighbour *)ctx);
-		break;
-	}
-	case (NETEVENT_PMTU_UPDATE):
-		break;
-	case (NETEVENT_REDIRECT):{
-		struct netevent_redirect *nr = ctx;
-		cxgb_redirect(nr->old, nr->new);
-		cxgb_neigh_update(nr->new->neighbour);
-		break;
-	}
-	default:
-		break;
+		case (NETEVENT_NEIGH_UPDATE): {
+			cxgb_neigh_update((struct neighbour *)ctx);
+			break;
+		}
+#ifdef DIVY	/* XXX Divy no NETEVENT_ROUTE_UPDATE definition */
+		case (NETEVENT_ROUTE_UPDATE):
+			break;
+#endif
+		case (NETEVENT_PMTU_UPDATE):
+			break;
+		case (NETEVENT_REDIRECT): {
+			struct netevent_redirect *nr = ctx;
+			cxgb_redirect(nr->old, nr->new);
+			cxgb_neigh_update(nr->new->neighbour);
+			break;
+		}
+		default:
+			break;
 	}
 	return 0;
 }
 
+#elif defined(OFLD_USE_KPROBES)
+
+#ifndef AUTOCONF_INCLUDED
+#include <linux/autoconf.h>
+#endif
+#include <linux/kallsyms.h>
+#include <linux/kprobes.h>
+#include <net/arp.h>
+
+static int (*orig_arp_constructor)(struct neighbour *);
+
+static void neigh_suspect(struct neighbour *neigh)
+{
+	struct hh_cache *hh;
+
+	neigh->output = neigh->ops->output;
+
+	for (hh = neigh->hh; hh; hh = hh->hh_next)
+		hh->hh_output = neigh->ops->output;
+}
+
+static void neigh_connect(struct neighbour *neigh)
+{
+	struct hh_cache *hh;
+
+	neigh->output = neigh->ops->connected_output;
+
+	for (hh = neigh->hh; hh; hh = hh->hh_next)
+		hh->hh_output = neigh->ops->hh_output;
+}
+
+static inline int neigh_max_probes(const struct neighbour *n)
+{
+	const struct neigh_parms *p = n->parms;
+	return (n->nud_state & NUD_PROBE ?
+		p->ucast_probes :
+		p->ucast_probes + p->app_probes + p->mcast_probes);
+}
+
+static void neigh_timer_handler_offload(unsigned long arg)
+{
+	unsigned long now, next;
+	struct neighbour *neigh = (struct neighbour *)arg;
+	unsigned state;
+	int notify = 0;
+
+	write_lock(&neigh->lock);
+
+	state = neigh->nud_state;
+	now = jiffies;
+	next = now + HZ;
+
+	if (!(state & NUD_IN_TIMER)) {
+#ifndef CONFIG_SMP
+		printk(KERN_WARNING "neigh: timer & !nud_in_timer\n");
+#endif
+		goto out;
+	}
+
+	if (state & NUD_REACHABLE) {
+		if (time_before_eq(now,
+				   neigh->confirmed +
+				   neigh->parms->reachable_time)) {
+			next = neigh->confirmed + neigh->parms->reachable_time;
+		} else if (time_before_eq(now,
+					  neigh->used +
+					  neigh->parms->delay_probe_time)) {
+			neigh->nud_state = NUD_DELAY;
+			neigh->updated = jiffies;
+			neigh_suspect(neigh);
+			next = now + neigh->parms->delay_probe_time;
+		} else {
+			neigh->nud_state = NUD_STALE;
+			neigh->updated = jiffies;
+			neigh_suspect(neigh);
+			cxgb_neigh_update(neigh);
+		}
+	} else if (state & NUD_DELAY) {
+		if (time_before_eq(now,
+				   neigh->confirmed +
+				   neigh->parms->delay_probe_time)) {
+			neigh->nud_state = NUD_REACHABLE;
+			neigh->updated = jiffies;
+			neigh_connect(neigh);
+			cxgb_neigh_update(neigh);
+			next = neigh->confirmed + neigh->parms->reachable_time;
+		} else {
+			neigh->nud_state = NUD_PROBE;
+			neigh->updated = jiffies;
+			atomic_set(&neigh->probes, 0);
+			next = now + neigh->parms->retrans_time;
+		}
+	} else {
+		/* NUD_PROBE|NUD_INCOMPLETE */
+		next = now + neigh->parms->retrans_time;
+	}
+
+	if ((neigh->nud_state & (NUD_INCOMPLETE | NUD_PROBE)) &&
+	    atomic_read(&neigh->probes) >= neigh_max_probes(neigh)) {
+		struct sk_buff *skb;
+
+		neigh->nud_state = NUD_FAILED;
+		neigh->updated = jiffies;
+		notify = 1;
+		cxgb_neigh_update(neigh);
+		NEIGH_CACHE_STAT_INC(neigh->tbl, res_failed);
+
+		/* It is very thin place. report_unreachable is very
+		   complicated routine. Particularly, it can hit the same
+		   neighbour entry!
+		   So that, we try to be accurate and avoid dead loop. --ANK
+		 */
+		while (neigh->nud_state == NUD_FAILED &&
+		       (skb = __skb_dequeue(&neigh->arp_queue)) != NULL) {
+			write_unlock(&neigh->lock);
+			neigh->ops->error_report(neigh, skb);
+			write_lock(&neigh->lock);
+		}
+		skb_queue_purge(&neigh->arp_queue);
+	}
+
+	if (neigh->nud_state & NUD_IN_TIMER) {
+		if (time_before(next, jiffies + HZ/2))
+			next = jiffies + HZ/2;
+		if (!mod_timer(&neigh->timer, next))
+			neigh_hold(neigh);
+	}
+	if (neigh->nud_state & (NUD_INCOMPLETE | NUD_PROBE)) {
+		struct sk_buff *skb = skb_peek(&neigh->arp_queue);
+		/* keep skb alive even if arp_queue overflows */
+		if (skb)
+			skb_get(skb);
+		write_unlock(&neigh->lock);
+		neigh->ops->solicit(neigh, skb);
+		atomic_inc(&neigh->probes);
+		if (skb)
+			kfree_skb(skb);
+	} else {
+out:
+		write_unlock(&neigh->lock);
+	}
+
+#ifdef CONFIG_ARPD
+	if (notify && neigh->parms->app_probes)
+		neigh_app_notify(neigh);
+#endif
+	neigh_release(neigh);
+}
+
+static int arp_constructor_offload(struct neighbour *neigh)
+{
+	if (dev2tdev(neigh->dev))
+		neigh->timer.function = neigh_timer_handler_offload;
+	return orig_arp_constructor(neigh);
+}
+
+/*
+ * This must match exactly the signature of neigh_update for jprobes to work.
+ * It runs from a trap handler with interrupts off so don't disable BH.
+ */
+static int neigh_update_offload(struct neighbour *neigh, const u8 *lladdr,
+				u8 new, u32 flags)
+{
+	write_lock(&neigh->lock);
+	cxgb_neigh_update(neigh);
+	write_unlock(&neigh->lock);
+	jprobe_return();
+	/* NOTREACHED */
+	return 0;
+}
+
+static struct jprobe neigh_update_jprobe = {
+	.entry = (kprobe_opcode_t *) neigh_update_offload,
+	.kp.addr = (kprobe_opcode_t *) neigh_update
+};
+
+static int prepare_arp_with_t3core(void)
+{
+	int err;
+
+	err = register_jprobe(&neigh_update_jprobe);
+	if (err) {
+		printk(KERN_ERR "Could not install neigh_update jprobe, "
+				"error %d\n", err);
+		return err;
+	}
+
+	orig_arp_constructor = arp_tbl.constructor;
+	arp_tbl.constructor  = arp_constructor_offload;
+
+	return 0;
+}
+
+static void restore_arp_sans_t3core(void)
+{
+	arp_tbl.constructor = orig_arp_constructor;
+	unregister_jprobe(&neigh_update_jprobe);
+}
+
+#else /* Module suport */
+
+static inline int prepare_arp_with_t3core(void)
+{
+	return 0;
+}
+
+static inline void restore_arp_sans_t3core(void)
+{}
+#endif
+
+#if defined(NETEVENT)
 static struct notifier_block nb = {
 	.notifier_call = nb_callback
 };
+#endif
 
 /*
  * Process a received packet with an unknown/unexpected CPL opcode.
@@ -1008,7 +1565,6 @@
 		printk(KERN_ERR "T3C: handler registration for "
 		       "opcode %x failed\n", opcode);
 }
-
 EXPORT_SYMBOL(t3_register_cpl_handler);
 
 /*
@@ -1018,7 +1574,7 @@
 {
 	while (n--) {
 		struct sk_buff *skb = *skbs++;
-		unsigned int opcode = get_opcode(skb);
+		unsigned int opcode = G_OPCODE(ntohl(skb->csum));
 		int ret = cpl_handlers[opcode] (dev, skb);
 
 #if VALIDATE_TID
@@ -1044,43 +1600,61 @@
 	int r;
 
 	local_bh_disable();
+#if defined(CONFIG_CHELSIO_T3)
+	if (unlikely(netdev_nit)) {      /* deal with active taps */
+		skb->nh.raw = skb->data;
+		if (!skb->dev)
+			skb->dev = dev->lldev;
+		dev_queue_xmit_nit(skb, skb->dev);
+	}
+#endif
 	r = dev->send(dev, skb);
+
 	local_bh_enable();
 	return r;
 }
-
 EXPORT_SYMBOL(cxgb3_ofld_send);
 
-static int is_offloading(struct net_device *dev)
+/**
+ * cxgb3_ofld_skb - process n received offload packets
+ * @dev: the offload device
+ * @skb: an array of offload packets
+ * @n: the number of offload packets
+ *
+ * Process an array of ingress offload packets.  Each packet is forwarded
+ * to any active network taps and then passed to the offload device's receive
+ * method.  We optimize passing packets to the receive method by passing
+ * it the whole array at once except when there are active taps.
+ */
+int cxgb3_ofld_recv(struct t3cdev *dev, struct sk_buff **skb, int n)
 {
-	struct adapter *adapter;
-	int i;
+#if defined(CONFIG_CHELSIO_T3)
+	if (likely(!netdev_nit))
+		return dev->recv(dev, skb, n);
 
-	read_lock_bh(&adapter_list_lock);
-	list_for_each_entry(adapter, &adapter_list, adapter_list) {
-		for_each_port(adapter, i) {
-			if (dev == adapter->port[i]) {
-				read_unlock_bh(&adapter_list_lock);
-				return 1;
-			}
-		}
+	for ( ; n; n--, skb++) {
+		skb[0]->dev = dev->lldev;
+		dev_queue_xmit_nit(skb[0], dev->lldev);
+		skb[0]->dev = NULL;
+		dev->recv(dev, skb, 1);
 	}
-	read_unlock_bh(&adapter_list_lock);
 	return 0;
+#else
+	return dev->recv(dev, skb, n);
+#endif
 }
 
+#if defined(NETEVENT) || defined(OFLD_USE_KPROBES)
 void cxgb_neigh_update(struct neighbour *neigh)
 {
-	struct net_device *dev = neigh->dev;
+	struct t3cdev *tdev = dev2tdev(neigh->dev);
 
-	if (dev && (is_offloading(dev))) {
-		struct t3cdev *tdev = dev2t3cdev(dev);
+	if (tdev)
+		t3_l2t_update(tdev, neigh);
+}
+#endif
 
-		BUG_ON(!tdev);
-		t3_l2t_update(tdev, neigh);
-	}
-}
-
+#if defined(NETEVENT)
 static void set_l2t_ix(struct t3cdev *tdev, u32 tid, struct l2t_entry *e)
 {
 	struct sk_buff *skb;
@@ -1088,14 +1662,14 @@
 
 	skb = alloc_skb(sizeof(*req), GFP_ATOMIC);
 	if (!skb) {
-		printk(KERN_ERR "%s: cannot allocate skb!\n", __func__);
+		printk(KERN_ERR "%s: cannot allocate skb!\n", __FUNCTION__);
 		return;
 	}
 	skb->priority = CPL_PRIORITY_CONTROL;
 	req = (struct cpl_set_tcb_field *)skb_put(skb, sizeof(*req));
 	req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
 	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_SET_TCB_FIELD, tid));
-	req->reply = 0;
+	req->reply = V_NO_REPLY(1);
 	req->cpu_idx = 0;
 	req->word = htons(W_TCB_L2T_IX);
 	req->mask = cpu_to_be64(V_TCB_L2T_IX(M_TCB_L2T_IX));
@@ -1105,54 +1679,103 @@
 
 void cxgb_redirect(struct dst_entry *old, struct dst_entry *new)
 {
-	struct net_device *olddev, *newdev;
 	struct tid_info *ti;
-	struct t3cdev *tdev;
+	struct t3cdev *old_tdev, *new_tdev;
 	u32 tid;
 	int update_tcb;
 	struct l2t_entry *e;
 	struct t3c_tid_entry *te;
 
-	olddev = old->neighbour->dev;
-	newdev = new->neighbour->dev;
-	if (!is_offloading(olddev))
+	old_tdev = dev2tdev(old->neighbour->dev);
+	new_tdev = dev2tdev(new->neighbour->dev);
+
+	if (!old_tdev)
 		return;
-	if (!is_offloading(newdev)) {
-		printk(KERN_WARNING "%s: Redirect to non-offload "
-		       "device ignored.\n", __func__);
+	if (new_tdev) {
+		printk(KERN_WARNING "%s: Redirect to non-offload"
+		       "device ignored.\n", __FUNCTION__);
 		return;
 	}
-	tdev = dev2t3cdev(olddev);
-	BUG_ON(!tdev);
-	if (tdev != dev2t3cdev(newdev)) {
+
+	if (old_tdev != new_tdev) {
 		printk(KERN_WARNING "%s: Redirect to different "
-		       "offload device ignored.\n", __func__);
+		       "offload device ignored.\n", __FUNCTION__);
 		return;
 	}
 
 	/* Add new L2T entry */
-	e = t3_l2t_get(tdev, new->neighbour, newdev);
+	e = t3_l2t_get(new_tdev, new->neighbour, new->neighbour->dev);
 	if (!e) {
 		printk(KERN_ERR "%s: couldn't allocate new l2t entry!\n",
-		       __func__);
+		       __FUNCTION__);
 		return;
 	}
 
 	/* Walk tid table and notify clients of dst change. */
-	ti = &(T3C_DATA(tdev))->tid_maps;
+	ti = &(T3C_DATA(new_tdev))->tid_maps;
 	for (tid = 0; tid < ti->ntids; tid++) {
 		te = lookup_tid(ti, tid);
 		BUG_ON(!te);
 		if (te && te->ctx && te->client && te->client->redirect) {
-			update_tcb = te->client->redirect(te->ctx, old, new, e);
-			if (update_tcb) {
-				l2t_hold(L2DATA(tdev), e);
-				set_l2t_ix(tdev, tid, e);
+			update_tcb = te->client->redirect(te->ctx, old, new,
+							  e);
+			if (update_tcb)  {
+				l2t_hold(L2DATA(new_tdev), e);
+				set_l2t_ix(new_tdev, tid, e);
 			}
 		}
 	}
-	l2t_release(L2DATA(tdev), e);
+	l2t_release(L2DATA(new_tdev), e);
 }
+#endif
+
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#ifndef LINUX_2_4
+/*
+ * An administrator has requested that a set of offload policies be attached
+ * to the interface.  This functionality is actually managed by toecore and
+ * the new policy will be hung off this net_device's corresponding toedev but
+ * we don't have access to call toecore code.  Thus, we need to have one of
+ * our clients -- which can call toecore code -- proxy the call for us.
+ */
+int req_set_offload_policy(struct net_device *dev,
+			   const struct ofld_policy_file *opf,
+			   size_t len)
+{
+	struct cxgb3_client *client;
+	int found = 0;
+	int ret = -EINVAL;
+
+	/*
+	 * Make sure we're dealing with a network device with offload
+	 * activated ...
+	 */
+	if (!offload_activated(dev2t3cdev(dev)))
+		return ret;
+
+	mutex_lock(&cxgb3_db_lock);
+	list_for_each_entry(client, &client_list, client_list) {
+		/*
+		 * We want to restrict ourself to t3_tom module in order to
+		 * request our proxy service since A. it talks to toecore and
+		 * B. it's the only module which supports the extended
+		 * cxgb3_client data structure and has a set_offload_policy
+		 * structure element.
+		 */
+		if (client->name && strcmp(client->name, "tom_cxgb3") == 0 &&
+		    client->set_offload_policy) {
+			found = 1;
+			ret = client->set_offload_policy(dev, opf, len);
+			break;
+		}
+	}
+	mutex_unlock(&cxgb3_db_lock);
+	if (!found)
+		printk(KERN_ERR "req_set_offload_policy: no proxy found\n");
+	return ret;
+}
+#endif /* !LINUX_2_4 */
+#endif /* !CONFIG_XEN && !CONFIG_CRASH_DUMP */
 
 /*
  * Allocate a chunk of memory using kmalloc or, if that fails, vmalloc.
@@ -1170,16 +1793,73 @@
 }
 
 /*
- * Free memory allocated through t3_alloc_mem().
+ * Free memory allocated through cxgb3_alloc_mem().
  */
 void cxgb_free_mem(void *addr)
 {
-	if (is_vmalloc_addr(addr))
+	unsigned long p = (unsigned long) addr;
+
+	if (p >= VMALLOC_START && p < VMALLOC_END)
 		vfree(addr);
 	else
 		kfree(addr);
 }
 
+static int offload_info_read_proc(char *buf, char **start, off_t offset,
+				  int length, int *eof, void *data)
+{
+	struct t3c_data *d = data;
+	struct tid_info *t = &d->tid_maps;
+	int len;
+
+	len = sprintf(buf, "TID range: 0..%d, in use: %u\n"
+		      "STID range: %d..%d, in use: %u\n"
+		      "ATID range: %d..%d, in use: %u\n"
+		      "MSS: %u\n",
+		      t->ntids - 1, atomic_read(&t->tids_in_use), t->stid_base,
+		      t->stid_base + t->nstids - 1, t->stids_in_use,
+		      t->atid_base, t->atid_base + t->natids - 1,
+		      t->atids_in_use, d->tx_max_chunk);
+	if (len > length)
+		len = length;
+	*eof = 1;
+	return len;
+}
+
+static int offload_info_proc_setup(struct proc_dir_entry *dir,
+				   struct t3c_data *d)
+{
+	struct proc_dir_entry *p;
+
+	if (!dir)
+		return -EINVAL;
+
+	p = create_proc_read_entry("info", 0, dir, offload_info_read_proc, d);
+	if (!p)
+		return -ENOMEM;
+
+	SET_PROC_NODE_OWNER(p, THIS_MODULE);
+	return 0;
+}
+
+static void offload_proc_dev_setup(struct t3cdev *dev)
+{
+	t3_l2t_proc_setup(dev->proc_dir, L2DATA(dev));
+	offload_info_proc_setup(dev->proc_dir, T3C_DATA(dev));
+}
+
+static void offload_info_proc_free(struct proc_dir_entry *dir)
+{
+	if (dir)
+		remove_proc_entry("info", dir);
+}
+
+static void offload_proc_dev_cleanup(struct t3cdev *dev)
+{
+	t3_l2t_proc_free(dev->proc_dir);
+	offload_info_proc_free(dev->proc_dir);
+}
+
 /*
  * Allocate and initialize the TID tables.  Returns 0 on success.
  */
@@ -1229,20 +1909,6 @@
 	cxgb_free_mem(t->tid_tab);
 }
 
-static inline void add_adapter(struct adapter *adap)
-{
-	write_lock_bh(&adapter_list_lock);
-	list_add_tail(&adap->adapter_list, &adapter_list);
-	write_unlock_bh(&adapter_list_lock);
-}
-
-static inline void remove_adapter(struct adapter *adap)
-{
-	write_lock_bh(&adapter_list_lock);
-	list_del(&adap->adapter_list);
-	write_unlock_bh(&adapter_list_lock);
-}
-
 int cxgb3_offload_activate(struct adapter *adapter)
 {
 	struct t3cdev *dev = &adapter->tdev;
@@ -1279,23 +1945,31 @@
 	t->mtus = mtutab.mtus;
 	t->nmtus = mtutab.size;
 
-	INIT_WORK(&t->tid_release_task, t3_process_tid_release_list);
 	spin_lock_init(&t->tid_release_lock);
 	INIT_LIST_HEAD(&t->list_node);
 	t->dev = dev;
 
 	T3C_DATA(dev) = t;
 	dev->recv = process_rx;
+#if defined(NETEVENT)
 	dev->neigh_update = t3_l2t_update;
+#endif
+
+	T3_INIT_WORK(&t->tid_release_task, t3_process_tid_release_list, t);
+
+	offload_proc_dev_setup(dev);
 
 	/* Register netevent handler once */
-	if (list_empty(&adapter_list))
+	if (!atomic_read(&registered_ofld_adapters)) {
+#if defined(NETEVENT)
 		register_netevent_notifier(&nb);
+#elif defined(OFLD_USE_KPROBES)
+		if (prepare_arp_with_t3core())
+			printk(KERN_ERR "Unable to set offload capabilities\n");
+#endif
+	}
+	atomic_inc(&registered_ofld_adapters);
 
-	t->nofail_skb = alloc_skb(sizeof(struct cpl_tid_release), GFP_KERNEL);
-	t->release_list_incomplete = 0;
-
-	add_adapter(adapter);
 	return 0;
 
 out_free_l2t:
@@ -1311,25 +1985,28 @@
 	struct t3cdev *tdev = &adapter->tdev;
 	struct t3c_data *t = T3C_DATA(tdev);
 
-	remove_adapter(adapter);
-	if (list_empty(&adapter_list))
+	offload_proc_dev_cleanup(tdev);
+
+	atomic_dec(&registered_ofld_adapters);
+	if (!atomic_read(&registered_ofld_adapters)) {
+#if defined(NETEVENT)
 		unregister_netevent_notifier(&nb);
-
+#else
+#if defined(OFLD_USE_KPROBES)
+		restore_arp_sans_t3core();
+#endif
+#endif
+	}
 	free_tid_maps(&t->tid_maps);
 	T3C_DATA(tdev) = NULL;
 	t3_free_l2t(L2DATA(tdev));
 	L2DATA(tdev) = NULL;
-	if (t->nofail_skb)
-		kfree_skb(t->nofail_skb);
 	kfree(t);
 }
 
 static inline void register_tdev(struct t3cdev *tdev)
 {
-	static int unit;
-
 	mutex_lock(&cxgb3_db_lock);
-	snprintf(tdev->name, sizeof(tdev->name), "ofld_dev%d", unit++);
 	list_add_tail(&tdev->ofld_dev_list, &ofld_dev_list);
 	mutex_unlock(&cxgb3_db_lock);
 }
@@ -1359,7 +2036,7 @@
 	}
 	return type;
 }
-
+		
 void __devinit cxgb3_adapter_ofld(struct adapter *adapter)
 {
 	struct t3cdev *tdev = &adapter->tdev;
@@ -1378,12 +2055,41 @@
 {
 	struct t3cdev *tdev = &adapter->tdev;
 
-	tdev->recv = NULL;
-	tdev->neigh_update = NULL;
+	cxgb3_set_dummy_ops(tdev);
 
 	unregister_tdev(tdev);
 }
 
+int offload_devices_read_proc(char *buf, char **start, off_t offset,
+				     int length, int *eof, void *data)
+{
+	int i, len = 0;
+	struct t3cdev *tdev;
+	struct net_device *ndev;
+	struct adapter *adapter;
+
+	len += sprintf(buf, "Device           Interfaces\n");
+
+	mutex_lock(&cxgb3_db_lock);
+	list_for_each_entry(tdev, &ofld_dev_list, ofld_dev_list) {
+		len += sprintf(buf + len, "%-16s", tdev->name);
+		adapter = tdev2adap(tdev);
+		for (i = 0; i < adapter->params.nports; i++) {
+			ndev = adapter->port[i];
+			len += sprintf(buf + len, " %s", ndev->name);
+		}
+		len += sprintf(buf + len, "\n");
+		if (len >= length)
+			break;
+	}
+	mutex_unlock(&cxgb3_db_lock);
+
+	if (len > length)
+		len = length;
+	*eof = 1;
+	return len;
+}
+
 void __init cxgb3_offload_init(void)
 {
 	int i;
@@ -1409,12 +2115,19 @@
 	t3_register_cpl_handler(CPL_CLOSE_CON_RPL, do_hwtid_rpl);
 	t3_register_cpl_handler(CPL_ABORT_REQ_RSS, do_abort_req_rss);
 	t3_register_cpl_handler(CPL_ACT_ESTABLISH, do_act_establish);
-	t3_register_cpl_handler(CPL_SET_TCB_RPL, do_hwtid_rpl);
-	t3_register_cpl_handler(CPL_GET_TCB_RPL, do_hwtid_rpl);
 	t3_register_cpl_handler(CPL_RDMA_TERMINATE, do_term);
 	t3_register_cpl_handler(CPL_RDMA_EC_STATUS, do_hwtid_rpl);
 	t3_register_cpl_handler(CPL_TRACE_PKT, do_trace);
 	t3_register_cpl_handler(CPL_RX_DATA_DDP, do_hwtid_rpl);
 	t3_register_cpl_handler(CPL_RX_DDP_COMPLETE, do_hwtid_rpl);
+	/* for iSCSI */
 	t3_register_cpl_handler(CPL_ISCSI_HDR, do_hwtid_rpl);
+	t3_register_cpl_handler(CPL_GET_TCB_RPL, do_hwtid_rpl);
+	t3_register_cpl_handler(CPL_SET_TCB_RPL, do_hwtid_rpl);
+
 }
+
+void __exit cxgb3_offload_exit(void)
+{
+	//offload_proc_cleanup();
+}
diff --git a/drivers/net/cxgb3/cxgb3_offload.h b/drivers/net/cxgb3/cxgb3_offload.h
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/cxgb3_offload.h
+++ b/drivers/net/cxgb3/cxgb3_offload.h
@@ -1,54 +1,46 @@
 /*
- * Copyright (c) 2006-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef _CXGB3_OFFLOAD_H
 #define _CXGB3_OFFLOAD_H
 
 #include <linux/list.h>
 #include <linux/skbuff.h>
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+#include <net/offload.h>
+#endif
 
+#include "tcb.h"
 #include "l2t.h"
 
 #include "t3cdev.h"
 #include "t3_cpl.h"
 
 struct adapter;
+extern struct list_head adapter_list;
+extern rwlock_t adapter_list_lock;
 
 void cxgb3_offload_init(void);
+void cxgb3_offload_exit(void);
 
 void cxgb3_adapter_ofld(struct adapter *adapter);
 void cxgb3_adapter_unofld(struct adapter *adapter);
 int cxgb3_offload_activate(struct adapter *adapter);
 void cxgb3_offload_deactivate(struct adapter *adapter);
 
+int offload_devices_read_proc(char *buf, char **start, off_t offset,
+			      int length, int *eof, void *data);
+
+int cxgb3_ofld_recv(struct t3cdev *dev, struct sk_buff **skb, int n);
+
 void cxgb3_set_dummy_ops(struct t3cdev *dev);
 
 struct t3cdev *dev2t3cdev(struct net_device *dev);
@@ -64,27 +56,44 @@
 void cxgb3_unregister_client(struct cxgb3_client *client);
 void cxgb3_add_clients(struct t3cdev *tdev);
 void cxgb3_remove_clients(struct t3cdev *tdev);
-void cxgb3_event_notify(struct t3cdev *tdev, u32 event, u32 port);
+void cxgb3_err_notify(struct t3cdev *tdev, u32 status, u32 error);
 
 typedef int (*cxgb3_cpl_handler_func)(struct t3cdev *dev,
 				      struct sk_buff *skb, void *ctx);
-
 enum {
 	OFFLOAD_STATUS_UP,
 	OFFLOAD_STATUS_DOWN,
-	OFFLOAD_PORT_DOWN,
-	OFFLOAD_PORT_UP
+	OFFLOAD_DB_FULL,
+	OFFLOAD_DB_EMPTY,
+	OFFLOAD_DB_DROP
 };
 
 struct cxgb3_client {
-	char *name;
-	void (*add) (struct t3cdev *);
-	void (*remove) (struct t3cdev *);
-	cxgb3_cpl_handler_func *handlers;
-	int (*redirect)(void *ctx, struct dst_entry *old,
-			struct dst_entry *new, struct l2t_entry *l2t);
-	struct list_head client_list;
-	void (*event_handler)(struct t3cdev *tdev, u32 event, u32 port);
+	char 			*name;
+	void 			(*add) (struct t3cdev *);
+	void 			(*remove) (struct t3cdev *);
+	cxgb3_cpl_handler_func 	*handlers;
+	int			(*redirect)(void *ctx, struct dst_entry *old,
+					    struct dst_entry *new,
+					    struct l2t_entry *l2t);
+	struct list_head	client_list;
+
+	/*
+	 * End of legacy cxgb3_client data structure.  No part of the above
+	 * may be changed.
+	 */
+
+	/*
+	 * Start of t3_tom-extended cxgb3_client data structure.  Only t3_tom
+	 * has these fields ...
+	 */
+	/* Added in kernel.org */
+	void (*event_handler)(struct t3cdev *tdev, u32 status, u32 error);
+#if !defined(CONFIG_XEN) && !defined(CONFIG_CRASH_DUMP)
+	int			(*set_offload_policy)(struct net_device *,
+						      const struct ofld_policy_file *,
+						      size_t);
+#endif
 };
 
 /*
@@ -97,30 +106,31 @@
 void *cxgb3_free_atid(struct t3cdev *dev, int atid);
 void cxgb3_free_stid(struct t3cdev *dev, int stid);
 void cxgb3_insert_tid(struct t3cdev *dev, struct cxgb3_client *client,
-		      void *ctx, unsigned int tid);
+		      void *ctx,
+	unsigned int tid);
 void cxgb3_queue_tid_release(struct t3cdev *dev, unsigned int tid);
 void cxgb3_remove_tid(struct t3cdev *dev, void *ctx, unsigned int tid);
 
 struct t3c_tid_entry {
-	struct cxgb3_client *client;
-	void *ctx;
+	struct cxgb3_client 	*client;
+	void 			*ctx;
 };
 
 /* CPL message priority levels */
 enum {
-	CPL_PRIORITY_DATA = 0,	/* data messages */
-	CPL_PRIORITY_SETUP = 1,	/* connection setup messages */
-	CPL_PRIORITY_TEARDOWN = 0,	/* connection teardown messages */
-	CPL_PRIORITY_LISTEN = 1,	/* listen start/stop messages */
-	CPL_PRIORITY_ACK = 1,	/* RX ACK messages */
-	CPL_PRIORITY_CONTROL = 1	/* offload control messages */
+	CPL_PRIORITY_DATA = 0,     /* data messages */
+	CPL_PRIORITY_SETUP = 1,	   /* connection setup messages */
+	CPL_PRIORITY_TEARDOWN = 0, /* connection teardown messages */
+	CPL_PRIORITY_LISTEN = 1,   /* listen start/stop messages */
+	CPL_PRIORITY_ACK = 1,      /* RX ACK messages */
+	CPL_PRIORITY_CONTROL = 1   /* offload control messages */
 };
 
 /* Flags for return value of CPL message handlers */
 enum {
-	CPL_RET_BUF_DONE = 1, /* buffer processing done, buffer may be freed */
-	CPL_RET_BAD_MSG = 2,  /* bad CPL message (e.g., unknown opcode) */
-	CPL_RET_UNKNOWN_TID = 4	/* unexpected unknown TID */
+	CPL_RET_BUF_DONE = 1,   // buffer processing done, buffer may be freed
+	CPL_RET_BAD_MSG = 2,    // bad CPL message (e.g., unknown opcode)
+	CPL_RET_UNKNOWN_TID = 4	// unexpected unknown TID
 };
 
 typedef int (*cpl_handler_func)(struct t3cdev *dev, struct sk_buff *skb);
@@ -184,8 +194,8 @@
 struct t3c_data {
 	struct list_head list_node;
 	struct t3cdev *dev;
-	unsigned int tx_max_chunk;	/* max payload for TX_DATA */
-	unsigned int max_wrs;	/* max in-flight WRs per connection */
+	unsigned int tx_max_chunk;  /* max payload for TX_DATA */
+	unsigned int max_wrs;       /* max in-flight WRs per connection */
 	unsigned int nmtus;
 	const unsigned short *mtus;
 	struct tid_info tid_maps;
@@ -193,9 +203,6 @@
 	struct t3c_tid_entry *tid_release_list;
 	spinlock_t tid_release_lock;
 	struct work_struct tid_release_task;
-
-	struct sk_buff *nofail_skb;
-	unsigned int release_list_incomplete;
 };
 
 /*
diff --git a/drivers/net/cxgb3/firmware_exports.h b/drivers/net/cxgb3/firmware_exports.h
--- a/drivers/net/cxgb3/firmware_exports.h
+++ b/drivers/net/cxgb3/firmware_exports.h
@@ -1,33 +1,27 @@
-/*
- * Copyright (c) 2004-2008 Chelsio, Inc. All rights reserved.
+/* 
+ * ----------------------------------------------------------------------------
+ * >>>>>>>>>>>>>>>>>>>>>>>>>>>>> COPYRIGHT NOTICE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<
+ * ----------------------------------------------------------------------------
+ * Copyright 2004-2009 (C) Chelsio Communications, Inc. (Chelsio)
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Chelsio Communications, Inc. owns the sole copyright to this software.
+ * You may not make a copy, you may not derive works herefrom, and you may
+ * not distribute this work to others. Other restrictions of rights may apply
+ * as well. This is unpublished, confidential information. All rights reserved.
+ * This software contains confidential information and trade secrets of Chelsio
+ * Communications, Inc. Use, disclosure, or reproduction is prohibited without
+ * the prior express written permission of Chelsio Communications, Inc.
+ * ----------------------------------------------------------------------------
+ * >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Warranty <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
+ * ----------------------------------------------------------------------------
+ * CHELSIO MAKES NO WARRANTY OF ANY KIND WITH REGARD TO THE USE OF THIS
+ * SOFTWARE, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.
+ * ----------------------------------------------------------------------------
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
+ * This is the firmware_exports.h header file, firmware interface defines.
  *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * Written January 2005 by felix marti (felix@chelsio.com)
  */
 #ifndef _FIRMWARE_EXPORTS_H_
 #define _FIRMWARE_EXPORTS_H_
@@ -69,21 +63,27 @@
 #define FW_WROPCODE_RI_MODIFY_QP		0x1B
 #define FW_WROPCODE_RI_BYPASS			0x1C
 
-#define FW_WROPOCDE_RSVD			0x1E
+#define FW_WROPOCDE_READ_AND_FLUSH		0x1E
 
 #define FW_WROPCODE_SGE_EGRESSCONTEXT_RR	0x1F
 
 #define FW_WROPCODE_MNGT			0x1D
 #define FW_MNGTOPCODE_PKTSCHED_SET		0x00
+#define FW_MNGTOPCODE_WRC_SET			0x01
+#define FW_MNGTOPCODE_TUNNEL_CR_FLUSH		0x02
+#define FW_MNGTOPCODE_WRC_ULPTX_CHANNEL_SET	0x03
+#define FW_MNGTOPCODE_SGECMD			0x04
+#define FW_MNGTOPCODE_DRIVERWATCHDOG		0x05
+#define FW_MNGTOPCODE_FIRMWAREWATCHDOG		0x06
 
-/* Maximum size of a WR sent from the host, limited by the SGE.
+/* Maximum size of a WR sent from the host, limited by the SGE. 
  *
- * Note: WR coming from ULP or TP are only limited by CIM.
+ * Note: WR coming from ULP or TP are only limited by CIM. 
  */
 #define FW_WR_SIZE			128
 
 /* Maximum number of outstanding WRs sent from the host. Value must be
- * programmed in the CTRL/TUNNEL/QP SGE Egress Context and used by
+ * programmed in the CTRL/TUNNEL/QP SGE Egress Context and used by 
  * offload modules to limit the number of WRs per connection.
  */
 #define FW_T3_WR_NUM			16
@@ -99,27 +99,28 @@
  * queues must start at SGE Egress Context FW_TUNNEL_SGEEC_START and must
  * start at 'TID' (or 'uP Token') FW_TUNNEL_TID_START.
  *
- * Ingress Traffic (e.g. DMA completion credit)  for TUNNEL Queue[i] is sent
+ * Ingress Traffic (e.g. DMA completion credit)  for TUNNEL Queue[i] is sent 
  * to RESP Queue[i].
  */
 #define FW_TUNNEL_NUM			8
 #define FW_TUNNEL_SGEEC_START		8
 #define FW_TUNNEL_TID_START		65544
 
+
 /* FW_CTRL_NUM corresponds to the number of supported CTRL Queues. These queues
  * must start at SGE Egress Context FW_CTRL_SGEEC_START and must start at 'TID'
  * (or 'uP Token') FW_CTRL_TID_START.
  *
  * Ingress Traffic for CTRL Queue[i] is sent to RESP Queue[i].
- */
+ */ 
 #define FW_CTRL_NUM			8
 #define FW_CTRL_SGEEC_START		65528
 #define FW_CTRL_TID_START		65536
 
-/* FW_OFLD_NUM corresponds to the number of supported OFFLOAD Queues. These
- * queues must start at SGE Egress Context FW_OFLD_SGEEC_START.
- *
- * Note: the 'uP Token' in the SGE Egress Context fields is irrelevant for
+/* FW_OFLD_NUM corresponds to the number of supported OFFLOAD Queues. These 
+ * queues must start at SGE Egress Context FW_OFLD_SGEEC_START. 
+ * 
+ * Note: the 'uP Token' in the SGE Egress Context fields is irrelevant for 
  * OFFLOAD Queues, as the host is responsible for providing the correct TID in
  * every WR.
  *
@@ -136,7 +137,7 @@
 #define FW_RI_TID_START			65552
 
 /*
- * The RX_PKT_TID
+ * The RX_PKT_TID 
  */
 #define FW_RX_PKT_NUM			1
 #define FW_RX_PKT_TID_START		65553
@@ -174,4 +175,4 @@
 #define G_FW_VERSION_MICRO(x)		\
     (((x) >> S_FW_VERSION_MICRO) & M_FW_VERSION_MICRO)
 
-#endif				/* _FIRMWARE_EXPORTS_H_ */
+#endif /* _FIRMWARE_EXPORTS_H_ */
diff --git a/drivers/net/cxgb3/l2t.c b/drivers/net/cxgb3/l2t.c
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/l2t.c
+++ b/drivers/net/cxgb3/l2t.c
@@ -1,34 +1,14 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include <linux/skbuff.h>
 #include <linux/netdevice.h>
 #include <linux/if.h>
@@ -77,6 +57,23 @@
 	e->neigh = n;
 }
 
+static void setup_l2e(struct t3cdev *dev, struct sk_buff *skb,
+		      struct l2t_entry *e)
+{
+	struct cpl_l2t_write_req *req;
+
+	req = (struct cpl_l2t_write_req *)__skb_put(skb, sizeof(*req));
+	req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
+	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_L2T_WRITE_REQ, e->idx));
+	req->params = htonl(V_L2T_W_IDX(e->idx) | V_L2T_W_IFF(e->smt_idx) |
+			    V_L2T_W_VLAN(e->vlan & VLAN_VID_MASK) |
+			    V_L2T_W_PRIO(vlan_prio(e)));
+	req->port_idx = e->smt_idx;
+	memcpy(req->dst_mac, e->dmac, sizeof(req->dst_mac));
+	skb->priority = CPL_PRIORITY_CONTROL;
+	cxgb3_ofld_send(dev, skb);
+}
+
 /*
  * Set up an L2T entry and send any packets waiting in the arp queue.  The
  * supplied skb is used for the CPL_L2T_WRITE_REQ.  Must be called with the
@@ -85,42 +82,56 @@
 static int setup_l2e_send_pending(struct t3cdev *dev, struct sk_buff *skb,
 				  struct l2t_entry *e)
 {
-	struct cpl_l2t_write_req *req;
-	struct sk_buff *tmp;
 
 	if (!skb) {
-		skb = alloc_skb(sizeof(*req), GFP_ATOMIC);
+		skb = alloc_skb(sizeof(struct cpl_l2t_write_req), GFP_ATOMIC);
 		if (!skb)
 			return -ENOMEM;
 	}
 
-	req = (struct cpl_l2t_write_req *)__skb_put(skb, sizeof(*req));
-	req->wr.wr_hi = htonl(V_WR_OP(FW_WROPCODE_FORWARD));
-	OPCODE_TID(req) = htonl(MK_OPCODE_TID(CPL_L2T_WRITE_REQ, e->idx));
-	req->params = htonl(V_L2T_W_IDX(e->idx) | V_L2T_W_IFF(e->smt_idx) |
-			    V_L2T_W_VLAN(e->vlan & VLAN_VID_MASK) |
-			    V_L2T_W_PRIO(vlan_prio(e)));
 	memcpy(e->dmac, e->neigh->ha, sizeof(e->dmac));
-	memcpy(req->dst_mac, e->dmac, sizeof(req->dst_mac));
-	skb->priority = CPL_PRIORITY_CONTROL;
-	cxgb3_ofld_send(dev, skb);
+	setup_l2e(dev, skb, e);
 
-	skb_queue_walk_safe(&e->arpq, skb, tmp) {
-		__skb_unlink(skb, &e->arpq);
+	while (e->arpq_head) {
+		skb = e->arpq_head;
+		e->arpq_head = skb->next;
+		skb->next = NULL;
 		cxgb3_ofld_send(dev, skb);
 	}
+	e->arpq_tail = NULL;
 	e->state = L2T_STATE_VALID;
 
 	return 0;
 }
 
 /*
+ * Update an L2T entry.
+ * Must be called with the entry locked.
+ */
+int t3_l2t_update_l2e(struct t3cdev *dev, struct l2t_entry *e)
+{
+	struct sk_buff * skb = alloc_skb(sizeof(struct cpl_l2t_write_req),
+					 GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	setup_l2e(dev, skb, e);
+
+	return 0;
+}
+
+/*
  * Add a packet to the an L2T entry's queue of packets awaiting resolution.
  * Must be called with the entry's lock held.
  */
 static inline void arpq_enqueue(struct l2t_entry *e, struct sk_buff *skb)
 {
-	__skb_queue_tail(&e->arpq, skb);
+	skb->next = NULL;
+	if (e->arpq_head)
+		e->arpq_tail->next = skb;
+	else
+		e->arpq_head = skb;
+	e->arpq_tail = skb;
 }
 
 int t3_l2t_send_slow(struct t3cdev *dev, struct sk_buff *skb,
@@ -128,18 +139,17 @@
 {
 again:
 	switch (e->state) {
-	case L2T_STATE_STALE:	/* entry is stale, kick off revalidation */
+	case L2T_STATE_STALE:     /* entry is stale, kick off revalidation */
 		neigh_event_send(e->neigh, NULL);
 		spin_lock_bh(&e->lock);
 		if (e->state == L2T_STATE_STALE)
 			e->state = L2T_STATE_VALID;
 		spin_unlock_bh(&e->lock);
-	case L2T_STATE_VALID:	/* fast-path, send the packet on */
+	case L2T_STATE_VALID:     /* fast-path, send the packet on */
 		return cxgb3_ofld_send(dev, skb);
 	case L2T_STATE_RESOLVING:
 		spin_lock_bh(&e->lock);
-		if (e->state != L2T_STATE_RESOLVING) {
-			/* ARP already completed */
+		if (e->state != L2T_STATE_RESOLVING) { // ARP already completed
 			spin_unlock_bh(&e->lock);
 			goto again;
 		}
@@ -161,23 +171,22 @@
 				break;
 
 			spin_lock_bh(&e->lock);
-			if (!skb_queue_empty(&e->arpq))
+			if (e->arpq_head)
 				setup_l2e_send_pending(dev, skb, e);
-			else	/* we lost the race */
+			else                           /* we lost the race */
 				__kfree_skb(skb);
 			spin_unlock_bh(&e->lock);
 		}
 	}
 	return 0;
 }
-
 EXPORT_SYMBOL(t3_l2t_send_slow);
 
 void t3_l2t_send_event(struct t3cdev *dev, struct l2t_entry *e)
 {
 again:
 	switch (e->state) {
-	case L2T_STATE_STALE:	/* entry is stale, kick off revalidation */
+	case L2T_STATE_STALE:     /* entry is stale, kick off revalidation */
 		neigh_event_send(e->neigh, NULL);
 		spin_lock_bh(&e->lock);
 		if (e->state == L2T_STATE_STALE) {
@@ -185,12 +194,11 @@
 		}
 		spin_unlock_bh(&e->lock);
 		return;
-	case L2T_STATE_VALID:	/* fast-path, send the packet on */
+	case L2T_STATE_VALID:     /* fast-path, send the packet on */
 		return;
 	case L2T_STATE_RESOLVING:
 		spin_lock_bh(&e->lock);
-		if (e->state != L2T_STATE_RESOLVING) {
-			/* ARP already completed */
+		if (e->state != L2T_STATE_RESOLVING) { // ARP already completed
 			spin_unlock_bh(&e->lock);
 			goto again;
 		}
@@ -208,7 +216,6 @@
 	}
 	return;
 }
-
 EXPORT_SYMBOL(t3_l2t_send_event);
 
 /*
@@ -262,7 +269,7 @@
 void t3_l2e_free(struct l2t_data *d, struct l2t_entry *e)
 {
 	spin_lock_bh(&e->lock);
-	if (atomic_read(&e->refcnt) == 0) {	/* hasn't been recycled */
+	if (atomic_read(&e->refcnt) == 0) {  /* hasn't been recycled */
 		if (e->neigh) {
 			neigh_release(e->neigh);
 			e->neigh = NULL;
@@ -271,7 +278,6 @@
 	spin_unlock_bh(&e->lock);
 	atomic_inc(&d->nfree);
 }
-
 EXPORT_SYMBOL(t3_l2e_free);
 
 /*
@@ -282,7 +288,7 @@
 {
 	unsigned int nud_state;
 
-	spin_lock(&e->lock);	/* avoid race with t3_l2t_free */
+	spin_lock(&e->lock);                /* avoid race with t3_l2t_free */
 
 	if (neigh != e->neigh)
 		neigh_replace(e, neigh);
@@ -321,13 +327,15 @@
 	/* Need to allocate a new entry */
 	e = alloc_l2e(d);
 	if (e) {
-		spin_lock(&e->lock);	/* avoid race with t3_l2t_free */
+		spin_lock(&e->lock);          /* avoid race with t3_l2t_free */
 		e->next = d->l2tab[hash].first;
 		d->l2tab[hash].first = e;
 		e->state = L2T_STATE_RESOLVING;
 		e->addr = addr;
 		e->ifindex = ifidx;
 		e->smt_idx = smt_idx;
+		e->orig_smt_idx = smt_idx;
+		e->chan_idx = p->txpkt_intf & 1;
 		atomic_set(&e->refcnt, 1);
 		neigh_replace(e, neigh);
 		if (neigh->dev->priv_flags & IFF_802_1Q_VLAN)
@@ -340,7 +348,6 @@
 	write_unlock_bh(&d->lock);
 	return e;
 }
-
 EXPORT_SYMBOL(t3_l2t_get);
 
 /*
@@ -351,14 +358,14 @@
  * XXX: maybe we should abandon the latter behavior and just require a failure
  * handler.
  */
-static void handle_failed_resolution(struct t3cdev *dev, struct sk_buff_head *arpq)
+static void handle_failed_resolution(struct t3cdev *dev, struct sk_buff *arpq)
 {
-	struct sk_buff *skb, *tmp;
-
-	skb_queue_walk_safe(arpq, skb, tmp) {
+	while (arpq) {
+		struct sk_buff *skb = arpq;
 		struct l2t_skb_cb *cb = L2T_SKB_CB(skb);
 
-		__skb_unlink(skb, arpq);
+		arpq = skb->next;
+		skb->next = NULL;
 		if (cb->arp_failure_handler)
 			cb->arp_failure_handler(dev, skb);
 		else
@@ -366,14 +373,15 @@
 	}
 }
 
+#if defined(NETEVENT) || !defined(OFLD_USE_KPROBES)
 /*
  * Called when the host's ARP layer makes a change to some entry that is
  * loaded into the HW L2 table.
  */
 void t3_l2t_update(struct t3cdev *dev, struct neighbour *neigh)
 {
-	struct sk_buff_head arpq;
 	struct l2t_entry *e;
+	struct sk_buff *arpq = NULL;
 	struct l2t_data *d = L2DATA(dev);
 	u32 addr = *(u32 *) neigh->primary_key;
 	int ifidx = neigh->dev->ifindex;
@@ -389,8 +397,6 @@
 	return;
 
 found:
-	__skb_queue_head_init(&arpq);
-
 	read_unlock(&d->lock);
 	if (atomic_read(&e->refcnt)) {
 		if (neigh != e->neigh)
@@ -398,21 +404,92 @@
 
 		if (e->state == L2T_STATE_RESOLVING) {
 			if (neigh->nud_state & NUD_FAILED) {
-				skb_queue_splice_init(&e->arpq, &arpq);
+				arpq = e->arpq_head;
+				e->arpq_head = e->arpq_tail = NULL;
 			} else if (neigh->nud_state & (NUD_CONNECTED|NUD_STALE))
 				setup_l2e_send_pending(dev, NULL, e);
 		} else {
 			e->state = neigh->nud_state & NUD_CONNECTED ?
-			    L2T_STATE_VALID : L2T_STATE_STALE;
+				L2T_STATE_VALID : L2T_STATE_STALE;
 			if (memcmp(e->dmac, neigh->ha, 6))
 				setup_l2e_send_pending(dev, NULL, e);
 		}
 	}
 	spin_unlock_bh(&e->lock);
 
-	if (!skb_queue_empty(&arpq))
-		handle_failed_resolution(dev, &arpq);
+	if (arpq)
+		handle_failed_resolution(dev, arpq);
 }
+#else
+/*
+ * Called from a kprobe, interrupts are off.
+ */
+void t3_l2t_update(struct t3cdev *dev, struct neighbour *neigh)
+{
+	struct l2t_entry *e;
+	struct l2t_data *d = L2DATA(dev);
+	u32 addr = *(u32 *) neigh->primary_key;
+	int ifidx = neigh->dev->ifindex;
+	int hash = arp_hash(addr, ifidx, d);
+
+	read_lock(&d->lock);
+	for (e = d->l2tab[hash].first; e; e = e->next)
+		if (e->addr == addr && e->ifindex == ifidx) {
+			spin_lock(&e->lock);
+			if (atomic_read(&e->refcnt)) {
+				if (neigh != e->neigh)
+					neigh_replace(e, neigh);
+				e->tdev = dev;
+				mod_timer(&e->update_timer, jiffies + 1);
+			}
+			spin_unlock(&e->lock);
+			break;
+		}
+	read_unlock(&d->lock);
+}
+
+static void update_timer_cb(unsigned long data)
+{
+	struct sk_buff *arpq = NULL;
+	struct l2t_entry *e = (struct l2t_entry *)data;
+	struct neighbour *neigh;
+	struct t3cdev *dev = e->tdev;
+
+	spin_lock(&e->lock);
+	neigh = e->neigh;
+	if (neigh)
+		neigh_hold(neigh);
+	spin_unlock(&e->lock);
+
+	if (!neigh)
+		return;
+
+	read_lock(&neigh->lock);
+	spin_lock(&e->lock);
+
+	if (atomic_read(&e->refcnt) && neigh == e->neigh) {
+		if (e->state == L2T_STATE_RESOLVING) {
+			if (neigh->nud_state & NUD_FAILED) {
+				arpq = e->arpq_head;
+				e->arpq_head = e->arpq_tail = NULL;
+			} else if ((neigh->nud_state &
+				    (NUD_CONNECTED|NUD_STALE)) && e->arpq_head)
+				setup_l2e_send_pending(dev, NULL, e);
+		} else {
+			e->state = neigh->nud_state & NUD_CONNECTED ?
+				L2T_STATE_VALID : L2T_STATE_STALE;
+			if (memcmp(e->dmac, neigh->ha, sizeof(e->dmac)))
+				setup_l2e_send_pending(dev, NULL, e);
+		}
+	}
+	spin_unlock(&e->lock);
+	read_unlock(&neigh->lock);
+	neigh_release(neigh);
+
+	if (arpq)
+		handle_failed_resolution(dev, arpq);
+}
+#endif
 
 struct l2t_data *t3_init_l2t(unsigned int l2t_capacity)
 {
@@ -431,15 +508,146 @@
 	for (i = 0; i < l2t_capacity; ++i) {
 		d->l2tab[i].idx = i;
 		d->l2tab[i].state = L2T_STATE_UNUSED;
-		__skb_queue_head_init(&d->l2tab[i].arpq);
 		spin_lock_init(&d->l2tab[i].lock);
 		atomic_set(&d->l2tab[i].refcnt, 0);
+#ifndef NETEVENT
+#ifdef OFLD_USE_KPROBES
+		setup_timer(&d->l2tab[i].update_timer, update_timer_cb,
+			    (unsigned long)&d->l2tab[i]);
+#endif
+#endif
 	}
 	return d;
 }
 
 void t3_free_l2t(struct l2t_data *d)
 {
+#ifndef NETEVENT
+#ifdef OFLD_USE_KPROBES
+	int i;
+
+	/* Stop all L2T timers */
+	for (i = 0; i < d->nentries; ++i)
+		del_timer_sync(&d->l2tab[i].update_timer);
+#endif
+#endif
 	cxgb_free_mem(d);
 }
 
+#ifdef CONFIG_PROC_FS
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+
+static inline void *l2t_get_idx(struct seq_file *seq, loff_t pos)
+{
+	struct l2t_data *d = seq->private;
+
+	return pos >= d->nentries ? NULL : &d->l2tab[pos];
+}
+
+static void *l2t_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	return *pos ? l2t_get_idx(seq, *pos) : SEQ_START_TOKEN;
+}
+
+static void *l2t_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	v = l2t_get_idx(seq, *pos + 1);
+	if (v)
+		++*pos;
+	return v;
+}
+
+static void l2t_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static char l2e_state(const struct l2t_entry *e)
+{
+	switch (e->state) {
+	case L2T_STATE_VALID: return 'V';  /* valid, fast-path entry */
+	case L2T_STATE_STALE: return 'S';  /* needs revalidation, but usable */
+	case L2T_STATE_RESOLVING:
+		return e->arpq_head ? 'A' : 'R';
+	default:
+		return 'U';
+	}
+}
+
+static int l2t_seq_show(struct seq_file *seq, void *v)
+{
+	if (v == SEQ_START_TOKEN)
+		seq_puts(seq, "Index IP address      Ethernet address   VLAN  "
+			 "Prio  State   Users SMTIDX  Port\n");
+	else {
+		char ip[20];
+		struct l2t_entry *e = v;
+
+		spin_lock_bh(&e->lock);
+		sprintf(ip, "%u.%u.%u.%u", NIPQUAD(e->addr));
+		seq_printf(seq, "%-5u %-15s %02x:%02x:%02x:%02x:%02x:%02x  %4d"
+			   "  %3u     %c   %7u   %4u %s\n",
+			   e->idx, ip, e->dmac[0], e->dmac[1], e->dmac[2],
+			   e->dmac[3], e->dmac[4], e->dmac[5],
+			   e->vlan & VLAN_VID_MASK, vlan_prio(e),
+			   l2e_state(e), atomic_read(&e->refcnt), e->smt_idx,
+			   e->neigh ? e->neigh->dev->name : "");
+		spin_unlock_bh(&e->lock);
+	}
+	return 0;
+}
+
+static struct seq_operations l2t_seq_ops = {
+	.start = l2t_seq_start,
+	.next = l2t_seq_next,
+	.stop = l2t_seq_stop,
+	.show = l2t_seq_show
+};
+
+static int l2t_seq_open(struct inode *inode, struct file *file)
+{
+	int rc = seq_open(file, &l2t_seq_ops);
+
+	if (!rc) {
+		struct proc_dir_entry *dp = PDE(inode);
+		struct seq_file *seq = file->private_data;
+
+		seq->private = dp->data;
+	}
+	return rc;
+}
+
+static struct file_operations l2t_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = l2t_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+/*
+ * Create the proc entries for the L2 table under dir.
+ */
+int t3_l2t_proc_setup(struct proc_dir_entry *dir, struct l2t_data *d)
+{
+	struct proc_dir_entry *p;
+
+	if (!dir)
+		return -EINVAL;
+
+	p = create_proc_entry("l2t", S_IRUGO, dir);
+	if (!p)
+		return -ENOMEM;
+
+	p->proc_fops = &l2t_seq_fops;
+	p->data = d;
+	return 0;
+}
+
+void t3_l2t_proc_free(struct proc_dir_entry *dir)
+{
+	if (dir)
+		remove_proc_entry("l2t", dir);
+}
+#endif
diff --git a/drivers/net/cxgb3/l2t.h b/drivers/net/cxgb3/l2t.h
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/l2t.h
+++ b/drivers/net/cxgb3/l2t.h
@@ -1,46 +1,29 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef _CHELSIO_L2T_H
 #define _CHELSIO_L2T_H
 
+#ifndef AUTOCONF_INCLUDED
+#include <linux/autoconf.h>
+#endif
 #include <linux/spinlock.h>
 #include "t3cdev.h"
 #include <asm/atomic.h>
 
 enum {
-	L2T_STATE_VALID,	/* entry is up to date */
-	L2T_STATE_STALE,	/* entry may be used but needs revalidation */
-	L2T_STATE_RESOLVING,	/* entry needs address resolution */
-	L2T_STATE_UNUSED	/* entry not in use */
+	L2T_STATE_VALID,      /* entry is up to date */
+	L2T_STATE_STALE,      /* entry may be used but needs revalidation */
+	L2T_STATE_RESOLVING,  /* entry needs address resolution */
+	L2T_STATE_UNUSED      /* entry not in use */
 };
 
 struct neighbour;
@@ -55,31 +38,40 @@
  * first element in its chain through its first pointer.
  */
 struct l2t_entry {
-	u16 state;		/* entry state */
-	u16 idx;		/* entry index */
-	u32 addr;		/* dest IP address */
-	int ifindex;		/* neighbor's net_device's ifindex */
-	u16 smt_idx;		/* SMT index */
-	u16 vlan;		/* VLAN TCI (id: bits 0-11, prio: 13-15 */
-	struct neighbour *neigh;	/* associated neighbour */
-	struct l2t_entry *first;	/* start of hash chain */
-	struct l2t_entry *next;	/* next l2t_entry on chain */
-	struct sk_buff_head arpq;	/* queue of packets awaiting resolution */
+	u16 state;                  /* entry state */
+	u16 idx;                    /* entry index */
+	u32 addr;                   /* dest IP address */
+	int ifindex;                /* neighbor's net_device's ifindex */
+	u16 smt_idx;                /* SMT index */
+	u16 vlan;                   /* VLAN TCI (id: bits 0-11, prio: 13-15 */
+	struct neighbour *neigh;    /* associated neighbour */
+	struct l2t_entry *first;    /* start of hash chain */
+	struct l2t_entry *next;     /* next l2t_entry on chain */
+	struct sk_buff *arpq_head;  /* queue of packets awaiting resolution */
+	struct sk_buff *arpq_tail;
 	spinlock_t lock;
-	atomic_t refcnt;	/* entry reference count */
-	u8 dmac[6];		/* neighbour's MAC address */
+	atomic_t refcnt;            /* entry reference count */
+	u8 dmac[6];                 /* neighbour's MAC address */
+	u8 chan_idx;                /* channel index */
+	u16 orig_smt_idx;           /* original SMT index in a bond */
+#ifndef NETEVENT
+#ifdef OFLD_USE_KPROBES
+	struct timer_list update_timer;
+	struct t3cdev *tdev;
+#endif
+#endif
 };
 
 struct l2t_data {
-	unsigned int nentries;	/* number of entries */
-	struct l2t_entry *rover;	/* starting point for next allocation */
-	atomic_t nfree;		/* number of free entries */
+	unsigned int nentries;      /* number of entries */
+	struct l2t_entry *rover;    /* starting point for next allocation */
+	atomic_t nfree;             /* number of free entries */
 	rwlock_t lock;
 	struct l2t_entry l2tab[0];
 };
 
-typedef void (*arp_failure_handler_func)(struct t3cdev * dev,
-					 struct sk_buff * skb);
+typedef void (*arp_failure_handler_func)(struct t3cdev *dev,
+					 struct sk_buff *skb);
 
 /*
  * Callback stored in an skb to handle address resolution failure.
@@ -101,11 +93,6 @@
  */
 #define L2DATA(dev) ((dev)->l2opt)
 
-#define W_TCB_L2T_IX    0
-#define S_TCB_L2T_IX    7
-#define M_TCB_L2T_IX    0x7ffULL
-#define V_TCB_L2T_IX(x) ((x) << S_TCB_L2T_IX)
-
 void t3_l2e_free(struct l2t_data *d, struct l2t_entry *e);
 void t3_l2t_update(struct t3cdev *dev, struct neighbour *neigh);
 struct l2t_entry *t3_l2t_get(struct t3cdev *cdev, struct neighbour *neigh,
@@ -115,6 +102,15 @@
 void t3_l2t_send_event(struct t3cdev *dev, struct l2t_entry *e);
 struct l2t_data *t3_init_l2t(unsigned int l2t_capacity);
 void t3_free_l2t(struct l2t_data *d);
+int t3_l2t_update_l2e(struct t3cdev *dev, struct l2t_entry *e);
+
+#ifdef CONFIG_PROC_FS
+int t3_l2t_proc_setup(struct proc_dir_entry *dir, struct l2t_data *d);
+void t3_l2t_proc_free(struct proc_dir_entry *dir);
+#else
+#define l2t_proc_setup(dir, d) 0
+#define l2t_proc_free(dir)
+#endif
 
 int cxgb3_ofld_send(struct t3cdev *dev, struct sk_buff *skb);
 
@@ -134,7 +130,7 @@
 
 static inline void l2t_hold(struct l2t_data *d, struct l2t_entry *e)
 {
-	if (atomic_add_return(1, &e->refcnt) == 1)	/* 0 -> 1 transition */
+	if (atomic_add_return(1, &e->refcnt) == 1)  /* 0 -> 1 transition */
 		atomic_dec(&d->nfree);
 }
 
diff --git a/drivers/net/cxgb3/linux_2_4_compat.c b/drivers/net/cxgb3/linux_2_4_compat.c
new file mode 100755
--- /dev/null
+++ b/drivers/net/cxgb3/linux_2_4_compat.c
@@ -0,0 +1,224 @@
+/*
+ * Copyright (c) 2003-2009 Chelsio, Inc. All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/mii.h>
+#include <linux/sockios.h>
+#include <linux/proc_fs.h>
+#include <linux/rtnetlink.h>
+#include <asm/uaccess.h>
+#include <linux/pci.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/timer.h>
+#include <linux/cache.h>
+#include <asm/atomic.h>
+#include <asm/semaphore.h>
+#include <asm/bitops.h>
+#include <asm/io.h>
+#include "common.h"
+
+int atomic_add_return(int i, atomic_t *v)
+{
+        int __i;
+        /* Modern 486+ processor */
+        __i = i;
+        __asm__ __volatile__(
+                LOCK_PREFIX "xaddl %0, %1;"
+                :"=r"(i)
+                :"m"(v->counter), "0"(i));
+        return i + __i;
+}
+
+__inline__ int generic_fls(int x)
+{
+        int r = 32;
+
+        if (!x)
+                return 0;
+        if (!(x & 0xffff0000)) {
+                x <<= 16;
+                r -= 16;
+        }
+        if (!(x & 0xff000000)) {
+                x <<= 8;
+                r -= 8;
+        }
+        if (!(x & 0xf0000000)) {
+                x <<= 4;
+                r -= 4;
+        }
+        if (!(x & 0xc0000000)) {
+                x <<= 2;
+                r -= 2;
+        }
+        if (!(x & 0x80000000)) {
+                x <<= 1;
+                r -= 1;
+        }
+        return r;
+}
+
+inline int t3_os_pci_save_state(struct adapter *adapter)
+{
+        return pci_save_state(adapter->pdev, adapter->t3_config_space);
+}
+
+inline int t3_os_pci_restore_state(struct adapter *adapter)
+{
+        return pci_restore_state(adapter->pdev, adapter->t3_config_space);
+}
+
+#ifndef CONFIG_PCI_MSI
+
+int pci_enable_msi(struct pci_dev* dev)
+{
+	return(-EINVAL);
+}
+
+int pci_disable_msi(struct pci_dev* dev)
+{
+	return(-EINVAL);
+}
+
+int pci_enable_msix(struct pci_dev* dev, 
+				  struct msix_entry *entries, int nvec)
+{
+	return(-EINVAL);
+}
+
+int pci_disable_msix(struct pci_dev* dev)
+{
+	return(-EINVAL);
+}
+
+#endif /* !CONFIG_PCI_MSI */
+
+void *kzalloc(size_t size, gfp_t flags)
+{
+	void *p;
+
+	p = kmalloc(size, flags);
+	if (p != NULL)
+		memset(p, 0, size);
+        return(p);
+}
+
+void *kcalloc(size_t n, size_t size, gfp_t flags)
+{
+        if (n != 0 && size > ULONG_MAX / n)
+                return NULL;
+        return kzalloc(n * size, flags);
+}
+
+#ifndef ALLOC_NETDEV
+struct net_device *alloc_netdev(int sizeof_priv, const char *mask,
+                                       void (*setup)(struct net_device *))
+{
+        struct net_device *dev;
+        int alloc_size;
+
+        /* ensure 32-byte alignment of the private area */
+        alloc_size = sizeof (*dev) + sizeof_priv + 31;
+
+        dev = (struct net_device *) kmalloc (alloc_size, GFP_KERNEL);
+        if (dev == NULL)
+        {
+                printk(KERN_ERR "alloc_dev: Unable to allocate device memory.\n");
+                return NULL;
+        }
+
+        memset(dev, 0, alloc_size);
+
+        if (sizeof_priv)
+                dev->priv = (void *) (((long)(dev + 1) + 31) & ~31);
+
+        setup(dev);
+        strcpy(dev->name, mask);
+
+        return dev;
+}
+#endif
+
+#define MSEC_PER_SEC    1000L
+#define USEC_PER_MSEC   1000L
+#define NSEC_PER_USEC   1000L
+#define NSEC_PER_MSEC   1000000L
+#define USEC_PER_SEC    1000000L
+#define NSEC_PER_SEC    1000000000L
+#define FSEC_PER_SEC    1000000000000000L
+
+unsigned int jiffies_to_msecs(const unsigned long j)
+{
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+        return (MSEC_PER_SEC / HZ) * j;
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+        return (j + (HZ / MSEC_PER_SEC) - 1)/(HZ / MSEC_PER_SEC);
+#else
+        return (j * MSEC_PER_SEC) / HZ;
+#endif
+}
+
+unsigned long msecs_to_jiffies(const unsigned int m)
+{
+        if (m > jiffies_to_msecs(MAX_JIFFY_OFFSET))
+                return MAX_JIFFY_OFFSET;
+#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)
+        return (m + (MSEC_PER_SEC / HZ) - 1) / (MSEC_PER_SEC / HZ);
+#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)
+        return m * (HZ / MSEC_PER_SEC);
+#else
+        return (m * HZ + MSEC_PER_SEC - 1) / MSEC_PER_SEC;
+#endif
+}
+
+signed long schedule_timeout_interruptible(signed long timeout)
+{
+        __set_current_state(TASK_INTERRUPTIBLE);
+        return schedule_timeout(timeout);
+}
+
+signed long schedule_timeout_uninterruptible(signed long timeout)
+{
+        __set_current_state(TASK_UNINTERRUPTIBLE);
+        return schedule_timeout(timeout);
+}
+
+void msleep(unsigned int msecs)
+{
+        unsigned long timeout = msecs_to_jiffies(msecs) + 1;
+
+        while (timeout)
+                timeout = schedule_timeout_uninterruptible(timeout);
+}
+
+unsigned long msleep_interruptible(unsigned int msecs)
+{
+        unsigned long timeout = msecs_to_jiffies(msecs) + 1;
+
+        while (timeout && !signal_pending(current))
+                timeout = schedule_timeout_interruptible(timeout);
+        return jiffies_to_msecs(timeout);
+}
+
+
+int
+pci_set_consistent_dma_mask(struct pci_dev *dev, u64 mask)
+{
+        if (!pci_dma_supported(dev, mask))
+                return -EIO;
+
+#ifdef XXX
+        dev->dev.coherent_dma_mask = mask;
+#endif
+
+        return 0;
+}
diff --git a/drivers/net/cxgb3/linux_2_4_compat.h b/drivers/net/cxgb3/linux_2_4_compat.h
new file mode 100755
--- /dev/null
+++ b/drivers/net/cxgb3/linux_2_4_compat.h
@@ -0,0 +1,775 @@
+/*
+ * Copyright (c) 2003-2009 Chelsio, Inc. All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __LINUX_2_4_COMPAT_H__
+#define __LINUX_2_4_COMPAT_H__
+
+#include <linux/pci.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/timer.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/filter.h>
+#include <linux/cache.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <net/sock.h>
+#include <net/snmp.h>
+#include <net/ip.h>
+#include <asm/atomic.h>
+#include <asm/semaphore.h>
+#include <asm/bitops.h>
+#include <asm/io.h>
+
+/******************************************************************************
+ * directives
+ ******************************************************************************/
+#define	__read_mostly
+#define	__user
+#define	__iomem
+
+/******************************************************************************
+ * types
+ ******************************************************************************/
+typedef unsigned int gfp_t;
+
+#define	__be64	u64
+#define	__be32	u32
+#define	__be16	u16
+
+/******************************************************************************
+ * debug compat
+ ******************************************************************************/
+#define WARN_ON(condition) do { \
+        if (unlikely((condition)!=0)) { \
+                printk("BUG: warning at %s:%d/%s()\n", __FILE__, __LINE__, __FUNCTION__); \
+                dump_stack(); \
+        } \
+} while (0)
+
+#define dev_printk(level, dev, format, arg...)  \
+         printk(level  format , ## arg)
+
+#define dev_err(dev, format, arg...)            \
+        dev_printk(KERN_ERR , dev , format , ## arg)
+
+#define dev_info(dev, format, arg...)            \
+        dev_printk(KERN_INFO , dev , format , ## arg)
+
+#define dev_warn(dev, format, arg...)            \
+        dev_printk(KERN_WARNING , dev , format , ## arg)
+
+
+/******************************************************************************
+ * lock compatibility
+ ******************************************************************************/
+#define	DEFINE_MUTEX(l)			DECLARE_MUTEX((l))
+#define mutex_lock(l)			down((l))
+#define mutex_unlock(l)			up((l))
+#define mutex_init(l)			sema_init((l), 1)
+#define	DEFINE_RWLOCK(l)		rwlock_t (l) = RW_LOCK_UNLOCKED
+
+#ifndef spin_trylock_irq
+#define spin_trylock_irq(lock) \
+({ \
+	local_irq_disable(); \
+	spin_trylock(lock) ? \
+	1 : ({ local_irq_enable(); 0;  }); \
+})
+#endif
+
+#ifndef spin_trylock_irqsave
+#define _spin_trylock	spin_trylock
+#define spin_trylock_irqsave(lock, flags) \
+({ \
+	local_irq_save(flags); \
+	_spin_trylock(lock) ? \
+	1 : ({ local_irq_restore(flags); 0; }); \
+})
+#endif
+
+/******************************************************************************
+ * module compatibility
+ ******************************************************************************/
+#define	MODULE_VERSION(x)
+
+/*
+ * this macro only works for integer type parameters. If other types of
+ * module parameters are added then this will need to be updated.
+ */
+#define module_param(param, type, perm)		MODULE_PARM(param, "i");
+
+typedef void irqreturn_t;
+
+int atomic_add_return(int i, atomic_t *v);
+
+#define SEQ_START_TOKEN ((void *)1)
+/******************************************************************************
+ * TCP compatibility
+ ******************************************************************************/
+#define TCP_NAGLE_OFF           1
+#define TCP_NAGLE_CORK          2
+#define TCP_CONGESTION  	13      /* Congestion control algorithm */
+#define TCP_CA_NAME_MAX 	16
+
+/* TCP congestion stuff. Added for compilation only */
+#define TCP_CA_NAME_MAX 16
+struct tcp_congestion_ops {
+        void (*get_info)(struct sock *sk, u32 ext, struct sk_buff *skb);
+
+        char            name[TCP_CA_NAME_MAX];
+        struct module   *owner;
+};
+
+void tcp_v4_setup_caps(struct sock *sk, struct dst_entry *dst);
+
+static inline void t3_set_ca_ops(struct sock *sk,
+                                 struct tcp_congestion_ops *t_ops)
+{}
+
+/******************************************************************************
+ * socket compatibility
+ ******************************************************************************/
+enum sock_flags {
+        SOCK_DEAD,
+        SOCK_DONE,
+        SOCK_URGINLINE,
+        SOCK_KEEPOPEN,
+        SOCK_LINGER,
+        SOCK_DESTROY,
+        SOCK_BROADCAST,
+        SOCK_TIMESTAMP,
+        SOCK_ZAPPED,
+        SOCK_USE_WRITE_QUEUE, /* whether to call sk->sk_write_space in sock_wfree */
+        SOCK_DBG, /* %SO_DEBUG setting */
+        SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
+        SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
+        SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+};
+
+static inline void sock_set_flag_val(struct sock *sk, enum sock_flags flag, int val)
+{
+	switch (flag) {
+        	case SOCK_DEAD:
+			sk->dead = val;
+			break;
+        	case SOCK_DONE:
+			sk->done = val;
+			break;
+        	case SOCK_URGINLINE:
+			sk->urginline = val;
+			break;
+        	case SOCK_KEEPOPEN:
+			sk->keepopen = val;
+			break;
+        	case SOCK_LINGER:
+			sk->linger = val;
+			break;
+        	case SOCK_DESTROY:
+			sk->destroy = val;
+			break;
+        	case SOCK_BROADCAST:
+			sk->broadcast = val;
+			break;
+        	case SOCK_USE_WRITE_QUEUE:
+			sk->use_write_queue = val;
+			break;
+        	case SOCK_DBG:
+			sk->debug = val;
+			break;
+        	case SOCK_RCVTSTAMP:
+			sk->rcvtstamp = val;
+			break;
+        	case SOCK_ZAPPED:
+			sk->zapped = val;
+			break;
+        	case SOCK_LOCALROUTE:
+        	case SOCK_TIMESTAMP:
+        	case SOCK_QUEUE_SHRUNK:
+		default:
+			/* XXX */
+			printk("sock_set_flag_val: unknown flag\n");
+			break;
+	}
+}
+
+static inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)
+{
+	sock_set_flag_val(sk, flag, 0);
+}
+
+static inline int sock_flag(struct sock *sk, enum sock_flags flag)
+{
+	int val = 0;
+
+	switch (flag) {
+        	case SOCK_DEAD:
+			val = sk->dead;
+			break;
+        	case SOCK_DONE:
+			val = sk->done;
+			break;
+        	case SOCK_URGINLINE:
+			val = sk->urginline;
+			break;
+        	case SOCK_KEEPOPEN:
+			val = sk->keepopen;
+			break;
+        	case SOCK_LINGER:
+			val = sk->linger;
+			break;
+        	case SOCK_DESTROY:
+			val = sk->destroy;
+			break;
+        	case SOCK_BROADCAST:
+			val = sk->broadcast;
+			break;
+        	case SOCK_USE_WRITE_QUEUE:
+			val = sk->use_write_queue;
+			break;
+        	case SOCK_DBG:
+			val = sk->debug;
+			break;
+        	case SOCK_RCVTSTAMP:
+			val = sk->rcvtstamp;
+			break;
+        	case SOCK_ZAPPED:
+			val = sk->zapped;
+			break;
+        	case SOCK_LOCALROUTE:
+        	case SOCK_TIMESTAMP:
+        	case SOCK_QUEUE_SHRUNK:
+		default:
+			/* XXX */
+			printk("sock_set_flag_val: unknown flag\n");
+			break;
+	}
+        return val;
+}
+
+static inline void sock_set_flag(struct sock *sk, enum sock_flags flag)
+{
+	sock_set_flag_val(sk, flag, 1);
+}
+
+#define sock_owned_by_user(sk)  ((sk)->lock.users != 0)
+
+/*
+ * map 2.6 field names to 2.4 names
+ */
+#define sk_state		state
+#define sk_sleep		sleep
+#define sk_write_queue		write_queue
+#define sk_userlocks		userlocks
+#define sk_sndbuf		sndbuf
+#define sk_prot			prot
+#define sk_backlog_rcv		backlog_rcv
+#define sk_write_space		write_space
+#define sk_timer		timer
+#define sk_dst_cache		dst_cache
+#define sk_data_ready		data_ready
+#define sk_user_data		user_data
+#define sk_state_change		state_change
+#define sk_err			err
+#define sk_wmem_queued		wmem_queued
+#define sk_error_report		error_report
+#define sk_shutdown		shutdown
+#define sk_receive_queue	receive_queue
+#define sk_data_ready		data_ready
+#define sk_bound_dev_if		bound_dev_if
+
+/******************************************************************************
+ * routing compatibility
+ ******************************************************************************/
+#define ROUTE_REQ
+
+static inline struct rtattr *
+__rta_reserve(struct sk_buff *skb, int attrtype, int attrlen)
+{
+        struct rtattr *rta;
+        int size = RTA_LENGTH(attrlen);
+
+        rta = (struct rtattr*)skb_put(skb, RTA_ALIGN(size));
+        rta->rta_type = attrtype;
+        rta->rta_len = size;
+        memset(RTA_DATA(rta) + attrlen, 0, RTA_ALIGN(size) - size);
+        return rta;
+}
+
+#define __RTA_PUT(skb, attrtype, attrlen) \
+({      if (unlikely(skb_tailroom(skb) < (int)RTA_SPACE(attrlen))) \
+                goto rtattr_failure; \
+        __rta_reserve(skb, attrtype, attrlen); })
+
+#define INET_DIAG_MAX INET_DIAG_CONG
+
+#ifdef CONFIG_TCP_OFFLOAD_MODULE
+extern atomic_t tcp_orphan_count_offload;
+extern int ip_route_output_flow_offload(struct rtable **rp,
+                                        struct flowi *flp,
+                                        struct sock *sk, int flags);
+#define ip_route_output_flow ip_route_output_flow_offload
+#define INC_ORPHAN_COUNT(sk) (atomic_inc(&tcp_orphan_count_offload))
+#else
+#define INC_ORPHAN_COUNT(sk) (atomic_inc(&tcp_orphan_count))
+#endif /* CONFIG_TCP_OFFLOAD_MODULE */
+
+#define dst_mtu(dst) dst_metric(dst, RTAX_MTU)
+
+/******************************************************************************
+ * net data structure compatibility
+ ******************************************************************************/
+#define tcp_sock tcp_opt
+#define inet_sock inet_opt
+#define request_sock open_request
+
+#define inet_csk(sk) tcp_sk(sk)
+#define inet_csk_destroy_sock(sk) tcp_destroy_sock(sk)
+#define inet_csk_route_req(lsk, oreq) tcp_v4_route_req(lsk, oreq)
+
+#define inet_connection_sock tcp_opt
+#define icsk_af_ops af_specific
+#define icsk_ack ack
+#define icsk_pmtu_cookie pmtu_cookie
+#define inet_csk_reqsk_queue_removed tcp_synq_removed
+#define inet_csk_delete_keepalive_timer tcp_delete_keepalive_timer
+#define inet_csk_reqsk_queue_is_full tcp_synq_is_full
+#define inet_csk_reqsk_queue_add tcp_acceptq_queue
+#define inet_csk_reqsk_queue_added(sk, timeo) tcp_synq_added(sk)
+
+#define __reqsk_free tcp_openreq_fastfree
+#define tcp_rsk
+#define inet_rsk
+
+#define inet_inherit_port(p_hashinfo, lsk, newsk) tcp_inherit_port(lsk, newsk)
+#define inet_put_port(a, sk) tcp_put_port(sk)
+
+#define ACCEPT_QUEUE(sk) (&(tcp_sk(sk)->accept_queue))
+#define ACCEPT_QUEUE_TAIL(sk) (&(tcp_sk(sk)->accept_queue_tail))
+#define LISTEN_OPT(sk) (&(tcp_sk(sk)->listen_opt))
+
+#define PACKETS_OUT(tcp_sock) ((tcp_sock)->packets_out)
+#define LEFT_OUT(tcp_sock) ((tcp_sock)->left_out)
+#define RETRANS_OUT(tcp_sock) ((tcp_sock)->retrans_out)
+
+#define MSS_CLAMP(tp) ((tp)->mss_clamp)
+#define SND_WSCALE(tp) ((tp)->snd_wscale)
+#define RCV_WSCALE(tp) ((tp)->rcv_wscale)
+#define USER_MSS(tp) ((tp)->user_mss)
+#define NUM_SACKS(tp) ((tp)->num_sacks)
+#define TS_RECENT_STAMP(tp) ((tp)->ts_recent_stamp)
+#define WSCALE_OK(tp) ((tp)->wscale_ok)
+#define TSTAMP_OK(tp) ((tp)->tstamp_ok)
+#define SACK_OK(tp) ((tp)->sack_ok)
+
+#define forward_skb_hint ucopy.prequeue.next
+#define fastpath_skb_hint ucopy.prequeue.prev
+
+
+/******************************************************************************
+ * netdev compatibility
+ ******************************************************************************/
+static inline int __netif_rx_schedule_prep(struct net_device *dev)
+{
+	return !test_and_set_bit(__LINK_STATE_RX_SCHED, &dev->state);
+}
+
+#define NETDEV_TX_OK 0          /* driver took care of packet */
+#define NETDEV_TX_BUSY 1        /* driver tx path was busy*/
+#define NETDEV_TX_LOCKED -1     /* driver tx lock was already taken */
+
+#ifndef SET_NETDEV_DEV
+#define	SET_NETDEV_DEV(netdev, pdev)
+#endif
+
+#ifndef NETIF_F_LLTX
+#define NETIF_F_LLTX	0
+#endif
+#ifndef NETIF_F_TSO
+#define NETIF_F_TSO	0
+#define NETIF_F_TSO_FAKE
+#endif
+
+#define NETDEV_ALIGN            32
+#define NETDEV_ALIGN_CONST      (NETDEV_ALIGN - 1)
+
+static inline void *netdev_priv(struct net_device *dev)
+{
+        return (char *)dev + ((sizeof(struct net_device)
+                                        + NETDEV_ALIGN_CONST)
+                                & ~NETDEV_ALIGN_CONST);
+}
+
+#ifndef ALLOC_NETDEV
+struct net_device *alloc_netdev(int sizeof_priv, const char *mask,
+                                       void (*setup)(struct net_device *));
+#endif
+
+/******************************************************************************
+ * stat compatibility
+ ******************************************************************************/
+
+#define LINUX_MIB_TCPABORTONDATA		1
+#define LINUX_MIB_TCPABORTONLINGER		2
+#define LINUX_MIB_TCPABORTONSYN			3
+#define LINUX_MIB_TCPABORTONTIMEOUT		4
+#define LINUX_MIB_LISTENOVERFLOWS		5
+#define LINUX_MIB_LISTENDROPS			6
+#define LINUX_MIB_TCPABORTONCLOSE		7
+#define LINUX_MIB_TCPABORTONMEMORY		8
+
+#define	IPSTATS_MIB_OUTNOROUTES 		IpOutNoRoutes
+
+#define	TCP_MIB_ATTEMPTFAILS			TcpAttemptFails
+#define	TCP_MIB_ACTIVEOPENS			TcpActiveOpens
+
+
+static inline void net_inc_stats(int stat)
+{
+	switch(stat) {
+		case LINUX_MIB_TCPABORTONDATA:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnData);
+			break;
+		case LINUX_MIB_TCPABORTONLINGER:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnLinger);
+			break;
+		case LINUX_MIB_TCPABORTONSYN:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnSyn);
+			break;
+		case LINUX_MIB_TCPABORTONTIMEOUT:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnTimeout);
+			break;
+		case LINUX_MIB_LISTENOVERFLOWS:
+			SNMP_INC_STATS(net_statistics, ListenOverflows);
+			break;
+		case LINUX_MIB_LISTENDROPS:
+			SNMP_INC_STATS(net_statistics, ListenDrops);
+			break;
+		case LINUX_MIB_TCPABORTONCLOSE:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnClose);
+			break;
+		case LINUX_MIB_TCPABORTONMEMORY:
+			SNMP_INC_STATS(net_statistics, TCPAbortOnMemory);
+			break;
+	}
+}
+
+#if XXX
+#undef	NET_INC_STATS_USER
+#undef	NET_INC_STATS_BH
+
+#define	NET_INC_STATS_BH(stat)			net_inc_stats((stat))
+#define	NET_INC_STATS_USER(stat)		net_inc_stats((stat))
+#endif
+
+struct request_sock_ops {
+        int             family;
+        int             obj_size;
+};
+
+/* Inet diag stuff. Added for compilation only */
+enum {
+        INET_DIAG_NONE,
+        INET_DIAG_MEMINFO,
+        INET_DIAG_INFO,
+        INET_DIAG_VEGASINFO,
+        INET_DIAG_CONG,
+};
+
+static inline void t3_init_rsk_ops(struct proto *t3_tcp_prot,
+                                   struct request_sock_ops *t3_tcp_ops,
+                                   struct proto *tcp_prot)
+{}
+
+#ifdef XXX
+static inline struct open_request *reqsk_alloc(struct request_sock_ops *rsk)
+{
+        struct open_request *oreq = tcp_openreq_alloc();
+
+        if (oreq)
+                oreq->class = (struct or_calltable *)rsk;
+
+        return oreq;
+}
+
+static inline void t3_set_req_addr(struct open_request *oreq,
+                                   __u32 local_ip, __u32 peer_ip)
+{
+        oreq->af.v4_req.loc_addr = local_ip;
+        oreq->af.v4_req.rmt_addr = peer_ip;
+}
+
+static inline void t3_set_req_opt(struct open_request *oreq,
+                                  struct ip_options *ip_opt)
+{
+}
+#endif
+
+static inline void sk_setup_caps(struct sock *sk, struct dst_entry *dst)
+{
+        __sk_dst_set(sk, dst);
+        tcp_v4_setup_caps(sk, dst);
+}
+
+static inline void setup_timer(struct timer_list * timer,
+                                void (*function)(unsigned long),
+                                unsigned long data)
+{
+        timer->function = function;
+        timer->data = data;
+        init_timer(timer);
+}
+
+static inline int synq_empty(struct sock *sk)
+{
+	return skb_queue_empty(&tcp_sk(sk)->ucopy.prequeue);
+}
+
+static inline void reset_synq(struct tcp_sock *tp)
+{
+	skb_queue_head_init(&tp->ucopy.prequeue);
+}
+
+static inline void reset_wr_list(struct tcp_sock *tp)
+{
+	skb_queue_head_init(&tp->ucopy.prequeue);
+}
+
+static inline int wr_list_empty(struct tcp_sock *tp)
+{
+	return (skb_queue_empty(&tp->ucopy.prequeue));
+}
+
+/*
+ * Add a WR to a socket's list of pending WRs.  This is a singly-linked list
+ */
+static inline void enqueue_wr(struct tcp_sock *tp, struct sk_buff *skb)
+{
+        struct sk_buff *tail = (struct sk_buff *)tp->ucopy.iov;
+
+        skb->dev = NULL;
+        /*
+         *  We want to take an extra reference since both us and the driver
+         *  need to free the packet before it's really freed.  We know there's
+         * just one user currently so we use atomic_set rather than skb_get
+         * to avoid the atomic op.
+         */
+        atomic_set(&skb->users, 2);
+
+        if (wr_list_empty(tp))
+                tp->ucopy.prequeue.next = skb;
+        else
+                tail->dev = (void *)skb;
+        tp->ucopy.iov = (void *)skb;
+
+}
+
+
+/*
+ * Return the first pending WR without removing it from the list.
+ */
+static inline struct sk_buff *peek_wr(struct tcp_sock *tp)
+{
+        if (unlikely(wr_list_empty(tp)))
+                return NULL;
+        return (tp->ucopy.prequeue.next);
+}
+
+/*
+ * Dequeue and return the first unacknowledged's WR on a socket's pending list.
+ */
+static inline struct sk_buff *dequeue_wr(struct tcp_sock *tp)
+{
+        struct sk_buff *skb = tp->ucopy.prequeue.next;
+
+        if (unlikely(wr_list_empty(tp)))
+                        return NULL;
+
+        if (!skb->dev)
+                tp->ucopy.prequeue.next = tp->ucopy.prequeue.prev;
+        else
+                tp->ucopy.prequeue.next = (void *)skb->dev;
+
+        skb->dev = NULL;
+        return skb;
+}
+
+extern int prepare_tom_for_offload(void);
+
+/* these are defined to nothing since 2.4 interrupt handlers are voids */
+#define	IRQ_NONE
+#define	IRQ_HANDLED
+#define	IRQ_RETVAL(x)
+
+/******************************************************************************
+ * DMA compatibility
+ ******************************************************************************/
+#define DMA_64BIT_MASK  0xffffffffffffffffULL
+#define DMA_48BIT_MASK  0x0000ffffffffffffULL
+#define DMA_40BIT_MASK  0x000000ffffffffffULL
+#define DMA_39BIT_MASK  0x0000007fffffffffULL
+#define DMA_32BIT_MASK  0x00000000ffffffffULL
+#define DMA_31BIT_MASK  0x000000007fffffffULL
+#define DMA_30BIT_MASK  0x000000003fffffffULL
+#define DMA_29BIT_MASK  0x000000001fffffffULL
+#define DMA_28BIT_MASK  0x000000000fffffffULL
+#define DMA_24BIT_MASK  0x0000000000ffffffULL
+
+/******************************************************************************
+ * PHY compatibility
+ ******************************************************************************/
+#define BMCR_SPEED1000          0x0040  /* MSB of Speed (1000)         */
+
+#define MII_CTRL1000        0x09        /* 1000BASE-T control          */
+
+#define ADVERTISE_1000XFULL     0x0020  /* Try for 1000BASE-X full-duplex */
+#define ADVERTISE_1000XHALF     0x0040  /* Try for 1000BASE-X half-duplex */
+#define ADVERTISE_1000XPAUSE	0x0080	/* Try for 1000BASE-X pause    */
+#define ADVERTISE_1000XPSE_ASYM	0x0100	/* Try for 1000BASE-X asym pause */
+#define ADVERTISE_PAUSE_CAP     0x0400  /* Try for pause               */
+#define ADVERTISE_PAUSE_ASYM    0x0800  /* Try for asymetric pause     */
+
+/* 1000BASE-T Control register */
+#define ADVERTISE_1000FULL      0x0200  /* Advertise 1000BASE-T full duplex */
+#define ADVERTISE_1000HALF      0x0100  /* Advertise 1000BASE-T half duplex */
+
+#define ADVERTISED_Pause                (1 << 13)
+#define ADVERTISED_Asym_Pause           (1 << 14)
+
+/******************************************************************************
+ * PCI compatibility
+ ******************************************************************************/
+int pci_set_consistent_dma_mask(struct pci_dev *dev, u64 mask);
+static inline int pci_dma_mapping_error(dma_addr_t addr)
+{
+	return (addr == 0);
+}
+
+#define PCI_VPD_ADDR            2       /* Address to access (15 bits!) */
+#define  PCI_VPD_ADDR_MASK      0x7fff  /* Address mask */
+#define  PCI_VPD_ADDR_F         0x8000  /* Write 0, 1 indicates completion */
+#define PCI_VPD_DATA            4       /* 32-bits of data returned here */
+
+/* PCI Express capability registers */
+#define PCI_EXP_FLAGS           2       /* Capabilities register */
+#define PCI_EXP_FLAGS_VERS      0x000f  /* Capability version */
+#define PCI_EXP_FLAGS_TYPE      0x00f0  /* Device/Port type */
+#define  PCI_EXP_TYPE_ENDPOINT  0x0     /* Express Endpoint */
+#define  PCI_EXP_TYPE_LEG_END   0x1     /* Legacy Endpoint */
+#define  PCI_EXP_TYPE_ROOT_PORT 0x4     /* Root Port */
+#define  PCI_EXP_TYPE_UPSTREAM  0x5     /* Upstream Port */
+#define  PCI_EXP_TYPE_DOWNSTREAM 0x6    /* Downstream Port */
+#define  PCI_EXP_TYPE_PCI_BRIDGE 0x7    /* PCI/PCI-X Bridge */
+#define PCI_EXP_FLAGS_SLOT      0x0100  /* Slot implemented */
+#define PCI_EXP_FLAGS_IRQ       0x3e00  /* Interrupt message number */
+#define PCI_EXP_DEVCAP          4       /* Device capabilities */
+#define  PCI_EXP_DEVCAP_PAYLOAD 0x07    /* Max_Payload_Size */
+#define  PCI_EXP_DEVCAP_PHANTOM 0x18    /* Phantom functions */
+#define  PCI_EXP_DEVCAP_EXT_TAG 0x20    /* Extended tags */
+#define  PCI_EXP_DEVCAP_L0S     0x1c0   /* L0s Acceptable Latency */
+#define  PCI_EXP_DEVCAP_L1      0xe00   /* L1 Acceptable Latency */
+#define  PCI_EXP_DEVCAP_ATN_BUT 0x1000  /* Attention Button Present */
+#define  PCI_EXP_DEVCAP_ATN_IND 0x2000  /* Attention Indicator Present */
+#define  PCI_EXP_DEVCAP_PWR_IND 0x4000  /* Power Indicator Present */
+#define  PCI_EXP_DEVCAP_PWR_VAL 0x3fc0000 /* Slot Power Limit Value */
+#define  PCI_EXP_DEVCAP_PWR_SCL 0xc000000 /* Slot Power Limit Scale */
+#define PCI_EXP_DEVCTL          8       /* Device Control */
+#define  PCI_EXP_DEVCTL_CERE    0x0001  /* Correctable Error Reporting En. */
+#define  PCI_EXP_DEVCTL_NFERE   0x0002  /* Non-Fatal Error Reporting Enable */
+#define  PCI_EXP_DEVCTL_FERE    0x0004  /* Fatal Error Reporting Enable */
+#define  PCI_EXP_DEVCTL_URRE    0x0008  /* Unsupported Request Reporting En. */
+#define  PCI_EXP_DEVCTL_RELAX_EN 0x0010 /* Enable relaxed ordering */
+#define  PCI_EXP_DEVCTL_PAYLOAD 0x00e0  /* Max_Payload_Size */
+#define  PCI_EXP_DEVCTL_EXT_TAG 0x0100  /* Extended Tag Field Enable */
+#define  PCI_EXP_DEVCTL_PHANTOM 0x0200  /* Phantom Functions Enable */
+#define  PCI_EXP_DEVCTL_AUX_PME 0x0400  /* Auxiliary Power PM Enable */
+#define  PCI_EXP_DEVCTL_NOSNOOP_EN 0x0800  /* Enable No Snoop */
+#define  PCI_EXP_DEVCTL_READRQ  0x7000  /* Max_Read_Request_Size */
+#define PCI_EXP_DEVSTA          10      /* Device Status */
+#define  PCI_EXP_DEVSTA_CED     0x01    /* Correctable Error Detected */
+#define  PCI_EXP_DEVSTA_NFED    0x02    /* Non-Fatal Error Detected */
+#define  PCI_EXP_DEVSTA_FED     0x04    /* Fatal Error Detected */
+#define  PCI_EXP_DEVSTA_URD     0x08    /* Unsupported Request Detected */
+#define  PCI_EXP_DEVSTA_AUXPD   0x10    /* AUX Power Detected */
+#define  PCI_EXP_DEVSTA_TRPND   0x20    /* Transactions Pending */
+#define PCI_EXP_LNKCAP          12      /* Link Capabilities */
+#define PCI_EXP_LNKCTL          16      /* Link Control */
+#define PCI_EXP_LNKSTA          18      /* Link Status */
+#define PCI_EXP_SLTCAP          20      /* Slot Capabilities */
+#define PCI_EXP_SLTCTL          24      /* Slot Control */
+#define PCI_EXP_SLTSTA          26      /* Slot Status */
+#define PCI_EXP_RTCTL           28      /* Root Control */
+#define  PCI_EXP_RTCTL_SECEE    0x01    /* System Error on Correctable Error */
+#define  PCI_EXP_RTCTL_SENFEE   0x02    /* System Error on Non-Fatal Error */
+#define  PCI_EXP_RTCTL_SEFEE    0x04    /* System Error on Fatal Error */
+#define  PCI_EXP_RTCTL_PMEIE    0x08    /* PME Interrupt Enable */
+#define  PCI_EXP_RTCTL_CRSSVE   0x10    /* CRS Software Visibility Enable */
+#define PCI_EXP_RTCAP           30      /* Root Capabilities */
+#define PCI_EXP_RTSTA           32      /* Root Status */
+
+#define  PCI_CAP_ID_EXP         0x10    /* PCI Express */
+
+struct msix_entry {
+        u16     vector; /* kernel uses to write allocated vector */
+        u16     entry;  /* driver uses to specify entry, OS writes */
+};
+
+int pci_enable_msi(struct pci_dev* dev);
+int pci_disable_msi(struct pci_dev* dev);
+int pci_enable_msix(struct pci_dev* dev, struct msix_entry *entries, int nvec);
+int pci_disable_msix(struct pci_dev* dev);
+
+inline int t3_os_pci_save_state(adapter_t *adapter);
+inline int t3_os_pci_restore_state(adapter_t *adapter);
+
+#define	pci_dma_sync_single_for_cpu	pci_dma_sync_single
+#define	pci_dma_sync_single_for_device	pci_dma_sync_single
+
+
+/******************************************************************************
+ * memory management compatibility
+ ******************************************************************************/
+#define	__GFP_NOFAIL	0
+#define __GFP_COMP	0
+
+void *kzalloc(size_t size, gfp_t flags);
+void *kcalloc(size_t n, size_t size, gfp_t flags);
+
+/******************************************************************************
+ * timer compatibility
+ ******************************************************************************/
+unsigned int jiffies_to_msecs(const unsigned long j);
+unsigned long msecs_to_jiffies(const unsigned int m);
+signed long schedule_timeout_interruptible(signed long timeout);
+signed long schedule_timeout_uninterruptible(signed long timeout);
+void msleep(unsigned int msecs);
+unsigned long msleep_interruptible(unsigned int msecs);
+
+/******************************************************************************
+ * SMP compatibility
+ ******************************************************************************/
+static inline int num_online_cpus(void)
+{
+	return smp_num_cpus;
+}
+
+int generic_fls(int x);
+
+#ifndef IF_MII
+/******************************************************************************
+ * MII compatibility
+ ******************************************************************************/
+static inline struct mii_ioctl_data *if_mii(struct ifreq *rq)
+{
+	return (struct mii_ioctl_data *) &rq->ifr_ifru;
+} 
+#endif
+
+#endif 	/* __LINUX_2_4_COMPAT_H__ */
diff --git a/drivers/net/cxgb3/linux_2_4_compat_workqueue.c b/drivers/net/cxgb3/linux_2_4_compat_workqueue.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/linux_2_4_compat_workqueue.c
@@ -0,0 +1,290 @@
+/*
+ * Copyright (c) 2003-2009 Chelsio, Inc. All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include <linux/kernel.h>
+#include <linux/workqueue.h>
+
+#include "osdep.h"
+#include "linux_2_4_compat_workqueue.h"
+
+#include <linux/mm.h>
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/completion.h>
+#include <asm/semaphore.h>
+
+extern struct workqueue_struct *cxgb3_wq;
+
+struct workqueue_struct {
+        spinlock_t lock;
+
+        long remove_sequence;   /* Least-recently added (next to run) */
+        long insert_sequence;   /* Next to add */
+
+        struct list_head worklist;
+        wait_queue_head_t more_work;
+        wait_queue_head_t work_done;
+
+        struct workqueue_struct *wq;
+        struct task_struct *thread;
+	struct completion th_exit;
+
+        int run_depth;          /* Detect run_workqueue() recursion depth */
+        const char *name;
+} ____cacheline_aligned;
+
+static struct semaphore workqueue_mutex = __MUTEX_INITIALIZER(workqueue_mutex);
+
+static void run_workqueue(struct workqueue_struct *cwq)
+{
+        unsigned long flags;
+
+        /*
+         * Keep taking off work from the queue until
+         * done.
+         */
+        spin_lock_irqsave(&cwq->lock, flags);
+        cwq->run_depth++;
+        if (cwq->run_depth > 3) {
+                /* morton gets to eat his hat */
+                printk("%s: recursion depth exceeded: %d\n",
+                        __FUNCTION__, cwq->run_depth);
+                dump_stack();
+        }
+
+        while (!list_empty(&cwq->worklist)) {
+                struct work_struct *work = list_entry(cwq->worklist.next,
+                                                struct work_struct, entry);
+                void (*f) (void *) = work->func;
+                void *data = work->data;
+
+                list_del_init(cwq->worklist.next);
+                spin_unlock_irqrestore(&cwq->lock, flags);
+
+                BUG_ON(work->wq_data != cwq);
+                clear_bit(0, &work->pending);
+                f(data);
+
+                spin_lock_irqsave(&cwq->lock, flags);
+                cwq->remove_sequence++;
+                wake_up(&cwq->work_done);
+        }
+        cwq->run_depth--;
+        spin_unlock_irqrestore(&cwq->lock, flags);
+}
+
+static int worker_thread(void *__cwq)
+{
+        struct workqueue_struct *cwq = __cwq;
+        DECLARE_WAITQUEUE(wait, current);
+
+	cwq->thread = current;
+
+        daemonize();
+        reparent_to_init();
+	sprintf(current->comm, cwq->name);
+
+	sigdelset(&current->blocked, SIGTERM);
+        flush_signals(current);
+
+        set_current_state(TASK_INTERRUPTIBLE);
+	init_waitqueue_entry(&wait, current);
+	add_wait_queue(&cwq->more_work, &wait);
+        while (1) {
+		if (signal_pending(current))
+			break;
+
+                if (list_empty(&cwq->worklist))
+			schedule();
+		else
+                        __set_current_state(TASK_RUNNING);
+
+
+                if (!list_empty(&cwq->worklist))
+                        run_workqueue(cwq);
+                set_current_state(TASK_INTERRUPTIBLE);
+        }
+
+        __set_current_state(TASK_RUNNING);
+	remove_wait_queue(&cwq->more_work, &wait);
+
+        complete_and_exit(&cwq->th_exit, 0);
+	return 0;
+}
+
+static int create_workqueue_thread(struct workqueue_struct *cwq)
+{
+        spin_lock_init(&cwq->lock);
+        cwq->thread = NULL;
+        cwq->insert_sequence = 0;
+        cwq->remove_sequence = 0;
+        INIT_LIST_HEAD(&cwq->worklist);
+        init_waitqueue_head(&cwq->more_work);
+        init_waitqueue_head(&cwq->work_done);
+	init_completion (&cwq->th_exit);
+	if (kernel_thread(worker_thread, (void *) (long) cwq,
+			  CLONE_FS | CLONE_FILES) < 0)
+                return 0;
+
+        return 1;
+}
+
+struct workqueue_struct * create_singlethread_workqueue(const char *name)
+{
+        struct workqueue_struct *wq;
+
+        wq = kzalloc(sizeof(*wq), GFP_KERNEL);
+        if (!wq)
+                return NULL;
+
+        wq->name = name;
+        mutex_lock(&workqueue_mutex);
+
+	if (create_workqueue_thread(wq) == 0) {
+		kfree(wq);
+		wq = NULL;
+	}
+
+        mutex_unlock(&workqueue_mutex);
+
+        return wq;
+}
+
+static void cleanup_workqueue_thread(struct workqueue_struct *cwq)
+{
+        unsigned long flags;
+        struct task_struct *p;
+
+        spin_lock_irqsave(&cwq->lock, flags);
+        p = cwq->thread;
+        cwq->thread = NULL;
+        spin_unlock_irqrestore(&cwq->lock, flags);
+        if (p)
+                send_sig(SIGTERM, p, 0);
+
+	wait_for_completion(&cwq->th_exit);
+}
+
+void destroy_workqueue(struct workqueue_struct *wq)
+{
+        flush_workqueue(wq);
+
+        mutex_lock(&workqueue_mutex);
+	cleanup_workqueue_thread(wq);
+        mutex_unlock(&workqueue_mutex);
+        kfree(wq);
+}
+
+static void flush_cpu_workqueue(struct workqueue_struct *cwq)
+{
+        if (cwq->thread == current) {
+                run_workqueue(cwq);
+        } else {
+		wait_queue_t __wait;
+                long sequence_needed;
+		unsigned long flags;
+
+		init_waitqueue_entry(&__wait, current);
+
+                spin_lock_irqsave(&cwq->lock, flags);
+                sequence_needed = cwq->insert_sequence;
+
+		init_waitqueue_entry(&__wait, current);
+		add_wait_queue(&cwq->work_done, &__wait);
+                while (sequence_needed - cwq->remove_sequence > 0) {
+                        spin_unlock_irqrestore(&cwq->lock, flags);
+			schedule();
+                        spin_lock_irqsave(&cwq->lock, flags);
+                }
+		remove_wait_queue(&cwq->work_done, &__wait);
+                spin_unlock_irqrestore(&cwq->lock, flags);
+        }
+}
+
+void flush_workqueue(struct workqueue_struct *wq)
+{
+        cond_resched();
+	flush_cpu_workqueue(wq);
+}
+
+static void __queue_work(struct workqueue_struct *cwq, struct work_struct *work)
+{
+        unsigned long flags;
+
+        spin_lock_irqsave(&cwq->lock, flags);
+        work->wq_data = cwq;
+        list_add_tail(&work->entry, &cwq->worklist);
+        cwq->insert_sequence++;
+        wake_up(&cwq->more_work);
+        spin_unlock_irqrestore(&cwq->lock, flags);
+}
+
+int queue_work(struct workqueue_struct *cwq, struct work_struct *work)
+{
+        int ret = 0;
+
+        if (!test_and_set_bit(0, &work->pending)) {
+                BUG_ON(!list_empty(&work->entry));
+                __queue_work(cwq, work);
+                ret = 1;
+        }
+        return ret;
+}
+
+int schedule_work(struct work_struct *work)
+{
+        return queue_work(cxgb3_wq, work);
+}
+
+static void delayed_work_timer_fn(unsigned long __data)
+{
+        struct work_struct *work = (struct work_struct *)__data;
+        struct workqueue_struct *wq = work->wq_data;
+
+        __queue_work(wq, work);
+}
+
+int queue_delayed_work(struct workqueue_struct *wq,
+                        struct work_struct *work, unsigned long delay)
+{
+        int ret = 0;
+        struct timer_list *timer = &work->timer;
+
+        if (!test_and_set_bit(0, &work->pending)) {
+                BUG_ON(timer_pending(timer));
+                BUG_ON(!list_empty(&work->entry));
+
+                /* This stores wq for the moment, for the timer_fn */
+                work->wq_data = wq;
+                timer->expires = jiffies + delay;
+                timer->data = (unsigned long)work;
+                timer->function = delayed_work_timer_fn;
+                add_timer(timer);
+                ret = 1;
+        }
+        return ret;
+}
+
+static int cancel_delayed_work(struct work_struct *work)
+{
+        int ret;
+
+        ret = del_timer_sync(&work->timer);
+        if (ret)
+                clear_bit(0, &work->pending);
+        return ret;
+}
+
+void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
+                                       struct work_struct *work)
+{
+        while (!cancel_delayed_work(work))
+                flush_workqueue(wq);
+}
diff --git a/drivers/net/cxgb3/linux_2_4_compat_workqueue.h b/drivers/net/cxgb3/linux_2_4_compat_workqueue.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/linux_2_4_compat_workqueue.h
@@ -0,0 +1,52 @@
+/*
+ * Copyright (c) 2003-2009 Chelsio, Inc. All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __LINUX_2_4_COMPAT_WORKQUEUE_H__
+#define __LINUX_2_4_COMPAT_WORKQUEUE_H__
+
+#include <linux/workqueue.h>
+
+#include <stddef.h>
+#include <linux/list.h>
+#include <linux/timer.h>
+
+/******************************************************************************
+ * work queue compatibility
+ ******************************************************************************/
+
+#ifndef PREPARE_WORK
+#define PREPARE_WORK(_work, _func, _data)                       \
+        do {                                                    \
+                (_work)->func = _func;                          \
+                (_work)->data = _data;                          \
+        } while (0)
+#endif
+
+#ifndef INIT_WORK
+#define INIT_WORK(_work, _func, _data)                          \
+        do {                                                    \
+                INIT_LIST_HEAD(&(_work)->entry);                \
+                (_work)->pending = 0;                           \
+                PREPARE_WORK((_work), (_func), (_data));        \
+                init_timer(&(_work)->timer);                    \
+        } while (0)
+#endif
+
+struct workqueue_struct * create_singlethread_workqueue(const char *name);
+void destroy_workqueue(struct workqueue_struct *wq);
+int queue_work(struct workqueue_struct *cwq, struct work_struct *work);
+int queue_delayed_work(struct workqueue_struct *wq,
+                        struct work_struct *work, unsigned long delay);
+void cancel_rearming_delayed_workqueue(struct workqueue_struct *wq,
+                                       struct work_struct *work);
+void flush_workqueue(struct workqueue_struct *wq);
+
+int schedule_work(struct work_struct *work);
+
+#endif /* __LINUX_2_4_COMPAT_WORKQUEUE_H__ */
diff --git a/drivers/net/cxgb3/mc5.c b/drivers/net/cxgb3/mc5.c
--- a/drivers/net/cxgb3/mc5.c
+++ b/drivers/net/cxgb3/mc5.c
@@ -1,34 +1,14 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include "common.h"
 #include "regs.h"
 
@@ -90,31 +70,28 @@
  * Issue a command to the TCAM and wait for its completion.  The address and
  * any data required by the command must have been setup by the caller.
  */
-static int mc5_cmd_write(struct adapter *adapter, u32 cmd)
+static int mc5_cmd_write(adapter_t *adapter, u32 cmd)
 {
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_CMD, cmd);
 	return t3_wait_op_done(adapter, A_MC5_DB_DBGI_RSP_STATUS,
 			       F_DBGIRSPVALID, 1, MAX_WRITE_ATTEMPTS, 1);
 }
 
-static inline void dbgi_wr_addr3(struct adapter *adapter, u32 v1, u32 v2,
-				 u32 v3)
+static inline void dbgi_wr_addr3(adapter_t *adapter, u32 v1, u32 v2, u32 v3)
 {
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_ADDR0, v1);
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_ADDR1, v2);
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_ADDR2, v3);
 }
 
-static inline void dbgi_wr_data3(struct adapter *adapter, u32 v1, u32 v2,
-				 u32 v3)
+static inline void dbgi_wr_data3(adapter_t *adapter, u32 v1, u32 v2, u32 v3)
 {
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_DATA0, v1);
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_DATA1, v2);
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_DATA2, v3);
 }
 
-static inline void dbgi_rd_rsp3(struct adapter *adapter, u32 *v1, u32 *v2,
-				u32 *v3)
+static inline void dbgi_rd_rsp3(adapter_t *adapter, u32 *v1, u32 *v2, u32 *v3)
 {
 	*v1 = t3_read_reg(adapter, A_MC5_DB_DBGI_RSP_DATA0);
 	*v2 = t3_read_reg(adapter, A_MC5_DB_DBGI_RSP_DATA1);
@@ -126,22 +103,21 @@
  * command cmd.  The data to be written must have been set up by the caller.
  * Returns -1 on failure, 0 on success.
  */
-static int mc5_write(struct adapter *adapter, u32 addr_lo, u32 cmd)
+static int mc5_write(adapter_t *adapter, u32 addr_lo, u32 cmd)
 {
 	t3_write_reg(adapter, A_MC5_DB_DBGI_REQ_ADDR0, addr_lo);
 	if (mc5_cmd_write(adapter, cmd) == 0)
 		return 0;
-	CH_ERR(adapter, "MC5 timeout writing to TCAM address 0x%x\n",
-	       addr_lo);
+	CH_ERR(adapter, "MC5 timeout writing to TCAM address 0x%x\n", addr_lo);
 	return -1;
 }
 
 static int init_mask_data_array(struct mc5 *mc5, u32 mask_array_base,
 				u32 data_array_base, u32 write_cmd,
-				int addr_shift)
+			        int addr_shift)
 {
 	unsigned int i;
-	struct adapter *adap = mc5->adapter;
+	adapter_t *adap = mc5->adapter;
 
 	/*
 	 * We need the size of the TCAM data and mask arrays in terms of
@@ -151,7 +127,7 @@
 	unsigned int server_base = t3_read_reg(adap, A_MC5_DB_SERVER_INDEX);
 
 	if (mc5->mode == MC5_MODE_144_BIT) {
-		size72 *= 2;	/* 1 144-bit entry is 2 72-bit entries */
+		size72 *= 2;      /* 1 144-bit entry is 2 72-bit entries */
 		server_base *= 2;
 	}
 
@@ -163,23 +139,33 @@
 			return -1;
 
 	/* Initialize the mask array. */
-	dbgi_wr_data3(adap, 0xffffffff, 0xffffffff, 0xff);
-	for (i = 0; i < size72; i++) {
-		if (i == server_base)	/* entering server or routing region */
-			t3_write_reg(adap, A_MC5_DB_DBGI_REQ_DATA0,
-				     mc5->mode == MC5_MODE_144_BIT ?
-				     0xfffffff9 : 0xfffffffd);
+	for (i = 0; i < server_base; i++) {
+		dbgi_wr_data3(adap, 0x3fffffff, 0xfff80000, 0xff);
+		if (mc5_write(adap, mask_array_base + (i << addr_shift),
+			      write_cmd))
+			return -1;
+		i++;
+		dbgi_wr_data3(adap, 0xffffffff, 0xffffffff, 0xff);
 		if (mc5_write(adap, mask_array_base + (i << addr_shift),
 			      write_cmd))
 			return -1;
 	}
+
+	dbgi_wr_data3(adap,
+		      mc5->mode == MC5_MODE_144_BIT ? 0xfffffff9 : 0xfffffffd,
+		      0xffffffff, 0xff);
+	for (; i < size72; i++)
+		if (mc5_write(adap, mask_array_base + (i << addr_shift),
+			      write_cmd))
+			return -1;
+
 	return 0;
 }
 
 static int init_idt52100(struct mc5 *mc5)
 {
 	int i;
-	struct adapter *adap = mc5->adapter;
+	adapter_t *adap = mc5->adapter;
 
 	t3_write_reg(adap, A_MC5_DB_RSP_LATENCY,
 		     V_RDLAT(0x15) | V_LRNLAT(0x15) | V_SRCHLAT(0x15));
@@ -236,18 +222,18 @@
 
 	return init_mask_data_array(mc5, IDT_MSKARY_BASE_ADR0,
 				    IDT_DATARY_BASE_ADR0, IDT_CMD_WRITE, 0);
-err:
+ err:
 	return -EIO;
 }
 
 static int init_idt43102(struct mc5 *mc5)
 {
 	int i;
-	struct adapter *adap = mc5->adapter;
+	adapter_t *adap = mc5->adapter;
 
 	t3_write_reg(adap, A_MC5_DB_RSP_LATENCY,
 		     adap->params.rev == 0 ? V_RDLAT(0xd) | V_SRCHLAT(0x11) :
-		     V_RDLAT(0xd) | V_SRCHLAT(0x12));
+					     V_RDLAT(0xd) | V_SRCHLAT(0x12));
 
 	/*
 	 * Use GMRs 24-25 for ELOOKUP, GMRs 20-21 for SYN lookups, and no mask
@@ -296,37 +282,42 @@
 
 	return init_mask_data_array(mc5, IDT4_MSKARY_BASE_ADR0,
 				    IDT4_DATARY_BASE_ADR0, IDT4_CMD_WRITE, 1);
-err:
+ err:
 	return -EIO;
 }
 
 /* Put MC5 in DBGI mode. */
 static inline void mc5_dbgi_mode_enable(const struct mc5 *mc5)
 {
-	t3_write_reg(mc5->adapter, A_MC5_DB_CONFIG,
-		     V_TMMODE(mc5->mode == MC5_MODE_72_BIT) | F_DBGIEN);
+	t3_set_reg_field(mc5->adapter, A_MC5_DB_CONFIG, F_PRTYEN | F_MBUSEN,
+			 F_DBGIEN);
 }
 
 /* Put MC5 in M-Bus mode. */
 static void mc5_dbgi_mode_disable(const struct mc5 *mc5)
 {
-	t3_write_reg(mc5->adapter, A_MC5_DB_CONFIG,
-		     V_TMMODE(mc5->mode == MC5_MODE_72_BIT) |
-		     V_COMPEN(mc5->mode == MC5_MODE_72_BIT) |
-		     V_PRTYEN(mc5->parity_enabled) | F_MBUSEN);
+	t3_set_reg_field(mc5->adapter, A_MC5_DB_CONFIG, F_DBGIEN,
+			 V_PRTYEN(mc5->parity_enabled) | F_MBUSEN);
 }
 
-/*
- * Initialization that requires the OS and protocol layers to already
- * be intialized goes here.
+/**
+ *	t3_mc5_init - initialize MC5 and the TCAM
+ *	@mc5: the MC5 handle
+ *	@nservers: desired number the TCP servers (listening ports)
+ *	@nfilters: desired number of HW filters (classifiers)
+ *	@nroutes: desired number of routes
+ *
+ *	Initialize MC5 and the TCAM and partition the TCAM for the requested
+ *	number of servers, filters, and routes.  The number of routes is
+ *	typically 0 except for specialized uses of the T3 adapters.
  */
 int t3_mc5_init(struct mc5 *mc5, unsigned int nservers, unsigned int nfilters,
 		unsigned int nroutes)
 {
-	u32 cfg;
 	int err;
 	unsigned int tcam_size = mc5->tcam_size;
-	struct adapter *adap = mc5->adapter;
+	unsigned int mode72 = mc5->mode == MC5_MODE_72_BIT;
+	adapter_t *adap = mc5->adapter;
 
 	if (!tcam_size)
 		return 0;
@@ -334,10 +325,12 @@
 	if (nroutes > MAX_ROUTES || nroutes + nservers + nfilters > tcam_size)
 		return -EINVAL;
 
+	if (nfilters)
+		mc5->parity_enabled = 0;
+
 	/* Reset the TCAM */
-	cfg = t3_read_reg(adap, A_MC5_DB_CONFIG) & ~F_TMMODE;
-	cfg |= V_TMMODE(mc5->mode == MC5_MODE_72_BIT) | F_TMRST;
-	t3_write_reg(adap, A_MC5_DB_CONFIG, cfg);
+	t3_set_reg_field(adap, A_MC5_DB_CONFIG, F_TMMODE | F_COMPEN,
+			 V_COMPEN(mode72) | V_TMMODE(mode72) | F_TMRST);
 	if (t3_wait_op_done(adap, A_MC5_DB_CONFIG, F_TMRDY, 1, 500, 0)) {
 		CH_ERR(adap, "TCAM reset timed out\n");
 		return -1;
@@ -349,8 +342,6 @@
 	t3_write_reg(adap, A_MC5_DB_SERVER_INDEX,
 		     tcam_size - nroutes - nfilters - nservers);
 
-	mc5->parity_enabled = 1;
-
 	/* All the TCAM addresses we access have only the low 32 bits non 0 */
 	t3_write_reg(adap, A_MC5_DB_DBGI_REQ_ADDR1, 0);
 	t3_write_reg(adap, A_MC5_DB_DBGI_REQ_ADDR2, 0);
@@ -374,7 +365,7 @@
 	return err;
 }
 
-/*
+/**
  *	read_mc5_range - dump a part of the memory managed by MC5
  *	@mc5: the MC5 handle
  *	@start: the start address for the dump
@@ -388,7 +379,7 @@
 {
 	u32 read_cmd;
 	int err = 0;
-	struct adapter *adap = mc5->adapter;
+	adapter_t *adap = mc5->adapter;
 
 	if (mc5->part_type == IDT75P52100)
 		read_cmd = IDT_CMD_READ;
@@ -410,17 +401,20 @@
 	}
 
 	mc5_dbgi_mode_disable(mc5);
-	return 0;
+	return err;
 }
 
 #define MC5_INT_FATAL (F_PARITYERR | F_REQQPARERR | F_DISPQPARERR)
 
-/*
- * MC5 interrupt handler
+/**
+ *	t3_mc5_intr_handler - MC5 interrupt handler
+ *	@mc5: the MC5 handle
+ *
+ *	The MC5 interrupt handler.
  */
 void t3_mc5_intr_handler(struct mc5 *mc5)
 {
-	struct adapter *adap = mc5->adapter;
+	adapter_t *adap = mc5->adapter;
 	u32 cause = t3_read_reg(adap, A_MC5_DB_INT_CAUSE);
 
 	if ((cause & F_PARITYERR) && mc5->parity_enabled) {
@@ -452,11 +446,20 @@
 	t3_write_reg(adap, A_MC5_DB_INT_CAUSE, cause);
 }
 
-void t3_mc5_prep(struct adapter *adapter, struct mc5 *mc5, int mode)
+/**
+ *	t3_mc5_prep - initialize the SW state for MC5
+ *	@adapter: the adapter
+ *	@mc5: the MC5 handle
+ *	@mode: whether the TCAM will be in 72- or 144-bit mode
+ *
+ *	Initialize the SW state associated with MC5.  Among other things
+ *	this determines the size of the attached TCAM.
+ */
+void __devinit t3_mc5_prep(adapter_t *adapter, struct mc5 *mc5, int mode)
 {
 #define K * 1024
 
-	static unsigned int tcam_part_size[] = {	/* in K 72-bit entries */
+	static unsigned int tcam_part_size[] = {  /* in K 72-bit entries */
 		64 K, 128 K, 256 K, 32 K
 	};
 
@@ -465,8 +468,9 @@
 	u32 cfg = t3_read_reg(adapter, A_MC5_DB_CONFIG);
 
 	mc5->adapter = adapter;
-	mc5->mode = (unsigned char)mode;
-	mc5->part_type = (unsigned char)G_TMTYPE(cfg);
+	mc5->parity_enabled = 1;
+	mc5->mode = (unsigned char) mode;
+	mc5->part_type = (unsigned char) G_TMTYPE(cfg);
 	if (cfg & F_TMTYPEHI)
 		mc5->part_type |= 4;
 
diff --git a/drivers/net/cxgb3/mv88e1xxx.c b/drivers/net/cxgb3/mv88e1xxx.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/mv88e1xxx.c
@@ -0,0 +1,295 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include "common.h"
+
+/* Marvell PHY interrupt status bits. */
+#define MV_INTR_JABBER          0x0001
+#define MV_INTR_POLARITY_CHNG   0x0002
+#define MV_INTR_ENG_DETECT_CHNG 0x0010
+#define MV_INTR_DOWNSHIFT       0x0020
+#define MV_INTR_MDI_XOVER_CHNG  0x0040
+#define MV_INTR_FIFO_OVER_UNDER 0x0080
+#define MV_INTR_FALSE_CARRIER   0x0100
+#define MV_INTR_SYMBOL_ERROR    0x0200
+#define MV_INTR_LINK_CHNG       0x0400
+#define MV_INTR_AUTONEG_DONE    0x0800
+#define MV_INTR_PAGE_RECV       0x1000
+#define MV_INTR_DUPLEX_CHNG     0x2000
+#define MV_INTR_SPEED_CHNG      0x4000
+#define MV_INTR_AUTONEG_ERR     0x8000
+
+/* Marvell PHY specific registers. */
+#define MV88E1XXX_SPECIFIC_CNTRL          16
+#define MV88E1XXX_SPECIFIC_STATUS         17
+#define MV88E1XXX_INTR_ENABLE             18
+#define MV88E1XXX_INTR_STATUS             19
+#define MV88E1XXX_EXT_SPECIFIC_CNTRL      20
+#define MV88E1XXX_RECV_ERR                21
+#define MV88E1XXX_EXT_ADDR                22
+#define MV88E1XXX_GLOBAL_STATUS           23
+#define MV88E1XXX_LED_CNTRL               24
+#define MV88E1XXX_LED_OVERRIDE            25
+#define MV88E1XXX_EXT_SPECIFIC_CNTRL2     26
+#define MV88E1XXX_EXT_SPECIFIC_STATUS     27
+#define MV88E1XXX_VIRTUAL_CABLE_TESTER    28
+#define MV88E1XXX_EXTENDED_ADDR           29
+#define MV88E1XXX_EXTENDED_DATA           30
+
+/* PHY specific control register fields */
+#define S_PSCR_MDI_XOVER_MODE    5
+#define M_PSCR_MDI_XOVER_MODE    0x3
+#define V_PSCR_MDI_XOVER_MODE(x) ((x) << S_PSCR_MDI_XOVER_MODE)
+
+/* Extended PHY specific control register fields */
+#define S_DOWNSHIFT_ENABLE 8
+#define V_DOWNSHIFT_ENABLE (1 << S_DOWNSHIFT_ENABLE)
+
+#define S_DOWNSHIFT_CNT    9
+#define M_DOWNSHIFT_CNT    0x7
+#define V_DOWNSHIFT_CNT(x) ((x) << S_DOWNSHIFT_CNT)
+
+/* PHY specific status register fields */
+#define S_PSSR_JABBER 0
+#define V_PSSR_JABBER (1 << S_PSSR_JABBER)
+
+#define S_PSSR_POLARITY 1
+#define V_PSSR_POLARITY (1 << S_PSSR_POLARITY)
+
+#define S_PSSR_RX_PAUSE 2
+#define V_PSSR_RX_PAUSE (1 << S_PSSR_RX_PAUSE)
+
+#define S_PSSR_TX_PAUSE 3
+#define V_PSSR_TX_PAUSE (1 << S_PSSR_TX_PAUSE)
+
+#define S_PSSR_ENERGY_DETECT 4
+#define V_PSSR_ENERGY_DETECT (1 << S_PSSR_ENERGY_DETECT)
+
+#define S_PSSR_DOWNSHIFT_STATUS 5
+#define V_PSSR_DOWNSHIFT_STATUS (1 << S_PSSR_DOWNSHIFT_STATUS)
+
+#define S_PSSR_MDI 6
+#define V_PSSR_MDI (1 << S_PSSR_MDI)
+
+#define S_PSSR_CABLE_LEN    7
+#define M_PSSR_CABLE_LEN    0x7
+#define V_PSSR_CABLE_LEN(x) ((x) << S_PSSR_CABLE_LEN)
+#define G_PSSR_CABLE_LEN(x) (((x) >> S_PSSR_CABLE_LEN) & M_PSSR_CABLE_LEN)
+
+#define S_PSSR_LINK 10
+#define V_PSSR_LINK (1 << S_PSSR_LINK)
+
+#define S_PSSR_STATUS_RESOLVED 11
+#define V_PSSR_STATUS_RESOLVED (1 << S_PSSR_STATUS_RESOLVED)
+
+#define S_PSSR_PAGE_RECEIVED 12
+#define V_PSSR_PAGE_RECEIVED (1 << S_PSSR_PAGE_RECEIVED)
+
+#define S_PSSR_DUPLEX 13
+#define V_PSSR_DUPLEX (1 << S_PSSR_DUPLEX)
+
+#define S_PSSR_SPEED    14
+#define M_PSSR_SPEED    0x3
+#define V_PSSR_SPEED(x) ((x) << S_PSSR_SPEED)
+#define G_PSSR_SPEED(x) (((x) >> S_PSSR_SPEED) & M_PSSR_SPEED)
+
+/* MV88E1XXX MDI crossover register values */
+#define CROSSOVER_MDI   0
+#define CROSSOVER_MDIX  1
+#define CROSSOVER_AUTO  3
+
+#define INTR_ENABLE_MASK (MV_INTR_SPEED_CHNG | MV_INTR_DUPLEX_CHNG | \
+	MV_INTR_AUTONEG_DONE | MV_INTR_LINK_CHNG | MV_INTR_FIFO_OVER_UNDER | \
+	MV_INTR_ENG_DETECT_CHNG)
+
+/*
+ * Reset the PHY.  If 'wait' is set wait until the reset completes.
+ */
+static int mv88e1xxx_reset(struct cphy *cphy, int wait)
+{
+	return t3_phy_reset(cphy, 0, wait);
+}
+
+static int mv88e1xxx_intr_enable(struct cphy *cphy)
+{
+	return mdio_write(cphy, 0, MV88E1XXX_INTR_ENABLE, INTR_ENABLE_MASK);
+}
+
+static int mv88e1xxx_intr_disable(struct cphy *cphy)
+{
+	return mdio_write(cphy, 0, MV88E1XXX_INTR_ENABLE, 0);
+}
+
+static int mv88e1xxx_intr_clear(struct cphy *cphy)
+{
+	u32 val;
+
+	/* Clear PHY interrupts by reading the register. */
+	return mdio_read(cphy, 0, MV88E1XXX_INTR_STATUS, &val);
+}
+
+static int mv88e1xxx_crossover_set(struct cphy *cphy, int crossover)
+{
+	return t3_mdio_change_bits(cphy, 0, MV88E1XXX_SPECIFIC_CNTRL,
+				   V_PSCR_MDI_XOVER_MODE(M_PSCR_MDI_XOVER_MODE),
+				   V_PSCR_MDI_XOVER_MODE(crossover));
+}
+
+static int mv88e1xxx_autoneg_enable(struct cphy *cphy)
+{
+	mv88e1xxx_crossover_set(cphy, CROSSOVER_AUTO);
+
+	/* restart autoneg for change to take effect */
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_PDOWN | BMCR_ISOLATE,
+			 	   BMCR_ANENABLE | BMCR_ANRESTART);
+}
+
+static int mv88e1xxx_autoneg_restart(struct cphy *cphy)
+{
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_PDOWN | BMCR_ISOLATE,
+			 	   BMCR_ANRESTART);
+}
+
+static int mv88e1xxx_set_loopback(struct cphy *cphy, int mmd, int dir, int on)
+{
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_LOOPBACK,
+			 	   on ? BMCR_LOOPBACK : 0);
+}
+
+static int mv88e1xxx_get_link_status(struct cphy *cphy, int *link_ok,
+				     int *speed, int *duplex, int *fc)
+{
+	u32 status;
+	int sp = -1, dplx = -1, pause = 0;
+
+	mdio_read(cphy, 0, MV88E1XXX_SPECIFIC_STATUS, &status);
+	if ((status & V_PSSR_STATUS_RESOLVED) != 0) {
+		if (status & V_PSSR_RX_PAUSE)
+			pause |= PAUSE_RX;
+		if (status & V_PSSR_TX_PAUSE)
+			pause |= PAUSE_TX;
+		dplx = (status & V_PSSR_DUPLEX) ? DUPLEX_FULL : DUPLEX_HALF;
+		sp = G_PSSR_SPEED(status);
+		if (sp == 0)
+			sp = SPEED_10;
+		else if (sp == 1)
+			sp = SPEED_100;
+		else
+			sp = SPEED_1000;
+	}
+	if (link_ok)
+		*link_ok = (status & V_PSSR_LINK) != 0;
+	if (speed)
+		*speed = sp;
+	if (duplex)
+		*duplex = dplx;
+	if (fc)
+		*fc = pause;
+	return 0;
+}
+
+static int mv88e1xxx_set_speed_duplex(struct cphy *phy, int speed, int duplex)
+{
+	int err = t3_set_phy_speed_duplex(phy, speed, duplex);
+
+	/* PHY needs reset for new settings to take effect */
+	if (!err)
+		err = mv88e1xxx_reset(phy, 0);
+	return err;
+}
+
+static int mv88e1xxx_downshift_set(struct cphy *cphy, int downshift_enable)
+{
+	/*
+	 * Set the downshift counter to 2 so we try to establish Gb link
+	 * twice before downshifting.
+	 */
+	return t3_mdio_change_bits(cphy, 0, MV88E1XXX_EXT_SPECIFIC_CNTRL,
+		V_DOWNSHIFT_ENABLE | V_DOWNSHIFT_CNT(M_DOWNSHIFT_CNT),
+		downshift_enable ? V_DOWNSHIFT_ENABLE | V_DOWNSHIFT_CNT(2) : 0);
+}
+
+static int mv88e1xxx_power_down(struct cphy *cphy, int enable)
+{
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_PDOWN,
+				   enable ? BMCR_PDOWN : 0);
+}
+
+static int mv88e1xxx_intr_handler(struct cphy *cphy)
+{
+	const u32 link_change_intrs = MV_INTR_LINK_CHNG |
+		MV_INTR_AUTONEG_DONE | MV_INTR_DUPLEX_CHNG |
+		MV_INTR_SPEED_CHNG | MV_INTR_DOWNSHIFT;
+
+	u32 cause;
+	int cphy_cause = 0;
+
+	mdio_read(cphy, 0, MV88E1XXX_INTR_STATUS, &cause);
+	cause &= INTR_ENABLE_MASK;
+	if (cause & link_change_intrs)
+		cphy_cause |= cphy_cause_link_change;
+	if (cause & MV_INTR_FIFO_OVER_UNDER)
+		cphy_cause |= cphy_cause_fifo_error;
+	return cphy_cause;
+}
+
+#ifdef C99_NOT_SUPPORTED
+static struct cphy_ops mv88e1xxx_ops = {
+	mv88e1xxx_reset,
+	mv88e1xxx_intr_enable,
+	mv88e1xxx_intr_disable,
+	mv88e1xxx_intr_clear,
+	mv88e1xxx_intr_handler,
+	mv88e1xxx_autoneg_enable,
+	mv88e1xxx_autoneg_restart,
+	t3_phy_advertise,
+	mv88e1xxx_set_loopback,
+	mv88e1xxx_set_speed_duplex,
+	mv88e1xxx_get_link_status,
+	mv88e1xxx_power_down,
+};
+#else
+static struct cphy_ops mv88e1xxx_ops = {
+	.reset             = mv88e1xxx_reset,
+	.intr_enable       = mv88e1xxx_intr_enable,
+	.intr_disable      = mv88e1xxx_intr_disable,
+	.intr_clear        = mv88e1xxx_intr_clear,
+	.intr_handler      = mv88e1xxx_intr_handler,
+	.autoneg_enable    = mv88e1xxx_autoneg_enable,
+	.autoneg_restart   = mv88e1xxx_autoneg_restart,
+	.advertise         = t3_phy_advertise,
+	.set_loopback      = mv88e1xxx_set_loopback,
+	.set_speed_duplex  = mv88e1xxx_set_speed_duplex,
+	.get_link_status   = mv88e1xxx_get_link_status,
+	.power_down        = mv88e1xxx_power_down,
+};
+#endif
+
+int t3_mv88e1xxx_phy_prep(pinfo_t *pinfo, int phy_addr,
+			  const struct mdio_ops *mdio_ops)
+{
+	struct cphy *phy = &pinfo->phy;
+	int err;
+
+	cphy_init(phy, pinfo->adapter, pinfo, phy_addr, &mv88e1xxx_ops, mdio_ops,
+		  SUPPORTED_10baseT_Full | SUPPORTED_100baseT_Full |
+		  SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg | SUPPORTED_MII |
+		  SUPPORTED_TP | SUPPORTED_IRQ, "10/100/1000BASE-T");
+
+	/* Configure copper PHY transmitter as class A to reduce EMI. */
+	err = mdio_write(phy, 0, MV88E1XXX_EXTENDED_ADDR, 0xb);
+	if (!err)
+		err = mdio_write(phy, 0, MV88E1XXX_EXTENDED_DATA, 0x8004);
+
+	if (!err)
+		err = mv88e1xxx_downshift_set(phy, 1);   /* Enable downshift */
+	return err;
+}
diff --git a/drivers/net/cxgb3/osdep.h b/drivers/net/cxgb3/osdep.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/osdep.h
@@ -0,0 +1,216 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#ifndef __CHELSIO_OSDEP_H
+#define __CHELSIO_OSDEP_H
+
+#include <linux/kernel.h>
+#include <linux/types.h>
+#include <linux/ctype.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/netdevice.h>
+#include <linux/ethtool.h>
+#include <linux/mii.h>
+#include <linux/version.h>
+#include "version.h"
+
+#define CH_ERR(adap, fmt, ...)   dev_err(&adap->pdev->dev, fmt, ## __VA_ARGS__)
+#define CH_WARN(adap, fmt, ...)  dev_warn(&adap->pdev->dev, fmt, ## __VA_ARGS__)
+#define CH_ALERT(adap, fmt, ...) \
+	dev_printk(KERN_ALERT, &adap->pdev->dev, fmt, ## __VA_ARGS__)
+
+/*
+ * More powerful macro that selectively prints messages based on msg_enable.
+ * For info and debugging messages.
+ */
+#define CH_MSG(adapter, level, category, fmt, ...) do { \
+	if ((adapter)->msg_enable & NETIF_MSG_##category) \
+		dev_printk(KERN_##level, &adapter->pdev->dev, fmt, \
+			   ## __VA_ARGS__); \
+} while (0)
+
+#ifdef DEBUG
+# define CH_DBG(adapter, category, fmt, ...) \
+	CH_MSG(adapter, DEBUG, category, fmt, ## __VA_ARGS__)
+#else
+# define CH_DBG(adapter, category, fmt, ...)
+#endif
+
+/* Additional NETIF_MSG_* categories */
+#define NETIF_MSG_OFLD 0x4000000
+#define NETIF_MSG_MMIO 0x8000000
+
+#define IFF_FILTER_ETH_P_SLOW 0x4
+
+typedef struct adapter adapter_t;
+typedef struct port_info pinfo_t;
+
+/**
+ * struct t3_rx_mode - encapsulates the Rx mode for a port
+ * @dev: the net_device associated with the port
+ * @mclist: the multicast address list for the port
+ * @idx: current position within the multicast list
+ *
+ * This structure is passed to the MAC routines that configure the Rx mode
+ * of a port.  The structure is opaque to the common code.  It invokes a few
+ * functions on this structure including promisc_rx_mode()
+ * that returns whether the port should be in promiscuous mode,
+ * allmulti_rx_mode() to check if the port should be in ALLMULTI mode,
+ * and t3_get_next_mcaddr() that returns the multicast addresses for the
+ * port one at a time.
+ */
+struct t3_rx_mode {
+	struct net_device *dev;
+	struct dev_mc_list *mclist;
+	unsigned int idx;
+};
+
+static inline void init_rx_mode(struct t3_rx_mode *p, struct net_device *dev)
+{
+	p->dev = dev;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,34)
+	p->mclist = dev->mc_list;
+#else
+	p->mclist = NULL;
+#endif
+	p->idx = 0;
+}
+
+#define promisc_rx_mode(rm)  ((rm)->dev->flags & IFF_PROMISC) 
+#define allmulti_rx_mode(rm) ((rm)->dev->flags & IFF_ALLMULTI) 
+
+/**
+ * t3_get_next_mcaddr - return the next L2 multicast address for a port
+ * @rm: the Rx mode info
+ *
+ * Returns the next Ethernet multicast address for a port or %NULL if there are
+ * no more.
+ */
+static inline u8 *t3_get_next_mcaddr(struct t3_rx_mode *rm)
+{
+	u8 *addr = NULL;
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,6,34)
+	if (rm->mclist && rm->idx < rm->dev->mc_count) {
+		addr = rm->mclist->dmi_addr;
+		rm->mclist = rm->mclist->next;
+		rm->idx++;
+	}
+#else
+	struct netdev_hw_addr *ha;
+	int cur = 0;	
+
+	netdev_for_each_mc_addr(ha, rm->dev) {
+		if (cur == rm->idx) {
+			addr = ha->addr;
+			rm->idx++;
+		}
+		cur++;
+	}
+#endif
+
+	return addr;
+}
+
+enum {
+	TP_TMR_RES = 200,	/* TP timer resolution in usec */
+	MAX_NPORTS = 4,		/* max # of ports */
+	TP_SRAM_OFFSET = 4096,	/* TP SRAM content offset in eeprom */
+	TP_SRAM_LEN = 2112,	/* TP SRAM content offset in eeprom */
+};
+
+/* compatibility stuff for older kernels */
+#ifndef PCI_EXP_LNKSTA
+#define PCI_EXP_LNKSTA          18      /* Link Status */
+#endif
+
+#ifndef PCI_EXP_LNKCTL
+#define PCI_EXP_LNKCTL		16	/* Link Control */
+#endif
+
+#ifndef PCI_EXP_LNKCAP
+#define PCI_EXP_LNKCAP		12	/* Link Capabilities */
+#endif
+
+#ifndef PCI_EXP_DEVCTL
+#define PCI_EXP_DEVCTL		8	/* Device Control */
+#endif
+
+#ifndef PCI_EXP_DEVCTL_PAYLOAD
+#define  PCI_EXP_DEVCTL_PAYLOAD	0x00e0	/* Max_Payload_Size */
+#endif
+
+#ifndef PCI_EXP_DEVCTL_READRQ
+#define  PCI_EXP_DEVCTL_READRQ  0x7000  /* Max_Read_Request_Size */
+#endif
+
+#ifndef BMCR_SPEED1000
+#define BMCR_SPEED1000		0x0040  /* MSB of Speed (1000) */
+#endif
+
+#ifndef MII_CTRL1000
+#define MII_CTRL1000            0x09    /* 1000BASE-T control */
+#define ADVERTISE_1000FULL      0x0200  /* Advertise 1000BASE-T full duplex */
+#define ADVERTISE_1000HALF      0x0100  /* Advertise 1000BASE-T half duplex */
+#endif
+
+#ifndef ADVERTISE_PAUSE_CAP
+#define ADVERTISE_PAUSE_CAP     0x0400  /* Try for pause               */
+#define ADVERTISE_PAUSE_ASYM    0x0800  /* Try for asymetric pause     */
+#endif
+
+#ifndef ADVERTISED_Pause
+#define ADVERTISED_Pause        (1 << 13)
+#define ADVERTISED_Asym_Pause   (1 << 14)
+#endif
+
+#ifndef NETDEV_TX_OK
+#define NETDEV_TX_OK 0		/* driver took care of packet */
+#define NETDEV_TX_BUSY 1	/* driver tx path was busy*/
+#define NETDEV_TX_LOCKED -1	/* driver tx lock was already taken */
+#endif
+
+#ifndef ADVERTISE_1000XFULL
+#define ADVERTISE_1000XFULL	0x0020
+#endif
+
+#ifndef ADVERTISE_1000XHALF
+#define ADVERTISE_1000XHALF	0x0040
+#endif
+
+#ifndef ADVERTISE_1000XPAUSE
+#define ADVERTISE_1000XPAUSE	0x0080
+#endif
+
+#ifndef ADVERTISE_1000XPSE_ASYM
+#define ADVERTISE_1000XPSE_ASYM 0x0100
+#endif
+
+/* Note: cxgb3_compat.h assumes that struct adapter is already defined. 
+ * delayed_work is used in struct adapter definition, hence backporting
+ * its definition here.
+ */
+#include <linux/version.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,20)
+#define delayed_work work_struct
+#endif
+
+#ifdef	LINUX_2_4
+#include "linux_2_4_compat.h"
+#include "linux_2_4_compat_workqueue.h"
+#endif
+
+#ifdef CONFIG_XEN
+#define CHELSIO_FREE_TXBUF_ASAP 1	/* VMs need TX bufs freed ASAP */
+#endif
+
+#endif  /* !__CHELSIO_OSDEP_H */
diff --git a/drivers/net/cxgb3/regs.h b/drivers/net/cxgb3/regs.h
--- a/drivers/net/cxgb3/regs.h
+++ b/drivers/net/cxgb3/regs.h
@@ -1,3 +1,8 @@
+/* This file is automatically generated --- do not edit */
+
+/* registers for module SGE3 */
+#define SGE3_BASE_ADDR 0x0
+
 #define A_SG_CONTROL 0x0
 
 #define S_CONGMODE    29
@@ -12,6 +17,30 @@
 #define V_FATLPERREN(x) ((x) << S_FATLPERREN)
 #define F_FATLPERREN    V_FATLPERREN(1U)
 
+#define S_URGTNL    26
+#define V_URGTNL(x) ((x) << S_URGTNL)
+#define F_URGTNL    V_URGTNL(1U)
+
+#define S_NEWNOTIFY    25
+#define V_NEWNOTIFY(x) ((x) << S_NEWNOTIFY)
+#define F_NEWNOTIFY    V_NEWNOTIFY(1U)
+
+#define S_AVOIDCQOVFL    24
+#define V_AVOIDCQOVFL(x) ((x) << S_AVOIDCQOVFL)
+#define F_AVOIDCQOVFL    V_AVOIDCQOVFL(1U)
+
+#define S_OPTONEINTMULTQ    23
+#define V_OPTONEINTMULTQ(x) ((x) << S_OPTONEINTMULTQ)
+#define F_OPTONEINTMULTQ    V_OPTONEINTMULTQ(1U)
+
+#define S_CQCRDTCTRL    22
+#define V_CQCRDTCTRL(x) ((x) << S_CQCRDTCTRL)
+#define F_CQCRDTCTRL    V_CQCRDTCTRL(1U)
+
+#define S_EGRENUPBP    21
+#define V_EGRENUPBP(x) ((x) << S_EGRENUPBP)
+#define F_EGRENUPBP    V_EGRENUPBP(1U)
+
 #define S_DROPPKT    20
 #define V_DROPPKT(x) ((x) << S_DROPPKT)
 #define F_DROPPKT    V_DROPPKT(1U)
@@ -23,10 +52,16 @@
 #define S_USERSPACESIZE    14
 #define M_USERSPACESIZE    0x1f
 #define V_USERSPACESIZE(x) ((x) << S_USERSPACESIZE)
+#define G_USERSPACESIZE(x) (((x) >> S_USERSPACESIZE) & M_USERSPACESIZE)
 
 #define S_HOSTPAGESIZE    11
 #define M_HOSTPAGESIZE    0x7
 #define V_HOSTPAGESIZE(x) ((x) << S_HOSTPAGESIZE)
+#define G_HOSTPAGESIZE(x) (((x) >> S_HOSTPAGESIZE) & M_HOSTPAGESIZE)
+
+#define S_PCIRELAX    10
+#define V_PCIRELAX(x) ((x) << S_PCIRELAX)
+#define F_PCIRELAX    V_PCIRELAX(1U)
 
 #define S_FLMODE    9
 #define V_FLMODE(x) ((x) << S_FLMODE)
@@ -35,11 +70,20 @@
 #define S_PKTSHIFT    6
 #define M_PKTSHIFT    0x7
 #define V_PKTSHIFT(x) ((x) << S_PKTSHIFT)
+#define G_PKTSHIFT(x) (((x) >> S_PKTSHIFT) & M_PKTSHIFT)
 
 #define S_ONEINTMULTQ    5
 #define V_ONEINTMULTQ(x) ((x) << S_ONEINTMULTQ)
 #define F_ONEINTMULTQ    V_ONEINTMULTQ(1U)
 
+#define S_FLPICKAVAIL    4
+#define V_FLPICKAVAIL(x) ((x) << S_FLPICKAVAIL)
+#define F_FLPICKAVAIL    V_FLPICKAVAIL(1U)
+
+#define S_BIGENDIANEGRESS    3
+#define V_BIGENDIANEGRESS(x) ((x) << S_BIGENDIANEGRESS)
+#define F_BIGENDIANEGRESS    V_BIGENDIANEGRESS(1U)
+
 #define S_BIGENDIANINGRESS    2
 #define V_BIGENDIANINGRESS(x) ((x) << S_BIGENDIANINGRESS)
 #define F_BIGENDIANINGRESS    V_BIGENDIANINGRESS(1U)
@@ -52,18 +96,6 @@
 #define V_GLOBALENABLE(x) ((x) << S_GLOBALENABLE)
 #define F_GLOBALENABLE    V_GLOBALENABLE(1U)
 
-#define S_AVOIDCQOVFL    24
-#define V_AVOIDCQOVFL(x) ((x) << S_AVOIDCQOVFL)
-#define F_AVOIDCQOVFL    V_AVOIDCQOVFL(1U)
-
-#define S_OPTONEINTMULTQ    23
-#define V_OPTONEINTMULTQ(x) ((x) << S_OPTONEINTMULTQ)
-#define F_OPTONEINTMULTQ    V_OPTONEINTMULTQ(1U)
-
-#define S_CQCRDTCTRL    22
-#define V_CQCRDTCTRL(x) ((x) << S_CQCRDTCTRL)
-#define F_CQCRDTCTRL    V_CQCRDTCTRL(1U)
-
 #define A_SG_KDOORBELL 0x4
 
 #define S_SELEGRCNTX    31
@@ -73,6 +105,7 @@
 #define S_EGRCNTX    0
 #define M_EGRCNTX    0xffff
 #define V_EGRCNTX(x) ((x) << S_EGRCNTX)
+#define G_EGRCNTX(x) (((x) >> S_EGRCNTX) & M_EGRCNTX)
 
 #define A_SG_GTS 0x8
 
@@ -84,31 +117,30 @@
 #define S_NEWTIMER    16
 #define M_NEWTIMER    0x1fff
 #define V_NEWTIMER(x) ((x) << S_NEWTIMER)
+#define G_NEWTIMER(x) (((x) >> S_NEWTIMER) & M_NEWTIMER)
 
 #define S_NEWINDEX    0
 #define M_NEWINDEX    0xffff
 #define V_NEWINDEX(x) ((x) << S_NEWINDEX)
+#define G_NEWINDEX(x) (((x) >> S_NEWINDEX) & M_NEWINDEX)
 
 #define A_SG_CONTEXT_CMD 0xc
 
 #define S_CONTEXT_CMD_OPCODE    28
 #define M_CONTEXT_CMD_OPCODE    0xf
 #define V_CONTEXT_CMD_OPCODE(x) ((x) << S_CONTEXT_CMD_OPCODE)
+#define G_CONTEXT_CMD_OPCODE(x) (((x) >> S_CONTEXT_CMD_OPCODE) & M_CONTEXT_CMD_OPCODE)
 
 #define S_CONTEXT_CMD_BUSY    27
 #define V_CONTEXT_CMD_BUSY(x) ((x) << S_CONTEXT_CMD_BUSY)
 #define F_CONTEXT_CMD_BUSY    V_CONTEXT_CMD_BUSY(1U)
 
 #define S_CQ_CREDIT    20
-
 #define M_CQ_CREDIT    0x7f
-
 #define V_CQ_CREDIT(x) ((x) << S_CQ_CREDIT)
-
 #define G_CQ_CREDIT(x) (((x) >> S_CQ_CREDIT) & M_CQ_CREDIT)
 
 #define S_CQ    19
-
 #define V_CQ(x) ((x) << S_CQ)
 #define F_CQ    V_CQ(1U)
 
@@ -127,30 +159,22 @@
 #define S_CONTEXT    0
 #define M_CONTEXT    0xffff
 #define V_CONTEXT(x) ((x) << S_CONTEXT)
-
 #define G_CONTEXT(x) (((x) >> S_CONTEXT) & M_CONTEXT)
 
 #define A_SG_CONTEXT_DATA0 0x10
-
 #define A_SG_CONTEXT_DATA1 0x14
-
 #define A_SG_CONTEXT_DATA2 0x18
-
 #define A_SG_CONTEXT_DATA3 0x1c
-
 #define A_SG_CONTEXT_MASK0 0x20
-
 #define A_SG_CONTEXT_MASK1 0x24
-
 #define A_SG_CONTEXT_MASK2 0x28
-
 #define A_SG_CONTEXT_MASK3 0x2c
-
 #define A_SG_RSPQ_CREDIT_RETURN 0x30
 
 #define S_CREDITS    0
 #define M_CREDITS    0xffff
 #define V_CREDITS(x) ((x) << S_CREDITS)
+#define G_CREDITS(x) (((x) >> S_CREDITS) & M_CREDITS)
 
 #define A_SG_DATA_INTR 0x34
 
@@ -158,34 +182,221 @@
 #define V_ERRINTR(x) ((x) << S_ERRINTR)
 #define F_ERRINTR    V_ERRINTR(1U)
 
+#define S_DATAINTR    0
+#define M_DATAINTR    0xff
+#define V_DATAINTR(x) ((x) << S_DATAINTR)
+#define G_DATAINTR(x) (((x) >> S_DATAINTR) & M_DATAINTR)
+
 #define A_SG_HI_DRB_HI_THRSH 0x38
 
+#define S_HIDRBHITHRSH    0
+#define M_HIDRBHITHRSH    0x3ff
+#define V_HIDRBHITHRSH(x) ((x) << S_HIDRBHITHRSH)
+#define G_HIDRBHITHRSH(x) (((x) >> S_HIDRBHITHRSH) & M_HIDRBHITHRSH)
+
 #define A_SG_HI_DRB_LO_THRSH 0x3c
 
+#define S_HIDRBLOTHRSH    0
+#define M_HIDRBLOTHRSH    0x3ff
+#define V_HIDRBLOTHRSH(x) ((x) << S_HIDRBLOTHRSH)
+#define G_HIDRBLOTHRSH(x) (((x) >> S_HIDRBLOTHRSH) & M_HIDRBLOTHRSH)
+
 #define A_SG_LO_DRB_HI_THRSH 0x40
 
+#define S_LODRBHITHRSH    0
+#define M_LODRBHITHRSH    0x3ff
+#define V_LODRBHITHRSH(x) ((x) << S_LODRBHITHRSH)
+#define G_LODRBHITHRSH(x) (((x) >> S_LODRBHITHRSH) & M_LODRBHITHRSH)
+
 #define A_SG_LO_DRB_LO_THRSH 0x44
 
+#define S_LODRBLOTHRSH    0
+#define M_LODRBLOTHRSH    0x3ff
+#define V_LODRBLOTHRSH(x) ((x) << S_LODRBLOTHRSH)
+#define G_LODRBLOTHRSH(x) (((x) >> S_LODRBLOTHRSH) & M_LODRBLOTHRSH)
+
+#define A_SG_ONE_INT_MULT_Q_COALESCING_TIMER 0x48
 #define A_SG_RSPQ_FL_STATUS 0x4c
 
+#define S_RSPQ0STARVED    0
+#define V_RSPQ0STARVED(x) ((x) << S_RSPQ0STARVED)
+#define F_RSPQ0STARVED    V_RSPQ0STARVED(1U)
+
+#define S_RSPQ1STARVED    1
+#define V_RSPQ1STARVED(x) ((x) << S_RSPQ1STARVED)
+#define F_RSPQ1STARVED    V_RSPQ1STARVED(1U)
+
+#define S_RSPQ2STARVED    2
+#define V_RSPQ2STARVED(x) ((x) << S_RSPQ2STARVED)
+#define F_RSPQ2STARVED    V_RSPQ2STARVED(1U)
+
+#define S_RSPQ3STARVED    3
+#define V_RSPQ3STARVED(x) ((x) << S_RSPQ3STARVED)
+#define F_RSPQ3STARVED    V_RSPQ3STARVED(1U)
+
+#define S_RSPQ4STARVED    4
+#define V_RSPQ4STARVED(x) ((x) << S_RSPQ4STARVED)
+#define F_RSPQ4STARVED    V_RSPQ4STARVED(1U)
+
+#define S_RSPQ5STARVED    5
+#define V_RSPQ5STARVED(x) ((x) << S_RSPQ5STARVED)
+#define F_RSPQ5STARVED    V_RSPQ5STARVED(1U)
+
+#define S_RSPQ6STARVED    6
+#define V_RSPQ6STARVED(x) ((x) << S_RSPQ6STARVED)
+#define F_RSPQ6STARVED    V_RSPQ6STARVED(1U)
+
+#define S_RSPQ7STARVED    7
+#define V_RSPQ7STARVED(x) ((x) << S_RSPQ7STARVED)
+#define F_RSPQ7STARVED    V_RSPQ7STARVED(1U)
+
+#define S_RSPQXSTARVED    0
+#define M_RSPQXSTARVED    0xff
+#define V_RSPQXSTARVED(x) ((x) << S_RSPQXSTARVED)
+#define G_RSPQXSTARVED(x) (((x) >> S_RSPQXSTARVED) & M_RSPQXSTARVED)
+
 #define S_RSPQ0DISABLED    8
+#define V_RSPQ0DISABLED(x) ((x) << S_RSPQ0DISABLED)
+#define F_RSPQ0DISABLED    V_RSPQ0DISABLED(1U)
+
+#define S_RSPQ1DISABLED    9
+#define V_RSPQ1DISABLED(x) ((x) << S_RSPQ1DISABLED)
+#define F_RSPQ1DISABLED    V_RSPQ1DISABLED(1U)
+
+#define S_RSPQ2DISABLED    10
+#define V_RSPQ2DISABLED(x) ((x) << S_RSPQ2DISABLED)
+#define F_RSPQ2DISABLED    V_RSPQ2DISABLED(1U)
+
+#define S_RSPQ3DISABLED    11
+#define V_RSPQ3DISABLED(x) ((x) << S_RSPQ3DISABLED)
+#define F_RSPQ3DISABLED    V_RSPQ3DISABLED(1U)
+
+#define S_RSPQ4DISABLED    12
+#define V_RSPQ4DISABLED(x) ((x) << S_RSPQ4DISABLED)
+#define F_RSPQ4DISABLED    V_RSPQ4DISABLED(1U)
+
+#define S_RSPQ5DISABLED    13
+#define V_RSPQ5DISABLED(x) ((x) << S_RSPQ5DISABLED)
+#define F_RSPQ5DISABLED    V_RSPQ5DISABLED(1U)
+
+#define S_RSPQ6DISABLED    14
+#define V_RSPQ6DISABLED(x) ((x) << S_RSPQ6DISABLED)
+#define F_RSPQ6DISABLED    V_RSPQ6DISABLED(1U)
+
+#define S_RSPQ7DISABLED    15
+#define V_RSPQ7DISABLED(x) ((x) << S_RSPQ7DISABLED)
+#define F_RSPQ7DISABLED    V_RSPQ7DISABLED(1U)
 
 #define S_FL0EMPTY    16
 #define V_FL0EMPTY(x) ((x) << S_FL0EMPTY)
 #define F_FL0EMPTY    V_FL0EMPTY(1U)
 
+#define S_FL1EMPTY    17
+#define V_FL1EMPTY(x) ((x) << S_FL1EMPTY)
+#define F_FL1EMPTY    V_FL1EMPTY(1U)
+
+#define S_FL2EMPTY    18
+#define V_FL2EMPTY(x) ((x) << S_FL2EMPTY)
+#define F_FL2EMPTY    V_FL2EMPTY(1U)
+
+#define S_FL3EMPTY    19
+#define V_FL3EMPTY(x) ((x) << S_FL3EMPTY)
+#define F_FL3EMPTY    V_FL3EMPTY(1U)
+
+#define S_FL4EMPTY    20
+#define V_FL4EMPTY(x) ((x) << S_FL4EMPTY)
+#define F_FL4EMPTY    V_FL4EMPTY(1U)
+
+#define S_FL5EMPTY    21
+#define V_FL5EMPTY(x) ((x) << S_FL5EMPTY)
+#define F_FL5EMPTY    V_FL5EMPTY(1U)
+
+#define S_FL6EMPTY    22
+#define V_FL6EMPTY(x) ((x) << S_FL6EMPTY)
+#define F_FL6EMPTY    V_FL6EMPTY(1U)
+
+#define S_FL7EMPTY    23
+#define V_FL7EMPTY(x) ((x) << S_FL7EMPTY)
+#define F_FL7EMPTY    V_FL7EMPTY(1U)
+
+#define S_FL8EMPTY    24
+#define V_FL8EMPTY(x) ((x) << S_FL8EMPTY)
+#define F_FL8EMPTY    V_FL8EMPTY(1U)
+
+#define S_FL9EMPTY    25
+#define V_FL9EMPTY(x) ((x) << S_FL9EMPTY)
+#define F_FL9EMPTY    V_FL9EMPTY(1U)
+
+#define S_FL10EMPTY    26
+#define V_FL10EMPTY(x) ((x) << S_FL10EMPTY)
+#define F_FL10EMPTY    V_FL10EMPTY(1U)
+
+#define S_FL11EMPTY    27
+#define V_FL11EMPTY(x) ((x) << S_FL11EMPTY)
+#define F_FL11EMPTY    V_FL11EMPTY(1U)
+
+#define S_FL12EMPTY    28
+#define V_FL12EMPTY(x) ((x) << S_FL12EMPTY)
+#define F_FL12EMPTY    V_FL12EMPTY(1U)
+
+#define S_FL13EMPTY    29
+#define V_FL13EMPTY(x) ((x) << S_FL13EMPTY)
+#define F_FL13EMPTY    V_FL13EMPTY(1U)
+
+#define S_FL14EMPTY    30
+#define V_FL14EMPTY(x) ((x) << S_FL14EMPTY)
+#define F_FL14EMPTY    V_FL14EMPTY(1U)
+
+#define S_FL15EMPTY    31
+#define V_FL15EMPTY(x) ((x) << S_FL15EMPTY)
+#define F_FL15EMPTY    V_FL15EMPTY(1U)
+
+#define S_FLXEMPTY    16
+#define M_FLXEMPTY    0xffff
+#define V_FLXEMPTY(x) ((x) << S_FLXEMPTY)
+#define G_FLXEMPTY(x) (((x) >> S_FLXEMPTY) & M_FLXEMPTY)
+
+#define A_SG_EGR_PRI_CNT 0x50
+
+#define S_EGRERROPCODE    24
+#define M_EGRERROPCODE    0xff
+#define V_EGRERROPCODE(x) ((x) << S_EGRERROPCODE)
+#define G_EGRERROPCODE(x) (((x) >> S_EGRERROPCODE) & M_EGRERROPCODE)
+
+#define S_EGRHIOPCODE    16
+#define M_EGRHIOPCODE    0xff
+#define V_EGRHIOPCODE(x) ((x) << S_EGRHIOPCODE)
+#define G_EGRHIOPCODE(x) (((x) >> S_EGRHIOPCODE) & M_EGRHIOPCODE)
+
+#define S_EGRLOOPCODE    8
+#define M_EGRLOOPCODE    0xff
+#define V_EGRLOOPCODE(x) ((x) << S_EGRLOOPCODE)
+#define G_EGRLOOPCODE(x) (((x) >> S_EGRLOOPCODE) & M_EGRLOOPCODE)
+
+#define S_EGRPRICNT    0
+#define M_EGRPRICNT    0x1f
+#define V_EGRPRICNT(x) ((x) << S_EGRPRICNT)
+#define G_EGRPRICNT(x) (((x) >> S_EGRPRICNT) & M_EGRPRICNT)
+
 #define A_SG_EGR_RCQ_DRB_THRSH 0x54
 
 #define S_HIRCQDRBTHRSH    16
 #define M_HIRCQDRBTHRSH    0x7ff
 #define V_HIRCQDRBTHRSH(x) ((x) << S_HIRCQDRBTHRSH)
+#define G_HIRCQDRBTHRSH(x) (((x) >> S_HIRCQDRBTHRSH) & M_HIRCQDRBTHRSH)
 
 #define S_LORCQDRBTHRSH    0
 #define M_LORCQDRBTHRSH    0x7ff
 #define V_LORCQDRBTHRSH(x) ((x) << S_LORCQDRBTHRSH)
+#define G_LORCQDRBTHRSH(x) (((x) >> S_LORCQDRBTHRSH) & M_LORCQDRBTHRSH)
 
 #define A_SG_EGR_CNTX_BADDR 0x58
 
+#define S_EGRCNTXBADDR    5
+#define M_EGRCNTXBADDR    0x7ffffff
+#define V_EGRCNTXBADDR(x) ((x) << S_EGRCNTXBADDR)
+#define G_EGRCNTXBADDR(x) (((x) >> S_EGRCNTXBADDR) & M_EGRCNTXBADDR)
+
 #define A_SG_INT_CAUSE 0x5c
 
 #define S_HIRCQPARITYERROR    31
@@ -254,6 +465,30 @@
 #define V_LOPIODRBDROPERR(x) ((x) << S_LOPIODRBDROPERR)
 #define F_LOPIODRBDROPERR    V_LOPIODRBDROPERR(1U)
 
+#define S_HICRDTUNDFLOWERR    9
+#define V_HICRDTUNDFLOWERR(x) ((x) << S_HICRDTUNDFLOWERR)
+#define F_HICRDTUNDFLOWERR    V_HICRDTUNDFLOWERR(1U)
+
+#define S_LOCRDTUNDFLOWERR    8
+#define V_LOCRDTUNDFLOWERR(x) ((x) << S_LOCRDTUNDFLOWERR)
+#define F_LOCRDTUNDFLOWERR    V_LOCRDTUNDFLOWERR(1U)
+
+#define S_HIPRIORITYDBFULL    7
+#define V_HIPRIORITYDBFULL(x) ((x) << S_HIPRIORITYDBFULL)
+#define F_HIPRIORITYDBFULL    V_HIPRIORITYDBFULL(1U)
+
+#define S_HIPRIORITYDBEMPTY    6
+#define V_HIPRIORITYDBEMPTY(x) ((x) << S_HIPRIORITYDBEMPTY)
+#define F_HIPRIORITYDBEMPTY    V_HIPRIORITYDBEMPTY(1U)
+
+#define S_LOPRIORITYDBFULL    5
+#define V_LOPRIORITYDBFULL(x) ((x) << S_LOPRIORITYDBFULL)
+#define F_LOPRIORITYDBFULL    V_LOPRIORITYDBFULL(1U)
+
+#define S_LOPRIORITYDBEMPTY    4
+#define V_LOPRIORITYDBEMPTY(x) ((x) << S_LOPRIORITYDBEMPTY)
+#define F_LOPRIORITYDBEMPTY    V_LOPRIORITYDBEMPTY(1U)
+
 #define S_RSPQDISABLED    3
 #define V_RSPQDISABLED(x) ((x) << S_RSPQDISABLED)
 #define F_RSPQDISABLED    V_RSPQDISABLED(1U)
@@ -266,51 +501,77 @@
 #define V_FLEMPTY(x) ((x) << S_FLEMPTY)
 #define F_FLEMPTY    V_FLEMPTY(1U)
 
+#define S_RSPQSTARVE    0
+#define V_RSPQSTARVE(x) ((x) << S_RSPQSTARVE)
+#define F_RSPQSTARVE    V_RSPQSTARVE(1U)
+
 #define A_SG_INT_ENABLE 0x60
-
 #define A_SG_CMDQ_CREDIT_TH 0x64
 
 #define S_TIMEOUT    8
 #define M_TIMEOUT    0xffffff
 #define V_TIMEOUT(x) ((x) << S_TIMEOUT)
+#define G_TIMEOUT(x) (((x) >> S_TIMEOUT) & M_TIMEOUT)
 
 #define S_THRESHOLD    0
 #define M_THRESHOLD    0xff
 #define V_THRESHOLD(x) ((x) << S_THRESHOLD)
+#define G_THRESHOLD(x) (((x) >> S_THRESHOLD) & M_THRESHOLD)
 
 #define A_SG_TIMER_TICK 0x68
-
 #define A_SG_CQ_CONTEXT_BADDR 0x6c
 
+#define S_BASEADDR    5
+#define M_BASEADDR    0x7ffffff
+#define V_BASEADDR(x) ((x) << S_BASEADDR)
+#define G_BASEADDR(x) (((x) >> S_BASEADDR) & M_BASEADDR)
+
 #define A_SG_OCO_BASE 0x70
 
 #define S_BASE1    16
 #define M_BASE1    0xffff
 #define V_BASE1(x) ((x) << S_BASE1)
+#define G_BASE1(x) (((x) >> S_BASE1) & M_BASE1)
+
+#define S_BASE0    0
+#define M_BASE0    0xffff
+#define V_BASE0(x) ((x) << S_BASE0)
+#define G_BASE0(x) (((x) >> S_BASE0) & M_BASE0)
 
 #define A_SG_DRB_PRI_THRESH 0x74
 
+#define S_DRBPRITHRSH    0
+#define M_DRBPRITHRSH    0xffff
+#define V_DRBPRITHRSH(x) ((x) << S_DRBPRITHRSH)
+#define G_DRBPRITHRSH(x) (((x) >> S_DRBPRITHRSH) & M_DRBPRITHRSH)
+
+#define A_SG_DEBUG_INDEX 0x78
+#define A_SG_DEBUG_DATA 0x7c
+
+/* registers for module PCIX1 */
+#define PCIX1_BASE_ADDR 0x80
+
 #define A_PCIX_INT_ENABLE 0x80
 
 #define S_MSIXPARERR    22
 #define M_MSIXPARERR    0x7
-
 #define V_MSIXPARERR(x) ((x) << S_MSIXPARERR)
+#define G_MSIXPARERR(x) (((x) >> S_MSIXPARERR) & M_MSIXPARERR)
 
 #define S_CFPARERR    18
 #define M_CFPARERR    0xf
-
 #define V_CFPARERR(x) ((x) << S_CFPARERR)
+#define G_CFPARERR(x) (((x) >> S_CFPARERR) & M_CFPARERR)
 
 #define S_RFPARERR    14
 #define M_RFPARERR    0xf
-
 #define V_RFPARERR(x) ((x) << S_RFPARERR)
+#define G_RFPARERR(x) (((x) >> S_RFPARERR) & M_RFPARERR)
 
 #define S_WFPARERR    12
 #define M_WFPARERR    0x3
-
 #define V_WFPARERR(x) ((x) << S_WFPARERR)
+#define G_WFPARERR(x) (((x) >> S_WFPARERR) & M_WFPARERR)
 
 #define S_PIOPARERR    11
 #define V_PIOPARERR(x) ((x) << S_PIOPARERR)
@@ -361,7 +622,6 @@
 #define F_MSTDETPARERR    V_MSTDETPARERR(1U)
 
 #define A_PCIX_INT_CAUSE 0x84
-
 #define A_PCIX_CFG 0x88
 
 #define S_DMASTOPEN    19
@@ -372,6 +632,46 @@
 #define V_CLIDECEN(x) ((x) << S_CLIDECEN)
 #define F_CLIDECEN    V_CLIDECEN(1U)
 
+#define S_LATTMRDIS    17
+#define V_LATTMRDIS(x) ((x) << S_LATTMRDIS)
+#define F_LATTMRDIS    V_LATTMRDIS(1U)
+
+#define S_LOWPWREN    16
+#define V_LOWPWREN(x) ((x) << S_LOWPWREN)
+#define F_LOWPWREN    V_LOWPWREN(1U)
+
+#define S_ASYNCINTVEC    11
+#define M_ASYNCINTVEC    0x1f
+#define V_ASYNCINTVEC(x) ((x) << S_ASYNCINTVEC)
+#define G_ASYNCINTVEC(x) (((x) >> S_ASYNCINTVEC) & M_ASYNCINTVEC)
+
+#define S_MAXSPLTRNC    8
+#define M_MAXSPLTRNC    0x7
+#define V_MAXSPLTRNC(x) ((x) << S_MAXSPLTRNC)
+#define G_MAXSPLTRNC(x) (((x) >> S_MAXSPLTRNC) & M_MAXSPLTRNC)
+
+#define S_MAXSPLTRNR    5
+#define M_MAXSPLTRNR    0x7
+#define V_MAXSPLTRNR(x) ((x) << S_MAXSPLTRNR)
+#define G_MAXSPLTRNR(x) (((x) >> S_MAXSPLTRNR) & M_MAXSPLTRNR)
+
+#define S_MAXWRBYTECNT    3
+#define M_MAXWRBYTECNT    0x3
+#define V_MAXWRBYTECNT(x) ((x) << S_MAXWRBYTECNT)
+#define G_MAXWRBYTECNT(x) (((x) >> S_MAXWRBYTECNT) & M_MAXWRBYTECNT)
+
+#define S_WRREQATOMICEN    2
+#define V_WRREQATOMICEN(x) ((x) << S_WRREQATOMICEN)
+#define F_WRREQATOMICEN    V_WRREQATOMICEN(1U)
+
+#define S_RSTWRMMODE    1
+#define V_RSTWRMMODE(x) ((x) << S_RSTWRMMODE)
+#define F_RSTWRMMODE    V_RSTWRMMODE(1U)
+
+#define S_PIOACK64EN    0
+#define V_PIOACK64EN(x) ((x) << S_PIOACK64EN)
+#define F_PIOACK64EN    V_PIOACK64EN(1U)
+
 #define A_PCIX_MODE 0x8c
 
 #define S_PCLKRANGE    6
@@ -384,16 +684,241 @@
 #define V_PCIXINITPAT(x) ((x) << S_PCIXINITPAT)
 #define G_PCIXINITPAT(x) (((x) >> S_PCIXINITPAT) & M_PCIXINITPAT)
 
+#define S_66MHZ    1
+#define V_66MHZ(x) ((x) << S_66MHZ)
+#define F_66MHZ    V_66MHZ(1U)
+
 #define S_64BIT    0
 #define V_64BIT(x) ((x) << S_64BIT)
 #define F_64BIT    V_64BIT(1U)
 
+#define A_PCIX_CAL 0x90
+
+#define S_BUSY    31
+#define V_BUSY(x) ((x) << S_BUSY)
+#define F_BUSY    V_BUSY(1U)
+
+#define S_PERCALDIV    22
+#define M_PERCALDIV    0xff
+#define V_PERCALDIV(x) ((x) << S_PERCALDIV)
+#define G_PERCALDIV(x) (((x) >> S_PERCALDIV) & M_PERCALDIV)
+
+#define S_PERCALEN    21
+#define V_PERCALEN(x) ((x) << S_PERCALEN)
+#define F_PERCALEN    V_PERCALEN(1U)
+
+#define S_SGLCALEN    20
+#define V_SGLCALEN(x) ((x) << S_SGLCALEN)
+#define F_SGLCALEN    V_SGLCALEN(1U)
+
+#define S_ZINUPDMODE    19
+#define V_ZINUPDMODE(x) ((x) << S_ZINUPDMODE)
+#define F_ZINUPDMODE    V_ZINUPDMODE(1U)
+
+#define S_ZINSEL    18
+#define V_ZINSEL(x) ((x) << S_ZINSEL)
+#define F_ZINSEL    V_ZINSEL(1U)
+
+#define S_ZPDMAN    15
+#define M_ZPDMAN    0x7
+#define V_ZPDMAN(x) ((x) << S_ZPDMAN)
+#define G_ZPDMAN(x) (((x) >> S_ZPDMAN) & M_ZPDMAN)
+
+#define S_ZPUMAN    12
+#define M_ZPUMAN    0x7
+#define V_ZPUMAN(x) ((x) << S_ZPUMAN)
+#define G_ZPUMAN(x) (((x) >> S_ZPUMAN) & M_ZPUMAN)
+
+#define S_ZPDOUT    9
+#define M_ZPDOUT    0x7
+#define V_ZPDOUT(x) ((x) << S_ZPDOUT)
+#define G_ZPDOUT(x) (((x) >> S_ZPDOUT) & M_ZPDOUT)
+
+#define S_ZPUOUT    6
+#define M_ZPUOUT    0x7
+#define V_ZPUOUT(x) ((x) << S_ZPUOUT)
+#define G_ZPUOUT(x) (((x) >> S_ZPUOUT) & M_ZPUOUT)
+
+#define S_ZPDIN    3
+#define M_ZPDIN    0x7
+#define V_ZPDIN(x) ((x) << S_ZPDIN)
+#define G_ZPDIN(x) (((x) >> S_ZPDIN) & M_ZPDIN)
+
+#define S_ZPUIN    0
+#define M_ZPUIN    0x7
+#define V_ZPUIN(x) ((x) << S_ZPUIN)
+#define G_ZPUIN(x) (((x) >> S_ZPUIN) & M_ZPUIN)
+
+#define A_PCIX_WOL 0x94
+
+#define S_WAKEUP1    3
+#define V_WAKEUP1(x) ((x) << S_WAKEUP1)
+#define F_WAKEUP1    V_WAKEUP1(1U)
+
+#define S_WAKEUP0    2
+#define V_WAKEUP0(x) ((x) << S_WAKEUP0)
+#define F_WAKEUP0    V_WAKEUP0(1U)
+
+#define S_SLEEPMODE1    1
+#define V_SLEEPMODE1(x) ((x) << S_SLEEPMODE1)
+#define F_SLEEPMODE1    V_SLEEPMODE1(1U)
+
+#define S_SLEEPMODE0    0
+#define V_SLEEPMODE0(x) ((x) << S_SLEEPMODE0)
+#define F_SLEEPMODE0    V_SLEEPMODE0(1U)
+
+#define A_PCIX_STAT0 0x98
+
+#define S_PIOREQFIFOLEVEL    26
+#define M_PIOREQFIFOLEVEL    0x3f
+#define V_PIOREQFIFOLEVEL(x) ((x) << S_PIOREQFIFOLEVEL)
+#define G_PIOREQFIFOLEVEL(x) (((x) >> S_PIOREQFIFOLEVEL) & M_PIOREQFIFOLEVEL)
+
+#define S_RFINIST    24
+#define M_RFINIST    0x3
+#define V_RFINIST(x) ((x) << S_RFINIST)
+#define G_RFINIST(x) (((x) >> S_RFINIST) & M_RFINIST)
+
+#define S_RFRESPRDST    22
+#define M_RFRESPRDST    0x3
+#define V_RFRESPRDST(x) ((x) << S_RFRESPRDST)
+#define G_RFRESPRDST(x) (((x) >> S_RFRESPRDST) & M_RFRESPRDST)
+
+#define S_TARCST    19
+#define M_TARCST    0x7
+#define V_TARCST(x) ((x) << S_TARCST)
+#define G_TARCST(x) (((x) >> S_TARCST) & M_TARCST)
+
+#define S_TARXST    16
+#define M_TARXST    0x7
+#define V_TARXST(x) ((x) << S_TARXST)
+#define G_TARXST(x) (((x) >> S_TARXST) & M_TARXST)
+
+#define S_WFREQWRST    13
+#define M_WFREQWRST    0x7
+#define V_WFREQWRST(x) ((x) << S_WFREQWRST)
+#define G_WFREQWRST(x) (((x) >> S_WFREQWRST) & M_WFREQWRST)
+
+#define S_WFRESPFIFOEMPTY    12
+#define V_WFRESPFIFOEMPTY(x) ((x) << S_WFRESPFIFOEMPTY)
+#define F_WFRESPFIFOEMPTY    V_WFRESPFIFOEMPTY(1U)
+
+#define S_WFREQFIFOEMPTY    11
+#define V_WFREQFIFOEMPTY(x) ((x) << S_WFREQFIFOEMPTY)
+#define F_WFREQFIFOEMPTY    V_WFREQFIFOEMPTY(1U)
+
+#define S_RFRESPFIFOEMPTY    10
+#define V_RFRESPFIFOEMPTY(x) ((x) << S_RFRESPFIFOEMPTY)
+#define F_RFRESPFIFOEMPTY    V_RFRESPFIFOEMPTY(1U)
+
+#define S_RFREQFIFOEMPTY    9
+#define V_RFREQFIFOEMPTY(x) ((x) << S_RFREQFIFOEMPTY)
+#define F_RFREQFIFOEMPTY    V_RFREQFIFOEMPTY(1U)
+
+#define S_PIORESPFIFOLEVEL    7
+#define M_PIORESPFIFOLEVEL    0x3
+#define V_PIORESPFIFOLEVEL(x) ((x) << S_PIORESPFIFOLEVEL)
+#define G_PIORESPFIFOLEVEL(x) (((x) >> S_PIORESPFIFOLEVEL) & M_PIORESPFIFOLEVEL)
+
+#define S_CFRESPFIFOEMPTY    6
+#define V_CFRESPFIFOEMPTY(x) ((x) << S_CFRESPFIFOEMPTY)
+#define F_CFRESPFIFOEMPTY    V_CFRESPFIFOEMPTY(1U)
+
+#define S_CFREQFIFOEMPTY    5
+#define V_CFREQFIFOEMPTY(x) ((x) << S_CFREQFIFOEMPTY)
+#define F_CFREQFIFOEMPTY    V_CFREQFIFOEMPTY(1U)
+
+#define S_VPDRESPFIFOEMPTY    4
+#define V_VPDRESPFIFOEMPTY(x) ((x) << S_VPDRESPFIFOEMPTY)
+#define F_VPDRESPFIFOEMPTY    V_VPDRESPFIFOEMPTY(1U)
+
+#define S_VPDREQFIFOEMPTY    3
+#define V_VPDREQFIFOEMPTY(x) ((x) << S_VPDREQFIFOEMPTY)
+#define F_VPDREQFIFOEMPTY    V_VPDREQFIFOEMPTY(1U)
+
+#define S_PIO_RSPPND    2
+#define V_PIO_RSPPND(x) ((x) << S_PIO_RSPPND)
+#define F_PIO_RSPPND    V_PIO_RSPPND(1U)
+
+#define S_DLYTRNPND    1
+#define V_DLYTRNPND(x) ((x) << S_DLYTRNPND)
+#define F_DLYTRNPND    V_DLYTRNPND(1U)
+
+#define S_SPLTRNPND    0
+#define V_SPLTRNPND(x) ((x) << S_SPLTRNPND)
+#define F_SPLTRNPND    V_SPLTRNPND(1U)
+
+#define A_PCIX_STAT1 0x9c
+
+#define S_WFINIST    26
+#define M_WFINIST    0xf
+#define V_WFINIST(x) ((x) << S_WFINIST)
+#define G_WFINIST(x) (((x) >> S_WFINIST) & M_WFINIST)
+
+#define S_ARBST    23
+#define M_ARBST    0x7
+#define V_ARBST(x) ((x) << S_ARBST)
+#define G_ARBST(x) (((x) >> S_ARBST) & M_ARBST)
+
+#define S_PMIST    21
+#define M_PMIST    0x3
+#define V_PMIST(x) ((x) << S_PMIST)
+#define G_PMIST(x) (((x) >> S_PMIST) & M_PMIST)
+
+#define S_CALST    19
+#define M_CALST    0x3
+#define V_CALST(x) ((x) << S_CALST)
+#define G_CALST(x) (((x) >> S_CALST) & M_CALST)
+
+#define S_CFREQRDST    17
+#define M_CFREQRDST    0x3
+#define V_CFREQRDST(x) ((x) << S_CFREQRDST)
+#define G_CFREQRDST(x) (((x) >> S_CFREQRDST) & M_CFREQRDST)
+
+#define S_CFINIST    15
+#define M_CFINIST    0x3
+#define V_CFINIST(x) ((x) << S_CFINIST)
+#define G_CFINIST(x) (((x) >> S_CFINIST) & M_CFINIST)
+
+#define S_CFRESPRDST    13
+#define M_CFRESPRDST    0x3
+#define V_CFRESPRDST(x) ((x) << S_CFRESPRDST)
+#define G_CFRESPRDST(x) (((x) >> S_CFRESPRDST) & M_CFRESPRDST)
+
+#define S_INICST    10
+#define M_INICST    0x7
+#define V_INICST(x) ((x) << S_INICST)
+#define G_INICST(x) (((x) >> S_INICST) & M_INICST)
+
+#define S_INIXST    7
+#define M_INIXST    0x7
+#define V_INIXST(x) ((x) << S_INIXST)
+#define G_INIXST(x) (((x) >> S_INIXST) & M_INIXST)
+
+#define S_INTST    4
+#define M_INTST    0x7
+#define V_INTST(x) ((x) << S_INTST)
+#define G_INTST(x) (((x) >> S_INTST) & M_INTST)
+
+#define S_PIOST    2
+#define M_PIOST    0x3
+#define V_PIOST(x) ((x) << S_PIOST)
+#define G_PIOST(x) (((x) >> S_PIOST) & M_PIOST)
+
+#define S_RFREQRDST    0
+#define M_RFREQRDST    0x3
+#define V_RFREQRDST(x) ((x) << S_RFREQRDST)
+#define G_RFREQRDST(x) (((x) >> S_RFREQRDST) & M_RFREQRDST)
+
+/* registers for module PCIE0 */
+#define PCIE0_BASE_ADDR 0x80
+
 #define A_PCIE_INT_ENABLE 0x80
 
-#define S_BISTERR    15
+#define S_BISTERR    19
 #define M_BISTERR    0xff
-
 #define V_BISTERR(x) ((x) << S_BISTERR)
+#define G_BISTERR(x) (((x) >> S_BISTERR) & M_BISTERR)
 
 #define S_TXPARERR    18
 #define V_TXPARERR(x) ((x) << S_TXPARERR)
@@ -413,8 +938,8 @@
 
 #define S_PCIE_MSIXPARERR    12
 #define M_PCIE_MSIXPARERR    0x7
-
 #define V_PCIE_MSIXPARERR(x) ((x) << S_PCIE_MSIXPARERR)
+#define G_PCIE_MSIXPARERR(x) (((x) >> S_PCIE_MSIXPARERR) & M_PCIE_MSIXPARERR)
 
 #define S_PCIE_CFPARERR    11
 #define V_PCIE_CFPARERR(x) ((x) << S_PCIE_CFPARERR)
@@ -440,17 +965,44 @@
 #define V_UNXSPLCPLERRR(x) ((x) << S_UNXSPLCPLERRR)
 #define F_UNXSPLCPLERRR    V_UNXSPLCPLERRR(1U)
 
+#define S_VPDADDRCHNG    5
+#define V_VPDADDRCHNG(x) ((x) << S_VPDADDRCHNG)
+#define F_VPDADDRCHNG    V_VPDADDRCHNG(1U)
+
+#define S_BUSMSTREN    4
+#define V_BUSMSTREN(x) ((x) << S_BUSMSTREN)
+#define F_BUSMSTREN    V_BUSMSTREN(1U)
+
+#define S_PMSTCHNG    3
+#define V_PMSTCHNG(x) ((x) << S_PMSTCHNG)
+#define F_PMSTCHNG    V_PMSTCHNG(1U)
+
+#define S_PEXMSG    2
+#define V_PEXMSG(x) ((x) << S_PEXMSG)
+#define F_PEXMSG    V_PEXMSG(1U)
+
+#define S_ZEROLENRD    1
+#define V_ZEROLENRD(x) ((x) << S_ZEROLENRD)
+#define F_ZEROLENRD    V_ZEROLENRD(1U)
+
 #define S_PEXERR    0
 #define V_PEXERR(x) ((x) << S_PEXERR)
 #define F_PEXERR    V_PEXERR(1U)
 
 #define A_PCIE_INT_CAUSE 0x84
+#define A_PCIE_CFG 0x88
 
 #define S_PCIE_DMASTOPEN    24
 #define V_PCIE_DMASTOPEN(x) ((x) << S_PCIE_DMASTOPEN)
 #define F_PCIE_DMASTOPEN    V_PCIE_DMASTOPEN(1U)
 
-#define A_PCIE_CFG 0x88
+#define S_PRIORITYINTA    23
+#define V_PRIORITYINTA(x) ((x) << S_PRIORITYINTA)
+#define F_PRIORITYINTA    V_PRIORITYINTA(1U)
+
+#define S_INIFULLPKT    22
+#define V_INIFULLPKT(x) ((x) << S_INIFULLPKT)
+#define F_INIFULLPKT    V_INIFULLPKT(1U)
 
 #define S_ENABLELINKDWNDRST    21
 #define V_ENABLELINKDWNDRST(x) ((x) << S_ENABLELINKDWNDRST)
@@ -460,23 +1012,213 @@
 #define V_ENABLELINKDOWNRST(x) ((x) << S_ENABLELINKDOWNRST)
 #define F_ENABLELINKDOWNRST    V_ENABLELINKDOWNRST(1U)
 
+#define S_ENABLEHOTRST    19
+#define V_ENABLEHOTRST(x) ((x) << S_ENABLEHOTRST)
+#define F_ENABLEHOTRST    V_ENABLEHOTRST(1U)
+
+#define S_INIWAITFORGNT    18
+#define V_INIWAITFORGNT(x) ((x) << S_INIWAITFORGNT)
+#define F_INIWAITFORGNT    V_INIWAITFORGNT(1U)
+
+#define S_INIBEDIS    17
+#define V_INIBEDIS(x) ((x) << S_INIBEDIS)
+#define F_INIBEDIS    V_INIBEDIS(1U)
+
 #define S_PCIE_CLIDECEN    16
 #define V_PCIE_CLIDECEN(x) ((x) << S_PCIE_CLIDECEN)
 #define F_PCIE_CLIDECEN    V_PCIE_CLIDECEN(1U)
 
+#define S_PCIE_MAXSPLTRNC    7
+#define M_PCIE_MAXSPLTRNC    0xf
+#define V_PCIE_MAXSPLTRNC(x) ((x) << S_PCIE_MAXSPLTRNC)
+#define G_PCIE_MAXSPLTRNC(x) (((x) >> S_PCIE_MAXSPLTRNC) & M_PCIE_MAXSPLTRNC)
+
+#define S_PCIE_MAXSPLTRNR    1
+#define M_PCIE_MAXSPLTRNR    0x3f
+#define V_PCIE_MAXSPLTRNR(x) ((x) << S_PCIE_MAXSPLTRNR)
+#define G_PCIE_MAXSPLTRNR(x) (((x) >> S_PCIE_MAXSPLTRNR) & M_PCIE_MAXSPLTRNR)
+
 #define S_CRSTWRMMODE    0
 #define V_CRSTWRMMODE(x) ((x) << S_CRSTWRMMODE)
 #define F_CRSTWRMMODE    V_CRSTWRMMODE(1U)
 
 #define A_PCIE_MODE 0x8c
 
+#define S_TAR_STATE    29
+#define M_TAR_STATE    0x7
+#define V_TAR_STATE(x) ((x) << S_TAR_STATE)
+#define G_TAR_STATE(x) (((x) >> S_TAR_STATE) & M_TAR_STATE)
+
+#define S_RF_STATEINI    26
+#define M_RF_STATEINI    0x7
+#define V_RF_STATEINI(x) ((x) << S_RF_STATEINI)
+#define G_RF_STATEINI(x) (((x) >> S_RF_STATEINI) & M_RF_STATEINI)
+
+#define S_CF_STATEINI    23
+#define M_CF_STATEINI    0x7
+#define V_CF_STATEINI(x) ((x) << S_CF_STATEINI)
+#define G_CF_STATEINI(x) (((x) >> S_CF_STATEINI) & M_CF_STATEINI)
+
+#define S_PIO_STATEPL    20
+#define M_PIO_STATEPL    0x7
+#define V_PIO_STATEPL(x) ((x) << S_PIO_STATEPL)
+#define G_PIO_STATEPL(x) (((x) >> S_PIO_STATEPL) & M_PIO_STATEPL)
+
+#define S_PIO_STATEISC    18
+#define M_PIO_STATEISC    0x3
+#define V_PIO_STATEISC(x) ((x) << S_PIO_STATEISC)
+#define G_PIO_STATEISC(x) (((x) >> S_PIO_STATEISC) & M_PIO_STATEISC)
+
 #define S_NUMFSTTRNSEQRX    10
 #define M_NUMFSTTRNSEQRX    0xff
 #define V_NUMFSTTRNSEQRX(x) ((x) << S_NUMFSTTRNSEQRX)
 #define G_NUMFSTTRNSEQRX(x) (((x) >> S_NUMFSTTRNSEQRX) & M_NUMFSTTRNSEQRX)
 
+#define S_LNKCNTLSTATE    2
+#define M_LNKCNTLSTATE    0xff
+#define V_LNKCNTLSTATE(x) ((x) << S_LNKCNTLSTATE)
+#define G_LNKCNTLSTATE(x) (((x) >> S_LNKCNTLSTATE) & M_LNKCNTLSTATE)
+
+#define S_VC0UP    1
+#define V_VC0UP(x) ((x) << S_VC0UP)
+#define F_VC0UP    V_VC0UP(1U)
+
+#define S_LNKINITIAL    0
+#define V_LNKINITIAL(x) ((x) << S_LNKINITIAL)
+#define F_LNKINITIAL    V_LNKINITIAL(1U)
+
+#define A_PCIE_STAT 0x90
+
+#define S_INI_STATE    28
+#define M_INI_STATE    0xf
+#define V_INI_STATE(x) ((x) << S_INI_STATE)
+#define G_INI_STATE(x) (((x) >> S_INI_STATE) & M_INI_STATE)
+
+#define S_WF_STATEINI    24
+#define M_WF_STATEINI    0xf
+#define V_WF_STATEINI(x) ((x) << S_WF_STATEINI)
+#define G_WF_STATEINI(x) (((x) >> S_WF_STATEINI) & M_WF_STATEINI)
+
+#define S_PLM_REQFIFOCNT    22
+#define M_PLM_REQFIFOCNT    0x3
+#define V_PLM_REQFIFOCNT(x) ((x) << S_PLM_REQFIFOCNT)
+#define G_PLM_REQFIFOCNT(x) (((x) >> S_PLM_REQFIFOCNT) & M_PLM_REQFIFOCNT)
+
+#define S_ER_REQFIFOEMPTY    21
+#define V_ER_REQFIFOEMPTY(x) ((x) << S_ER_REQFIFOEMPTY)
+#define F_ER_REQFIFOEMPTY    V_ER_REQFIFOEMPTY(1U)
+
+#define S_WF_RSPFIFOEMPTY    20
+#define V_WF_RSPFIFOEMPTY(x) ((x) << S_WF_RSPFIFOEMPTY)
+#define F_WF_RSPFIFOEMPTY    V_WF_RSPFIFOEMPTY(1U)
+
+#define S_WF_REQFIFOEMPTY    19
+#define V_WF_REQFIFOEMPTY(x) ((x) << S_WF_REQFIFOEMPTY)
+#define F_WF_REQFIFOEMPTY    V_WF_REQFIFOEMPTY(1U)
+
+#define S_RF_RSPFIFOEMPTY    18
+#define V_RF_RSPFIFOEMPTY(x) ((x) << S_RF_RSPFIFOEMPTY)
+#define F_RF_RSPFIFOEMPTY    V_RF_RSPFIFOEMPTY(1U)
+
+#define S_RF_REQFIFOEMPTY    17
+#define V_RF_REQFIFOEMPTY(x) ((x) << S_RF_REQFIFOEMPTY)
+#define F_RF_REQFIFOEMPTY    V_RF_REQFIFOEMPTY(1U)
+
+#define S_RF_ACTEMPTY    16
+#define V_RF_ACTEMPTY(x) ((x) << S_RF_ACTEMPTY)
+#define F_RF_ACTEMPTY    V_RF_ACTEMPTY(1U)
+
+#define S_PIO_RSPFIFOCNT    11
+#define M_PIO_RSPFIFOCNT    0x1f
+#define V_PIO_RSPFIFOCNT(x) ((x) << S_PIO_RSPFIFOCNT)
+#define G_PIO_RSPFIFOCNT(x) (((x) >> S_PIO_RSPFIFOCNT) & M_PIO_RSPFIFOCNT)
+
+#define S_PIO_REQFIFOCNT    5
+#define M_PIO_REQFIFOCNT    0x3f
+#define V_PIO_REQFIFOCNT(x) ((x) << S_PIO_REQFIFOCNT)
+#define G_PIO_REQFIFOCNT(x) (((x) >> S_PIO_REQFIFOCNT) & M_PIO_REQFIFOCNT)
+
+#define S_CF_RSPFIFOEMPTY    4
+#define V_CF_RSPFIFOEMPTY(x) ((x) << S_CF_RSPFIFOEMPTY)
+#define F_CF_RSPFIFOEMPTY    V_CF_RSPFIFOEMPTY(1U)
+
+#define S_CF_REQFIFOEMPTY    3
+#define V_CF_REQFIFOEMPTY(x) ((x) << S_CF_REQFIFOEMPTY)
+#define F_CF_REQFIFOEMPTY    V_CF_REQFIFOEMPTY(1U)
+
+#define S_CF_ACTEMPTY    2
+#define V_CF_ACTEMPTY(x) ((x) << S_CF_ACTEMPTY)
+#define F_CF_ACTEMPTY    V_CF_ACTEMPTY(1U)
+
+#define S_VPD_RSPFIFOEMPTY    1
+#define V_VPD_RSPFIFOEMPTY(x) ((x) << S_VPD_RSPFIFOEMPTY)
+#define F_VPD_RSPFIFOEMPTY    V_VPD_RSPFIFOEMPTY(1U)
+
+#define S_VPD_REQFIFOEMPTY    0
+#define V_VPD_REQFIFOEMPTY(x) ((x) << S_VPD_REQFIFOEMPTY)
+#define F_VPD_REQFIFOEMPTY    V_VPD_REQFIFOEMPTY(1U)
+
+#define A_PCIE_CAL 0x90
+
+#define S_CALBUSY    31
+#define V_CALBUSY(x) ((x) << S_CALBUSY)
+#define F_CALBUSY    V_CALBUSY(1U)
+
+#define S_CALFAULT    30
+#define V_CALFAULT(x) ((x) << S_CALFAULT)
+#define F_CALFAULT    V_CALFAULT(1U)
+
+#define S_PCIE_ZINSEL    11
+#define V_PCIE_ZINSEL(x) ((x) << S_PCIE_ZINSEL)
+#define F_PCIE_ZINSEL    V_PCIE_ZINSEL(1U)
+
+#define S_ZMAN    8
+#define M_ZMAN    0x7
+#define V_ZMAN(x) ((x) << S_ZMAN)
+#define G_ZMAN(x) (((x) >> S_ZMAN) & M_ZMAN)
+
+#define S_ZOUT    3
+#define M_ZOUT    0x1f
+#define V_ZOUT(x) ((x) << S_ZOUT)
+#define G_ZOUT(x) (((x) >> S_ZOUT) & M_ZOUT)
+
+#define S_ZIN    0
+#define M_ZIN    0x7
+#define V_ZIN(x) ((x) << S_ZIN)
+#define G_ZIN(x) (((x) >> S_ZIN) & M_ZIN)
+
+#define A_PCIE_WOL 0x94
+
+#define S_CF_RSPSTATE    12
+#define M_CF_RSPSTATE    0x3
+#define V_CF_RSPSTATE(x) ((x) << S_CF_RSPSTATE)
+#define G_CF_RSPSTATE(x) (((x) >> S_CF_RSPSTATE) & M_CF_RSPSTATE)
+
+#define S_RF_RSPSTATE    10
+#define M_RF_RSPSTATE    0x3
+#define V_RF_RSPSTATE(x) ((x) << S_RF_RSPSTATE)
+#define G_RF_RSPSTATE(x) (((x) >> S_RF_RSPSTATE) & M_RF_RSPSTATE)
+
+#define S_PME_STATE    7
+#define M_PME_STATE    0x7
+#define V_PME_STATE(x) ((x) << S_PME_STATE)
+#define G_PME_STATE(x) (((x) >> S_PME_STATE) & M_PME_STATE)
+
+#define S_INT_STATE    4
+#define M_INT_STATE    0x7
+#define V_INT_STATE(x) ((x) << S_INT_STATE)
+#define G_INT_STATE(x) (((x) >> S_INT_STATE) & M_INT_STATE)
+
 #define A_PCIE_PEX_CTRL0 0x98
 
+#define S_CPLTIMEOUTRETRY    31
+#define V_CPLTIMEOUTRETRY(x) ((x) << S_CPLTIMEOUTRETRY)
+#define F_CPLTIMEOUTRETRY    V_CPLTIMEOUTRETRY(1U)
+
+#define S_STRICTTSMN    30
+#define V_STRICTTSMN(x) ((x) << S_STRICTTSMN)
+#define F_STRICTTSMN    V_STRICTTSMN(1U)
+
 #define S_NUMFSTTRNSEQ    22
 #define M_NUMFSTTRNSEQ    0xff
 #define V_NUMFSTTRNSEQ(x) ((x) << S_NUMFSTTRNSEQ)
@@ -484,23 +1226,854 @@
 
 #define S_REPLAYLMT    2
 #define M_REPLAYLMT    0xfffff
-
 #define V_REPLAYLMT(x) ((x) << S_REPLAYLMT)
+#define G_REPLAYLMT(x) (((x) >> S_REPLAYLMT) & M_REPLAYLMT)
+
+#define S_TXPNDCHKEN    1
+#define V_TXPNDCHKEN(x) ((x) << S_TXPNDCHKEN)
+#define F_TXPNDCHKEN    V_TXPNDCHKEN(1U)
+
+#define S_CPLPNDCHKEN    0
+#define V_CPLPNDCHKEN(x) ((x) << S_CPLPNDCHKEN)
+#define F_CPLPNDCHKEN    V_CPLPNDCHKEN(1U)
 
 #define A_PCIE_PEX_CTRL1 0x9c
 
+#define S_RXPHYERREN    31
+#define V_RXPHYERREN(x) ((x) << S_RXPHYERREN)
+#define F_RXPHYERREN    V_RXPHYERREN(1U)
+
+#define S_DLLPTIMEOUTLMT    13
+#define M_DLLPTIMEOUTLMT    0x3ffff
+#define V_DLLPTIMEOUTLMT(x) ((x) << S_DLLPTIMEOUTLMT)
+#define G_DLLPTIMEOUTLMT(x) (((x) >> S_DLLPTIMEOUTLMT) & M_DLLPTIMEOUTLMT)
+
+#define S_ACKLAT    0
+#define M_ACKLAT    0x1fff
+#define V_ACKLAT(x) ((x) << S_ACKLAT)
+#define G_ACKLAT(x) (((x) >> S_ACKLAT) & M_ACKLAT)
+
+#define S_T3A_DLLPTIMEOUTLMT    11
+#define M_T3A_DLLPTIMEOUTLMT    0xfffff
+#define V_T3A_DLLPTIMEOUTLMT(x) ((x) << S_T3A_DLLPTIMEOUTLMT)
+#define G_T3A_DLLPTIMEOUTLMT(x) (((x) >> S_T3A_DLLPTIMEOUTLMT) & M_T3A_DLLPTIMEOUTLMT)
+
 #define S_T3A_ACKLAT    0
 #define M_T3A_ACKLAT    0x7ff
-
 #define V_T3A_ACKLAT(x) ((x) << S_T3A_ACKLAT)
-
-#define S_ACKLAT    0
-#define M_ACKLAT    0x1fff
-
-#define V_ACKLAT(x) ((x) << S_ACKLAT)
+#define G_T3A_ACKLAT(x) (((x) >> S_T3A_ACKLAT) & M_T3A_ACKLAT)
+
+#define A_PCIE_PEX_CTRL2 0xa0
+
+#define S_LNKCNTLDETDIR    30
+#define V_LNKCNTLDETDIR(x) ((x) << S_LNKCNTLDETDIR)
+#define F_LNKCNTLDETDIR    V_LNKCNTLDETDIR(1U)
+
+#define S_ENTERL1REN    29
+#define V_ENTERL1REN(x) ((x) << S_ENTERL1REN)
+#define F_ENTERL1REN    V_ENTERL1REN(1U)
+
+#define S_PMEXITL1REQ    28
+#define V_PMEXITL1REQ(x) ((x) << S_PMEXITL1REQ)
+#define F_PMEXITL1REQ    V_PMEXITL1REQ(1U)
+
+#define S_PMTXIDLE    27
+#define V_PMTXIDLE(x) ((x) << S_PMTXIDLE)
+#define F_PMTXIDLE    V_PMTXIDLE(1U)
+
+#define S_PCIMODELOOP    26
+#define V_PCIMODELOOP(x) ((x) << S_PCIMODELOOP)
+#define F_PCIMODELOOP    V_PCIMODELOOP(1U)
+
+#define S_L1ASPMTXRXL0STIME    14
+#define M_L1ASPMTXRXL0STIME    0xfff
+#define V_L1ASPMTXRXL0STIME(x) ((x) << S_L1ASPMTXRXL0STIME)
+#define G_L1ASPMTXRXL0STIME(x) (((x) >> S_L1ASPMTXRXL0STIME) & M_L1ASPMTXRXL0STIME)
+
+#define S_L0SIDLETIME    3
+#define M_L0SIDLETIME    0x7ff
+#define V_L0SIDLETIME(x) ((x) << S_L0SIDLETIME)
+#define G_L0SIDLETIME(x) (((x) >> S_L0SIDLETIME) & M_L0SIDLETIME)
+
+#define S_ENTERL1ASPMEN    2
+#define V_ENTERL1ASPMEN(x) ((x) << S_ENTERL1ASPMEN)
+#define F_ENTERL1ASPMEN    V_ENTERL1ASPMEN(1U)
+
+#define S_ENTERL1EN    1
+#define V_ENTERL1EN(x) ((x) << S_ENTERL1EN)
+#define F_ENTERL1EN    V_ENTERL1EN(1U)
+
+#define S_ENTERL0SEN    0
+#define V_ENTERL0SEN(x) ((x) << S_ENTERL0SEN)
+#define F_ENTERL0SEN    V_ENTERL0SEN(1U)
+
+#define S_ENTERL23    3
+#define V_ENTERL23(x) ((x) << S_ENTERL23)
+#define F_ENTERL23    V_ENTERL23(1U)
 
 #define A_PCIE_PEX_ERR 0xa4
 
+#define S_CPLTIMEOUTID    18
+#define M_CPLTIMEOUTID    0x7f
+#define V_CPLTIMEOUTID(x) ((x) << S_CPLTIMEOUTID)
+#define G_CPLTIMEOUTID(x) (((x) >> S_CPLTIMEOUTID) & M_CPLTIMEOUTID)
+
+#define S_FLOWCTLOFLOWERR    17
+#define V_FLOWCTLOFLOWERR(x) ((x) << S_FLOWCTLOFLOWERR)
+#define F_FLOWCTLOFLOWERR    V_FLOWCTLOFLOWERR(1U)
+
+#define S_REPLAYTIMEOUT    16
+#define V_REPLAYTIMEOUT(x) ((x) << S_REPLAYTIMEOUT)
+#define F_REPLAYTIMEOUT    V_REPLAYTIMEOUT(1U)
+
+#define S_REPLAYROLLOVER    15
+#define V_REPLAYROLLOVER(x) ((x) << S_REPLAYROLLOVER)
+#define F_REPLAYROLLOVER    V_REPLAYROLLOVER(1U)
+
+#define S_BADDLLP    14
+#define V_BADDLLP(x) ((x) << S_BADDLLP)
+#define F_BADDLLP    V_BADDLLP(1U)
+
+#define S_DLLPERR    13
+#define V_DLLPERR(x) ((x) << S_DLLPERR)
+#define F_DLLPERR    V_DLLPERR(1U)
+
+#define S_FLOWCTLPROTERR    12
+#define V_FLOWCTLPROTERR(x) ((x) << S_FLOWCTLPROTERR)
+#define F_FLOWCTLPROTERR    V_FLOWCTLPROTERR(1U)
+
+#define S_CPLTIMEOUT    11
+#define V_CPLTIMEOUT(x) ((x) << S_CPLTIMEOUT)
+#define F_CPLTIMEOUT    V_CPLTIMEOUT(1U)
+
+#define S_PHYRCVERR    10
+#define V_PHYRCVERR(x) ((x) << S_PHYRCVERR)
+#define F_PHYRCVERR    V_PHYRCVERR(1U)
+
+#define S_DISTLP    9
+#define V_DISTLP(x) ((x) << S_DISTLP)
+#define F_DISTLP    V_DISTLP(1U)
+
+#define S_BADECRC    8
+#define V_BADECRC(x) ((x) << S_BADECRC)
+#define F_BADECRC    V_BADECRC(1U)
+
+#define S_BADTLP    7
+#define V_BADTLP(x) ((x) << S_BADTLP)
+#define F_BADTLP    V_BADTLP(1U)
+
+#define S_MALTLP    6
+#define V_MALTLP(x) ((x) << S_MALTLP)
+#define F_MALTLP    V_MALTLP(1U)
+
+#define S_UNXCPL    5
+#define V_UNXCPL(x) ((x) << S_UNXCPL)
+#define F_UNXCPL    V_UNXCPL(1U)
+
+#define S_UNSREQ    4
+#define V_UNSREQ(x) ((x) << S_UNSREQ)
+#define F_UNSREQ    V_UNSREQ(1U)
+
+#define S_PSNREQ    3
+#define V_PSNREQ(x) ((x) << S_PSNREQ)
+#define F_PSNREQ    V_PSNREQ(1U)
+
+#define S_UNSCPL    2
+#define V_UNSCPL(x) ((x) << S_UNSCPL)
+#define F_UNSCPL    V_UNSCPL(1U)
+
+#define S_CPLABT    1
+#define V_CPLABT(x) ((x) << S_CPLABT)
+#define F_CPLABT    V_CPLABT(1U)
+
+#define S_PSNCPL    0
+#define V_PSNCPL(x) ((x) << S_PSNCPL)
+#define F_PSNCPL    V_PSNCPL(1U)
+
+#define A_PCIE_SERDES_CTRL 0xa8
+
+#define S_PMASEL    3
+#define V_PMASEL(x) ((x) << S_PMASEL)
+#define F_PMASEL    V_PMASEL(1U)
+
+#define S_LANE    0
+#define M_LANE    0x7
+#define V_LANE(x) ((x) << S_LANE)
+#define G_LANE(x) (((x) >> S_LANE) & M_LANE)
+
+#define A_PCIE_PIPE_CTRL 0xa8
+
+#define S_RECDETUSEC    19
+#define M_RECDETUSEC    0x7
+#define V_RECDETUSEC(x) ((x) << S_RECDETUSEC)
+#define G_RECDETUSEC(x) (((x) >> S_RECDETUSEC) & M_RECDETUSEC)
+
+#define S_PLLLCKCYC    6
+#define M_PLLLCKCYC    0x1fff
+#define V_PLLLCKCYC(x) ((x) << S_PLLLCKCYC)
+#define G_PLLLCKCYC(x) (((x) >> S_PLLLCKCYC) & M_PLLLCKCYC)
+
+#define S_ELECIDLEDETCYC    3
+#define M_ELECIDLEDETCYC    0x7
+#define V_ELECIDLEDETCYC(x) ((x) << S_ELECIDLEDETCYC)
+#define G_ELECIDLEDETCYC(x) (((x) >> S_ELECIDLEDETCYC) & M_ELECIDLEDETCYC)
+
+#define S_USECDRLOS    2
+#define V_USECDRLOS(x) ((x) << S_USECDRLOS)
+#define F_USECDRLOS    V_USECDRLOS(1U)
+
+#define S_PCLKREQINP1    1
+#define V_PCLKREQINP1(x) ((x) << S_PCLKREQINP1)
+#define F_PCLKREQINP1    V_PCLKREQINP1(1U)
+
+#define S_PCLKOFFINP1    0
+#define V_PCLKOFFINP1(x) ((x) << S_PCLKOFFINP1)
+#define F_PCLKOFFINP1    V_PCLKOFFINP1(1U)
+
+#define A_PCIE_SERDES_QUAD_CTRL0 0xac
+
+#define S_TESTSIG    10
+#define M_TESTSIG    0x7ffff
+#define V_TESTSIG(x) ((x) << S_TESTSIG)
+#define G_TESTSIG(x) (((x) >> S_TESTSIG) & M_TESTSIG)
+
+#define S_OFFSET    2
+#define M_OFFSET    0xff
+#define V_OFFSET(x) ((x) << S_OFFSET)
+#define G_OFFSET(x) (((x) >> S_OFFSET) & M_OFFSET)
+
+#define S_OFFSETEN    1
+#define V_OFFSETEN(x) ((x) << S_OFFSETEN)
+#define F_OFFSETEN    V_OFFSETEN(1U)
+
+#define S_IDDQB    0
+#define V_IDDQB(x) ((x) << S_IDDQB)
+#define F_IDDQB    V_IDDQB(1U)
+
+#define S_MANMODE    31
+#define V_MANMODE(x) ((x) << S_MANMODE)
+#define F_MANMODE    V_MANMODE(1U)
+
+#define S_MANLPBKEN    29
+#define M_MANLPBKEN    0x3
+#define V_MANLPBKEN(x) ((x) << S_MANLPBKEN)
+#define G_MANLPBKEN(x) (((x) >> S_MANLPBKEN) & M_MANLPBKEN)
+
+#define S_MANTXRECDETEN    28
+#define V_MANTXRECDETEN(x) ((x) << S_MANTXRECDETEN)
+#define F_MANTXRECDETEN    V_MANTXRECDETEN(1U)
+
+#define S_MANTXBEACON    27
+#define V_MANTXBEACON(x) ((x) << S_MANTXBEACON)
+#define F_MANTXBEACON    V_MANTXBEACON(1U)
+
+#define S_MANTXEI    26
+#define V_MANTXEI(x) ((x) << S_MANTXEI)
+#define F_MANTXEI    V_MANTXEI(1U)
+
+#define S_MANRXPOLARITY    25
+#define V_MANRXPOLARITY(x) ((x) << S_MANRXPOLARITY)
+#define F_MANRXPOLARITY    V_MANRXPOLARITY(1U)
+
+#define S_MANTXRST    24
+#define V_MANTXRST(x) ((x) << S_MANTXRST)
+#define F_MANTXRST    V_MANTXRST(1U)
+
+#define S_MANRXRST    23
+#define V_MANRXRST(x) ((x) << S_MANRXRST)
+#define F_MANRXRST    V_MANRXRST(1U)
+
+#define S_MANTXEN    22
+#define V_MANTXEN(x) ((x) << S_MANTXEN)
+#define F_MANTXEN    V_MANTXEN(1U)
+
+#define S_MANRXEN    21
+#define V_MANRXEN(x) ((x) << S_MANRXEN)
+#define F_MANRXEN    V_MANRXEN(1U)
+
+#define S_MANEN    20
+#define V_MANEN(x) ((x) << S_MANEN)
+#define F_MANEN    V_MANEN(1U)
+
+#define S_PCIE_CMURANGE    17
+#define M_PCIE_CMURANGE    0x7
+#define V_PCIE_CMURANGE(x) ((x) << S_PCIE_CMURANGE)
+#define G_PCIE_CMURANGE(x) (((x) >> S_PCIE_CMURANGE) & M_PCIE_CMURANGE)
+
+#define S_PCIE_BGENB    16
+#define V_PCIE_BGENB(x) ((x) << S_PCIE_BGENB)
+#define F_PCIE_BGENB    V_PCIE_BGENB(1U)
+
+#define S_PCIE_ENSKPDROP    15
+#define V_PCIE_ENSKPDROP(x) ((x) << S_PCIE_ENSKPDROP)
+#define F_PCIE_ENSKPDROP    V_PCIE_ENSKPDROP(1U)
+
+#define S_PCIE_ENCOMMA    14
+#define V_PCIE_ENCOMMA(x) ((x) << S_PCIE_ENCOMMA)
+#define F_PCIE_ENCOMMA    V_PCIE_ENCOMMA(1U)
+
+#define S_PCIE_EN8B10B    13
+#define V_PCIE_EN8B10B(x) ((x) << S_PCIE_EN8B10B)
+#define F_PCIE_EN8B10B    V_PCIE_EN8B10B(1U)
+
+#define S_PCIE_ENELBUF    12
+#define V_PCIE_ENELBUF(x) ((x) << S_PCIE_ENELBUF)
+#define F_PCIE_ENELBUF    V_PCIE_ENELBUF(1U)
+
+#define S_PCIE_GAIN    7
+#define M_PCIE_GAIN    0x1f
+#define V_PCIE_GAIN(x) ((x) << S_PCIE_GAIN)
+#define G_PCIE_GAIN(x) (((x) >> S_PCIE_GAIN) & M_PCIE_GAIN)
+
+#define S_PCIE_BANDGAP    3
+#define M_PCIE_BANDGAP    0xf
+#define V_PCIE_BANDGAP(x) ((x) << S_PCIE_BANDGAP)
+#define G_PCIE_BANDGAP(x) (((x) >> S_PCIE_BANDGAP) & M_PCIE_BANDGAP)
+
+#define S_RXCOMADJ    2
+#define V_RXCOMADJ(x) ((x) << S_RXCOMADJ)
+#define F_RXCOMADJ    V_RXCOMADJ(1U)
+
+#define S_PREEMPH    0
+#define M_PREEMPH    0x3
+#define V_PREEMPH(x) ((x) << S_PREEMPH)
+#define G_PREEMPH(x) (((x) >> S_PREEMPH) & M_PREEMPH)
+
+#define A_PCIE_SERDES_QUAD_CTRL1 0xb0
+
+#define S_FASTINIT    28
+#define V_FASTINIT(x) ((x) << S_FASTINIT)
+#define F_FASTINIT    V_FASTINIT(1U)
+
+#define S_CTCDISABLE    27
+#define V_CTCDISABLE(x) ((x) << S_CTCDISABLE)
+#define F_CTCDISABLE    V_CTCDISABLE(1U)
+
+#define S_MANRESETPLL    26
+#define V_MANRESETPLL(x) ((x) << S_MANRESETPLL)
+#define F_MANRESETPLL    V_MANRESETPLL(1U)
+
+#define S_MANL2PWRDN    25
+#define V_MANL2PWRDN(x) ((x) << S_MANL2PWRDN)
+#define F_MANL2PWRDN    V_MANL2PWRDN(1U)
+
+#define S_MANQUADEN    24
+#define V_MANQUADEN(x) ((x) << S_MANQUADEN)
+#define F_MANQUADEN    V_MANQUADEN(1U)
+
+#define S_RXEQCTL    22
+#define M_RXEQCTL    0x3
+#define V_RXEQCTL(x) ((x) << S_RXEQCTL)
+#define G_RXEQCTL(x) (((x) >> S_RXEQCTL) & M_RXEQCTL)
+
+#define S_HIVMODE    21
+#define V_HIVMODE(x) ((x) << S_HIVMODE)
+#define F_HIVMODE    V_HIVMODE(1U)
+
+#define S_REFSEL    19
+#define M_REFSEL    0x3
+#define V_REFSEL(x) ((x) << S_REFSEL)
+#define G_REFSEL(x) (((x) >> S_REFSEL) & M_REFSEL)
+
+#define S_RXTERMADJ    17
+#define M_RXTERMADJ    0x3
+#define V_RXTERMADJ(x) ((x) << S_RXTERMADJ)
+#define G_RXTERMADJ(x) (((x) >> S_RXTERMADJ) & M_RXTERMADJ)
+
+#define S_TXTERMADJ    15
+#define M_TXTERMADJ    0x3
+#define V_TXTERMADJ(x) ((x) << S_TXTERMADJ)
+#define G_TXTERMADJ(x) (((x) >> S_TXTERMADJ) & M_TXTERMADJ)
+
+#define S_DEQ    11
+#define M_DEQ    0xf
+#define V_DEQ(x) ((x) << S_DEQ)
+#define G_DEQ(x) (((x) >> S_DEQ) & M_DEQ)
+
+#define S_DTX    7
+#define M_DTX    0xf
+#define V_DTX(x) ((x) << S_DTX)
+#define G_DTX(x) (((x) >> S_DTX) & M_DTX)
+
+#define S_LODRV    6
+#define V_LODRV(x) ((x) << S_LODRV)
+#define F_LODRV    V_LODRV(1U)
+
+#define S_HIDRV    5
+#define V_HIDRV(x) ((x) << S_HIDRV)
+#define F_HIDRV    V_HIDRV(1U)
+
+#define S_INTPARRESET    4
+#define V_INTPARRESET(x) ((x) << S_INTPARRESET)
+#define F_INTPARRESET    V_INTPARRESET(1U)
+
+#define S_INTPARLPBK    3
+#define V_INTPARLPBK(x) ((x) << S_INTPARLPBK)
+#define F_INTPARLPBK    V_INTPARLPBK(1U)
+
+#define S_INTSERLPBKWDRV    2
+#define V_INTSERLPBKWDRV(x) ((x) << S_INTSERLPBKWDRV)
+#define F_INTSERLPBKWDRV    V_INTSERLPBKWDRV(1U)
+
+#define S_PW    1
+#define V_PW(x) ((x) << S_PW)
+#define F_PW    V_PW(1U)
+
+#define S_PCLKDETECT    0
+#define V_PCLKDETECT(x) ((x) << S_PCLKDETECT)
+#define F_PCLKDETECT    V_PCLKDETECT(1U)
+
+#define A_PCIE_SERDES_STATUS0 0xb0
+
+#define S_RXERRLANE7    21
+#define M_RXERRLANE7    0x7
+#define V_RXERRLANE7(x) ((x) << S_RXERRLANE7)
+#define G_RXERRLANE7(x) (((x) >> S_RXERRLANE7) & M_RXERRLANE7)
+
+#define S_RXERRLANE6    18
+#define M_RXERRLANE6    0x7
+#define V_RXERRLANE6(x) ((x) << S_RXERRLANE6)
+#define G_RXERRLANE6(x) (((x) >> S_RXERRLANE6) & M_RXERRLANE6)
+
+#define S_RXERRLANE5    15
+#define M_RXERRLANE5    0x7
+#define V_RXERRLANE5(x) ((x) << S_RXERRLANE5)
+#define G_RXERRLANE5(x) (((x) >> S_RXERRLANE5) & M_RXERRLANE5)
+
+#define S_RXERRLANE4    12
+#define M_RXERRLANE4    0x7
+#define V_RXERRLANE4(x) ((x) << S_RXERRLANE4)
+#define G_RXERRLANE4(x) (((x) >> S_RXERRLANE4) & M_RXERRLANE4)
+
+#define S_PCIE_RXERRLANE3    9
+#define M_PCIE_RXERRLANE3    0x7
+#define V_PCIE_RXERRLANE3(x) ((x) << S_PCIE_RXERRLANE3)
+#define G_PCIE_RXERRLANE3(x) (((x) >> S_PCIE_RXERRLANE3) & M_PCIE_RXERRLANE3)
+
+#define S_PCIE_RXERRLANE2    6
+#define M_PCIE_RXERRLANE2    0x7
+#define V_PCIE_RXERRLANE2(x) ((x) << S_PCIE_RXERRLANE2)
+#define G_PCIE_RXERRLANE2(x) (((x) >> S_PCIE_RXERRLANE2) & M_PCIE_RXERRLANE2)
+
+#define S_PCIE_RXERRLANE1    3
+#define M_PCIE_RXERRLANE1    0x7
+#define V_PCIE_RXERRLANE1(x) ((x) << S_PCIE_RXERRLANE1)
+#define G_PCIE_RXERRLANE1(x) (((x) >> S_PCIE_RXERRLANE1) & M_PCIE_RXERRLANE1)
+
+#define S_PCIE_RXERRLANE0    0
+#define M_PCIE_RXERRLANE0    0x7
+#define V_PCIE_RXERRLANE0(x) ((x) << S_PCIE_RXERRLANE0)
+#define G_PCIE_RXERRLANE0(x) (((x) >> S_PCIE_RXERRLANE0) & M_PCIE_RXERRLANE0)
+
+#define A_PCIE_SERDES_LANE_CTRL 0xb4
+
+#define S_EXTBISTCHKERRCLR    22
+#define V_EXTBISTCHKERRCLR(x) ((x) << S_EXTBISTCHKERRCLR)
+#define F_EXTBISTCHKERRCLR    V_EXTBISTCHKERRCLR(1U)
+
+#define S_EXTBISTCHKEN    21
+#define V_EXTBISTCHKEN(x) ((x) << S_EXTBISTCHKEN)
+#define F_EXTBISTCHKEN    V_EXTBISTCHKEN(1U)
+
+#define S_EXTBISTGENEN    20
+#define V_EXTBISTGENEN(x) ((x) << S_EXTBISTGENEN)
+#define F_EXTBISTGENEN    V_EXTBISTGENEN(1U)
+
+#define S_EXTBISTPAT    17
+#define M_EXTBISTPAT    0x7
+#define V_EXTBISTPAT(x) ((x) << S_EXTBISTPAT)
+#define G_EXTBISTPAT(x) (((x) >> S_EXTBISTPAT) & M_EXTBISTPAT)
+
+#define S_EXTPARRESET    16
+#define V_EXTPARRESET(x) ((x) << S_EXTPARRESET)
+#define F_EXTPARRESET    V_EXTPARRESET(1U)
+
+#define S_EXTPARLPBK    15
+#define V_EXTPARLPBK(x) ((x) << S_EXTPARLPBK)
+#define F_EXTPARLPBK    V_EXTPARLPBK(1U)
+
+#define S_MANRXTERMEN    14
+#define V_MANRXTERMEN(x) ((x) << S_MANRXTERMEN)
+#define F_MANRXTERMEN    V_MANRXTERMEN(1U)
+
+#define S_MANBEACONTXEN    13
+#define V_MANBEACONTXEN(x) ((x) << S_MANBEACONTXEN)
+#define F_MANBEACONTXEN    V_MANBEACONTXEN(1U)
+
+#define S_MANRXDETECTEN    12
+#define V_MANRXDETECTEN(x) ((x) << S_MANRXDETECTEN)
+#define F_MANRXDETECTEN    V_MANRXDETECTEN(1U)
+
+#define S_MANTXIDLEEN    11
+#define V_MANTXIDLEEN(x) ((x) << S_MANTXIDLEEN)
+#define F_MANTXIDLEEN    V_MANTXIDLEEN(1U)
+
+#define S_MANRXIDLEEN    10
+#define V_MANRXIDLEEN(x) ((x) << S_MANRXIDLEEN)
+#define F_MANRXIDLEEN    V_MANRXIDLEEN(1U)
+
+#define S_MANL1PWRDN    9
+#define V_MANL1PWRDN(x) ((x) << S_MANL1PWRDN)
+#define F_MANL1PWRDN    V_MANL1PWRDN(1U)
+
+#define S_MANRESET    8
+#define V_MANRESET(x) ((x) << S_MANRESET)
+#define F_MANRESET    V_MANRESET(1U)
+
+#define S_MANFMOFFSET    3
+#define M_MANFMOFFSET    0x1f
+#define V_MANFMOFFSET(x) ((x) << S_MANFMOFFSET)
+#define G_MANFMOFFSET(x) (((x) >> S_MANFMOFFSET) & M_MANFMOFFSET)
+
+#define S_MANFMOFFSETEN    2
+#define V_MANFMOFFSETEN(x) ((x) << S_MANFMOFFSETEN)
+#define F_MANFMOFFSETEN    V_MANFMOFFSETEN(1U)
+
+#define S_MANLANEEN    1
+#define V_MANLANEEN(x) ((x) << S_MANLANEEN)
+#define F_MANLANEEN    V_MANLANEEN(1U)
+
+#define S_INTSERLPBK    0
+#define V_INTSERLPBK(x) ((x) << S_INTSERLPBK)
+#define F_INTSERLPBK    V_INTSERLPBK(1U)
+
+#define A_PCIE_SERDES_STATUS1 0xb4
+
+#define S_CMULOCK    31
+#define V_CMULOCK(x) ((x) << S_CMULOCK)
+#define F_CMULOCK    V_CMULOCK(1U)
+
+#define S_RXKLOCKLANE7    23
+#define V_RXKLOCKLANE7(x) ((x) << S_RXKLOCKLANE7)
+#define F_RXKLOCKLANE7    V_RXKLOCKLANE7(1U)
+
+#define S_RXKLOCKLANE6    22
+#define V_RXKLOCKLANE6(x) ((x) << S_RXKLOCKLANE6)
+#define F_RXKLOCKLANE6    V_RXKLOCKLANE6(1U)
+
+#define S_RXKLOCKLANE5    21
+#define V_RXKLOCKLANE5(x) ((x) << S_RXKLOCKLANE5)
+#define F_RXKLOCKLANE5    V_RXKLOCKLANE5(1U)
+
+#define S_RXKLOCKLANE4    20
+#define V_RXKLOCKLANE4(x) ((x) << S_RXKLOCKLANE4)
+#define F_RXKLOCKLANE4    V_RXKLOCKLANE4(1U)
+
+#define S_PCIE_RXKLOCKLANE3    19
+#define V_PCIE_RXKLOCKLANE3(x) ((x) << S_PCIE_RXKLOCKLANE3)
+#define F_PCIE_RXKLOCKLANE3    V_PCIE_RXKLOCKLANE3(1U)
+
+#define S_PCIE_RXKLOCKLANE2    18
+#define V_PCIE_RXKLOCKLANE2(x) ((x) << S_PCIE_RXKLOCKLANE2)
+#define F_PCIE_RXKLOCKLANE2    V_PCIE_RXKLOCKLANE2(1U)
+
+#define S_PCIE_RXKLOCKLANE1    17
+#define V_PCIE_RXKLOCKLANE1(x) ((x) << S_PCIE_RXKLOCKLANE1)
+#define F_PCIE_RXKLOCKLANE1    V_PCIE_RXKLOCKLANE1(1U)
+
+#define S_PCIE_RXKLOCKLANE0    16
+#define V_PCIE_RXKLOCKLANE0(x) ((x) << S_PCIE_RXKLOCKLANE0)
+#define F_PCIE_RXKLOCKLANE0    V_PCIE_RXKLOCKLANE0(1U)
+
+#define S_RXUFLOWLANE7    15
+#define V_RXUFLOWLANE7(x) ((x) << S_RXUFLOWLANE7)
+#define F_RXUFLOWLANE7    V_RXUFLOWLANE7(1U)
+
+#define S_RXUFLOWLANE6    14
+#define V_RXUFLOWLANE6(x) ((x) << S_RXUFLOWLANE6)
+#define F_RXUFLOWLANE6    V_RXUFLOWLANE6(1U)
+
+#define S_RXUFLOWLANE5    13
+#define V_RXUFLOWLANE5(x) ((x) << S_RXUFLOWLANE5)
+#define F_RXUFLOWLANE5    V_RXUFLOWLANE5(1U)
+
+#define S_RXUFLOWLANE4    12
+#define V_RXUFLOWLANE4(x) ((x) << S_RXUFLOWLANE4)
+#define F_RXUFLOWLANE4    V_RXUFLOWLANE4(1U)
+
+#define S_PCIE_RXUFLOWLANE3    11
+#define V_PCIE_RXUFLOWLANE3(x) ((x) << S_PCIE_RXUFLOWLANE3)
+#define F_PCIE_RXUFLOWLANE3    V_PCIE_RXUFLOWLANE3(1U)
+
+#define S_PCIE_RXUFLOWLANE2    10
+#define V_PCIE_RXUFLOWLANE2(x) ((x) << S_PCIE_RXUFLOWLANE2)
+#define F_PCIE_RXUFLOWLANE2    V_PCIE_RXUFLOWLANE2(1U)
+
+#define S_PCIE_RXUFLOWLANE1    9
+#define V_PCIE_RXUFLOWLANE1(x) ((x) << S_PCIE_RXUFLOWLANE1)
+#define F_PCIE_RXUFLOWLANE1    V_PCIE_RXUFLOWLANE1(1U)
+
+#define S_PCIE_RXUFLOWLANE0    8
+#define V_PCIE_RXUFLOWLANE0(x) ((x) << S_PCIE_RXUFLOWLANE0)
+#define F_PCIE_RXUFLOWLANE0    V_PCIE_RXUFLOWLANE0(1U)
+
+#define S_RXOFLOWLANE7    7
+#define V_RXOFLOWLANE7(x) ((x) << S_RXOFLOWLANE7)
+#define F_RXOFLOWLANE7    V_RXOFLOWLANE7(1U)
+
+#define S_RXOFLOWLANE6    6
+#define V_RXOFLOWLANE6(x) ((x) << S_RXOFLOWLANE6)
+#define F_RXOFLOWLANE6    V_RXOFLOWLANE6(1U)
+
+#define S_RXOFLOWLANE5    5
+#define V_RXOFLOWLANE5(x) ((x) << S_RXOFLOWLANE5)
+#define F_RXOFLOWLANE5    V_RXOFLOWLANE5(1U)
+
+#define S_RXOFLOWLANE4    4
+#define V_RXOFLOWLANE4(x) ((x) << S_RXOFLOWLANE4)
+#define F_RXOFLOWLANE4    V_RXOFLOWLANE4(1U)
+
+#define S_PCIE_RXOFLOWLANE3    3
+#define V_PCIE_RXOFLOWLANE3(x) ((x) << S_PCIE_RXOFLOWLANE3)
+#define F_PCIE_RXOFLOWLANE3    V_PCIE_RXOFLOWLANE3(1U)
+
+#define S_PCIE_RXOFLOWLANE2    2
+#define V_PCIE_RXOFLOWLANE2(x) ((x) << S_PCIE_RXOFLOWLANE2)
+#define F_PCIE_RXOFLOWLANE2    V_PCIE_RXOFLOWLANE2(1U)
+
+#define S_PCIE_RXOFLOWLANE1    1
+#define V_PCIE_RXOFLOWLANE1(x) ((x) << S_PCIE_RXOFLOWLANE1)
+#define F_PCIE_RXOFLOWLANE1    V_PCIE_RXOFLOWLANE1(1U)
+
+#define S_PCIE_RXOFLOWLANE0    0
+#define V_PCIE_RXOFLOWLANE0(x) ((x) << S_PCIE_RXOFLOWLANE0)
+#define F_PCIE_RXOFLOWLANE0    V_PCIE_RXOFLOWLANE0(1U)
+
+#define A_PCIE_SERDES_LANE_STAT 0xb8
+
+#define S_EXTBISTCHKERRCNT    8
+#define M_EXTBISTCHKERRCNT    0xffffff
+#define V_EXTBISTCHKERRCNT(x) ((x) << S_EXTBISTCHKERRCNT)
+#define G_EXTBISTCHKERRCNT(x) (((x) >> S_EXTBISTCHKERRCNT) & M_EXTBISTCHKERRCNT)
+
+#define S_EXTBISTCHKFMD    7
+#define V_EXTBISTCHKFMD(x) ((x) << S_EXTBISTCHKFMD)
+#define F_EXTBISTCHKFMD    V_EXTBISTCHKFMD(1U)
+
+#define S_BEACONDETECTCHG    6
+#define V_BEACONDETECTCHG(x) ((x) << S_BEACONDETECTCHG)
+#define F_BEACONDETECTCHG    V_BEACONDETECTCHG(1U)
+
+#define S_RXDETECTCHG    5
+#define V_RXDETECTCHG(x) ((x) << S_RXDETECTCHG)
+#define F_RXDETECTCHG    V_RXDETECTCHG(1U)
+
+#define S_TXIDLEDETECTCHG    4
+#define V_TXIDLEDETECTCHG(x) ((x) << S_TXIDLEDETECTCHG)
+#define F_TXIDLEDETECTCHG    V_TXIDLEDETECTCHG(1U)
+
+#define S_BEACONDETECT    2
+#define V_BEACONDETECT(x) ((x) << S_BEACONDETECT)
+#define F_BEACONDETECT    V_BEACONDETECT(1U)
+
+#define S_RXDETECT    1
+#define V_RXDETECT(x) ((x) << S_RXDETECT)
+#define F_RXDETECT    V_RXDETECT(1U)
+
+#define S_TXIDLEDETECT    0
+#define V_TXIDLEDETECT(x) ((x) << S_TXIDLEDETECT)
+#define F_TXIDLEDETECT    V_TXIDLEDETECT(1U)
+
+#define A_PCIE_SERDES_STATUS2 0xb8
+
+#define S_TXRECDETLANE7    31
+#define V_TXRECDETLANE7(x) ((x) << S_TXRECDETLANE7)
+#define F_TXRECDETLANE7    V_TXRECDETLANE7(1U)
+
+#define S_TXRECDETLANE6    30
+#define V_TXRECDETLANE6(x) ((x) << S_TXRECDETLANE6)
+#define F_TXRECDETLANE6    V_TXRECDETLANE6(1U)
+
+#define S_TXRECDETLANE5    29
+#define V_TXRECDETLANE5(x) ((x) << S_TXRECDETLANE5)
+#define F_TXRECDETLANE5    V_TXRECDETLANE5(1U)
+
+#define S_TXRECDETLANE4    28
+#define V_TXRECDETLANE4(x) ((x) << S_TXRECDETLANE4)
+#define F_TXRECDETLANE4    V_TXRECDETLANE4(1U)
+
+#define S_TXRECDETLANE3    27
+#define V_TXRECDETLANE3(x) ((x) << S_TXRECDETLANE3)
+#define F_TXRECDETLANE3    V_TXRECDETLANE3(1U)
+
+#define S_TXRECDETLANE2    26
+#define V_TXRECDETLANE2(x) ((x) << S_TXRECDETLANE2)
+#define F_TXRECDETLANE2    V_TXRECDETLANE2(1U)
+
+#define S_TXRECDETLANE1    25
+#define V_TXRECDETLANE1(x) ((x) << S_TXRECDETLANE1)
+#define F_TXRECDETLANE1    V_TXRECDETLANE1(1U)
+
+#define S_TXRECDETLANE0    24
+#define V_TXRECDETLANE0(x) ((x) << S_TXRECDETLANE0)
+#define F_TXRECDETLANE0    V_TXRECDETLANE0(1U)
+
+#define S_RXEIDLANE7    23
+#define V_RXEIDLANE7(x) ((x) << S_RXEIDLANE7)
+#define F_RXEIDLANE7    V_RXEIDLANE7(1U)
+
+#define S_RXEIDLANE6    22
+#define V_RXEIDLANE6(x) ((x) << S_RXEIDLANE6)
+#define F_RXEIDLANE6    V_RXEIDLANE6(1U)
+
+#define S_RXEIDLANE5    21
+#define V_RXEIDLANE5(x) ((x) << S_RXEIDLANE5)
+#define F_RXEIDLANE5    V_RXEIDLANE5(1U)
+
+#define S_RXEIDLANE4    20
+#define V_RXEIDLANE4(x) ((x) << S_RXEIDLANE4)
+#define F_RXEIDLANE4    V_RXEIDLANE4(1U)
+
+#define S_RXEIDLANE3    19
+#define V_RXEIDLANE3(x) ((x) << S_RXEIDLANE3)
+#define F_RXEIDLANE3    V_RXEIDLANE3(1U)
+
+#define S_RXEIDLANE2    18
+#define V_RXEIDLANE2(x) ((x) << S_RXEIDLANE2)
+#define F_RXEIDLANE2    V_RXEIDLANE2(1U)
+
+#define S_RXEIDLANE1    17
+#define V_RXEIDLANE1(x) ((x) << S_RXEIDLANE1)
+#define F_RXEIDLANE1    V_RXEIDLANE1(1U)
+
+#define S_RXEIDLANE0    16
+#define V_RXEIDLANE0(x) ((x) << S_RXEIDLANE0)
+#define F_RXEIDLANE0    V_RXEIDLANE0(1U)
+
+#define S_RXREMSKIPLANE7    15
+#define V_RXREMSKIPLANE7(x) ((x) << S_RXREMSKIPLANE7)
+#define F_RXREMSKIPLANE7    V_RXREMSKIPLANE7(1U)
+
+#define S_RXREMSKIPLANE6    14
+#define V_RXREMSKIPLANE6(x) ((x) << S_RXREMSKIPLANE6)
+#define F_RXREMSKIPLANE6    V_RXREMSKIPLANE6(1U)
+
+#define S_RXREMSKIPLANE5    13
+#define V_RXREMSKIPLANE5(x) ((x) << S_RXREMSKIPLANE5)
+#define F_RXREMSKIPLANE5    V_RXREMSKIPLANE5(1U)
+
+#define S_RXREMSKIPLANE4    12
+#define V_RXREMSKIPLANE4(x) ((x) << S_RXREMSKIPLANE4)
+#define F_RXREMSKIPLANE4    V_RXREMSKIPLANE4(1U)
+
+#define S_PCIE_RXREMSKIPLANE3    11
+#define V_PCIE_RXREMSKIPLANE3(x) ((x) << S_PCIE_RXREMSKIPLANE3)
+#define F_PCIE_RXREMSKIPLANE3    V_PCIE_RXREMSKIPLANE3(1U)
+
+#define S_PCIE_RXREMSKIPLANE2    10
+#define V_PCIE_RXREMSKIPLANE2(x) ((x) << S_PCIE_RXREMSKIPLANE2)
+#define F_PCIE_RXREMSKIPLANE2    V_PCIE_RXREMSKIPLANE2(1U)
+
+#define S_PCIE_RXREMSKIPLANE1    9
+#define V_PCIE_RXREMSKIPLANE1(x) ((x) << S_PCIE_RXREMSKIPLANE1)
+#define F_PCIE_RXREMSKIPLANE1    V_PCIE_RXREMSKIPLANE1(1U)
+
+#define S_PCIE_RXREMSKIPLANE0    8
+#define V_PCIE_RXREMSKIPLANE0(x) ((x) << S_PCIE_RXREMSKIPLANE0)
+#define F_PCIE_RXREMSKIPLANE0    V_PCIE_RXREMSKIPLANE0(1U)
+
+#define S_RXADDSKIPLANE7    7
+#define V_RXADDSKIPLANE7(x) ((x) << S_RXADDSKIPLANE7)
+#define F_RXADDSKIPLANE7    V_RXADDSKIPLANE7(1U)
+
+#define S_RXADDSKIPLANE6    6
+#define V_RXADDSKIPLANE6(x) ((x) << S_RXADDSKIPLANE6)
+#define F_RXADDSKIPLANE6    V_RXADDSKIPLANE6(1U)
+
+#define S_RXADDSKIPLANE5    5
+#define V_RXADDSKIPLANE5(x) ((x) << S_RXADDSKIPLANE5)
+#define F_RXADDSKIPLANE5    V_RXADDSKIPLANE5(1U)
+
+#define S_RXADDSKIPLANE4    4
+#define V_RXADDSKIPLANE4(x) ((x) << S_RXADDSKIPLANE4)
+#define F_RXADDSKIPLANE4    V_RXADDSKIPLANE4(1U)
+
+#define S_PCIE_RXADDSKIPLANE3    3
+#define V_PCIE_RXADDSKIPLANE3(x) ((x) << S_PCIE_RXADDSKIPLANE3)
+#define F_PCIE_RXADDSKIPLANE3    V_PCIE_RXADDSKIPLANE3(1U)
+
+#define S_PCIE_RXADDSKIPLANE2    2
+#define V_PCIE_RXADDSKIPLANE2(x) ((x) << S_PCIE_RXADDSKIPLANE2)
+#define F_PCIE_RXADDSKIPLANE2    V_PCIE_RXADDSKIPLANE2(1U)
+
+#define S_PCIE_RXADDSKIPLANE1    1
+#define V_PCIE_RXADDSKIPLANE1(x) ((x) << S_PCIE_RXADDSKIPLANE1)
+#define F_PCIE_RXADDSKIPLANE1    V_PCIE_RXADDSKIPLANE1(1U)
+
+#define S_PCIE_RXADDSKIPLANE0    0
+#define V_PCIE_RXADDSKIPLANE0(x) ((x) << S_PCIE_RXADDSKIPLANE0)
+#define F_PCIE_RXADDSKIPLANE0    V_PCIE_RXADDSKIPLANE0(1U)
+
+#define A_PCIE_PEX_WMARK 0xbc
+
+#define S_P_WMARK    18
+#define M_P_WMARK    0x7ff
+#define V_P_WMARK(x) ((x) << S_P_WMARK)
+#define G_P_WMARK(x) (((x) >> S_P_WMARK) & M_P_WMARK)
+
+#define S_NP_WMARK    11
+#define M_NP_WMARK    0x7f
+#define V_NP_WMARK(x) ((x) << S_NP_WMARK)
+#define G_NP_WMARK(x) (((x) >> S_NP_WMARK) & M_NP_WMARK)
+
+#define S_CPL_WMARK    0
+#define M_CPL_WMARK    0x7ff
+#define V_CPL_WMARK(x) ((x) << S_CPL_WMARK)
+#define G_CPL_WMARK(x) (((x) >> S_CPL_WMARK) & M_CPL_WMARK)
+
+#define A_PCIE_SERDES_BIST 0xbc
+
+#define S_PCIE_BISTDONE    24
+#define M_PCIE_BISTDONE    0xff
+#define V_PCIE_BISTDONE(x) ((x) << S_PCIE_BISTDONE)
+#define G_PCIE_BISTDONE(x) (((x) >> S_PCIE_BISTDONE) & M_PCIE_BISTDONE)
+
+#define S_PCIE_BISTCYCLETHRESH    3
+#define M_PCIE_BISTCYCLETHRESH    0xffff
+#define V_PCIE_BISTCYCLETHRESH(x) ((x) << S_PCIE_BISTCYCLETHRESH)
+#define G_PCIE_BISTCYCLETHRESH(x) (((x) >> S_PCIE_BISTCYCLETHRESH) & M_PCIE_BISTCYCLETHRESH)
+
+#define S_BISTMODE    0
+#define M_BISTMODE    0x7
+#define V_BISTMODE(x) ((x) << S_BISTMODE)
+#define G_BISTMODE(x) (((x) >> S_BISTMODE) & M_BISTMODE)
+
+/* registers for module T3DBG */
+#define T3DBG_BASE_ADDR 0xc0
+
+#define A_T3DBG_DBG0_CFG 0xc0
+
+#define S_REGSELECT    9
+#define M_REGSELECT    0xff
+#define V_REGSELECT(x) ((x) << S_REGSELECT)
+#define G_REGSELECT(x) (((x) >> S_REGSELECT) & M_REGSELECT)
+
+#define S_MODULESELECT    4
+#define M_MODULESELECT    0x1f
+#define V_MODULESELECT(x) ((x) << S_MODULESELECT)
+#define G_MODULESELECT(x) (((x) >> S_MODULESELECT) & M_MODULESELECT)
+
+#define S_CLKSELECT    0
+#define M_CLKSELECT    0xf
+#define V_CLKSELECT(x) ((x) << S_CLKSELECT)
+#define G_CLKSELECT(x) (((x) >> S_CLKSELECT) & M_CLKSELECT)
+
+#define A_T3DBG_DBG0_EN 0xc4
+
+#define S_SDRBYTE0    8
+#define V_SDRBYTE0(x) ((x) << S_SDRBYTE0)
+#define F_SDRBYTE0    V_SDRBYTE0(1U)
+
+#define S_DDREN    4
+#define V_DDREN(x) ((x) << S_DDREN)
+#define F_DDREN    V_DDREN(1U)
+
+#define S_PORTEN    0
+#define V_PORTEN(x) ((x) << S_PORTEN)
+#define F_PORTEN    V_PORTEN(1U)
+
+#define A_T3DBG_DBG1_CFG 0xc8
+#define A_T3DBG_DBG1_EN 0xcc
 #define A_T3DBG_GPIO_EN 0xd0
 
 #define S_GPIO11_OEN    27
@@ -511,6 +2084,14 @@
 #define V_GPIO10_OEN(x) ((x) << S_GPIO10_OEN)
 #define F_GPIO10_OEN    V_GPIO10_OEN(1U)
 
+#define S_GPIO9_OEN    25
+#define V_GPIO9_OEN(x) ((x) << S_GPIO9_OEN)
+#define F_GPIO9_OEN    V_GPIO9_OEN(1U)
+
+#define S_GPIO8_OEN    24
+#define V_GPIO8_OEN(x) ((x) << S_GPIO8_OEN)
+#define F_GPIO8_OEN    V_GPIO8_OEN(1U)
+
 #define S_GPIO7_OEN    23
 #define V_GPIO7_OEN(x) ((x) << S_GPIO7_OEN)
 #define F_GPIO7_OEN    V_GPIO7_OEN(1U)
@@ -527,6 +2108,10 @@
 #define V_GPIO4_OEN(x) ((x) << S_GPIO4_OEN)
 #define F_GPIO4_OEN    V_GPIO4_OEN(1U)
 
+#define S_GPIO3_OEN    19
+#define V_GPIO3_OEN(x) ((x) << S_GPIO3_OEN)
+#define F_GPIO3_OEN    V_GPIO3_OEN(1U)
+
 #define S_GPIO2_OEN    18
 #define V_GPIO2_OEN(x) ((x) << S_GPIO2_OEN)
 #define F_GPIO2_OEN    V_GPIO2_OEN(1U)
@@ -539,10 +2124,22 @@
 #define V_GPIO0_OEN(x) ((x) << S_GPIO0_OEN)
 #define F_GPIO0_OEN    V_GPIO0_OEN(1U)
 
+#define S_GPIO11_OUT_VAL    11
+#define V_GPIO11_OUT_VAL(x) ((x) << S_GPIO11_OUT_VAL)
+#define F_GPIO11_OUT_VAL    V_GPIO11_OUT_VAL(1U)
+
 #define S_GPIO10_OUT_VAL    10
 #define V_GPIO10_OUT_VAL(x) ((x) << S_GPIO10_OUT_VAL)
 #define F_GPIO10_OUT_VAL    V_GPIO10_OUT_VAL(1U)
 
+#define S_GPIO9_OUT_VAL    9
+#define V_GPIO9_OUT_VAL(x) ((x) << S_GPIO9_OUT_VAL)
+#define F_GPIO9_OUT_VAL    V_GPIO9_OUT_VAL(1U)
+
+#define S_GPIO8_OUT_VAL    8
+#define V_GPIO8_OUT_VAL(x) ((x) << S_GPIO8_OUT_VAL)
+#define F_GPIO8_OUT_VAL    V_GPIO8_OUT_VAL(1U)
+
 #define S_GPIO7_OUT_VAL    7
 #define V_GPIO7_OUT_VAL(x) ((x) << S_GPIO7_OUT_VAL)
 #define F_GPIO7_OUT_VAL    V_GPIO7_OUT_VAL(1U)
@@ -559,6 +2156,10 @@
 #define V_GPIO4_OUT_VAL(x) ((x) << S_GPIO4_OUT_VAL)
 #define F_GPIO4_OUT_VAL    V_GPIO4_OUT_VAL(1U)
 
+#define S_GPIO3_OUT_VAL    3
+#define V_GPIO3_OUT_VAL(x) ((x) << S_GPIO3_OUT_VAL)
+#define F_GPIO3_OUT_VAL    V_GPIO3_OUT_VAL(1U)
+
 #define S_GPIO2_OUT_VAL    2
 #define V_GPIO2_OUT_VAL(x) ((x) << S_GPIO2_OUT_VAL)
 #define F_GPIO2_OUT_VAL    V_GPIO2_OUT_VAL(1U)
@@ -571,8 +2172,126 @@
 #define V_GPIO0_OUT_VAL(x) ((x) << S_GPIO0_OUT_VAL)
 #define F_GPIO0_OUT_VAL    V_GPIO0_OUT_VAL(1U)
 
+#define A_T3DBG_GPIO_IN 0xd4
+
+#define S_GPIO11_CHG_DET    27
+#define V_GPIO11_CHG_DET(x) ((x) << S_GPIO11_CHG_DET)
+#define F_GPIO11_CHG_DET    V_GPIO11_CHG_DET(1U)
+
+#define S_GPIO10_CHG_DET    26
+#define V_GPIO10_CHG_DET(x) ((x) << S_GPIO10_CHG_DET)
+#define F_GPIO10_CHG_DET    V_GPIO10_CHG_DET(1U)
+
+#define S_GPIO9_CHG_DET    25
+#define V_GPIO9_CHG_DET(x) ((x) << S_GPIO9_CHG_DET)
+#define F_GPIO9_CHG_DET    V_GPIO9_CHG_DET(1U)
+
+#define S_GPIO8_CHG_DET    24
+#define V_GPIO8_CHG_DET(x) ((x) << S_GPIO8_CHG_DET)
+#define F_GPIO8_CHG_DET    V_GPIO8_CHG_DET(1U)
+
+#define S_GPIO7_CHG_DET    23
+#define V_GPIO7_CHG_DET(x) ((x) << S_GPIO7_CHG_DET)
+#define F_GPIO7_CHG_DET    V_GPIO7_CHG_DET(1U)
+
+#define S_GPIO6_CHG_DET    22
+#define V_GPIO6_CHG_DET(x) ((x) << S_GPIO6_CHG_DET)
+#define F_GPIO6_CHG_DET    V_GPIO6_CHG_DET(1U)
+
+#define S_GPIO5_CHG_DET    21
+#define V_GPIO5_CHG_DET(x) ((x) << S_GPIO5_CHG_DET)
+#define F_GPIO5_CHG_DET    V_GPIO5_CHG_DET(1U)
+
+#define S_GPIO4_CHG_DET    20
+#define V_GPIO4_CHG_DET(x) ((x) << S_GPIO4_CHG_DET)
+#define F_GPIO4_CHG_DET    V_GPIO4_CHG_DET(1U)
+
+#define S_GPIO3_CHG_DET    19
+#define V_GPIO3_CHG_DET(x) ((x) << S_GPIO3_CHG_DET)
+#define F_GPIO3_CHG_DET    V_GPIO3_CHG_DET(1U)
+
+#define S_GPIO2_CHG_DET    18
+#define V_GPIO2_CHG_DET(x) ((x) << S_GPIO2_CHG_DET)
+#define F_GPIO2_CHG_DET    V_GPIO2_CHG_DET(1U)
+
+#define S_GPIO1_CHG_DET    17
+#define V_GPIO1_CHG_DET(x) ((x) << S_GPIO1_CHG_DET)
+#define F_GPIO1_CHG_DET    V_GPIO1_CHG_DET(1U)
+
+#define S_GPIO0_CHG_DET    16
+#define V_GPIO0_CHG_DET(x) ((x) << S_GPIO0_CHG_DET)
+#define F_GPIO0_CHG_DET    V_GPIO0_CHG_DET(1U)
+
+#define S_GPIO11_IN    11
+#define V_GPIO11_IN(x) ((x) << S_GPIO11_IN)
+#define F_GPIO11_IN    V_GPIO11_IN(1U)
+
+#define S_GPIO10_IN    10
+#define V_GPIO10_IN(x) ((x) << S_GPIO10_IN)
+#define F_GPIO10_IN    V_GPIO10_IN(1U)
+
+#define S_GPIO9_IN    9
+#define V_GPIO9_IN(x) ((x) << S_GPIO9_IN)
+#define F_GPIO9_IN    V_GPIO9_IN(1U)
+
+#define S_GPIO8_IN    8
+#define V_GPIO8_IN(x) ((x) << S_GPIO8_IN)
+#define F_GPIO8_IN    V_GPIO8_IN(1U)
+
+#define S_GPIO7_IN    7
+#define V_GPIO7_IN(x) ((x) << S_GPIO7_IN)
+#define F_GPIO7_IN    V_GPIO7_IN(1U)
+
+#define S_GPIO6_IN    6
+#define V_GPIO6_IN(x) ((x) << S_GPIO6_IN)
+#define F_GPIO6_IN    V_GPIO6_IN(1U)
+
+#define S_GPIO5_IN    5
+#define V_GPIO5_IN(x) ((x) << S_GPIO5_IN)
+#define F_GPIO5_IN    V_GPIO5_IN(1U)
+
+#define S_GPIO4_IN    4
+#define V_GPIO4_IN(x) ((x) << S_GPIO4_IN)
+#define F_GPIO4_IN    V_GPIO4_IN(1U)
+
+#define S_GPIO3_IN    3
+#define V_GPIO3_IN(x) ((x) << S_GPIO3_IN)
+#define F_GPIO3_IN    V_GPIO3_IN(1U)
+
+#define S_GPIO2_IN    2
+#define V_GPIO2_IN(x) ((x) << S_GPIO2_IN)
+#define F_GPIO2_IN    V_GPIO2_IN(1U)
+
+#define S_GPIO1_IN    1
+#define V_GPIO1_IN(x) ((x) << S_GPIO1_IN)
+#define F_GPIO1_IN    V_GPIO1_IN(1U)
+
+#define S_GPIO0_IN    0
+#define V_GPIO0_IN(x) ((x) << S_GPIO0_IN)
+#define F_GPIO0_IN    V_GPIO0_IN(1U)
+
 #define A_T3DBG_INT_ENABLE 0xd8
 
+#define S_C_LOCK    21
+#define V_C_LOCK(x) ((x) << S_C_LOCK)
+#define F_C_LOCK    V_C_LOCK(1U)
+
+#define S_M_LOCK    20
+#define V_M_LOCK(x) ((x) << S_M_LOCK)
+#define F_M_LOCK    V_M_LOCK(1U)
+
+#define S_U_LOCK    19
+#define V_U_LOCK(x) ((x) << S_U_LOCK)
+#define F_U_LOCK    V_U_LOCK(1U)
+
+#define S_R_LOCK    18
+#define V_R_LOCK(x) ((x) << S_R_LOCK)
+#define F_R_LOCK    V_R_LOCK(1U)
+
+#define S_PX_LOCK    17
+#define V_PX_LOCK(x) ((x) << S_PX_LOCK)
+#define F_PX_LOCK    V_PX_LOCK(1U)
+
 #define S_GPIO11    11
 #define V_GPIO11(x) ((x) << S_GPIO11)
 #define F_GPIO11    V_GPIO11(1U)
@@ -585,6 +2304,10 @@
 #define V_GPIO9(x) ((x) << S_GPIO9)
 #define F_GPIO9    V_GPIO9(1U)
 
+#define S_GPIO8    8
+#define V_GPIO8(x) ((x) << S_GPIO8)
+#define F_GPIO8    V_GPIO8(1U)
+
 #define S_GPIO7    7
 #define V_GPIO7(x) ((x) << S_GPIO7)
 #define F_GPIO7    V_GPIO7(1U)
@@ -617,18 +2340,337 @@
 #define V_GPIO0(x) ((x) << S_GPIO0)
 #define F_GPIO0    V_GPIO0(1U)
 
+#define S_PE_LOCK    16
+#define V_PE_LOCK(x) ((x) << S_PE_LOCK)
+#define F_PE_LOCK    V_PE_LOCK(1U)
+
 #define A_T3DBG_INT_CAUSE 0xdc
+#define A_T3DBG_DBG0_RST_VALUE 0xe0
+
+#define S_DEBUGDATA    0
+#define M_DEBUGDATA    0xff
+#define V_DEBUGDATA(x) ((x) << S_DEBUGDATA)
+#define G_DEBUGDATA(x) (((x) >> S_DEBUGDATA) & M_DEBUGDATA)
+
+#define A_T3DBG_PLL_OCLK_PAD_EN 0xe4
+
+#define S_PCIE_OCLK_EN    20
+#define V_PCIE_OCLK_EN(x) ((x) << S_PCIE_OCLK_EN)
+#define F_PCIE_OCLK_EN    V_PCIE_OCLK_EN(1U)
+
+#define S_PCLKTREE_DBG_EN    17
+#define V_PCLKTREE_DBG_EN(x) ((x) << S_PCLKTREE_DBG_EN)
+#define F_PCLKTREE_DBG_EN    V_PCLKTREE_DBG_EN(1U)
+
+#define S_PCIX_OCLK_EN    16
+#define V_PCIX_OCLK_EN(x) ((x) << S_PCIX_OCLK_EN)
+#define F_PCIX_OCLK_EN    V_PCIX_OCLK_EN(1U)
+
+#define S_U_OCLK_EN    12
+#define V_U_OCLK_EN(x) ((x) << S_U_OCLK_EN)
+#define F_U_OCLK_EN    V_U_OCLK_EN(1U)
+
+#define S_R_OCLK_EN    8
+#define V_R_OCLK_EN(x) ((x) << S_R_OCLK_EN)
+#define F_R_OCLK_EN    V_R_OCLK_EN(1U)
+
+#define S_M_OCLK_EN    4
+#define V_M_OCLK_EN(x) ((x) << S_M_OCLK_EN)
+#define F_M_OCLK_EN    V_M_OCLK_EN(1U)
+
+#define S_C_OCLK_EN    0
+#define V_C_OCLK_EN(x) ((x) << S_C_OCLK_EN)
+#define F_C_OCLK_EN    V_C_OCLK_EN(1U)
+
+#define A_T3DBG_PLL_LOCK 0xe8
+
+#define S_PCIX_LOCK    16
+#define V_PCIX_LOCK(x) ((x) << S_PCIX_LOCK)
+#define F_PCIX_LOCK    V_PCIX_LOCK(1U)
+
+#define S_PLL_U_LOCK    12
+#define V_PLL_U_LOCK(x) ((x) << S_PLL_U_LOCK)
+#define F_PLL_U_LOCK    V_PLL_U_LOCK(1U)
+
+#define S_PLL_R_LOCK    8
+#define V_PLL_R_LOCK(x) ((x) << S_PLL_R_LOCK)
+#define F_PLL_R_LOCK    V_PLL_R_LOCK(1U)
+
+#define S_PLL_M_LOCK    4
+#define V_PLL_M_LOCK(x) ((x) << S_PLL_M_LOCK)
+#define F_PLL_M_LOCK    V_PLL_M_LOCK(1U)
+
+#define S_PLL_C_LOCK    0
+#define V_PLL_C_LOCK(x) ((x) << S_PLL_C_LOCK)
+#define F_PLL_C_LOCK    V_PLL_C_LOCK(1U)
+
+#define S_PCIE_LOCK    20
+#define V_PCIE_LOCK(x) ((x) << S_PCIE_LOCK)
+#define F_PCIE_LOCK    V_PCIE_LOCK(1U)
+
+#define A_T3DBG_SERDES_RBC_CFG 0xec
+
+#define S_X_RBC_LANE_SEL    16
+#define M_X_RBC_LANE_SEL    0x3
+#define V_X_RBC_LANE_SEL(x) ((x) << S_X_RBC_LANE_SEL)
+#define G_X_RBC_LANE_SEL(x) (((x) >> S_X_RBC_LANE_SEL) & M_X_RBC_LANE_SEL)
+
+#define S_X_RBC_DBG_EN    12
+#define V_X_RBC_DBG_EN(x) ((x) << S_X_RBC_DBG_EN)
+#define F_X_RBC_DBG_EN    V_X_RBC_DBG_EN(1U)
+
+#define S_X_SERDES_SEL    8
+#define V_X_SERDES_SEL(x) ((x) << S_X_SERDES_SEL)
+#define F_X_SERDES_SEL    V_X_SERDES_SEL(1U)
+
+#define S_PE_RBC_LANE_SEL    4
+#define M_PE_RBC_LANE_SEL    0x7
+#define V_PE_RBC_LANE_SEL(x) ((x) << S_PE_RBC_LANE_SEL)
+#define G_PE_RBC_LANE_SEL(x) (((x) >> S_PE_RBC_LANE_SEL) & M_PE_RBC_LANE_SEL)
+
+#define S_PE_RBC_DBG_EN    0
+#define V_PE_RBC_DBG_EN(x) ((x) << S_PE_RBC_DBG_EN)
+#define F_PE_RBC_DBG_EN    V_PE_RBC_DBG_EN(1U)
 
 #define A_T3DBG_GPIO_ACT_LOW 0xf0
 
+#define S_C_LOCK_ACT_LOW    21
+#define V_C_LOCK_ACT_LOW(x) ((x) << S_C_LOCK_ACT_LOW)
+#define F_C_LOCK_ACT_LOW    V_C_LOCK_ACT_LOW(1U)
+
+#define S_M_LOCK_ACT_LOW    20
+#define V_M_LOCK_ACT_LOW(x) ((x) << S_M_LOCK_ACT_LOW)
+#define F_M_LOCK_ACT_LOW    V_M_LOCK_ACT_LOW(1U)
+
+#define S_U_LOCK_ACT_LOW    19
+#define V_U_LOCK_ACT_LOW(x) ((x) << S_U_LOCK_ACT_LOW)
+#define F_U_LOCK_ACT_LOW    V_U_LOCK_ACT_LOW(1U)
+
+#define S_R_LOCK_ACT_LOW    18
+#define V_R_LOCK_ACT_LOW(x) ((x) << S_R_LOCK_ACT_LOW)
+#define F_R_LOCK_ACT_LOW    V_R_LOCK_ACT_LOW(1U)
+
+#define S_PX_LOCK_ACT_LOW    17
+#define V_PX_LOCK_ACT_LOW(x) ((x) << S_PX_LOCK_ACT_LOW)
+#define F_PX_LOCK_ACT_LOW    V_PX_LOCK_ACT_LOW(1U)
+
+#define S_GPIO11_ACT_LOW    11
+#define V_GPIO11_ACT_LOW(x) ((x) << S_GPIO11_ACT_LOW)
+#define F_GPIO11_ACT_LOW    V_GPIO11_ACT_LOW(1U)
+
+#define S_GPIO10_ACT_LOW    10
+#define V_GPIO10_ACT_LOW(x) ((x) << S_GPIO10_ACT_LOW)
+#define F_GPIO10_ACT_LOW    V_GPIO10_ACT_LOW(1U)
+
+#define S_GPIO9_ACT_LOW    9
+#define V_GPIO9_ACT_LOW(x) ((x) << S_GPIO9_ACT_LOW)
+#define F_GPIO9_ACT_LOW    V_GPIO9_ACT_LOW(1U)
+
+#define S_GPIO8_ACT_LOW    8
+#define V_GPIO8_ACT_LOW(x) ((x) << S_GPIO8_ACT_LOW)
+#define F_GPIO8_ACT_LOW    V_GPIO8_ACT_LOW(1U)
+
+#define S_GPIO7_ACT_LOW    7
+#define V_GPIO7_ACT_LOW(x) ((x) << S_GPIO7_ACT_LOW)
+#define F_GPIO7_ACT_LOW    V_GPIO7_ACT_LOW(1U)
+
+#define S_GPIO6_ACT_LOW    6
+#define V_GPIO6_ACT_LOW(x) ((x) << S_GPIO6_ACT_LOW)
+#define F_GPIO6_ACT_LOW    V_GPIO6_ACT_LOW(1U)
+
+#define S_GPIO5_ACT_LOW    5
+#define V_GPIO5_ACT_LOW(x) ((x) << S_GPIO5_ACT_LOW)
+#define F_GPIO5_ACT_LOW    V_GPIO5_ACT_LOW(1U)
+
+#define S_GPIO4_ACT_LOW    4
+#define V_GPIO4_ACT_LOW(x) ((x) << S_GPIO4_ACT_LOW)
+#define F_GPIO4_ACT_LOW    V_GPIO4_ACT_LOW(1U)
+
+#define S_GPIO3_ACT_LOW    3
+#define V_GPIO3_ACT_LOW(x) ((x) << S_GPIO3_ACT_LOW)
+#define F_GPIO3_ACT_LOW    V_GPIO3_ACT_LOW(1U)
+
+#define S_GPIO2_ACT_LOW    2
+#define V_GPIO2_ACT_LOW(x) ((x) << S_GPIO2_ACT_LOW)
+#define F_GPIO2_ACT_LOW    V_GPIO2_ACT_LOW(1U)
+
+#define S_GPIO1_ACT_LOW    1
+#define V_GPIO1_ACT_LOW(x) ((x) << S_GPIO1_ACT_LOW)
+#define F_GPIO1_ACT_LOW    V_GPIO1_ACT_LOW(1U)
+
+#define S_GPIO0_ACT_LOW    0
+#define V_GPIO0_ACT_LOW(x) ((x) << S_GPIO0_ACT_LOW)
+#define F_GPIO0_ACT_LOW    V_GPIO0_ACT_LOW(1U)
+
+#define S_PE_LOCK_ACT_LOW    16
+#define V_PE_LOCK_ACT_LOW(x) ((x) << S_PE_LOCK_ACT_LOW)
+#define F_PE_LOCK_ACT_LOW    V_PE_LOCK_ACT_LOW(1U)
+
+#define A_T3DBG_PMON_CFG 0xf4
+
+#define S_PMON_DONE    29
+#define V_PMON_DONE(x) ((x) << S_PMON_DONE)
+#define F_PMON_DONE    V_PMON_DONE(1U)
+
+#define S_PMON_FAIL    28
+#define V_PMON_FAIL(x) ((x) << S_PMON_FAIL)
+#define F_PMON_FAIL    V_PMON_FAIL(1U)
+
+#define S_PMON_FDEL_AUTO    22
+#define M_PMON_FDEL_AUTO    0x3f
+#define V_PMON_FDEL_AUTO(x) ((x) << S_PMON_FDEL_AUTO)
+#define G_PMON_FDEL_AUTO(x) (((x) >> S_PMON_FDEL_AUTO) & M_PMON_FDEL_AUTO)
+
+#define S_PMON_CDEL_AUTO    16
+#define M_PMON_CDEL_AUTO    0x3f
+#define V_PMON_CDEL_AUTO(x) ((x) << S_PMON_CDEL_AUTO)
+#define G_PMON_CDEL_AUTO(x) (((x) >> S_PMON_CDEL_AUTO) & M_PMON_CDEL_AUTO)
+
+#define S_PMON_FDEL_MANUAL    10
+#define M_PMON_FDEL_MANUAL    0x3f
+#define V_PMON_FDEL_MANUAL(x) ((x) << S_PMON_FDEL_MANUAL)
+#define G_PMON_FDEL_MANUAL(x) (((x) >> S_PMON_FDEL_MANUAL) & M_PMON_FDEL_MANUAL)
+
+#define S_PMON_CDEL_MANUAL    4
+#define M_PMON_CDEL_MANUAL    0x3f
+#define V_PMON_CDEL_MANUAL(x) ((x) << S_PMON_CDEL_MANUAL)
+#define G_PMON_CDEL_MANUAL(x) (((x) >> S_PMON_CDEL_MANUAL) & M_PMON_CDEL_MANUAL)
+
+#define S_PMON_MANUAL    1
+#define V_PMON_MANUAL(x) ((x) << S_PMON_MANUAL)
+#define F_PMON_MANUAL    V_PMON_MANUAL(1U)
+
+#define S_PMON_AUTO    0
+#define V_PMON_AUTO(x) ((x) << S_PMON_AUTO)
+#define F_PMON_AUTO    V_PMON_AUTO(1U)
+
+#define A_T3DBG_SERDES_REFCLK_CFG 0xf8
+
+#define S_PE_REFCLK_DBG_EN    12
+#define V_PE_REFCLK_DBG_EN(x) ((x) << S_PE_REFCLK_DBG_EN)
+#define F_PE_REFCLK_DBG_EN    V_PE_REFCLK_DBG_EN(1U)
+
+#define S_X_REFCLK_DBG_EN    8
+#define V_X_REFCLK_DBG_EN(x) ((x) << S_X_REFCLK_DBG_EN)
+#define F_X_REFCLK_DBG_EN    V_X_REFCLK_DBG_EN(1U)
+
+#define S_PE_REFCLK_TERMADJ    5
+#define M_PE_REFCLK_TERMADJ    0x3
+#define V_PE_REFCLK_TERMADJ(x) ((x) << S_PE_REFCLK_TERMADJ)
+#define G_PE_REFCLK_TERMADJ(x) (((x) >> S_PE_REFCLK_TERMADJ) & M_PE_REFCLK_TERMADJ)
+
+#define S_PE_REFCLK_PD    4
+#define V_PE_REFCLK_PD(x) ((x) << S_PE_REFCLK_PD)
+#define F_PE_REFCLK_PD    V_PE_REFCLK_PD(1U)
+
+#define S_X_REFCLK_TERMADJ    1
+#define M_X_REFCLK_TERMADJ    0x3
+#define V_X_REFCLK_TERMADJ(x) ((x) << S_X_REFCLK_TERMADJ)
+#define G_X_REFCLK_TERMADJ(x) (((x) >> S_X_REFCLK_TERMADJ) & M_X_REFCLK_TERMADJ)
+
+#define S_X_REFCLK_PD    0
+#define V_X_REFCLK_PD(x) ((x) << S_X_REFCLK_PD)
+#define F_X_REFCLK_PD    V_X_REFCLK_PD(1U)
+
+#define A_T3DBG_PCIE_PMA_BSPIN_CFG 0xfc
+
+#define S_BSMODEQUAD1    31
+#define V_BSMODEQUAD1(x) ((x) << S_BSMODEQUAD1)
+#define F_BSMODEQUAD1    V_BSMODEQUAD1(1U)
+
+#define S_BSINSELLANE7    29
+#define M_BSINSELLANE7    0x3
+#define V_BSINSELLANE7(x) ((x) << S_BSINSELLANE7)
+#define G_BSINSELLANE7(x) (((x) >> S_BSINSELLANE7) & M_BSINSELLANE7)
+
+#define S_BSENLANE7    28
+#define V_BSENLANE7(x) ((x) << S_BSENLANE7)
+#define F_BSENLANE7    V_BSENLANE7(1U)
+
+#define S_BSINSELLANE6    25
+#define M_BSINSELLANE6    0x3
+#define V_BSINSELLANE6(x) ((x) << S_BSINSELLANE6)
+#define G_BSINSELLANE6(x) (((x) >> S_BSINSELLANE6) & M_BSINSELLANE6)
+
+#define S_BSENLANE6    24
+#define V_BSENLANE6(x) ((x) << S_BSENLANE6)
+#define F_BSENLANE6    V_BSENLANE6(1U)
+
+#define S_BSINSELLANE5    21
+#define M_BSINSELLANE5    0x3
+#define V_BSINSELLANE5(x) ((x) << S_BSINSELLANE5)
+#define G_BSINSELLANE5(x) (((x) >> S_BSINSELLANE5) & M_BSINSELLANE5)
+
+#define S_BSENLANE5    20
+#define V_BSENLANE5(x) ((x) << S_BSENLANE5)
+#define F_BSENLANE5    V_BSENLANE5(1U)
+
+#define S_BSINSELLANE4    17
+#define M_BSINSELLANE4    0x3
+#define V_BSINSELLANE4(x) ((x) << S_BSINSELLANE4)
+#define G_BSINSELLANE4(x) (((x) >> S_BSINSELLANE4) & M_BSINSELLANE4)
+
+#define S_BSENLANE4    16
+#define V_BSENLANE4(x) ((x) << S_BSENLANE4)
+#define F_BSENLANE4    V_BSENLANE4(1U)
+
+#define S_BSMODEQUAD0    15
+#define V_BSMODEQUAD0(x) ((x) << S_BSMODEQUAD0)
+#define F_BSMODEQUAD0    V_BSMODEQUAD0(1U)
+
+#define S_BSINSELLANE3    13
+#define M_BSINSELLANE3    0x3
+#define V_BSINSELLANE3(x) ((x) << S_BSINSELLANE3)
+#define G_BSINSELLANE3(x) (((x) >> S_BSINSELLANE3) & M_BSINSELLANE3)
+
+#define S_BSENLANE3    12
+#define V_BSENLANE3(x) ((x) << S_BSENLANE3)
+#define F_BSENLANE3    V_BSENLANE3(1U)
+
+#define S_BSINSELLANE2    9
+#define M_BSINSELLANE2    0x3
+#define V_BSINSELLANE2(x) ((x) << S_BSINSELLANE2)
+#define G_BSINSELLANE2(x) (((x) >> S_BSINSELLANE2) & M_BSINSELLANE2)
+
+#define S_BSENLANE2    8
+#define V_BSENLANE2(x) ((x) << S_BSENLANE2)
+#define F_BSENLANE2    V_BSENLANE2(1U)
+
+#define S_BSINSELLANE1    5
+#define M_BSINSELLANE1    0x3
+#define V_BSINSELLANE1(x) ((x) << S_BSINSELLANE1)
+#define G_BSINSELLANE1(x) (((x) >> S_BSINSELLANE1) & M_BSINSELLANE1)
+
+#define S_BSENLANE1    4
+#define V_BSENLANE1(x) ((x) << S_BSENLANE1)
+#define F_BSENLANE1    V_BSENLANE1(1U)
+
+#define S_BSINSELLANE0    1
+#define M_BSINSELLANE0    0x3
+#define V_BSINSELLANE0(x) ((x) << S_BSINSELLANE0)
+#define G_BSINSELLANE0(x) (((x) >> S_BSINSELLANE0) & M_BSINSELLANE0)
+
+#define S_BSENLANE0    0
+#define V_BSENLANE0(x) ((x) << S_BSENLANE0)
+#define F_BSENLANE0    V_BSENLANE0(1U)
+
+/* registers for module MC7_PMRX */
 #define MC7_PMRX_BASE_ADDR 0x100
 
 #define A_MC7_CFG 0x100
 
+#define S_IMPSETUPDATE    14
+#define V_IMPSETUPDATE(x) ((x) << S_IMPSETUPDATE)
+#define F_IMPSETUPDATE    V_IMPSETUPDATE(1U)
+
 #define S_IFEN    13
 #define V_IFEN(x) ((x) << S_IFEN)
 #define F_IFEN    V_IFEN(1U)
 
+#define S_TERM300    12
+#define V_TERM300(x) ((x) << S_TERM300)
+#define F_TERM300    V_TERM300(1U)
+
 #define S_TERM150    11
 #define V_TERM150(x) ((x) << S_TERM150)
 #define F_TERM150    V_TERM150(1U)
@@ -642,6 +2684,10 @@
 #define V_WIDTH(x) ((x) << S_WIDTH)
 #define G_WIDTH(x) (((x) >> S_WIDTH) & M_WIDTH)
 
+#define S_ODTEN    7
+#define V_ODTEN(x) ((x) << S_ODTEN)
+#define F_ODTEN    V_ODTEN(1U)
+
 #define S_BKS    6
 #define V_BKS(x) ((x) << S_BKS)
 #define F_BKS    V_BKS(1U)
@@ -665,94 +2711,234 @@
 
 #define A_MC7_MODE 0x104
 
+#define S_MODE    0
+#define M_MODE    0xffff
+#define V_MODE(x) ((x) << S_MODE)
+#define G_MODE(x) (((x) >> S_MODE) & M_MODE)
+
+#define A_MC7_EXT_MODE1 0x108
+
+#define S_OCDADJUSTMODE    20
+#define V_OCDADJUSTMODE(x) ((x) << S_OCDADJUSTMODE)
+#define F_OCDADJUSTMODE    V_OCDADJUSTMODE(1U)
+
+#define S_OCDCODE    16
+#define M_OCDCODE    0xf
+#define V_OCDCODE(x) ((x) << S_OCDCODE)
+#define G_OCDCODE(x) (((x) >> S_OCDCODE) & M_OCDCODE)
+
+#define S_EXTMODE1    0
+#define M_EXTMODE1    0xffff
+#define V_EXTMODE1(x) ((x) << S_EXTMODE1)
+#define G_EXTMODE1(x) (((x) >> S_EXTMODE1) & M_EXTMODE1)
+
+#define A_MC7_EXT_MODE2 0x10c
+
+#define S_EXTMODE2    0
+#define M_EXTMODE2    0xffff
+#define V_EXTMODE2(x) ((x) << S_EXTMODE2)
+#define G_EXTMODE2(x) (((x) >> S_EXTMODE2) & M_EXTMODE2)
+
+#define A_MC7_EXT_MODE3 0x110
+
+#define S_EXTMODE3    0
+#define M_EXTMODE3    0xffff
+#define V_EXTMODE3(x) ((x) << S_EXTMODE3)
+#define G_EXTMODE3(x) (((x) >> S_EXTMODE3) & M_EXTMODE3)
+
+#define A_MC7_PRE 0x114
+#define A_MC7_REF 0x118
+
+#define S_PREREFDIV    1
+#define M_PREREFDIV    0x3fff
+#define V_PREREFDIV(x) ((x) << S_PREREFDIV)
+#define G_PREREFDIV(x) (((x) >> S_PREREFDIV) & M_PREREFDIV)
+
+#define S_PERREFEN    0
+#define V_PERREFEN(x) ((x) << S_PERREFEN)
+#define F_PERREFEN    V_PERREFEN(1U)
+
+#define A_MC7_DLL 0x11c
+
+#define S_DLLLOCK    31
+#define V_DLLLOCK(x) ((x) << S_DLLLOCK)
+#define F_DLLLOCK    V_DLLLOCK(1U)
+
+#define S_DLLDELTA    24
+#define M_DLLDELTA    0x7f
+#define V_DLLDELTA(x) ((x) << S_DLLDELTA)
+#define G_DLLDELTA(x) (((x) >> S_DLLDELTA) & M_DLLDELTA)
+
+#define S_MANDELTA    3
+#define M_MANDELTA    0x7f
+#define V_MANDELTA(x) ((x) << S_MANDELTA)
+#define G_MANDELTA(x) (((x) >> S_MANDELTA) & M_MANDELTA)
+
+#define S_DLLDELTASEL    2
+#define V_DLLDELTASEL(x) ((x) << S_DLLDELTASEL)
+#define F_DLLDELTASEL    V_DLLDELTASEL(1U)
+
+#define S_DLLENB    1
+#define V_DLLENB(x) ((x) << S_DLLENB)
+#define F_DLLENB    V_DLLENB(1U)
+
+#define S_DLLRST    0
+#define V_DLLRST(x) ((x) << S_DLLRST)
+#define F_DLLRST    V_DLLRST(1U)
+
+#define A_MC7_PARM 0x120
+
+#define S_ACTTOPREDLY    26
+#define M_ACTTOPREDLY    0xf
+#define V_ACTTOPREDLY(x) ((x) << S_ACTTOPREDLY)
+#define G_ACTTOPREDLY(x) (((x) >> S_ACTTOPREDLY) & M_ACTTOPREDLY)
+
+#define S_ACTTORDWRDLY    23
+#define M_ACTTORDWRDLY    0x7
+#define V_ACTTORDWRDLY(x) ((x) << S_ACTTORDWRDLY)
+#define G_ACTTORDWRDLY(x) (((x) >> S_ACTTORDWRDLY) & M_ACTTORDWRDLY)
+
+#define S_PRECYC    20
+#define M_PRECYC    0x7
+#define V_PRECYC(x) ((x) << S_PRECYC)
+#define G_PRECYC(x) (((x) >> S_PRECYC) & M_PRECYC)
+
+#define S_REFCYC    13
+#define M_REFCYC    0x7f
+#define V_REFCYC(x) ((x) << S_REFCYC)
+#define G_REFCYC(x) (((x) >> S_REFCYC) & M_REFCYC)
+
+#define S_BKCYC    8
+#define M_BKCYC    0x1f
+#define V_BKCYC(x) ((x) << S_BKCYC)
+#define G_BKCYC(x) (((x) >> S_BKCYC) & M_BKCYC)
+
+#define S_WRTORDDLY    4
+#define M_WRTORDDLY    0xf
+#define V_WRTORDDLY(x) ((x) << S_WRTORDDLY)
+#define G_WRTORDDLY(x) (((x) >> S_WRTORDDLY) & M_WRTORDDLY)
+
+#define S_RDTOWRDLY    0
+#define M_RDTOWRDLY    0xf
+#define V_RDTOWRDLY(x) ((x) << S_RDTOWRDLY)
+#define G_RDTOWRDLY(x) (((x) >> S_RDTOWRDLY) & M_RDTOWRDLY)
+
+#define A_MC7_HWM_WRR 0x124
+
+#define S_MEM_HWM    26
+#define M_MEM_HWM    0x3f
+#define V_MEM_HWM(x) ((x) << S_MEM_HWM)
+#define G_MEM_HWM(x) (((x) >> S_MEM_HWM) & M_MEM_HWM)
+
+#define S_ULP_HWM    22
+#define M_ULP_HWM    0xf
+#define V_ULP_HWM(x) ((x) << S_ULP_HWM)
+#define G_ULP_HWM(x) (((x) >> S_ULP_HWM) & M_ULP_HWM)
+
+#define S_TOT_RLD_WT    14
+#define M_TOT_RLD_WT    0xff
+#define V_TOT_RLD_WT(x) ((x) << S_TOT_RLD_WT)
+#define G_TOT_RLD_WT(x) (((x) >> S_TOT_RLD_WT) & M_TOT_RLD_WT)
+
+#define S_MEM_RLD_WT    7
+#define M_MEM_RLD_WT    0x7f
+#define V_MEM_RLD_WT(x) ((x) << S_MEM_RLD_WT)
+#define G_MEM_RLD_WT(x) (((x) >> S_MEM_RLD_WT) & M_MEM_RLD_WT)
+
+#define S_ULP_RLD_WT    0
+#define M_ULP_RLD_WT    0x7f
+#define V_ULP_RLD_WT(x) ((x) << S_ULP_RLD_WT)
+#define G_ULP_RLD_WT(x) (((x) >> S_ULP_RLD_WT) & M_ULP_RLD_WT)
+
+#define A_MC7_CAL 0x128
+
 #define S_BUSY    31
 #define V_BUSY(x) ((x) << S_BUSY)
 #define F_BUSY    V_BUSY(1U)
 
-#define S_BUSY    31
-#define V_BUSY(x) ((x) << S_BUSY)
-#define F_BUSY    V_BUSY(1U)
-
-#define A_MC7_EXT_MODE1 0x108
-
-#define A_MC7_EXT_MODE2 0x10c
-
-#define A_MC7_EXT_MODE3 0x110
-
-#define A_MC7_PRE 0x114
-
-#define A_MC7_REF 0x118
-
-#define S_PREREFDIV    1
-#define M_PREREFDIV    0x3fff
-#define V_PREREFDIV(x) ((x) << S_PREREFDIV)
-
-#define S_PERREFEN    0
-#define V_PERREFEN(x) ((x) << S_PERREFEN)
-#define F_PERREFEN    V_PERREFEN(1U)
-
-#define A_MC7_DLL 0x11c
-
-#define S_DLLENB    1
-#define V_DLLENB(x) ((x) << S_DLLENB)
-#define F_DLLENB    V_DLLENB(1U)
-
-#define S_DLLRST    0
-#define V_DLLRST(x) ((x) << S_DLLRST)
-#define F_DLLRST    V_DLLRST(1U)
-
-#define A_MC7_PARM 0x120
-
-#define S_ACTTOPREDLY    26
-#define M_ACTTOPREDLY    0xf
-#define V_ACTTOPREDLY(x) ((x) << S_ACTTOPREDLY)
-
-#define S_ACTTORDWRDLY    23
-#define M_ACTTORDWRDLY    0x7
-#define V_ACTTORDWRDLY(x) ((x) << S_ACTTORDWRDLY)
-
-#define S_PRECYC    20
-#define M_PRECYC    0x7
-#define V_PRECYC(x) ((x) << S_PRECYC)
-
-#define S_REFCYC    13
-#define M_REFCYC    0x7f
-#define V_REFCYC(x) ((x) << S_REFCYC)
-
-#define S_BKCYC    8
-#define M_BKCYC    0x1f
-#define V_BKCYC(x) ((x) << S_BKCYC)
-
-#define S_WRTORDDLY    4
-#define M_WRTORDDLY    0xf
-#define V_WRTORDDLY(x) ((x) << S_WRTORDDLY)
-
-#define S_RDTOWRDLY    0
-#define M_RDTOWRDLY    0xf
-#define V_RDTOWRDLY(x) ((x) << S_RDTOWRDLY)
-
-#define A_MC7_CAL 0x128
-
-#define S_BUSY    31
-#define V_BUSY(x) ((x) << S_BUSY)
-#define F_BUSY    V_BUSY(1U)
-
-#define S_BUSY    31
-#define V_BUSY(x) ((x) << S_BUSY)
-#define F_BUSY    V_BUSY(1U)
-
 #define S_CAL_FAULT    30
 #define V_CAL_FAULT(x) ((x) << S_CAL_FAULT)
 #define F_CAL_FAULT    V_CAL_FAULT(1U)
 
+#define S_PER_CAL_DIV    22
+#define M_PER_CAL_DIV    0xff
+#define V_PER_CAL_DIV(x) ((x) << S_PER_CAL_DIV)
+#define G_PER_CAL_DIV(x) (((x) >> S_PER_CAL_DIV) & M_PER_CAL_DIV)
+
+#define S_PER_CAL_EN    21
+#define V_PER_CAL_EN(x) ((x) << S_PER_CAL_EN)
+#define F_PER_CAL_EN    V_PER_CAL_EN(1U)
+
 #define S_SGL_CAL_EN    20
 #define V_SGL_CAL_EN(x) ((x) << S_SGL_CAL_EN)
 #define F_SGL_CAL_EN    V_SGL_CAL_EN(1U)
 
+#define S_IMP_UPD_MODE    19
+#define V_IMP_UPD_MODE(x) ((x) << S_IMP_UPD_MODE)
+#define F_IMP_UPD_MODE    V_IMP_UPD_MODE(1U)
+
+#define S_IMP_SEL    18
+#define V_IMP_SEL(x) ((x) << S_IMP_SEL)
+#define F_IMP_SEL    V_IMP_SEL(1U)
+
+#define S_IMP_MAN_PD    15
+#define M_IMP_MAN_PD    0x7
+#define V_IMP_MAN_PD(x) ((x) << S_IMP_MAN_PD)
+#define G_IMP_MAN_PD(x) (((x) >> S_IMP_MAN_PD) & M_IMP_MAN_PD)
+
+#define S_IMP_MAN_PU    12
+#define M_IMP_MAN_PU    0x7
+#define V_IMP_MAN_PU(x) ((x) << S_IMP_MAN_PU)
+#define G_IMP_MAN_PU(x) (((x) >> S_IMP_MAN_PU) & M_IMP_MAN_PU)
+
+#define S_IMP_CAL_PD    9
+#define M_IMP_CAL_PD    0x7
+#define V_IMP_CAL_PD(x) ((x) << S_IMP_CAL_PD)
+#define G_IMP_CAL_PD(x) (((x) >> S_IMP_CAL_PD) & M_IMP_CAL_PD)
+
+#define S_IMP_CAL_PU    6
+#define M_IMP_CAL_PU    0x7
+#define V_IMP_CAL_PU(x) ((x) << S_IMP_CAL_PU)
+#define G_IMP_CAL_PU(x) (((x) >> S_IMP_CAL_PU) & M_IMP_CAL_PU)
+
+#define S_IMP_SET_PD    3
+#define M_IMP_SET_PD    0x7
+#define V_IMP_SET_PD(x) ((x) << S_IMP_SET_PD)
+#define G_IMP_SET_PD(x) (((x) >> S_IMP_SET_PD) & M_IMP_SET_PD)
+
+#define S_IMP_SET_PU    0
+#define M_IMP_SET_PU    0x7
+#define V_IMP_SET_PU(x) ((x) << S_IMP_SET_PU)
+#define G_IMP_SET_PU(x) (((x) >> S_IMP_SET_PU) & M_IMP_SET_PU)
+
 #define A_MC7_ERR_ADDR 0x12c
 
+#define S_ERRADDRESS    3
+#define M_ERRADDRESS    0x1fffffff
+#define V_ERRADDRESS(x) ((x) << S_ERRADDRESS)
+#define G_ERRADDRESS(x) (((x) >> S_ERRADDRESS) & M_ERRADDRESS)
+
+#define S_ERRAGENT    1
+#define M_ERRAGENT    0x3
+#define V_ERRAGENT(x) ((x) << S_ERRAGENT)
+#define G_ERRAGENT(x) (((x) >> S_ERRAGENT) & M_ERRAGENT)
+
+#define S_ERROP    0
+#define V_ERROP(x) ((x) << S_ERROP)
+#define F_ERROP    V_ERROP(1U)
+
 #define A_MC7_ECC 0x130
 
+#define S_UECNT    10
+#define M_UECNT    0xff
+#define V_UECNT(x) ((x) << S_UECNT)
+#define G_UECNT(x) (((x) >> S_UECNT) & M_UECNT)
+
+#define S_CECNT    2
+#define M_CECNT    0xff
+#define V_CECNT(x) ((x) << S_CECNT)
+#define G_CECNT(x) (((x) >> S_CECNT) & M_CECNT)
+
 #define S_ECCCHKEN    1
 #define V_ECCCHKEN(x) ((x) << S_ECCCHKEN)
 #define F_ECCCHKEN    V_ECCCHKEN(1U)
@@ -762,59 +2948,65 @@
 #define F_ECCGENEN    V_ECCGENEN(1U)
 
 #define A_MC7_CE_ADDR 0x134
-
 #define A_MC7_CE_DATA0 0x138
-
 #define A_MC7_CE_DATA1 0x13c
-
 #define A_MC7_CE_DATA2 0x140
 
 #define S_DATA    0
 #define M_DATA    0xff
-
+#define V_DATA(x) ((x) << S_DATA)
 #define G_DATA(x) (((x) >> S_DATA) & M_DATA)
 
 #define A_MC7_UE_ADDR 0x144
-
 #define A_MC7_UE_DATA0 0x148
-
 #define A_MC7_UE_DATA1 0x14c
-
 #define A_MC7_UE_DATA2 0x150
-
 #define A_MC7_BD_ADDR 0x154
 
 #define S_ADDR    3
-
 #define M_ADDR    0x1fffffff
+#define V_ADDR(x) ((x) << S_ADDR)
+#define G_ADDR(x) (((x) >> S_ADDR) & M_ADDR)
 
 #define A_MC7_BD_DATA0 0x158
-
 #define A_MC7_BD_DATA1 0x15c
-
+#define A_MC7_BD_DATA2 0x160
 #define A_MC7_BD_OP 0x164
 
 #define S_OP    0
-
 #define V_OP(x) ((x) << S_OP)
 #define F_OP    V_OP(1U)
 
-#define F_OP    V_OP(1U)
-#define A_SF_OP 0x6dc
-
 #define A_MC7_BIST_ADDR_BEG 0x168
 
+#define S_ADDRBEG    5
+#define M_ADDRBEG    0x7ffffff
+#define V_ADDRBEG(x) ((x) << S_ADDRBEG)
+#define G_ADDRBEG(x) (((x) >> S_ADDRBEG) & M_ADDRBEG)
+
 #define A_MC7_BIST_ADDR_END 0x16c
 
+#define S_ADDREND    5
+#define M_ADDREND    0x7ffffff
+#define V_ADDREND(x) ((x) << S_ADDREND)
+#define G_ADDREND(x) (((x) >> S_ADDREND) & M_ADDREND)
+
 #define A_MC7_BIST_DATA 0x170
-
 #define A_MC7_BIST_OP 0x174
 
+#define S_GAP    4
+#define M_GAP    0x1f
+#define V_GAP(x) ((x) << S_GAP)
+#define G_GAP(x) (((x) >> S_GAP) & M_GAP)
+
 #define S_CONT    3
 #define V_CONT(x) ((x) << S_CONT)
 #define F_CONT    V_CONT(1U)
 
-#define F_CONT    V_CONT(1U)
+#define S_DATAPAT    1
+#define M_DATAPAT    0x3
+#define V_DATAPAT(x) ((x) << S_DATAPAT)
+#define G_DATAPAT(x) (((x) >> S_DATAPAT) & M_DATAPAT)
 
 #define A_MC7_INT_ENABLE 0x178
 
@@ -824,9 +3016,7 @@
 
 #define S_PE    2
 #define M_PE    0x7fff
-
 #define V_PE(x) ((x) << S_PE)
-
 #define G_PE(x) (((x) >> S_PE) & M_PE)
 
 #define S_UE    1
@@ -839,20 +3029,65 @@
 
 #define A_MC7_INT_CAUSE 0x17c
 
+/* registers for module MC7_PMTX */
 #define MC7_PMTX_BASE_ADDR 0x180
 
+/* registers for module MC7_CM */
 #define MC7_CM_BASE_ADDR 0x200
 
+/* registers for module CIM */
+#define CIM_BASE_ADDR 0x280
+
 #define A_CIM_BOOT_CFG 0x280
 
 #define S_BOOTADDR    2
 #define M_BOOTADDR    0x3fffffff
 #define V_BOOTADDR(x) ((x) << S_BOOTADDR)
+#define G_BOOTADDR(x) (((x) >> S_BOOTADDR) & M_BOOTADDR)
+
+#define S_BOOTSDRAM    1
+#define V_BOOTSDRAM(x) ((x) << S_BOOTSDRAM)
+#define F_BOOTSDRAM    V_BOOTSDRAM(1U)
+
+#define S_UPCRST    0
+#define V_UPCRST(x) ((x) << S_UPCRST)
+#define F_UPCRST    V_UPCRST(1U)
+
+#define A_CIM_FLASH_BASE_ADDR 0x284
+
+#define S_FLASHBASEADDR    2
+#define M_FLASHBASEADDR    0x3fffff
+#define V_FLASHBASEADDR(x) ((x) << S_FLASHBASEADDR)
+#define G_FLASHBASEADDR(x) (((x) >> S_FLASHBASEADDR) & M_FLASHBASEADDR)
+
+#define A_CIM_FLASH_ADDR_SIZE 0x288
+
+#define S_FLASHADDRSIZE    2
+#define M_FLASHADDRSIZE    0x3fffff
+#define V_FLASHADDRSIZE(x) ((x) << S_FLASHADDRSIZE)
+#define G_FLASHADDRSIZE(x) (((x) >> S_FLASHADDRSIZE) & M_FLASHADDRSIZE)
 
 #define A_CIM_SDRAM_BASE_ADDR 0x28c
 
+#define S_SDRAMBASEADDR    2
+#define M_SDRAMBASEADDR    0x3fffffff
+#define V_SDRAMBASEADDR(x) ((x) << S_SDRAMBASEADDR)
+#define G_SDRAMBASEADDR(x) (((x) >> S_SDRAMBASEADDR) & M_SDRAMBASEADDR)
+
 #define A_CIM_SDRAM_ADDR_SIZE 0x290
 
+#define S_SDRAMADDRSIZE    2
+#define M_SDRAMADDRSIZE    0x3fffffff
+#define V_SDRAMADDRSIZE(x) ((x) << S_SDRAMADDRSIZE)
+#define G_SDRAMADDRSIZE(x) (((x) >> S_SDRAMADDRSIZE) & M_SDRAMADDRSIZE)
+
+#define A_CIM_UP_SPARE_INT 0x294
+
+#define S_UPSPAREINT    0
+#define M_UPSPAREINT    0x7
+#define V_UPSPAREINT(x) ((x) << S_UPSPAREINT)
+#define G_UPSPAREINT(x) (((x) >> S_UPSPAREINT) & M_UPSPAREINT)
+
 #define A_CIM_HOST_INT_ENABLE 0x298
 
 #define S_DTAGPARERR    28
@@ -903,8 +3138,84 @@
 #define V_DRAMPARERR(x) ((x) << S_DRAMPARERR)
 #define F_DRAMPARERR    V_DRAMPARERR(1U)
 
+#define S_TIMER1INTEN    15
+#define V_TIMER1INTEN(x) ((x) << S_TIMER1INTEN)
+#define F_TIMER1INTEN    V_TIMER1INTEN(1U)
+
+#define S_TIMER0INTEN    14
+#define V_TIMER0INTEN(x) ((x) << S_TIMER0INTEN)
+#define F_TIMER0INTEN    V_TIMER0INTEN(1U)
+
+#define S_PREFDROPINTEN    13
+#define V_PREFDROPINTEN(x) ((x) << S_PREFDROPINTEN)
+#define F_PREFDROPINTEN    V_PREFDROPINTEN(1U)
+
+#define S_BLKWRPLINTEN    12
+#define V_BLKWRPLINTEN(x) ((x) << S_BLKWRPLINTEN)
+#define F_BLKWRPLINTEN    V_BLKWRPLINTEN(1U)
+
+#define S_BLKRDPLINTEN    11
+#define V_BLKRDPLINTEN(x) ((x) << S_BLKRDPLINTEN)
+#define F_BLKRDPLINTEN    V_BLKRDPLINTEN(1U)
+
+#define S_BLKWRCTLINTEN    10
+#define V_BLKWRCTLINTEN(x) ((x) << S_BLKWRCTLINTEN)
+#define F_BLKWRCTLINTEN    V_BLKWRCTLINTEN(1U)
+
+#define S_BLKRDCTLINTEN    9
+#define V_BLKRDCTLINTEN(x) ((x) << S_BLKRDCTLINTEN)
+#define F_BLKRDCTLINTEN    V_BLKRDCTLINTEN(1U)
+
+#define S_BLKWRFLASHINTEN    8
+#define V_BLKWRFLASHINTEN(x) ((x) << S_BLKWRFLASHINTEN)
+#define F_BLKWRFLASHINTEN    V_BLKWRFLASHINTEN(1U)
+
+#define S_BLKRDFLASHINTEN    7
+#define V_BLKRDFLASHINTEN(x) ((x) << S_BLKRDFLASHINTEN)
+#define F_BLKRDFLASHINTEN    V_BLKRDFLASHINTEN(1U)
+
+#define S_SGLWRFLASHINTEN    6
+#define V_SGLWRFLASHINTEN(x) ((x) << S_SGLWRFLASHINTEN)
+#define F_SGLWRFLASHINTEN    V_SGLWRFLASHINTEN(1U)
+
+#define S_WRBLKFLASHINTEN    5
+#define V_WRBLKFLASHINTEN(x) ((x) << S_WRBLKFLASHINTEN)
+#define F_WRBLKFLASHINTEN    V_WRBLKFLASHINTEN(1U)
+
+#define S_BLKWRBOOTINTEN    4
+#define V_BLKWRBOOTINTEN(x) ((x) << S_BLKWRBOOTINTEN)
+#define F_BLKWRBOOTINTEN    V_BLKWRBOOTINTEN(1U)
+
+#define S_BLKRDBOOTINTEN    3
+#define V_BLKRDBOOTINTEN(x) ((x) << S_BLKRDBOOTINTEN)
+#define F_BLKRDBOOTINTEN    V_BLKRDBOOTINTEN(1U)
+
+#define S_FLASHRANGEINTEN    2
+#define V_FLASHRANGEINTEN(x) ((x) << S_FLASHRANGEINTEN)
+#define F_FLASHRANGEINTEN    V_FLASHRANGEINTEN(1U)
+
+#define S_SDRAMRANGEINTEN    1
+#define V_SDRAMRANGEINTEN(x) ((x) << S_SDRAMRANGEINTEN)
+#define F_SDRAMRANGEINTEN    V_SDRAMRANGEINTEN(1U)
+
+#define S_RSVDSPACEINTEN    0
+#define V_RSVDSPACEINTEN(x) ((x) << S_RSVDSPACEINTEN)
+#define F_RSVDSPACEINTEN    V_RSVDSPACEINTEN(1U)
+
 #define A_CIM_HOST_INT_CAUSE 0x29c
 
+#define S_TIMER1INT    15
+#define V_TIMER1INT(x) ((x) << S_TIMER1INT)
+#define F_TIMER1INT    V_TIMER1INT(1U)
+
+#define S_TIMER0INT    14
+#define V_TIMER0INT(x) ((x) << S_TIMER0INT)
+#define F_TIMER0INT    V_TIMER0INT(1U)
+
+#define S_PREFDROPINT    13
+#define V_PREFDROPINT(x) ((x) << S_PREFDROPINT)
+#define F_PREFDROPINT    V_PREFDROPINT(1U)
+
 #define S_BLKWRPLINT    12
 #define V_BLKWRPLINT(x) ((x) << S_BLKWRPLINT)
 #define F_BLKWRPLINT    V_BLKWRPLINT(1U)
@@ -941,6 +3252,10 @@
 #define V_BLKWRBOOTINT(x) ((x) << S_BLKWRBOOTINT)
 #define F_BLKWRBOOTINT    V_BLKWRBOOTINT(1U)
 
+#define S_BLKRDBOOTINT    3
+#define V_BLKRDBOOTINT(x) ((x) << S_BLKRDBOOTINT)
+#define F_BLKRDBOOTINT    V_BLKRDBOOTINT(1U)
+
 #define S_FLASHRANGEINT    2
 #define V_FLASHRANGEINT(x) ((x) << S_FLASHRANGEINT)
 #define F_FLASHRANGEINT    V_FLASHRANGEINT(1U)
@@ -953,14 +3268,66 @@
 #define V_RSVDSPACEINT(x) ((x) << S_RSVDSPACEINT)
 #define F_RSVDSPACEINT    V_RSVDSPACEINT(1U)
 
-#define A_CIM_HOST_ACC_CTRL 0x2b0
+#define A_CIM_UP_INT_ENABLE 0x2a0
+
+#define S_MSTPLINTEN    16
+#define V_MSTPLINTEN(x) ((x) << S_MSTPLINTEN)
+#define F_MSTPLINTEN    V_MSTPLINTEN(1U)
+
+#define A_CIM_UP_INT_CAUSE 0x2a4
+
+#define S_MSTPLINT    16
+#define V_MSTPLINT(x) ((x) << S_MSTPLINT)
+#define F_MSTPLINT    V_MSTPLINT(1U)
+
+#define A_CIM_IBQ_FULLA_THRSH 0x2a8
+
+#define S_IBQ0FULLTHRSH    0
+#define M_IBQ0FULLTHRSH    0x1ff
+#define V_IBQ0FULLTHRSH(x) ((x) << S_IBQ0FULLTHRSH)
+#define G_IBQ0FULLTHRSH(x) (((x) >> S_IBQ0FULLTHRSH) & M_IBQ0FULLTHRSH)
+
+#define S_IBQ1FULLTHRSH    16
+#define M_IBQ1FULLTHRSH    0x1ff
+#define V_IBQ1FULLTHRSH(x) ((x) << S_IBQ1FULLTHRSH)
+#define G_IBQ1FULLTHRSH(x) (((x) >> S_IBQ1FULLTHRSH) & M_IBQ1FULLTHRSH)
+
+#define A_CIM_IBQ_FULLB_THRSH 0x2ac
+
+#define S_IBQ2FULLTHRSH    0
+#define M_IBQ2FULLTHRSH    0x1ff
+#define V_IBQ2FULLTHRSH(x) ((x) << S_IBQ2FULLTHRSH)
+#define G_IBQ2FULLTHRSH(x) (((x) >> S_IBQ2FULLTHRSH) & M_IBQ2FULLTHRSH)
+
+#define S_IBQ3FULLTHRSH    16
+#define M_IBQ3FULLTHRSH    0x1ff
+#define V_IBQ3FULLTHRSH(x) ((x) << S_IBQ3FULLTHRSH)
+#define G_IBQ3FULLTHRSH(x) (((x) >> S_IBQ3FULLTHRSH) & M_IBQ3FULLTHRSH)
+
+#define A_CIM_CTL_BASE			0x2000
+#define A_CIM_HOST_ACC_CTRL		0x2b0
+#define A_CIM_CTL_CONFIG		0x00
+#define CIM_CTL_CONFIG_TIMER0_EN	(1 << 3)
+#define CIM_CTL_CONFIG_GLB_TIMER_EN	(1 << 1)
+#define A_CIM_CTL_GLB_TIMER		0x94
+#define A_CIM_CTL_TIMER0		0x98
+#define A_CIM_HOST_INTR_EN		0x298
+#define A_CIM_HOST_INTR_CAUSE		0x29c
 
 #define S_HOSTBUSY    17
 #define V_HOSTBUSY(x) ((x) << S_HOSTBUSY)
 #define F_HOSTBUSY    V_HOSTBUSY(1U)
 
+#define S_HOSTWRITE    16
+#define V_HOSTWRITE(x) ((x) << S_HOSTWRITE)
+#define F_HOSTWRITE    V_HOSTWRITE(1U)
+
+#define S_HOSTADDR    0
+#define M_HOSTADDR    0xffff
+#define V_HOSTADDR(x) ((x) << S_HOSTADDR)
+#define G_HOSTADDR(x) (((x) >> S_HOSTADDR) & M_HOSTADDR)
+
 #define A_CIM_HOST_ACC_DATA 0x2b4
-
 #define A_CIM_IBQ_DBG_CFG 0x2c0
 
 #define S_IBQDBGADDR    16
@@ -985,7 +3352,87 @@
 #define V_IBQDBGEN(x) ((x) << S_IBQDBGEN)
 #define F_IBQDBGEN    V_IBQDBGEN(1U)
 
+#define A_CIM_OBQ_DBG_CFG 0x2c4
+
+#define S_OBQDBGADDR    16
+#define M_OBQDBGADDR    0x1ff
+#define V_OBQDBGADDR(x) ((x) << S_OBQDBGADDR)
+#define G_OBQDBGADDR(x) (((x) >> S_OBQDBGADDR) & M_OBQDBGADDR)
+
+#define S_OBQDBGQID    3
+#define M_OBQDBGQID    0x3
+#define V_OBQDBGQID(x) ((x) << S_OBQDBGQID)
+#define G_OBQDBGQID(x) (((x) >> S_OBQDBGQID) & M_OBQDBGQID)
+
+#define S_OBQDBGWR    2
+#define V_OBQDBGWR(x) ((x) << S_OBQDBGWR)
+#define F_OBQDBGWR    V_OBQDBGWR(1U)
+
+#define S_OBQDBGBUSY    1
+#define V_OBQDBGBUSY(x) ((x) << S_OBQDBGBUSY)
+#define F_OBQDBGBUSY    V_OBQDBGBUSY(1U)
+
+#define S_OBQDBGEN    0
+#define V_OBQDBGEN(x) ((x) << S_OBQDBGEN)
+#define F_OBQDBGEN    V_OBQDBGEN(1U)
+
 #define A_CIM_IBQ_DBG_DATA 0x2c8
+#define A_CIM_OBQ_DBG_DATA 0x2cc
+#define A_CIM_CDEBUGDATA 0x2d0
+
+#define S_CDEBUGDATAH    16
+#define M_CDEBUGDATAH    0xffff
+#define V_CDEBUGDATAH(x) ((x) << S_CDEBUGDATAH)
+#define G_CDEBUGDATAH(x) (((x) >> S_CDEBUGDATAH) & M_CDEBUGDATAH)
+
+#define S_CDEBUGDATAL    0
+#define M_CDEBUGDATAL    0xffff
+#define V_CDEBUGDATAL(x) ((x) << S_CDEBUGDATAL)
+#define G_CDEBUGDATAL(x) (((x) >> S_CDEBUGDATAL) & M_CDEBUGDATAL)
+
+#define A_CIM_DEBUGCFG 0x2e0
+
+#define S_POLADBGRDPTR    23
+#define M_POLADBGRDPTR    0x1ff
+#define V_POLADBGRDPTR(x) ((x) << S_POLADBGRDPTR)
+#define G_POLADBGRDPTR(x) (((x) >> S_POLADBGRDPTR) & M_POLADBGRDPTR)
+
+#define S_PILADBGRDPTR    14
+#define M_PILADBGRDPTR    0x1ff
+#define V_PILADBGRDPTR(x) ((x) << S_PILADBGRDPTR)
+#define G_PILADBGRDPTR(x) (((x) >> S_PILADBGRDPTR) & M_PILADBGRDPTR)
+
+#define S_CIM_LADBGEN    12
+#define V_CIM_LADBGEN(x) ((x) << S_CIM_LADBGEN)
+#define F_CIM_LADBGEN    V_CIM_LADBGEN(1U)
+
+#define S_DEBUGSELHI    5
+#define M_DEBUGSELHI    0x1f
+#define V_DEBUGSELHI(x) ((x) << S_DEBUGSELHI)
+#define G_DEBUGSELHI(x) (((x) >> S_DEBUGSELHI) & M_DEBUGSELHI)
+
+#define S_DEBUGSELLO    0
+#define M_DEBUGSELLO    0x1f
+#define V_DEBUGSELLO(x) ((x) << S_DEBUGSELLO)
+#define G_DEBUGSELLO(x) (((x) >> S_DEBUGSELLO) & M_DEBUGSELLO)
+
+#define A_CIM_DEBUGSTS 0x2e4
+
+#define S_POLADBGWRPTR    16
+#define M_POLADBGWRPTR    0x1ff
+#define V_POLADBGWRPTR(x) ((x) << S_POLADBGWRPTR)
+#define G_POLADBGWRPTR(x) (((x) >> S_POLADBGWRPTR) & M_POLADBGWRPTR)
+
+#define S_PILADBGWRPTR    0
+#define M_PILADBGWRPTR    0x1ff
+#define V_PILADBGWRPTR(x) ((x) << S_PILADBGWRPTR)
+#define G_PILADBGWRPTR(x) (((x) >> S_PILADBGWRPTR) & M_PILADBGWRPTR)
+
+#define A_CIM_PO_LA_DEBUGDATA 0x2e8
+#define A_CIM_PI_LA_DEBUGDATA 0x2ec
+
+/* registers for module TP1 */
+#define TP1_BASE_ADDR 0x300
 
 #define A_TP_IN_CONFIG 0x300
 
@@ -997,30 +3444,153 @@
 #define V_TXFBARBPRIO(x) ((x) << S_TXFBARBPRIO)
 #define F_TXFBARBPRIO    V_TXFBARBPRIO(1U)
 
+#define S_DBMAXOPCNT    16
+#define M_DBMAXOPCNT    0xff
+#define V_DBMAXOPCNT(x) ((x) << S_DBMAXOPCNT)
+#define G_DBMAXOPCNT(x) (((x) >> S_DBMAXOPCNT) & M_DBMAXOPCNT)
+
+#define S_IPV6ENABLE    15
+#define V_IPV6ENABLE(x) ((x) << S_IPV6ENABLE)
+#define F_IPV6ENABLE    V_IPV6ENABLE(1U)
+
 #define S_NICMODE    14
 #define V_NICMODE(x) ((x) << S_NICMODE)
 #define F_NICMODE    V_NICMODE(1U)
 
-#define F_NICMODE    V_NICMODE(1U)
-
-#define S_IPV6ENABLE    15
-#define V_IPV6ENABLE(x) ((x) << S_IPV6ENABLE)
-#define F_IPV6ENABLE    V_IPV6ENABLE(1U)
+#define S_ECHECKSUMCHECKTCP    13
+#define V_ECHECKSUMCHECKTCP(x) ((x) << S_ECHECKSUMCHECKTCP)
+#define F_ECHECKSUMCHECKTCP    V_ECHECKSUMCHECKTCP(1U)
+
+#define S_ECHECKSUMCHECKIP    12
+#define V_ECHECKSUMCHECKIP(x) ((x) << S_ECHECKSUMCHECKIP)
+#define F_ECHECKSUMCHECKIP    V_ECHECKSUMCHECKIP(1U)
+
+#define S_ECPL    10
+#define V_ECPL(x) ((x) << S_ECPL)
+#define F_ECPL    V_ECPL(1U)
+
+#define S_EETHERNET    8
+#define V_EETHERNET(x) ((x) << S_EETHERNET)
+#define F_EETHERNET    V_EETHERNET(1U)
+
+#define S_ETUNNEL    7
+#define V_ETUNNEL(x) ((x) << S_ETUNNEL)
+#define F_ETUNNEL    V_ETUNNEL(1U)
+
+#define S_CCHECKSUMCHECKTCP    6
+#define V_CCHECKSUMCHECKTCP(x) ((x) << S_CCHECKSUMCHECKTCP)
+#define F_CCHECKSUMCHECKTCP    V_CCHECKSUMCHECKTCP(1U)
+
+#define S_CCHECKSUMCHECKIP    5
+#define V_CCHECKSUMCHECKIP(x) ((x) << S_CCHECKSUMCHECKIP)
+#define F_CCHECKSUMCHECKIP    V_CCHECKSUMCHECKIP(1U)
+
+#define S_CCPL    3
+#define V_CCPL(x) ((x) << S_CCPL)
+#define F_CCPL    V_CCPL(1U)
+
+#define S_CETHERNET    1
+#define V_CETHERNET(x) ((x) << S_CETHERNET)
+#define F_CETHERNET    V_CETHERNET(1U)
+
+#define S_CTUNNEL    0
+#define V_CTUNNEL(x) ((x) << S_CTUNNEL)
+#define F_CTUNNEL    V_CTUNNEL(1U)
 
 #define A_TP_OUT_CONFIG 0x304
 
+#define S_IPIDSPLITMODE    16
+#define V_IPIDSPLITMODE(x) ((x) << S_IPIDSPLITMODE)
+#define F_IPIDSPLITMODE    V_IPIDSPLITMODE(1U)
+
+#define S_VLANEXTRACTIONENABLE2NDPORT    13
+#define V_VLANEXTRACTIONENABLE2NDPORT(x) ((x) << S_VLANEXTRACTIONENABLE2NDPORT)
+#define F_VLANEXTRACTIONENABLE2NDPORT    V_VLANEXTRACTIONENABLE2NDPORT(1U)
+
 #define S_VLANEXTRACTIONENABLE    12
+#define V_VLANEXTRACTIONENABLE(x) ((x) << S_VLANEXTRACTIONENABLE)
+#define F_VLANEXTRACTIONENABLE    V_VLANEXTRACTIONENABLE(1U)
+
+#define S_ECHECKSUMGENERATETCP    11
+#define V_ECHECKSUMGENERATETCP(x) ((x) << S_ECHECKSUMGENERATETCP)
+#define F_ECHECKSUMGENERATETCP    V_ECHECKSUMGENERATETCP(1U)
+
+#define S_ECHECKSUMGENERATEIP    10
+#define V_ECHECKSUMGENERATEIP(x) ((x) << S_ECHECKSUMGENERATEIP)
+#define F_ECHECKSUMGENERATEIP    V_ECHECKSUMGENERATEIP(1U)
+
+#define S_OUT_ECPL    8
+#define V_OUT_ECPL(x) ((x) << S_OUT_ECPL)
+#define F_OUT_ECPL    V_OUT_ECPL(1U)
+
+#define S_OUT_EETHERNET    6
+#define V_OUT_EETHERNET(x) ((x) << S_OUT_EETHERNET)
+#define F_OUT_EETHERNET    V_OUT_EETHERNET(1U)
+
+#define S_CCHECKSUMGENERATETCP    5
+#define V_CCHECKSUMGENERATETCP(x) ((x) << S_CCHECKSUMGENERATETCP)
+#define F_CCHECKSUMGENERATETCP    V_CCHECKSUMGENERATETCP(1U)
+
+#define S_CCHECKSUMGENERATEIP    4
+#define V_CCHECKSUMGENERATEIP(x) ((x) << S_CCHECKSUMGENERATEIP)
+#define F_CCHECKSUMGENERATEIP    V_CCHECKSUMGENERATEIP(1U)
+
+#define S_OUT_CCPL    2
+#define V_OUT_CCPL(x) ((x) << S_OUT_CCPL)
+#define F_OUT_CCPL    V_OUT_CCPL(1U)
+
+#define S_OUT_CETHERNET    0
+#define V_OUT_CETHERNET(x) ((x) << S_OUT_CETHERNET)
+#define F_OUT_CETHERNET    V_OUT_CETHERNET(1U)
 
 #define A_TP_GLOBAL_CONFIG 0x308
 
+#define S_SYNCOOKIEPARAMS    26
+#define M_SYNCOOKIEPARAMS    0x3f
+#define V_SYNCOOKIEPARAMS(x) ((x) << S_SYNCOOKIEPARAMS)
+#define G_SYNCOOKIEPARAMS(x) (((x) >> S_SYNCOOKIEPARAMS) & M_SYNCOOKIEPARAMS)
+
+#define S_RXFLOWCONTROLDISABLE    25
+#define V_RXFLOWCONTROLDISABLE(x) ((x) << S_RXFLOWCONTROLDISABLE)
+#define F_RXFLOWCONTROLDISABLE    V_RXFLOWCONTROLDISABLE(1U)
+
 #define S_TXPACINGENABLE    24
 #define V_TXPACINGENABLE(x) ((x) << S_TXPACINGENABLE)
 #define F_TXPACINGENABLE    V_TXPACINGENABLE(1U)
 
+#define S_ATTACKFILTERENABLE    23
+#define V_ATTACKFILTERENABLE(x) ((x) << S_ATTACKFILTERENABLE)
+#define F_ATTACKFILTERENABLE    V_ATTACKFILTERENABLE(1U)
+
+#define S_SYNCOOKIENOOPTIONS    22
+#define V_SYNCOOKIENOOPTIONS(x) ((x) << S_SYNCOOKIENOOPTIONS)
+#define F_SYNCOOKIENOOPTIONS    V_SYNCOOKIENOOPTIONS(1U)
+
+#define S_PROTECTEDMODE    21
+#define V_PROTECTEDMODE(x) ((x) << S_PROTECTEDMODE)
+#define F_PROTECTEDMODE    V_PROTECTEDMODE(1U)
+
+#define S_PINGDROP    20
+#define V_PINGDROP(x) ((x) << S_PINGDROP)
+#define F_PINGDROP    V_PINGDROP(1U)
+
+#define S_FRAGMENTDROP    19
+#define V_FRAGMENTDROP(x) ((x) << S_FRAGMENTDROP)
+#define F_FRAGMENTDROP    V_FRAGMENTDROP(1U)
+
+#define S_FIVETUPLELOOKUP    17
+#define M_FIVETUPLELOOKUP    0x3
+#define V_FIVETUPLELOOKUP(x) ((x) << S_FIVETUPLELOOKUP)
+#define G_FIVETUPLELOOKUP(x) (((x) >> S_FIVETUPLELOOKUP) & M_FIVETUPLELOOKUP)
+
 #define S_PATHMTU    15
 #define V_PATHMTU(x) ((x) << S_PATHMTU)
 #define F_PATHMTU    V_PATHMTU(1U)
 
+#define S_IPIDENTSPLIT    14
+#define V_IPIDENTSPLIT(x) ((x) << S_IPIDENTSPLIT)
+#define F_IPIDENTSPLIT    V_IPIDENTSPLIT(1U)
+
 #define S_IPCHECKSUMOFFLOAD    13
 #define V_IPCHECKSUMOFFLOAD(x) ((x) << S_IPCHECKSUMOFFLOAD)
 #define F_IPCHECKSUMOFFLOAD    V_IPCHECKSUMOFFLOAD(1U)
@@ -1033,83 +3603,133 @@
 #define V_TCPCHECKSUMOFFLOAD(x) ((x) << S_TCPCHECKSUMOFFLOAD)
 #define F_TCPCHECKSUMOFFLOAD    V_TCPCHECKSUMOFFLOAD(1U)
 
+#define S_QOSMAPPING    10
+#define V_QOSMAPPING(x) ((x) << S_QOSMAPPING)
+#define F_QOSMAPPING    V_QOSMAPPING(1U)
+
+#define S_TCAMSERVERUSE    8
+#define M_TCAMSERVERUSE    0x3
+#define V_TCAMSERVERUSE(x) ((x) << S_TCAMSERVERUSE)
+#define G_TCAMSERVERUSE(x) (((x) >> S_TCAMSERVERUSE) & M_TCAMSERVERUSE)
+
 #define S_IPTTL    0
 #define M_IPTTL    0xff
 #define V_IPTTL(x) ((x) << S_IPTTL)
+#define G_IPTTL(x) (((x) >> S_IPTTL) & M_IPTTL)
+
+#define A_TP_GLOBAL_RX_CREDIT 0x30c
+#define A_TP_CMM_SIZE 0x310
+
+#define S_CMMEMMGRSIZE    0
+#define M_CMMEMMGRSIZE    0xfffffff
+#define V_CMMEMMGRSIZE(x) ((x) << S_CMMEMMGRSIZE)
+#define G_CMMEMMGRSIZE(x) (((x) >> S_CMMEMMGRSIZE) & M_CMMEMMGRSIZE)
 
 #define A_TP_CMM_MM_BASE 0x314
 
+#define S_CMMEMMGRBASE    0
+#define M_CMMEMMGRBASE    0xfffffff
+#define V_CMMEMMGRBASE(x) ((x) << S_CMMEMMGRBASE)
+#define G_CMMEMMGRBASE(x) (((x) >> S_CMMEMMGRBASE) & M_CMMEMMGRBASE)
+
 #define A_TP_CMM_TIMER_BASE 0x318
 
 #define S_CMTIMERMAXNUM    28
 #define M_CMTIMERMAXNUM    0x3
 #define V_CMTIMERMAXNUM(x) ((x) << S_CMTIMERMAXNUM)
+#define G_CMTIMERMAXNUM(x) (((x) >> S_CMTIMERMAXNUM) & M_CMTIMERMAXNUM)
+
+#define S_CMTIMERBASE    0
+#define M_CMTIMERBASE    0xfffffff
+#define V_CMTIMERBASE(x) ((x) << S_CMTIMERBASE)
+#define G_CMTIMERBASE(x) (((x) >> S_CMTIMERBASE) & M_CMTIMERBASE)
 
 #define A_TP_PMM_SIZE 0x31c
 
+#define S_PMSIZE    0
+#define M_PMSIZE    0xfffffff
+#define V_PMSIZE(x) ((x) << S_PMSIZE)
+#define G_PMSIZE(x) (((x) >> S_PMSIZE) & M_PMSIZE)
+
 #define A_TP_PMM_TX_BASE 0x320
-
+#define A_TP_PMM_DEFRAG_BASE 0x324
 #define A_TP_PMM_RX_BASE 0x328
-
 #define A_TP_PMM_RX_PAGE_SIZE 0x32c
-
 #define A_TP_PMM_RX_MAX_PAGE 0x330
 
+#define S_PMRXMAXPAGE    0
+#define M_PMRXMAXPAGE    0x1fffff
+#define V_PMRXMAXPAGE(x) ((x) << S_PMRXMAXPAGE)
+#define G_PMRXMAXPAGE(x) (((x) >> S_PMRXMAXPAGE) & M_PMRXMAXPAGE)
+
 #define A_TP_PMM_TX_PAGE_SIZE 0x334
-
 #define A_TP_PMM_TX_MAX_PAGE 0x338
 
+#define S_PMTXMAXPAGE    0
+#define M_PMTXMAXPAGE    0x1fffff
+#define V_PMTXMAXPAGE(x) ((x) << S_PMTXMAXPAGE)
+#define G_PMTXMAXPAGE(x) (((x) >> S_PMTXMAXPAGE) & M_PMTXMAXPAGE)
+
 #define A_TP_TCP_OPTIONS 0x340
 
 #define S_MTUDEFAULT    16
 #define M_MTUDEFAULT    0xffff
 #define V_MTUDEFAULT(x) ((x) << S_MTUDEFAULT)
+#define G_MTUDEFAULT(x) (((x) >> S_MTUDEFAULT) & M_MTUDEFAULT)
 
 #define S_MTUENABLE    10
 #define V_MTUENABLE(x) ((x) << S_MTUENABLE)
 #define F_MTUENABLE    V_MTUENABLE(1U)
 
+#define S_SACKTX    9
+#define V_SACKTX(x) ((x) << S_SACKTX)
+#define F_SACKTX    V_SACKTX(1U)
+
 #define S_SACKRX    8
 #define V_SACKRX(x) ((x) << S_SACKRX)
 #define F_SACKRX    V_SACKRX(1U)
 
 #define S_SACKMODE    4
-
 #define M_SACKMODE    0x3
-
 #define V_SACKMODE(x) ((x) << S_SACKMODE)
+#define G_SACKMODE(x) (((x) >> S_SACKMODE) & M_SACKMODE)
 
 #define S_WINDOWSCALEMODE    2
 #define M_WINDOWSCALEMODE    0x3
 #define V_WINDOWSCALEMODE(x) ((x) << S_WINDOWSCALEMODE)
+#define G_WINDOWSCALEMODE(x) (((x) >> S_WINDOWSCALEMODE) & M_WINDOWSCALEMODE)
 
 #define S_TIMESTAMPSMODE    0
-
 #define M_TIMESTAMPSMODE    0x3
-
 #define V_TIMESTAMPSMODE(x) ((x) << S_TIMESTAMPSMODE)
+#define G_TIMESTAMPSMODE(x) (((x) >> S_TIMESTAMPSMODE) & M_TIMESTAMPSMODE)
 
 #define A_TP_DACK_CONFIG 0x344
 
 #define S_AUTOSTATE3    30
 #define M_AUTOSTATE3    0x3
 #define V_AUTOSTATE3(x) ((x) << S_AUTOSTATE3)
+#define G_AUTOSTATE3(x) (((x) >> S_AUTOSTATE3) & M_AUTOSTATE3)
 
 #define S_AUTOSTATE2    28
 #define M_AUTOSTATE2    0x3
 #define V_AUTOSTATE2(x) ((x) << S_AUTOSTATE2)
+#define G_AUTOSTATE2(x) (((x) >> S_AUTOSTATE2) & M_AUTOSTATE2)
 
 #define S_AUTOSTATE1    26
 #define M_AUTOSTATE1    0x3
 #define V_AUTOSTATE1(x) ((x) << S_AUTOSTATE1)
+#define G_AUTOSTATE1(x) (((x) >> S_AUTOSTATE1) & M_AUTOSTATE1)
 
 #define S_BYTETHRESHOLD    5
 #define M_BYTETHRESHOLD    0xfffff
 #define V_BYTETHRESHOLD(x) ((x) << S_BYTETHRESHOLD)
+#define G_BYTETHRESHOLD(x) (((x) >> S_BYTETHRESHOLD) & M_BYTETHRESHOLD)
 
 #define S_MSSTHRESHOLD    3
 #define M_MSSTHRESHOLD    0x3
 #define V_MSSTHRESHOLD(x) ((x) << S_MSSTHRESHOLD)
+#define G_MSSTHRESHOLD(x) (((x) >> S_MSSTHRESHOLD) & M_MSSTHRESHOLD)
 
 #define S_AUTOCAREFUL    2
 #define V_AUTOCAREFUL(x) ((x) << S_AUTOCAREFUL)
@@ -1125,10 +3745,38 @@
 
 #define A_TP_PC_CONFIG 0x348
 
+#define S_CMCACHEDISABLE    31
+#define V_CMCACHEDISABLE(x) ((x) << S_CMCACHEDISABLE)
+#define F_CMCACHEDISABLE    V_CMCACHEDISABLE(1U)
+
+#define S_ENABLEOCSPIFULL    30
+#define V_ENABLEOCSPIFULL(x) ((x) << S_ENABLEOCSPIFULL)
+#define F_ENABLEOCSPIFULL    V_ENABLEOCSPIFULL(1U)
+
+#define S_ENABLEFLMERRORDDP    29
+#define V_ENABLEFLMERRORDDP(x) ((x) << S_ENABLEFLMERRORDDP)
+#define F_ENABLEFLMERRORDDP    V_ENABLEFLMERRORDDP(1U)
+
+#define S_LOCKTID    28
+#define V_LOCKTID(x) ((x) << S_LOCKTID)
+#define F_LOCKTID    V_LOCKTID(1U)
+
+#define S_FIXRCVWND    27
+#define V_FIXRCVWND(x) ((x) << S_FIXRCVWND)
+#define F_FIXRCVWND    V_FIXRCVWND(1U)
+
 #define S_TXTOSQUEUEMAPMODE    26
 #define V_TXTOSQUEUEMAPMODE(x) ((x) << S_TXTOSQUEUEMAPMODE)
 #define F_TXTOSQUEUEMAPMODE    V_TXTOSQUEUEMAPMODE(1U)
 
+#define S_RDDPCONGEN    25
+#define V_RDDPCONGEN(x) ((x) << S_RDDPCONGEN)
+#define F_RDDPCONGEN    V_RDDPCONGEN(1U)
+
+#define S_ENABLEONFLYPDU    24
+#define V_ENABLEONFLYPDU(x) ((x) << S_ENABLEONFLYPDU)
+#define F_ENABLEONFLYPDU    V_ENABLEONFLYPDU(1U)
+
 #define S_ENABLEEPCMDAFULL    23
 #define V_ENABLEEPCMDAFULL(x) ((x) << S_ENABLEEPCMDAFULL)
 #define F_ENABLEEPCMDAFULL    V_ENABLEEPCMDAFULL(1U)
@@ -1137,6 +3785,10 @@
 #define V_MODULATEUNIONMODE(x) ((x) << S_MODULATEUNIONMODE)
 #define F_MODULATEUNIONMODE    V_MODULATEUNIONMODE(1U)
 
+#define S_TXDATAACKRATEENABLE    21
+#define V_TXDATAACKRATEENABLE(x) ((x) << S_TXDATAACKRATEENABLE)
+#define F_TXDATAACKRATEENABLE    V_TXDATAACKRATEENABLE(1U)
+
 #define S_TXDEFERENABLE    20
 #define V_TXDEFERENABLE(x) ((x) << S_TXDEFERENABLE)
 #define F_TXDEFERENABLE    V_TXDEFERENABLE(1U)
@@ -1145,6 +3797,14 @@
 #define V_RXCONGESTIONMODE(x) ((x) << S_RXCONGESTIONMODE)
 #define F_RXCONGESTIONMODE    V_RXCONGESTIONMODE(1U)
 
+#define S_HEARBEATONCEDACK    18
+#define V_HEARBEATONCEDACK(x) ((x) << S_HEARBEATONCEDACK)
+#define F_HEARBEATONCEDACK    V_HEARBEATONCEDACK(1U)
+
+#define S_HEARBEATONCEHEAP    17
+#define V_HEARBEATONCEHEAP(x) ((x) << S_HEARBEATONCEHEAP)
+#define F_HEARBEATONCEHEAP    V_HEARBEATONCEHEAP(1U)
+
 #define S_HEARBEATDACK    16
 #define V_HEARBEATDACK(x) ((x) << S_HEARBEATDACK)
 #define F_HEARBEATDACK    V_HEARBEATDACK(1U)
@@ -1153,19 +3813,54 @@
 #define V_TXCONGESTIONMODE(x) ((x) << S_TXCONGESTIONMODE)
 #define F_TXCONGESTIONMODE    V_TXCONGESTIONMODE(1U)
 
-#define S_ENABLEOCSPIFULL    30
-#define V_ENABLEOCSPIFULL(x) ((x) << S_ENABLEOCSPIFULL)
-#define F_ENABLEOCSPIFULL    V_ENABLEOCSPIFULL(1U)
-
-#define S_LOCKTID    28
-#define V_LOCKTID(x) ((x) << S_LOCKTID)
-#define F_LOCKTID    V_LOCKTID(1U)
+#define S_ACCEPTLATESTRCVADV    14
+#define V_ACCEPTLATESTRCVADV(x) ((x) << S_ACCEPTLATESTRCVADV)
+#define F_ACCEPTLATESTRCVADV    V_ACCEPTLATESTRCVADV(1U)
+
+#define S_DISABLESYNDATA    13
+#define V_DISABLESYNDATA(x) ((x) << S_DISABLESYNDATA)
+#define F_DISABLESYNDATA    V_DISABLESYNDATA(1U)
+
+#define S_DISABLEWINDOWPSH    12
+#define V_DISABLEWINDOWPSH(x) ((x) << S_DISABLEWINDOWPSH)
+#define F_DISABLEWINDOWPSH    V_DISABLEWINDOWPSH(1U)
+
+#define S_DISABLEFINOLDDATA    11
+#define V_DISABLEFINOLDDATA(x) ((x) << S_DISABLEFINOLDDATA)
+#define F_DISABLEFINOLDDATA    V_DISABLEFINOLDDATA(1U)
+
+#define S_ENABLEFLMERROR    10
+#define V_ENABLEFLMERROR(x) ((x) << S_ENABLEFLMERROR)
+#define F_ENABLEFLMERROR    V_ENABLEFLMERROR(1U)
+
+#define S_DISABLENEXTMTU    9
+#define V_DISABLENEXTMTU(x) ((x) << S_DISABLENEXTMTU)
+#define F_DISABLENEXTMTU    V_DISABLENEXTMTU(1U)
+
+#define S_FILTERPEERFIN    8
+#define V_FILTERPEERFIN(x) ((x) << S_FILTERPEERFIN)
+#define F_FILTERPEERFIN    V_FILTERPEERFIN(1U)
+
+#define S_ENABLEFEEDBACKSEND    7
+#define V_ENABLEFEEDBACKSEND(x) ((x) << S_ENABLEFEEDBACKSEND)
+#define F_ENABLEFEEDBACKSEND    V_ENABLEFEEDBACKSEND(1U)
+
+#define S_ENABLERDMAERROR    6
+#define V_ENABLERDMAERROR(x) ((x) << S_ENABLERDMAERROR)
+#define F_ENABLERDMAERROR    V_ENABLERDMAERROR(1U)
+
+#define S_ENABLEDDPFLOWCONTROL    5
+#define V_ENABLEDDPFLOWCONTROL(x) ((x) << S_ENABLEDDPFLOWCONTROL)
+#define F_ENABLEDDPFLOWCONTROL    V_ENABLEDDPFLOWCONTROL(1U)
+
+#define S_DISABLEHELDFIN    4
+#define V_DISABLEHELDFIN(x) ((x) << S_DISABLEHELDFIN)
+#define F_DISABLEHELDFIN    V_DISABLEHELDFIN(1U)
 
 #define S_TABLELATENCYDELTA    0
 #define M_TABLELATENCYDELTA    0xf
 #define V_TABLELATENCYDELTA(x) ((x) << S_TABLELATENCYDELTA)
-#define G_TABLELATENCYDELTA(x) \
-	(((x) >> S_TABLELATENCYDELTA) & M_TABLELATENCYDELTA)
+#define G_TABLELATENCYDELTA(x) (((x) >> S_TABLELATENCYDELTA) & M_TABLELATENCYDELTA)
 
 #define A_TP_PC_CONFIG2 0x34c
 
@@ -1185,34 +3880,201 @@
 #define V_ENABLEIPV6RSS(x) ((x) << S_ENABLEIPV6RSS)
 #define F_ENABLEIPV6RSS    V_ENABLEIPV6RSS(1U)
 
+#define S_ENABLEDROPRQEMPTYPKT    10
+#define V_ENABLEDROPRQEMPTYPKT(x) ((x) << S_ENABLEDROPRQEMPTYPKT)
+#define F_ENABLEDROPRQEMPTYPKT    V_ENABLEDROPRQEMPTYPKT(1U)
+
+#define S_ENABLETXPORTFROMDA2    9
+#define V_ENABLETXPORTFROMDA2(x) ((x) << S_ENABLETXPORTFROMDA2)
+#define F_ENABLETXPORTFROMDA2    V_ENABLETXPORTFROMDA2(1U)
+
+#define S_ENABLERXPKTTMSTPRSS    8
+#define V_ENABLERXPKTTMSTPRSS(x) ((x) << S_ENABLERXPKTTMSTPRSS)
+#define F_ENABLERXPKTTMSTPRSS    V_ENABLERXPKTTMSTPRSS(1U)
+
+#define S_ENABLESNDUNAINRXDATA    7
+#define V_ENABLESNDUNAINRXDATA(x) ((x) << S_ENABLESNDUNAINRXDATA)
+#define F_ENABLESNDUNAINRXDATA    V_ENABLESNDUNAINRXDATA(1U)
+
+#define S_ENABLERXPORTFROMADDR    6
+#define V_ENABLERXPORTFROMADDR(x) ((x) << S_ENABLERXPORTFROMADDR)
+#define F_ENABLERXPORTFROMADDR    V_ENABLERXPORTFROMADDR(1U)
+
+#define S_ENABLETXPORTFROMDA    5
+#define V_ENABLETXPORTFROMDA(x) ((x) << S_ENABLETXPORTFROMDA)
+#define F_ENABLETXPORTFROMDA    V_ENABLETXPORTFROMDA(1U)
+
+#define S_ENABLECHDRAFULL    4
+#define V_ENABLECHDRAFULL(x) ((x) << S_ENABLECHDRAFULL)
+#define F_ENABLECHDRAFULL    V_ENABLECHDRAFULL(1U)
+
+#define S_ENABLENONOFDSCBBIT    3
+#define V_ENABLENONOFDSCBBIT(x) ((x) << S_ENABLENONOFDSCBBIT)
+#define F_ENABLENONOFDSCBBIT    V_ENABLENONOFDSCBBIT(1U)
+
+#define S_ENABLENONOFDTIDRSS    2
+#define V_ENABLENONOFDTIDRSS(x) ((x) << S_ENABLENONOFDTIDRSS)
+#define F_ENABLENONOFDTIDRSS    V_ENABLENONOFDTIDRSS(1U)
+
+#define S_ENABLENONOFDTCBRSS    1
+#define V_ENABLENONOFDTCBRSS(x) ((x) << S_ENABLENONOFDTCBRSS)
+#define F_ENABLENONOFDTCBRSS    V_ENABLENONOFDTCBRSS(1U)
+
+#define S_ENABLEOLDRXFORWARD    0
+#define V_ENABLEOLDRXFORWARD(x) ((x) << S_ENABLEOLDRXFORWARD)
+#define F_ENABLEOLDRXFORWARD    V_ENABLEOLDRXFORWARD(1U)
+
 #define S_CHDRAFULL    4
 #define V_CHDRAFULL(x) ((x) << S_CHDRAFULL)
 #define F_CHDRAFULL    V_CHDRAFULL(1U)
 
 #define A_TP_TCP_BACKOFF_REG0 0x350
 
+#define S_TIMERBACKOFFINDEX3    24
+#define M_TIMERBACKOFFINDEX3    0xff
+#define V_TIMERBACKOFFINDEX3(x) ((x) << S_TIMERBACKOFFINDEX3)
+#define G_TIMERBACKOFFINDEX3(x) (((x) >> S_TIMERBACKOFFINDEX3) & M_TIMERBACKOFFINDEX3)
+
+#define S_TIMERBACKOFFINDEX2    16
+#define M_TIMERBACKOFFINDEX2    0xff
+#define V_TIMERBACKOFFINDEX2(x) ((x) << S_TIMERBACKOFFINDEX2)
+#define G_TIMERBACKOFFINDEX2(x) (((x) >> S_TIMERBACKOFFINDEX2) & M_TIMERBACKOFFINDEX2)
+
+#define S_TIMERBACKOFFINDEX1    8
+#define M_TIMERBACKOFFINDEX1    0xff
+#define V_TIMERBACKOFFINDEX1(x) ((x) << S_TIMERBACKOFFINDEX1)
+#define G_TIMERBACKOFFINDEX1(x) (((x) >> S_TIMERBACKOFFINDEX1) & M_TIMERBACKOFFINDEX1)
+
+#define S_TIMERBACKOFFINDEX0    0
+#define M_TIMERBACKOFFINDEX0    0xff
+#define V_TIMERBACKOFFINDEX0(x) ((x) << S_TIMERBACKOFFINDEX0)
+#define G_TIMERBACKOFFINDEX0(x) (((x) >> S_TIMERBACKOFFINDEX0) & M_TIMERBACKOFFINDEX0)
+
 #define A_TP_TCP_BACKOFF_REG1 0x354
 
+#define S_TIMERBACKOFFINDEX7    24
+#define M_TIMERBACKOFFINDEX7    0xff
+#define V_TIMERBACKOFFINDEX7(x) ((x) << S_TIMERBACKOFFINDEX7)
+#define G_TIMERBACKOFFINDEX7(x) (((x) >> S_TIMERBACKOFFINDEX7) & M_TIMERBACKOFFINDEX7)
+
+#define S_TIMERBACKOFFINDEX6    16
+#define M_TIMERBACKOFFINDEX6    0xff
+#define V_TIMERBACKOFFINDEX6(x) ((x) << S_TIMERBACKOFFINDEX6)
+#define G_TIMERBACKOFFINDEX6(x) (((x) >> S_TIMERBACKOFFINDEX6) & M_TIMERBACKOFFINDEX6)
+
+#define S_TIMERBACKOFFINDEX5    8
+#define M_TIMERBACKOFFINDEX5    0xff
+#define V_TIMERBACKOFFINDEX5(x) ((x) << S_TIMERBACKOFFINDEX5)
+#define G_TIMERBACKOFFINDEX5(x) (((x) >> S_TIMERBACKOFFINDEX5) & M_TIMERBACKOFFINDEX5)
+
+#define S_TIMERBACKOFFINDEX4    0
+#define M_TIMERBACKOFFINDEX4    0xff
+#define V_TIMERBACKOFFINDEX4(x) ((x) << S_TIMERBACKOFFINDEX4)
+#define G_TIMERBACKOFFINDEX4(x) (((x) >> S_TIMERBACKOFFINDEX4) & M_TIMERBACKOFFINDEX4)
+
 #define A_TP_TCP_BACKOFF_REG2 0x358
 
+#define S_TIMERBACKOFFINDEX11    24
+#define M_TIMERBACKOFFINDEX11    0xff
+#define V_TIMERBACKOFFINDEX11(x) ((x) << S_TIMERBACKOFFINDEX11)
+#define G_TIMERBACKOFFINDEX11(x) (((x) >> S_TIMERBACKOFFINDEX11) & M_TIMERBACKOFFINDEX11)
+
+#define S_TIMERBACKOFFINDEX10    16
+#define M_TIMERBACKOFFINDEX10    0xff
+#define V_TIMERBACKOFFINDEX10(x) ((x) << S_TIMERBACKOFFINDEX10)
+#define G_TIMERBACKOFFINDEX10(x) (((x) >> S_TIMERBACKOFFINDEX10) & M_TIMERBACKOFFINDEX10)
+
+#define S_TIMERBACKOFFINDEX9    8
+#define M_TIMERBACKOFFINDEX9    0xff
+#define V_TIMERBACKOFFINDEX9(x) ((x) << S_TIMERBACKOFFINDEX9)
+#define G_TIMERBACKOFFINDEX9(x) (((x) >> S_TIMERBACKOFFINDEX9) & M_TIMERBACKOFFINDEX9)
+
+#define S_TIMERBACKOFFINDEX8    0
+#define M_TIMERBACKOFFINDEX8    0xff
+#define V_TIMERBACKOFFINDEX8(x) ((x) << S_TIMERBACKOFFINDEX8)
+#define G_TIMERBACKOFFINDEX8(x) (((x) >> S_TIMERBACKOFFINDEX8) & M_TIMERBACKOFFINDEX8)
+
 #define A_TP_TCP_BACKOFF_REG3 0x35c
 
+#define S_TIMERBACKOFFINDEX15    24
+#define M_TIMERBACKOFFINDEX15    0xff
+#define V_TIMERBACKOFFINDEX15(x) ((x) << S_TIMERBACKOFFINDEX15)
+#define G_TIMERBACKOFFINDEX15(x) (((x) >> S_TIMERBACKOFFINDEX15) & M_TIMERBACKOFFINDEX15)
+
+#define S_TIMERBACKOFFINDEX14    16
+#define M_TIMERBACKOFFINDEX14    0xff
+#define V_TIMERBACKOFFINDEX14(x) ((x) << S_TIMERBACKOFFINDEX14)
+#define G_TIMERBACKOFFINDEX14(x) (((x) >> S_TIMERBACKOFFINDEX14) & M_TIMERBACKOFFINDEX14)
+
+#define S_TIMERBACKOFFINDEX13    8
+#define M_TIMERBACKOFFINDEX13    0xff
+#define V_TIMERBACKOFFINDEX13(x) ((x) << S_TIMERBACKOFFINDEX13)
+#define G_TIMERBACKOFFINDEX13(x) (((x) >> S_TIMERBACKOFFINDEX13) & M_TIMERBACKOFFINDEX13)
+
+#define S_TIMERBACKOFFINDEX12    0
+#define M_TIMERBACKOFFINDEX12    0xff
+#define V_TIMERBACKOFFINDEX12(x) ((x) << S_TIMERBACKOFFINDEX12)
+#define G_TIMERBACKOFFINDEX12(x) (((x) >> S_TIMERBACKOFFINDEX12) & M_TIMERBACKOFFINDEX12)
+
+#define A_TP_PARA_REG0 0x360
+
+#define S_INITCWND    24
+#define M_INITCWND    0x7
+#define V_INITCWND(x) ((x) << S_INITCWND)
+#define G_INITCWND(x) (((x) >> S_INITCWND) & M_INITCWND)
+
+#define S_DUPACKTHRESH    20
+#define M_DUPACKTHRESH    0xf
+#define V_DUPACKTHRESH(x) ((x) << S_DUPACKTHRESH)
+#define G_DUPACKTHRESH(x) (((x) >> S_DUPACKTHRESH) & M_DUPACKTHRESH)
+
+#define A_TP_PARA_REG1 0x364
+
+#define S_INITRWND    16
+#define M_INITRWND    0xffff
+#define V_INITRWND(x) ((x) << S_INITRWND)
+#define G_INITRWND(x) (((x) >> S_INITRWND) & M_INITRWND)
+
+#define S_INITIALSSTHRESH    0
+#define M_INITIALSSTHRESH    0xffff
+#define V_INITIALSSTHRESH(x) ((x) << S_INITIALSSTHRESH)
+#define G_INITIALSSTHRESH(x) (((x) >> S_INITIALSSTHRESH) & M_INITIALSSTHRESH)
+
 #define A_TP_PARA_REG2 0x368
 
 #define S_MAXRXDATA    16
 #define M_MAXRXDATA    0xffff
 #define V_MAXRXDATA(x) ((x) << S_MAXRXDATA)
+#define G_MAXRXDATA(x) (((x) >> S_MAXRXDATA) & M_MAXRXDATA)
 
 #define S_RXCOALESCESIZE    0
 #define M_RXCOALESCESIZE    0xffff
 #define V_RXCOALESCESIZE(x) ((x) << S_RXCOALESCESIZE)
+#define G_RXCOALESCESIZE(x) (((x) >> S_RXCOALESCESIZE) & M_RXCOALESCESIZE)
 
 #define A_TP_PARA_REG3 0x36c
 
+#define S_TUNNELCNGDROP1    21
+#define V_TUNNELCNGDROP1(x) ((x) << S_TUNNELCNGDROP1)
+#define F_TUNNELCNGDROP1    V_TUNNELCNGDROP1(1U)
+
+#define S_TUNNELCNGDROP0    20
+#define V_TUNNELCNGDROP0(x) ((x) << S_TUNNELCNGDROP0)
+#define F_TUNNELCNGDROP0    V_TUNNELCNGDROP0(1U)
+
 #define S_TXDATAACKIDX    16
 #define M_TXDATAACKIDX    0xf
-
 #define V_TXDATAACKIDX(x) ((x) << S_TXDATAACKIDX)
+#define G_TXDATAACKIDX(x) (((x) >> S_TXDATAACKIDX) & M_TXDATAACKIDX)
+
+#define S_RXFRAGENABLE    12
+#define M_RXFRAGENABLE    0x7
+#define V_RXFRAGENABLE(x) ((x) << S_RXFRAGENABLE)
+#define G_RXFRAGENABLE(x) (((x) >> S_RXFRAGENABLE) & M_RXFRAGENABLE)
+
+#define S_TXPACEFIXEDSTRICT    11
+#define V_TXPACEFIXEDSTRICT(x) ((x) << S_TXPACEFIXEDSTRICT)
+#define F_TXPACEFIXEDSTRICT    V_TXPACEFIXEDSTRICT(1U)
 
 #define S_TXPACEAUTOSTRICT    10
 #define V_TXPACEAUTOSTRICT(x) ((x) << S_TXPACEAUTOSTRICT)
@@ -1226,6 +4088,23 @@
 #define V_TXPACEAUTO(x) ((x) << S_TXPACEAUTO)
 #define F_TXPACEAUTO    V_TXPACEAUTO(1U)
 
+#define S_RXURGTUNNEL    6
+#define V_RXURGTUNNEL(x) ((x) << S_RXURGTUNNEL)
+#define F_RXURGTUNNEL    V_RXURGTUNNEL(1U)
+
+#define S_RXURGMODE    5
+#define V_RXURGMODE(x) ((x) << S_RXURGMODE)
+#define F_RXURGMODE    V_RXURGMODE(1U)
+
+#define S_TXURGMODE    4
+#define V_TXURGMODE(x) ((x) << S_TXURGMODE)
+#define F_TXURGMODE    V_TXURGMODE(1U)
+
+#define S_CNGCTRLMODE    2
+#define M_CNGCTRLMODE    0x3
+#define V_CNGCTRLMODE(x) ((x) << S_CNGCTRLMODE)
+#define G_CNGCTRLMODE(x) (((x) >> S_CNGCTRLMODE) & M_CNGCTRLMODE)
+
 #define S_RXCOALESCEENABLE    1
 #define V_RXCOALESCEENABLE(x) ((x) << S_RXCOALESCEENABLE)
 #define F_RXCOALESCEENABLE    V_RXCOALESCEENABLE(1U)
@@ -1236,110 +4115,336 @@
 
 #define A_TP_PARA_REG4 0x370
 
+#define S_HIGHSPEEDCFG    24
+#define M_HIGHSPEEDCFG    0xff
+#define V_HIGHSPEEDCFG(x) ((x) << S_HIGHSPEEDCFG)
+#define G_HIGHSPEEDCFG(x) (((x) >> S_HIGHSPEEDCFG) & M_HIGHSPEEDCFG)
+
+#define S_NEWRENOCFG    16
+#define M_NEWRENOCFG    0xff
+#define V_NEWRENOCFG(x) ((x) << S_NEWRENOCFG)
+#define G_NEWRENOCFG(x) (((x) >> S_NEWRENOCFG) & M_NEWRENOCFG)
+
+#define S_TAHOECFG    8
+#define M_TAHOECFG    0xff
+#define V_TAHOECFG(x) ((x) << S_TAHOECFG)
+#define G_TAHOECFG(x) (((x) >> S_TAHOECFG) & M_TAHOECFG)
+
+#define S_RENOCFG    0
+#define M_RENOCFG    0xff
+#define V_RENOCFG(x) ((x) << S_RENOCFG)
+#define G_RENOCFG(x) (((x) >> S_RENOCFG) & M_RENOCFG)
+
 #define A_TP_PARA_REG5 0x374
 
+#define S_INDICATESIZE    16
+#define M_INDICATESIZE    0xffff
+#define V_INDICATESIZE(x) ((x) << S_INDICATESIZE)
+#define G_INDICATESIZE(x) (((x) >> S_INDICATESIZE) & M_INDICATESIZE)
+
+#define S_SCHDENABLE    8
+#define V_SCHDENABLE(x) ((x) << S_SCHDENABLE)
+#define F_SCHDENABLE    V_SCHDENABLE(1U)
+
 #define S_RXDDPOFFINIT    3
 #define V_RXDDPOFFINIT(x) ((x) << S_RXDDPOFFINIT)
 #define F_RXDDPOFFINIT    V_RXDDPOFFINIT(1U)
 
+#define S_ONFLYDDPENABLE    2
+#define V_ONFLYDDPENABLE(x) ((x) << S_ONFLYDDPENABLE)
+#define F_ONFLYDDPENABLE    V_ONFLYDDPENABLE(1U)
+
+#define S_DACKTIMERSPIN    1
+#define V_DACKTIMERSPIN(x) ((x) << S_DACKTIMERSPIN)
+#define F_DACKTIMERSPIN    V_DACKTIMERSPIN(1U)
+
+#define S_PUSHTIMERENABLE    0
+#define V_PUSHTIMERENABLE(x) ((x) << S_PUSHTIMERENABLE)
+#define F_PUSHTIMERENABLE    V_PUSHTIMERENABLE(1U)
+
 #define A_TP_PARA_REG6 0x378
 
+#define S_TXPDUSIZEADJ    16
+#define M_TXPDUSIZEADJ    0xff
+#define V_TXPDUSIZEADJ(x) ((x) << S_TXPDUSIZEADJ)
+#define G_TXPDUSIZEADJ(x) (((x) >> S_TXPDUSIZEADJ) & M_TXPDUSIZEADJ)
+
+#define S_ENABLEDEFERACK    12
+#define V_ENABLEDEFERACK(x) ((x) << S_ENABLEDEFERACK)
+#define F_ENABLEDEFERACK    V_ENABLEDEFERACK(1U)
+
+#define S_ENABLEESND    11
+#define V_ENABLEESND(x) ((x) << S_ENABLEESND)
+#define F_ENABLEESND    V_ENABLEESND(1U)
+
+#define S_ENABLECSND    10
+#define V_ENABLECSND(x) ((x) << S_ENABLECSND)
+#define F_ENABLECSND    V_ENABLECSND(1U)
+
+#define S_ENABLEPDUE    9
+#define V_ENABLEPDUE(x) ((x) << S_ENABLEPDUE)
+#define F_ENABLEPDUE    V_ENABLEPDUE(1U)
+
+#define S_ENABLEPDUC    8
+#define V_ENABLEPDUC(x) ((x) << S_ENABLEPDUC)
+#define F_ENABLEPDUC    V_ENABLEPDUC(1U)
+
+#define S_ENABLEBUFI    7
+#define V_ENABLEBUFI(x) ((x) << S_ENABLEBUFI)
+#define F_ENABLEBUFI    V_ENABLEBUFI(1U)
+
+#define S_ENABLEBUFE    6
+#define V_ENABLEBUFE(x) ((x) << S_ENABLEBUFE)
+#define F_ENABLEBUFE    V_ENABLEBUFE(1U)
+
+#define S_ENABLEDEFER    5
+#define V_ENABLEDEFER(x) ((x) << S_ENABLEDEFER)
+#define F_ENABLEDEFER    V_ENABLEDEFER(1U)
+
+#define S_ENABLECLEARRXMTOOS    4
+#define V_ENABLECLEARRXMTOOS(x) ((x) << S_ENABLECLEARRXMTOOS)
+#define F_ENABLECLEARRXMTOOS    V_ENABLECLEARRXMTOOS(1U)
+
+#define S_DISABLEPDUCNG    3
+#define V_DISABLEPDUCNG(x) ((x) << S_DISABLEPDUCNG)
+#define F_DISABLEPDUCNG    V_DISABLEPDUCNG(1U)
+
+#define S_DISABLEPDUTIMEOUT    2
+#define V_DISABLEPDUTIMEOUT(x) ((x) << S_DISABLEPDUTIMEOUT)
+#define F_DISABLEPDUTIMEOUT    V_DISABLEPDUTIMEOUT(1U)
+
+#define S_DISABLEPDURXMT    1
+#define V_DISABLEPDURXMT(x) ((x) << S_DISABLEPDURXMT)
+#define F_DISABLEPDURXMT    V_DISABLEPDURXMT(1U)
+
+#define S_DISABLEPDUXMT    0
+#define V_DISABLEPDUXMT(x) ((x) << S_DISABLEPDUXMT)
+#define F_DISABLEPDUXMT    V_DISABLEPDUXMT(1U)
+
+#define S_ENABLEEPDU    14
+#define V_ENABLEEPDU(x) ((x) << S_ENABLEEPDU)
+#define F_ENABLEEPDU    V_ENABLEEPDU(1U)
+
 #define S_T3A_ENABLEESND    13
 #define V_T3A_ENABLEESND(x) ((x) << S_T3A_ENABLEESND)
 #define F_T3A_ENABLEESND    V_T3A_ENABLEESND(1U)
 
-#define S_ENABLEESND    11
-#define V_ENABLEESND(x) ((x) << S_ENABLEESND)
-#define F_ENABLEESND    V_ENABLEESND(1U)
+#define S_T3A_ENABLECSND    12
+#define V_T3A_ENABLECSND(x) ((x) << S_T3A_ENABLECSND)
+#define F_T3A_ENABLECSND    V_T3A_ENABLECSND(1U)
+
+#define S_T3A_ENABLEDEFERACK    9
+#define V_T3A_ENABLEDEFERACK(x) ((x) << S_T3A_ENABLEDEFERACK)
+#define F_T3A_ENABLEDEFERACK    V_T3A_ENABLEDEFERACK(1U)
+
+#define S_ENABLEPDUI    7
+#define V_ENABLEPDUI(x) ((x) << S_ENABLEPDUI)
+#define F_ENABLEPDUI    V_ENABLEPDUI(1U)
+
+#define S_T3A_ENABLEPDUE    6
+#define V_T3A_ENABLEPDUE(x) ((x) << S_T3A_ENABLEPDUE)
+#define F_T3A_ENABLEPDUE    V_T3A_ENABLEPDUE(1U)
 
 #define A_TP_PARA_REG7 0x37c
 
 #define S_PMMAXXFERLEN1    16
 #define M_PMMAXXFERLEN1    0xffff
 #define V_PMMAXXFERLEN1(x) ((x) << S_PMMAXXFERLEN1)
+#define G_PMMAXXFERLEN1(x) (((x) >> S_PMMAXXFERLEN1) & M_PMMAXXFERLEN1)
 
 #define S_PMMAXXFERLEN0    0
 #define M_PMMAXXFERLEN0    0xffff
 #define V_PMMAXXFERLEN0(x) ((x) << S_PMMAXXFERLEN0)
+#define G_PMMAXXFERLEN0(x) (((x) >> S_PMMAXXFERLEN0) & M_PMMAXXFERLEN0)
 
 #define A_TP_TIMER_RESOLUTION 0x390
 
 #define S_TIMERRESOLUTION    16
 #define M_TIMERRESOLUTION    0xff
 #define V_TIMERRESOLUTION(x) ((x) << S_TIMERRESOLUTION)
+#define G_TIMERRESOLUTION(x) (((x) >> S_TIMERRESOLUTION) & M_TIMERRESOLUTION)
 
 #define S_TIMESTAMPRESOLUTION    8
 #define M_TIMESTAMPRESOLUTION    0xff
 #define V_TIMESTAMPRESOLUTION(x) ((x) << S_TIMESTAMPRESOLUTION)
+#define G_TIMESTAMPRESOLUTION(x) (((x) >> S_TIMESTAMPRESOLUTION) & M_TIMESTAMPRESOLUTION)
 
 #define S_DELAYEDACKRESOLUTION    0
 #define M_DELAYEDACKRESOLUTION    0xff
 #define V_DELAYEDACKRESOLUTION(x) ((x) << S_DELAYEDACKRESOLUTION)
+#define G_DELAYEDACKRESOLUTION(x) (((x) >> S_DELAYEDACKRESOLUTION) & M_DELAYEDACKRESOLUTION)
 
 #define A_TP_MSL 0x394
 
+#define S_MSL    0
+#define M_MSL    0x3fffffff
+#define V_MSL(x) ((x) << S_MSL)
+#define G_MSL(x) (((x) >> S_MSL) & M_MSL)
+
 #define A_TP_RXT_MIN 0x398
 
+#define S_RXTMIN    0
+#define M_RXTMIN    0x3fffffff
+#define V_RXTMIN(x) ((x) << S_RXTMIN)
+#define G_RXTMIN(x) (((x) >> S_RXTMIN) & M_RXTMIN)
+
 #define A_TP_RXT_MAX 0x39c
 
+#define S_RXTMAX    0
+#define M_RXTMAX    0x3fffffff
+#define V_RXTMAX(x) ((x) << S_RXTMAX)
+#define G_RXTMAX(x) (((x) >> S_RXTMAX) & M_RXTMAX)
+
 #define A_TP_PERS_MIN 0x3a0
 
+#define S_PERSMIN    0
+#define M_PERSMIN    0x3fffffff
+#define V_PERSMIN(x) ((x) << S_PERSMIN)
+#define G_PERSMIN(x) (((x) >> S_PERSMIN) & M_PERSMIN)
+
 #define A_TP_PERS_MAX 0x3a4
 
+#define S_PERSMAX    0
+#define M_PERSMAX    0x3fffffff
+#define V_PERSMAX(x) ((x) << S_PERSMAX)
+#define G_PERSMAX(x) (((x) >> S_PERSMAX) & M_PERSMAX)
+
 #define A_TP_KEEP_IDLE 0x3a8
 
+#define S_KEEPALIVEIDLE    0
+#define M_KEEPALIVEIDLE    0x3fffffff
+#define V_KEEPALIVEIDLE(x) ((x) << S_KEEPALIVEIDLE)
+#define G_KEEPALIVEIDLE(x) (((x) >> S_KEEPALIVEIDLE) & M_KEEPALIVEIDLE)
+
 #define A_TP_KEEP_INTVL 0x3ac
 
+#define S_KEEPALIVEINTVL    0
+#define M_KEEPALIVEINTVL    0x3fffffff
+#define V_KEEPALIVEINTVL(x) ((x) << S_KEEPALIVEINTVL)
+#define G_KEEPALIVEINTVL(x) (((x) >> S_KEEPALIVEINTVL) & M_KEEPALIVEINTVL)
+
 #define A_TP_INIT_SRTT 0x3b0
 
+#define S_INITSRTT    0
+#define M_INITSRTT    0xffff
+#define V_INITSRTT(x) ((x) << S_INITSRTT)
+#define G_INITSRTT(x) (((x) >> S_INITSRTT) & M_INITSRTT)
+
 #define A_TP_DACK_TIMER 0x3b4
 
+#define S_DACKTIME    0
+#define M_DACKTIME    0xfff
+#define V_DACKTIME(x) ((x) << S_DACKTIME)
+#define G_DACKTIME(x) (((x) >> S_DACKTIME) & M_DACKTIME)
+
 #define A_TP_FINWAIT2_TIMER 0x3b8
 
+#define S_FINWAIT2TIME    0
+#define M_FINWAIT2TIME    0x3fffffff
+#define V_FINWAIT2TIME(x) ((x) << S_FINWAIT2TIME)
+#define G_FINWAIT2TIME(x) (((x) >> S_FINWAIT2TIME) & M_FINWAIT2TIME)
+
+#define A_TP_FAST_FINWAIT2_TIMER 0x3bc
+
+#define S_FASTFINWAIT2TIME    0
+#define M_FASTFINWAIT2TIME    0x3fffffff
+#define V_FASTFINWAIT2TIME(x) ((x) << S_FASTFINWAIT2TIME)
+#define G_FASTFINWAIT2TIME(x) (((x) >> S_FASTFINWAIT2TIME) & M_FASTFINWAIT2TIME)
+
 #define A_TP_SHIFT_CNT 0x3c0
 
 #define S_SYNSHIFTMAX    24
-
 #define M_SYNSHIFTMAX    0xff
-
 #define V_SYNSHIFTMAX(x) ((x) << S_SYNSHIFTMAX)
+#define G_SYNSHIFTMAX(x) (((x) >> S_SYNSHIFTMAX) & M_SYNSHIFTMAX)
 
 #define S_RXTSHIFTMAXR1    20
-
 #define M_RXTSHIFTMAXR1    0xf
-
 #define V_RXTSHIFTMAXR1(x) ((x) << S_RXTSHIFTMAXR1)
+#define G_RXTSHIFTMAXR1(x) (((x) >> S_RXTSHIFTMAXR1) & M_RXTSHIFTMAXR1)
 
 #define S_RXTSHIFTMAXR2    16
-
 #define M_RXTSHIFTMAXR2    0xf
-
 #define V_RXTSHIFTMAXR2(x) ((x) << S_RXTSHIFTMAXR2)
+#define G_RXTSHIFTMAXR2(x) (((x) >> S_RXTSHIFTMAXR2) & M_RXTSHIFTMAXR2)
 
 #define S_PERSHIFTBACKOFFMAX    12
 #define M_PERSHIFTBACKOFFMAX    0xf
 #define V_PERSHIFTBACKOFFMAX(x) ((x) << S_PERSHIFTBACKOFFMAX)
+#define G_PERSHIFTBACKOFFMAX(x) (((x) >> S_PERSHIFTBACKOFFMAX) & M_PERSHIFTBACKOFFMAX)
 
 #define S_PERSHIFTMAX    8
 #define M_PERSHIFTMAX    0xf
 #define V_PERSHIFTMAX(x) ((x) << S_PERSHIFTMAX)
+#define G_PERSHIFTMAX(x) (((x) >> S_PERSHIFTMAX) & M_PERSHIFTMAX)
 
 #define S_KEEPALIVEMAX    0
-
 #define M_KEEPALIVEMAX    0xff
-
 #define V_KEEPALIVEMAX(x) ((x) << S_KEEPALIVEMAX)
-
+#define G_KEEPALIVEMAX(x) (((x) >> S_KEEPALIVEMAX) & M_KEEPALIVEMAX)
+
+#define A_TP_TIME_HI 0x3c8
+#define A_TP_TIME_LO 0x3cc
 #define A_TP_MTU_PORT_TABLE 0x3d0
 
+#define S_PORT1MTUVALUE    16
+#define M_PORT1MTUVALUE    0xffff
+#define V_PORT1MTUVALUE(x) ((x) << S_PORT1MTUVALUE)
+#define G_PORT1MTUVALUE(x) (((x) >> S_PORT1MTUVALUE) & M_PORT1MTUVALUE)
+
+#define S_PORT0MTUVALUE    0
+#define M_PORT0MTUVALUE    0xffff
+#define V_PORT0MTUVALUE(x) ((x) << S_PORT0MTUVALUE)
+#define G_PORT0MTUVALUE(x) (((x) >> S_PORT0MTUVALUE) & M_PORT0MTUVALUE)
+
+#define A_TP_ULP_TABLE 0x3d4
+
+#define S_ULPTYPE7FIELD    28
+#define M_ULPTYPE7FIELD    0xf
+#define V_ULPTYPE7FIELD(x) ((x) << S_ULPTYPE7FIELD)
+#define G_ULPTYPE7FIELD(x) (((x) >> S_ULPTYPE7FIELD) & M_ULPTYPE7FIELD)
+
+#define S_ULPTYPE6FIELD    24
+#define M_ULPTYPE6FIELD    0xf
+#define V_ULPTYPE6FIELD(x) ((x) << S_ULPTYPE6FIELD)
+#define G_ULPTYPE6FIELD(x) (((x) >> S_ULPTYPE6FIELD) & M_ULPTYPE6FIELD)
+
+#define S_ULPTYPE5FIELD    20
+#define M_ULPTYPE5FIELD    0xf
+#define V_ULPTYPE5FIELD(x) ((x) << S_ULPTYPE5FIELD)
+#define G_ULPTYPE5FIELD(x) (((x) >> S_ULPTYPE5FIELD) & M_ULPTYPE5FIELD)
+
+#define S_ULPTYPE4FIELD    16
+#define M_ULPTYPE4FIELD    0xf
+#define V_ULPTYPE4FIELD(x) ((x) << S_ULPTYPE4FIELD)
+#define G_ULPTYPE4FIELD(x) (((x) >> S_ULPTYPE4FIELD) & M_ULPTYPE4FIELD)
+
+#define S_ULPTYPE3FIELD    12
+#define M_ULPTYPE3FIELD    0xf
+#define V_ULPTYPE3FIELD(x) ((x) << S_ULPTYPE3FIELD)
+#define G_ULPTYPE3FIELD(x) (((x) >> S_ULPTYPE3FIELD) & M_ULPTYPE3FIELD)
+
+#define S_ULPTYPE2FIELD    8
+#define M_ULPTYPE2FIELD    0xf
+#define V_ULPTYPE2FIELD(x) ((x) << S_ULPTYPE2FIELD)
+#define G_ULPTYPE2FIELD(x) (((x) >> S_ULPTYPE2FIELD) & M_ULPTYPE2FIELD)
+
+#define S_ULPTYPE1FIELD    4
+#define M_ULPTYPE1FIELD    0xf
+#define V_ULPTYPE1FIELD(x) ((x) << S_ULPTYPE1FIELD)
+#define G_ULPTYPE1FIELD(x) (((x) >> S_ULPTYPE1FIELD) & M_ULPTYPE1FIELD)
+
+#define S_ULPTYPE0FIELD    0
+#define M_ULPTYPE0FIELD    0xf
+#define V_ULPTYPE0FIELD(x) ((x) << S_ULPTYPE0FIELD)
+#define G_ULPTYPE0FIELD(x) (((x) >> S_ULPTYPE0FIELD) & M_ULPTYPE0FIELD)
+
+#define A_TP_PACE_TABLE 0x3d8
 #define A_TP_CCTRL_TABLE 0x3dc
-
+#define A_TP_TOS_TABLE 0x3e0
 #define A_TP_MTU_TABLE 0x3e4
-
 #define A_TP_RSS_MAP_TABLE 0x3e8
-
 #define A_TP_RSS_LKP_TABLE 0x3ec
-
 #define A_TP_RSS_CONFIG 0x3f0
 
 #define S_TNL4TUPEN    29
@@ -1362,6 +4467,38 @@
 #define V_TNLLKPEN(x) ((x) << S_TNLLKPEN)
 #define F_TNLLKPEN    V_TNLLKPEN(1U)
 
+#define S_OFD4TUPEN    21
+#define V_OFD4TUPEN(x) ((x) << S_OFD4TUPEN)
+#define F_OFD4TUPEN    V_OFD4TUPEN(1U)
+
+#define S_OFD2TUPEN    20
+#define V_OFD2TUPEN(x) ((x) << S_OFD2TUPEN)
+#define F_OFD2TUPEN    V_OFD2TUPEN(1U)
+
+#define S_OFDMAPEN    17
+#define V_OFDMAPEN(x) ((x) << S_OFDMAPEN)
+#define F_OFDMAPEN    V_OFDMAPEN(1U)
+
+#define S_OFDLKPEN    16
+#define V_OFDLKPEN(x) ((x) << S_OFDLKPEN)
+#define F_OFDLKPEN    V_OFDLKPEN(1U)
+
+#define S_SYN4TUPEN    13
+#define V_SYN4TUPEN(x) ((x) << S_SYN4TUPEN)
+#define F_SYN4TUPEN    V_SYN4TUPEN(1U)
+
+#define S_SYN2TUPEN    12
+#define V_SYN2TUPEN(x) ((x) << S_SYN2TUPEN)
+#define F_SYN2TUPEN    V_SYN2TUPEN(1U)
+
+#define S_SYNMAPEN    9
+#define V_SYNMAPEN(x) ((x) << S_SYNMAPEN)
+#define F_SYNMAPEN    V_SYNMAPEN(1U)
+
+#define S_SYNLKPEN    8
+#define V_SYNLKPEN(x) ((x) << S_SYNLKPEN)
+#define F_SYNLKPEN    V_SYNLKPEN(1U)
+
 #define S_RRCPLMAPEN    7
 #define V_RRCPLMAPEN(x) ((x) << S_RRCPLMAPEN)
 #define F_RRCPLMAPEN    V_RRCPLMAPEN(1U)
@@ -1369,6 +4506,7 @@
 #define S_RRCPLCPUSIZE    4
 #define M_RRCPLCPUSIZE    0x7
 #define V_RRCPLCPUSIZE(x) ((x) << S_RRCPLCPUSIZE)
+#define G_RRCPLCPUSIZE(x) (((x) >> S_RRCPLCPUSIZE) & M_RRCPLCPUSIZE)
 
 #define S_RQFEEDBACKENABLE    3
 #define V_RQFEEDBACKENABLE(x) ((x) << S_RQFEEDBACKENABLE)
@@ -1378,34 +4516,179 @@
 #define V_HASHTOEPLITZ(x) ((x) << S_HASHTOEPLITZ)
 #define F_HASHTOEPLITZ    V_HASHTOEPLITZ(1U)
 
+#define S_HASHSAVE    1
+#define V_HASHSAVE(x) ((x) << S_HASHSAVE)
+#define F_HASHSAVE    V_HASHSAVE(1U)
+
 #define S_DISABLE    0
-
+#define V_DISABLE(x) ((x) << S_DISABLE)
+#define F_DISABLE    V_DISABLE(1U)
+
+#define A_TP_RSS_CONFIG_TNL 0x3f4
+
+#define S_MASKSIZE    28
+#define M_MASKSIZE    0x7
+#define V_MASKSIZE(x) ((x) << S_MASKSIZE)
+#define G_MASKSIZE(x) (((x) >> S_MASKSIZE) & M_MASKSIZE)
+
+#define S_DEFAULTCPUBASE    22
+#define M_DEFAULTCPUBASE    0x3f
+#define V_DEFAULTCPUBASE(x) ((x) << S_DEFAULTCPUBASE)
+#define G_DEFAULTCPUBASE(x) (((x) >> S_DEFAULTCPUBASE) & M_DEFAULTCPUBASE)
+
+#define S_DEFAULTCPU    16
+#define M_DEFAULTCPU    0x3f
+#define V_DEFAULTCPU(x) ((x) << S_DEFAULTCPU)
+#define G_DEFAULTCPU(x) (((x) >> S_DEFAULTCPU) & M_DEFAULTCPU)
+
+#define S_DEFAULTQUEUE    0
+#define M_DEFAULTQUEUE    0xffff
+#define V_DEFAULTQUEUE(x) ((x) << S_DEFAULTQUEUE)
+#define G_DEFAULTQUEUE(x) (((x) >> S_DEFAULTQUEUE) & M_DEFAULTQUEUE)
+
+#define A_TP_RSS_CONFIG_OFD 0x3f8
+#define A_TP_RSS_CONFIG_SYN 0x3fc
+#define A_TP_RSS_SECRET_KEY0 0x400
+#define A_TP_RSS_SECRET_KEY1 0x404
+#define A_TP_RSS_SECRET_KEY2 0x408
+#define A_TP_RSS_SECRET_KEY3 0x40c
 #define A_TP_TM_PIO_ADDR 0x418
-
 #define A_TP_TM_PIO_DATA 0x41c
-
 #define A_TP_TX_MOD_QUE_TABLE 0x420
-
 #define A_TP_TX_RESOURCE_LIMIT 0x424
 
+#define S_TX_RESOURCE_LIMIT_CH1_PC    24
+#define M_TX_RESOURCE_LIMIT_CH1_PC    0xff
+#define V_TX_RESOURCE_LIMIT_CH1_PC(x) ((x) << S_TX_RESOURCE_LIMIT_CH1_PC)
+#define G_TX_RESOURCE_LIMIT_CH1_PC(x) (((x) >> S_TX_RESOURCE_LIMIT_CH1_PC) & M_TX_RESOURCE_LIMIT_CH1_PC)
+
+#define S_TX_RESOURCE_LIMIT_CH1_NON_PC    16
+#define M_TX_RESOURCE_LIMIT_CH1_NON_PC    0xff
+#define V_TX_RESOURCE_LIMIT_CH1_NON_PC(x) ((x) << S_TX_RESOURCE_LIMIT_CH1_NON_PC)
+#define G_TX_RESOURCE_LIMIT_CH1_NON_PC(x) (((x) >> S_TX_RESOURCE_LIMIT_CH1_NON_PC) & M_TX_RESOURCE_LIMIT_CH1_NON_PC)
+
+#define S_TX_RESOURCE_LIMIT_CH0_PC    8
+#define M_TX_RESOURCE_LIMIT_CH0_PC    0xff
+#define V_TX_RESOURCE_LIMIT_CH0_PC(x) ((x) << S_TX_RESOURCE_LIMIT_CH0_PC)
+#define G_TX_RESOURCE_LIMIT_CH0_PC(x) (((x) >> S_TX_RESOURCE_LIMIT_CH0_PC) & M_TX_RESOURCE_LIMIT_CH0_PC)
+
+#define S_TX_RESOURCE_LIMIT_CH0_NON_PC    0
+#define M_TX_RESOURCE_LIMIT_CH0_NON_PC    0xff
+#define V_TX_RESOURCE_LIMIT_CH0_NON_PC(x) ((x) << S_TX_RESOURCE_LIMIT_CH0_NON_PC)
+#define G_TX_RESOURCE_LIMIT_CH0_NON_PC(x) (((x) >> S_TX_RESOURCE_LIMIT_CH0_NON_PC) & M_TX_RESOURCE_LIMIT_CH0_NON_PC)
+
 #define A_TP_TX_MOD_QUEUE_REQ_MAP 0x428
 
+#define S_RX_MOD_WEIGHT    24
+#define M_RX_MOD_WEIGHT    0xff
+#define V_RX_MOD_WEIGHT(x) ((x) << S_RX_MOD_WEIGHT)
+#define G_RX_MOD_WEIGHT(x) (((x) >> S_RX_MOD_WEIGHT) & M_RX_MOD_WEIGHT)
+
+#define S_TX_MOD_WEIGHT    16
+#define M_TX_MOD_WEIGHT    0xff
+#define V_TX_MOD_WEIGHT(x) ((x) << S_TX_MOD_WEIGHT)
+#define G_TX_MOD_WEIGHT(x) (((x) >> S_TX_MOD_WEIGHT) & M_TX_MOD_WEIGHT)
+
+#define S_TX_MOD_TIMER_MODE    8
+#define M_TX_MOD_TIMER_MODE    0xff
+#define V_TX_MOD_TIMER_MODE(x) ((x) << S_TX_MOD_TIMER_MODE)
+#define G_TX_MOD_TIMER_MODE(x) (((x) >> S_TX_MOD_TIMER_MODE) & M_TX_MOD_TIMER_MODE)
+
 #define S_TX_MOD_QUEUE_REQ_MAP    0
 #define M_TX_MOD_QUEUE_REQ_MAP    0xff
 #define V_TX_MOD_QUEUE_REQ_MAP(x) ((x) << S_TX_MOD_QUEUE_REQ_MAP)
+#define G_TX_MOD_QUEUE_REQ_MAP(x) (((x) >> S_TX_MOD_QUEUE_REQ_MAP) & M_TX_MOD_QUEUE_REQ_MAP)
 
 #define A_TP_TX_MOD_QUEUE_WEIGHT1 0x42c
 
+#define S_TP_TX_MODQ_WGHT7    24
+#define M_TP_TX_MODQ_WGHT7    0xff
+#define V_TP_TX_MODQ_WGHT7(x) ((x) << S_TP_TX_MODQ_WGHT7)
+#define G_TP_TX_MODQ_WGHT7(x) (((x) >> S_TP_TX_MODQ_WGHT7) & M_TP_TX_MODQ_WGHT7)
+
+#define S_TP_TX_MODQ_WGHT6    16
+#define M_TP_TX_MODQ_WGHT6    0xff
+#define V_TP_TX_MODQ_WGHT6(x) ((x) << S_TP_TX_MODQ_WGHT6)
+#define G_TP_TX_MODQ_WGHT6(x) (((x) >> S_TP_TX_MODQ_WGHT6) & M_TP_TX_MODQ_WGHT6)
+
+#define S_TP_TX_MODQ_WGHT5    8
+#define M_TP_TX_MODQ_WGHT5    0xff
+#define V_TP_TX_MODQ_WGHT5(x) ((x) << S_TP_TX_MODQ_WGHT5)
+#define G_TP_TX_MODQ_WGHT5(x) (((x) >> S_TP_TX_MODQ_WGHT5) & M_TP_TX_MODQ_WGHT5)
+
+#define S_TP_TX_MODQ_WGHT4    0
+#define M_TP_TX_MODQ_WGHT4    0xff
+#define V_TP_TX_MODQ_WGHT4(x) ((x) << S_TP_TX_MODQ_WGHT4)
+#define G_TP_TX_MODQ_WGHT4(x) (((x) >> S_TP_TX_MODQ_WGHT4) & M_TP_TX_MODQ_WGHT4)
+
 #define A_TP_TX_MOD_QUEUE_WEIGHT0 0x430
 
+#define S_TP_TX_MODQ_WGHT3    24
+#define M_TP_TX_MODQ_WGHT3    0xff
+#define V_TP_TX_MODQ_WGHT3(x) ((x) << S_TP_TX_MODQ_WGHT3)
+#define G_TP_TX_MODQ_WGHT3(x) (((x) >> S_TP_TX_MODQ_WGHT3) & M_TP_TX_MODQ_WGHT3)
+
+#define S_TP_TX_MODQ_WGHT2    16
+#define M_TP_TX_MODQ_WGHT2    0xff
+#define V_TP_TX_MODQ_WGHT2(x) ((x) << S_TP_TX_MODQ_WGHT2)
+#define G_TP_TX_MODQ_WGHT2(x) (((x) >> S_TP_TX_MODQ_WGHT2) & M_TP_TX_MODQ_WGHT2)
+
+#define S_TP_TX_MODQ_WGHT1    8
+#define M_TP_TX_MODQ_WGHT1    0xff
+#define V_TP_TX_MODQ_WGHT1(x) ((x) << S_TP_TX_MODQ_WGHT1)
+#define G_TP_TX_MODQ_WGHT1(x) (((x) >> S_TP_TX_MODQ_WGHT1) & M_TP_TX_MODQ_WGHT1)
+
+#define S_TP_TX_MODQ_WGHT0    0
+#define M_TP_TX_MODQ_WGHT0    0xff
+#define V_TP_TX_MODQ_WGHT0(x) ((x) << S_TP_TX_MODQ_WGHT0)
+#define G_TP_TX_MODQ_WGHT0(x) (((x) >> S_TP_TX_MODQ_WGHT0) & M_TP_TX_MODQ_WGHT0)
+
 #define A_TP_MOD_CHANNEL_WEIGHT 0x434
 
+#define S_RX_MOD_CHANNEL_WEIGHT1    24
+#define M_RX_MOD_CHANNEL_WEIGHT1    0xff
+#define V_RX_MOD_CHANNEL_WEIGHT1(x) ((x) << S_RX_MOD_CHANNEL_WEIGHT1)
+#define G_RX_MOD_CHANNEL_WEIGHT1(x) (((x) >> S_RX_MOD_CHANNEL_WEIGHT1) & M_RX_MOD_CHANNEL_WEIGHT1)
+
+#define S_RX_MOD_CHANNEL_WEIGHT0    16
+#define M_RX_MOD_CHANNEL_WEIGHT0    0xff
+#define V_RX_MOD_CHANNEL_WEIGHT0(x) ((x) << S_RX_MOD_CHANNEL_WEIGHT0)
+#define G_RX_MOD_CHANNEL_WEIGHT0(x) (((x) >> S_RX_MOD_CHANNEL_WEIGHT0) & M_RX_MOD_CHANNEL_WEIGHT0)
+
+#define S_TX_MOD_CHANNEL_WEIGHT1    8
+#define M_TX_MOD_CHANNEL_WEIGHT1    0xff
+#define V_TX_MOD_CHANNEL_WEIGHT1(x) ((x) << S_TX_MOD_CHANNEL_WEIGHT1)
+#define G_TX_MOD_CHANNEL_WEIGHT1(x) (((x) >> S_TX_MOD_CHANNEL_WEIGHT1) & M_TX_MOD_CHANNEL_WEIGHT1)
+
+#define S_TX_MOD_CHANNEL_WEIGHT0    0
+#define M_TX_MOD_CHANNEL_WEIGHT0    0xff
+#define V_TX_MOD_CHANNEL_WEIGHT0(x) ((x) << S_TX_MOD_CHANNEL_WEIGHT0)
+#define G_TX_MOD_CHANNEL_WEIGHT0(x) (((x) >> S_TX_MOD_CHANNEL_WEIGHT0) & M_TX_MOD_CHANNEL_WEIGHT0)
+
 #define A_TP_MOD_RATE_LIMIT 0x438
 
+#define S_RX_MOD_RATE_LIMIT_INC    24
+#define M_RX_MOD_RATE_LIMIT_INC    0xff
+#define V_RX_MOD_RATE_LIMIT_INC(x) ((x) << S_RX_MOD_RATE_LIMIT_INC)
+#define G_RX_MOD_RATE_LIMIT_INC(x) (((x) >> S_RX_MOD_RATE_LIMIT_INC) & M_RX_MOD_RATE_LIMIT_INC)
+
+#define S_RX_MOD_RATE_LIMIT_TICK    16
+#define M_RX_MOD_RATE_LIMIT_TICK    0xff
+#define V_RX_MOD_RATE_LIMIT_TICK(x) ((x) << S_RX_MOD_RATE_LIMIT_TICK)
+#define G_RX_MOD_RATE_LIMIT_TICK(x) (((x) >> S_RX_MOD_RATE_LIMIT_TICK) & M_RX_MOD_RATE_LIMIT_TICK)
+
+#define S_TX_MOD_RATE_LIMIT_INC    8
+#define M_TX_MOD_RATE_LIMIT_INC    0xff
+#define V_TX_MOD_RATE_LIMIT_INC(x) ((x) << S_TX_MOD_RATE_LIMIT_INC)
+#define G_TX_MOD_RATE_LIMIT_INC(x) (((x) >> S_TX_MOD_RATE_LIMIT_INC) & M_TX_MOD_RATE_LIMIT_INC)
+
+#define S_TX_MOD_RATE_LIMIT_TICK    0
+#define M_TX_MOD_RATE_LIMIT_TICK    0xff
+#define V_TX_MOD_RATE_LIMIT_TICK(x) ((x) << S_TX_MOD_RATE_LIMIT_TICK)
+#define G_TX_MOD_RATE_LIMIT_TICK(x) (((x) >> S_TX_MOD_RATE_LIMIT_TICK) & M_TX_MOD_RATE_LIMIT_TICK)
+
 #define A_TP_PIO_ADDR 0x440
-
 #define A_TP_PIO_DATA 0x444
-
 #define A_TP_RESET 0x44c
 
 #define S_FLSTINITENABLE    1
@@ -1416,18 +4699,38 @@
 #define V_TPRESET(x) ((x) << S_TPRESET)
 #define F_TPRESET    V_TPRESET(1U)
 
+#define A_TP_MIB_INDEX 0x450
+#define A_TP_MIB_RDATA 0x454
+#define A_TP_SYNC_TIME_HI 0x458
+#define A_TP_SYNC_TIME_LO 0x45c
 #define A_TP_CMM_MM_RX_FLST_BASE 0x460
 
+#define S_CMRXFLSTBASE    0
+#define M_CMRXFLSTBASE    0xfffffff
+#define V_CMRXFLSTBASE(x) ((x) << S_CMRXFLSTBASE)
+#define G_CMRXFLSTBASE(x) (((x) >> S_CMRXFLSTBASE) & M_CMRXFLSTBASE)
+
 #define A_TP_CMM_MM_TX_FLST_BASE 0x464
 
+#define S_CMTXFLSTBASE    0
+#define M_CMTXFLSTBASE    0xfffffff
+#define V_CMTXFLSTBASE(x) ((x) << S_CMTXFLSTBASE)
+#define G_CMTXFLSTBASE(x) (((x) >> S_CMTXFLSTBASE) & M_CMTXFLSTBASE)
+
 #define A_TP_CMM_MM_PS_FLST_BASE 0x468
 
-#define A_TP_MIB_INDEX 0x450
-
-#define A_TP_MIB_RDATA 0x454
+#define S_CMPSFLSTBASE    0
+#define M_CMPSFLSTBASE    0xfffffff
+#define V_CMPSFLSTBASE(x) ((x) << S_CMPSFLSTBASE)
+#define G_CMPSFLSTBASE(x) (((x) >> S_CMPSFLSTBASE) & M_CMPSFLSTBASE)
 
 #define A_TP_CMM_MM_MAX_PSTRUCT 0x46c
 
+#define S_CMMAXPSTRUCT    0
+#define M_CMMAXPSTRUCT    0x1fffff
+#define V_CMMAXPSTRUCT(x) ((x) << S_CMMAXPSTRUCT)
+#define G_CMMAXPSTRUCT(x) (((x) >> S_CMMAXPSTRUCT) & M_CMMAXPSTRUCT)
+
 #define A_TP_INT_ENABLE 0x470
 
 #define S_FLMTXFLSTEMPTY    30
@@ -1438,55 +4741,629 @@
 #define V_FLMRXFLSTEMPTY(x) ((x) << S_FLMRXFLSTEMPTY)
 #define F_FLMRXFLSTEMPTY    V_FLMRXFLSTEMPTY(1U)
 
+#define S_FLMPERRSET    28
+#define V_FLMPERRSET(x) ((x) << S_FLMPERRSET)
+#define F_FLMPERRSET    V_FLMPERRSET(1U)
+
+#define S_PROTOCOLSRAMPERR    27
+#define V_PROTOCOLSRAMPERR(x) ((x) << S_PROTOCOLSRAMPERR)
+#define F_PROTOCOLSRAMPERR    V_PROTOCOLSRAMPERR(1U)
+
 #define S_ARPLUTPERR    26
 #define V_ARPLUTPERR(x) ((x) << S_ARPLUTPERR)
 #define F_ARPLUTPERR    V_ARPLUTPERR(1U)
 
+#define S_CMRCFOPPERR    25
+#define V_CMRCFOPPERR(x) ((x) << S_CMRCFOPPERR)
+#define F_CMRCFOPPERR    V_CMRCFOPPERR(1U)
+
 #define S_CMCACHEPERR    24
 #define V_CMCACHEPERR(x) ((x) << S_CMCACHEPERR)
 #define F_CMCACHEPERR    V_CMCACHEPERR(1U)
 
+#define S_CMRCFDATAPERR    23
+#define V_CMRCFDATAPERR(x) ((x) << S_CMRCFDATAPERR)
+#define F_CMRCFDATAPERR    V_CMRCFDATAPERR(1U)
+
+#define S_DBL2TLUTPERR    22
+#define V_DBL2TLUTPERR(x) ((x) << S_DBL2TLUTPERR)
+#define F_DBL2TLUTPERR    V_DBL2TLUTPERR(1U)
+
+#define S_DBTXTIDPERR    21
+#define V_DBTXTIDPERR(x) ((x) << S_DBTXTIDPERR)
+#define F_DBTXTIDPERR    V_DBTXTIDPERR(1U)
+
+#define S_DBEXTPERR    20
+#define V_DBEXTPERR(x) ((x) << S_DBEXTPERR)
+#define F_DBEXTPERR    V_DBEXTPERR(1U)
+
+#define S_DBOPPERR    19
+#define V_DBOPPERR(x) ((x) << S_DBOPPERR)
+#define F_DBOPPERR    V_DBOPPERR(1U)
+
+#define S_TMCACHEPERR    18
+#define V_TMCACHEPERR(x) ((x) << S_TMCACHEPERR)
+#define F_TMCACHEPERR    V_TMCACHEPERR(1U)
+
+#define S_ETPOUTCPLFIFOPERR    17
+#define V_ETPOUTCPLFIFOPERR(x) ((x) << S_ETPOUTCPLFIFOPERR)
+#define F_ETPOUTCPLFIFOPERR    V_ETPOUTCPLFIFOPERR(1U)
+
+#define S_ETPOUTTCPFIFOPERR    16
+#define V_ETPOUTTCPFIFOPERR(x) ((x) << S_ETPOUTTCPFIFOPERR)
+#define F_ETPOUTTCPFIFOPERR    V_ETPOUTTCPFIFOPERR(1U)
+
+#define S_ETPOUTIPFIFOPERR    15
+#define V_ETPOUTIPFIFOPERR(x) ((x) << S_ETPOUTIPFIFOPERR)
+#define F_ETPOUTIPFIFOPERR    V_ETPOUTIPFIFOPERR(1U)
+
+#define S_ETPOUTETHFIFOPERR    14
+#define V_ETPOUTETHFIFOPERR(x) ((x) << S_ETPOUTETHFIFOPERR)
+#define F_ETPOUTETHFIFOPERR    V_ETPOUTETHFIFOPERR(1U)
+
+#define S_ETPINCPLFIFOPERR    13
+#define V_ETPINCPLFIFOPERR(x) ((x) << S_ETPINCPLFIFOPERR)
+#define F_ETPINCPLFIFOPERR    V_ETPINCPLFIFOPERR(1U)
+
+#define S_ETPINTCPOPTFIFOPERR    12
+#define V_ETPINTCPOPTFIFOPERR(x) ((x) << S_ETPINTCPOPTFIFOPERR)
+#define F_ETPINTCPOPTFIFOPERR    V_ETPINTCPOPTFIFOPERR(1U)
+
+#define S_ETPINTCPFIFOPERR    11
+#define V_ETPINTCPFIFOPERR(x) ((x) << S_ETPINTCPFIFOPERR)
+#define F_ETPINTCPFIFOPERR    V_ETPINTCPFIFOPERR(1U)
+
+#define S_ETPINIPFIFOPERR    10
+#define V_ETPINIPFIFOPERR(x) ((x) << S_ETPINIPFIFOPERR)
+#define F_ETPINIPFIFOPERR    V_ETPINIPFIFOPERR(1U)
+
+#define S_ETPINETHFIFOPERR    9
+#define V_ETPINETHFIFOPERR(x) ((x) << S_ETPINETHFIFOPERR)
+#define F_ETPINETHFIFOPERR    V_ETPINETHFIFOPERR(1U)
+
+#define S_CTPOUTCPLFIFOPERR    8
+#define V_CTPOUTCPLFIFOPERR(x) ((x) << S_CTPOUTCPLFIFOPERR)
+#define F_CTPOUTCPLFIFOPERR    V_CTPOUTCPLFIFOPERR(1U)
+
+#define S_CTPOUTTCPFIFOPERR    7
+#define V_CTPOUTTCPFIFOPERR(x) ((x) << S_CTPOUTTCPFIFOPERR)
+#define F_CTPOUTTCPFIFOPERR    V_CTPOUTTCPFIFOPERR(1U)
+
+#define S_CTPOUTIPFIFOPERR    6
+#define V_CTPOUTIPFIFOPERR(x) ((x) << S_CTPOUTIPFIFOPERR)
+#define F_CTPOUTIPFIFOPERR    V_CTPOUTIPFIFOPERR(1U)
+
+#define S_CTPOUTETHFIFOPERR    5
+#define V_CTPOUTETHFIFOPERR(x) ((x) << S_CTPOUTETHFIFOPERR)
+#define F_CTPOUTETHFIFOPERR    V_CTPOUTETHFIFOPERR(1U)
+
+#define S_CTPINCPLFIFOPERR    4
+#define V_CTPINCPLFIFOPERR(x) ((x) << S_CTPINCPLFIFOPERR)
+#define F_CTPINCPLFIFOPERR    V_CTPINCPLFIFOPERR(1U)
+
+#define S_CTPINTCPOPFIFOPERR    3
+#define V_CTPINTCPOPFIFOPERR(x) ((x) << S_CTPINTCPOPFIFOPERR)
+#define F_CTPINTCPOPFIFOPERR    V_CTPINTCPOPFIFOPERR(1U)
+
+#define S_CTPINTCPFIFOPERR    2
+#define V_CTPINTCPFIFOPERR(x) ((x) << S_CTPINTCPFIFOPERR)
+#define F_CTPINTCPFIFOPERR    V_CTPINTCPFIFOPERR(1U)
+
+#define S_CTPINIPFIFOPERR    1
+#define V_CTPINIPFIFOPERR(x) ((x) << S_CTPINIPFIFOPERR)
+#define F_CTPINIPFIFOPERR    V_CTPINIPFIFOPERR(1U)
+
+#define S_CTPINETHFIFOPERR    0
+#define V_CTPINETHFIFOPERR(x) ((x) << S_CTPINETHFIFOPERR)
+#define F_CTPINETHFIFOPERR    V_CTPINETHFIFOPERR(1U)
+
 #define A_TP_INT_CAUSE 0x474
-
-#define A_TP_TX_MOD_Q1_Q0_RATE_LIMIT 0x8
-
-#define A_TP_TX_DROP_CFG_CH0 0x12b
-
-#define A_TP_TX_DROP_MODE 0x12f
-
-#define A_TP_EGRESS_CONFIG 0x145
-
-#define S_REWRITEFORCETOSIZE    0
-#define V_REWRITEFORCETOSIZE(x) ((x) << S_REWRITEFORCETOSIZE)
-#define F_REWRITEFORCETOSIZE    V_REWRITEFORCETOSIZE(1U)
-
-#define A_TP_TX_TRC_KEY0 0x20
-
-#define A_TP_RX_TRC_KEY0 0x120
-
-#define A_TP_TX_DROP_CNT_CH0 0x12d
-
-#define S_TXDROPCNTCH0RCVD    0
-#define M_TXDROPCNTCH0RCVD    0xffff
-#define V_TXDROPCNTCH0RCVD(x) ((x) << S_TXDROPCNTCH0RCVD)
-#define G_TXDROPCNTCH0RCVD(x) (((x) >> S_TXDROPCNTCH0RCVD) & \
-			       M_TXDROPCNTCH0RCVD)
+#define A_TP_FLM_FREE_PS_CNT 0x480
+
+#define S_FREEPSTRUCTCOUNT    0
+#define M_FREEPSTRUCTCOUNT    0x1fffff
+#define V_FREEPSTRUCTCOUNT(x) ((x) << S_FREEPSTRUCTCOUNT)
+#define G_FREEPSTRUCTCOUNT(x) (((x) >> S_FREEPSTRUCTCOUNT) & M_FREEPSTRUCTCOUNT)
+
+#define A_TP_FLM_FREE_RX_CNT 0x484
+
+#define S_FREERXPAGECOUNT    0
+#define M_FREERXPAGECOUNT    0x1fffff
+#define V_FREERXPAGECOUNT(x) ((x) << S_FREERXPAGECOUNT)
+#define G_FREERXPAGECOUNT(x) (((x) >> S_FREERXPAGECOUNT) & M_FREERXPAGECOUNT)
+
+#define A_TP_FLM_FREE_TX_CNT 0x488
+
+#define S_FREETXPAGECOUNT    0
+#define M_FREETXPAGECOUNT    0x1fffff
+#define V_FREETXPAGECOUNT(x) ((x) << S_FREETXPAGECOUNT)
+#define G_FREETXPAGECOUNT(x) (((x) >> S_FREETXPAGECOUNT) & M_FREETXPAGECOUNT)
+
+#define A_TP_TM_HEAP_PUSH_CNT 0x48c
+#define A_TP_TM_HEAP_POP_CNT 0x490
+#define A_TP_TM_DACK_PUSH_CNT 0x494
+#define A_TP_TM_DACK_POP_CNT 0x498
+#define A_TP_TM_MOD_PUSH_CNT 0x49c
+#define A_TP_MOD_POP_CNT 0x4a0
+#define A_TP_TIMER_SEPARATOR 0x4a4
+#define A_TP_DEBUG_SEL 0x4a8
+#define A_TP_DEBUG_FLAGS 0x4ac
+
+#define S_RXTIMERDACKFIRST    26
+#define V_RXTIMERDACKFIRST(x) ((x) << S_RXTIMERDACKFIRST)
+#define F_RXTIMERDACKFIRST    V_RXTIMERDACKFIRST(1U)
+
+#define S_RXTIMERDACK    25
+#define V_RXTIMERDACK(x) ((x) << S_RXTIMERDACK)
+#define F_RXTIMERDACK    V_RXTIMERDACK(1U)
+
+#define S_RXTIMERHEARTBEAT    24
+#define V_RXTIMERHEARTBEAT(x) ((x) << S_RXTIMERHEARTBEAT)
+#define F_RXTIMERHEARTBEAT    V_RXTIMERHEARTBEAT(1U)
+
+#define S_RXPAWSDROP    23
+#define V_RXPAWSDROP(x) ((x) << S_RXPAWSDROP)
+#define F_RXPAWSDROP    V_RXPAWSDROP(1U)
+
+#define S_RXURGDATADROP    22
+#define V_RXURGDATADROP(x) ((x) << S_RXURGDATADROP)
+#define F_RXURGDATADROP    V_RXURGDATADROP(1U)
+
+#define S_RXFUTUREDATA    21
+#define V_RXFUTUREDATA(x) ((x) << S_RXFUTUREDATA)
+#define F_RXFUTUREDATA    V_RXFUTUREDATA(1U)
+
+#define S_RXRCVRXMDATA    20
+#define V_RXRCVRXMDATA(x) ((x) << S_RXRCVRXMDATA)
+#define F_RXRCVRXMDATA    V_RXRCVRXMDATA(1U)
+
+#define S_RXRCVOOODATAFIN    19
+#define V_RXRCVOOODATAFIN(x) ((x) << S_RXRCVOOODATAFIN)
+#define F_RXRCVOOODATAFIN    V_RXRCVOOODATAFIN(1U)
+
+#define S_RXRCVOOODATA    18
+#define V_RXRCVOOODATA(x) ((x) << S_RXRCVOOODATA)
+#define F_RXRCVOOODATA    V_RXRCVOOODATA(1U)
+
+#define S_RXRCVWNDZERO    17
+#define V_RXRCVWNDZERO(x) ((x) << S_RXRCVWNDZERO)
+#define F_RXRCVWNDZERO    V_RXRCVWNDZERO(1U)
+
+#define S_RXRCVWNDLTMSS    16
+#define V_RXRCVWNDLTMSS(x) ((x) << S_RXRCVWNDLTMSS)
+#define F_RXRCVWNDLTMSS    V_RXRCVWNDLTMSS(1U)
+
+#define S_TXDUPACKINC    11
+#define V_TXDUPACKINC(x) ((x) << S_TXDUPACKINC)
+#define F_TXDUPACKINC    V_TXDUPACKINC(1U)
+
+#define S_TXRXMURG    10
+#define V_TXRXMURG(x) ((x) << S_TXRXMURG)
+#define F_TXRXMURG    V_TXRXMURG(1U)
+
+#define S_TXRXMFIN    9
+#define V_TXRXMFIN(x) ((x) << S_TXRXMFIN)
+#define F_TXRXMFIN    V_TXRXMFIN(1U)
+
+#define S_TXRXMSYN    8
+#define V_TXRXMSYN(x) ((x) << S_TXRXMSYN)
+#define F_TXRXMSYN    V_TXRXMSYN(1U)
+
+#define S_TXRXMNEWRENO    7
+#define V_TXRXMNEWRENO(x) ((x) << S_TXRXMNEWRENO)
+#define F_TXRXMNEWRENO    V_TXRXMNEWRENO(1U)
+
+#define S_TXRXMFAST    6
+#define V_TXRXMFAST(x) ((x) << S_TXRXMFAST)
+#define F_TXRXMFAST    V_TXRXMFAST(1U)
+
+#define S_TXRXMTIMER    5
+#define V_TXRXMTIMER(x) ((x) << S_TXRXMTIMER)
+#define F_TXRXMTIMER    V_TXRXMTIMER(1U)
+
+#define S_TXRXMTIMERKEEPALIVE    4
+#define V_TXRXMTIMERKEEPALIVE(x) ((x) << S_TXRXMTIMERKEEPALIVE)
+#define F_TXRXMTIMERKEEPALIVE    V_TXRXMTIMERKEEPALIVE(1U)
+
+#define S_TXRXMTIMERPERSIST    3
+#define V_TXRXMTIMERPERSIST(x) ((x) << S_TXRXMTIMERPERSIST)
+#define F_TXRXMTIMERPERSIST    V_TXRXMTIMERPERSIST(1U)
+
+#define S_TXRCVADVSHRUNK    2
+#define V_TXRCVADVSHRUNK(x) ((x) << S_TXRCVADVSHRUNK)
+#define F_TXRCVADVSHRUNK    V_TXRCVADVSHRUNK(1U)
+
+#define S_TXRCVADVZERO    1
+#define V_TXRCVADVZERO(x) ((x) << S_TXRCVADVZERO)
+#define F_TXRCVADVZERO    V_TXRCVADVZERO(1U)
+
+#define S_TXRCVADVLTMSS    0
+#define V_TXRCVADVLTMSS(x) ((x) << S_TXRCVADVLTMSS)
+#define F_TXRCVADVLTMSS    V_TXRCVADVLTMSS(1U)
+
+#define S_RXDEBUGFLAGS    16
+#define M_RXDEBUGFLAGS    0xffff
+#define V_RXDEBUGFLAGS(x) ((x) << S_RXDEBUGFLAGS)
+#define G_RXDEBUGFLAGS(x) (((x) >> S_RXDEBUGFLAGS) & M_RXDEBUGFLAGS)
+
+#define S_TXDEBUGFLAGS    0
+#define M_TXDEBUGFLAGS    0xffff
+#define V_TXDEBUGFLAGS(x) ((x) << S_TXDEBUGFLAGS)
+#define G_TXDEBUGFLAGS(x) (((x) >> S_TXDEBUGFLAGS) & M_TXDEBUGFLAGS)
 
 #define A_TP_PROXY_FLOW_CNTL 0x4b0
-
+#define A_TP_CM_FLOW_CNTL_MODE 0x4b0
+
+#define S_CMFLOWCACHEDISABLE    0
+#define V_CMFLOWCACHEDISABLE(x) ((x) << S_CMFLOWCACHEDISABLE)
+#define F_CMFLOWCACHEDISABLE    V_CMFLOWCACHEDISABLE(1U)
+
+#define A_TP_PC_CONGESTION_CNTL 0x4b4
+
+#define S_EDROPTUNNEL    19
+#define V_EDROPTUNNEL(x) ((x) << S_EDROPTUNNEL)
+#define F_EDROPTUNNEL    V_EDROPTUNNEL(1U)
+
+#define S_CDROPTUNNEL    18
+#define V_CDROPTUNNEL(x) ((x) << S_CDROPTUNNEL)
+#define F_CDROPTUNNEL    V_CDROPTUNNEL(1U)
+
+#define S_ETHRESHOLD    12
+#define M_ETHRESHOLD    0x3f
+#define V_ETHRESHOLD(x) ((x) << S_ETHRESHOLD)
+#define G_ETHRESHOLD(x) (((x) >> S_ETHRESHOLD) & M_ETHRESHOLD)
+
+#define S_CTHRESHOLD    6
+#define M_CTHRESHOLD    0x3f
+#define V_CTHRESHOLD(x) ((x) << S_CTHRESHOLD)
+#define G_CTHRESHOLD(x) (((x) >> S_CTHRESHOLD) & M_CTHRESHOLD)
+
+#define S_TXTHRESHOLD    0
+#define M_TXTHRESHOLD    0x3f
+#define V_TXTHRESHOLD(x) ((x) << S_TXTHRESHOLD)
+#define G_TXTHRESHOLD(x) (((x) >> S_TXTHRESHOLD) & M_TXTHRESHOLD)
+
+#define A_TP_TX_DROP_COUNT 0x4bc
+#define A_TP_CLEAR_DEBUG 0x4c0
+
+#define S_CLRDEBUG    0
+#define V_CLRDEBUG(x) ((x) << S_CLRDEBUG)
+#define F_CLRDEBUG    V_CLRDEBUG(1U)
+
+#define A_TP_DEBUG_VEC 0x4c4
+#define A_TP_DEBUG_VEC2 0x4c8
+#define A_TP_DEBUG_REG_SEL 0x4cc
+#define A_TP_DEBUG 0x4d0
+#define A_TP_DBG_LA_CONFIG 0x4d4
+#define A_TP_DBG_LA_DATAH 0x4d8
+#define A_TP_DBG_LA_DATAL 0x4dc
 #define A_TP_EMBED_OP_FIELD0 0x4e8
 #define A_TP_EMBED_OP_FIELD1 0x4ec
 #define A_TP_EMBED_OP_FIELD2 0x4f0
 #define A_TP_EMBED_OP_FIELD3 0x4f4
 #define A_TP_EMBED_OP_FIELD4 0x4f8
 #define A_TP_EMBED_OP_FIELD5 0x4fc
+#define A_TP_TX_MOD_Q7_Q6_TIMER_SEPARATOR 0x0
+#define A_TP_TX_MOD_Q5_Q4_TIMER_SEPARATOR 0x1
+#define A_TP_TX_MOD_Q3_Q2_TIMER_SEPARATOR 0x2
+#define A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR 0x3
+#define A_TP_RX_MOD_Q1_Q0_TIMER_SEPARATOR 0x4
+#define A_TP_TX_MOD_Q7_Q6_RATE_LIMIT 0x5
+#define A_TP_TX_MOD_Q5_Q4_RATE_LIMIT 0x6
+#define A_TP_TX_MOD_Q3_Q2_RATE_LIMIT 0x7
+#define A_TP_TX_MOD_Q1_Q0_RATE_LIMIT 0x8
+#define A_TP_RX_MOD_Q1_Q0_RATE_LIMIT 0x9
+#define A_TP_TX_TRC_KEY0 0x20
+#define A_TP_TX_TRC_MASK0 0x21
+#define A_TP_TX_TRC_KEY1 0x22
+#define A_TP_TX_TRC_MASK1 0x23
+#define A_TP_TX_TRC_KEY2 0x24
+#define A_TP_TX_TRC_MASK2 0x25
+#define A_TP_TX_TRC_KEY3 0x26
+#define A_TP_TX_TRC_MASK3 0x27
+#define A_TP_IPMI_CFG1 0x28
+
+#define S_VLANENABLE    31
+#define V_VLANENABLE(x) ((x) << S_VLANENABLE)
+#define F_VLANENABLE    V_VLANENABLE(1U)
+
+#define S_PRIMARYPORTENABLE    30
+#define V_PRIMARYPORTENABLE(x) ((x) << S_PRIMARYPORTENABLE)
+#define F_PRIMARYPORTENABLE    V_PRIMARYPORTENABLE(1U)
+
+#define S_SECUREPORTENABLE    29
+#define V_SECUREPORTENABLE(x) ((x) << S_SECUREPORTENABLE)
+#define F_SECUREPORTENABLE    V_SECUREPORTENABLE(1U)
+
+#define S_ARPENABLE    28
+#define V_ARPENABLE(x) ((x) << S_ARPENABLE)
+#define F_ARPENABLE    V_ARPENABLE(1U)
+
+#define S_VLAN    0
+#define M_VLAN    0xffff
+#define V_VLAN(x) ((x) << S_VLAN)
+#define G_VLAN(x) (((x) >> S_VLAN) & M_VLAN)
+
+#define A_TP_IPMI_CFG2 0x29
+
+#define S_SECUREPORT    16
+#define M_SECUREPORT    0xffff
+#define V_SECUREPORT(x) ((x) << S_SECUREPORT)
+#define G_SECUREPORT(x) (((x) >> S_SECUREPORT) & M_SECUREPORT)
+
+#define S_PRIMARYPORT    0
+#define M_PRIMARYPORT    0xffff
+#define V_PRIMARYPORT(x) ((x) << S_PRIMARYPORT)
+#define G_PRIMARYPORT(x) (((x) >> S_PRIMARYPORT) & M_PRIMARYPORT)
+
+#define A_TP_RX_TRC_KEY0 0x120
+#define A_TP_RX_TRC_MASK0 0x121
+#define A_TP_RX_TRC_KEY1 0x122
+#define A_TP_RX_TRC_MASK1 0x123
+#define A_TP_RX_TRC_KEY2 0x124
+#define A_TP_RX_TRC_MASK2 0x125
+#define A_TP_RX_TRC_KEY3 0x126
+#define A_TP_RX_TRC_MASK3 0x127
+#define A_TP_QOS_RX_TOS_MAP_H 0x128
+#define A_TP_QOS_RX_TOS_MAP_L 0x129
+#define A_TP_QOS_RX_MAP_MODE 0x12a
+
+#define S_DEFAULTCH    11
+#define V_DEFAULTCH(x) ((x) << S_DEFAULTCH)
+#define F_DEFAULTCH    V_DEFAULTCH(1U)
+
+#define S_RXMAPMODE    8
+#define M_RXMAPMODE    0x7
+#define V_RXMAPMODE(x) ((x) << S_RXMAPMODE)
+#define G_RXMAPMODE(x) (((x) >> S_RXMAPMODE) & M_RXMAPMODE)
+
+#define S_RXVLANMAP    7
+#define V_RXVLANMAP(x) ((x) << S_RXVLANMAP)
+#define F_RXVLANMAP    V_RXVLANMAP(1U)
+
+#define A_TP_TX_DROP_CFG_CH0 0x12b
+
+#define S_TIMERENABLED    31
+#define V_TIMERENABLED(x) ((x) << S_TIMERENABLED)
+#define F_TIMERENABLED    V_TIMERENABLED(1U)
+
+#define S_TIMERERRORENABLE    30
+#define V_TIMERERRORENABLE(x) ((x) << S_TIMERERRORENABLE)
+#define F_TIMERERRORENABLE    V_TIMERERRORENABLE(1U)
+
+#define S_TIMERTHRESHOLD    4
+#define M_TIMERTHRESHOLD    0x3ffffff
+#define V_TIMERTHRESHOLD(x) ((x) << S_TIMERTHRESHOLD)
+#define G_TIMERTHRESHOLD(x) (((x) >> S_TIMERTHRESHOLD) & M_TIMERTHRESHOLD)
+
+#define S_PACKETDROPS    0
+#define M_PACKETDROPS    0xf
+#define V_PACKETDROPS(x) ((x) << S_PACKETDROPS)
+#define G_PACKETDROPS(x) (((x) >> S_PACKETDROPS) & M_PACKETDROPS)
+
+#define A_TP_TX_DROP_CFG_CH1 0x12c
+#define A_TP_TX_DROP_CNT_CH0 0x12d
+
+#define S_TXDROPCNTCH0SENT    16
+#define M_TXDROPCNTCH0SENT    0xffff
+#define V_TXDROPCNTCH0SENT(x) ((x) << S_TXDROPCNTCH0SENT)
+#define G_TXDROPCNTCH0SENT(x) (((x) >> S_TXDROPCNTCH0SENT) & M_TXDROPCNTCH0SENT)
+
+#define S_TXDROPCNTCH0RCVD    0
+#define M_TXDROPCNTCH0RCVD    0xffff
+#define V_TXDROPCNTCH0RCVD(x) ((x) << S_TXDROPCNTCH0RCVD)
+#define G_TXDROPCNTCH0RCVD(x) (((x) >> S_TXDROPCNTCH0RCVD) & M_TXDROPCNTCH0RCVD)
+
+#define A_TP_TX_DROP_CNT_CH1 0x12e
+
+#define S_TXDROPCNTCH1SENT    16
+#define M_TXDROPCNTCH1SENT    0xffff
+#define V_TXDROPCNTCH1SENT(x) ((x) << S_TXDROPCNTCH1SENT)
+#define G_TXDROPCNTCH1SENT(x) (((x) >> S_TXDROPCNTCH1SENT) & M_TXDROPCNTCH1SENT)
+
+#define S_TXDROPCNTCH1RCVD    0
+#define M_TXDROPCNTCH1RCVD    0xffff
+#define V_TXDROPCNTCH1RCVD(x) ((x) << S_TXDROPCNTCH1RCVD)
+#define G_TXDROPCNTCH1RCVD(x) (((x) >> S_TXDROPCNTCH1RCVD) & M_TXDROPCNTCH1RCVD)
+
+#define A_TP_TX_DROP_MODE 0x12f
+
+#define S_TXDROPMODECH1    1
+#define V_TXDROPMODECH1(x) ((x) << S_TXDROPMODECH1)
+#define F_TXDROPMODECH1    V_TXDROPMODECH1(1U)
+
+#define S_TXDROPMODECH0    0
+#define V_TXDROPMODECH0(x) ((x) << S_TXDROPMODECH0)
+#define F_TXDROPMODECH0    V_TXDROPMODECH0(1U)
+
+#define A_TP_VLAN_PRI_MAP 0x137
+
+#define S_VLANPRIMAP7    14
+#define M_VLANPRIMAP7    0x3
+#define V_VLANPRIMAP7(x) ((x) << S_VLANPRIMAP7)
+#define G_VLANPRIMAP7(x) (((x) >> S_VLANPRIMAP7) & M_VLANPRIMAP7)
+
+#define S_VLANPRIMAP6    12
+#define M_VLANPRIMAP6    0x3
+#define V_VLANPRIMAP6(x) ((x) << S_VLANPRIMAP6)
+#define G_VLANPRIMAP6(x) (((x) >> S_VLANPRIMAP6) & M_VLANPRIMAP6)
+
+#define S_VLANPRIMAP5    10
+#define M_VLANPRIMAP5    0x3
+#define V_VLANPRIMAP5(x) ((x) << S_VLANPRIMAP5)
+#define G_VLANPRIMAP5(x) (((x) >> S_VLANPRIMAP5) & M_VLANPRIMAP5)
+
+#define S_VLANPRIMAP4    8
+#define M_VLANPRIMAP4    0x3
+#define V_VLANPRIMAP4(x) ((x) << S_VLANPRIMAP4)
+#define G_VLANPRIMAP4(x) (((x) >> S_VLANPRIMAP4) & M_VLANPRIMAP4)
+
+#define S_VLANPRIMAP3    6
+#define M_VLANPRIMAP3    0x3
+#define V_VLANPRIMAP3(x) ((x) << S_VLANPRIMAP3)
+#define G_VLANPRIMAP3(x) (((x) >> S_VLANPRIMAP3) & M_VLANPRIMAP3)
+
+#define S_VLANPRIMAP2    4
+#define M_VLANPRIMAP2    0x3
+#define V_VLANPRIMAP2(x) ((x) << S_VLANPRIMAP2)
+#define G_VLANPRIMAP2(x) (((x) >> S_VLANPRIMAP2) & M_VLANPRIMAP2)
+
+#define S_VLANPRIMAP1    2
+#define M_VLANPRIMAP1    0x3
+#define V_VLANPRIMAP1(x) ((x) << S_VLANPRIMAP1)
+#define G_VLANPRIMAP1(x) (((x) >> S_VLANPRIMAP1) & M_VLANPRIMAP1)
+
+#define S_VLANPRIMAP0    0
+#define M_VLANPRIMAP0    0x3
+#define V_VLANPRIMAP0(x) ((x) << S_VLANPRIMAP0)
+#define G_VLANPRIMAP0(x) (((x) >> S_VLANPRIMAP0) & M_VLANPRIMAP0)
+
+#define A_TP_MAC_MATCH_MAP0 0x138
+
+#define S_MACMATCHMAP7    21
+#define M_MACMATCHMAP7    0x7
+#define V_MACMATCHMAP7(x) ((x) << S_MACMATCHMAP7)
+#define G_MACMATCHMAP7(x) (((x) >> S_MACMATCHMAP7) & M_MACMATCHMAP7)
+
+#define S_MACMATCHMAP6    18
+#define M_MACMATCHMAP6    0x7
+#define V_MACMATCHMAP6(x) ((x) << S_MACMATCHMAP6)
+#define G_MACMATCHMAP6(x) (((x) >> S_MACMATCHMAP6) & M_MACMATCHMAP6)
+
+#define S_MACMATCHMAP5    15
+#define M_MACMATCHMAP5    0x7
+#define V_MACMATCHMAP5(x) ((x) << S_MACMATCHMAP5)
+#define G_MACMATCHMAP5(x) (((x) >> S_MACMATCHMAP5) & M_MACMATCHMAP5)
+
+#define S_MACMATCHMAP4    12
+#define M_MACMATCHMAP4    0x7
+#define V_MACMATCHMAP4(x) ((x) << S_MACMATCHMAP4)
+#define G_MACMATCHMAP4(x) (((x) >> S_MACMATCHMAP4) & M_MACMATCHMAP4)
+
+#define S_MACMATCHMAP3    9
+#define M_MACMATCHMAP3    0x7
+#define V_MACMATCHMAP3(x) ((x) << S_MACMATCHMAP3)
+#define G_MACMATCHMAP3(x) (((x) >> S_MACMATCHMAP3) & M_MACMATCHMAP3)
+
+#define S_MACMATCHMAP2    6
+#define M_MACMATCHMAP2    0x7
+#define V_MACMATCHMAP2(x) ((x) << S_MACMATCHMAP2)
+#define G_MACMATCHMAP2(x) (((x) >> S_MACMATCHMAP2) & M_MACMATCHMAP2)
+
+#define S_MACMATCHMAP1    3
+#define M_MACMATCHMAP1    0x7
+#define V_MACMATCHMAP1(x) ((x) << S_MACMATCHMAP1)
+#define G_MACMATCHMAP1(x) (((x) >> S_MACMATCHMAP1) & M_MACMATCHMAP1)
+
+#define S_MACMATCHMAP0    0
+#define M_MACMATCHMAP0    0x7
+#define V_MACMATCHMAP0(x) ((x) << S_MACMATCHMAP0)
+#define G_MACMATCHMAP0(x) (((x) >> S_MACMATCHMAP0) & M_MACMATCHMAP0)
+
+#define A_TP_MAC_MATCH_MAP1 0x139
+#define A_TP_INGRESS_CONFIG 0x141
+
+#define S_LOOKUPEVERYPKT    28
+#define V_LOOKUPEVERYPKT(x) ((x) << S_LOOKUPEVERYPKT)
+#define F_LOOKUPEVERYPKT    V_LOOKUPEVERYPKT(1U)
+
+#define S_ENABLEINSERTIONSFD    27
+#define V_ENABLEINSERTIONSFD(x) ((x) << S_ENABLEINSERTIONSFD)
+#define F_ENABLEINSERTIONSFD    V_ENABLEINSERTIONSFD(1U)
+
+#define S_ENABLEINSERTION    26
+#define V_ENABLEINSERTION(x) ((x) << S_ENABLEINSERTION)
+#define F_ENABLEINSERTION    V_ENABLEINSERTION(1U)
+
+#define S_ENABLEEXTRACTIONSFD    25
+#define V_ENABLEEXTRACTIONSFD(x) ((x) << S_ENABLEEXTRACTIONSFD)
+#define F_ENABLEEXTRACTIONSFD    V_ENABLEEXTRACTIONSFD(1U)
+
+#define S_ENABLEEXTRACT    24
+#define V_ENABLEEXTRACT(x) ((x) << S_ENABLEEXTRACT)
+#define F_ENABLEEXTRACT    V_ENABLEEXTRACT(1U)
+
+#define S_BITPOS3    18
+#define M_BITPOS3    0x3f
+#define V_BITPOS3(x) ((x) << S_BITPOS3)
+#define G_BITPOS3(x) (((x) >> S_BITPOS3) & M_BITPOS3)
+
+#define S_BITPOS2    12
+#define M_BITPOS2    0x3f
+#define V_BITPOS2(x) ((x) << S_BITPOS2)
+#define G_BITPOS2(x) (((x) >> S_BITPOS2) & M_BITPOS2)
+
+#define S_BITPOS1    6
+#define M_BITPOS1    0x3f
+#define V_BITPOS1(x) ((x) << S_BITPOS1)
+#define G_BITPOS1(x) (((x) >> S_BITPOS1) & M_BITPOS1)
+
+#define S_BITPOS0    0
+#define M_BITPOS0    0x3f
+#define V_BITPOS0(x) ((x) << S_BITPOS0)
+#define G_BITPOS0(x) (((x) >> S_BITPOS0) & M_BITPOS0)
+
+#define A_TP_PREAMBLE_MSB 0x142
+#define A_TP_PREAMBLE_LSB 0x143
+#define A_TP_EGRESS_CONFIG 0x145
+
+#define S_REWRITEFORCETOSIZE    0
+#define V_REWRITEFORCETOSIZE(x) ((x) << S_REWRITEFORCETOSIZE)
+#define F_REWRITEFORCETOSIZE    V_REWRITEFORCETOSIZE(1U)
+
+#define A_TP_INTF_FROM_TX_PKT 0x244
+
+#define S_INTFFROMTXPKT    0
+#define V_INTFFROMTXPKT(x) ((x) << S_INTFFROMTXPKT)
+#define F_INTFFROMTXPKT    V_INTFFROMTXPKT(1U)
+
+#define A_TP_FIFO_CONFIG 0x8c0
+
+#define S_RXFIFOCONFIG    10
+#define M_RXFIFOCONFIG    0x3f
+#define V_RXFIFOCONFIG(x) ((x) << S_RXFIFOCONFIG)
+#define G_RXFIFOCONFIG(x) (((x) >> S_RXFIFOCONFIG) & M_RXFIFOCONFIG)
+
+#define S_TXFIFOCONFIG    2
+#define M_TXFIFOCONFIG    0x3f
+#define V_TXFIFOCONFIG(x) ((x) << S_TXFIFOCONFIG)
+#define G_TXFIFOCONFIG(x) (((x) >> S_TXFIFOCONFIG) & M_TXFIFOCONFIG)
+
+/* registers for module ULP2_RX */
+#define ULP2_RX_BASE_ADDR 0x500
 
 #define A_ULPRX_CTL 0x500
 
+#define S_PCMD1THRESHOLD    24
+#define M_PCMD1THRESHOLD    0xff
+#define V_PCMD1THRESHOLD(x) ((x) << S_PCMD1THRESHOLD)
+#define G_PCMD1THRESHOLD(x) (((x) >> S_PCMD1THRESHOLD) & M_PCMD1THRESHOLD)
+
+#define S_PCMD0THRESHOLD    16
+#define M_PCMD0THRESHOLD    0xff
+#define V_PCMD0THRESHOLD(x) ((x) << S_PCMD0THRESHOLD)
+#define G_PCMD0THRESHOLD(x) (((x) >> S_PCMD0THRESHOLD) & M_PCMD0THRESHOLD)
+
 #define S_ROUND_ROBIN    4
 #define V_ROUND_ROBIN(x) ((x) << S_ROUND_ROBIN)
 #define F_ROUND_ROBIN    V_ROUND_ROBIN(1U)
 
+#define S_RDMA_PERMISSIVE_MODE    3
+#define V_RDMA_PERMISSIVE_MODE(x) ((x) << S_RDMA_PERMISSIVE_MODE)
+#define F_RDMA_PERMISSIVE_MODE    V_RDMA_PERMISSIVE_MODE(1U)
+
+#define S_PAGEPODME    2
+#define V_PAGEPODME(x) ((x) << S_PAGEPODME)
+#define F_PAGEPODME    V_PAGEPODME(1U)
+
+#define S_ISCSITAGTCB    1
+#define V_ISCSITAGTCB(x) ((x) << S_ISCSITAGTCB)
+#define F_ISCSITAGTCB    V_ISCSITAGTCB(1U)
+
+#define S_TDDPTAGTCB    0
+#define V_TDDPTAGTCB(x) ((x) << S_TDDPTAGTCB)
+#define F_TDDPTAGTCB    V_TDDPTAGTCB(1U)
+
 #define A_ULPRX_INT_ENABLE 0x504
 
 #define S_DATASELFRAMEERR0    7
@@ -1521,51 +5398,85 @@
 #define V_PARERRDATA(x) ((x) << S_PARERRDATA)
 #define F_PARERRDATA    V_PARERRDATA(1U)
 
+#define S_PARERR    0
+#define V_PARERR(x) ((x) << S_PARERR)
+#define F_PARERR    V_PARERR(1U)
+
 #define A_ULPRX_INT_CAUSE 0x508
-
 #define A_ULPRX_ISCSI_LLIMIT 0x50c
 
+#define S_ISCSILLIMIT    6
+#define M_ISCSILLIMIT    0x3ffffff
+#define V_ISCSILLIMIT(x) ((x) << S_ISCSILLIMIT)
+#define G_ISCSILLIMIT(x) (((x) >> S_ISCSILLIMIT) & M_ISCSILLIMIT)
+
 #define A_ULPRX_ISCSI_ULIMIT 0x510
 
+#define S_ISCSIULIMIT    6
+#define M_ISCSIULIMIT    0x3ffffff
+#define V_ISCSIULIMIT(x) ((x) << S_ISCSIULIMIT)
+#define G_ISCSIULIMIT(x) (((x) >> S_ISCSIULIMIT) & M_ISCSIULIMIT)
+
 #define A_ULPRX_ISCSI_TAGMASK 0x514
 
+#define S_ISCSITAGMASK    6
+#define M_ISCSITAGMASK    0x3ffffff
+#define V_ISCSITAGMASK(x) ((x) << S_ISCSITAGMASK)
+#define G_ISCSITAGMASK(x) (((x) >> S_ISCSITAGMASK) & M_ISCSITAGMASK)
+
 #define A_ULPRX_ISCSI_PSZ 0x518
 
-#define A_ULPRX_TDDP_LLIMIT 0x51c
-
-#define A_ULPRX_TDDP_ULIMIT 0x520
-#define A_ULPRX_TDDP_PSZ 0x528
+#define S_HPZ3    24
+#define M_HPZ3    0xf
+#define V_HPZ3(x) ((x) << S_HPZ3)
+#define G_HPZ3(x) (((x) >> S_HPZ3) & M_HPZ3)
+
+#define S_HPZ2    16
+#define M_HPZ2    0xf
+#define V_HPZ2(x) ((x) << S_HPZ2)
+#define G_HPZ2(x) (((x) >> S_HPZ2) & M_HPZ2)
+
+#define S_HPZ1    8
+#define M_HPZ1    0xf
+#define V_HPZ1(x) ((x) << S_HPZ1)
+#define G_HPZ1(x) (((x) >> S_HPZ1) & M_HPZ1)
 
 #define S_HPZ0    0
 #define M_HPZ0    0xf
 #define V_HPZ0(x) ((x) << S_HPZ0)
 #define G_HPZ0(x) (((x) >> S_HPZ0) & M_HPZ0)
 
+#define A_ULPRX_TDDP_LLIMIT 0x51c
+
+#define S_TDDPLLIMIT    6
+#define M_TDDPLLIMIT    0x3ffffff
+#define V_TDDPLLIMIT(x) ((x) << S_TDDPLLIMIT)
+#define G_TDDPLLIMIT(x) (((x) >> S_TDDPLLIMIT) & M_TDDPLLIMIT)
+
+#define A_ULPRX_TDDP_ULIMIT 0x520
+
+#define S_TDDPULIMIT    6
+#define M_TDDPULIMIT    0x3ffffff
+#define V_TDDPULIMIT(x) ((x) << S_TDDPULIMIT)
+#define G_TDDPULIMIT(x) (((x) >> S_TDDPULIMIT) & M_TDDPULIMIT)
+
+#define A_ULPRX_TDDP_TAGMASK 0x524
+
+#define S_TDDPTAGMASK    6
+#define M_TDDPTAGMASK    0x3ffffff
+#define V_TDDPTAGMASK(x) ((x) << S_TDDPTAGMASK)
+#define G_TDDPTAGMASK(x) (((x) >> S_TDDPTAGMASK) & M_TDDPTAGMASK)
+
+#define A_ULPRX_TDDP_PSZ 0x528
 #define A_ULPRX_STAG_LLIMIT 0x52c
-
 #define A_ULPRX_STAG_ULIMIT 0x530
-
 #define A_ULPRX_RQ_LLIMIT 0x534
-#define A_ULPRX_RQ_LLIMIT 0x534
-
 #define A_ULPRX_RQ_ULIMIT 0x538
-#define A_ULPRX_RQ_ULIMIT 0x538
-
 #define A_ULPRX_PBL_LLIMIT 0x53c
-
 #define A_ULPRX_PBL_ULIMIT 0x540
-#define A_ULPRX_PBL_ULIMIT 0x540
-
-#define A_ULPRX_TDDP_TAGMASK 0x524
-
-#define A_ULPRX_RQ_LLIMIT 0x534
-#define A_ULPRX_RQ_LLIMIT 0x534
-
-#define A_ULPRX_RQ_ULIMIT 0x538
-#define A_ULPRX_RQ_ULIMIT 0x538
-
-#define A_ULPRX_PBL_ULIMIT 0x540
-#define A_ULPRX_PBL_ULIMIT 0x540
+
+/* registers for module ULP2_TX */
+#define ULP2_TX_BASE_ADDR 0x580
 
 #define A_ULPTX_CONFIG 0x580
 
@@ -1579,6 +5490,30 @@
 
 #define A_ULPTX_INT_ENABLE 0x584
 
+#define S_CMD_FIFO_PERR_SET1    7
+#define V_CMD_FIFO_PERR_SET1(x) ((x) << S_CMD_FIFO_PERR_SET1)
+#define F_CMD_FIFO_PERR_SET1    V_CMD_FIFO_PERR_SET1(1U)
+
+#define S_CMD_FIFO_PERR_SET0    6
+#define V_CMD_FIFO_PERR_SET0(x) ((x) << S_CMD_FIFO_PERR_SET0)
+#define F_CMD_FIFO_PERR_SET0    V_CMD_FIFO_PERR_SET0(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET1    5
+#define V_LSO_HDR_SRAM_PERR_SET1(x) ((x) << S_LSO_HDR_SRAM_PERR_SET1)
+#define F_LSO_HDR_SRAM_PERR_SET1    V_LSO_HDR_SRAM_PERR_SET1(1U)
+
+#define S_LSO_HDR_SRAM_PERR_SET0    4
+#define V_LSO_HDR_SRAM_PERR_SET0(x) ((x) << S_LSO_HDR_SRAM_PERR_SET0)
+#define F_LSO_HDR_SRAM_PERR_SET0    V_LSO_HDR_SRAM_PERR_SET0(1U)
+
+#define S_IMM_DATA_PERR_SET_CH1    3
+#define V_IMM_DATA_PERR_SET_CH1(x) ((x) << S_IMM_DATA_PERR_SET_CH1)
+#define F_IMM_DATA_PERR_SET_CH1    V_IMM_DATA_PERR_SET_CH1(1U)
+
+#define S_IMM_DATA_PERR_SET_CH0    2
+#define V_IMM_DATA_PERR_SET_CH0(x) ((x) << S_IMM_DATA_PERR_SET_CH0)
+#define F_IMM_DATA_PERR_SET_CH0    V_IMM_DATA_PERR_SET_CH0(1U)
+
 #define S_PBL_BOUND_ERR_CH1    1
 #define V_PBL_BOUND_ERR_CH1(x) ((x) << S_PBL_BOUND_ERR_CH1)
 #define F_PBL_BOUND_ERR_CH1    V_PBL_BOUND_ERR_CH1(1U)
@@ -1588,28 +5523,70 @@
 #define F_PBL_BOUND_ERR_CH0    V_PBL_BOUND_ERR_CH0(1U)
 
 #define A_ULPTX_INT_CAUSE 0x588
-
 #define A_ULPTX_TPT_LLIMIT 0x58c
-
 #define A_ULPTX_TPT_ULIMIT 0x590
-
 #define A_ULPTX_PBL_LLIMIT 0x594
-
 #define A_ULPTX_PBL_ULIMIT 0x598
+#define A_ULPTX_CPL_ERR_OFFSET 0x59c
+#define A_ULPTX_CPL_ERR_MASK 0x5a0
+#define A_ULPTX_CPL_ERR_VALUE 0x5a4
+#define A_ULPTX_CPL_PACK_SIZE 0x5a8
+
+#define S_VALUE    24
+#define M_VALUE    0xff
+#define V_VALUE(x) ((x) << S_VALUE)
+#define G_VALUE(x) (((x) >> S_VALUE) & M_VALUE)
+
+#define S_CH1SIZE2    24
+#define M_CH1SIZE2    0xff
+#define V_CH1SIZE2(x) ((x) << S_CH1SIZE2)
+#define G_CH1SIZE2(x) (((x) >> S_CH1SIZE2) & M_CH1SIZE2)
+
+#define S_CH1SIZE1    16
+#define M_CH1SIZE1    0xff
+#define V_CH1SIZE1(x) ((x) << S_CH1SIZE1)
+#define G_CH1SIZE1(x) (((x) >> S_CH1SIZE1) & M_CH1SIZE1)
+
+#define S_CH0SIZE2    8
+#define M_CH0SIZE2    0xff
+#define V_CH0SIZE2(x) ((x) << S_CH0SIZE2)
+#define G_CH0SIZE2(x) (((x) >> S_CH0SIZE2) & M_CH0SIZE2)
+
+#define S_CH0SIZE1    0
+#define M_CH0SIZE1    0xff
+#define V_CH0SIZE1(x) ((x) << S_CH0SIZE1)
+#define G_CH0SIZE1(x) (((x) >> S_CH0SIZE1) & M_CH0SIZE1)
 
 #define A_ULPTX_DMA_WEIGHT 0x5ac
 
 #define S_D1_WEIGHT    16
 #define M_D1_WEIGHT    0xffff
 #define V_D1_WEIGHT(x) ((x) << S_D1_WEIGHT)
+#define G_D1_WEIGHT(x) (((x) >> S_D1_WEIGHT) & M_D1_WEIGHT)
 
 #define S_D0_WEIGHT    0
 #define M_D0_WEIGHT    0xffff
 #define V_D0_WEIGHT(x) ((x) << S_D0_WEIGHT)
+#define G_D0_WEIGHT(x) (((x) >> S_D0_WEIGHT) & M_D0_WEIGHT)
+
+/* registers for module PM1_RX */
+#define PM1_RX_BASE_ADDR 0x5c0
 
 #define A_PM1_RX_CFG 0x5c0
 #define A_PM1_RX_MODE 0x5c4
 
+#define S_STAT_CHANNEL    1
+#define V_STAT_CHANNEL(x) ((x) << S_STAT_CHANNEL)
+#define F_STAT_CHANNEL    V_STAT_CHANNEL(1U)
+
+#define S_PRIORITY_CH    0
+#define V_PRIORITY_CH(x) ((x) << S_PRIORITY_CH)
+#define F_PRIORITY_CH    V_PRIORITY_CH(1U)
+
+#define A_PM1_RX_STAT_CONFIG 0x5c8
+#define A_PM1_RX_STAT_COUNT 0x5cc
+#define A_PM1_RX_STAT_MSB 0x5d0
+#define A_PM1_RX_STAT_LSB 0x5d4
 #define A_PM1_RX_INT_ENABLE 0x5d8
 
 #define S_ZERO_E_CMD_ERROR    18
@@ -1666,19 +5643,25 @@
 
 #define S_IESPI_PAR_ERROR    3
 #define M_IESPI_PAR_ERROR    0x7
-
 #define V_IESPI_PAR_ERROR(x) ((x) << S_IESPI_PAR_ERROR)
+#define G_IESPI_PAR_ERROR(x) (((x) >> S_IESPI_PAR_ERROR) & M_IESPI_PAR_ERROR)
 
 #define S_OCSPI_PAR_ERROR    0
 #define M_OCSPI_PAR_ERROR    0x7
-
 #define V_OCSPI_PAR_ERROR(x) ((x) << S_OCSPI_PAR_ERROR)
+#define G_OCSPI_PAR_ERROR(x) (((x) >> S_OCSPI_PAR_ERROR) & M_OCSPI_PAR_ERROR)
 
 #define A_PM1_RX_INT_CAUSE 0x5dc
 
+/* registers for module PM1_TX */
+#define PM1_TX_BASE_ADDR 0x5e0
+
 #define A_PM1_TX_CFG 0x5e0
 #define A_PM1_TX_MODE 0x5e4
-
+#define A_PM1_TX_STAT_CONFIG 0x5e8
+#define A_PM1_TX_STAT_COUNT 0x5ec
+#define A_PM1_TX_STAT_MSB 0x5f0
+#define A_PM1_TX_STAT_LSB 0x5f4
 #define A_PM1_TX_INT_ENABLE 0x5f8
 
 #define S_ZERO_C_CMD_ERROR    18
@@ -1735,18 +5718,42 @@
 
 #define S_ICSPI_PAR_ERROR    3
 #define M_ICSPI_PAR_ERROR    0x7
-
 #define V_ICSPI_PAR_ERROR(x) ((x) << S_ICSPI_PAR_ERROR)
+#define G_ICSPI_PAR_ERROR(x) (((x) >> S_ICSPI_PAR_ERROR) & M_ICSPI_PAR_ERROR)
 
 #define S_OESPI_PAR_ERROR    0
 #define M_OESPI_PAR_ERROR    0x7
-
 #define V_OESPI_PAR_ERROR(x) ((x) << S_OESPI_PAR_ERROR)
+#define G_OESPI_PAR_ERROR(x) (((x) >> S_OESPI_PAR_ERROR) & M_OESPI_PAR_ERROR)
 
 #define A_PM1_TX_INT_CAUSE 0x5fc
 
+/* registers for module MPS0 */
+#define MPS0_BASE_ADDR 0x600
+
 #define A_MPS_CFG 0x600
 
+#define S_ENFORCEPKT    11
+#define V_ENFORCEPKT(x) ((x) << S_ENFORCEPKT)
+#define F_ENFORCEPKT    V_ENFORCEPKT(1U)
+
+#define S_SGETPQID    8
+#define M_SGETPQID    0x7
+#define V_SGETPQID(x) ((x) << S_SGETPQID)
+#define G_SGETPQID(x) (((x) >> S_SGETPQID) & M_SGETPQID)
+
+#define S_TPRXPORTSIZE    7
+#define V_TPRXPORTSIZE(x) ((x) << S_TPRXPORTSIZE)
+#define F_TPRXPORTSIZE    V_TPRXPORTSIZE(1U)
+
+#define S_TPTXPORT1SIZE    6
+#define V_TPTXPORT1SIZE(x) ((x) << S_TPTXPORT1SIZE)
+#define F_TPTXPORT1SIZE    V_TPTXPORT1SIZE(1U)
+
+#define S_TPTXPORT0SIZE    5
+#define V_TPTXPORT0SIZE(x) ((x) << S_TPTXPORT0SIZE)
+#define F_TPTXPORT0SIZE    V_TPTXPORT0SIZE(1U)
+
 #define S_TPRXPORTEN    4
 #define V_TPRXPORTEN(x) ((x) << S_TPRXPORTEN)
 #define F_TPRXPORTEN    V_TPRXPORTEN(1U)
@@ -1767,56 +5774,162 @@
 #define V_PORT0ACTIVE(x) ((x) << S_PORT0ACTIVE)
 #define F_PORT0ACTIVE    V_PORT0ACTIVE(1U)
 
-#define S_ENFORCEPKT    11
-#define V_ENFORCEPKT(x) ((x) << S_ENFORCEPKT)
-#define F_ENFORCEPKT    V_ENFORCEPKT(1U)
+#define A_MPS_DRR_CFG1 0x604
+
+#define S_RLDWTTPD1    11
+#define M_RLDWTTPD1    0x7ff
+#define V_RLDWTTPD1(x) ((x) << S_RLDWTTPD1)
+#define G_RLDWTTPD1(x) (((x) >> S_RLDWTTPD1) & M_RLDWTTPD1)
+
+#define S_RLDWTTPD0    0
+#define M_RLDWTTPD0    0x7ff
+#define V_RLDWTTPD0(x) ((x) << S_RLDWTTPD0)
+#define G_RLDWTTPD0(x) (((x) >> S_RLDWTTPD0) & M_RLDWTTPD0)
+
+#define A_MPS_DRR_CFG2 0x608
+
+#define S_RLDWTTOTAL    0
+#define M_RLDWTTOTAL    0xfff
+#define V_RLDWTTOTAL(x) ((x) << S_RLDWTTOTAL)
+#define G_RLDWTTOTAL(x) (((x) >> S_RLDWTTOTAL) & M_RLDWTTOTAL)
+
+#define A_MPS_MCA_STATUS 0x60c
+
+#define S_MCAPKTCNT    12
+#define M_MCAPKTCNT    0xfffff
+#define V_MCAPKTCNT(x) ((x) << S_MCAPKTCNT)
+#define G_MCAPKTCNT(x) (((x) >> S_MCAPKTCNT) & M_MCAPKTCNT)
+
+#define S_MCADEPTH    0
+#define M_MCADEPTH    0xfff
+#define V_MCADEPTH(x) ((x) << S_MCADEPTH)
+#define G_MCADEPTH(x) (((x) >> S_MCADEPTH) & M_MCADEPTH)
+
+#define A_MPS_TX0_TP_CNT 0x610
+
+#define S_TX0TPDISCNT    24
+#define M_TX0TPDISCNT    0xff
+#define V_TX0TPDISCNT(x) ((x) << S_TX0TPDISCNT)
+#define G_TX0TPDISCNT(x) (((x) >> S_TX0TPDISCNT) & M_TX0TPDISCNT)
+
+#define S_TX0TPCNT    0
+#define M_TX0TPCNT    0xffffff
+#define V_TX0TPCNT(x) ((x) << S_TX0TPCNT)
+#define G_TX0TPCNT(x) (((x) >> S_TX0TPCNT) & M_TX0TPCNT)
+
+#define A_MPS_TX1_TP_CNT 0x614
+
+#define S_TX1TPDISCNT    24
+#define M_TX1TPDISCNT    0xff
+#define V_TX1TPDISCNT(x) ((x) << S_TX1TPDISCNT)
+#define G_TX1TPDISCNT(x) (((x) >> S_TX1TPDISCNT) & M_TX1TPDISCNT)
+
+#define S_TX1TPCNT    0
+#define M_TX1TPCNT    0xffffff
+#define V_TX1TPCNT(x) ((x) << S_TX1TPCNT)
+#define G_TX1TPCNT(x) (((x) >> S_TX1TPCNT) & M_TX1TPCNT)
+
+#define A_MPS_RX_TP_CNT 0x618
+
+#define S_RXTPDISCNT    24
+#define M_RXTPDISCNT    0xff
+#define V_RXTPDISCNT(x) ((x) << S_RXTPDISCNT)
+#define G_RXTPDISCNT(x) (((x) >> S_RXTPDISCNT) & M_RXTPDISCNT)
+
+#define S_RXTPCNT    0
+#define M_RXTPCNT    0xffffff
+#define V_RXTPCNT(x) ((x) << S_RXTPCNT)
+#define G_RXTPCNT(x) (((x) >> S_RXTPCNT) & M_RXTPCNT)
 
 #define A_MPS_INT_ENABLE 0x61c
 
 #define S_MCAPARERRENB    6
 #define M_MCAPARERRENB    0x7
-
 #define V_MCAPARERRENB(x) ((x) << S_MCAPARERRENB)
+#define G_MCAPARERRENB(x) (((x) >> S_MCAPARERRENB) & M_MCAPARERRENB)
 
 #define S_RXTPPARERRENB    4
 #define M_RXTPPARERRENB    0x3
-
 #define V_RXTPPARERRENB(x) ((x) << S_RXTPPARERRENB)
+#define G_RXTPPARERRENB(x) (((x) >> S_RXTPPARERRENB) & M_RXTPPARERRENB)
 
 #define S_TX1TPPARERRENB    2
 #define M_TX1TPPARERRENB    0x3
-
 #define V_TX1TPPARERRENB(x) ((x) << S_TX1TPPARERRENB)
+#define G_TX1TPPARERRENB(x) (((x) >> S_TX1TPPARERRENB) & M_TX1TPPARERRENB)
 
 #define S_TX0TPPARERRENB    0
 #define M_TX0TPPARERRENB    0x3
-
 #define V_TX0TPPARERRENB(x) ((x) << S_TX0TPPARERRENB)
+#define G_TX0TPPARERRENB(x) (((x) >> S_TX0TPPARERRENB) & M_TX0TPPARERRENB)
 
 #define A_MPS_INT_CAUSE 0x620
 
 #define S_MCAPARERR    6
 #define M_MCAPARERR    0x7
-
 #define V_MCAPARERR(x) ((x) << S_MCAPARERR)
+#define G_MCAPARERR(x) (((x) >> S_MCAPARERR) & M_MCAPARERR)
 
 #define S_RXTPPARERR    4
 #define M_RXTPPARERR    0x3
-
 #define V_RXTPPARERR(x) ((x) << S_RXTPPARERR)
+#define G_RXTPPARERR(x) (((x) >> S_RXTPPARERR) & M_RXTPPARERR)
 
 #define S_TX1TPPARERR    2
 #define M_TX1TPPARERR    0x3
-
 #define V_TX1TPPARERR(x) ((x) << S_TX1TPPARERR)
+#define G_TX1TPPARERR(x) (((x) >> S_TX1TPPARERR) & M_TX1TPPARERR)
 
 #define S_TX0TPPARERR    0
 #define M_TX0TPPARERR    0x3
-
 #define V_TX0TPPARERR(x) ((x) << S_TX0TPPARERR)
+#define G_TX0TPPARERR(x) (((x) >> S_TX0TPPARERR) & M_TX0TPPARERR)
+
+/* registers for module CPL_SWITCH */
+#define CPL_SWITCH_BASE_ADDR 0x640
 
 #define A_CPL_SWITCH_CNTRL 0x640
 
+#define S_CPL_PKT_TID    8
+#define M_CPL_PKT_TID    0xffffff
+#define V_CPL_PKT_TID(x) ((x) << S_CPL_PKT_TID)
+#define G_CPL_PKT_TID(x) (((x) >> S_CPL_PKT_TID) & M_CPL_PKT_TID)
+
+#define S_CIM_TO_UP_FULL_SIZE    4
+#define V_CIM_TO_UP_FULL_SIZE(x) ((x) << S_CIM_TO_UP_FULL_SIZE)
+#define F_CIM_TO_UP_FULL_SIZE    V_CIM_TO_UP_FULL_SIZE(1U)
+
+#define S_CPU_NO_3F_CIM_ENABLE    3
+#define V_CPU_NO_3F_CIM_ENABLE(x) ((x) << S_CPU_NO_3F_CIM_ENABLE)
+#define F_CPU_NO_3F_CIM_ENABLE    V_CPU_NO_3F_CIM_ENABLE(1U)
+
+#define S_SWITCH_TABLE_ENABLE    2
+#define V_SWITCH_TABLE_ENABLE(x) ((x) << S_SWITCH_TABLE_ENABLE)
+#define F_SWITCH_TABLE_ENABLE    V_SWITCH_TABLE_ENABLE(1U)
+
+#define S_SGE_ENABLE    1
+#define V_SGE_ENABLE(x) ((x) << S_SGE_ENABLE)
+#define F_SGE_ENABLE    V_SGE_ENABLE(1U)
+
+#define S_CIM_ENABLE    0
+#define V_CIM_ENABLE(x) ((x) << S_CIM_ENABLE)
+#define F_CIM_ENABLE    V_CIM_ENABLE(1U)
+
+#define A_CPL_SWITCH_TBL_IDX 0x644
+
+#define S_SWITCH_TBL_IDX    0
+#define M_SWITCH_TBL_IDX    0xf
+#define V_SWITCH_TBL_IDX(x) ((x) << S_SWITCH_TBL_IDX)
+#define G_SWITCH_TBL_IDX(x) (((x) >> S_SWITCH_TBL_IDX) & M_SWITCH_TBL_IDX)
+
+#define A_CPL_SWITCH_TBL_DATA 0x648
+#define A_CPL_SWITCH_ZERO_ERROR 0x64c
+
+#define S_ZERO_CMD    0
+#define M_ZERO_CMD    0xff
+#define V_ZERO_CMD(x) ((x) << S_ZERO_CMD)
+#define G_ZERO_CMD(x) (((x) >> S_ZERO_CMD) & M_ZERO_CMD)
+
 #define A_CPL_INTR_ENABLE 0x650
 
 #define S_CIM_OP_MAP_PERR    5
@@ -1844,29 +5957,289 @@
 #define F_ZERO_SWITCH_ERROR    V_ZERO_SWITCH_ERROR(1U)
 
 #define A_CPL_INTR_CAUSE 0x654
+#define A_CPL_MAP_TBL_IDX 0x658
+
+#define S_CPL_MAP_TBL_IDX    0
+#define M_CPL_MAP_TBL_IDX    0xff
+#define V_CPL_MAP_TBL_IDX(x) ((x) << S_CPL_MAP_TBL_IDX)
+#define G_CPL_MAP_TBL_IDX(x) (((x) >> S_CPL_MAP_TBL_IDX) & M_CPL_MAP_TBL_IDX)
 
 #define A_CPL_MAP_TBL_DATA 0x65c
 
+#define S_CPL_MAP_TBL_DATA    0
+#define M_CPL_MAP_TBL_DATA    0xff
+#define V_CPL_MAP_TBL_DATA(x) ((x) << S_CPL_MAP_TBL_DATA)
+#define G_CPL_MAP_TBL_DATA(x) (((x) >> S_CPL_MAP_TBL_DATA) & M_CPL_MAP_TBL_DATA)
+
+/* registers for module SMB0 */
+#define SMB0_BASE_ADDR 0x660
+
 #define A_SMB_GLOBAL_TIME_CFG 0x660
 
+#define S_LADBGWRPTR    24
+#define M_LADBGWRPTR    0xff
+#define V_LADBGWRPTR(x) ((x) << S_LADBGWRPTR)
+#define G_LADBGWRPTR(x) (((x) >> S_LADBGWRPTR) & M_LADBGWRPTR)
+
+#define S_LADBGRDPTR    16
+#define M_LADBGRDPTR    0xff
+#define V_LADBGRDPTR(x) ((x) << S_LADBGRDPTR)
+#define G_LADBGRDPTR(x) (((x) >> S_LADBGRDPTR) & M_LADBGRDPTR)
+
+#define S_LADBGEN    13
+#define V_LADBGEN(x) ((x) << S_LADBGEN)
+#define F_LADBGEN    V_LADBGEN(1U)
+
+#define S_MACROCNTCFG    8
+#define M_MACROCNTCFG    0x1f
+#define V_MACROCNTCFG(x) ((x) << S_MACROCNTCFG)
+#define G_MACROCNTCFG(x) (((x) >> S_MACROCNTCFG) & M_MACROCNTCFG)
+
+#define S_MICROCNTCFG    0
+#define M_MICROCNTCFG    0xff
+#define V_MICROCNTCFG(x) ((x) << S_MICROCNTCFG)
+#define G_MICROCNTCFG(x) (((x) >> S_MICROCNTCFG) & M_MICROCNTCFG)
+
+#define A_SMB_MST_TIMEOUT_CFG 0x664
+
+#define S_DEBUGSELH    28
+#define M_DEBUGSELH    0xf
+#define V_DEBUGSELH(x) ((x) << S_DEBUGSELH)
+#define G_DEBUGSELH(x) (((x) >> S_DEBUGSELH) & M_DEBUGSELH)
+
+#define S_DEBUGSELL    24
+#define M_DEBUGSELL    0xf
+#define V_DEBUGSELL(x) ((x) << S_DEBUGSELL)
+#define G_DEBUGSELL(x) (((x) >> S_DEBUGSELL) & M_DEBUGSELL)
+
+#define S_MSTTIMEOUTCFG    0
+#define M_MSTTIMEOUTCFG    0xffffff
+#define V_MSTTIMEOUTCFG(x) ((x) << S_MSTTIMEOUTCFG)
+#define G_MSTTIMEOUTCFG(x) (((x) >> S_MSTTIMEOUTCFG) & M_MSTTIMEOUTCFG)
+
+#define A_SMB_MST_CTL_CFG 0x668
+
+#define S_MSTFIFODBG    31
+#define V_MSTFIFODBG(x) ((x) << S_MSTFIFODBG)
+#define F_MSTFIFODBG    V_MSTFIFODBG(1U)
+
+#define S_MSTFIFODBGCLR    30
+#define V_MSTFIFODBGCLR(x) ((x) << S_MSTFIFODBGCLR)
+#define F_MSTFIFODBGCLR    V_MSTFIFODBGCLR(1U)
+
+#define S_MSTRXBYTECFG    12
+#define M_MSTRXBYTECFG    0x3f
+#define V_MSTRXBYTECFG(x) ((x) << S_MSTRXBYTECFG)
+#define G_MSTRXBYTECFG(x) (((x) >> S_MSTRXBYTECFG) & M_MSTRXBYTECFG)
+
+#define S_MSTTXBYTECFG    6
+#define M_MSTTXBYTECFG    0x3f
+#define V_MSTTXBYTECFG(x) ((x) << S_MSTTXBYTECFG)
+#define G_MSTTXBYTECFG(x) (((x) >> S_MSTTXBYTECFG) & M_MSTTXBYTECFG)
+
+#define S_MSTRESET    1
+#define V_MSTRESET(x) ((x) << S_MSTRESET)
+#define F_MSTRESET    V_MSTRESET(1U)
+
+#define S_MSTCTLEN    0
+#define V_MSTCTLEN(x) ((x) << S_MSTCTLEN)
+#define F_MSTCTLEN    V_MSTCTLEN(1U)
+
+#define A_SMB_MST_CTL_STS 0x66c
+
+#define S_MSTRXBYTECNT    12
+#define M_MSTRXBYTECNT    0x3f
+#define V_MSTRXBYTECNT(x) ((x) << S_MSTRXBYTECNT)
+#define G_MSTRXBYTECNT(x) (((x) >> S_MSTRXBYTECNT) & M_MSTRXBYTECNT)
+
+#define S_MSTTXBYTECNT    6
+#define M_MSTTXBYTECNT    0x3f
+#define V_MSTTXBYTECNT(x) ((x) << S_MSTTXBYTECNT)
+#define G_MSTTXBYTECNT(x) (((x) >> S_MSTTXBYTECNT) & M_MSTTXBYTECNT)
+
+#define S_MSTBUSYSTS    0
+#define V_MSTBUSYSTS(x) ((x) << S_MSTBUSYSTS)
+#define F_MSTBUSYSTS    V_MSTBUSYSTS(1U)
+
+#define A_SMB_MST_TX_FIFO_RDWR 0x670
+#define A_SMB_MST_RX_FIFO_RDWR 0x674
+#define A_SMB_SLV_TIMEOUT_CFG 0x678
+
+#define S_SLVTIMEOUTCFG    0
+#define M_SLVTIMEOUTCFG    0xffffff
+#define V_SLVTIMEOUTCFG(x) ((x) << S_SLVTIMEOUTCFG)
+#define G_SLVTIMEOUTCFG(x) (((x) >> S_SLVTIMEOUTCFG) & M_SLVTIMEOUTCFG)
+
+#define A_SMB_SLV_CTL_CFG 0x67c
+
+#define S_SLVFIFODBG    31
+#define V_SLVFIFODBG(x) ((x) << S_SLVFIFODBG)
+#define F_SLVFIFODBG    V_SLVFIFODBG(1U)
+
+#define S_SLVFIFODBGCLR    30
+#define V_SLVFIFODBGCLR(x) ((x) << S_SLVFIFODBGCLR)
+#define F_SLVFIFODBGCLR    V_SLVFIFODBGCLR(1U)
+
+#define S_SLVADDRCFG    4
+#define M_SLVADDRCFG    0x7f
+#define V_SLVADDRCFG(x) ((x) << S_SLVADDRCFG)
+#define G_SLVADDRCFG(x) (((x) >> S_SLVADDRCFG) & M_SLVADDRCFG)
+
+#define S_SLVALRTSET    2
+#define V_SLVALRTSET(x) ((x) << S_SLVALRTSET)
+#define F_SLVALRTSET    V_SLVALRTSET(1U)
+
+#define S_SLVRESET    1
+#define V_SLVRESET(x) ((x) << S_SLVRESET)
+#define F_SLVRESET    V_SLVRESET(1U)
+
+#define S_SLVCTLEN    0
+#define V_SLVCTLEN(x) ((x) << S_SLVCTLEN)
+#define F_SLVCTLEN    V_SLVCTLEN(1U)
+
+#define A_SMB_SLV_CTL_STS 0x680
+
+#define S_SLVFIFOTXCNT    12
+#define M_SLVFIFOTXCNT    0x3f
+#define V_SLVFIFOTXCNT(x) ((x) << S_SLVFIFOTXCNT)
+#define G_SLVFIFOTXCNT(x) (((x) >> S_SLVFIFOTXCNT) & M_SLVFIFOTXCNT)
+
+#define S_SLVFIFOCNT    6
+#define M_SLVFIFOCNT    0x3f
+#define V_SLVFIFOCNT(x) ((x) << S_SLVFIFOCNT)
+#define G_SLVFIFOCNT(x) (((x) >> S_SLVFIFOCNT) & M_SLVFIFOCNT)
+
+#define S_SLVALRTSTS    2
+#define V_SLVALRTSTS(x) ((x) << S_SLVALRTSTS)
+#define F_SLVALRTSTS    V_SLVALRTSTS(1U)
+
+#define S_SLVBUSYSTS    0
+#define V_SLVBUSYSTS(x) ((x) << S_SLVBUSYSTS)
+#define F_SLVBUSYSTS    V_SLVBUSYSTS(1U)
+
+#define A_SMB_SLV_FIFO_RDWR 0x684
+#define A_SMB_SLV_CMD_FIFO_RDWR 0x688
+#define A_SMB_INT_ENABLE 0x68c
+
+#define S_SLVTIMEOUTINTEN    7
+#define V_SLVTIMEOUTINTEN(x) ((x) << S_SLVTIMEOUTINTEN)
+#define F_SLVTIMEOUTINTEN    V_SLVTIMEOUTINTEN(1U)
+
+#define S_SLVERRINTEN    6
+#define V_SLVERRINTEN(x) ((x) << S_SLVERRINTEN)
+#define F_SLVERRINTEN    V_SLVERRINTEN(1U)
+
+#define S_SLVDONEINTEN    5
+#define V_SLVDONEINTEN(x) ((x) << S_SLVDONEINTEN)
+#define F_SLVDONEINTEN    V_SLVDONEINTEN(1U)
+
+#define S_SLVRXRDYINTEN    4
+#define V_SLVRXRDYINTEN(x) ((x) << S_SLVRXRDYINTEN)
+#define F_SLVRXRDYINTEN    V_SLVRXRDYINTEN(1U)
+
+#define S_MSTTIMEOUTINTEN    3
+#define V_MSTTIMEOUTINTEN(x) ((x) << S_MSTTIMEOUTINTEN)
+#define F_MSTTIMEOUTINTEN    V_MSTTIMEOUTINTEN(1U)
+
+#define S_MSTNACKINTEN    2
+#define V_MSTNACKINTEN(x) ((x) << S_MSTNACKINTEN)
+#define F_MSTNACKINTEN    V_MSTNACKINTEN(1U)
+
+#define S_MSTLOSTARBINTEN    1
+#define V_MSTLOSTARBINTEN(x) ((x) << S_MSTLOSTARBINTEN)
+#define F_MSTLOSTARBINTEN    V_MSTLOSTARBINTEN(1U)
+
+#define S_MSTDONEINTEN    0
+#define V_MSTDONEINTEN(x) ((x) << S_MSTDONEINTEN)
+#define F_MSTDONEINTEN    V_MSTDONEINTEN(1U)
+
+#define A_SMB_INT_CAUSE 0x690
+
+#define S_SLVTIMEOUTINT    7
+#define V_SLVTIMEOUTINT(x) ((x) << S_SLVTIMEOUTINT)
+#define F_SLVTIMEOUTINT    V_SLVTIMEOUTINT(1U)
+
+#define S_SLVERRINT    6
+#define V_SLVERRINT(x) ((x) << S_SLVERRINT)
+#define F_SLVERRINT    V_SLVERRINT(1U)
+
+#define S_SLVDONEINT    5
+#define V_SLVDONEINT(x) ((x) << S_SLVDONEINT)
+#define F_SLVDONEINT    V_SLVDONEINT(1U)
+
+#define S_SLVRXRDYINT    4
+#define V_SLVRXRDYINT(x) ((x) << S_SLVRXRDYINT)
+#define F_SLVRXRDYINT    V_SLVRXRDYINT(1U)
+
+#define S_MSTTIMEOUTINT    3
+#define V_MSTTIMEOUTINT(x) ((x) << S_MSTTIMEOUTINT)
+#define F_MSTTIMEOUTINT    V_MSTTIMEOUTINT(1U)
+
+#define S_MSTNACKINT    2
+#define V_MSTNACKINT(x) ((x) << S_MSTNACKINT)
+#define F_MSTNACKINT    V_MSTNACKINT(1U)
+
+#define S_MSTLOSTARBINT    1
+#define V_MSTLOSTARBINT(x) ((x) << S_MSTLOSTARBINT)
+#define F_MSTLOSTARBINT    V_MSTLOSTARBINT(1U)
+
+#define S_MSTDONEINT    0
+#define V_MSTDONEINT(x) ((x) << S_MSTDONEINT)
+#define F_MSTDONEINT    V_MSTDONEINT(1U)
+
+#define A_SMB_DEBUG_DATA 0x694
+
+#define S_DEBUGDATAH    16
+#define M_DEBUGDATAH    0xffff
+#define V_DEBUGDATAH(x) ((x) << S_DEBUGDATAH)
+#define G_DEBUGDATAH(x) (((x) >> S_DEBUGDATAH) & M_DEBUGDATAH)
+
+#define S_DEBUGDATAL    0
+#define M_DEBUGDATAL    0xffff
+#define V_DEBUGDATAL(x) ((x) << S_DEBUGDATAL)
+#define G_DEBUGDATAL(x) (((x) >> S_DEBUGDATAL) & M_DEBUGDATAL)
+
+#define A_SMB_DEBUG_LA 0x69c
+
+#define S_DEBUGLAREQADDR    0
+#define M_DEBUGLAREQADDR    0x3ff
+#define V_DEBUGLAREQADDR(x) ((x) << S_DEBUGLAREQADDR)
+#define G_DEBUGLAREQADDR(x) (((x) >> S_DEBUGLAREQADDR) & M_DEBUGLAREQADDR)
+
+/* registers for module I2CM0 */
+#define I2CM0_BASE_ADDR 0x6a0
+
 #define A_I2C_CFG 0x6a0
 
 #define S_I2C_CLKDIV    0
 #define M_I2C_CLKDIV    0xfff
 #define V_I2C_CLKDIV(x) ((x) << S_I2C_CLKDIV)
+#define G_I2C_CLKDIV(x) (((x) >> S_I2C_CLKDIV) & M_I2C_CLKDIV)
+
+#define A_I2C_DATA 0x6a4
+#define A_I2C_OP 0x6a8
+
+#define S_ACK    30
+#define V_ACK(x) ((x) << S_ACK)
+#define F_ACK    V_ACK(1U)
+
+#define S_I2C_CONT    1
+#define V_I2C_CONT(x) ((x) << S_I2C_CONT)
+#define F_I2C_CONT    V_I2C_CONT(1U)
+
+/* registers for module MI1 */
+#define MI1_BASE_ADDR 0x6b0
 
 #define A_MI1_CFG 0x6b0
 
 #define S_CLKDIV    5
 #define M_CLKDIV    0xff
 #define V_CLKDIV(x) ((x) << S_CLKDIV)
+#define G_CLKDIV(x) (((x) >> S_CLKDIV) & M_CLKDIV)
 
 #define S_ST    3
-
 #define M_ST    0x3
-
 #define V_ST(x) ((x) << S_ST)
-
 #define G_ST(x) (((x) >> S_ST) & M_ST)
 
 #define S_PREEN    2
@@ -1886,29 +6259,82 @@
 #define S_PHYADDR    5
 #define M_PHYADDR    0x1f
 #define V_PHYADDR(x) ((x) << S_PHYADDR)
+#define G_PHYADDR(x) (((x) >> S_PHYADDR) & M_PHYADDR)
 
 #define S_REGADDR    0
 #define M_REGADDR    0x1f
 #define V_REGADDR(x) ((x) << S_REGADDR)
+#define G_REGADDR(x) (((x) >> S_REGADDR) & M_REGADDR)
 
 #define A_MI1_DATA 0x6b8
 
+#define S_MDI_DATA    0
+#define M_MDI_DATA    0xffff
+#define V_MDI_DATA(x) ((x) << S_MDI_DATA)
+#define G_MDI_DATA(x) (((x) >> S_MDI_DATA) & M_MDI_DATA)
+
 #define A_MI1_OP 0x6bc
 
+#define S_INC    2
+#define V_INC(x) ((x) << S_INC)
+#define F_INC    V_INC(1U)
+
 #define S_MDI_OP    0
 #define M_MDI_OP    0x3
 #define V_MDI_OP(x) ((x) << S_MDI_OP)
+#define G_MDI_OP(x) (((x) >> S_MDI_OP) & M_MDI_OP)
+
+/* registers for module JM1 */
+#define JM1_BASE_ADDR 0x6c0
+
+#define A_JM_CFG 0x6c0
+
+#define S_JM_CLKDIV    2
+#define M_JM_CLKDIV    0xff
+#define V_JM_CLKDIV(x) ((x) << S_JM_CLKDIV)
+#define G_JM_CLKDIV(x) (((x) >> S_JM_CLKDIV) & M_JM_CLKDIV)
+
+#define S_TRST    1
+#define V_TRST(x) ((x) << S_TRST)
+#define F_TRST    V_TRST(1U)
+
+#define S_EN    0
+#define V_EN(x) ((x) << S_EN)
+#define F_EN    V_EN(1U)
+
+#define A_JM_MODE 0x6c4
+#define A_JM_DATA 0x6c8
+#define A_JM_OP 0x6cc
+
+#define S_CNT    0
+#define M_CNT    0x1f
+#define V_CNT(x) ((x) << S_CNT)
+#define G_CNT(x) (((x) >> S_CNT) & M_CNT)
+
+/* registers for module SF1 */
+#define SF1_BASE_ADDR 0x6d8
 
 #define A_SF_DATA 0x6d8
-
 #define A_SF_OP 0x6dc
 
 #define S_BYTECNT    1
 #define M_BYTECNT    0x3
 #define V_BYTECNT(x) ((x) << S_BYTECNT)
+#define G_BYTECNT(x) (((x) >> S_BYTECNT) & M_BYTECNT)
+
+/* registers for module PL3 */
+#define PL3_BASE_ADDR 0x6e0
 
 #define A_PL_INT_ENABLE0 0x6e0
 
+#define S_SW    25
+#define V_SW(x) ((x) << S_SW)
+#define F_SW    V_SW(1U)
+
+#define S_EXT    24
+#define V_EXT(x) ((x) << S_EXT)
+#define F_EXT    V_EXT(1U)
+
 #define S_T3DBG    23
 #define V_T3DBG(x) ((x) << S_T3DBG)
 #define F_T3DBG    V_T3DBG(1U)
@@ -1925,6 +6351,22 @@
 #define V_MC5A(x) ((x) << S_MC5A)
 #define F_MC5A    V_MC5A(1U)
 
+#define S_SF1    17
+#define V_SF1(x) ((x) << S_SF1)
+#define F_SF1    V_SF1(1U)
+
+#define S_SMB0    15
+#define V_SMB0(x) ((x) << S_SMB0)
+#define F_SMB0    V_SMB0(1U)
+
+#define S_I2CM0    14
+#define V_I2CM0(x) ((x) << S_I2CM0)
+#define F_I2CM0    V_I2CM0(1U)
+
+#define S_MI1    13
+#define V_MI1(x) ((x) << S_MI1)
+#define F_MI1    V_MI1(1U)
+
 #define S_CPL_SWITCH    12
 #define V_CPL_SWITCH(x) ((x) << S_CPL_SWITCH)
 #define F_CPL_SWITCH    V_CPL_SWITCH(1U)
@@ -1978,19 +6420,142 @@
 #define F_SGE3    V_SGE3(1U)
 
 #define A_PL_INT_CAUSE0 0x6e4
-
+#define A_PL_INT_ENABLE1 0x6e8
+#define A_PL_INT_CAUSE1 0x6ec
 #define A_PL_RST 0x6f0
 
+#define S_FATALPERREN    4
+#define V_FATALPERREN(x) ((x) << S_FATALPERREN)
+#define F_FATALPERREN    V_FATALPERREN(1U)
+
+#define S_SWINT1    3
+#define V_SWINT1(x) ((x) << S_SWINT1)
+#define F_SWINT1    V_SWINT1(1U)
+
+#define S_SWINT0    2
+#define V_SWINT0(x) ((x) << S_SWINT0)
+#define F_SWINT0    V_SWINT0(1U)
+
 #define S_CRSTWRM    1
 #define V_CRSTWRM(x) ((x) << S_CRSTWRM)
 #define F_CRSTWRM    V_CRSTWRM(1U)
 
 #define A_PL_REV 0x6f4
 
+#define S_REV    0
+#define M_REV    0xf
+#define V_REV(x) ((x) << S_REV)
+#define G_REV(x) (((x) >> S_REV) & M_REV)
+
 #define A_PL_CLI 0x6f8
+#define A_PL_LCK 0x6fc
+
+#define S_LCK    0
+#define M_LCK    0x3
+#define V_LCK(x) ((x) << S_LCK)
+#define G_LCK(x) (((x) >> S_LCK) & M_LCK)
+
+/* registers for module MC5A */
+#define MC5A_BASE_ADDR 0x700
+
+#define A_MC5_BUF_CONFIG 0x700
+
+#define S_TERM300_240    31
+#define V_TERM300_240(x) ((x) << S_TERM300_240)
+#define F_TERM300_240    V_TERM300_240(1U)
+
+#define S_MC5_TERM150    30
+#define V_MC5_TERM150(x) ((x) << S_MC5_TERM150)
+#define F_MC5_TERM150    V_MC5_TERM150(1U)
+
+#define S_TERM60    29
+#define V_TERM60(x) ((x) << S_TERM60)
+#define F_TERM60    V_TERM60(1U)
+
+#define S_GDDRIII    28
+#define V_GDDRIII(x) ((x) << S_GDDRIII)
+#define F_GDDRIII    V_GDDRIII(1U)
+
+#define S_GDDRII    27
+#define V_GDDRII(x) ((x) << S_GDDRII)
+#define F_GDDRII    V_GDDRII(1U)
+
+#define S_GDDRI    26
+#define V_GDDRI(x) ((x) << S_GDDRI)
+#define F_GDDRI    V_GDDRI(1U)
+
+#define S_READ    25
+#define V_READ(x) ((x) << S_READ)
+#define F_READ    V_READ(1U)
+
+#define S_IMP_SET_UPDATE    24
+#define V_IMP_SET_UPDATE(x) ((x) << S_IMP_SET_UPDATE)
+#define F_IMP_SET_UPDATE    V_IMP_SET_UPDATE(1U)
+
+#define S_CAL_UPDATE    23
+#define V_CAL_UPDATE(x) ((x) << S_CAL_UPDATE)
+#define F_CAL_UPDATE    V_CAL_UPDATE(1U)
+
+#define S_CAL_BUSY    22
+#define V_CAL_BUSY(x) ((x) << S_CAL_BUSY)
+#define F_CAL_BUSY    V_CAL_BUSY(1U)
+
+#define S_CAL_ERROR    21
+#define V_CAL_ERROR(x) ((x) << S_CAL_ERROR)
+#define F_CAL_ERROR    V_CAL_ERROR(1U)
+
+#define S_SGL_CAL_EN    20
+#define V_SGL_CAL_EN(x) ((x) << S_SGL_CAL_EN)
+#define F_SGL_CAL_EN    V_SGL_CAL_EN(1U)
+
+#define S_IMP_UPD_MODE    19
+#define V_IMP_UPD_MODE(x) ((x) << S_IMP_UPD_MODE)
+#define F_IMP_UPD_MODE    V_IMP_UPD_MODE(1U)
+
+#define S_IMP_SEL    18
+#define V_IMP_SEL(x) ((x) << S_IMP_SEL)
+#define F_IMP_SEL    V_IMP_SEL(1U)
+
+#define S_MAN_PU    15
+#define M_MAN_PU    0x7
+#define V_MAN_PU(x) ((x) << S_MAN_PU)
+#define G_MAN_PU(x) (((x) >> S_MAN_PU) & M_MAN_PU)
+
+#define S_MAN_PD    12
+#define M_MAN_PD    0x7
+#define V_MAN_PD(x) ((x) << S_MAN_PD)
+#define G_MAN_PD(x) (((x) >> S_MAN_PD) & M_MAN_PD)
+
+#define S_CAL_PU    9
+#define M_CAL_PU    0x7
+#define V_CAL_PU(x) ((x) << S_CAL_PU)
+#define G_CAL_PU(x) (((x) >> S_CAL_PU) & M_CAL_PU)
+
+#define S_CAL_PD    6
+#define M_CAL_PD    0x7
+#define V_CAL_PD(x) ((x) << S_CAL_PD)
+#define G_CAL_PD(x) (((x) >> S_CAL_PD) & M_CAL_PD)
+
+#define S_SET_PU    3
+#define M_SET_PU    0x7
+#define V_SET_PU(x) ((x) << S_SET_PU)
+#define G_SET_PU(x) (((x) >> S_SET_PU) & M_SET_PU)
+
+#define S_SET_PD    0
+#define M_SET_PD    0x7
+#define V_SET_PD(x) ((x) << S_SET_PD)
+#define G_SET_PD(x) (((x) >> S_SET_PD) & M_SET_PD)
+
+#define S_CAL_IMP_UPD    23
+#define V_CAL_IMP_UPD(x) ((x) << S_CAL_IMP_UPD)
+#define F_CAL_IMP_UPD    V_CAL_IMP_UPD(1U)
 
 #define A_MC5_DB_CONFIG 0x704
 
+#define S_TMCFGWRLOCK    31
+#define V_TMCFGWRLOCK(x) ((x) << S_TMCFGWRLOCK)
+#define F_TMCFGWRLOCK    V_TMCFGWRLOCK(1U)
+
 #define S_TMTYPEHI    30
 #define V_TMTYPEHI(x) ((x) << S_TMTYPEHI)
 #define F_TMTYPEHI    V_TMTYPEHI(1U)
@@ -2005,10 +6570,41 @@
 #define V_TMTYPE(x) ((x) << S_TMTYPE)
 #define G_TMTYPE(x) (((x) >> S_TMTYPE) & M_TMTYPE)
 
+#define S_TMPARTCOUNT    24
+#define M_TMPARTCOUNT    0x3
+#define V_TMPARTCOUNT(x) ((x) << S_TMPARTCOUNT)
+#define G_TMPARTCOUNT(x) (((x) >> S_TMPARTCOUNT) & M_TMPARTCOUNT)
+
+#define S_NLIP    18
+#define M_NLIP    0x3f
+#define V_NLIP(x) ((x) << S_NLIP)
+#define G_NLIP(x) (((x) >> S_NLIP) & M_NLIP)
+
 #define S_COMPEN    17
 #define V_COMPEN(x) ((x) << S_COMPEN)
 #define F_COMPEN    V_COMPEN(1U)
 
+#define S_BUILD    16
+#define V_BUILD(x) ((x) << S_BUILD)
+#define F_BUILD    V_BUILD(1U)
+
+#define S_FILTEREN    11
+#define V_FILTEREN(x) ((x) << S_FILTEREN)
+#define F_FILTEREN    V_FILTEREN(1U)
+
+#define S_CLIPUPDATE    10
+#define V_CLIPUPDATE(x) ((x) << S_CLIPUPDATE)
+#define F_CLIPUPDATE    V_CLIPUPDATE(1U)
+
+#define S_TM_IO_PDOWN    9
+#define V_TM_IO_PDOWN(x) ((x) << S_TM_IO_PDOWN)
+#define F_TM_IO_PDOWN    V_TM_IO_PDOWN(1U)
+
+#define S_SYNMODE    7
+#define M_SYNMODE    0x3
+#define V_SYNMODE(x) ((x) << S_SYNMODE)
+#define G_SYNMODE(x) (((x) >> S_SYNMODE) & M_SYNMODE)
+
 #define S_PRTYEN    6
 #define V_PRTYEN(x) ((x) << S_PRTYEN)
 #define F_PRTYEN    V_PRTYEN(1U)
@@ -2021,6 +6617,10 @@
 #define V_DBGIEN(x) ((x) << S_DBGIEN)
 #define F_DBGIEN    V_DBGIEN(1U)
 
+#define S_TCMCFGOVR    3
+#define V_TCMCFGOVR(x) ((x) << S_TCMCFGOVR)
+#define F_TCMCFGOVR    V_TCMCFGOVR(1U)
+
 #define S_TMRDY    2
 #define V_TMRDY(x) ((x) << S_TMRDY)
 #define F_TMRDY    V_TMRDY(1U)
@@ -2033,32 +6633,123 @@
 #define V_TMMODE(x) ((x) << S_TMMODE)
 #define F_TMMODE    V_TMMODE(1U)
 
-#define F_TMMODE    V_TMMODE(1U)
+#define A_MC5_MISC 0x708
+
+#define S_LIP_CMP_UNAVAILABLE    0
+#define M_LIP_CMP_UNAVAILABLE    0xf
+#define V_LIP_CMP_UNAVAILABLE(x) ((x) << S_LIP_CMP_UNAVAILABLE)
+#define G_LIP_CMP_UNAVAILABLE(x) (((x) >> S_LIP_CMP_UNAVAILABLE) & M_LIP_CMP_UNAVAILABLE)
 
 #define A_MC5_DB_ROUTING_TABLE_INDEX 0x70c
 
+#define S_RTINDX    0
+#define M_RTINDX    0x3fffff
+#define V_RTINDX(x) ((x) << S_RTINDX)
+#define G_RTINDX(x) (((x) >> S_RTINDX) & M_RTINDX)
+
 #define A_MC5_DB_FILTER_TABLE 0x710
 
+#define S_SRINDX    0
+#define M_SRINDX    0x3fffff
+#define V_SRINDX(x) ((x) << S_SRINDX)
+#define G_SRINDX(x) (((x) >> S_SRINDX) & M_SRINDX)
+
 #define A_MC5_DB_SERVER_INDEX 0x714
-
+#define A_MC5_DB_LIP_RAM_ADDR 0x718
+
+#define S_RAMWR    8
+#define V_RAMWR(x) ((x) << S_RAMWR)
+#define F_RAMWR    V_RAMWR(1U)
+
+#define S_RAMADDR    0
+#define M_RAMADDR    0x3f
+#define V_RAMADDR(x) ((x) << S_RAMADDR)
+#define G_RAMADDR(x) (((x) >> S_RAMADDR) & M_RAMADDR)
+
+#define A_MC5_DB_LIP_RAM_DATA 0x71c
 #define A_MC5_DB_RSP_LATENCY 0x720
 
 #define S_RDLAT    16
 #define M_RDLAT    0x1f
 #define V_RDLAT(x) ((x) << S_RDLAT)
+#define G_RDLAT(x) (((x) >> S_RDLAT) & M_RDLAT)
 
 #define S_LRNLAT    8
 #define M_LRNLAT    0x1f
 #define V_LRNLAT(x) ((x) << S_LRNLAT)
+#define G_LRNLAT(x) (((x) >> S_LRNLAT) & M_LRNLAT)
 
 #define S_SRCHLAT    0
 #define M_SRCHLAT    0x1f
 #define V_SRCHLAT(x) ((x) << S_SRCHLAT)
+#define G_SRCHLAT(x) (((x) >> S_SRCHLAT) & M_SRCHLAT)
+
+#define A_MC5_DB_PARITY_LATENCY 0x724
+
+#define S_PARLAT    0
+#define M_PARLAT    0xf
+#define V_PARLAT(x) ((x) << S_PARLAT)
+#define G_PARLAT(x) (((x) >> S_PARLAT) & M_PARLAT)
+
+#define A_MC5_DB_WR_LRN_VERIFY 0x728
+
+#define S_VWVEREN    2
+#define V_VWVEREN(x) ((x) << S_VWVEREN)
+#define F_VWVEREN    V_VWVEREN(1U)
+
+#define S_LRNVEREN    1
+#define V_LRNVEREN(x) ((x) << S_LRNVEREN)
+#define F_LRNVEREN    V_LRNVEREN(1U)
+
+#define S_POVEREN    0
+#define V_POVEREN(x) ((x) << S_POVEREN)
+#define F_POVEREN    V_POVEREN(1U)
 
 #define A_MC5_DB_PART_ID_INDEX 0x72c
 
+#define S_IDINDEX    0
+#define M_IDINDEX    0xf
+#define V_IDINDEX(x) ((x) << S_IDINDEX)
+#define G_IDINDEX(x) (((x) >> S_IDINDEX) & M_IDINDEX)
+
+#define A_MC5_DB_RESET_MAX 0x730
+
+#define S_RSTMAX    0
+#define M_RSTMAX    0xf
+#define V_RSTMAX(x) ((x) << S_RSTMAX)
+#define G_RSTMAX(x) (((x) >> S_RSTMAX) & M_RSTMAX)
+
+#define A_MC5_DB_ACT_CNT 0x734
+
+#define S_ACTCNT    0
+#define M_ACTCNT    0xfffff
+#define V_ACTCNT(x) ((x) << S_ACTCNT)
+#define G_ACTCNT(x) (((x) >> S_ACTCNT) & M_ACTCNT)
+
+#define A_MC5_DB_CLIP_MAP 0x738
+
+#define S_CLIPMAPOP    31
+#define V_CLIPMAPOP(x) ((x) << S_CLIPMAPOP)
+#define F_CLIPMAPOP    V_CLIPMAPOP(1U)
+
+#define S_CLIPMAPVAL    16
+#define M_CLIPMAPVAL    0x3f
+#define V_CLIPMAPVAL(x) ((x) << S_CLIPMAPVAL)
+#define G_CLIPMAPVAL(x) (((x) >> S_CLIPMAPVAL) & M_CLIPMAPVAL)
+
+#define S_CLIPMAPADDR    0
+#define M_CLIPMAPADDR    0x3f
+#define V_CLIPMAPADDR(x) ((x) << S_CLIPMAPADDR)
+#define G_CLIPMAPADDR(x) (((x) >> S_CLIPMAPADDR) & M_CLIPMAPADDR)
+
+#define A_MC5_DB_SIZE 0x73c
 #define A_MC5_DB_INT_ENABLE 0x740
 
+#define S_MSGSEL    28
+#define M_MSGSEL    0xf
+#define V_MSGSEL(x) ((x) << S_MSGSEL)
+#define G_MSGSEL(x) (((x) >> S_MSGSEL) & M_MSGSEL)
+
 #define S_DELACTEMPTY    18
 #define V_DELACTEMPTY(x) ((x) << S_DELACTEMPTY)
 #define F_DELACTEMPTY    V_DELACTEMPTY(1U)
@@ -2075,6 +6766,18 @@
 #define V_UNKNOWNCMD(x) ((x) << S_UNKNOWNCMD)
 #define F_UNKNOWNCMD    V_UNKNOWNCMD(1U)
 
+#define S_SYNCOOKIEOFF    11
+#define V_SYNCOOKIEOFF(x) ((x) << S_SYNCOOKIEOFF)
+#define F_SYNCOOKIEOFF    V_SYNCOOKIEOFF(1U)
+
+#define S_SYNCOOKIEBAD    10
+#define V_SYNCOOKIEBAD(x) ((x) << S_SYNCOOKIEBAD)
+#define F_SYNCOOKIEBAD    V_SYNCOOKIEBAD(1U)
+
+#define S_SYNCOOKIE    9
+#define V_SYNCOOKIE(x) ((x) << S_SYNCOOKIE)
+#define F_SYNCOOKIE    V_SYNCOOKIE(1U)
+
 #define S_NFASRCHFAIL    8
 #define V_NFASRCHFAIL(x) ((x) << S_NFASRCHFAIL)
 #define F_NFASRCHFAIL    V_NFASRCHFAIL(1U)
@@ -2087,76 +6790,296 @@
 #define V_PARITYERR(x) ((x) << S_PARITYERR)
 #define F_PARITYERR    V_PARITYERR(1U)
 
+#define S_LIPMISS    5
+#define V_LIPMISS(x) ((x) << S_LIPMISS)
+#define F_LIPMISS    V_LIPMISS(1U)
+
+#define S_LIP0    4
+#define V_LIP0(x) ((x) << S_LIP0)
+#define F_LIP0    V_LIP0(1U)
+
+#define S_MISS    3
+#define V_MISS(x) ((x) << S_MISS)
+#define F_MISS    V_MISS(1U)
+
+#define S_ROUTINGHIT    2
+#define V_ROUTINGHIT(x) ((x) << S_ROUTINGHIT)
+#define F_ROUTINGHIT    V_ROUTINGHIT(1U)
+
+#define S_ACTIVEHIT    1
+#define V_ACTIVEHIT(x) ((x) << S_ACTIVEHIT)
+#define F_ACTIVEHIT    V_ACTIVEHIT(1U)
+
+#define S_ACTIVEOUTHIT    0
+#define V_ACTIVEOUTHIT(x) ((x) << S_ACTIVEOUTHIT)
+#define F_ACTIVEOUTHIT    V_ACTIVEOUTHIT(1U)
+
 #define A_MC5_DB_INT_CAUSE 0x744
+#define A_MC5_DB_INT_TID 0x748
+
+#define S_INTTID    0
+#define M_INTTID    0xfffff
+#define V_INTTID(x) ((x) << S_INTTID)
+#define G_INTTID(x) (((x) >> S_INTTID) & M_INTTID)
+
+#define A_MC5_DB_INT_PTID 0x74c
+
+#define S_INTPTID    0
+#define M_INTPTID    0xfffff
+#define V_INTPTID(x) ((x) << S_INTPTID)
+#define G_INTPTID(x) (((x) >> S_INTPTID) & M_INTPTID)
 
 #define A_MC5_DB_DBGI_CONFIG 0x774
 
+#define S_WRREQSIZE    22
+#define M_WRREQSIZE    0x3ff
+#define V_WRREQSIZE(x) ((x) << S_WRREQSIZE)
+#define G_WRREQSIZE(x) (((x) >> S_WRREQSIZE) & M_WRREQSIZE)
+
+#define S_SADRSEL    4
+#define V_SADRSEL(x) ((x) << S_SADRSEL)
+#define F_SADRSEL    V_SADRSEL(1U)
+
+#define S_CMDMODE    0
+#define M_CMDMODE    0x7
+#define V_CMDMODE(x) ((x) << S_CMDMODE)
+#define G_CMDMODE(x) (((x) >> S_CMDMODE) & M_CMDMODE)
+
 #define A_MC5_DB_DBGI_REQ_CMD 0x778
 
+#define S_MBUSCMD    0
+#define M_MBUSCMD    0xf
+#define V_MBUSCMD(x) ((x) << S_MBUSCMD)
+#define G_MBUSCMD(x) (((x) >> S_MBUSCMD) & M_MBUSCMD)
+
+#define S_IDTCMDHI    11
+#define M_IDTCMDHI    0x7
+#define V_IDTCMDHI(x) ((x) << S_IDTCMDHI)
+#define G_IDTCMDHI(x) (((x) >> S_IDTCMDHI) & M_IDTCMDHI)
+
+#define S_IDTCMDLO    0
+#define M_IDTCMDLO    0xf
+#define V_IDTCMDLO(x) ((x) << S_IDTCMDLO)
+#define G_IDTCMDLO(x) (((x) >> S_IDTCMDLO) & M_IDTCMDLO)
+
+#define S_IDTCMD    0
+#define M_IDTCMD    0xfffff
+#define V_IDTCMD(x) ((x) << S_IDTCMD)
+#define G_IDTCMD(x) (((x) >> S_IDTCMD) & M_IDTCMD)
+
+#define S_LCMDB    16
+#define M_LCMDB    0x7ff
+#define V_LCMDB(x) ((x) << S_LCMDB)
+#define G_LCMDB(x) (((x) >> S_LCMDB) & M_LCMDB)
+
+#define S_LCMDA    0
+#define M_LCMDA    0x7ff
+#define V_LCMDA(x) ((x) << S_LCMDA)
+#define G_LCMDA(x) (((x) >> S_LCMDA) & M_LCMDA)
+
 #define A_MC5_DB_DBGI_REQ_ADDR0 0x77c
-
 #define A_MC5_DB_DBGI_REQ_ADDR1 0x780
-
 #define A_MC5_DB_DBGI_REQ_ADDR2 0x784
 
+#define S_DBGIREQADRHI    0
+#define M_DBGIREQADRHI    0xff
+#define V_DBGIREQADRHI(x) ((x) << S_DBGIREQADRHI)
+#define G_DBGIREQADRHI(x) (((x) >> S_DBGIREQADRHI) & M_DBGIREQADRHI)
+
 #define A_MC5_DB_DBGI_REQ_DATA0 0x788
-
 #define A_MC5_DB_DBGI_REQ_DATA1 0x78c
-
 #define A_MC5_DB_DBGI_REQ_DATA2 0x790
+#define A_MC5_DB_DBGI_REQ_DATA3 0x794
+#define A_MC5_DB_DBGI_REQ_DATA4 0x798
+
+#define S_DBGIREQDATA4    0
+#define M_DBGIREQDATA4    0xffff
+#define V_DBGIREQDATA4(x) ((x) << S_DBGIREQDATA4)
+#define G_DBGIREQDATA4(x) (((x) >> S_DBGIREQDATA4) & M_DBGIREQDATA4)
+
+#define A_MC5_DB_DBGI_REQ_MASK0 0x79c
+#define A_MC5_DB_DBGI_REQ_MASK1 0x7a0
+#define A_MC5_DB_DBGI_REQ_MASK2 0x7a4
+#define A_MC5_DB_DBGI_REQ_MASK3 0x7a8
+#define A_MC5_DB_DBGI_REQ_MASK4 0x7ac
+
+#define S_DBGIREQMSK4    0
+#define M_DBGIREQMSK4    0xffff
+#define V_DBGIREQMSK4(x) ((x) << S_DBGIREQMSK4)
+#define G_DBGIREQMSK4(x) (((x) >> S_DBGIREQMSK4) & M_DBGIREQMSK4)
 
 #define A_MC5_DB_DBGI_RSP_STATUS 0x7b0
 
+#define S_DBGIRSPMSG    8
+#define M_DBGIRSPMSG    0xf
+#define V_DBGIRSPMSG(x) ((x) << S_DBGIRSPMSG)
+#define G_DBGIRSPMSG(x) (((x) >> S_DBGIRSPMSG) & M_DBGIRSPMSG)
+
+#define S_DBGIRSPMSGVLD    2
+#define V_DBGIRSPMSGVLD(x) ((x) << S_DBGIRSPMSGVLD)
+#define F_DBGIRSPMSGVLD    V_DBGIRSPMSGVLD(1U)
+
+#define S_DBGIRSPHIT    1
+#define V_DBGIRSPHIT(x) ((x) << S_DBGIRSPHIT)
+#define F_DBGIRSPHIT    V_DBGIRSPHIT(1U)
+
 #define S_DBGIRSPVALID    0
 #define V_DBGIRSPVALID(x) ((x) << S_DBGIRSPVALID)
 #define F_DBGIRSPVALID    V_DBGIRSPVALID(1U)
 
 #define A_MC5_DB_DBGI_RSP_DATA0 0x7b4
-
 #define A_MC5_DB_DBGI_RSP_DATA1 0x7b8
-
 #define A_MC5_DB_DBGI_RSP_DATA2 0x7bc
+#define A_MC5_DB_DBGI_RSP_DATA3 0x7c0
+#define A_MC5_DB_DBGI_RSP_DATA4 0x7c4
+
+#define S_DBGIRSPDATA3    0
+#define M_DBGIRSPDATA3    0xffff
+#define V_DBGIRSPDATA3(x) ((x) << S_DBGIRSPDATA3)
+#define G_DBGIRSPDATA3(x) (((x) >> S_DBGIRSPDATA3) & M_DBGIRSPDATA3)
+
+#define A_MC5_DB_DBGI_RSP_LAST_CMD 0x7c8
+
+#define S_LASTCMDB    16
+#define M_LASTCMDB    0x7ff
+#define V_LASTCMDB(x) ((x) << S_LASTCMDB)
+#define G_LASTCMDB(x) (((x) >> S_LASTCMDB) & M_LASTCMDB)
+
+#define S_LASTCMDA    0
+#define M_LASTCMDA    0x7ff
+#define V_LASTCMDA(x) ((x) << S_LASTCMDA)
+#define G_LASTCMDA(x) (((x) >> S_LASTCMDA) & M_LASTCMDA)
 
 #define A_MC5_DB_POPEN_DATA_WR_CMD 0x7cc
 
+#define S_PO_DWR    0
+#define M_PO_DWR    0xfffff
+#define V_PO_DWR(x) ((x) << S_PO_DWR)
+#define G_PO_DWR(x) (((x) >> S_PO_DWR) & M_PO_DWR)
+
 #define A_MC5_DB_POPEN_MASK_WR_CMD 0x7d0
 
+#define S_PO_MWR    0
+#define M_PO_MWR    0xfffff
+#define V_PO_MWR(x) ((x) << S_PO_MWR)
+#define G_PO_MWR(x) (((x) >> S_PO_MWR) & M_PO_MWR)
+
 #define A_MC5_DB_AOPEN_SRCH_CMD 0x7d4
 
+#define S_AO_SRCH    0
+#define M_AO_SRCH    0xfffff
+#define V_AO_SRCH(x) ((x) << S_AO_SRCH)
+#define G_AO_SRCH(x) (((x) >> S_AO_SRCH) & M_AO_SRCH)
+
 #define A_MC5_DB_AOPEN_LRN_CMD 0x7d8
 
+#define S_AO_LRN    0
+#define M_AO_LRN    0xfffff
+#define V_AO_LRN(x) ((x) << S_AO_LRN)
+#define G_AO_LRN(x) (((x) >> S_AO_LRN) & M_AO_LRN)
+
 #define A_MC5_DB_SYN_SRCH_CMD 0x7dc
 
+#define S_SYN_SRCH    0
+#define M_SYN_SRCH    0xfffff
+#define V_SYN_SRCH(x) ((x) << S_SYN_SRCH)
+#define G_SYN_SRCH(x) (((x) >> S_SYN_SRCH) & M_SYN_SRCH)
+
 #define A_MC5_DB_SYN_LRN_CMD 0x7e0
 
+#define S_SYN_LRN    0
+#define M_SYN_LRN    0xfffff
+#define V_SYN_LRN(x) ((x) << S_SYN_LRN)
+#define G_SYN_LRN(x) (((x) >> S_SYN_LRN) & M_SYN_LRN)
+
 #define A_MC5_DB_ACK_SRCH_CMD 0x7e4
 
+#define S_ACK_SRCH    0
+#define M_ACK_SRCH    0xfffff
+#define V_ACK_SRCH(x) ((x) << S_ACK_SRCH)
+#define G_ACK_SRCH(x) (((x) >> S_ACK_SRCH) & M_ACK_SRCH)
+
 #define A_MC5_DB_ACK_LRN_CMD 0x7e8
 
+#define S_ACK_LRN    0
+#define M_ACK_LRN    0xfffff
+#define V_ACK_LRN(x) ((x) << S_ACK_LRN)
+#define G_ACK_LRN(x) (((x) >> S_ACK_LRN) & M_ACK_LRN)
+
 #define A_MC5_DB_ILOOKUP_CMD 0x7ec
 
+#define S_I_SRCH    0
+#define M_I_SRCH    0xfffff
+#define V_I_SRCH(x) ((x) << S_I_SRCH)
+#define G_I_SRCH(x) (((x) >> S_I_SRCH) & M_I_SRCH)
+
 #define A_MC5_DB_ELOOKUP_CMD 0x7f0
 
+#define S_E_SRCH    0
+#define M_E_SRCH    0xfffff
+#define V_E_SRCH(x) ((x) << S_E_SRCH)
+#define G_E_SRCH(x) (((x) >> S_E_SRCH) & M_E_SRCH)
+
 #define A_MC5_DB_DATA_WRITE_CMD 0x7f4
 
+#define S_WRITE    0
+#define M_WRITE    0xfffff
+#define V_WRITE(x) ((x) << S_WRITE)
+#define G_WRITE(x) (((x) >> S_WRITE) & M_WRITE)
+
 #define A_MC5_DB_DATA_READ_CMD 0x7f8
 
+#define S_READCMD    0
+#define M_READCMD    0xfffff
+#define V_READCMD(x) ((x) << S_READCMD)
+#define G_READCMD(x) (((x) >> S_READCMD) & M_READCMD)
+
+#define A_MC5_DB_MASK_WRITE_CMD 0x7fc
+
+#define S_MASKWR    0
+#define M_MASKWR    0xffff
+#define V_MASKWR(x) ((x) << S_MASKWR)
+#define G_MASKWR(x) (((x) >> S_MASKWR) & M_MASKWR)
+
+/* registers for module XGMAC0_0 */
 #define XGMAC0_0_BASE_ADDR 0x800
 
 #define A_XGM_TX_CTRL 0x800
 
+#define S_SENDPAUSE    2
+#define V_SENDPAUSE(x) ((x) << S_SENDPAUSE)
+#define F_SENDPAUSE    V_SENDPAUSE(1U)
+
+#define S_SENDZEROPAUSE    1
+#define V_SENDZEROPAUSE(x) ((x) << S_SENDZEROPAUSE)
+#define F_SENDZEROPAUSE    V_SENDZEROPAUSE(1U)
+
 #define S_TXEN    0
 #define V_TXEN(x) ((x) << S_TXEN)
 #define F_TXEN    V_TXEN(1U)
 
 #define A_XGM_TX_CFG 0x804
 
+#define S_CFGCLKSPEED    2
+#define M_CFGCLKSPEED    0x7
+#define V_CFGCLKSPEED(x) ((x) << S_CFGCLKSPEED)
+#define G_CFGCLKSPEED(x) (((x) >> S_CFGCLKSPEED) & M_CFGCLKSPEED)
+
+#define S_STRETCHMODE    1
+#define V_STRETCHMODE(x) ((x) << S_STRETCHMODE)
+#define F_STRETCHMODE    V_STRETCHMODE(1U)
+
 #define S_TXPAUSEEN    0
 #define V_TXPAUSEEN(x) ((x) << S_TXPAUSEEN)
 #define F_TXPAUSEEN    V_TXPAUSEEN(1U)
 
 #define A_XGM_TX_PAUSE_QUANTA 0x808
 
+#define S_TXPAUSEQUANTA    0
+#define M_TXPAUSEQUANTA    0xffff
+#define V_TXPAUSEQUANTA(x) ((x) << S_TXPAUSEQUANTA)
+#define G_TXPAUSEQUANTA(x) (((x) >> S_TXPAUSEQUANTA) & M_TXPAUSEQUANTA)
+
 #define A_XGM_RX_CTRL 0x80c
 
 #define S_RXEN    0
@@ -2165,6 +7088,18 @@
 
 #define A_XGM_RX_CFG 0x810
 
+#define S_CON802_3PREAMBLE    12
+#define V_CON802_3PREAMBLE(x) ((x) << S_CON802_3PREAMBLE)
+#define F_CON802_3PREAMBLE    V_CON802_3PREAMBLE(1U)
+
+#define S_ENNON802_3PREAMBLE    11
+#define V_ENNON802_3PREAMBLE(x) ((x) << S_ENNON802_3PREAMBLE)
+#define F_ENNON802_3PREAMBLE    V_ENNON802_3PREAMBLE(1U)
+
+#define S_COPYPREAMBLE    10
+#define V_COPYPREAMBLE(x) ((x) << S_COPYPREAMBLE)
+#define F_COPYPREAMBLE    V_COPYPREAMBLE(1U)
+
 #define S_DISPAUSEFRAMES    9
 #define V_DISPAUSEFRAMES(x) ((x) << S_DISPAUSEFRAMES)
 #define F_DISPAUSEFRAMES    V_DISPAUSEFRAMES(1U)
@@ -2181,75 +7116,178 @@
 #define V_RMFCS(x) ((x) << S_RMFCS)
 #define F_RMFCS    V_RMFCS(1U)
 
+#define S_DISNONVLAN    5
+#define V_DISNONVLAN(x) ((x) << S_DISNONVLAN)
+#define F_DISNONVLAN    V_DISNONVLAN(1U)
+
+#define S_ENEXTMATCH    4
+#define V_ENEXTMATCH(x) ((x) << S_ENEXTMATCH)
+#define F_ENEXTMATCH    V_ENEXTMATCH(1U)
+
+#define S_ENHASHUCAST    3
+#define V_ENHASHUCAST(x) ((x) << S_ENHASHUCAST)
+#define F_ENHASHUCAST    V_ENHASHUCAST(1U)
+
 #define S_ENHASHMCAST    2
 #define V_ENHASHMCAST(x) ((x) << S_ENHASHMCAST)
 #define F_ENHASHMCAST    V_ENHASHMCAST(1U)
 
+#define S_DISBCAST    1
+#define V_DISBCAST(x) ((x) << S_DISBCAST)
+#define F_DISBCAST    V_DISBCAST(1U)
+
 #define S_COPYALLFRAMES    0
 #define V_COPYALLFRAMES(x) ((x) << S_COPYALLFRAMES)
 #define F_COPYALLFRAMES    V_COPYALLFRAMES(1U)
 
-#define S_DISBCAST    1
-#define V_DISBCAST(x) ((x) << S_DISBCAST)
-#define F_DISBCAST    V_DISBCAST(1U)
-
 #define A_XGM_RX_HASH_LOW 0x814
-
 #define A_XGM_RX_HASH_HIGH 0x818
-
 #define A_XGM_RX_EXACT_MATCH_LOW_1 0x81c
-
 #define A_XGM_RX_EXACT_MATCH_HIGH_1 0x820
 
+#define S_ADDRESS_HIGH    0
+#define M_ADDRESS_HIGH    0xffff
+#define V_ADDRESS_HIGH(x) ((x) << S_ADDRESS_HIGH)
+#define G_ADDRESS_HIGH(x) (((x) >> S_ADDRESS_HIGH) & M_ADDRESS_HIGH)
+
 #define A_XGM_RX_EXACT_MATCH_LOW_2 0x824
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_2 0x828
 #define A_XGM_RX_EXACT_MATCH_LOW_3 0x82c
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_3 0x830
 #define A_XGM_RX_EXACT_MATCH_LOW_4 0x834
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_4 0x838
 #define A_XGM_RX_EXACT_MATCH_LOW_5 0x83c
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_5 0x840
 #define A_XGM_RX_EXACT_MATCH_LOW_6 0x844
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_6 0x848
 #define A_XGM_RX_EXACT_MATCH_LOW_7 0x84c
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_7 0x850
 #define A_XGM_RX_EXACT_MATCH_LOW_8 0x854
-
+#define A_XGM_RX_EXACT_MATCH_HIGH_8 0x858
+#define A_XGM_RX_TYPE_MATCH_1 0x85c
+
+#define S_ENTYPEMATCH    31
+#define V_ENTYPEMATCH(x) ((x) << S_ENTYPEMATCH)
+#define F_ENTYPEMATCH    V_ENTYPEMATCH(1U)
+
+#define S_TYPE    0
+#define M_TYPE    0xffff
+#define V_TYPE(x) ((x) << S_TYPE)
+#define G_TYPE(x) (((x) >> S_TYPE) & M_TYPE)
+
+#define A_XGM_RX_TYPE_MATCH_2 0x860
+#define A_XGM_RX_TYPE_MATCH_3 0x864
+#define A_XGM_RX_TYPE_MATCH_4 0x868
 #define A_XGM_INT_STATUS 0x86c
 
+#define S_XGMIIEXTINT    10
+#define V_XGMIIEXTINT(x) ((x) << S_XGMIIEXTINT)
+#define F_XGMIIEXTINT    V_XGMIIEXTINT(1U)
+
 #define S_LINKFAULTCHANGE    9
 #define V_LINKFAULTCHANGE(x) ((x) << S_LINKFAULTCHANGE)
 #define F_LINKFAULTCHANGE    V_LINKFAULTCHANGE(1U)
 
+#define S_PHYFRAMECOMPLETE    8
+#define V_PHYFRAMECOMPLETE(x) ((x) << S_PHYFRAMECOMPLETE)
+#define F_PHYFRAMECOMPLETE    V_PHYFRAMECOMPLETE(1U)
+
+#define S_PAUSEFRAMETXMT    7
+#define V_PAUSEFRAMETXMT(x) ((x) << S_PAUSEFRAMETXMT)
+#define F_PAUSEFRAMETXMT    V_PAUSEFRAMETXMT(1U)
+
+#define S_PAUSECNTRTIMEOUT    6
+#define V_PAUSECNTRTIMEOUT(x) ((x) << S_PAUSECNTRTIMEOUT)
+#define F_PAUSECNTRTIMEOUT    V_PAUSECNTRTIMEOUT(1U)
+
+#define S_NON0PAUSERCVD    5
+#define V_NON0PAUSERCVD(x) ((x) << S_NON0PAUSERCVD)
+#define F_NON0PAUSERCVD    V_NON0PAUSERCVD(1U)
+
+#define S_STATOFLOW    4
+#define V_STATOFLOW(x) ((x) << S_STATOFLOW)
+#define F_STATOFLOW    V_STATOFLOW(1U)
+
+#define S_TXERRFIFO    3
+#define V_TXERRFIFO(x) ((x) << S_TXERRFIFO)
+#define F_TXERRFIFO    V_TXERRFIFO(1U)
+
+#define S_TXUFLOW    2
+#define V_TXUFLOW(x) ((x) << S_TXUFLOW)
+#define F_TXUFLOW    V_TXUFLOW(1U)
+
+#define S_FRAMETXMT    1
+#define V_FRAMETXMT(x) ((x) << S_FRAMETXMT)
+#define F_FRAMETXMT    V_FRAMETXMT(1U)
+
+#define S_FRAMERCVD    0
+#define V_FRAMERCVD(x) ((x) << S_FRAMERCVD)
+#define F_FRAMERCVD    V_FRAMERCVD(1U)
+
+#define A_XGM_XGM_INT_MASK 0x870
 #define A_XGM_XGM_INT_ENABLE 0x874
 #define A_XGM_XGM_INT_DISABLE 0x878
+#define A_XGM_TX_PAUSE_TIMER 0x87c
+
+#define S_CURPAUSETIMER    0
+#define M_CURPAUSETIMER    0xffff
+#define V_CURPAUSETIMER(x) ((x) << S_CURPAUSETIMER)
+#define G_CURPAUSETIMER(x) (((x) >> S_CURPAUSETIMER) & M_CURPAUSETIMER)
 
 #define A_XGM_STAT_CTRL 0x880
 
+#define S_READSNPSHOT    4
+#define V_READSNPSHOT(x) ((x) << S_READSNPSHOT)
+#define F_READSNPSHOT    V_READSNPSHOT(1U)
+
+#define S_TAKESNPSHOT    3
+#define V_TAKESNPSHOT(x) ((x) << S_TAKESNPSHOT)
+#define F_TAKESNPSHOT    V_TAKESNPSHOT(1U)
+
 #define S_CLRSTATS    2
 #define V_CLRSTATS(x) ((x) << S_CLRSTATS)
 #define F_CLRSTATS    V_CLRSTATS(1U)
 
+#define S_INCRSTATS    1
+#define V_INCRSTATS(x) ((x) << S_INCRSTATS)
+#define F_INCRSTATS    V_INCRSTATS(1U)
+
+#define S_ENTESTMODEWR    0
+#define V_ENTESTMODEWR(x) ((x) << S_ENTESTMODEWR)
+#define F_ENTESTMODEWR    V_ENTESTMODEWR(1U)
+
 #define A_XGM_RXFIFO_CFG 0x884
 
 #define S_RXFIFO_EMPTY    31
 #define V_RXFIFO_EMPTY(x) ((x) << S_RXFIFO_EMPTY)
 #define F_RXFIFO_EMPTY    V_RXFIFO_EMPTY(1U)
 
+#define S_RXFIFO_FULL    30
+#define V_RXFIFO_FULL(x) ((x) << S_RXFIFO_FULL)
+#define F_RXFIFO_FULL    V_RXFIFO_FULL(1U)
+
 #define S_RXFIFOPAUSEHWM    17
 #define M_RXFIFOPAUSEHWM    0xfff
-
 #define V_RXFIFOPAUSEHWM(x) ((x) << S_RXFIFOPAUSEHWM)
-
 #define G_RXFIFOPAUSEHWM(x) (((x) >> S_RXFIFOPAUSEHWM) & M_RXFIFOPAUSEHWM)
 
 #define S_RXFIFOPAUSELWM    5
 #define M_RXFIFOPAUSELWM    0xfff
-
 #define V_RXFIFOPAUSELWM(x) ((x) << S_RXFIFOPAUSELWM)
-
 #define G_RXFIFOPAUSELWM(x) (((x) >> S_RXFIFOPAUSELWM) & M_RXFIFOPAUSELWM)
 
+#define S_FORCEDPAUSE    4
+#define V_FORCEDPAUSE(x) ((x) << S_FORCEDPAUSE)
+#define F_FORCEDPAUSE    V_FORCEDPAUSE(1U)
+
+#define S_EXTERNLOOPBACK    3
+#define V_EXTERNLOOPBACK(x) ((x) << S_EXTERNLOOPBACK)
+#define F_EXTERNLOOPBACK    V_EXTERNLOOPBACK(1U)
+
+#define S_RXBYTESWAP    2
+#define V_RXBYTESWAP(x) ((x) << S_RXBYTESWAP)
+#define F_RXBYTESWAP    V_RXBYTESWAP(1U)
+
 #define S_RXSTRFRWRD    1
 #define V_RXSTRFRWRD(x) ((x) << S_RXSTRFRWRD)
 #define F_RXSTRFRWRD    V_RXSTRFRWRD(1U)
@@ -2260,10 +7298,22 @@
 
 #define A_XGM_TXFIFO_CFG 0x888
 
+#define S_TXFIFO_EMPTY    31
+#define V_TXFIFO_EMPTY(x) ((x) << S_TXFIFO_EMPTY)
+#define F_TXFIFO_EMPTY    V_TXFIFO_EMPTY(1U)
+
+#define S_TXFIFO_FULL    30
+#define V_TXFIFO_FULL(x) ((x) << S_TXFIFO_FULL)
+#define F_TXFIFO_FULL    V_TXFIFO_FULL(1U)
+
 #define S_UNDERUNFIX    22
 #define V_UNDERUNFIX(x) ((x) << S_UNDERUNFIX)
 #define F_UNDERUNFIX    V_UNDERUNFIX(1U)
 
+#define S_ENDROPPKT    21
+#define V_ENDROPPKT(x) ((x) << S_ENDROPPKT)
+#define F_ENDROPPKT    V_ENDROPPKT(1U)
+
 #define S_TXIPG    13
 #define M_TXIPG    0xff
 #define V_TXIPG(x) ((x) << S_TXIPG)
@@ -2271,20 +7321,93 @@
 
 #define S_TXFIFOTHRESH    4
 #define M_TXFIFOTHRESH    0x1ff
-
 #define V_TXFIFOTHRESH(x) ((x) << S_TXFIFOTHRESH)
-
-#define S_ENDROPPKT    21
-#define V_ENDROPPKT(x) ((x) << S_ENDROPPKT)
-#define F_ENDROPPKT    V_ENDROPPKT(1U)
+#define G_TXFIFOTHRESH(x) (((x) >> S_TXFIFOTHRESH) & M_TXFIFOTHRESH)
+
+#define S_INTERNLOOPBACK    3
+#define V_INTERNLOOPBACK(x) ((x) << S_INTERNLOOPBACK)
+#define F_INTERNLOOPBACK    V_INTERNLOOPBACK(1U)
+
+#define S_TXBYTESWAP    2
+#define V_TXBYTESWAP(x) ((x) << S_TXBYTESWAP)
+#define F_TXBYTESWAP    V_TXBYTESWAP(1U)
+
+#define S_DISCRC    1
+#define V_DISCRC(x) ((x) << S_DISCRC)
+#define F_DISCRC    V_DISCRC(1U)
+
+#define S_DISPREAMBLE    0
+#define V_DISPREAMBLE(x) ((x) << S_DISPREAMBLE)
+#define F_DISPREAMBLE    V_DISPREAMBLE(1U)
+
+#define A_XGM_SLOW_TIMER 0x88c
+
+#define S_PAUSESLOWTIMEREN    31
+#define V_PAUSESLOWTIMEREN(x) ((x) << S_PAUSESLOWTIMEREN)
+#define F_PAUSESLOWTIMEREN    V_PAUSESLOWTIMEREN(1U)
+
+#define S_PAUSESLOWTIMER    0
+#define M_PAUSESLOWTIMER    0xfffff
+#define V_PAUSESLOWTIMER(x) ((x) << S_PAUSESLOWTIMER)
+#define G_PAUSESLOWTIMER(x) (((x) >> S_PAUSESLOWTIMER) & M_PAUSESLOWTIMER)
+
+#define A_XGM_PAUSE_TIMER 0x890
+
+#define S_PAUSETIMER    0
+#define M_PAUSETIMER    0xfffff
+#define V_PAUSETIMER(x) ((x) << S_PAUSETIMER)
+#define G_PAUSETIMER(x) (((x) >> S_PAUSETIMER) & M_PAUSETIMER)
 
 #define A_XGM_SERDES_CTRL 0x890
-#define A_XGM_SERDES_CTRL0 0x8e0
+
+#define S_SERDESEN    25
+#define V_SERDESEN(x) ((x) << S_SERDESEN)
+#define F_SERDESEN    V_SERDESEN(1U)
 
 #define S_SERDESRESET_    24
 #define V_SERDESRESET_(x) ((x) << S_SERDESRESET_)
 #define F_SERDESRESET_    V_SERDESRESET_(1U)
 
+#define S_CMURANGE    21
+#define M_CMURANGE    0x7
+#define V_CMURANGE(x) ((x) << S_CMURANGE)
+#define G_CMURANGE(x) (((x) >> S_CMURANGE) & M_CMURANGE)
+
+#define S_BGENB    20
+#define V_BGENB(x) ((x) << S_BGENB)
+#define F_BGENB    V_BGENB(1U)
+
+#define S_ENSKPDROP    19
+#define V_ENSKPDROP(x) ((x) << S_ENSKPDROP)
+#define F_ENSKPDROP    V_ENSKPDROP(1U)
+
+#define S_ENCOMMA    18
+#define V_ENCOMMA(x) ((x) << S_ENCOMMA)
+#define F_ENCOMMA    V_ENCOMMA(1U)
+
+#define S_EN8B10B    17
+#define V_EN8B10B(x) ((x) << S_EN8B10B)
+#define F_EN8B10B    V_EN8B10B(1U)
+
+#define S_ENELBUF    16
+#define V_ENELBUF(x) ((x) << S_ENELBUF)
+#define F_ENELBUF    V_ENELBUF(1U)
+
+#define S_GAIN    11
+#define M_GAIN    0x1f
+#define V_GAIN(x) ((x) << S_GAIN)
+#define G_GAIN(x) (((x) >> S_GAIN) & M_GAIN)
+
+#define S_BANDGAP    7
+#define M_BANDGAP    0xf
+#define V_BANDGAP(x) ((x) << S_BANDGAP)
+#define G_BANDGAP(x) (((x) >> S_BANDGAP) & M_BANDGAP)
+
+#define S_LPBKEN    5
+#define M_LPBKEN    0x3
+#define V_LPBKEN(x) ((x) << S_LPBKEN)
+#define G_LPBKEN(x) (((x) >> S_LPBKEN) & M_LPBKEN)
+
 #define S_RXENABLE    4
 #define V_RXENABLE(x) ((x) << S_RXENABLE)
 #define F_RXENABLE    V_RXENABLE(1U)
@@ -2293,10 +7416,38 @@
 #define V_TXENABLE(x) ((x) << S_TXENABLE)
 #define F_TXENABLE    V_TXENABLE(1U)
 
-#define A_XGM_PAUSE_TIMER 0x890
+#define A_XGM_XAUI_PCS_TEST 0x894
+
+#define S_TESTPATTERN    1
+#define M_TESTPATTERN    0x3
+#define V_TESTPATTERN(x) ((x) << S_TESTPATTERN)
+#define G_TESTPATTERN(x) (((x) >> S_TESTPATTERN) & M_TESTPATTERN)
+
+#define S_ENTEST    0
+#define V_ENTEST(x) ((x) << S_ENTEST)
+#define F_ENTEST    V_ENTEST(1U)
+
+#define A_XGM_RGMII_CTRL 0x898
+
+#define S_PHALIGNFIFOTHRESH    1
+#define M_PHALIGNFIFOTHRESH    0x3
+#define V_PHALIGNFIFOTHRESH(x) ((x) << S_PHALIGNFIFOTHRESH)
+#define G_PHALIGNFIFOTHRESH(x) (((x) >> S_PHALIGNFIFOTHRESH) & M_PHALIGNFIFOTHRESH)
+
+#define S_TXCLK90SHIFT    0
+#define V_TXCLK90SHIFT(x) ((x) << S_TXCLK90SHIFT)
+#define F_TXCLK90SHIFT    V_TXCLK90SHIFT(1U)
 
 #define A_XGM_RGMII_IMP 0x89c
 
+#define S_CALRESET    8
+#define V_CALRESET(x) ((x) << S_CALRESET)
+#define F_CALRESET    V_CALRESET(1U)
+
+#define S_CALUPDATE    7
+#define V_CALUPDATE(x) ((x) << S_CALUPDATE)
+#define F_CALUPDATE    V_CALUPDATE(1U)
+
 #define S_XGM_IMPSETUPDATE    6
 #define V_XGM_IMPSETUPDATE(x) ((x) << S_XGM_IMPSETUPDATE)
 #define F_XGM_IMPSETUPDATE    V_XGM_IMPSETUPDATE(1U)
@@ -2304,25 +7455,15 @@
 #define S_RGMIIIMPPD    3
 #define M_RGMIIIMPPD    0x7
 #define V_RGMIIIMPPD(x) ((x) << S_RGMIIIMPPD)
+#define G_RGMIIIMPPD(x) (((x) >> S_RGMIIIMPPD) & M_RGMIIIMPPD)
 
 #define S_RGMIIIMPPU    0
 #define M_RGMIIIMPPU    0x7
 #define V_RGMIIIMPPU(x) ((x) << S_RGMIIIMPPU)
-
-#define S_CALRESET    8
-#define V_CALRESET(x) ((x) << S_CALRESET)
-#define F_CALRESET    V_CALRESET(1U)
-
-#define S_CALUPDATE    7
-#define V_CALUPDATE(x) ((x) << S_CALUPDATE)
-#define F_CALUPDATE    V_CALUPDATE(1U)
+#define G_RGMIIIMPPU(x) (((x) >> S_RGMIIIMPPU) & M_RGMIIIMPPU)
 
 #define A_XGM_XAUI_IMP 0x8a0
 
-#define S_CALBUSY    31
-#define V_CALBUSY(x) ((x) << S_CALBUSY)
-#define F_CALBUSY    V_CALBUSY(1U)
-
 #define S_XGM_CALFAULT    29
 #define V_XGM_CALFAULT(x) ((x) << S_XGM_CALFAULT)
 #define F_XGM_CALFAULT    V_XGM_CALFAULT(1U)
@@ -2335,6 +7476,19 @@
 #define S_XAUIIMP    0
 #define M_XAUIIMP    0x7
 #define V_XAUIIMP(x) ((x) << S_XAUIIMP)
+#define G_XAUIIMP(x) (((x) >> S_XAUIIMP) & M_XAUIIMP)
+
+#define A_XGM_SERDES_BIST 0x8a4
+
+#define S_BISTDONE    28
+#define M_BISTDONE    0xf
+#define V_BISTDONE(x) ((x) << S_BISTDONE)
+#define G_BISTDONE(x) (((x) >> S_BISTDONE) & M_BISTDONE)
+
+#define S_BISTCYCLETHRESH    3
+#define M_BISTCYCLETHRESH    0x1ffff
+#define V_BISTCYCLETHRESH(x) ((x) << S_BISTCYCLETHRESH)
+#define G_BISTCYCLETHRESH(x) (((x) >> S_BISTCYCLETHRESH) & M_BISTCYCLETHRESH)
 
 #define A_XGM_RX_MAX_PKT_SIZE 0x8a8
 
@@ -2343,6 +7497,14 @@
 #define V_RXMAXFRAMERSIZE(x) ((x) << S_RXMAXFRAMERSIZE)
 #define G_RXMAXFRAMERSIZE(x) (((x) >> S_RXMAXFRAMERSIZE) & M_RXMAXFRAMERSIZE)
 
+#define S_RXENERRORGATHER    16
+#define V_RXENERRORGATHER(x) ((x) << S_RXENERRORGATHER)
+#define F_RXENERRORGATHER    V_RXENERRORGATHER(1U)
+
+#define S_RXENSINGLEFLIT    15
+#define V_RXENSINGLEFLIT(x) ((x) << S_RXENSINGLEFLIT)
+#define F_RXENSINGLEFLIT    V_RXENSINGLEFLIT(1U)
+
 #define S_RXENFRAMER    14
 #define V_RXENFRAMER(x) ((x) << S_RXENFRAMER)
 #define F_RXENFRAMER    V_RXENFRAMER(1U)
@@ -2374,32 +7536,98 @@
 #define V_MAC_RESET_(x) ((x) << S_MAC_RESET_)
 #define F_MAC_RESET_    V_MAC_RESET_(1U)
 
+#define A_XGM_XAUI1G_CTRL 0x8b0
+
+#define S_XAUI1GLINKID    0
+#define M_XAUI1GLINKID    0x3
+#define V_XAUI1GLINKID(x) ((x) << S_XAUI1GLINKID)
+#define G_XAUI1GLINKID(x) (((x) >> S_XAUI1GLINKID) & M_XAUI1GLINKID)
+
+#define A_XGM_SERDES_LANE_CTRL 0x8b4
+
+#define S_LANEREVERSAL    8
+#define V_LANEREVERSAL(x) ((x) << S_LANEREVERSAL)
+#define F_LANEREVERSAL    V_LANEREVERSAL(1U)
+
+#define S_TXPOLARITY    4
+#define M_TXPOLARITY    0xf
+#define V_TXPOLARITY(x) ((x) << S_TXPOLARITY)
+#define G_TXPOLARITY(x) (((x) >> S_TXPOLARITY) & M_TXPOLARITY)
+
+#define S_RXPOLARITY    0
+#define M_RXPOLARITY    0xf
+#define V_RXPOLARITY(x) ((x) << S_RXPOLARITY)
+#define G_RXPOLARITY(x) (((x) >> S_RXPOLARITY) & M_RXPOLARITY)
+
 #define A_XGM_PORT_CFG 0x8b8
 
+#define S_SAFESPEEDCHANGE    4
+#define V_SAFESPEEDCHANGE(x) ((x) << S_SAFESPEEDCHANGE)
+#define F_SAFESPEEDCHANGE    V_SAFESPEEDCHANGE(1U)
+
 #define S_CLKDIVRESET_    3
 #define V_CLKDIVRESET_(x) ((x) << S_CLKDIVRESET_)
 #define F_CLKDIVRESET_    V_CLKDIVRESET_(1U)
 
 #define S_PORTSPEED    1
 #define M_PORTSPEED    0x3
-
 #define V_PORTSPEED(x) ((x) << S_PORTSPEED)
+#define G_PORTSPEED(x) (((x) >> S_PORTSPEED) & M_PORTSPEED)
 
 #define S_ENRGMII    0
 #define V_ENRGMII(x) ((x) << S_ENRGMII)
 #define F_ENRGMII    V_ENRGMII(1U)
 
+#define A_XGM_EPIO_DATA0 0x8c0
+#define A_XGM_EPIO_DATA1 0x8c4
+#define A_XGM_EPIO_DATA2 0x8c8
+#define A_XGM_EPIO_DATA3 0x8cc
+#define A_XGM_EPIO_OP 0x8d0
+
+#define S_PIO_READY    31
+#define V_PIO_READY(x) ((x) << S_PIO_READY)
+#define F_PIO_READY    V_PIO_READY(1U)
+
+#define S_PIO_WRRD    24
+#define V_PIO_WRRD(x) ((x) << S_PIO_WRRD)
+#define F_PIO_WRRD    V_PIO_WRRD(1U)
+
+#define S_PIO_ADDRESS    0
+#define M_PIO_ADDRESS    0xff
+#define V_PIO_ADDRESS(x) ((x) << S_PIO_ADDRESS)
+#define G_PIO_ADDRESS(x) (((x) >> S_PIO_ADDRESS) & M_PIO_ADDRESS)
+
 #define A_XGM_INT_ENABLE 0x8d4
 
+#define S_XAUIPCSDECERR    24
+#define V_XAUIPCSDECERR(x) ((x) << S_XAUIPCSDECERR)
+#define F_XAUIPCSDECERR    V_XAUIPCSDECERR(1U)
+
+#define S_RGMIIRXFIFOOVERFLOW    23
+#define V_RGMIIRXFIFOOVERFLOW(x) ((x) << S_RGMIIRXFIFOOVERFLOW)
+#define F_RGMIIRXFIFOOVERFLOW    V_RGMIIRXFIFOOVERFLOW(1U)
+
+#define S_RGMIIRXFIFOUNDERFLOW    22
+#define V_RGMIIRXFIFOUNDERFLOW(x) ((x) << S_RGMIIRXFIFOUNDERFLOW)
+#define F_RGMIIRXFIFOUNDERFLOW    V_RGMIIRXFIFOUNDERFLOW(1U)
+
+#define S_RXPKTSIZEERROR    21
+#define V_RXPKTSIZEERROR(x) ((x) << S_RXPKTSIZEERROR)
+#define F_RXPKTSIZEERROR    V_RXPKTSIZEERROR(1U)
+
+#define S_WOLPATDETECTED    20
+#define V_WOLPATDETECTED(x) ((x) << S_WOLPATDETECTED)
+#define F_WOLPATDETECTED    V_WOLPATDETECTED(1U)
+
 #define S_TXFIFO_PRTY_ERR    17
 #define M_TXFIFO_PRTY_ERR    0x7
-
 #define V_TXFIFO_PRTY_ERR(x) ((x) << S_TXFIFO_PRTY_ERR)
+#define G_TXFIFO_PRTY_ERR(x) (((x) >> S_TXFIFO_PRTY_ERR) & M_TXFIFO_PRTY_ERR)
 
 #define S_RXFIFO_PRTY_ERR    14
 #define M_RXFIFO_PRTY_ERR    0x7
-
 #define V_RXFIFO_PRTY_ERR(x) ((x) << S_RXFIFO_PRTY_ERR)
+#define G_RXFIFO_PRTY_ERR(x) (((x) >> S_RXFIFO_PRTY_ERR) & M_RXFIFO_PRTY_ERR)
 
 #define S_TXFIFO_UNDERRUN    13
 #define V_TXFIFO_UNDERRUN(x) ((x) << S_TXFIFO_UNDERRUN)
@@ -2409,10 +7637,15 @@
 #define V_RXFIFO_OVERFLOW(x) ((x) << S_RXFIFO_OVERFLOW)
 #define F_RXFIFO_OVERFLOW    V_RXFIFO_OVERFLOW(1U)
 
-#define S_SERDES_LOS    4
-#define M_SERDES_LOS    0xf
-
-#define V_SERDES_LOS(x) ((x) << S_SERDES_LOS)
+#define S_SERDESBISTERR    8
+#define M_SERDESBISTERR    0xf
+#define V_SERDESBISTERR(x) ((x) << S_SERDESBISTERR)
+#define G_SERDESBISTERR(x) (((x) >> S_SERDESBISTERR) & M_SERDESBISTERR)
+
+#define S_SERDESLOWSIGCHANGE    4
+#define M_SERDESLOWSIGCHANGE    0xf
+#define V_SERDESLOWSIGCHANGE(x) ((x) << S_SERDESLOWSIGCHANGE)
+#define G_SERDESLOWSIGCHANGE(x) (((x) >> S_SERDESLOWSIGCHANGE) & M_SERDESLOWSIGCHANGE)
 
 #define S_XAUIPCSCTCERR    3
 #define V_XAUIPCSCTCERR(x) ((x) << S_XAUIPCSCTCERR)
@@ -2422,12 +7655,29 @@
 #define V_XAUIPCSALIGNCHANGE(x) ((x) << S_XAUIPCSALIGNCHANGE)
 #define F_XAUIPCSALIGNCHANGE    V_XAUIPCSALIGNCHANGE(1U)
 
+#define S_RGMIILINKSTSCHANGE    1
+#define V_RGMIILINKSTSCHANGE(x) ((x) << S_RGMIILINKSTSCHANGE)
+#define F_RGMIILINKSTSCHANGE    V_RGMIILINKSTSCHANGE(1U)
+
 #define S_XGM_INT    0
 #define V_XGM_INT(x) ((x) << S_XGM_INT)
 #define F_XGM_INT    V_XGM_INT(1U)
 
+#define S_SERDESCMULOCK_LOSS    24
+#define V_SERDESCMULOCK_LOSS(x) ((x) << S_SERDESCMULOCK_LOSS)
+#define F_SERDESCMULOCK_LOSS    V_SERDESCMULOCK_LOSS(1U)
+
+#define S_SERDESBIST_ERR    8
+#define M_SERDESBIST_ERR    0xf
+#define V_SERDESBIST_ERR(x) ((x) << S_SERDESBIST_ERR)
+#define G_SERDESBIST_ERR(x) (((x) >> S_SERDESBIST_ERR) & M_SERDESBIST_ERR)
+
+#define S_SERDES_LOS    4
+#define M_SERDES_LOS    0xf
+#define V_SERDES_LOS(x) ((x) << S_SERDES_LOS)
+#define G_SERDES_LOS(x) (((x) >> S_SERDES_LOS) & M_SERDES_LOS)
+
 #define A_XGM_INT_CAUSE 0x8d8
-
 #define A_XGM_XAUI_ACT_CTRL 0x8dc
 
 #define S_TXACTENABLE    1
@@ -2436,6 +7686,22 @@
 
 #define A_XGM_SERDES_CTRL0 0x8e0
 
+#define S_INTSERLPBK3    27
+#define V_INTSERLPBK3(x) ((x) << S_INTSERLPBK3)
+#define F_INTSERLPBK3    V_INTSERLPBK3(1U)
+
+#define S_INTSERLPBK2    26
+#define V_INTSERLPBK2(x) ((x) << S_INTSERLPBK2)
+#define F_INTSERLPBK2    V_INTSERLPBK2(1U)
+
+#define S_INTSERLPBK1    25
+#define V_INTSERLPBK1(x) ((x) << S_INTSERLPBK1)
+#define F_INTSERLPBK1    V_INTSERLPBK1(1U)
+
+#define S_INTSERLPBK0    24
+#define V_INTSERLPBK0(x) ((x) << S_INTSERLPBK0)
+#define F_INTSERLPBK0    V_INTSERLPBK0(1U)
+
 #define S_RESET3    23
 #define V_RESET3(x) ((x) << S_RESET3)
 #define F_RESET3    V_RESET3(1U)
@@ -2476,96 +7742,582 @@
 #define V_RESETPLL01(x) ((x) << S_RESETPLL01)
 #define F_RESETPLL01    V_RESETPLL01(1U)
 
+#define S_PW23    12
+#define M_PW23    0x3
+#define V_PW23(x) ((x) << S_PW23)
+#define G_PW23(x) (((x) >> S_PW23) & M_PW23)
+
+#define S_PW01    10
+#define M_PW01    0x3
+#define V_PW01(x) ((x) << S_PW01)
+#define G_PW01(x) (((x) >> S_PW01) & M_PW01)
+
+#define S_XGM_DEQ    6
+#define M_XGM_DEQ    0xf
+#define V_XGM_DEQ(x) ((x) << S_XGM_DEQ)
+#define G_XGM_DEQ(x) (((x) >> S_XGM_DEQ) & M_XGM_DEQ)
+
+#define S_XGM_DTX    2
+#define M_XGM_DTX    0xf
+#define V_XGM_DTX(x) ((x) << S_XGM_DTX)
+#define G_XGM_DTX(x) (((x) >> S_XGM_DTX) & M_XGM_DTX)
+
+#define S_XGM_LODRV    1
+#define V_XGM_LODRV(x) ((x) << S_XGM_LODRV)
+#define F_XGM_LODRV    V_XGM_LODRV(1U)
+
+#define S_XGM_HIDRV    0
+#define V_XGM_HIDRV(x) ((x) << S_XGM_HIDRV)
+#define F_XGM_HIDRV    V_XGM_HIDRV(1U)
+
+#define A_XGM_SERDES_CTRL1 0x8e4
+
+#define S_FMOFFSET3    19
+#define M_FMOFFSET3    0x1f
+#define V_FMOFFSET3(x) ((x) << S_FMOFFSET3)
+#define G_FMOFFSET3(x) (((x) >> S_FMOFFSET3) & M_FMOFFSET3)
+
+#define S_FMOFFSETEN3    18
+#define V_FMOFFSETEN3(x) ((x) << S_FMOFFSETEN3)
+#define F_FMOFFSETEN3    V_FMOFFSETEN3(1U)
+
+#define S_FMOFFSET2    13
+#define M_FMOFFSET2    0x1f
+#define V_FMOFFSET2(x) ((x) << S_FMOFFSET2)
+#define G_FMOFFSET2(x) (((x) >> S_FMOFFSET2) & M_FMOFFSET2)
+
+#define S_FMOFFSETEN2    12
+#define V_FMOFFSETEN2(x) ((x) << S_FMOFFSETEN2)
+#define F_FMOFFSETEN2    V_FMOFFSETEN2(1U)
+
+#define S_FMOFFSET1    7
+#define M_FMOFFSET1    0x1f
+#define V_FMOFFSET1(x) ((x) << S_FMOFFSET1)
+#define G_FMOFFSET1(x) (((x) >> S_FMOFFSET1) & M_FMOFFSET1)
+
+#define S_FMOFFSETEN1    6
+#define V_FMOFFSETEN1(x) ((x) << S_FMOFFSETEN1)
+#define F_FMOFFSETEN1    V_FMOFFSETEN1(1U)
+
+#define S_FMOFFSET0    1
+#define M_FMOFFSET0    0x1f
+#define V_FMOFFSET0(x) ((x) << S_FMOFFSET0)
+#define G_FMOFFSET0(x) (((x) >> S_FMOFFSET0) & M_FMOFFSET0)
+
+#define S_FMOFFSETEN0    0
+#define V_FMOFFSETEN0(x) ((x) << S_FMOFFSETEN0)
+#define F_FMOFFSETEN0    V_FMOFFSETEN0(1U)
+
+#define A_XGM_SERDES_CTRL2 0x8e8
+
+#define S_DNIN3    11
+#define V_DNIN3(x) ((x) << S_DNIN3)
+#define F_DNIN3    V_DNIN3(1U)
+
+#define S_UPIN3    10
+#define V_UPIN3(x) ((x) << S_UPIN3)
+#define F_UPIN3    V_UPIN3(1U)
+
+#define S_RXSLAVE3    9
+#define V_RXSLAVE3(x) ((x) << S_RXSLAVE3)
+#define F_RXSLAVE3    V_RXSLAVE3(1U)
+
+#define S_DNIN2    8
+#define V_DNIN2(x) ((x) << S_DNIN2)
+#define F_DNIN2    V_DNIN2(1U)
+
+#define S_UPIN2    7
+#define V_UPIN2(x) ((x) << S_UPIN2)
+#define F_UPIN2    V_UPIN2(1U)
+
+#define S_RXSLAVE2    6
+#define V_RXSLAVE2(x) ((x) << S_RXSLAVE2)
+#define F_RXSLAVE2    V_RXSLAVE2(1U)
+
+#define S_DNIN1    5
+#define V_DNIN1(x) ((x) << S_DNIN1)
+#define F_DNIN1    V_DNIN1(1U)
+
+#define S_UPIN1    4
+#define V_UPIN1(x) ((x) << S_UPIN1)
+#define F_UPIN1    V_UPIN1(1U)
+
+#define S_RXSLAVE1    3
+#define V_RXSLAVE1(x) ((x) << S_RXSLAVE1)
+#define F_RXSLAVE1    V_RXSLAVE1(1U)
+
+#define S_DNIN0    2
+#define V_DNIN0(x) ((x) << S_DNIN0)
+#define F_DNIN0    V_DNIN0(1U)
+
+#define S_UPIN0    1
+#define V_UPIN0(x) ((x) << S_UPIN0)
+#define F_UPIN0    V_UPIN0(1U)
+
+#define S_RXSLAVE0    0
+#define V_RXSLAVE0(x) ((x) << S_RXSLAVE0)
+#define F_RXSLAVE0    V_RXSLAVE0(1U)
+
+#define A_XGM_SERDES_CTRL3 0x8ec
+
+#define S_EXTBISTCHKERRCLR3    31
+#define V_EXTBISTCHKERRCLR3(x) ((x) << S_EXTBISTCHKERRCLR3)
+#define F_EXTBISTCHKERRCLR3    V_EXTBISTCHKERRCLR3(1U)
+
+#define S_EXTBISTCHKEN3    30
+#define V_EXTBISTCHKEN3(x) ((x) << S_EXTBISTCHKEN3)
+#define F_EXTBISTCHKEN3    V_EXTBISTCHKEN3(1U)
+
+#define S_EXTBISTGENEN3    29
+#define V_EXTBISTGENEN3(x) ((x) << S_EXTBISTGENEN3)
+#define F_EXTBISTGENEN3    V_EXTBISTGENEN3(1U)
+
+#define S_EXTBISTPAT3    26
+#define M_EXTBISTPAT3    0x7
+#define V_EXTBISTPAT3(x) ((x) << S_EXTBISTPAT3)
+#define G_EXTBISTPAT3(x) (((x) >> S_EXTBISTPAT3) & M_EXTBISTPAT3)
+
+#define S_EXTPARRESET3    25
+#define V_EXTPARRESET3(x) ((x) << S_EXTPARRESET3)
+#define F_EXTPARRESET3    V_EXTPARRESET3(1U)
+
+#define S_EXTPARLPBK3    24
+#define V_EXTPARLPBK3(x) ((x) << S_EXTPARLPBK3)
+#define F_EXTPARLPBK3    V_EXTPARLPBK3(1U)
+
+#define S_EXTBISTCHKERRCLR2    23
+#define V_EXTBISTCHKERRCLR2(x) ((x) << S_EXTBISTCHKERRCLR2)
+#define F_EXTBISTCHKERRCLR2    V_EXTBISTCHKERRCLR2(1U)
+
+#define S_EXTBISTCHKEN2    22
+#define V_EXTBISTCHKEN2(x) ((x) << S_EXTBISTCHKEN2)
+#define F_EXTBISTCHKEN2    V_EXTBISTCHKEN2(1U)
+
+#define S_EXTBISTGENEN2    21
+#define V_EXTBISTGENEN2(x) ((x) << S_EXTBISTGENEN2)
+#define F_EXTBISTGENEN2    V_EXTBISTGENEN2(1U)
+
+#define S_EXTBISTPAT2    18
+#define M_EXTBISTPAT2    0x7
+#define V_EXTBISTPAT2(x) ((x) << S_EXTBISTPAT2)
+#define G_EXTBISTPAT2(x) (((x) >> S_EXTBISTPAT2) & M_EXTBISTPAT2)
+
+#define S_EXTPARRESET2    17
+#define V_EXTPARRESET2(x) ((x) << S_EXTPARRESET2)
+#define F_EXTPARRESET2    V_EXTPARRESET2(1U)
+
+#define S_EXTPARLPBK2    16
+#define V_EXTPARLPBK2(x) ((x) << S_EXTPARLPBK2)
+#define F_EXTPARLPBK2    V_EXTPARLPBK2(1U)
+
+#define S_EXTBISTCHKERRCLR1    15
+#define V_EXTBISTCHKERRCLR1(x) ((x) << S_EXTBISTCHKERRCLR1)
+#define F_EXTBISTCHKERRCLR1    V_EXTBISTCHKERRCLR1(1U)
+
+#define S_EXTBISTCHKEN1    14
+#define V_EXTBISTCHKEN1(x) ((x) << S_EXTBISTCHKEN1)
+#define F_EXTBISTCHKEN1    V_EXTBISTCHKEN1(1U)
+
+#define S_EXTBISTGENEN1    13
+#define V_EXTBISTGENEN1(x) ((x) << S_EXTBISTGENEN1)
+#define F_EXTBISTGENEN1    V_EXTBISTGENEN1(1U)
+
+#define S_EXTBISTPAT1    10
+#define M_EXTBISTPAT1    0x7
+#define V_EXTBISTPAT1(x) ((x) << S_EXTBISTPAT1)
+#define G_EXTBISTPAT1(x) (((x) >> S_EXTBISTPAT1) & M_EXTBISTPAT1)
+
+#define S_EXTPARRESET1    9
+#define V_EXTPARRESET1(x) ((x) << S_EXTPARRESET1)
+#define F_EXTPARRESET1    V_EXTPARRESET1(1U)
+
+#define S_EXTPARLPBK1    8
+#define V_EXTPARLPBK1(x) ((x) << S_EXTPARLPBK1)
+#define F_EXTPARLPBK1    V_EXTPARLPBK1(1U)
+
+#define S_EXTBISTCHKERRCLR0    7
+#define V_EXTBISTCHKERRCLR0(x) ((x) << S_EXTBISTCHKERRCLR0)
+#define F_EXTBISTCHKERRCLR0    V_EXTBISTCHKERRCLR0(1U)
+
+#define S_EXTBISTCHKEN0    6
+#define V_EXTBISTCHKEN0(x) ((x) << S_EXTBISTCHKEN0)
+#define F_EXTBISTCHKEN0    V_EXTBISTCHKEN0(1U)
+
+#define S_EXTBISTGENEN0    5
+#define V_EXTBISTGENEN0(x) ((x) << S_EXTBISTGENEN0)
+#define F_EXTBISTGENEN0    V_EXTBISTGENEN0(1U)
+
+#define S_EXTBISTPAT0    2
+#define M_EXTBISTPAT0    0x7
+#define V_EXTBISTPAT0(x) ((x) << S_EXTBISTPAT0)
+#define G_EXTBISTPAT0(x) (((x) >> S_EXTBISTPAT0) & M_EXTBISTPAT0)
+
+#define S_EXTPARRESET0    1
+#define V_EXTPARRESET0(x) ((x) << S_EXTPARRESET0)
+#define F_EXTPARRESET0    V_EXTPARRESET0(1U)
+
+#define S_EXTPARLPBK0    0
+#define V_EXTPARLPBK0(x) ((x) << S_EXTPARLPBK0)
+#define F_EXTPARLPBK0    V_EXTPARLPBK0(1U)
+
 #define A_XGM_SERDES_STAT0 0x8f0
-#define A_XGM_SERDES_STAT1 0x8f4
-#define A_XGM_SERDES_STAT2 0x8f8
+
+#define S_EXTBISTCHKERRCNT0    4
+#define M_EXTBISTCHKERRCNT0    0xffffff
+#define V_EXTBISTCHKERRCNT0(x) ((x) << S_EXTBISTCHKERRCNT0)
+#define G_EXTBISTCHKERRCNT0(x) (((x) >> S_EXTBISTCHKERRCNT0) & M_EXTBISTCHKERRCNT0)
+
+#define S_EXTBISTCHKFMD0    3
+#define V_EXTBISTCHKFMD0(x) ((x) << S_EXTBISTCHKFMD0)
+#define F_EXTBISTCHKFMD0    V_EXTBISTCHKFMD0(1U)
+
+#define S_LOWSIGFORCEEN0    2
+#define V_LOWSIGFORCEEN0(x) ((x) << S_LOWSIGFORCEEN0)
+#define F_LOWSIGFORCEEN0    V_LOWSIGFORCEEN0(1U)
+
+#define S_LOWSIGFORCEVALUE0    1
+#define V_LOWSIGFORCEVALUE0(x) ((x) << S_LOWSIGFORCEVALUE0)
+#define F_LOWSIGFORCEVALUE0    V_LOWSIGFORCEVALUE0(1U)
 
 #define S_LOWSIG0    0
 #define V_LOWSIG0(x) ((x) << S_LOWSIG0)
 #define F_LOWSIG0    V_LOWSIG0(1U)
 
+#define A_XGM_SERDES_STAT1 0x8f4
+
+#define S_EXTBISTCHKERRCNT1    4
+#define M_EXTBISTCHKERRCNT1    0xffffff
+#define V_EXTBISTCHKERRCNT1(x) ((x) << S_EXTBISTCHKERRCNT1)
+#define G_EXTBISTCHKERRCNT1(x) (((x) >> S_EXTBISTCHKERRCNT1) & M_EXTBISTCHKERRCNT1)
+
+#define S_EXTBISTCHKFMD1    3
+#define V_EXTBISTCHKFMD1(x) ((x) << S_EXTBISTCHKFMD1)
+#define F_EXTBISTCHKFMD1    V_EXTBISTCHKFMD1(1U)
+
+#define S_LOWSIGFORCEEN1    2
+#define V_LOWSIGFORCEEN1(x) ((x) << S_LOWSIGFORCEEN1)
+#define F_LOWSIGFORCEEN1    V_LOWSIGFORCEEN1(1U)
+
+#define S_LOWSIGFORCEVALUE1    1
+#define V_LOWSIGFORCEVALUE1(x) ((x) << S_LOWSIGFORCEVALUE1)
+#define F_LOWSIGFORCEVALUE1    V_LOWSIGFORCEVALUE1(1U)
+
+#define S_LOWSIG1    0
+#define V_LOWSIG1(x) ((x) << S_LOWSIG1)
+#define F_LOWSIG1    V_LOWSIG1(1U)
+
+#define A_XGM_SERDES_STAT2 0x8f8
+
+#define S_EXTBISTCHKERRCNT2    4
+#define M_EXTBISTCHKERRCNT2    0xffffff
+#define V_EXTBISTCHKERRCNT2(x) ((x) << S_EXTBISTCHKERRCNT2)
+#define G_EXTBISTCHKERRCNT2(x) (((x) >> S_EXTBISTCHKERRCNT2) & M_EXTBISTCHKERRCNT2)
+
+#define S_EXTBISTCHKFMD2    3
+#define V_EXTBISTCHKFMD2(x) ((x) << S_EXTBISTCHKFMD2)
+#define F_EXTBISTCHKFMD2    V_EXTBISTCHKFMD2(1U)
+
+#define S_LOWSIGFORCEEN2    2
+#define V_LOWSIGFORCEEN2(x) ((x) << S_LOWSIGFORCEEN2)
+#define F_LOWSIGFORCEEN2    V_LOWSIGFORCEEN2(1U)
+
+#define S_LOWSIGFORCEVALUE2    1
+#define V_LOWSIGFORCEVALUE2(x) ((x) << S_LOWSIGFORCEVALUE2)
+#define F_LOWSIGFORCEVALUE2    V_LOWSIGFORCEVALUE2(1U)
+
+#define S_LOWSIG2    0
+#define V_LOWSIG2(x) ((x) << S_LOWSIG2)
+#define F_LOWSIG2    V_LOWSIG2(1U)
+
 #define A_XGM_SERDES_STAT3 0x8fc
 
+#define S_EXTBISTCHKERRCNT3    4
+#define M_EXTBISTCHKERRCNT3    0xffffff
+#define V_EXTBISTCHKERRCNT3(x) ((x) << S_EXTBISTCHKERRCNT3)
+#define G_EXTBISTCHKERRCNT3(x) (((x) >> S_EXTBISTCHKERRCNT3) & M_EXTBISTCHKERRCNT3)
+
+#define S_EXTBISTCHKFMD3    3
+#define V_EXTBISTCHKFMD3(x) ((x) << S_EXTBISTCHKFMD3)
+#define F_EXTBISTCHKFMD3    V_EXTBISTCHKFMD3(1U)
+
+#define S_LOWSIGFORCEEN3    2
+#define V_LOWSIGFORCEEN3(x) ((x) << S_LOWSIGFORCEEN3)
+#define F_LOWSIGFORCEEN3    V_LOWSIGFORCEEN3(1U)
+
+#define S_LOWSIGFORCEVALUE3    1
+#define V_LOWSIGFORCEVALUE3(x) ((x) << S_LOWSIGFORCEVALUE3)
+#define F_LOWSIGFORCEVALUE3    V_LOWSIGFORCEVALUE3(1U)
+
+#define S_LOWSIG3    0
+#define V_LOWSIG3(x) ((x) << S_LOWSIG3)
+#define F_LOWSIG3    V_LOWSIG3(1U)
+
 #define A_XGM_STAT_TX_BYTE_LOW 0x900
-
 #define A_XGM_STAT_TX_BYTE_HIGH 0x904
 
+#define S_TXBYTES_HIGH    0
+#define M_TXBYTES_HIGH    0x1fff
+#define V_TXBYTES_HIGH(x) ((x) << S_TXBYTES_HIGH)
+#define G_TXBYTES_HIGH(x) (((x) >> S_TXBYTES_HIGH) & M_TXBYTES_HIGH)
+
 #define A_XGM_STAT_TX_FRAME_LOW 0x908
-
 #define A_XGM_STAT_TX_FRAME_HIGH 0x90c
 
+#define S_TXFRAMES_HIGH    0
+#define M_TXFRAMES_HIGH    0xf
+#define V_TXFRAMES_HIGH(x) ((x) << S_TXFRAMES_HIGH)
+#define G_TXFRAMES_HIGH(x) (((x) >> S_TXFRAMES_HIGH) & M_TXFRAMES_HIGH)
+
 #define A_XGM_STAT_TX_BCAST 0x910
-
 #define A_XGM_STAT_TX_MCAST 0x914
-
 #define A_XGM_STAT_TX_PAUSE 0x918
-
 #define A_XGM_STAT_TX_64B_FRAMES 0x91c
-
 #define A_XGM_STAT_TX_65_127B_FRAMES 0x920
-
 #define A_XGM_STAT_TX_128_255B_FRAMES 0x924
-
 #define A_XGM_STAT_TX_256_511B_FRAMES 0x928
-
 #define A_XGM_STAT_TX_512_1023B_FRAMES 0x92c
-
 #define A_XGM_STAT_TX_1024_1518B_FRAMES 0x930
-
 #define A_XGM_STAT_TX_1519_MAXB_FRAMES 0x934
-
 #define A_XGM_STAT_TX_ERR_FRAMES 0x938
-
 #define A_XGM_STAT_RX_BYTES_LOW 0x93c
-
 #define A_XGM_STAT_RX_BYTES_HIGH 0x940
 
+#define S_RXBYTES_HIGH    0
+#define M_RXBYTES_HIGH    0x1fff
+#define V_RXBYTES_HIGH(x) ((x) << S_RXBYTES_HIGH)
+#define G_RXBYTES_HIGH(x) (((x) >> S_RXBYTES_HIGH) & M_RXBYTES_HIGH)
+
 #define A_XGM_STAT_RX_FRAMES_LOW 0x944
-
 #define A_XGM_STAT_RX_FRAMES_HIGH 0x948
 
+#define S_RXFRAMES_HIGH    0
+#define M_RXFRAMES_HIGH    0xf
+#define V_RXFRAMES_HIGH(x) ((x) << S_RXFRAMES_HIGH)
+#define G_RXFRAMES_HIGH(x) (((x) >> S_RXFRAMES_HIGH) & M_RXFRAMES_HIGH)
+
 #define A_XGM_STAT_RX_BCAST_FRAMES 0x94c
-
 #define A_XGM_STAT_RX_MCAST_FRAMES 0x950
-
 #define A_XGM_STAT_RX_PAUSE_FRAMES 0x954
 
+#define S_RXPAUSEFRAMES    0
+#define M_RXPAUSEFRAMES    0xffff
+#define V_RXPAUSEFRAMES(x) ((x) << S_RXPAUSEFRAMES)
+#define G_RXPAUSEFRAMES(x) (((x) >> S_RXPAUSEFRAMES) & M_RXPAUSEFRAMES)
+
 #define A_XGM_STAT_RX_64B_FRAMES 0x958
-
 #define A_XGM_STAT_RX_65_127B_FRAMES 0x95c
-
 #define A_XGM_STAT_RX_128_255B_FRAMES 0x960
-
 #define A_XGM_STAT_RX_256_511B_FRAMES 0x964
-
 #define A_XGM_STAT_RX_512_1023B_FRAMES 0x968
-
 #define A_XGM_STAT_RX_1024_1518B_FRAMES 0x96c
-
 #define A_XGM_STAT_RX_1519_MAXB_FRAMES 0x970
-
 #define A_XGM_STAT_RX_SHORT_FRAMES 0x974
 
+#define S_RXSHORTFRAMES    0
+#define M_RXSHORTFRAMES    0xffff
+#define V_RXSHORTFRAMES(x) ((x) << S_RXSHORTFRAMES)
+#define G_RXSHORTFRAMES(x) (((x) >> S_RXSHORTFRAMES) & M_RXSHORTFRAMES)
+
 #define A_XGM_STAT_RX_OVERSIZE_FRAMES 0x978
 
+#define S_RXOVERSIZEFRAMES    0
+#define M_RXOVERSIZEFRAMES    0xffff
+#define V_RXOVERSIZEFRAMES(x) ((x) << S_RXOVERSIZEFRAMES)
+#define G_RXOVERSIZEFRAMES(x) (((x) >> S_RXOVERSIZEFRAMES) & M_RXOVERSIZEFRAMES)
+
 #define A_XGM_STAT_RX_JABBER_FRAMES 0x97c
 
+#define S_RXJABBERFRAMES    0
+#define M_RXJABBERFRAMES    0xffff
+#define V_RXJABBERFRAMES(x) ((x) << S_RXJABBERFRAMES)
+#define G_RXJABBERFRAMES(x) (((x) >> S_RXJABBERFRAMES) & M_RXJABBERFRAMES)
+
 #define A_XGM_STAT_RX_CRC_ERR_FRAMES 0x980
 
+#define S_RXCRCERRFRAMES    0
+#define M_RXCRCERRFRAMES    0xffff
+#define V_RXCRCERRFRAMES(x) ((x) << S_RXCRCERRFRAMES)
+#define G_RXCRCERRFRAMES(x) (((x) >> S_RXCRCERRFRAMES) & M_RXCRCERRFRAMES)
+
 #define A_XGM_STAT_RX_LENGTH_ERR_FRAMES 0x984
 
+#define S_RXLENGTHERRFRAMES    0
+#define M_RXLENGTHERRFRAMES    0xffff
+#define V_RXLENGTHERRFRAMES(x) ((x) << S_RXLENGTHERRFRAMES)
+#define G_RXLENGTHERRFRAMES(x) (((x) >> S_RXLENGTHERRFRAMES) & M_RXLENGTHERRFRAMES)
+
 #define A_XGM_STAT_RX_SYM_CODE_ERR_FRAMES 0x988
 
+#define S_RXSYMCODEERRFRAMES    0
+#define M_RXSYMCODEERRFRAMES    0xffff
+#define V_RXSYMCODEERRFRAMES(x) ((x) << S_RXSYMCODEERRFRAMES)
+#define G_RXSYMCODEERRFRAMES(x) (((x) >> S_RXSYMCODEERRFRAMES) & M_RXSYMCODEERRFRAMES)
+
 #define A_XGM_SERDES_STATUS0 0x98c
 
+#define S_RXERRLANE3    9
+#define M_RXERRLANE3    0x7
+#define V_RXERRLANE3(x) ((x) << S_RXERRLANE3)
+#define G_RXERRLANE3(x) (((x) >> S_RXERRLANE3) & M_RXERRLANE3)
+
+#define S_RXERRLANE2    6
+#define M_RXERRLANE2    0x7
+#define V_RXERRLANE2(x) ((x) << S_RXERRLANE2)
+#define G_RXERRLANE2(x) (((x) >> S_RXERRLANE2) & M_RXERRLANE2)
+
+#define S_RXERRLANE1    3
+#define M_RXERRLANE1    0x7
+#define V_RXERRLANE1(x) ((x) << S_RXERRLANE1)
+#define G_RXERRLANE1(x) (((x) >> S_RXERRLANE1) & M_RXERRLANE1)
+
+#define S_RXERRLANE0    0
+#define M_RXERRLANE0    0x7
+#define V_RXERRLANE0(x) ((x) << S_RXERRLANE0)
+#define G_RXERRLANE0(x) (((x) >> S_RXERRLANE0) & M_RXERRLANE0)
+
 #define A_XGM_SERDES_STATUS1 0x990
 
-#define S_CMULOCK    31
-#define V_CMULOCK(x) ((x) << S_CMULOCK)
-#define F_CMULOCK    V_CMULOCK(1U)
+#define S_RXKLOCKLANE3    11
+#define V_RXKLOCKLANE3(x) ((x) << S_RXKLOCKLANE3)
+#define F_RXKLOCKLANE3    V_RXKLOCKLANE3(1U)
+
+#define S_RXKLOCKLANE2    10
+#define V_RXKLOCKLANE2(x) ((x) << S_RXKLOCKLANE2)
+#define F_RXKLOCKLANE2    V_RXKLOCKLANE2(1U)
+
+#define S_RXKLOCKLANE1    9
+#define V_RXKLOCKLANE1(x) ((x) << S_RXKLOCKLANE1)
+#define F_RXKLOCKLANE1    V_RXKLOCKLANE1(1U)
+
+#define S_RXKLOCKLANE0    8
+#define V_RXKLOCKLANE0(x) ((x) << S_RXKLOCKLANE0)
+#define F_RXKLOCKLANE0    V_RXKLOCKLANE0(1U)
+
+#define S_RXUFLOWLANE3    7
+#define V_RXUFLOWLANE3(x) ((x) << S_RXUFLOWLANE3)
+#define F_RXUFLOWLANE3    V_RXUFLOWLANE3(1U)
+
+#define S_RXUFLOWLANE2    6
+#define V_RXUFLOWLANE2(x) ((x) << S_RXUFLOWLANE2)
+#define F_RXUFLOWLANE2    V_RXUFLOWLANE2(1U)
+
+#define S_RXUFLOWLANE1    5
+#define V_RXUFLOWLANE1(x) ((x) << S_RXUFLOWLANE1)
+#define F_RXUFLOWLANE1    V_RXUFLOWLANE1(1U)
+
+#define S_RXUFLOWLANE0    4
+#define V_RXUFLOWLANE0(x) ((x) << S_RXUFLOWLANE0)
+#define F_RXUFLOWLANE0    V_RXUFLOWLANE0(1U)
+
+#define S_RXOFLOWLANE3    3
+#define V_RXOFLOWLANE3(x) ((x) << S_RXOFLOWLANE3)
+#define F_RXOFLOWLANE3    V_RXOFLOWLANE3(1U)
+
+#define S_RXOFLOWLANE2    2
+#define V_RXOFLOWLANE2(x) ((x) << S_RXOFLOWLANE2)
+#define F_RXOFLOWLANE2    V_RXOFLOWLANE2(1U)
+
+#define S_RXOFLOWLANE1    1
+#define V_RXOFLOWLANE1(x) ((x) << S_RXOFLOWLANE1)
+#define F_RXOFLOWLANE1    V_RXOFLOWLANE1(1U)
+
+#define S_RXOFLOWLANE0    0
+#define V_RXOFLOWLANE0(x) ((x) << S_RXOFLOWLANE0)
+#define F_RXOFLOWLANE0    V_RXOFLOWLANE0(1U)
+
+#define A_XGM_SERDES_STATUS2 0x994
+
+#define S_XGM_RXEIDLANE3    11
+#define V_XGM_RXEIDLANE3(x) ((x) << S_XGM_RXEIDLANE3)
+#define F_XGM_RXEIDLANE3    V_XGM_RXEIDLANE3(1U)
+
+#define S_XGM_RXEIDLANE2    10
+#define V_XGM_RXEIDLANE2(x) ((x) << S_XGM_RXEIDLANE2)
+#define F_XGM_RXEIDLANE2    V_XGM_RXEIDLANE2(1U)
+
+#define S_XGM_RXEIDLANE1    9
+#define V_XGM_RXEIDLANE1(x) ((x) << S_XGM_RXEIDLANE1)
+#define F_XGM_RXEIDLANE1    V_XGM_RXEIDLANE1(1U)
+
+#define S_XGM_RXEIDLANE0    8
+#define V_XGM_RXEIDLANE0(x) ((x) << S_XGM_RXEIDLANE0)
+#define F_XGM_RXEIDLANE0    V_XGM_RXEIDLANE0(1U)
+
+#define S_RXREMSKIPLANE3    7
+#define V_RXREMSKIPLANE3(x) ((x) << S_RXREMSKIPLANE3)
+#define F_RXREMSKIPLANE3    V_RXREMSKIPLANE3(1U)
+
+#define S_RXREMSKIPLANE2    6
+#define V_RXREMSKIPLANE2(x) ((x) << S_RXREMSKIPLANE2)
+#define F_RXREMSKIPLANE2    V_RXREMSKIPLANE2(1U)
+
+#define S_RXREMSKIPLANE1    5
+#define V_RXREMSKIPLANE1(x) ((x) << S_RXREMSKIPLANE1)
+#define F_RXREMSKIPLANE1    V_RXREMSKIPLANE1(1U)
+
+#define S_RXREMSKIPLANE0    4
+#define V_RXREMSKIPLANE0(x) ((x) << S_RXREMSKIPLANE0)
+#define F_RXREMSKIPLANE0    V_RXREMSKIPLANE0(1U)
+
+#define S_RXADDSKIPLANE3    3
+#define V_RXADDSKIPLANE3(x) ((x) << S_RXADDSKIPLANE3)
+#define F_RXADDSKIPLANE3    V_RXADDSKIPLANE3(1U)
+
+#define S_RXADDSKIPLANE2    2
+#define V_RXADDSKIPLANE2(x) ((x) << S_RXADDSKIPLANE2)
+#define F_RXADDSKIPLANE2    V_RXADDSKIPLANE2(1U)
+
+#define S_RXADDSKIPLANE1    1
+#define V_RXADDSKIPLANE1(x) ((x) << S_RXADDSKIPLANE1)
+#define F_RXADDSKIPLANE1    V_RXADDSKIPLANE1(1U)
+
+#define S_RXADDSKIPLANE0    0
+#define V_RXADDSKIPLANE0(x) ((x) << S_RXADDSKIPLANE0)
+#define F_RXADDSKIPLANE0    V_RXADDSKIPLANE0(1U)
+
+#define A_XGM_XAUI_PCS_ERR 0x998
+
+#define S_PCS_SYNCSTATUS    5
+#define M_PCS_SYNCSTATUS    0xf
+#define V_PCS_SYNCSTATUS(x) ((x) << S_PCS_SYNCSTATUS)
+#define G_PCS_SYNCSTATUS(x) (((x) >> S_PCS_SYNCSTATUS) & M_PCS_SYNCSTATUS)
+
+#define S_PCS_CTCFIFOERR    1
+#define M_PCS_CTCFIFOERR    0xf
+#define V_PCS_CTCFIFOERR(x) ((x) << S_PCS_CTCFIFOERR)
+#define G_PCS_CTCFIFOERR(x) (((x) >> S_PCS_CTCFIFOERR) & M_PCS_CTCFIFOERR)
+
+#define S_PCS_NOTALIGNED    0
+#define V_PCS_NOTALIGNED(x) ((x) << S_PCS_NOTALIGNED)
+#define F_PCS_NOTALIGNED    V_PCS_NOTALIGNED(1U)
+
+#define A_XGM_RGMII_STATUS 0x99c
+
+#define S_GMIIDUPLEX    3
+#define V_GMIIDUPLEX(x) ((x) << S_GMIIDUPLEX)
+#define F_GMIIDUPLEX    V_GMIIDUPLEX(1U)
+
+#define S_GMIISPEED    1
+#define M_GMIISPEED    0x3
+#define V_GMIISPEED(x) ((x) << S_GMIISPEED)
+#define G_GMIISPEED(x) (((x) >> S_GMIISPEED) & M_GMIISPEED)
+
+#define S_GMIILINKSTATUS    0
+#define V_GMIILINKSTATUS(x) ((x) << S_GMIILINKSTATUS)
+#define F_GMIILINKSTATUS    V_GMIILINKSTATUS(1U)
+
+#define A_XGM_WOL_STATUS 0x9a0
+
+#define S_PATDETECTED    31
+#define V_PATDETECTED(x) ((x) << S_PATDETECTED)
+#define F_PATDETECTED    V_PATDETECTED(1U)
+
+#define S_MATCHEDFILTER    0
+#define M_MATCHEDFILTER    0x7
+#define V_MATCHEDFILTER(x) ((x) << S_MATCHEDFILTER)
+#define G_MATCHEDFILTER(x) (((x) >> S_MATCHEDFILTER) & M_MATCHEDFILTER)
 
 #define A_XGM_RX_MAX_PKT_SIZE_ERR_CNT 0x9a4
-
 #define A_XGM_TX_SPI4_SOP_EOP_CNT 0x9a8
 
 #define S_TXSPI4SOPCNT    16
@@ -2573,6 +8325,22 @@
 #define V_TXSPI4SOPCNT(x) ((x) << S_TXSPI4SOPCNT)
 #define G_TXSPI4SOPCNT(x) (((x) >> S_TXSPI4SOPCNT) & M_TXSPI4SOPCNT)
 
+#define S_TXSPI4EOPCNT    0
+#define M_TXSPI4EOPCNT    0xffff
+#define V_TXSPI4EOPCNT(x) ((x) << S_TXSPI4EOPCNT)
+#define G_TXSPI4EOPCNT(x) (((x) >> S_TXSPI4EOPCNT) & M_TXSPI4EOPCNT)
+
 #define A_XGM_RX_SPI4_SOP_EOP_CNT 0x9ac
 
+#define S_RXSPI4SOPCNT    16
+#define M_RXSPI4SOPCNT    0xffff
+#define V_RXSPI4SOPCNT(x) ((x) << S_RXSPI4SOPCNT)
+#define G_RXSPI4SOPCNT(x) (((x) >> S_RXSPI4SOPCNT) & M_RXSPI4SOPCNT)
+
+#define S_RXSPI4EOPCNT    0
+#define M_RXSPI4EOPCNT    0xffff
+#define V_RXSPI4EOPCNT(x) ((x) << S_RXSPI4EOPCNT)
+#define G_RXSPI4EOPCNT(x) (((x) >> S_RXSPI4EOPCNT) & M_RXSPI4EOPCNT)
+
+/* registers for module XGMAC0_1 */
 #define XGMAC0_1_BASE_ADDR 0xa00
diff --git a/drivers/net/cxgb3/sge.c b/drivers/net/cxgb3/sge.c
--- a/drivers/net/cxgb3/sge.c
+++ b/drivers/net/cxgb3/sge.c
@@ -1,66 +1,69 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
+#include <linux/spinlock.h>
 #include <linux/skbuff.h>
 #include <linux/netdevice.h>
 #include <linux/etherdevice.h>
 #include <linux/if_vlan.h>
 #include <linux/ip.h>
 #include <linux/tcp.h>
+
+#include <net/tcp.h>
+
+#ifndef	LINUX_2_4
 #include <linux/dma-mapping.h>
 #include <net/arp.h>
+#endif
 #include "common.h"
 #include "regs.h"
 #include "sge_defs.h"
 #include "t3_cpl.h"
+#include "cxgb3_offload.h"
 #include "firmware_exports.h"
 
+#include "cxgb3_compat.h"
+
 #define USE_GTS 0
 
 #define SGE_RX_SM_BUF_SIZE 1536
-
 #define SGE_RX_COPY_THRES  256
 #define SGE_RX_PULL_LEN    128
 
 #define SGE_PG_RSVD SMP_CACHE_BYTES
+
 /*
  * Page chunk size for FL0 buffers if FL0 is to be populated with page chunks.
  * It must be a divisor of PAGE_SIZE.  If set to 0 FL0 will use sk_buffs
  * directly.
  */
-#define FL0_PG_CHUNK_SIZE  2048
+#if !defined(CONFIG_XEN)
+#define FL0_PG_CHUNK_SIZE 2048
+#else
+/* Use skbuffs for XEN kernels. LRO is already disabled */
+#define FL0_PG_CHUNK_SIZE 0
+#endif
 #define FL0_PG_ORDER 0
 #define FL0_PG_ALLOC_SIZE (PAGE_SIZE << FL0_PG_ORDER)
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,15) && !defined(CONFIG_XEN)
+#define FL_GFP_FLAGS __GFP_COMP
 #define FL1_PG_CHUNK_SIZE (PAGE_SIZE > 8192 ? 16384 : 8192)
 #define FL1_PG_ORDER (PAGE_SIZE > 8192 ? 0 : 1)
+#else
+#define FL1_PG_CHUNK_SIZE 0
+#define FL1_PG_ORDER 0
+#define FL_GFP_FLAGS 0
+#endif
+
 #define FL1_PG_ALLOC_SIZE (PAGE_SIZE << FL1_PG_ORDER)
 
 #define SGE_RX_DROP_THRES 16
@@ -70,6 +73,7 @@
  * Max number of Rx buffers we replenish at a time.
  */
 #define MAX_RX_REFILL 16U
+
 /*
  * Period of the Tx buffer reclaim timer.  This timer does not need to run
  * frequently as Tx buffers are usually reclaimed by new Tx packets.
@@ -88,12 +92,12 @@
 
 /* Values for sge_txq.flags */
 enum {
-	TXQ_RUNNING = 1 << 0,	/* fetch engine is running */
-	TXQ_LAST_PKT_DB = 1 << 1,	/* last packet rang the doorbell */
+	TXQ_RUNNING     = 1 << 0,  /* fetch engine is running */
+	TXQ_LAST_PKT_DB = 1 << 1,  /* last packet rang the doorbell */
 };
 
 struct tx_desc {
-	__be64 flit[TX_DESC_FLITS];
+	u64 flit[TX_DESC_FLITS];
 };
 
 struct rx_desc {
@@ -103,14 +107,30 @@
 	__be32 addr_hi;
 };
 
-struct tx_sw_desc {		/* SW state per Tx descriptor */
+/*
+ * A single WR can reference up to 7 wire packets when we coalesce egress
+ * packets. Instead of growing the shared tx sw desc we allocate a seperate
+ * coalesce sw descriptor queue. The generic tx sw desc indicates if the new
+ * software descriptor is valid or not.
+ */
+#define ETH_COALESCE_PKT_NUM 7
+#define ETH_COALESCE_DUMMY_SKB ((struct sk_buff*)1)
+
+enum { LAST_PKT_DESC = 1, PKT_COALESCE_WR = 2 };
+
+struct tx_sw_desc {                /* SW state per Tx descriptor */
 	struct sk_buff *skb;
-	u8 eop;       /* set if last descriptor for packet */
-	u8 addr_idx;  /* buffer index of first SGL entry in descriptor */
+	u8 eop_coalesce; /* 1 if last descriptor for pkt, 2 if coalesce wr */
+	u8 addr_idx_coalesce_num; /* buffer index of first SGL entry in
+				     descriptor, # of coalesced pkts */
 	u8 fragidx;   /* first page fragment associated with descriptor */
 	s8 sflit;     /* start flit of first SGL entry in descriptor */
 };
 
+struct eth_coalesce_sw_desc {      /* SW state for a Coalesce WR descriptor */
+	struct sk_buff *skb[ETH_COALESCE_PKT_NUM];
+};
+
 struct rx_sw_desc {                /* SW state per Rx descriptor */
 	union {
 		struct sk_buff *skb;
@@ -119,7 +139,7 @@
 	DECLARE_PCI_UNMAP_ADDR(dma_addr);
 };
 
-struct rsp_desc {		/* response queue descriptor */
+struct rsp_desc {                  /* response queue descriptor */
 	struct rss_header rss_hdr;
 	__be32 flags;
 	__be32 len_cq;
@@ -185,10 +205,9 @@
  *	Replenishes a response queue by making the supplied number of responses
  *	available to HW.
  */
-static inline void refill_rspq(struct adapter *adapter,
-			       const struct sge_rspq *q, unsigned int credits)
+static inline void refill_rspq(adapter_t *adapter, const struct sge_rspq *q,
+			       unsigned int credits)
 {
-	rmb();
 	t3_write_reg(adapter, A_SG_RSPQ_CREDIT_RETURN,
 		     V_RSPQ(q->cntxt_id) | V_CREDITS(credits));
 }
@@ -239,7 +258,7 @@
 {
 	const struct sg_ent *sgp;
 	struct tx_sw_desc *d = &q->sdesc[cidx];
-	int nfrags, frag_idx, curflit, j = d->addr_idx;
+	int nfrags, frag_idx, curflit, j = d->addr_idx_coalesce_num;
 
 	sgp = (struct sg_ent *)&q->desc[cidx].flit[d->sflit];
 	frag_idx = d->fragidx;
@@ -254,8 +273,18 @@
 	nfrags = skb_shinfo(skb)->nr_frags;
 
 	while (frag_idx < nfrags && curflit < WR_FLITS) {
-		pci_unmap_page(pdev, be64_to_cpu(sgp->addr[j]),
-			       skb_shinfo(skb)->frags[frag_idx].size,
+		/*
+		 * frag->size might be a 16 bit integer, which is a problem 
+		 * for 64K page size configurations. Assuming the current
+		 * page is valid, fix up a zeroed size to the page size.
+		 */
+		int size = skb_shinfo(skb)->frags[frag_idx].size;
+
+		if (PAGE_SIZE == 65536) 
+			if (!size)
+				size = PAGE_SIZE;
+
+		pci_unmap_page(pdev, be64_to_cpu(sgp->addr[j]), size,
 			       PCI_DMA_TODEVICE);
 		j ^= 1;
 		if (j == 0) {
@@ -269,11 +298,34 @@
 	if (frag_idx < nfrags) {   /* SGL continues into next Tx descriptor */
 		d = cidx + 1 == q->size ? q->sdesc : d + 1;
 		d->fragidx = frag_idx;
-		d->addr_idx = j;
+		d->addr_idx_coalesce_num = j;
 		d->sflit = curflit - WR_FLITS - j; /* sflit can be -1 */
 	}
 }
 
+static inline void unmap_tx_pkt_coalesce_wr(struct sge_txq *q,
+					    unsigned int cidx,
+					    unsigned int num,
+					    struct pci_dev *pdev)
+{
+	struct eth_coalesce_sw_desc *csd = &q->eth_coalesce_sdesc[cidx];
+	struct tx_pkt_coalesce_wr *wr =
+	    (struct tx_pkt_coalesce_wr *)&q->desc[cidx];
+	int i;
+
+	for (i = 0; i < num; i++) {
+		struct cpl_tx_pkt_coalesce *cpl = &wr->cpl[i];
+		unsigned int len = csd->skb[i]->len;
+
+		if (skb_headlen(csd->skb[i]))
+			pci_unmap_single(pdev, be64_to_cpu(cpl->addr),
+					 len, PCI_DMA_TODEVICE);
+		else
+			pci_unmap_page(pdev, be64_to_cpu(cpl->addr), len,
+				       PCI_DMA_TODEVICE);
+	}
+}
+
 /**
  *	free_tx_desc - reclaims Tx descriptors and their buffers
  *	@adapter: the adapter
@@ -283,23 +335,48 @@
  *	Reclaims Tx descriptors from an SGE Tx queue and frees the associated
  *	Tx buffers.  Called with the Tx queue lock held.
  */
-static void free_tx_desc(struct adapter *adapter, struct sge_txq *q,
-			 unsigned int n)
+static void free_tx_desc(adapter_t *adapter, struct sge_txq *q, unsigned int n)
 {
 	struct tx_sw_desc *d;
 	struct pci_dev *pdev = adapter->pdev;
-	unsigned int cidx = q->cidx;
+	unsigned int cidx = q->cidx, i;
 
 	const int need_unmap = need_skb_unmap() &&
 			       q->cntxt_id >= FW_TUNNEL_SGEEC_START;
 
+#ifdef T3_TRACE
+	T3_TRACE3(adapter->tb[q->cntxt_id & 7],
+		  "reclaiming %u Tx descriptors at cidx %u (used %u)", n,
+		  cidx, q->in_use - n);
+#endif
 	d = &q->sdesc[cidx];
 	while (n--) {
-		if (d->skb) {	/* an SGL is present */
-			if (need_unmap)
-				unmap_skb(d->skb, q, cidx, pdev);
-			if (d->eop)
-				kfree_skb(d->skb);
+		if (d->skb) {                       /* an SGL is present */
+			if (need_unmap) {
+				if (d->eop_coalesce == PKT_COALESCE_WR)
+					unmap_tx_pkt_coalesce_wr(q, cidx,
+					    d->addr_idx_coalesce_num, pdev);
+				else
+					unmap_skb(d->skb, q, cidx, pdev);
+			}
+
+			if (d->eop_coalesce == PKT_COALESCE_WR)
+				for (i = 0; i < d->addr_idx_coalesce_num; i++) {
+					struct eth_coalesce_sw_desc *csd =
+					    &q->eth_coalesce_sdesc[cidx];
+
+					/*
+					 * We can be called from interrupt and
+					 * TX buffers may have implicit
+					 * "destructor" code associated with
+					 * them to free up memory tied down in
+					 * virtual machines ...
+					 */
+					dev_kfree_skb_any(csd->skb[i]);
+				}
+			else if (d->eop_coalesce)
+				/* see above: can be called from interrupt */
+				dev_kfree_skb_any(d->skb);
 		}
 		++d;
 		if (++cidx == q->size) {
@@ -314,25 +391,23 @@
  *	reclaim_completed_tx - reclaims completed Tx descriptors
  *	@adapter: the adapter
  *	@q: the Tx queue to reclaim completed descriptors from
- *	@chunk: maximum number of descriptors to reclaim
  *
  *	Reclaims Tx descriptors that the SGE has indicated it has processed,
  *	and frees the associated buffers if possible.  Called with the Tx
  *	queue's lock held.
  */
-static inline unsigned int reclaim_completed_tx(struct adapter *adapter,
-						struct sge_txq *q,
-						unsigned int chunk)
+static inline unsigned int reclaim_completed_tx(adapter_t *adapter, struct sge_txq *q, unsigned int chunk)
 {
 	unsigned int reclaim = q->processed - q->cleaned;
 
 	reclaim = min(chunk, reclaim);
+
 	if (reclaim) {
 		free_tx_desc(adapter, q, reclaim);
 		q->cleaned += reclaim;
 		q->in_use -= reclaim;
 	}
-	return q->processed - q->cleaned;
+	return (q->processed - q->cleaned);
 }
 
 /**
@@ -349,7 +424,7 @@
 }
 
 static void clear_rx_desc(struct pci_dev *pdev, const struct sge_fl *q,
-			  struct rx_sw_desc *d)
+                          struct rx_sw_desc *d)
 {
 	if (q->use_pages && d->pg_chunk.page) {
 		(*d->pg_chunk.p_cnt)--;
@@ -362,8 +437,8 @@
 		d->pg_chunk.page = NULL;
 	} else {
 		pci_unmap_single(pdev, pci_unmap_addr(d, dma_addr),
-				 q->buf_size, PCI_DMA_FROMDEVICE);
-		kfree_skb(d->skb);
+ 				 q->buf_size, PCI_DMA_FROMDEVICE);
+				 kfree_skb(d->skb);
 		d->skb = NULL;
 	}
 }
@@ -371,7 +446,7 @@
 /**
  *	free_rx_bufs - free the Rx buffers on an SGE free list
  *	@pdev: the PCI device associated with the adapter
- *	@rxq: the SGE free list to clean up
+ *	@q: the SGE free list to clean up
  *
  *	Release the buffers on an SGE free-buffer Rx queue.  HW fetching from
  *	this queue should be stopped before calling this function.
@@ -383,7 +458,6 @@
 	while (q->credits--) {
 		struct rx_sw_desc *d = &q->sdesc[cidx];
 
-
 		clear_rx_desc(pdev, q, d);
 		if (++cidx == q->size)
 			cidx = 0;
@@ -397,7 +471,7 @@
 
 /**
  *	add_one_rx_buf - add a packet buffer to a free-buffer list
- *	@va:  buffer start VA
+ *	@va: buffer start VA
  *	@len: the buffer length
  *	@d: the HW Rx descriptor to write
  *	@sd: the SW Rx descriptor to write
@@ -408,19 +482,19 @@
  *	descriptors.
  */
 static inline int add_one_rx_buf(void *va, unsigned int len,
-				 struct rx_desc *d, struct rx_sw_desc *sd,
-				 unsigned int gen, struct pci_dev *pdev)
+				  struct rx_desc *d, struct rx_sw_desc *sd,
+				  unsigned int gen, struct pci_dev *pdev)
 {
 	dma_addr_t mapping;
 
 	mapping = pci_map_single(pdev, va, len, PCI_DMA_FROMDEVICE);
-	if (unlikely(pci_dma_mapping_error(pdev, mapping)))
+	if (unlikely(t3_pci_dma_mapping_error(pdev, mapping)))
 		return -ENOMEM;
 
 	pci_unmap_addr_set(sd, dma_addr, mapping);
 
 	d->addr_lo = cpu_to_be32(mapping);
-	d->addr_hi = cpu_to_be32((u64) mapping >> 32);
+	d->addr_hi = cpu_to_be32((u64)mapping >> 32);
 	wmb();
 	d->len_gen = cpu_to_be32(V_FLD_GEN1(gen));
 	d->gen2 = cpu_to_be32(V_FLD_GEN2(gen));
@@ -428,7 +502,7 @@
 }
 
 static inline int add_one_rx_chunk(dma_addr_t mapping, struct rx_desc *d,
-				   unsigned int gen)
+                                   unsigned int gen)
 {
 	d->addr_lo = cpu_to_be32(mapping);
 	d->addr_hi = cpu_to_be32((u64) mapping >> 32);
@@ -439,8 +513,8 @@
 }
 
 static int alloc_pg_chunk(struct adapter *adapter, struct sge_fl *q,
-			  struct rx_sw_desc *sd, gfp_t gfp,
-			  unsigned int order)
+                          struct rx_sw_desc *sd, gfp_t gfp,
+                          unsigned int order)
 {
 	if (!q->pg_chunk.page) {
 		dma_addr_t mapping;
@@ -450,7 +524,7 @@
 			return -ENOMEM;
 		q->pg_chunk.va = page_address(q->pg_chunk.page);
 		q->pg_chunk.p_cnt = q->pg_chunk.va + (PAGE_SIZE << order) -
-				    SGE_PG_RSVD;
+    				    SGE_PG_RSVD;
 		q->pg_chunk.offset = 0;
 		mapping = pci_map_page(adapter->pdev, q->pg_chunk.page,
 				       0, q->alloc_size, PCI_DMA_FROMDEVICE);
@@ -480,23 +554,23 @@
 {
 	if (q->pend_cred >= q->credits / 4) {
 		q->pend_cred = 0;
-		wmb();
 		t3_write_reg(adap, A_SG_KDOORBELL, V_EGRCNTX(q->cntxt_id));
 	}
 }
 
 /**
  *	refill_fl - refill an SGE free-buffer list
- *	@adapter: the adapter
+ *	@adap: the adapter
  *	@q: the free-list to refill
  *	@n: the number of new buffers to allocate
  *	@gfp: the gfp flags for allocating new buffers
  *
  *	(Re)populate an SGE free-buffer list with up to @n new packet buffers,
  *	allocated with the supplied gfp flags.  The caller must assure that
- *	@n does not exceed the queue's capacity.
+ *	@n does not exceed the queue's capacity. Returns the number of buffers
+ *	allocated.
  */
-static int refill_fl(struct adapter *adap, struct sge_fl *q, int n, gfp_t gfp)
+static unsigned int refill_fl(adapter_t *adap, struct sge_fl *q, int n, gfp_t gfp)
 {
 	struct rx_sw_desc *sd = &q->sdesc[q->pidx];
 	struct rx_desc *d = &q->desc[q->pidx];
@@ -523,19 +597,21 @@
 			void *buf_start;
 
 			struct sk_buff *skb = alloc_skb(q->buf_size, gfp);
+
 			if (!skb)
 				goto nomem;
 
 			sd->skb = skb;
 			buf_start = skb->data;
-			err = add_one_rx_buf(buf_start, q->buf_size, d, sd,
-					     q->gen, adap->pdev);
+			err  = add_one_rx_buf(buf_start, q->buf_size, d, sd,
+					      q->gen, adap->pdev);
 			if (unlikely(err)) {
 				clear_rx_desc(adap->pdev, q, sd);
 				break;
 			}
 		}
 
+
 		d++;
 		sd++;
 		if (++q->pidx == q->size) {
@@ -550,34 +626,31 @@
 	q->credits += count;
 	q->pend_cred += count;
 	ring_fl_db(adap, q);
-
 	return count;
 }
 
-static inline void __refill_fl(struct adapter *adap, struct sge_fl *fl)
+static inline void __refill_fl(adapter_t *adap, struct sge_fl *fl)
 {
-	refill_fl(adap, fl, min(MAX_RX_REFILL, fl->size - fl->credits),
-		  GFP_ATOMIC | __GFP_COMP);
+	refill_fl(adap, fl, min(MAX_RX_REFILL, fl->size - fl->credits), GFP_ATOMIC | FL_GFP_FLAGS);
 }
 
 /**
  *	recycle_rx_buf - recycle a receive buffer
- *	@adapter: the adapter
+ *	@adap: the adapter
  *	@q: the SGE free list
  *	@idx: index of buffer to recycle
  *
  *	Recycles the specified buffer on the given free list by adding it at
  *	the next available slot on the list.
  */
-static void recycle_rx_buf(struct adapter *adap, struct sge_fl *q,
-			   unsigned int idx)
+static void recycle_rx_buf(adapter_t *adap, struct sge_fl *q, unsigned int idx)
 {
 	struct rx_desc *from = &q->desc[idx];
-	struct rx_desc *to = &q->desc[q->pidx];
+	struct rx_desc *to   = &q->desc[q->pidx];
 
 	q->sdesc[q->pidx] = q->sdesc[idx];
-	to->addr_lo = from->addr_lo;	/* already big endian */
-	to->addr_hi = from->addr_hi;	/* likewise */
+	to->addr_lo = from->addr_lo;        // already big endian
+	to->addr_hi = from->addr_hi;        // likewise
 	wmb();
 	to->len_gen = cpu_to_be32(V_FLD_GEN1(q->gen));
 	to->gen2 = cpu_to_be32(V_FLD_GEN2(q->gen));
@@ -610,23 +683,41 @@
  *	of the SW ring.
  */
 static void *alloc_ring(struct pci_dev *pdev, size_t nelem, size_t elem_size,
-			size_t sw_size, dma_addr_t * phys, void *metadata)
+			size_t sw_size, dma_addr_t *phys, void *metadata)
 {
 	size_t len = nelem * elem_size;
 	void *s = NULL;
-	void *p = dma_alloc_coherent(&pdev->dev, len, phys, GFP_KERNEL);
+	void *p;
+
+	/*
+	 * On some systems we disable jumbo packets and nelem comes in as zero
+	 * ...
+	 */
+	if (nelem == 0)
+		return NULL;
+
+#ifndef LINUX_2_4
+	p = dma_alloc_coherent(&pdev->dev, len, phys, GFP_KERNEL);
+#else
+        p = pci_alloc_consistent(pdev, len, phys);
+#endif
 
 	if (!p)
 		return NULL;
-	if (sw_size && metadata) {
+	if (sw_size) {
 		s = kcalloc(nelem, sw_size, GFP_KERNEL);
 
 		if (!s) {
+#ifndef LINUX_2_4
 			dma_free_coherent(&pdev->dev, len, p, *phys);
+#else
+                        pci_free_consistent(pdev, len, p, *phys);
+#endif
 			return NULL;
 		}
+	}
+	if (metadata)
 		*(void **)metadata = s;
-	}
 	memset(p, 0, len);
 	return p;
 }
@@ -641,6 +732,7 @@
  */
 static void t3_reset_qset(struct sge_qset *q)
 {
+#if defined(NAPI_UPDATE)
 	if (q->adap &&
 	    !(q->adap->flags & NAPI_INIT)) {
 		memset(q, 0, sizeof(*q));
@@ -654,11 +746,11 @@
 	q->txq_stopped = 0;
 	q->tx_reclaim_timer.function = NULL; /* for t3_stop_sge_timers() */
 	q->rx_reclaim_timer.function = NULL;
-	q->nomem = 0;
-	napi_free_frags(&q->napi);
+#else
+	memset(q, 0, sizeof(*q));
+#endif
 }
 
-
 /**
  *	free_qset - free the resources of an SGE queue set
  *	@adapter: the adapter owning the queue set
@@ -668,46 +760,63 @@
  *	as HW contexts, packet buffers, and descriptor rings.  Traffic to the
  *	queue set must be quiesced prior to calling this.
  */
-static void t3_free_qset(struct adapter *adapter, struct sge_qset *q)
+void t3_free_qset(adapter_t *adapter, struct sge_qset *q)
 {
 	int i;
 	struct pci_dev *pdev = adapter->pdev;
 
+	if (q->tx_reclaim_timer.function)
+		del_timer_sync(&q->tx_reclaim_timer);
+        if (q->rx_reclaim_timer.function)
+                del_timer_sync(&q->rx_reclaim_timer);
+
 	for (i = 0; i < SGE_RXQ_PER_SET; ++i)
 		if (q->fl[i].desc) {
-			spin_lock_irq(&adapter->sge.reg_lock);
+			spin_lock(&adapter->sge.reg_lock);
 			t3_sge_disable_fl(adapter, q->fl[i].cntxt_id);
-			spin_unlock_irq(&adapter->sge.reg_lock);
+			spin_unlock(&adapter->sge.reg_lock);
 			free_rx_bufs(pdev, &q->fl[i]);
 			kfree(q->fl[i].sdesc);
+#ifndef LINUX_2_4
 			dma_free_coherent(&pdev->dev,
-					  q->fl[i].size *
-					  sizeof(struct rx_desc), q->fl[i].desc,
-					  q->fl[i].phys_addr);
+#else
+                        pci_free_consistent(pdev,
+#endif
+					q->fl[i].size * sizeof(struct rx_desc),
+					q->fl[i].desc, q->fl[i].phys_addr);
 		}
 
 	for (i = 0; i < SGE_TXQ_PER_SET; ++i)
 		if (q->txq[i].desc) {
-			spin_lock_irq(&adapter->sge.reg_lock);
+			spin_lock(&adapter->sge.reg_lock);
 			t3_sge_enable_ecntxt(adapter, q->txq[i].cntxt_id, 0);
-			spin_unlock_irq(&adapter->sge.reg_lock);
+			spin_unlock(&adapter->sge.reg_lock);
 			if (q->txq[i].sdesc) {
 				free_tx_desc(adapter, &q->txq[i],
 					     q->txq[i].in_use);
 				kfree(q->txq[i].sdesc);
 			}
+#ifndef LINUX_2_4
 			dma_free_coherent(&pdev->dev,
-					  q->txq[i].size *
-					  sizeof(struct tx_desc),
-					  q->txq[i].desc, q->txq[i].phys_addr);
+#else
+                        pci_free_consistent(pdev,
+#endif
+				q->txq[i].size * sizeof(struct tx_desc),
+				q->txq[i].desc, q->txq[i].phys_addr);
 			__skb_queue_purge(&q->txq[i].sendq);
 		}
 
+	kfree(q->txq[TXQ_ETH].eth_coalesce_sdesc);
+
 	if (q->rspq.desc) {
-		spin_lock_irq(&adapter->sge.reg_lock);
+		spin_lock(&adapter->sge.reg_lock);
 		t3_sge_disable_rspcntxt(adapter, q->rspq.cntxt_id);
-		spin_unlock_irq(&adapter->sge.reg_lock);
+		spin_unlock(&adapter->sge.reg_lock);
+#ifndef LINUX_2_4
 		dma_free_coherent(&pdev->dev,
+#else
+                pci_free_consistent(pdev,
+#endif
 				  q->rspq.size * sizeof(struct rsp_desc),
 				  q->rspq.desc, q->rspq.phys_addr);
 	}
@@ -743,7 +852,7 @@
  */
 static inline unsigned int sgl_len(unsigned int n)
 {
-	/* alternatively: 3 * (n / 2) + 2 * (n & 1) */
+	// alternatively: 3 * (n / 2) + 2 * (n & 1)
 	return (3 * n) / 2 + (n & 1);
 }
 
@@ -775,7 +884,7 @@
  *	threshold and the packet is too big to copy, or (b) the packet should
  *	be copied but there is no memory for the copy.
  */
-static struct sk_buff *get_packet(struct adapter *adap, struct sge_fl *fl,
+static struct sk_buff *get_packet(adapter_t *adap, struct sge_fl *fl,
 				  unsigned int len, unsigned int drop_thres)
 {
 	struct sk_buff *skb = NULL;
@@ -791,7 +900,7 @@
 			pci_dma_sync_single_for_cpu(adap->pdev,
 					    pci_unmap_addr(sd, dma_addr), len,
 					    PCI_DMA_FROMDEVICE);
-			memcpy(skb->data, sd->skb->data, len);
+			skb_copy_from_linear_data(sd->skb, skb->data, len);
 			pci_dma_sync_single_for_device(adap->pdev,
 					    pci_unmap_addr(sd, dma_addr), len,
 					    PCI_DMA_FROMDEVICE);
@@ -804,7 +913,7 @@
 
 	if (unlikely(fl->credits < drop_thres) &&
 	    refill_fl(adap, fl, min(MAX_RX_REFILL, fl->size - fl->credits - 1),
-		      GFP_ATOMIC | __GFP_COMP) == 0)
+		      GFP_ATOMIC | FL_GFP_FLAGS) == 0)
 		goto recycle;
 
 use_orig_buf:
@@ -834,9 +943,8 @@
  * 	Note: this function is similar to @get_packet but deals with Rx buffers
  * 	that are page chunks rather than sk_buffs.
  */
-static struct sk_buff *get_packet_pg(struct adapter *adap, struct sge_fl *fl,
-				     struct sge_rspq *q, unsigned int len,
-				     unsigned int drop_thres)
+static struct sk_buff *get_packet_pg(adapter_t *adap, struct sge_fl *fl, struct sge_rspq *q,
+				     unsigned int len, unsigned int drop_thres)
 {
 	struct sk_buff *newskb, *skb;
 	struct rx_sw_desc *sd = &fl->sdesc[fl->cidx];
@@ -844,6 +952,7 @@
 	dma_addr_t dma_addr = pci_unmap_addr(sd, dma_addr);
 
 	newskb = skb = q->pg_skb;
+
 	if (!skb && (len <= SGE_RX_COPY_THRES)) {
 		newskb = alloc_skb(len, GFP_ATOMIC);
 		if (likely(newskb != NULL)) {
@@ -851,9 +960,8 @@
 			pci_dma_sync_single_for_cpu(adap->pdev, dma_addr, len,
 					    PCI_DMA_FROMDEVICE);
 			memcpy(newskb->data, sd->pg_chunk.va, len);
-			pci_dma_sync_single_for_device(adap->pdev, dma_addr,
-						       len,
-						       PCI_DMA_FROMDEVICE);
+			pci_dma_sync_single_for_device(adap->pdev, dma_addr, len,
+					    PCI_DMA_FROMDEVICE);
 		} else if (!drop_thres)
 			return NULL;
 recycle:
@@ -863,7 +971,7 @@
 		return newskb;
 	}
 
-	if (unlikely(q->rx_recycle_buf || (!skb && fl->credits <= drop_thres)))
+	if (q->rx_recycle_buf || (!skb && unlikely(fl->credits <= drop_thres)))
 		goto recycle;
 
 	prefetch(sd->pg_chunk.p_cnt);
@@ -889,8 +997,8 @@
 		__skb_put(newskb, SGE_RX_PULL_LEN);
 		memcpy(newskb->data, sd->pg_chunk.va, SGE_RX_PULL_LEN);
 		skb_fill_page_desc(newskb, 0, sd->pg_chunk.page,
-				   sd->pg_chunk.offset + SGE_RX_PULL_LEN,
-				   len - SGE_RX_PULL_LEN);
+			   sd->pg_chunk.offset + SGE_RX_PULL_LEN,
+			   len - SGE_RX_PULL_LEN);
 		newskb->len = len;
 		newskb->data_len = len - SGE_RX_PULL_LEN;
 		newskb->truesize += newskb->data_len;
@@ -943,8 +1051,11 @@
 		return 1;
 
 	flits = sgl_len(skb_shinfo(skb)->nr_frags + 1) + 2;
+#ifndef NETIF_F_TSO_FAKE
+	/* TSO supported */
 	if (skb_shinfo(skb)->gso_size)
 		flits++;
+#endif
 	return flits_to_desc(flits);
 }
 
@@ -977,10 +1088,20 @@
 	nfrags = skb_shinfo(skb)->nr_frags;
 	for (i = 0; i < nfrags; i++) {
 		skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+		int size = frag->size;
+	
+		/*
+		 * frag->size might be a 16 bit integer, which is a problem 
+		 * for 64K page size configurations. Assuming the current
+		 * page is valid, fix up a zeroed size to the page size.
+		 */
+		if (PAGE_SIZE == 65536) 
+			if (!size)
+				size = PAGE_SIZE;
 
 		mapping = pci_map_page(pdev, frag->page, frag->page_offset,
-				       frag->size, PCI_DMA_TODEVICE);
-		sgp->len[j] = cpu_to_be32(frag->size);
+				       size, PCI_DMA_TODEVICE);
+		sgp->len[j] = cpu_to_be32(size);
 		sgp->addr[j] = cpu_to_be64(mapping);
 		j ^= 1;
 		if (j == 0)
@@ -1003,17 +1124,21 @@
  *
  *	When GTS is disabled we unconditionally ring the doorbell.
  */
-static inline void check_ring_tx_db(struct adapter *adap, struct sge_txq *q)
+static inline void check_ring_tx_db(adapter_t *adap, struct sge_txq *q)
 {
 #if USE_GTS
 	clear_bit(TXQ_LAST_PKT_DB, &q->flags);
 	if (test_and_set_bit(TXQ_RUNNING, &q->flags) == 0) {
 		set_bit(TXQ_LAST_PKT_DB, &q->flags);
+#ifdef T3_TRACE
+		T3_TRACE1(adap->tb[q->cntxt_id & 7], "doorbell Tx, cntxt %d",
+			  q->cntxt_id);
+#endif
 		t3_write_reg(adap, A_SG_KDOORBELL,
 			     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));
 	}
 #else
-	wmb();			/* write descriptors before telling HW */
+	wmb();            /* write descriptors before telling HW */
 	t3_write_reg(adap, A_SG_KDOORBELL,
 		     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));
 #endif
@@ -1050,8 +1175,8 @@
 			     const struct sge_txq *q,
 			     const struct sg_ent *sgl,
 			     unsigned int flits, unsigned int sgl_flits,
-			     unsigned int gen, __be32 wr_hi,
-			     __be32 wr_lo)
+			     unsigned int gen, unsigned int wr_hi,
+			     unsigned int wr_lo)
 {
 	struct work_request_hdr *wrp = (struct work_request_hdr *)d;
 	struct tx_sw_desc *sd = &q->sdesc[pidx];
@@ -1059,12 +1184,12 @@
 	sd->skb = skb;
 	if (need_skb_unmap()) {
 		sd->fragidx = 0;
-		sd->addr_idx = 0;
+		sd->addr_idx_coalesce_num = 0;
 		sd->sflit = flits;
 	}
 
 	if (likely(ndesc == 1)) {
-		sd->eop = 1;
+		sd->eop_coalesce = LAST_PKT_DESC;
 		wrp->wr_hi = htonl(F_WR_SOP | F_WR_EOP | V_WR_DATATYPE(1) |
 				   V_WR_SGLSFLT(flits)) | wr_hi;
 		wmb();
@@ -1092,7 +1217,7 @@
 
 			fp += avail;
 			d++;
-			sd->eop = 0;
+			sd->eop_coalesce = 0;
 			sd++;
 			if (++pidx == q->size) {
 				pidx = 0;
@@ -1111,7 +1236,7 @@
 			wr_gen2(d, gen);
 			flits = 1;
 		}
-		sd->eop = 1;
+		sd->eop_coalesce = LAST_PKT_DESC;
 		wrp->wr_hi |= htonl(F_WR_EOP);
 		wmb();
 		wp->wr_lo = htonl(V_WR_LEN(WR_FLITS) | V_WR_GEN(ogen)) | wr_lo;
@@ -1124,7 +1249,7 @@
  *	write_tx_pkt_wr - write a TX_PKT work request
  *	@adap: the adapter
  *	@skb: the packet to send
- *	@pi: the egress interface
+ *	@pi: the egress interface port structure
  *	@pidx: index of the first Tx descriptor to write
  *	@gen: the generation value to use
  *	@q: the Tx queue
@@ -1133,41 +1258,52 @@
  *
  *	Generate a TX_PKT work request to send the supplied packet.
  */
-static void write_tx_pkt_wr(struct adapter *adap, struct sk_buff *skb,
+static void write_tx_pkt_wr(adapter_t *adap, struct sk_buff *skb,
 			    const struct port_info *pi,
 			    unsigned int pidx, unsigned int gen,
 			    struct sge_txq *q, unsigned int ndesc,
 			    unsigned int compl)
 {
-	unsigned int flits, sgl_flits, cntrl, tso_info;
+	unsigned int flits, sgl_flits, cntrl, tso_info, obey_port;
 	struct sg_ent *sgp, sgl[MAX_SKB_FRAGS / 2 + 1];
 	struct tx_desc *d = &q->desc[pidx];
 	struct cpl_tx_pkt *cpl = (struct cpl_tx_pkt *)d;
 
-	cpl->len = htonl(skb->len);
-	cntrl = V_TXPKT_INTF(pi->port_id);
+	if (adap->params.chan_map == 3 && adap->port[pi->port_id]->master)
+		obey_port = 0x80000000;
+	else
+	       obey_port = 0;
+	cpl->len = htonl(skb->len | obey_port);
+
+	cntrl = V_TXPKT_INTF(pi->txpkt_intf);
 
 	if (vlan_tx_tag_present(skb) && pi->vlan_grp)
 		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(vlan_tx_tag_get(skb));
 
+#ifdef NETIF_F_TSO_FAKE
+	/* TSO not supported */
+	tso_info = 0;
+#else
+	/* TSO supported */
 	tso_info = V_LSO_MSS(skb_shinfo(skb)->gso_size);
+#endif
 	if (tso_info) {
 		int eth_type;
-		struct cpl_tx_pkt_lso *hdr = (struct cpl_tx_pkt_lso *)cpl;
+		struct cpl_tx_pkt_lso *hdr = (struct cpl_tx_pkt_lso *) cpl;
 
 		d->flit[2] = 0;
 		cntrl |= V_TXPKT_OPCODE(CPL_TX_PKT_LSO);
 		hdr->cntrl = htonl(cntrl);
 		eth_type = skb_network_offset(skb) == ETH_HLEN ?
-		    CPL_ETH_II : CPL_ETH_II_VLAN;
+			CPL_ETH_II : CPL_ETH_II_VLAN;
 		tso_info |= V_LSO_ETH_TYPE(eth_type) |
-		    V_LSO_IPHDR_WORDS(ip_hdr(skb)->ihl) |
-		    V_LSO_TCPHDR_WORDS(tcp_hdr(skb)->doff);
+			    V_LSO_IPHDR_WORDS(ip_hdr(skb)->ihl) |
+			    V_LSO_TCPHDR_WORDS(tcp_hdr(skb)->doff);
 		hdr->lso_info = htonl(tso_info);
 		flits = 3;
 	} else {
 		cntrl |= V_TXPKT_OPCODE(CPL_TX_PKT);
-		cntrl |= F_TXPKT_IPCSUM_DIS;	/* SW calculates IP csum */
+		cntrl |= F_TXPKT_IPCSUM_DIS;       /* SW calculates IP csum */
 		cntrl |= V_TXPKT_L4CSUM_DIS(skb->ip_summed != CHECKSUM_PARTIAL);
 		cpl->cntrl = htonl(cntrl);
 
@@ -1181,8 +1317,8 @@
 
 			flits = (skb->len + 7) / 8 + 2;
 			cpl->wr.wr_hi = htonl(V_WR_BCNTLFLT(skb->len & 7) |
-					      V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT)
-					      | F_WR_SOP | F_WR_EOP | compl);
+					  V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT) |
+					  F_WR_SOP | F_WR_EOP | compl);
 			wmb();
 			cpl->wr.wr_lo = htonl(V_WR_LEN(flits) | V_WR_GEN(gen) |
 					      V_WR_TID(q->token));
@@ -1195,32 +1331,169 @@
 	}
 
 	sgp = ndesc == 1 ? (struct sg_ent *)&d->flit[flits] : sgl;
-	sgl_flits = make_sgl(skb, sgp, skb->data, skb_headlen(skb), adap->pdev);
+	sgl_flits = make_sgl(skb, sgp, skb->data, skb_headlen(skb),
+		       	     adap->pdev);
 
 	write_wr_hdr_sgl(ndesc, skb, d, pidx, q, sgl, flits, sgl_flits, gen,
 			 htonl(V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT) | compl),
 			 htonl(V_WR_TID(q->token)));
 }
 
-static inline void t3_stop_tx_queue(struct netdev_queue *txq,
-				    struct sge_qset *qs, struct sge_txq *q)
+/**
+ *	finalize_tx_pkt_coalesce_wr - complete a tx pkt coalesce wr
+ *	@q: the Tx queue
+ */
+static inline void finalize_tx_pkt_coalesce_wr(struct sge_txq *q)
 {
-	netif_tx_stop_queue(txq);
-	set_bit(TXQ_ETH, &qs->txq_stopped);
-	q->stops++;
+
+	struct work_request_hdr *wrp =
+	    (struct work_request_hdr *)&q->desc[q->pidx];
+
+	wmb();
+	wrp->wr_lo =
+	    htonl(V_WR_GEN(q->gen) | V_WR_TID(q->token) |
+	          V_WR_LEN(1 + (q->eth_coalesce_idx << 1)));
+	wr_gen2((struct tx_desc *)wrp, q->gen);
 }
 
 /**
+ *	ship_tx_pkt_coalesce_wr - ship a tx pkt coalesce wr
+ *	@adap: the adapter
+ *	@q: the Tx queue
+ */
+static inline void ship_tx_pkt_coalesce_wr(adapter_t *adap, struct sge_txq *q)
+{
+	finalize_tx_pkt_coalesce_wr(q);
+	check_ring_tx_db(adap, q);
+
+	q->eth_coalesce_idx = 0;
+	q->eth_coalesce_bytes = 0;
+
+	q->pidx++;
+	if (q->pidx >= q->size) {
+		q->pidx -= q->size;
+		q->gen ^= 1;
+	}
+}
+
+/**
+ *	try_finalize_tx_pkt_coalesce_wr - try sending a pend. tx pkt coalesce wr
+ *	@adap: the adapter
+ *	@q: the Tx queue
+ */
+static void try_finalize_tx_pkt_coalesce_wr(adapter_t *adap, struct sge_txq *q)
+{
+	if (spin_trylock(&q->lock)) {
+
+		if (q->eth_coalesce_idx)
+			ship_tx_pkt_coalesce_wr(adap, q);
+
+		spin_unlock(&q->lock);
+	}
+}
+
+/**
+ *	should_finalize_tx_pkt_coalescing - is it time to stop coalescing
+ *	@q: the Tx queue
+ */
+static inline int should_finalize_tx_pkt_coalescing(const struct sge_txq *q)
+{
+	unsigned int r = q->processed - q->cleaned;
+
+	return q->in_use - r < (q->size >> 3);
+}
+
+/**
+ *	write_tx_pkt_coalesce_wr - write a TX_PKT coalesce work request
+ *	@adap: the adapter
+ *	@skb: the packet to send
+ *	@pi: the egress interface port structure
+ *	@pidx: index of the first Tx descriptor to write
+ *	@gen: the generation value to use
+ *	@q: the Tx queue
+ *	@compl: the value of the COMPL bit to use
+ *	@coalesce_idx: idx in the coalesce WR
+ *
+ *	Generate a TX_PKT work request to send the supplied packet.
+ */
+static inline void write_tx_pkt_coalesce_wr(adapter_t *adap,
+					    struct sk_buff *skb,
+					    const struct port_info *pi,
+					    unsigned int pidx,
+					    unsigned int gen,
+					    struct sge_txq *q,
+					    unsigned int compl,
+					    unsigned int coalesce_idx)
+{
+	struct tx_pkt_coalesce_wr *wr =
+	    (struct tx_pkt_coalesce_wr *)&q->desc[pidx];
+	struct cpl_tx_pkt_coalesce *cpl = &wr->cpl[coalesce_idx];
+	struct tx_sw_desc *sd = &q->sdesc[pidx];
+	unsigned int cntrl, len = skb->len;
+
+	if (!coalesce_idx) {
+		wr->wr.wr_hi =
+		    htonl(V_WR_OP(FW_WROPCODE_TUNNEL_TX_PKT) | F_WR_SOP | F_WR_EOP |
+		          V_WR_DATATYPE(1) | compl);
+		sd->eop_coalesce = PKT_COALESCE_WR;
+		sd->skb = ETH_COALESCE_DUMMY_SKB;
+	}
+	sd->addr_idx_coalesce_num = coalesce_idx + 1;
+	q->eth_coalesce_sdesc[pidx].skb[coalesce_idx] = skb;
+
+	cntrl =
+	    V_TXPKT_OPCODE(CPL_TX_PKT) | V_TXPKT_INTF(pi->txpkt_intf) |
+	    F_TXPKT_IPCSUM_DIS |
+	    V_TXPKT_L4CSUM_DIS(skb->ip_summed != CHECKSUM_PARTIAL);
+
+	if (vlan_tx_tag_present(skb) && pi->vlan_grp)
+		cntrl |= F_TXPKT_VLAN_VLD | V_TXPKT_VLAN(vlan_tx_tag_get(skb));
+
+	cpl->cntrl = htonl(cntrl);
+	cpl->len = htonl(len | 0x81000000);
+
+	if (skb_headlen(skb)) {
+		cpl->addr =
+		    cpu_to_be64(pci_map_single(adap->pdev, skb->data, len,
+				PCI_DMA_TODEVICE));
+	} else {
+		skb_frag_t *frag = skb_shinfo(skb)->frags;
+
+		cpl->addr =
+		    cpu_to_be64(pci_map_page(adap->pdev, frag->page,
+				frag->page_offset, len, PCI_DMA_TODEVICE));
+	}
+}
+
+#if !defined(MQ_TX)
+#define SELECT_TX_Q(skb, pi, qs, txq)			\
+	do {						\
+		qs = (pi)->qs;				\
+		txq = NULL;				\
+	} while (0)
+#else
+#define SELECT_TX_Q(skb, pi, qs, txq)			\
+	do {						\
+		int qidx = skb_get_queue_mapping(skb);	\
+							\
+		qs = &(pi)->qs[qidx];			\
+		txq = netdev_get_tx_queue(dev, qidx);	\
+	} while (0)
+#endif
+
+/**
  *	eth_xmit - add a packet to the Ethernet Tx queue
  *	@skb: the packet
  *	@dev: the egress net device
  *
  *	Add a packet to an SGE Tx queue.  Runs with softirqs disabled.
  */
-netdev_tx_t t3_eth_xmit(struct sk_buff *skb, struct net_device *dev)
+int t3_eth_xmit(struct sk_buff *skb, struct net_device *dev)
 {
-	int qidx;
-	unsigned int ndesc, pidx, credits, gen, compl;
+	
+	unsigned int ndesc, pidx, pidx_ndesc, credits, gen, compl,
+		     len = skb->len;
+	int coalesce_idx = -1;
 	const struct port_info *pi = netdev_priv(dev);
 	struct adapter *adap = pi->adapter;
 	struct netdev_queue *txq;
@@ -1231,59 +1504,140 @@
 	 * The chip min packet length is 9 octets but play safe and reject
 	 * anything shorter than an Ethernet header.
 	 */
-	if (unlikely(skb->len < ETH_HLEN)) {
+	if (unlikely(len < ETH_HLEN)) {
 		dev_kfree_skb(skb);
 		return NETDEV_TX_OK;
 	}
 
-	qidx = skb_get_queue_mapping(skb);
-	qs = &pi->qs[qidx];
+	SELECT_TX_Q(skb, pi, qs, txq);
 	q = &qs->txq[TXQ_ETH];
-	txq = netdev_get_tx_queue(dev, qidx);
-
-	reclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);
+	
+	if (spin_trylock(&q->lock))
+		reclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);
+	else
+		return NETDEV_TX_LOCKED;
 
 	credits = q->size - q->in_use;
-	ndesc = calc_tx_descs(skb);
+
+#ifdef T3_TRACE
+	T3_TRACE5(adap->tb[q->cntxt_id & 7],
+		  "t3_eth_xmit: len %u headlen %u frags %u idx %u bytes %u",
+		  len, skb_headlen(skb), skb_shinfo(skb)->nr_frags,
+		  q->eth_coalesce_idx, q->eth_coalesce_bytes);
+#endif
+	/* If the Tx descriptor ring is filling up we try to coalesce small
+	 * outgoing packets into a single WR. The coalesce WR format doesn't
+	 * handle fragmented skbs but that is unlikely anyway for small pkts.
+	 * The benefit of coalescing are manifold, including more efficiency
+	 * on the IO bus as well as more efficient processing in the T3
+	 * silicon.
+	 */
+	if ((skb_shinfo(skb)->nr_frags < 2) &&
+	    ((skb_shinfo(skb)->nr_frags == 1) ^ !!skb_headlen(skb)) &&
+	    ((q->eth_coalesce_idx || credits < (q->size >> 1))  &&
+	     (q->eth_coalesce_bytes + len < 11000))) {
+
+			q->eth_coalesce_bytes += len;
+			coalesce_idx = q->eth_coalesce_idx++;
+
+			if (!coalesce_idx) {
+				ndesc = 1;
+				qs->port_stats[SGE_PSTAT_TX_COALESCE_WR]++;
+			} else
+				ndesc = 0;
+
+			qs->port_stats[SGE_PSTAT_TX_COALESCE_PKT]++;
+			pidx_ndesc = 0;
+	} else {
+		if (q->eth_coalesce_idx)
+			ship_tx_pkt_coalesce_wr(adap, q);
+
+		ndesc = pidx_ndesc = calc_tx_descs(skb);
+	}
 
 	if (unlikely(credits < ndesc)) {
-		t3_stop_tx_queue(txq, qs, q);
-		dev_err(&adap->pdev->dev,
-			"%s: Tx ring %u full while queue awake!\n",
-			dev->name, q->cntxt_id & 7);
+		q->eth_coalesce_idx = 0;
+		q->eth_coalesce_bytes = 0;
+
+		if (!t3_netif_tx_queue_stopped(dev, txq)) {
+			t3_netif_tx_stop_queue(dev, txq);
+			set_bit(TXQ_ETH, &qs->txq_stopped);
+			q->stops++;
+			dev_err(&adap->pdev->dev,
+				"%s: Tx ring %u full while queue awake!\n",
+				dev->name, q->cntxt_id & 7);
+		}
+		spin_unlock(&q->lock);
 		return NETDEV_TX_BUSY;
 	}
 
 	q->in_use += ndesc;
 	if (unlikely(credits - ndesc < q->stop_thres)) {
-		t3_stop_tx_queue(txq, qs, q);
-
+		q->stops++;
+		t3_netif_tx_stop_queue(dev, txq);
+		set_bit(TXQ_ETH, &qs->txq_stopped);
+#if !USE_GTS
 		if (should_restart_tx(q) &&
 		    test_and_clear_bit(TXQ_ETH, &qs->txq_stopped)) {
 			q->restarts++;
-			netif_tx_wake_queue(txq);
+			t3_netif_tx_wake_queue(dev, txq);
 		}
+#endif
 	}
 
 	gen = q->gen;
 	q->unacked += ndesc;
-	compl = (q->unacked & 8) << (S_WR_COMPL - 3);
-	q->unacked &= 7;
+#ifdef CHELSIO_FREE_TXBUF_ASAP
+	/*
+	 * Some Guest OS clients get terrible performance when they have bad
+	 * message size / socket send buffer space parameters.  For instance,
+	 * if an application selects an 8KB message size and an 8KB send
+	 * socket buffer size.  This forces the application into a single
+	 * packet stop-and-go mode where it's only willing to have a single
+	 * message outstanding.  The next message is only sent when the
+	 * previous message is noted as having been sent.  Until we issue a
+	 * kfree_skb() against the TX skb, the skb is charged against the
+	 * application's send buffer space.  We only free up TX skbs when we
+	 * get a TX credit return from the hardware / firmware which is fairly
+	 * lazy about this.  So we request a TX WR Completion Notification on
+	 * every TX descriptor in order to accellerate TX credit returns.  See
+	 * also the change in handle_rsp_cntrl_info() to free up TX skb's when
+	 * we receive the TX WR Completion Notifications ...
+	 */
+	compl = F_WR_COMPL;
+#else
+	compl = (q->unacked & 32) << (S_WR_COMPL - 5);
+#endif
+	q->unacked &= 31;
+
 	pidx = q->pidx;
-	q->pidx += ndesc;
+	q->pidx += pidx_ndesc;
 	if (q->pidx >= q->size) {
 		q->pidx -= q->size;
 		q->gen ^= 1;
 	}
 
+#ifdef T3_TRACE
+//	T3_TRACE5(adap->tb[q->cntxt_id & 7],
+//		  "eth_xmit: ndesc %u, credits %u, pidx %u, len %u, frags %u",
+//		  ndesc, credits, pidx, skb->len, skb_shinfo(skb)->nr_frags);
+#endif
 	/* update port statistics */
-	if (skb->ip_summed == CHECKSUM_COMPLETE)
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
 		qs->port_stats[SGE_PSTAT_TX_CSUM]++;
+#ifndef NETIF_F_TSO_FAKE
+	/* TSO supported */
 	if (skb_shinfo(skb)->gso_size)
 		qs->port_stats[SGE_PSTAT_TSO]++;
+#endif
 	if (vlan_tx_tag_present(skb) && pi->vlan_grp)
 		qs->port_stats[SGE_PSTAT_VLANINS]++;
 
+	dev->trans_start = jiffies;
+
+	if (coalesce_idx < 0)
+		spin_unlock(&q->lock);
+
 	/*
 	 * We do not use Tx completion interrupts to free DMAd Tx packets.
 	 * This is good for performamce but means that we rely on new Tx
@@ -1310,9 +1664,20 @@
 	 */
 	if (likely(!skb_shared(skb)))
 		skb_orphan(skb);
-
-	write_tx_pkt_wr(adap, skb, pi, pidx, gen, q, ndesc, compl);
-	check_ring_tx_db(adap, q);
+	if (coalesce_idx >= 0) {
+		write_tx_pkt_coalesce_wr(adap, skb, pi, pidx, gen, q,
+					 compl, coalesce_idx);
+
+		if (coalesce_idx == ETH_COALESCE_PKT_NUM - 1)
+			ship_tx_pkt_coalesce_wr(adap, q);
+
+		spin_unlock(&q->lock);
+	} else {
+		write_tx_pkt_wr(adap, skb, pi, pidx, gen, q, ndesc, compl);
+		check_ring_tx_db(adap, q);
+	}
+	q->tx_pkts++;
+
 	return NETDEV_TX_OK;
 }
 
@@ -1366,12 +1731,12 @@
  *	needs to retry because there weren't enough descriptors at the
  *	beginning of the call but some freed up in the mean time.
  */
-static inline int check_desc_avail(struct adapter *adap, struct sge_txq *q,
+static inline int check_desc_avail(adapter_t *adap, struct sge_txq *q,
 				   struct sk_buff *skb, unsigned int ndesc,
 				   unsigned int qid)
 {
 	if (unlikely(!skb_queue_empty(&q->sendq))) {
-	      addq_exit:__skb_queue_tail(&q->sendq, skb);
+addq_exit:	__skb_queue_tail(&q->sendq, skb);
 		return 1;
 	}
 	if (unlikely(q->size - q->in_use < ndesc)) {
@@ -1406,6 +1771,13 @@
 	q->cleaned += reclaim;
 }
 
+/**
+ *	immediate - check whether a packet can be sent as immediate data
+ *	@skb: the packet
+ *
+ *	Returns true if a packet can be sent as a WR with immediate data.
+ *	Currently this happens if the packet fits in one Tx descriptor.
+ */
 static inline int immediate(const struct sk_buff *skb)
 {
 	return skb->len <= WR_LEN;
@@ -1421,8 +1793,7 @@
  *	a control queue must fit entirely as immediate data in a single Tx
  *	descriptor and have no page fragments.
  */
-static int ctrl_xmit(struct adapter *adap, struct sge_txq *q,
-		     struct sk_buff *skb)
+static int ctrl_xmit(adapter_t *adap, struct sge_txq *q, struct sk_buff *skb)
 {
 	int ret;
 	struct work_request_hdr *wrp = (struct work_request_hdr *)skb->data;
@@ -1437,7 +1808,7 @@
 	wrp->wr_lo = htonl(V_WR_TID(q->token));
 
 	spin_lock(&q->lock);
-      again:reclaim_completed_tx_imm(q);
+again:	reclaim_completed_tx_imm(q);
 
 	ret = check_desc_avail(adap, q, skb, 1, TXQ_CTRL);
 	if (unlikely(ret)) {
@@ -1464,7 +1835,7 @@
 
 /**
  *	restart_ctrlq - restart a suspended control queue
- *	@qs: the queue set cotaining the control queue
+ *	@data: the queue set cotaining the control queue
  *
  *	Resumes transmission on a suspended Tx control queue.
  */
@@ -1475,7 +1846,7 @@
 	struct sge_txq *q = &qs->txq[TXQ_CTRL];
 
 	spin_lock(&q->lock);
-      again:reclaim_completed_tx_imm(q);
+again:	reclaim_completed_tx_imm(q);
 
 	while (q->in_use < q->size &&
 	       (skb = __skb_dequeue(&q->sendq)) != NULL) {
@@ -1505,16 +1876,20 @@
 		     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));
 }
 
-/*
- * Send a management message through control queue 0
+/**
+ *	t3_mgmt_tx - send a management message
+ *	@adap: the adapter
+ *	@skb: the packet containing the management message
+ *
+ *	Send a management message through control queue 0.
  */
 int t3_mgmt_tx(struct adapter *adap, struct sk_buff *skb)
 {
 	int ret;
+
 	local_bh_disable();
 	ret = ctrl_xmit(adap, &adap->sge.qs[0].txq[TXQ_CTRL], skb);
 	local_bh_enable();
-
 	return ret;
 }
 
@@ -1542,9 +1917,21 @@
 				 PCI_DMA_TODEVICE);
 
 	si = skb_shinfo(skb);
-	for (i = 0; i < si->nr_frags; i++)
-		pci_unmap_page(dui->pdev, *p++, si->frags[i].size,
+	for (i = 0; i < si->nr_frags; i++) {
+		/*
+		 * frag->size might be a 16 bit integer, which is a problem 
+		 * for 64K page size configurations. Assuming the current
+		 * page is valid, fix up a zeroed size to the page size.
+		 */
+		int size = si->frags[i].size;
+
+		if (PAGE_SIZE == 65536) 
+			if (!size)
+				size = PAGE_SIZE;
+
+		pci_unmap_page(dui->pdev, *p++, size,
 			       PCI_DMA_TODEVICE);
+	}
 }
 
 static void setup_deferred_unmapping(struct sk_buff *skb, struct pci_dev *pdev,
@@ -1575,7 +1962,7 @@
  *	Write an offload work request to send the supplied packet.  The packet
  *	data already carry the work request with most fields populated.
  */
-static void write_ofld_wr(struct adapter *adap, struct sk_buff *skb,
+static void write_ofld_wr(adapter_t *adap, struct sk_buff *skb,
 			  struct sge_txq *q, unsigned int pidx,
 			  unsigned int gen, unsigned int ndesc)
 {
@@ -1600,14 +1987,14 @@
 	sgp = ndesc == 1 ? (struct sg_ent *)&d->flit[flits] : sgl;
 	sgl_flits = make_sgl(skb, sgp, skb_transport_header(skb),
 			     skb->tail - skb->transport_header,
-			     adap->pdev);
+		       	     adap->pdev);
 	if (need_skb_unmap()) {
 		setup_deferred_unmapping(skb, adap->pdev, sgp, sgl_flits);
 		skb->destructor = deferred_unmap_destructor;
 	}
-
 	write_wr_hdr_sgl(ndesc, skb, d, pidx, q, sgl, flits, sgl_flits,
 			 gen, from->wr_hi, from->wr_lo);
+	q->tx_pkts++;
 }
 
 /**
@@ -1622,9 +2009,9 @@
 	unsigned int flits, cnt;
 
 	if (skb->len <= WR_LEN)
-		return 1;	/* packet fits as immediate data */
-
-	flits = skb_transport_offset(skb) / 8;	/* headers */
+		return 1;                 /* packet fits as immediate data */
+
+	flits = skb_transport_offset(skb) / 8;   /* headers */
 	cnt = skb_shinfo(skb)->nr_frags;
 	if (skb->tail != skb->transport_header)
 		cnt++;
@@ -1639,8 +2026,7 @@
  *
  *	Send an offload packet through an SGE offload queue.
  */
-static int ofld_xmit(struct adapter *adap, struct sge_txq *q,
-		     struct sk_buff *skb)
+static int ofld_xmit(adapter_t *adap, struct sge_txq *q, struct sk_buff *skb)
 {
 	int ret;
 	unsigned int ndesc = calc_tx_descs_ofld(skb), pidx, gen;
@@ -1651,7 +2037,7 @@
 	ret = check_desc_avail(adap, q, skb, ndesc, TXQ_OFLD);
 	if (unlikely(ret)) {
 		if (ret == 1) {
-			skb->priority = ndesc;	/* save for restart */
+			skb->priority = ndesc;     /* save for restart */
 			spin_unlock(&q->lock);
 			return NET_XMIT_CN;
 		}
@@ -1666,6 +2052,12 @@
 		q->pidx -= q->size;
 		q->gen ^= 1;
 	}
+#ifdef T3_TRACE
+	T3_TRACE5(adap->tb[q->cntxt_id & 7],
+		  "ofld_xmit: ndesc %u, pidx %u, len %u, main %u, frags %u",
+		  ndesc, pidx, skb->len, skb->len - skb->data_len,
+		  skb_shinfo(skb)->nr_frags);
+#endif
 	spin_unlock(&q->lock);
 
 	write_ofld_wr(adap, skb, q, pidx, gen, ndesc);
@@ -1675,7 +2067,7 @@
 
 /**
  *	restart_offloadq - restart a suspended offload queue
- *	@qs: the queue set cotaining the offload queue
+ *	@data: the queue set cotaining the offload queue
  *
  *	Resumes transmission on a suspended Tx offload queue.
  */
@@ -1684,11 +2076,9 @@
 	struct sk_buff *skb;
 	struct sge_qset *qs = (struct sge_qset *)data;
 	struct sge_txq *q = &qs->txq[TXQ_OFLD];
-	const struct port_info *pi = netdev_priv(qs->netdev);
-	struct adapter *adap = pi->adapter;
 
 	spin_lock(&q->lock);
-again:	reclaim_completed_tx(adap, q, TX_RECLAIM_CHUNK);
+again:	reclaim_completed_tx(qs->adap, q, TX_RECLAIM_CHUNK);
 
 	while ((skb = skb_peek(&q->sendq)) != NULL) {
 		unsigned int gen, pidx;
@@ -1716,7 +2106,7 @@
 		__skb_unlink(skb, &q->sendq);
 		spin_unlock(&q->lock);
 
-		write_ofld_wr(adap, skb, q, pidx, gen, ndesc);
+		write_ofld_wr(qs->adap, skb, q, pidx, gen, ndesc);
 		spin_lock(&q->lock);
 	}
 	spin_unlock(&q->lock);
@@ -1726,7 +2116,7 @@
 	set_bit(TXQ_LAST_PKT_DB, &q->flags);
 #endif
 	wmb();
-	t3_write_reg(adap, A_SG_KDOORBELL,
+	t3_write_reg(qs->adap, A_SG_KDOORBELL,
 		     F_SELEGRCNTX | V_EGRCNTX(q->cntxt_id));
 }
 
@@ -1765,7 +2155,7 @@
  */
 int t3_offload_tx(struct t3cdev *tdev, struct sk_buff *skb)
 {
-	struct adapter *adap = tdev2adap(tdev);
+	adapter_t *adap = tdev2adap(tdev);
 	struct sge_qset *qs = &adap->sge.qs[queue_set(skb)];
 
 	if (unlikely(is_ctrl_pkt(skb)))
@@ -1785,15 +2175,20 @@
  */
 static inline void offload_enqueue(struct sge_rspq *q, struct sk_buff *skb)
 {
-	int was_empty = skb_queue_empty(&q->rx_queue);
-
-	__skb_queue_tail(&q->rx_queue, skb);
-
-	if (was_empty) {
+	skb->next = skb->prev = NULL;
+	if (q->rx_tail)
+		q->rx_tail->next = skb;
+	else {
 		struct sge_qset *qs = rspq_to_qset(q);
-
+#if defined(NAPI_UPDATE)
 		napi_schedule(&qs->napi);
+#else
+		if (__netif_rx_schedule_prep(qs->netdev))
+			__netif_rx_schedule(qs->netdev);
+#endif
+		q->rx_head = skb;
 	}
+	q->rx_tail = skb;
 }
 
 /**
@@ -1811,7 +2206,7 @@
 {
 	if (n) {
 		q->offload_bundles++;
-		tdev->recv(tdev, skbs, n);
+		cxgb3_ofld_recv(tdev, skbs, n);
 	}
 }
 
@@ -1826,54 +2221,78 @@
  *	receive handler.  Batches need to be of modest size as we do prefetches
  *	on the packets in each.
  */
-static int ofld_poll(struct napi_struct *napi, int budget)
+DECLARE_OFLD_POLL(napi, dev, budget)
 {
-	struct sge_qset *qs = container_of(napi, struct sge_qset, napi);
+	struct sge_qset *qs = SGE_GET_OFLD_QS(napi, dev);
 	struct sge_rspq *q = &qs->rspq;
 	struct adapter *adapter = qs->adap;
-	int work_done = 0;
-
-	while (work_done < budget) {
-		struct sk_buff *skb, *tmp, *skbs[RX_BUNDLE_SIZE];
-		struct sk_buff_head queue;
+
+#if defined(NAPI_UPDATE)
+	int limit = budget;
+#else
+	int limit = min(*budget, dev->quota);
+#endif
+	int work_done, avail = limit;
+
+	while (avail) {
+		struct sk_buff *head, *tail, *skbs[RX_BUNDLE_SIZE];
 		int ngathered;
-
-		spin_lock_irq(&q->lock);
-		__skb_queue_head_init(&queue);
-		skb_queue_splice_init(&q->rx_queue, &queue);
-		if (skb_queue_empty(&queue)) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&q->lock, flags);
+		head = q->rx_head;
+		if (!head) {
+			work_done = limit - avail;
+#if defined(NAPI_UPDATE)
 			napi_complete(napi);
-			spin_unlock_irq(&q->lock);
+#else
+			*budget -= work_done;
+			dev->quota -= work_done;
+			__netif_rx_complete(dev);
+#endif
+			spin_unlock_irqrestore(&q->lock, flags);
+#if defined(NAPI_UPDATE)
 			return work_done;
+#else
+			return 0;
+#endif
 		}
-		spin_unlock_irq(&q->lock);
-
-		ngathered = 0;
-		skb_queue_walk_safe(&queue, skb, tmp) {
-			if (work_done >= budget)
-				break;
-			work_done++;
-
-			__skb_unlink(skb, &queue);
-			prefetch(skb->data);
-			skbs[ngathered] = skb;
+
+		tail = q->rx_tail;
+		q->rx_head = q->rx_tail = NULL;
+		spin_unlock_irqrestore(&q->lock, flags);
+
+		for (ngathered = 0; avail && head; avail--) {
+			prefetch(head->data);
+			skbs[ngathered] = head;
+			head = head->next;
+			skbs[ngathered]->next = NULL;
 			if (++ngathered == RX_BUNDLE_SIZE) {
 				q->offload_bundles++;
-				adapter->tdev.recv(&adapter->tdev, skbs,
-						   ngathered);
+				cxgb3_ofld_recv(&adapter->tdev, skbs,
+						ngathered);
 				ngathered = 0;
 			}
 		}
-		if (!skb_queue_empty(&queue)) {
-			/* splice remaining packets back onto Rx queue */
-			spin_lock_irq(&q->lock);
-			skb_queue_splice(&queue, &q->rx_queue);
-			spin_unlock_irq(&q->lock);
+		if (head) {  /* splice remaining packets back onto Rx queue */
+			spin_lock_irqsave(&q->lock, flags);
+			tail->next = q->rx_head;
+			if (!q->rx_head)
+				q->rx_tail = tail;
+			q->rx_head = head;
+			spin_unlock_irqrestore(&q->lock, flags);
 		}
 		deliver_partial_bundle(&adapter->tdev, q, skbs, ngathered);
 	}
 
+	work_done = limit - avail;
+#if defined(NAPI_UPDATE)
 	return work_done;
+#else
+	*budget -= work_done;
+	dev->quota -= work_done;
+	return 1;
+#endif
 }
 
 /**
@@ -1895,10 +2314,10 @@
 	skb_reset_network_header(skb);
 	skb_reset_transport_header(skb);
 
-	if (rq->polling) {
+	if (rq->flags & USING_POLLING) {
 		rx_gather[gather_idx++] = skb;
 		if (gather_idx == RX_BUNDLE_SIZE) {
-			tdev->recv(tdev, rx_gather, RX_BUNDLE_SIZE);
+			cxgb3_ofld_recv(tdev, rx_gather, RX_BUNDLE_SIZE);
 			gather_idx = 0;
 			rq->offload_bundles++;
 		}
@@ -1922,7 +2341,7 @@
 	    test_and_clear_bit(TXQ_ETH, &qs->txq_stopped)) {
 		qs->txq[TXQ_ETH].restarts++;
 		if (netif_running(qs->netdev))
-			netif_tx_wake_queue(qs->tx_q);
+			t3_netif_tx_wake_queue(qs->netdev, qs->tx_q);
 	}
 
 	if (test_bit(TXQ_OFLD, &qs->txq_stopped) &&
@@ -1993,59 +2412,61 @@
  *	@rq: the response queue that received the packet
  *	@skb: the packet
  *	@pad: amount of padding at the start of the buffer
+ *	@npkts: number of packets aggregated in the skb (>= 1 for LRO)
  *
  *	Process an ingress ethernet pakcet and deliver it to the stack.
  *	The padding is 2 if the packet was delivered in an Rx buffer and 0
- *	if it was immediate data in a response.
+ *	if it was immediate data in a response. @npkts represents the number
+ *	of Ethernet packets as seen by the device that have been collected in
+ *	the @skb; it's > 1 only in the case of LRO.
  */
-static void rx_eth(struct adapter *adap, struct sge_rspq *rq,
-		   struct sk_buff *skb, int pad, int lro)
+static void rx_eth(adapter_t *adap, struct sge_rspq *rq,
+		   struct sk_buff *skb, int pad, int npkts)
 {
 	struct cpl_rx_pkt *p = (struct cpl_rx_pkt *)(skb->data + pad);
 	struct sge_qset *qs = rspq_to_qset(rq);
 	struct port_info *pi;
 
+	struct ethhdr *ethhdr = (struct ethhdr *)(p + 1);
+
+	if (unlikely(ethhdr->h_proto == htons(ETH_P_LOOP)))
+		printk("%s: received a loopback packet\n", __func__);
+
+	rq->eth_pkts += npkts;
 	skb_pull(skb, sizeof(*p) + pad);
-	skb->protocol = eth_type_trans(skb, adap->port[p->iff]);
+	skb->dev = adap->port[adap->rxpkt_map[p->iff]];
+	skb->dev->last_rx = jiffies;
+	skb->protocol = eth_type_trans(skb, skb->dev);
 	pi = netdev_priv(skb->dev);
-	if ((pi->rx_offload & T3_RX_CSUM) && p->csum_valid &&
-	    p->csum == htons(0xffff) && !p->fragment) {
-		qs->port_stats[SGE_PSTAT_RX_CSUM_GOOD]++;
+
+	if (pi->rx_csum_offload && p->csum_valid && p->csum == 0xffff &&
+	    !p->fragment) {
+		qs->port_stats[SGE_PSTAT_RX_CSUM_GOOD] += npkts;
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 	} else
 		skb->ip_summed = CHECKSUM_NONE;
-	skb_record_rx_queue(skb, qs - &adap->sge.qs[0]);
+	skb_record_rx_queue(skb, qs - &adap->sge.qs[pi->first_qset]);
 
 	if (unlikely(p->vlan_valid)) {
 		struct vlan_group *grp = pi->vlan_grp;
 
-		qs->port_stats[SGE_PSTAT_VLANEX]++;
-		if (likely(grp))
-			if (lro)
-				vlan_gro_receive(&qs->napi, grp,
-						 ntohs(p->vlan), skb);
-			else {
-				if (unlikely(pi->iscsi_ipv4addr &&
-				    is_arp(skb))) {
-					unsigned short vtag = ntohs(p->vlan) &
-								VLAN_VID_MASK;
-					skb->dev = vlan_group_get_device(grp,
-									 vtag);
-					cxgb3_arp_process(adap, skb);
-				}
-				__vlan_hwaccel_rx(skb, grp, ntohs(p->vlan),
-					  	  rq->polling);
+		qs->port_stats[SGE_PSTAT_VLANEX] += npkts;
+		if (likely(grp != NULL)) {
+			if (unlikely(pi->iscsi_ipv4addr && is_arp(skb))) {
+				unsigned short vtag = ntohs(p->vlan) &
+						      VLAN_VID_MASK;
+				skb->dev = vlan_group_get_device(grp,
+								 vtag);
+				cxgb3_arp_process(adap, skb);
 			}
-		else
+			__vlan_hwaccel_rx(skb, grp, ntohs(p->vlan),
+					  rq->flags & USING_POLLING);
+		} else
 			dev_kfree_skb_any(skb);
-	} else if (rq->polling) {
-		if (lro)
-			napi_gro_receive(&qs->napi, skb);
-		else {
-			if (unlikely(pi->iscsi_ipv4addr && is_arp(skb)))
-				cxgb3_arp_process(adap, skb);
-			netif_receive_skb(skb);
-		}
+	} else if (rq->flags & USING_POLLING) {
+		if (unlikely(pi->iscsi_ipv4addr && is_arp(skb)))
+			cxgb3_arp_process(adap, skb);
+		netif_receive_skb(skb);
 	} else
 		netif_rx(skb);
 }
@@ -2055,96 +2476,443 @@
 	return G_HASHTYPE(ntohl(rss)) == RSS_HASH_4_TUPLE;
 }
 
+static inline int lro_active(const struct lro_session *s)
+{
+	return s->head != NULL;
+}
+
+/**
+ *	lro_match - check if a new packet matches an existing LRO packet
+ *	@skb: LRO packet
+ *	@iph: pointer to IP header of new packet
+ *
+ *	Determine whether a new packet with the given IP header belongs
+ *	to the same connection as an existing LRO packet by checking that the
+ *	two packets have the same 4-tuple.  Note that LRO assumes no IP options.
+ */
+static inline int lro_match(const struct sk_buff *skb, const struct iphdr *iph)
+{
+	const struct iphdr *s_iph = ip_hdr(skb);
+	const struct tcphdr *s_tcph = (const struct tcphdr *)(s_iph + 1);
+	const struct tcphdr *tcph = (const struct tcphdr *)(iph + 1);
+
+	return *(u32 *)&tcph->source == *(u32 *)&s_tcph->source &&
+	       iph->saddr == s_iph->saddr && iph->daddr == s_iph->daddr;
+}
+
+/**
+ *	lro_lookup - find an LRO session
+ *	@p: the LRO state
+ *	@idx: index of first session to try
+ *	@iph: IP header supplying the session information to look up
+ *
+ *	Return an exitsing LRO session that matches the TCP/IP information in
+ *	the supplied IP header.  @idx is a hint suggesting the first session
+ *	to try.  If no matching session is found %NULL is returned.
+ */
+static struct lro_session *lro_lookup(struct lro_state *p, int idx,
+				      const struct iphdr *iph)
+{
+	struct lro_session *s = NULL;
+	unsigned int active = p->nactive;
+
+	while (active) {
+		s = &p->sess[idx];
+		if (s->head) {
+			if (lro_match(s->head, iph))
+				break;
+			active--;
+		}
+		idx = (idx + 1) & (MAX_LRO_SES - 1);
+	}
+	return s;
+}
+
+#define IPH_OFFSET (2 + ETH_HLEN + sizeof(struct cpl_rx_pkt))
+
+/**
+ *	lro_init_session - initialize an LRO session
+ *	@s: LRO session to initialize
+ *	@skb: first packet for the session
+ *	@iph: pointer to start of IP header
+ *	@vlan: session vlan
+ *	@plen: TCP payload length
+ *
+ *	Initialize an LRO session with the given packet.
+ */
+static void lro_init_session(struct lro_session *s, struct sk_buff *skb,
+			     struct iphdr *iph, __be32 vlan, int plen)
+{
+	const struct tcphdr *tcph = (struct tcphdr *)(iph + 1);
+
+	cxgb3_set_skb_header(skb, iph, IPH_OFFSET);
+	s->head = s->tail = skb;
+	s->iplen = ntohs(iph->tot_len);
+	s->mss = plen;
+	s->seq = ntohl(tcph->seq) + plen;
+	s->vlan = vlan;
+	s->npkts = 1;
+}
+
+/**
+ *	lro_flush_session - complete an LRO session
+ *	@adap: the adapter
+ *	@qs: the queue set associated with the LRO session
+ *	@s: the LRO session
+ *
+ *	Complete an active LRO session and send the packet it has been building
+ *	upstream.
+ */
+static void lro_flush_session(struct adapter *adap, struct sge_qset *qs,
+			      struct lro_session *s)
+{
+	struct iphdr *iph = ip_hdr(s->head);
+
+	if (iph->tot_len != htons(s->iplen)) {
+		/* IP length has changed, fix up IP header */
+		iph->tot_len = htons(s->iplen);
+		iph->check = 0;
+		iph->check = ip_fast_csum((unsigned char *)iph, iph->ihl);
+#ifndef NETIF_F_TSO_FAKE
+		/* TSO supported */
+		/* tcp_measure_rcv_mss in recent kernels looks at gso_size */
+		skb_shinfo(s->head)->gso_size = s->mss;
+#ifdef GSO_TYPE
+		skb_shinfo(s->head)->gso_type = SKB_GSO_TCPV4;
+#endif
+#endif
+	}
+
+	qs->port_stats[SGE_PSTAT_LRO]++;
+	rx_eth(adap, &qs->rspq, s->head, 2, s->npkts);
+	s->head = NULL;
+	qs->lro.nactive--;
+}
+
+/**
+ *	lro_flush - flush all active LRO sessions
+ *	@adap: the adapter
+ *	@qs: associated queue set
+ *	@state: the LRO state
+ *
+ *	Flush all active LRO sessions and reset the LRO state.
+ */
+static void lro_flush(struct adapter *adap, struct sge_qset *qs,
+		      struct lro_state *state)
+{
+	unsigned int idx = state->active_idx;
+
+	while (state->nactive) {
+		struct lro_session *s = &state->sess[idx];
+
+		if (s->head)
+			lro_flush_session(adap, qs, s);
+		idx = (idx + 1) & (MAX_LRO_SES - 1);
+	}
+}
+
+/**
+ *	lro_alloc_session - allocate a new LRO session
+ *	@adap: the adapter
+ *	@qs: associated queue set
+ *	@hash: hash value for the connection to be associated with the session
+ *
+ *	Allocate a new LRO session.  If there are no more session slots one of
+ *	the existing active sessions is completed and taken over.
+ */
+static struct lro_session *lro_alloc_session(struct adapter *adap,
+					struct sge_qset *qs, unsigned int hash)
+{
+	struct lro_state *state = &qs->lro;
+	unsigned int idx = hash & (MAX_LRO_SES - 1);
+	struct lro_session *s = &state->sess[idx];
+
+	if (likely(!s->head))   /* session currently inactive, use it */
+		goto done;
+
+	if (unlikely(state->nactive == MAX_LRO_SES)) {
+		lro_flush_session(adap, qs, s);
+		qs->port_stats[SGE_PSTAT_LRO_OVFLOW]++;
+	} else {
+		qs->port_stats[SGE_PSTAT_LRO_COLSN]++;
+		do {
+			idx = (idx + 1) & (MAX_LRO_SES - 1);
+			s = &state->sess[idx];
+		} while (s->head);
+	}
+
+done:   state->nactive++;
+	state->active_idx = idx;
+	return s;
+}
+
+/**
+ *	lro_frame_ok - check if an ingress packet is eligible for LRO
+ *	@p: the CPL header of the packet
+ *
+ *	Returns true if a received packet is eligible for LRO.
+ *	The following conditions must be true:
+ *	- packet is TCP/IP Ethernet II (checked elsewhere)
+ *	- not an IP fragment
+ *	- no IP options
+ *	- TCP/IP checksums are correct
+ *	- the packet is for this host
+ */
+static inline int lro_frame_ok(const struct cpl_rx_pkt *p)
+{
+	const struct ethhdr *eh = (struct ethhdr *)(p + 1);
+	const struct iphdr *ih = (struct iphdr *)(eh + 1);
+
+	return (*((u8 *)p + 1) & 0x90) == 0x10 && p->csum == htons(0xffff) &&
+	       eh->h_proto == htons(ETH_P_IP) && ih->ihl == (sizeof(*ih) >> 2);
+}
+
+#define TCP_FLAG_MASK (TCP_FLAG_CWR | TCP_FLAG_ECE | TCP_FLAG_URG |\
+		       TCP_FLAG_ACK | TCP_FLAG_PSH | TCP_FLAG_RST |\
+		       TCP_FLAG_SYN | TCP_FLAG_FIN)
+#define TSTAMP_WORD ((TCPOPT_NOP << 24) | (TCPOPT_NOP << 16) |\
+		     (TCPOPT_TIMESTAMP << 8) | TCPOLEN_TIMESTAMP)
+
+/**
+ *	lro_segment_ok - check if a TCP segment is eligible for LRO
+ *	@tcph: the TCP header of the packet
+ *
+ *	Returns true if a TCP packet is eligible for LRO.  This requires that
+ *	the packet have only the ACK flag set and no TCP options besides
+ *	time stamps.
+ */
+static inline int lro_segment_ok(const struct tcphdr *tcph)
+{
+	int optlen;
+
+	if (unlikely((tcp_flag_word(tcph) & TCP_FLAG_MASK) != TCP_FLAG_ACK))
+		return 0;
+
+	optlen = (tcph->doff << 2) - sizeof(*tcph);
+	if (optlen) {
+		const u32 *opt = (const u32 *)(tcph + 1);
+
+		if (optlen != TCPOLEN_TSTAMP_ALIGNED ||
+		    *opt != htonl(TSTAMP_WORD) || !opt[2])
+			return 0;
+	}
+	return 1;
+}
+
+static int lro_update_session(struct lro_session *s,
+			      const struct iphdr *iph, __be16 vlan, int plen)
+{
+	struct sk_buff *skb;
+	const struct tcphdr *tcph;
+	struct tcphdr *s_tcph;
+
+	if (unlikely(vlan != s->vlan))
+		return -1;
+
+	tcph = (const struct tcphdr *)(iph + 1);
+	if (unlikely(ntohl(tcph->seq) != s->seq || plen > 65535 - s->iplen))
+		return -1;
+
+	skb = s->head;
+	s_tcph = (struct tcphdr *)(ip_hdr(skb) + 1);
+
+	if (tcph->doff != sizeof(*tcph) / 4) {        /* TCP options present */
+		const u32 *opt = (u32 *)(tcph + 1);
+		u32 *s_opt = (u32 *)(s_tcph + 1);
+
+		if (unlikely(ntohl(s_opt[1]) > ntohl(opt[1])))
+			return -1;
+		s_opt[1] = opt[1];
+		s_opt[2] = opt[2];
+	}
+	s_tcph->ack_seq = tcph->ack_seq;
+	s_tcph->window = tcph->window;
+
+	s->seq += plen;
+	s->iplen += plen;
+	if (plen > s->mss)
+		s->mss = plen;
+	s->npkts++;
+	skb->len += plen;
+	skb->data_len += plen;
+	return 0;
+}
+
+/*
+ * Length of a packet buffer examined by LRO, it extends up to and including TCP
+ * timestamps.  This part of the packet must be made memory coherent for CPU
+ * accesses.
+ */
+#define LRO_PEEK_LEN (IPH_OFFSET + sizeof(struct iphdr) + \
+		      sizeof(struct tcphdr) + 12)
+
 /**
  *	lro_add_page - add a page chunk to an LRO session
  *	@adap: the adapter
  *	@qs: the associated queue set
  *	@fl: the free list containing the page chunk to add
- *	@len: packet length
- *	@complete: Indicates the last fragment of a frame
+ *	@flags: response queue flags for RX buffer
+ *	@hash: hash value for the packet
  *
  *	Add a received packet contained in a page chunk to an existing LRO
- *	session.
+ *	session.  There are four possible outcomes:
+ *	- packet is not eligible for LRO; return -1
+ *	- packet is eligible but there's no appropriate session; return 1
+ *	- packet is added and the page chunk consumed; return 0
+ *	- packet is added but the page chunk isn't needed; return 0
  */
-static void lro_add_page(struct adapter *adap, struct sge_qset *qs,
-			 struct sge_fl *fl, int len, int complete)
+static int lro_add_page(struct adapter *adap, struct sge_qset *qs,
+			struct sge_fl *fl, u32 flags, u32 hash)
 {
+	int tcpiplen, plen, ret;
+	struct lro_session *s;
+	const struct iphdr *iph;
+	const struct tcphdr *tcph;
 	struct rx_sw_desc *sd = &fl->sdesc[fl->cidx];
-	struct port_info *pi = netdev_priv(qs->netdev);
-	struct sk_buff *skb = NULL;
-	struct cpl_rx_pkt *cpl;
-	struct skb_frag_struct *rx_frag;
-	int nr_frags;
-	int offset = 0;
-
-	if (!qs->nomem) {
-		skb = napi_get_frags(&qs->napi);
-		qs->nomem = !skb;
+	const struct cpl_rx_pkt *cpl = sd->pg_chunk.va + 2;
+
+	pci_dma_sync_single_for_cpu(adap->pdev, pci_unmap_addr(sd, dma_addr),
+				    LRO_PEEK_LEN, PCI_DMA_FROMDEVICE);
+
+	if ((flags & (F_RSPD_SOP|F_RSPD_EOP)) != (F_RSPD_SOP|F_RSPD_EOP) ||
+	    !lro_frame_ok(cpl)) {
+ret_1:		ret = -1;
+sync:		pci_dma_sync_single_for_device(adap->pdev,
+					pci_unmap_addr(sd, dma_addr),
+					LRO_PEEK_LEN, PCI_DMA_FROMDEVICE);
+		return ret;
 	}
 
+	iph = (const struct iphdr *)(sd->pg_chunk.va + IPH_OFFSET);
+	s = lro_lookup(&qs->lro, hash & (MAX_LRO_SES - 1), iph);
+	if (!s) {
+		ret = 1;
+		goto sync;
+	}
+
+	tcph = (const struct tcphdr *)(iph + 1);
+	tcpiplen = sizeof(*iph) + (tcph->doff << 2);
+	plen = ntohs(iph->tot_len) - tcpiplen;
+
+	if (!lro_segment_ok(tcph) ||
+	    lro_update_session(s, iph,
+			       cpl->vlan_valid ? cpl->vlan : htons(0xffff),
+			       plen)) {
+		lro_flush_session(adap, qs, s);
+		goto ret_1;
+	}
+
 	fl->credits--;
-
-	pci_dma_sync_single_for_cpu(adap->pdev,
-				    pci_unmap_addr(sd, dma_addr),
-				    fl->buf_size - SGE_PG_RSVD,
-				    PCI_DMA_FROMDEVICE);
-
-	(*sd->pg_chunk.p_cnt)--;
-	if (!*sd->pg_chunk.p_cnt && sd->pg_chunk.page != fl->pg_chunk.page)
-		pci_unmap_page(adap->pdev,
-			       sd->pg_chunk.mapping,
-			       fl->alloc_size,
-			       PCI_DMA_FROMDEVICE);
-
-	if (!skb) {
-		put_page(sd->pg_chunk.page);
-		if (complete)
-			qs->nomem = 0;
-		return;
+	if (plen) {
+		struct sk_buff *tskb = s->tail;
+		struct skb_shared_info *shinfo = skb_shinfo(tskb);
+
+		prefetch(sd->pg_chunk.p_cnt);
+
+		pci_dma_sync_single_for_cpu(adap->pdev,
+					   pci_unmap_addr(sd, dma_addr),
+					   fl->buf_size - SGE_PG_RSVD,
+					   PCI_DMA_FROMDEVICE);
+		(*sd->pg_chunk.p_cnt)--;
+		if (!*sd->pg_chunk.p_cnt && sd->pg_chunk.page != fl->pg_chunk.page)
+			pci_unmap_page(adap->pdev,
+				       sd->pg_chunk.mapping,
+				       fl->alloc_size,
+				       PCI_DMA_FROMDEVICE);
+
+		skb_fill_page_desc(tskb, shinfo->nr_frags, sd->pg_chunk.page,
+				   sd->pg_chunk.offset + IPH_OFFSET + tcpiplen,
+				   plen);
+		s->head->truesize += plen;
+		if (s->head != tskb) {
+			/*
+			 * lro_update_session updates the sizes of the head skb,
+			 * do the same here for the component skb the fragment
+			 * was actually added to.
+			 */
+			tskb->len += plen;
+			tskb->data_len += plen;
+			tskb->truesize += plen;
+		}
+		if (unlikely(shinfo->nr_frags == MAX_SKB_FRAGS))
+			lro_flush_session(adap, qs, s);
+		/* No refill, caller does it. */
+		qs->port_stats[SGE_PSTAT_LRO_PG]++;
+	} else {
+		pci_dma_sync_single_for_device(adap->pdev,
+					pci_unmap_addr(sd, dma_addr),
+					LRO_PEEK_LEN, PCI_DMA_FROMDEVICE);
+		recycle_rx_buf(adap, fl, fl->cidx);
+		qs->port_stats[SGE_PSTAT_LRO_ACK]++;
 	}
 
-	rx_frag = skb_shinfo(skb)->frags;
-	nr_frags = skb_shinfo(skb)->nr_frags;
-
-	if (!nr_frags) {
-		offset = 2 + sizeof(struct cpl_rx_pkt);
-		cpl = qs->lro_va = sd->pg_chunk.va + 2;
-
-		if ((pi->rx_offload & T3_RX_CSUM) &&
-		     cpl->csum_valid && cpl->csum == htons(0xffff)) {
-			skb->ip_summed = CHECKSUM_UNNECESSARY;
-			qs->port_stats[SGE_PSTAT_RX_CSUM_GOOD]++;
-		} else
-			skb->ip_summed = CHECKSUM_NONE;
-	} else
-		cpl = qs->lro_va;
-
-	len -= offset;
-
-	rx_frag += nr_frags;
-	rx_frag->page = sd->pg_chunk.page;
-	rx_frag->page_offset = sd->pg_chunk.offset + offset;
-	rx_frag->size = len;
-
-	skb->len += len;
-	skb->data_len += len;
-	skb->truesize += len;
-	skb_shinfo(skb)->nr_frags++;
-
-	if (!complete)
-		return;
-
-	skb_record_rx_queue(skb, qs - &adap->sge.qs[0]);
-
-	if (unlikely(cpl->vlan_valid)) {
-		struct vlan_group *grp = pi->vlan_grp;
-
-		if (likely(grp != NULL)) {
-			vlan_gro_frags(&qs->napi, grp, ntohs(cpl->vlan));
-			return;
+	return 0;
+}
+
+/**
+ *	lro_add_skb - add an sk_buff to an LRO session
+ *	@adap: the adapter
+ *	@qs: the associated queue set
+ *	@skb: the sk_buff to add
+ *	@hash: hash value for the packet
+ *
+ *	Add a received packet contained in an sk_buff to an existing LRO
+ *	session.  Returns -1 if the packet is not eligible for LRO, and 0
+ *	if it is added successfully.
+ */
+static int lro_add_skb(struct adapter *adap, struct sge_qset *qs,
+		       struct sk_buff *skb, u32 hash)
+{
+	__be16 vlan;
+	int tcpiplen, plen;
+	struct lro_session *s;
+	struct iphdr *iph;
+	const struct tcphdr *tcph;
+	const struct cpl_rx_pkt *cpl = (struct cpl_rx_pkt *)(skb->data + 2);
+
+	if (!lro_frame_ok(cpl))
+		return -1;
+
+	iph = (struct iphdr *)(skb->data + IPH_OFFSET);
+	s = lro_lookup(&qs->lro, hash & (MAX_LRO_SES - 1), iph);
+
+	tcph = (struct tcphdr *)(iph + 1);
+	if (!lro_segment_ok(tcph)) {
+		if (s)
+			lro_flush_session(adap, qs, s);
+		return -1;
+	}
+
+	tcpiplen = sizeof(*iph) + (tcph->doff << 2);
+	plen = ntohs(iph->tot_len) - tcpiplen;
+	vlan = cpl->vlan_valid ? cpl->vlan : htons(0xffff);
+	if (likely(s && !lro_update_session(s, iph, vlan, plen))) {
+		/*
+		 * Pure ACKs have nothing useful left and can be freed.
+		 */
+		if (plen) {
+			skb_pull(skb, IPH_OFFSET + tcpiplen);
+			s->head->truesize += skb->truesize;
+
+			/* TP trims IP packets, no skb_trim needed */
+			if (s->head == s->tail)
+				skb_shinfo(s->head)->frag_list = skb;
+			else
+				s->tail->next = skb;
+			s->tail = skb;
+			qs->port_stats[SGE_PSTAT_LRO_SKB]++;
+		} else {
+			__kfree_skb(skb);  /* no destructors, ok from irq */
+			qs->port_stats[SGE_PSTAT_LRO_ACK]++;
 		}
+	} else {
+		if (s)
+			lro_flush_session(adap, qs, s);
+		s = lro_alloc_session(adap, qs, hash);
+		lro_init_session(s, skb, iph, vlan, plen);
+		qs->port_stats[SGE_PSTAT_LRO_SKB]++;
 	}
-	napi_gro_frags(&qs->napi);
+	return 0;
 }
 
 /**
@@ -2166,8 +2934,38 @@
 #endif
 
 	credits = G_RSPD_TXQ0_CR(flags);
-	if (credits)
+	if (credits) {
 		qs->txq[TXQ_ETH].processed += credits;
+#ifdef CHELSIO_FREE_TXBUF_ASAP
+		/*
+		 * In the normal Linux driver t3_eth_xmit() routine, we call
+		 * skb_orphan() on unshared TX skb.  This results in a call to
+		 * the destructor for the skb which frees up the send buffer
+		 * space it was holding down.  This, in turn, allows the
+		 * application to make forward progress generating more data
+		 * which is important at 10Gb/s.  For Virtual Machine Guest
+		 * Operating Systems this doesn't work since the send buffer
+		 * space is being held down in the Virtual Machine.  Thus we
+		 * need to get the TX skb's freed up as soon as possible in
+		 * order to prevent applications from stalling.
+		 *
+		 * This code is largely copied from the corresponding code in
+		 * sge_timer_tx() and should probably be kept in sync with any
+		 * changes there.
+		 */
+		if (spin_trylock(&qs->txq[TXQ_ETH].lock)) {
+			struct sge_txq *q = &qs->txq[TXQ_ETH];
+			struct port_info *pi = netdev_priv(qs->netdev);
+			struct adapter *adap = pi->adapter;
+
+			if (q->eth_coalesce_idx)
+				ship_tx_pkt_coalesce_wr(adap, q);
+
+			reclaim_completed_tx(adap, &qs->txq[TXQ_ETH], TX_RECLAIM_CHUNK);
+			spin_unlock(&qs->txq[TXQ_ETH].lock);
+		}
+#endif
+	}
 
 	credits = G_RSPD_TXQ2_CR(flags);
 	if (credits)
@@ -2184,7 +2982,7 @@
 
 /**
  *	check_ring_db - check if we need to ring any doorbells
- *	@adapter: the adapter
+ *	@adap: the adapter
  *	@qs: the queue set whose Tx queues are to be examined
  *	@sleeping: indicates which Tx queue sent GTS
  *
@@ -2192,7 +2990,7 @@
  *	to resume transmission after idling while they still have unprocessed
  *	descriptors.
  */
-static void check_ring_db(struct adapter *adap, struct sge_qset *qs,
+static void check_ring_db(adapter_t *adap, struct sge_qset *qs,
 			  unsigned int sleeping)
 {
 	if (sleeping & F_RSPD_TXQ0_GTS) {
@@ -2201,6 +2999,9 @@
 		if (txq->cleaned + txq->in_use != txq->processed &&
 		    !test_and_set_bit(TXQ_LAST_PKT_DB, &txq->flags)) {
 			set_bit(TXQ_RUNNING, &txq->flags);
+#ifdef T3_TRACE
+			T3_TRACE0(adap->tb[txq->cntxt_id & 7], "doorbell ETH");
+#endif
 			t3_write_reg(adap, A_SG_KDOORBELL, F_SELEGRCNTX |
 				     V_EGRCNTX(txq->cntxt_id));
 		}
@@ -2212,6 +3013,10 @@
 		if (txq->cleaned + txq->in_use != txq->processed &&
 		    !test_and_set_bit(TXQ_LAST_PKT_DB, &txq->flags)) {
 			set_bit(TXQ_RUNNING, &txq->flags);
+#ifdef T3_TRACE
+			T3_TRACE0(adap->tb[txq->cntxt_id & 7],
+				  "doorbell offload");
+#endif
 			t3_write_reg(adap, A_SG_KDOORBELL, F_SELEGRCNTX |
 				     V_EGRCNTX(txq->cntxt_id));
 		}
@@ -2262,8 +3067,7 @@
  *	on this queue.  If the system is under memory shortage use a fairly
  *	long delay to help recovery.
  */
-static int process_responses(struct adapter *adap, struct sge_qset *qs,
-			     int budget)
+static int process_responses(adapter_t *adap, struct sge_qset *qs, int budget)
 {
 	struct sge_rspq *q = &qs->rspq;
 	struct rsp_desc *r = &q->desc[q->cidx];
@@ -2275,16 +3079,18 @@
 	q->next_holdoff = q->holdoff_tmr;
 
 	while (likely(budget_left && is_new_response(r, q))) {
-		int packet_complete, eth, ethpad = 2, lro = qs->lro_enabled;
+		int packet_complete, eth, ethpad = 2, lro = qs->lro.enabled;
+		u32 len, flags = ntohl(r->flags);
+		u32 rss_hi = *(const u32 *)r, rss_lo = r->rss_hdr.rss_hash_val;
 		struct sk_buff *skb = NULL;
-		u32 len, flags;
-		__be32 rss_hi, rss_lo;
-
-		rmb();
+
+#ifdef T3_TRACE
+		T3_TRACE5(adap->tb[q->cntxt_id],
+			  "response: RSS 0x%x flags 0x%x len %u, type 0x%x rss hash 0x%x",
+			  ntohl(rss_hi), flags, ntohl(r->len_cq),
+			  r->rss_hdr.hash_type, ntohl(rss_lo));
+#endif
 		eth = r->rss_hdr.opcode == CPL_RX_PKT;
-		rss_hi = *(const __be32 *)r;
-		rss_lo = r->rss_hdr.rss_hash_val;
-		flags = ntohl(r->flags);
 
 		if (unlikely(flags & F_RSPD_ASYNC_NOTIF)) {
 			skb = alloc_skb(AN_PKT_SIZE, GFP_ATOMIC);
@@ -2321,18 +3127,16 @@
 				prefetch(addr + L1_CACHE_BYTES);
 #endif
 				__refill_fl(adap, fl);
+
 				if (lro > 0) {
-					lro_add_page(adap, qs, fl,
-						     G_RSPD_LEN(len),
-						     flags & F_RSPD_EOP);
-					 goto next_fl;
+					lro = lro_add_page(adap, qs, fl,
+							   flags, rss_lo);
+					if (!lro)
+						goto next_fl;
 				}
 
-				skb = get_packet_pg(adap, fl, q,
-						    G_RSPD_LEN(len),
-						    eth ?
-						    SGE_RX_DROP_THRES : 0);
-				q->pg_skb = skb;
+				skb = q->pg_skb = get_packet_pg(adap, fl, q, G_RSPD_LEN(len),
+						 eth ? SGE_RX_DROP_THRES : 0);
 			} else
 				skb = get_packet(adap, fl, G_RSPD_LEN(len),
 						 eth ? SGE_RX_DROP_THRES : 0);
@@ -2361,19 +3165,21 @@
 		}
 		prefetch(r);
 
-		if (++q->credits >= (q->size / 4)) {
+		if ((++q->credits >= (q->size / 4)) || test_bit(RSPQ_STARVING, &q->flags)) {
 			refill_rspq(adap, q, q->credits);
 			q->credits = 0;
+			clear_bit(RSPQ_STARVING, &q->flags);
 		}
 
-		packet_complete = flags &
-				  (F_RSPD_EOP | F_RSPD_IMM_DATA_VALID |
-				   F_RSPD_ASYNC_NOTIF);
-
-		if (skb != NULL && packet_complete) {
-			if (eth)
-				rx_eth(adap, q, skb, ethpad, lro);
-			else {
+		packet_complete = flags & 
+			(F_RSPD_EOP | F_RSPD_IMM_DATA_VALID | F_RSPD_ASYNC_NOTIF);
+
+		if ((skb != NULL) && packet_complete) {
+			if (eth) {
+				if (lro <= 0 ||
+				    lro_add_skb(adap, qs, skb, rss_lo))
+					rx_eth(adap, q, skb, ethpad, 1);
+			} else {
 				q->offload_pkts++;
 				/* Preserve the RSS info in csum & priority */
 				skb->csum = rss_hi;
@@ -2382,180 +3188,298 @@
 						       offload_skbs,
 						       ngathered);
 			}
-
+		    
 			if (flags & F_RSPD_EOP)
 				clear_rspq_bufstate(q);
 		}
+
 		--budget_left;
 	}
 
 	deliver_partial_bundle(&adap->tdev, q, offload_skbs, ngathered);
+	lro_flush(adap, qs, &qs->lro);
 
 	if (sleeping)
 		check_ring_db(adap, qs, sleeping);
 
-	smp_mb();		/* commit Tx queue .processed updates */
+	smp_mb();  /* commit Tx queue .processed updates */
 	if (unlikely(qs->txq_stopped != 0))
 		restart_tx(qs);
 
+	if (qs->txq[TXQ_ETH].eth_coalesce_idx &&
+	    should_finalize_tx_pkt_coalescing(&qs->txq[TXQ_ETH]))
+		try_finalize_tx_pkt_coalesce_wr(adap, &qs->txq[TXQ_ETH]);
+
 	budget -= budget_left;
+#ifdef T3_TRACE
+	T3_TRACE4(adap->tb[q->cntxt_id],
+		  "process_responses: <- cidx %u gen %u ret %u credit %u",
+		  q->cidx, q->gen, budget, q->credits);
+#endif
 	return budget;
 }
 
 static inline int is_pure_response(const struct rsp_desc *r)
 {
-	__be32 n = r->flags & htonl(F_RSPD_ASYNC_NOTIF | F_RSPD_IMM_DATA_VALID);
+	u32 n = ntohl(r->flags) & (F_RSPD_ASYNC_NOTIF | F_RSPD_IMM_DATA_VALID);
 
 	return (n | r->len_cq) == 0;
 }
 
+/*
+ * Returns true if the device is already scheduled for polling.
+ */
+static inline int napi_is_scheduled(struct sge_qset *qs)
+{
+#if defined(NAPI_UPDATE)
+        struct napi_struct *napi = &qs->napi;
+        return test_bit(NAPI_STATE_SCHED, &napi->state);
+#else
+        struct net_device *dev = qs->netdev;
+        return test_bit(__LINK_STATE_RX_SCHED, &dev->state);
+#endif
+}
+
+/**
+ *      process_pure_responses - process pure responses from a response queue
+ *      @adap: the adapter
+ *      @qs: the queue set owning the response queue
+ *      @r: the first pure response to process
+ *
+ *      A simpler version of process_responses() that handles only pure (i.e.,
+ *      non data-carrying) responses.  Such respones are too light-weight to
+ *      justify calling a softirq under NAPI, so we handle them specially in
+ *      the interrupt handler.  The function is called with a pointer to a
+ *      response, which the caller must ensure is a valid pure response.
+ *
+ *      Returns 1 if it encounters a valid data-carrying response, 0 otherwise.
+ */
+static int process_pure_responses(adapter_t *adap, struct sge_qset *qs,
+                                  struct rsp_desc *r)
+{
+        struct sge_rspq *q = &qs->rspq;
+        unsigned int sleeping = 0;
+
+        do {
+                u32 flags = ntohl(r->flags);
+
+#ifdef T3_TRACE
+                T3_TRACE2(adap->tb[q->cntxt_id],
+                          "pure response: RSS 0x%x flags 0x%x",
+                          ntohl(*(u32 *)r), flags);
+#endif
+                r++;
+                if (unlikely(++q->cidx == q->size)) {
+                        q->cidx = 0;
+                        q->gen ^= 1;
+                        r = q->desc;
+                }
+                prefetch(r);
+
+                if (flags & RSPD_CTRL_MASK) {
+                        sleeping |= flags & RSPD_GTS_MASK;
+                        handle_rsp_cntrl_info(qs, flags);
+                }
+
+                q->pure_rsps++;
+                if ((++q->credits >= (q->size / 4)) || test_bit(RSPQ_STARVING, &q->flags)) {
+                        refill_rspq(adap, q, q->credits);
+                        q->credits = 0;
+                        clear_bit(RSPQ_STARVING, &q->flags);
+                }
+        } while (is_new_response(r, q) && is_pure_response(r));
+
+        if (sleeping)
+                check_ring_db(adap, qs, sleeping);
+
+        smp_mb();  /* commit Tx queue .processed updates */
+        if (unlikely(qs->txq_stopped != 0))
+                restart_tx(qs);
+
+        if (qs->txq[TXQ_ETH].eth_coalesce_idx &&
+            should_finalize_tx_pkt_coalescing(&qs->txq[TXQ_ETH]))
+                try_finalize_tx_pkt_coalesce_wr(adap, &qs->txq[TXQ_ETH]);
+
+        return is_new_response(r, q);
+}
+
+/**
+ *      handle_responses - decide what to do with new responses in NAPI mode
+ *      @adap: the adapter
+ *      @q: the response queue
+ *
+ *      This is used by the NAPI interrupt handlers to decide what to do with
+ *      new SGE responses.  If there are no new responses it returns -1.  If
+ *      there are new responses and they are pure (i.e., non-data carrying)
+ *      it handles them straight in hard interrupt context as they are very
+ *      cheap and don't deliver any packets.  Finally, if there are any data
+ *      signaling responses it schedules the NAPI handler.  Returns 1 if it
+ *      schedules NAPI, 0 if all new responses were pure.
+ *
+ *      The caller must ascertain NAPI is not already running.
+ */
+static inline int handle_responses(struct adapter *adap, struct sge_rspq *q)
+{
+        struct sge_qset *qs = rspq_to_qset(q);
+        struct rsp_desc *r = &q->desc[q->cidx];
+
+        if (!is_new_response(r, q))
+                return -1;
+        if (is_pure_response(r) && process_pure_responses(adap, qs, r) == 0) {
+                t3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |
+                             V_NEWTIMER(q->holdoff_tmr) |
+                             V_NEWINDEX(q->cidx));
+                return 0;
+        }
+#if defined(NAPI_UPDATE)
+        napi_schedule(&qs->napi);
+#else
+        if (likely(__netif_rx_schedule_prep(qs->netdev)))
+                __netif_rx_schedule(qs->netdev);
+#endif
+        return 1;
+}
+
+void check_rspq_fl_status(adapter_t *adapter)
+{
+	const struct adapter_params *p = &adapter->params;
+	unsigned int status, status_clr, reset, v;
+
+        status = t3_read_reg(adapter, A_SG_INT_CAUSE);
+        reset = 0;
+
+        if (status & F_FLEMPTY) {
+                int i = 0;
+                struct sge_qset *qs = &adapter->sge.qs[0];
+
+                reset |= F_FLEMPTY;
+                status_clr = v = t3_read_reg(adapter, A_SG_RSPQ_FL_STATUS);
+                status_clr &= (M_FLXEMPTY << S_FLXEMPTY);
+                v = G_FLXEMPTY(v);
+
+                while (v) {
+                        qs->fl[i].empty += (v & 1);
+                        if (i)
+                                qs++;
+                        i ^= 1;
+                        v >>= 1;
+                }
+                t3_write_reg(adapter, A_SG_RSPQ_FL_STATUS, status_clr);
+        }
+
+        if (status & F_RSPQSTARVE) {
+                struct sge_qset *qs = &adapter->sge.qs[0];
+
+                reset |= F_RSPQSTARVE;
+
+                status_clr = v = t3_read_reg(adapter, A_SG_RSPQ_FL_STATUS);
+		status_clr &= (M_RSPQXSTARVED << S_RSPQXSTARVED);
+		v  = G_RSPQXSTARVED(v);
+
+		while (v) {
+			if (v & 1) {
+				qs->rspq.starved++;
+				set_bit(RSPQ_STARVING, &qs->rspq.flags);
+                        	if (p->rev < T3_REV_C) {
+                           		spinlock_t *lock;
+                           		lock = adapter->params.rev > 0 ?
+                                			&qs->rspq.lock :
+                                			&adapter->sge.qs[0].rspq.lock;
+                           		if (spin_trylock_irq(lock)) {
+			   			if (qs->rspq.flags & USING_POLLING) {
+							if (!napi_is_scheduled(qs))
+								handle_responses(adapter, &qs->rspq);
+                           			} else if (qs->rspq.credits) {
+                           				qs->rspq.credits--;
+                             				t3_write_reg(adapter, A_SG_RSPQ_CREDIT_RETURN,
+                                					V_RSPQ(qs->rspq.cntxt_id) |
+                                					V_CREDITS(1));
+			   				qs->rspq.restarted++;
+			   			}
+			   			spin_unlock_irq(lock);
+					}
+				}
+			}
+			qs++;
+			v >>= 1;
+                 }
+
+                t3_write_reg(adapter, A_SG_RSPQ_FL_STATUS, status_clr);
+        }
+
+        t3_write_reg(adapter, A_SG_INT_CAUSE, reset);
+}
+
 /**
  *	napi_rx_handler - the NAPI handler for Rx processing
- *	@napi: the napi instance
+ *	@dev: the net device
  *	@budget: how many packets we can process in this round
  *
- *	Handler for new data events when using NAPI.
+ *	Handler for new data events when using NAPI.  This does not need any
+ *	locking or protection from interrupts as data interrupts are off at
+ *	this point and other adapter interrupts do not interfere (the latter
+ *	in not a concern at all with MSI-X as non-data interrupts then have
+ *	a separate handler).
  */
-static int napi_rx_handler(struct napi_struct *napi, int budget)
+DECLARE_NAPI_RX_HANDLER(napi, dev, budget)
 {
-	struct sge_qset *qs = container_of(napi, struct sge_qset, napi);
+	struct sge_qset *qs = SGE_GET_OFLD_QS(napi, dev);
 	struct adapter *adap = qs->adap;
-	int work_done = process_responses(adap, qs, budget);
-
-	if (likely(work_done < budget)) {
+#if defined(NAPI_UPDATE)
+	int effective_budget = budget;
+#else
+	int effective_budget = min(*budget, dev->quota);
+#endif
+	int work_done = process_responses(adap, qs, effective_budget);
+
+#if !defined(NAPI_UPDATE)
+	*budget -= work_done;
+	dev->quota -= work_done;
+#endif
+	if (likely(work_done < effective_budget)) {
+
+#if defined(NAPI_UPDATE)
 		napi_complete(napi);
-
+#else
+		netif_rx_complete(dev);
+#endif
 		/*
-		 * Because we don't atomically flush the following
-		 * write it is possible that in very rare cases it can
-		 * reach the device in a way that races with a new
-		 * response being written plus an error interrupt
-		 * causing the NAPI interrupt handler below to return
-		 * unhandled status to the OS.  To protect against
-		 * this would require flushing the write and doing
-		 * both the write and the flush with interrupts off.
-		 * Way too expensive and unjustifiable given the
-		 * rarity of the race.
+		 * Because we don't atomically flush the following write it is
+		 * possible that in very rare cases it can reach the device
+		 * in a way that races with a new response being written
+		 * plus an error interrupt causing the NAPI interrupt handler
+		 * below to return unhandled status to the OS.
+		 * To protect against this would require flushing the write
+		 * and doing both the write and the flush with interrupts off.
+		 * Way too expensive and unjustifiable given the rarity
+		 * of the race.
 		 *
 		 * The race cannot happen at all with MSI-X.
 		 */
 		t3_write_reg(adap, A_SG_GTS, V_RSPQ(qs->rspq.cntxt_id) |
-			     V_NEWTIMER(qs->rspq.next_holdoff) |
-			     V_NEWINDEX(qs->rspq.cidx));
+		     	     V_NEWTIMER(qs->rspq.next_holdoff) |
+		             V_NEWINDEX(qs->rspq.cidx));
 	}
+#if defined(NAPI_UPDATE)
 	return work_done;
-}
-
-/*
- * Returns true if the device is already scheduled for polling.
- */
-static inline int napi_is_scheduled(struct napi_struct *napi)
-{
-	return test_bit(NAPI_STATE_SCHED, &napi->state);
-}
-
-/**
- *	process_pure_responses - process pure responses from a response queue
- *	@adap: the adapter
- *	@qs: the queue set owning the response queue
- *	@r: the first pure response to process
- *
- *	A simpler version of process_responses() that handles only pure (i.e.,
- *	non data-carrying) responses.  Such respones are too light-weight to
- *	justify calling a softirq under NAPI, so we handle them specially in
- *	the interrupt handler.  The function is called with a pointer to a
- *	response, which the caller must ensure is a valid pure response.
- *
- *	Returns 1 if it encounters a valid data-carrying response, 0 otherwise.
- */
-static int process_pure_responses(struct adapter *adap, struct sge_qset *qs,
-				  struct rsp_desc *r)
-{
-	struct sge_rspq *q = &qs->rspq;
-	unsigned int sleeping = 0;
-
-	do {
-		u32 flags = ntohl(r->flags);
-
-		r++;
-		if (unlikely(++q->cidx == q->size)) {
-			q->cidx = 0;
-			q->gen ^= 1;
-			r = q->desc;
-		}
-		prefetch(r);
-
-		if (flags & RSPD_CTRL_MASK) {
-			sleeping |= flags & RSPD_GTS_MASK;
-			handle_rsp_cntrl_info(qs, flags);
-		}
-
-		q->pure_rsps++;
-		if (++q->credits >= (q->size / 4)) {
-			refill_rspq(adap, q, q->credits);
-			q->credits = 0;
-		}
-		if (!is_new_response(r, q))
-			break;
-		rmb();
-	} while (is_pure_response(r));
-
-	if (sleeping)
-		check_ring_db(adap, qs, sleeping);
-
-	smp_mb();		/* commit Tx queue .processed updates */
-	if (unlikely(qs->txq_stopped != 0))
-		restart_tx(qs);
-
-	return is_new_response(r, q);
-}
-
-/**
- *	handle_responses - decide what to do with new responses in NAPI mode
- *	@adap: the adapter
- *	@q: the response queue
- *
- *	This is used by the NAPI interrupt handlers to decide what to do with
- *	new SGE responses.  If there are no new responses it returns -1.  If
- *	there are new responses and they are pure (i.e., non-data carrying)
- *	it handles them straight in hard interrupt context as they are very
- *	cheap and don't deliver any packets.  Finally, if there are any data
- *	signaling responses it schedules the NAPI handler.  Returns 1 if it
- *	schedules NAPI, 0 if all new responses were pure.
- *
- *	The caller must ascertain NAPI is not already running.
- */
-static inline int handle_responses(struct adapter *adap, struct sge_rspq *q)
-{
-	struct sge_qset *qs = rspq_to_qset(q);
-	struct rsp_desc *r = &q->desc[q->cidx];
-
-	if (!is_new_response(r, q))
-		return -1;
-	rmb();
-	if (is_pure_response(r) && process_pure_responses(adap, qs, r) == 0) {
-		t3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |
-			     V_NEWTIMER(q->holdoff_tmr) | V_NEWINDEX(q->cidx));
-		return 0;
-	}
-	napi_schedule(&qs->napi);
-	return 1;
+#else
+	return (work_done >= effective_budget);
+#endif
 }
 
 /*
  * The MSI-X interrupt handler for an SGE response queue for the non-NAPI case
  * (i.e., response queue serviced in hard interrupt).
  */
-irqreturn_t t3_sge_intr_msix(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_sge_intr_msix, irq, cookie, regs)
 {
 	struct sge_qset *qs = cookie;
-	struct adapter *adap = qs->adap;
 	struct sge_rspq *q = &qs->rspq;
 
 	spin_lock(&q->lock);
-	if (process_responses(adap, qs, -1) == 0)
+	if (process_responses(qs->adap, qs, -1) == 0)
 		q->unhandled_irqs++;
-	t3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |
+	t3_write_reg(qs->adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |
 		     V_NEWTIMER(q->next_holdoff) | V_NEWINDEX(q->cidx));
 	spin_unlock(&q->lock);
 	return IRQ_HANDLED;
@@ -2565,13 +3489,12 @@
  * The MSI-X interrupt handler for an SGE response queue for the NAPI case
  * (i.e., response queue serviced by NAPI polling).
  */
-static irqreturn_t t3_sge_intr_msix_napi(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_sge_intr_msix_napi, irq, cookie, regs)
 {
 	struct sge_qset *qs = cookie;
 	struct sge_rspq *q = &qs->rspq;
 
 	spin_lock(&q->lock);
-
 	if (handle_responses(qs->adap, q) < 0)
 		q->unhandled_irqs++;
 	spin_unlock(&q->lock);
@@ -2584,79 +3507,70 @@
  * the same MSI vector.  We use one SGE response queue per port in this mode
  * and protect all response queues with queue 0's lock.
  */
-static irqreturn_t t3_intr_msi(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_intr_msi, irq, cookie, regs)
 {
-	int new_packets = 0;
-	struct adapter *adap = cookie;
+	int i, qset = 0;
+	adapter_t *adap = cookie;
 	struct sge_rspq *q = &adap->sge.qs[0].rspq;
 
+	for_each_port(adap, i) {
+		int j;
+		struct port_info *p = adap2pinfo(adap, i);
+
+		for (j = p->first_qset; j < p->first_qset + p->nqsets; j++, qset++) {
+
+			struct sge_rspq *q1 = &adap->sge.qs[qset].rspq;
+			
+			spin_lock(&q1->lock);
+			(void)handle_responses(adap, q1);
+			spin_unlock(&q1->lock);
+		}
+	}
+
 	spin_lock(&q->lock);
-
-	if (process_responses(adap, &adap->sge.qs[0], -1)) {
-		t3_write_reg(adap, A_SG_GTS, V_RSPQ(q->cntxt_id) |
-			     V_NEWTIMER(q->next_holdoff) | V_NEWINDEX(q->cidx));
-		new_packets = 1;
-	}
-
-	if (adap->params.nports == 2 &&
-	    process_responses(adap, &adap->sge.qs[1], -1)) {
-		struct sge_rspq *q1 = &adap->sge.qs[1].rspq;
-
-		t3_write_reg(adap, A_SG_GTS, V_RSPQ(q1->cntxt_id) |
-			     V_NEWTIMER(q1->next_holdoff) |
-			     V_NEWINDEX(q1->cidx));
-		new_packets = 1;
-	}
-
-	if (!new_packets && t3_slow_intr_handler(adap) == 0)
-		q->unhandled_irqs++;
-
+	(void)t3_slow_intr_handler(adap);
 	spin_unlock(&q->lock);
+
 	return IRQ_HANDLED;
 }
 
-static int rspq_check_napi(struct sge_qset *qs)
-{
-	struct sge_rspq *q = &qs->rspq;
-
-	if (!napi_is_scheduled(&qs->napi) &&
-	    is_new_response(&q->desc[q->cidx], q)) {
-		napi_schedule(&qs->napi);
-		return 1;
-	}
-	return 0;
-}
-
 /*
  * The MSI interrupt handler for the NAPI case (i.e., response queues serviced
  * by NAPI polling).  Handles data events from SGE response queues as well as
  * error and other async events as they all use the same MSI vector.  We use
- * one SGE response queue per port in this mode and protect all response
- * queues with queue 0's lock.
+ * queue 0's lock for handling non-data events.
  */
-static irqreturn_t t3_intr_msi_napi(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_intr_msi_napi, irq, cookie, regs)
 {
-	int new_packets;
-	struct adapter *adap = cookie;
+	int i, qset = 0;
+	adapter_t *adap = cookie;
 	struct sge_rspq *q = &adap->sge.qs[0].rspq;
 
+	for_each_port(adap, i) {
+		int j;
+		struct port_info *p = adap2pinfo(adap, i);
+
+		for (j = p->first_qset; j < p->first_qset + p->nqsets; j++, qset++) {
+			struct sge_rspq *q1 = &adap->sge.qs[qset].rspq;
+
+			spin_lock(&q1->lock);
+			if (!napi_is_scheduled(&adap->sge.qs[qset])) 
+				(void)handle_responses(adap, q1);
+			spin_unlock(&q1->lock);
+		}
+	}
+
 	spin_lock(&q->lock);
-
-	new_packets = rspq_check_napi(&adap->sge.qs[0]);
-	if (adap->params.nports == 2)
-		new_packets += rspq_check_napi(&adap->sge.qs[1]);
-	if (!new_packets && t3_slow_intr_handler(adap) == 0)
-		q->unhandled_irqs++;
-
+	(void)t3_slow_intr_handler(adap);
 	spin_unlock(&q->lock);
+
 	return IRQ_HANDLED;
 }
 
 /*
  * A helper function that processes responses and issues GTS.
  */
-static inline int process_responses_gts(struct adapter *adap,
-					struct sge_rspq *rq)
+static inline int process_responses_gts(adapter_t *adap, struct sge_rspq *rq)
 {
 	int work;
 
@@ -2672,10 +3586,10 @@
  * the same interrupt pin.  We use one SGE response queue per port in this mode
  * and protect all response queues with queue 0's lock.
  */
-static irqreturn_t t3_intr(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3_intr, irq, cookie, regs)
 {
 	int work_done, w0, w1;
-	struct adapter *adap = cookie;
+	adapter_t *adap = cookie;
 	struct sge_rspq *q0 = &adap->sge.qs[0].rspq;
 	struct sge_rspq *q1 = &adap->sge.qs[1].rspq;
 
@@ -2683,11 +3597,11 @@
 
 	w0 = is_new_response(&q0->desc[q0->cidx], q0);
 	w1 = adap->params.nports == 2 &&
-	    is_new_response(&q1->desc[q1->cidx], q1);
+	     is_new_response(&q1->desc[q1->cidx], q1);
 
 	if (likely(w0 | w1)) {
 		t3_write_reg(adap, A_PL_CLI, 0);
-		t3_read_reg(adap, A_PL_CLI);	/* flush */
+		(void) t3_read_reg(adap, A_PL_CLI);    /* flush */
 
 		if (likely(w0))
 			process_responses_gts(adap, q0);
@@ -2706,70 +3620,89 @@
 /*
  * Interrupt handler for legacy INTx interrupts for T3B-based cards.
  * Handles data events from SGE response queues as well as error and other
- * async events as they all use the same interrupt pin.  We use one SGE
- * response queue per port in this mode and protect all response queues with
- * queue 0's lock.
+ * async events as they all use the same interrupt pin.
  */
-static irqreturn_t t3b_intr(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3b_intr, irq, cookie, regs)
 {
-	u32 map;
-	struct adapter *adap = cookie;
+	u32 i, map;
+	adapter_t *adap = cookie;
 	struct sge_rspq *q0 = &adap->sge.qs[0].rspq;
+	int qset = 0;
 
 	t3_write_reg(adap, A_PL_CLI, 0);
 	map = t3_read_reg(adap, A_SG_DATA_INTR);
 
-	if (unlikely(!map))	/* shared interrupt, most likely */
+	if (unlikely(!map))          /* shared interrupt, most likely */
 		return IRQ_NONE;
 
-	spin_lock(&q0->lock);
-
-	if (unlikely(map & F_ERRINTR))
-		t3_slow_intr_handler(adap);
-
-	if (likely(map & 1))
-		process_responses_gts(adap, q0);
-
-	if (map & 2)
-		process_responses_gts(adap, &adap->sge.qs[1].rspq);
-
-	spin_unlock(&q0->lock);
-	return IRQ_HANDLED;
+        if (unlikely(map & F_ERRINTR)) {
+		spin_lock(&q0->lock);
+		(void)t3_slow_intr_handler(adap);
+		spin_unlock(&q0->lock);
+	}
+
+        for_each_port(adap, i) {
+		struct port_info *p = adap2pinfo(adap, i);
+		int j;
+
+		for (j = p->first_qset; j < p->first_qset + p->nqsets; j++, qset++) {
+                	if (map & (1 << qset)) {
+				spin_lock(&adap->sge.qs[qset].rspq.lock);
+                        	process_responses_gts(adap, &adap->sge.qs[qset].rspq);
+				spin_unlock(&adap->sge.qs[qset].rspq.lock);
+			}
+		}
+	}
+        return IRQ_HANDLED;
 }
 
 /*
  * NAPI interrupt handler for legacy INTx interrupts for T3B-based cards.
  * Handles data events from SGE response queues as well as error and other
- * async events as they all use the same interrupt pin.  We use one SGE
- * response queue per port in this mode and protect all response queues with
- * queue 0's lock.
+ * async events as they all use the same interrupt pin.
  */
-static irqreturn_t t3b_intr_napi(int irq, void *cookie)
+DECLARE_INTR_HANDLER(t3b_intr_napi, irq, cookie, regs)
 {
-	u32 map;
-	struct adapter *adap = cookie;
-	struct sge_qset *qs0 = &adap->sge.qs[0];
-	struct sge_rspq *q0 = &qs0->rspq;
+	u32 i, map;
+	adapter_t *adap = cookie;
+	struct sge_rspq *q0 = &adap->sge.qs[0].rspq;
+	int qset = 0;
 
 	t3_write_reg(adap, A_PL_CLI, 0);
 	map = t3_read_reg(adap, A_SG_DATA_INTR);
 
-	if (unlikely(!map))	/* shared interrupt, most likely */
+	if (unlikely(!map))          /* shared interrupt, most likely */
 		return IRQ_NONE;
 
-	spin_lock(&q0->lock);
-
-	if (unlikely(map & F_ERRINTR))
-		t3_slow_intr_handler(adap);
-
-	if (likely(map & 1))
-		napi_schedule(&qs0->napi);
-
-	if (map & 2)
-		napi_schedule(&adap->sge.qs[1].napi);
-
-	spin_unlock(&q0->lock);
-	return IRQ_HANDLED;
+        if (unlikely(map & F_ERRINTR)) {
+		spin_lock(&q0->lock);
+		(void)t3_slow_intr_handler(adap);
+		spin_unlock(&q0->lock);
+	}
+
+        for_each_port(adap, i) {
+		struct port_info *p = adap2pinfo(adap, i);
+		int j;
+
+		for (j = p->first_qset; j < p->first_qset + p->nqsets; j++, qset++) {
+                	if (map & (1 << qset)) {
+#if !defined(NAPI_UPDATE)
+				struct net_device *dev = adap->sge.qs[qset].netdev;
+#endif
+
+				spin_lock(&adap->sge.qs[qset].rspq.lock);
+#if defined(NAPI_UPDATE)
+                        	napi_schedule(&adap->sge.qs[qset].napi);
+#else
+
+                        	if (likely(__netif_rx_schedule_prep(dev)))
+                                	__netif_rx_schedule(dev);
+#endif
+				spin_unlock(&adap->sge.qs[qset].rspq.lock);
+			}
+                }
+	}
+        return IRQ_HANDLED;
 }
 
 /**
@@ -2781,7 +3714,7 @@
  *	(MSI-X, MSI, or legacy) and whether NAPI will be used to service the
  *	response queues.
  */
-irq_handler_t t3_intr_handler(struct adapter *adap, int polling)
+intr_handler_t t3_intr_handler(adapter_t *adap, int polling)
 {
 	if (adap->flags & USING_MSIX)
 		return polling ? t3_sge_intr_msix_napi : t3_sge_intr_msix;
@@ -2807,10 +3740,10 @@
  *
  *	Interrupt handler for SGE asynchronous (non-data) events.
  */
-void t3_sge_err_intr_handler(struct adapter *adapter)
+void t3_sge_err_intr_handler(adapter_t *adapter)
 {
-	unsigned int v, status = t3_read_reg(adapter, A_SG_INT_CAUSE) &
-				 ~F_FLEMPTY;
+	unsigned int v, status = (t3_read_reg(adapter, A_SG_INT_CAUSE)
+				  & ~(F_FLEMPTY|F_RSPQSTARVE));
 
 	if (status & SGE_PARERR)
 		CH_ALERT(adapter, "SGE parity error (0x%x)\n",
@@ -2818,7 +3751,6 @@
 	if (status & SGE_FRAMINGERR)
 		CH_ALERT(adapter, "SGE framing error (0x%x)\n",
 			 status & SGE_FRAMINGERR);
-
 	if (status & F_RSPQCREDITOVERFOW)
 		CH_ALERT(adapter, "SGE response queue credit overflow\n");
 
@@ -2826,19 +3758,55 @@
 		v = t3_read_reg(adapter, A_SG_RSPQ_FL_STATUS);
 
 		CH_ALERT(adapter,
-			 "packet delivered to disabled response queue "
-			 "(0x%x)\n", (v >> S_RSPQ0DISABLED) & 0xff);
+			 "packet delivered to disabled response queue (0x%x)\n",
+			 (v >> S_RSPQ0DISABLED) & 0xff);
 	}
 
 	if (status & (F_HIPIODRBDROPERR | F_LOPIODRBDROPERR))
-		CH_ALERT(adapter, "SGE dropped %s priority doorbell\n",
-			 status & F_HIPIODRBDROPERR ? "high" : "lo");
+		queue_work(cxgb3_wq, &adapter->db_drop_task);
+
+	if (status & (F_HIPRIORITYDBFULL | F_LOPRIORITYDBFULL))
+		queue_work(cxgb3_wq, &adapter->db_full_task);
+
+	if (status & (F_HIPRIORITYDBEMPTY | F_LOPRIORITYDBEMPTY))
+		queue_work(cxgb3_wq, &adapter->db_empty_task);
 
 	t3_write_reg(adapter, A_SG_INT_CAUSE, status);
-	if (status &  SGE_FATALERR)
+	if (status & SGE_FATALERR)
 		t3_fatal_err(adapter);
 }
 
+/* Update offload traffic scheduler for a particular port */
+static void update_max_bw(struct sge_qset *qs, struct port_info *pi)
+{
+	struct sge_txq *q = &qs->txq[TXQ_ETH];
+	int max_bw, update_bw;
+
+	if (!netif_carrier_ok(qs->netdev))
+		return;
+
+	if ((q->cntxt_id - FW_TUNNEL_SGEEC_START) != pi->first_qset)
+		return;
+
+	max_bw = pi->link_config.speed * 940;
+
+	/* use q->in_use as an indicator of ongoing NIC traffic */
+	update_bw = ((q->in_use && pi->max_ofld_bw == max_bw) ||
+		     (!q->in_use && pi->max_ofld_bw < max_bw));
+
+	if (update_bw) {
+		pi->max_ofld_bw = q->in_use ?
+				  pi->link_config.speed * 470 :
+				  pi->link_config.speed * 940;
+		t3_config_sched(pi->adapter, pi->max_ofld_bw, pi->port_id);
+#ifdef T3_TRACE
+		T3_TRACE3(pi->adapter->tb[q->cntxt_id & 7],
+			  "%s: updating max bw to %d for port %d",
+			  __func__, pi->max_ofld_bw, pi->port_id);
+#endif
+	}
+}
+
 /**
  *	sge_timer_tx - perform periodic maintenance of an SGE qset
  *	@data: the SGE queue set to maintain
@@ -2846,7 +3814,7 @@
  *	Runs periodically from a timer to perform maintenance of an SGE queue
  *	set.  It performs two tasks:
  *
- *	Cleans up any completed Tx descriptors that may still be pending.
+ *	a) Cleans up any completed Tx descriptors that may still be pending.
  *	Normal descriptor cleanup happens when new packets are added to a Tx
  *	queue so this timer is relatively infrequent and does any cleanup only
  *	if the Tx queue has not seen any new packets in a while.  We make a
@@ -2856,6 +3824,8 @@
  *	up).  Since control queues use immediate data exclusively we don't
  *	bother cleaning them up here.
  *
+ *	b) Ring doorbells for T304 tunnel queues since we have seen doorbell
+ *	fifo overflows and the FW doesn't implement any recovery scheme yet.
  */
 static void sge_timer_tx(unsigned long data)
 {
@@ -2865,70 +3835,69 @@
 	unsigned int tbd[SGE_TXQ_PER_SET] = {0, 0};
 	unsigned long next_period;
 
-	if (__netif_tx_trylock(qs->tx_q)) {
-                tbd[TXQ_ETH] = reclaim_completed_tx(adap, &qs->txq[TXQ_ETH],
-                                                     TX_RECLAIM_TIMER_CHUNK);
-		__netif_tx_unlock(qs->tx_q);
+	if (spin_trylock(&qs->txq[TXQ_ETH].lock)) {
+		struct sge_txq *q = &qs->txq[TXQ_ETH];
+
+		if (q->eth_coalesce_idx)
+			ship_tx_pkt_coalesce_wr(adap, q);
+
+		tbd[TXQ_ETH] = reclaim_completed_tx(adap, &qs->txq[TXQ_ETH], TX_RECLAIM_TIMER_CHUNK);
+		spin_unlock(&qs->txq[TXQ_ETH].lock);
 	}
-
 	if (spin_trylock(&qs->txq[TXQ_OFLD].lock)) {
-		tbd[TXQ_OFLD] = reclaim_completed_tx(adap, &qs->txq[TXQ_OFLD],
-						     TX_RECLAIM_TIMER_CHUNK);
+		tbd[TXQ_OFLD] = reclaim_completed_tx(adap, &qs->txq[TXQ_OFLD], TX_RECLAIM_TIMER_CHUNK);
 		spin_unlock(&qs->txq[TXQ_OFLD].lock);
 	}
 
-	next_period = TX_RECLAIM_PERIOD >>
-                      (max(tbd[TXQ_ETH], tbd[TXQ_OFLD]) /
-                      TX_RECLAIM_TIMER_CHUNK);
+	if (adap->params.nports > 2)
+		update_max_bw(qs, pi);
+
+	if (adap->params.nports > 2) {
+		int i;
+
+		for_each_port(adap, i) {
+			struct net_device *dev = adap->port[i];
+			const struct port_info *pi = netdev_priv(dev);
+
+			t3_write_reg(adap, A_SG_KDOORBELL,
+				     F_SELEGRCNTX |
+				     (FW_TUNNEL_SGEEC_START + pi->first_qset));
+		}
+	}
+	next_period = TX_RECLAIM_PERIOD >> (max(tbd[TXQ_ETH], tbd[TXQ_OFLD]) / TX_RECLAIM_TIMER_CHUNK);
 	mod_timer(&qs->tx_reclaim_timer, jiffies + next_period);
 }
 
 /*
- *	sge_timer_rx - perform periodic maintenance of an SGE qset
- *	@data: the SGE queue set to maintain
+ *      sge_timer_rx - perform periodic maintenance of an SGE qset
+ *      @data: the SGE queue set to maintain
  *
- *	a) Replenishes Rx queues that have run out due to memory shortage.
- *	Normally new Rx buffers are added when existing ones are consumed but
- *	when out of memory a queue can become empty.  We try to add only a few
- *	buffers here, the queue will be replenished fully as these new buffers
- *	are used up if memory shortage has subsided.
+ *      a) Replenishes Rx queues that have run out due to memory shortage.
+ *      Normally new Rx buffers are added when existing ones are consumed but
+ *      when out of memory a queue can become empty.  We try to add only a few
+ *      buffers here, the queue will be replenished fully as these new buffers
+ *      are used up if memory shortage has subsided.
  *
- *	b) Return coalesced response queue credits in case a response queue is
- *	starved.
+ *      b) Return coalesced response queue credits in case a response queue is
+ *      starved.
  *
  */
 static void sge_timer_rx(unsigned long data)
 {
-	spinlock_t *lock;
-	struct sge_qset *qs = (struct sge_qset *)data;
-	struct port_info *pi = netdev_priv(qs->netdev);
-	struct adapter *adap = pi->adapter;
-	u32 status;
-
-	lock = adap->params.rev > 0 ?
+        spinlock_t *lock;
+        struct sge_qset *qs = (struct sge_qset *)data;
+        struct port_info *pi = netdev_priv(qs->netdev);
+        struct adapter *adap = pi->adapter;
+
+        lock = adap->params.rev > 0 ?
 	       &qs->rspq.lock : &adap->sge.qs[0].rspq.lock;
 
-	if (!spin_trylock_irq(lock))
+        if (!spin_trylock_irq(lock))
 		goto out;
 
-	if (napi_is_scheduled(&qs->napi))
+        if (napi_is_scheduled(qs))
 		goto unlock;
 
-	if (adap->params.rev < 4) {
-		status = t3_read_reg(adap, A_SG_RSPQ_FL_STATUS);
-
-		if (status & (1 << qs->rspq.cntxt_id)) {
-			qs->rspq.starved++;
-			if (qs->rspq.credits) {
-				qs->rspq.credits--;
-				refill_rspq(adap, &qs->rspq, 1);
-				qs->rspq.restarted++;
-				t3_write_reg(adap, A_SG_RSPQ_FL_STATUS,
-					     1 << qs->rspq.cntxt_id);
-			}
-		}
-	}
-
 	if (qs->fl[0].credits < qs->fl[0].size)
 		__refill_fl(adap, &qs->fl[0]);
 	if (qs->fl[1].credits < qs->fl[1].size)
@@ -2937,7 +3906,7 @@
 unlock:
 	spin_unlock_irq(lock);
 out:
-	mod_timer(&qs->rx_reclaim_timer, jiffies + RX_RECLAIM_PERIOD);
+        mod_timer(&qs->rx_reclaim_timer, jiffies + RX_RECLAIM_PERIOD);
 }
 
 /**
@@ -2950,9 +3919,17 @@
  */
 void t3_update_qset_coalesce(struct sge_qset *qs, const struct qset_params *p)
 {
-	qs->rspq.holdoff_tmr = max(p->coalesce_usecs * 10, 1U);/* can't be 0 */
-	qs->rspq.polling = p->polling;
-	qs->napi.poll = p->polling ? napi_rx_handler : ofld_poll;
+	if (!qs->netdev)
+		return;
+
+	qs->rspq.holdoff_tmr = max(p->coalesce_usecs * 10, 1U); // can't be 0
+	qs->rspq.flags |= (p->polling ? USING_POLLING : 0);
+#if defined(NAPI_UPDATE)
+	qs->napi.poll = 
+#else
+	qs->netdev->poll =
+#endif
+		p->polling ? napi_rx_handler : ofld_poll;
 }
 
 /**
@@ -2971,36 +3948,42 @@
  *	Tx queues.  The Tx queues are assigned roles in the order Ethernet
  *	queue, offload queue, and control queue.
  */
-int t3_sge_alloc_qset(struct adapter *adapter, unsigned int id, int nports,
-		      int irq_vec_idx, const struct qset_params *p,
-		      int ntxq, struct net_device *dev,
+int t3_sge_alloc_qset(adapter_t *adapter, unsigned int id, int nports,
+	       	      int irq_vec_idx, const struct qset_params *p,
+		      int ntxq, struct net_device *netdev,
 		      struct netdev_queue *netdevq)
 {
-	int i, avail, ret = -ENOMEM;
+#if !defined(NAPI_UPDATE)
+	struct port_info *pi = netdev_priv(netdev);
+#endif
+	int i, ret = -ENOMEM;
 	struct sge_qset *q = &adapter->sge.qs[id];
 
 	init_qset_cntxt(q, id);
-	setup_timer(&q->tx_reclaim_timer, sge_timer_tx, (unsigned long)q);
-	setup_timer(&q->rx_reclaim_timer, sge_timer_rx, (unsigned long)q);
+	init_timer(&q->tx_reclaim_timer);
+	init_timer(&q->rx_reclaim_timer);
+	q->tx_reclaim_timer.data = q->rx_reclaim_timer.data = (unsigned long)q;
+	q->tx_reclaim_timer.function = sge_timer_tx;
+	q->rx_reclaim_timer.function = sge_timer_rx;
 
 	q->fl[0].desc = alloc_ring(adapter->pdev, p->fl_size,
 				   sizeof(struct rx_desc),
 				   sizeof(struct rx_sw_desc),
 				   &q->fl[0].phys_addr, &q->fl[0].sdesc);
-	if (!q->fl[0].desc)
+	if (!q->fl[0].desc && p->fl_size)
 		goto err;
 
 	q->fl[1].desc = alloc_ring(adapter->pdev, p->jumbo_size,
 				   sizeof(struct rx_desc),
 				   sizeof(struct rx_sw_desc),
 				   &q->fl[1].phys_addr, &q->fl[1].sdesc);
-	if (!q->fl[1].desc)
+	if (!q->fl[1].desc && p->jumbo_size)
 		goto err;
 
 	q->rspq.desc = alloc_ring(adapter->pdev, p->rspq_size,
 				  sizeof(struct rsp_desc), 0,
 				  &q->rspq.phys_addr, NULL);
-	if (!q->rspq.desc)
+	if (!q->rspq.desc && p->rspq_size)
 		goto err;
 
 	for (i = 0; i < ntxq; ++i) {
@@ -3021,8 +4004,14 @@
 		q->txq[i].size = p->txq_size[i];
 		spin_lock_init(&q->txq[i].lock);
 		skb_queue_head_init(&q->txq[i].sendq);
+		q->txq[i].sched_max = 100;
 	}
 
+	q->txq[TXQ_ETH].eth_coalesce_sdesc = kcalloc(p->txq_size[TXQ_ETH],
+			sizeof(struct eth_coalesce_sw_desc), GFP_KERNEL);
+	if (!q->txq[TXQ_ETH].eth_coalesce_sdesc)
+		goto err;
+
 	tasklet_init(&q->txq[TXQ_OFLD].qresume_tsk, restart_offloadq,
 		     (unsigned long)q);
 	tasklet_init(&q->txq[TXQ_CTRL].qresume_tsk, restart_ctrlq,
@@ -3035,10 +4024,9 @@
 	q->rspq.gen = 1;
 	q->rspq.size = p->rspq_size;
 	spin_lock_init(&q->rspq.lock);
-	skb_queue_head_init(&q->rspq.rx_queue);
 
 	q->txq[TXQ_ETH].stop_thres = nports *
-	    flits_to_desc(sgl_len(MAX_SKB_FRAGS + 1) + 3);
+		flits_to_desc(sgl_len(MAX_SKB_FRAGS + 1) + 3);
 
 #if FL0_PG_CHUNK_SIZE > 0
 	q->fl[0].buf_size = FL0_PG_CHUNK_SIZE;
@@ -3048,7 +4036,13 @@
 #if FL1_PG_CHUNK_SIZE > 0
 	q->fl[1].buf_size = FL1_PG_CHUNK_SIZE;
 #else
-	q->fl[1].buf_size = is_offload(adapter) ?
+	q->fl[1].buf_size =
+	    /*
+	     * For versions of the driver which can support TOE, the hardware
+	     * can drop up to 16KB into memory.  This really ought to be
+	     * covered by a different predicate ...
+	     */
+	    is_offload(adapter) ?
 		(16 * 1024) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) :
 		MAX_FRAME_SIZE + 2 + sizeof(struct cpl_rx_pkt);
 #endif
@@ -3060,7 +4054,7 @@
 	q->fl[0].alloc_size = FL0_PG_ALLOC_SIZE;
 	q->fl[1].alloc_size = FL1_PG_ALLOC_SIZE;
 
-	spin_lock_irq(&adapter->sge.reg_lock);
+	spin_lock(&adapter->sge.reg_lock);
 
 	/* FL threshold comparison uses < */
 	ret = t3_sge_init_rspcntxt(adapter, q->rspq.cntxt_id, irq_vec_idx,
@@ -3104,28 +4098,32 @@
 			goto err_unlock;
 	}
 
-	spin_unlock_irq(&adapter->sge.reg_lock);
+	spin_unlock(&adapter->sge.reg_lock);
 
 	q->adap = adapter;
-	q->netdev = dev;
+	q->netdev = netdev;
 	q->tx_q = netdevq;
 	t3_update_qset_coalesce(q, p);
-
-	avail = refill_fl(adapter, &q->fl[0], q->fl[0].size,
-			  GFP_KERNEL | __GFP_COMP);
-	if (!avail) {
+	q->lro.enabled = p->lro;
+
+#if !defined(NAPI_UPDATE)
+	/* Link the current queue to the corresponding dummy netdevice */
+	pi->qs = q;
+#endif
+
+	refill_fl(adapter, &q->fl[0], q->fl[0].size, GFP_KERNEL | FL_GFP_FLAGS);
+	if (!q->fl[0].credits) {
 		CH_ALERT(adapter, "free list queue 0 initialization failed\n");
 		goto err;
 	}
-	if (avail < q->fl[0].size)
+	if (q->fl[0].credits < q->fl[0].size)
 		CH_WARN(adapter, "free list queue 0 enabled with %d credits\n",
-			avail);
-
-	avail = refill_fl(adapter, &q->fl[1], q->fl[1].size,
-			  GFP_KERNEL | __GFP_COMP);
-	if (avail < q->fl[1].size)
+			q->fl[0].credits);
+
+	refill_fl(adapter, &q->fl[1], q->fl[1].size, GFP_KERNEL | FL_GFP_FLAGS);
+	if (q->fl[1].credits < q->fl[1].size)
 		CH_WARN(adapter, "free list queue 1 enabled with %d credits\n",
-			avail);
+			q->fl[1].credits);
 	refill_rspq(adapter, &q->rspq, q->rspq.size - 1);
 
 	t3_write_reg(adapter, A_SG_GTS, V_RSPQ(q->rspq.cntxt_id) |
@@ -3134,7 +4132,7 @@
 	return 0;
 
 err_unlock:
-	spin_unlock_irq(&adapter->sge.reg_lock);
+	spin_unlock(&adapter->sge.reg_lock);
 err:
 	t3_free_qset(adapter, q);
 	return ret;
@@ -3148,17 +4146,18 @@
  */
 void t3_start_sge_timers(struct adapter *adap)
 {
-	int i;
-
-	for (i = 0; i < SGE_QSETS; ++i) {
-		struct sge_qset *q = &adap->sge.qs[i];
-
-	if (q->tx_reclaim_timer.function)
-		mod_timer(&q->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);
-
-	if (q->rx_reclaim_timer.function)
-		mod_timer(&q->rx_reclaim_timer, jiffies + RX_RECLAIM_PERIOD);
-	}
+        int i;
+
+        for (i = 0; i < SGE_QSETS; ++i) {
+                struct sge_qset *q = &adap->sge.qs[i];
+
+                if (q->tx_reclaim_timer.function)
+                        mod_timer(&q->tx_reclaim_timer, jiffies + TX_RECLAIM_PERIOD);
+
+                if (q->rx_reclaim_timer.function)
+                        mod_timer(&q->rx_reclaim_timer, jiffies + RX_RECLAIM_PERIOD);
+
+        }
 }
 
 /**
@@ -3176,18 +4175,19 @@
 
 		if (q->tx_reclaim_timer.function)
 			del_timer_sync(&q->tx_reclaim_timer);
-		if (q->rx_reclaim_timer.function)
-			del_timer_sync(&q->rx_reclaim_timer);
+
+        	if (q->rx_reclaim_timer.function)
+                	del_timer_sync(&q->rx_reclaim_timer);
 	}
 }
-
+	
 /**
  *	t3_free_sge_resources - free SGE resources
  *	@adap: the adapter
  *
  *	Frees resources used by the SGE queue sets.
  */
-void t3_free_sge_resources(struct adapter *adap)
+void t3_free_sge_resources(adapter_t *adap)
 {
 	int i;
 
@@ -3202,7 +4202,7 @@
  *	Enables the SGE for DMAs.  This is the last step in starting packet
  *	transfers.
  */
-void t3_sge_start(struct adapter *adap)
+void t3_sge_start(adapter_t *adap)
 {
 	t3_set_reg_field(adap, A_SG_CONTROL, F_GLOBALENABLE, F_GLOBALENABLE);
 }
@@ -3220,7 +4220,7 @@
  *	later from process context, at which time the tasklets will be stopped
  *	if they are still running.
  */
-void t3_sge_stop(struct adapter *adap)
+void t3_sge_stop(adapter_t *adap)
 {
 	t3_set_reg_field(adap, A_SG_CONTROL, F_GLOBALENABLE, 0);
 	if (!in_interrupt()) {
@@ -3245,14 +4245,14 @@
  *	top-level must request those individually.  We also do not enable DMA
  *	here, that should be done after the queues have been set up.
  */
-void t3_sge_init(struct adapter *adap, struct sge_params *p)
+void t3_sge_init(adapter_t *adap, struct sge_params *p)
 {
 	unsigned int ctrl, ups = ffs(pci_resource_len(adap->pdev, 2) >> 12);
 
 	ctrl = F_DROPPKT | V_PKTSHIFT(2) | F_FLMODE | F_AVOIDCQOVFL |
-	    F_CQCRDTCTRL | F_CONGMODE | F_TNLFLMODE | F_FATLPERREN |
-	    V_HOSTPAGESIZE(PAGE_SHIFT - 11) | F_BIGENDIANINGRESS |
-	    V_USERSPACESIZE(ups ? ups - 1 : 0) | F_ISCSICOALESCING;
+	       F_CQCRDTCTRL | F_CONGMODE | F_TNLFLMODE | F_FATLPERREN |
+	       V_HOSTPAGESIZE(PAGE_SHIFT - 11) | F_BIGENDIANINGRESS |
+	       V_USERSPACESIZE(ups ? ups - 1 : 0) | F_ISCSICOALESCING;
 #if SGE_NUM_GENBITS == 1
 	ctrl |= F_EGRGENCTRL;
 #endif
@@ -3284,25 +4284,38 @@
  *	defaults for the assorted SGE parameters, which admins can change until
  *	they are used to initialize the SGE.
  */
-void t3_sge_prep(struct adapter *adap, struct sge_params *p)
+void __devinit t3_sge_prep(adapter_t *adap, struct sge_params *p)
 {
 	int i;
 
 	p->max_pkt_size = (16 * 1024) - sizeof(struct cpl_rx_data) -
-	    SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
+		SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
 
 	for (i = 0; i < SGE_QSETS; ++i) {
 		struct qset_params *q = p->qset + i;
 
+		if (adap->params.nports > 2)
+			q->coalesce_usecs = 50;
+		else
+			q->coalesce_usecs = 5;
+
 		q->polling = adap->params.rev > 0;
-		q->coalesce_usecs = 5;
-		q->rspq_size = 1024;
+#ifdef DISABLE_LRO
+		q->lro = 0;
+#else
+		q->lro = 1;
+#endif
+
 		q->fl_size = 1024;
- 		q->jumbo_size = 512;
+		q->jumbo_size = 512;
+
 		q->txq_size[TXQ_ETH] = 1024;
 		q->txq_size[TXQ_OFLD] = 1024;
 		q->txq_size[TXQ_CTRL] = 256;
 		q->cong_thres = 0;
+
+		q->rspq_size = (q->txq_size[TXQ_ETH] / 32) +
+				q->fl_size + q->jumbo_size;
 	}
 
 	spin_lock_init(&adap->sge.reg_lock);
diff --git a/drivers/net/cxgb3/sge_defs.h b/drivers/net/cxgb3/sge_defs.h
--- a/drivers/net/cxgb3/sge_defs.h
+++ b/drivers/net/cxgb3/sge_defs.h
@@ -252,4 +252,4 @@
 #define V_RSPD_INR_VEC(x) ((x) << S_RSPD_INR_VEC)
 #define G_RSPD_INR_VEC(x) (((x) >> S_RSPD_INR_VEC) & M_RSPD_INR_VEC)
 
-#endif				/* _SGE_DEFS_H */
+#endif /* _SGE_DEFS_H */
diff --git a/drivers/net/cxgb3/t3_cpl.h b/drivers/net/cxgb3/t3_cpl.h
--- a/drivers/net/cxgb3/t3_cpl.h
+++ b/drivers/net/cxgb3/t3_cpl.h
@@ -1,34 +1,16 @@
 /*
- * Copyright (c) 2004-2008 Chelsio, Inc. All rights reserved.
+ * Definitions of the CPL 5 commands and status codes.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2004-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
+ * Written by Dimitris Michailidis (dm@chelsio.com)
  *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #ifndef T3_CPL_H
 #define T3_CPL_H
 
@@ -37,121 +19,125 @@
 #endif
 
 enum CPL_opcode {
-	CPL_PASS_OPEN_REQ = 0x1,
-	CPL_PASS_ACCEPT_RPL = 0x2,
-	CPL_ACT_OPEN_REQ = 0x3,
-	CPL_SET_TCB = 0x4,
-	CPL_SET_TCB_FIELD = 0x5,
-	CPL_GET_TCB = 0x6,
-	CPL_PCMD = 0x7,
-	CPL_CLOSE_CON_REQ = 0x8,
+	CPL_PASS_OPEN_REQ     = 0x1,
+	CPL_PASS_ACCEPT_RPL   = 0x2,
+	CPL_ACT_OPEN_REQ      = 0x3,
+	CPL_SET_TCB           = 0x4,
+	CPL_SET_TCB_FIELD     = 0x5,
+	CPL_GET_TCB           = 0x6,
+	CPL_PCMD              = 0x7,
+	CPL_CLOSE_CON_REQ     = 0x8,
 	CPL_CLOSE_LISTSRV_REQ = 0x9,
-	CPL_ABORT_REQ = 0xA,
-	CPL_ABORT_RPL = 0xB,
-	CPL_TX_DATA = 0xC,
-	CPL_RX_DATA_ACK = 0xD,
-	CPL_TX_PKT = 0xE,
-	CPL_RTE_DELETE_REQ = 0xF,
-	CPL_RTE_WRITE_REQ = 0x10,
-	CPL_RTE_READ_REQ = 0x11,
-	CPL_L2T_WRITE_REQ = 0x12,
-	CPL_L2T_READ_REQ = 0x13,
-	CPL_SMT_WRITE_REQ = 0x14,
-	CPL_SMT_READ_REQ = 0x15,
-	CPL_TX_PKT_LSO = 0x16,
-	CPL_PCMD_READ = 0x17,
-	CPL_BARRIER = 0x18,
-	CPL_TID_RELEASE = 0x1A,
+	CPL_ABORT_REQ         = 0xA,
+	CPL_ABORT_RPL         = 0xB,
+	CPL_TX_DATA           = 0xC,
+	CPL_RX_DATA_ACK       = 0xD,
+	CPL_TX_PKT            = 0xE,
+	CPL_RTE_DELETE_REQ    = 0xF,
+	CPL_RTE_WRITE_REQ     = 0x10,
+	CPL_RTE_READ_REQ      = 0x11,
+	CPL_L2T_WRITE_REQ     = 0x12,
+	CPL_L2T_READ_REQ      = 0x13,
+	CPL_SMT_WRITE_REQ     = 0x14,
+	CPL_SMT_READ_REQ      = 0x15,
+	CPL_TX_PKT_LSO        = 0x16,
+	CPL_PCMD_READ         = 0x17,
+	CPL_BARRIER           = 0x18,
+	CPL_TID_RELEASE       = 0x1A,
 
 	CPL_CLOSE_LISTSRV_RPL = 0x20,
-	CPL_ERROR = 0x21,
-	CPL_GET_TCB_RPL = 0x22,
-	CPL_L2T_WRITE_RPL = 0x23,
-	CPL_PCMD_READ_RPL = 0x24,
-	CPL_PCMD_RPL = 0x25,
-	CPL_PEER_CLOSE = 0x26,
-	CPL_RTE_DELETE_RPL = 0x27,
-	CPL_RTE_WRITE_RPL = 0x28,
-	CPL_RX_DDP_COMPLETE = 0x29,
-	CPL_RX_PHYS_ADDR = 0x2A,
-	CPL_RX_PKT = 0x2B,
-	CPL_RX_URG_NOTIFY = 0x2C,
-	CPL_SET_TCB_RPL = 0x2D,
-	CPL_SMT_WRITE_RPL = 0x2E,
-	CPL_TX_DATA_ACK = 0x2F,
+	CPL_ERROR             = 0x21,
+	CPL_GET_TCB_RPL       = 0x22,
+	CPL_L2T_WRITE_RPL     = 0x23,
+	CPL_PCMD_READ_RPL     = 0x24,
+	CPL_PCMD_RPL          = 0x25,
+	CPL_PEER_CLOSE        = 0x26,
+	CPL_RTE_DELETE_RPL    = 0x27,
+	CPL_RTE_WRITE_RPL     = 0x28,
+	CPL_RX_DDP_COMPLETE   = 0x29,
+	CPL_RX_PHYS_ADDR      = 0x2A,
+	CPL_RX_PKT            = 0x2B,
+	CPL_RX_URG_NOTIFY     = 0x2C,
+	CPL_SET_TCB_RPL       = 0x2D,
+	CPL_SMT_WRITE_RPL     = 0x2E,
+	CPL_TX_DATA_ACK       = 0x2F,
 
-	CPL_ABORT_REQ_RSS = 0x30,
-	CPL_ABORT_RPL_RSS = 0x31,
-	CPL_CLOSE_CON_RPL = 0x32,
-	CPL_ISCSI_HDR = 0x33,
-	CPL_L2T_READ_RPL = 0x34,
-	CPL_RDMA_CQE = 0x35,
+	CPL_ABORT_REQ_RSS     = 0x30,
+	CPL_ABORT_RPL_RSS     = 0x31,
+	CPL_CLOSE_CON_RPL     = 0x32,
+	CPL_ISCSI_HDR         = 0x33,
+	CPL_L2T_READ_RPL      = 0x34,
+	CPL_RDMA_CQE          = 0x35,
 	CPL_RDMA_CQE_READ_RSP = 0x36,
-	CPL_RDMA_CQE_ERR = 0x37,
-	CPL_RTE_READ_RPL = 0x38,
-	CPL_RX_DATA = 0x39,
+	CPL_RDMA_CQE_ERR      = 0x37,
+	CPL_RTE_READ_RPL      = 0x38,
+	CPL_RX_DATA           = 0x39,
 
-	CPL_ACT_OPEN_RPL = 0x40,
-	CPL_PASS_OPEN_RPL = 0x41,
-	CPL_RX_DATA_DDP = 0x42,
-	CPL_SMT_READ_RPL = 0x43,
+	CPL_ACT_OPEN_RPL      = 0x40,
+	CPL_PASS_OPEN_RPL     = 0x41,
+	CPL_RX_DATA_DDP       = 0x42,
+	CPL_SMT_READ_RPL      = 0x43,
 
-	CPL_ACT_ESTABLISH = 0x50,
-	CPL_PASS_ESTABLISH = 0x51,
+	CPL_ACT_ESTABLISH     = 0x50,
+	CPL_PASS_ESTABLISH    = 0x51,
 
-	CPL_PASS_ACCEPT_REQ = 0x70,
+	CPL_PASS_ACCEPT_REQ   = 0x70,
 
-	CPL_ASYNC_NOTIF = 0x80,	/* fake opcode for async notifications */
+	CPL_ASYNC_NOTIF       = 0x80, /* fake opcode for async notifications */
 
-	CPL_TX_DMA_ACK = 0xA0,
-	CPL_RDMA_READ_REQ = 0xA1,
-	CPL_RDMA_TERMINATE = 0xA2,
-	CPL_TRACE_PKT = 0xA3,
-	CPL_RDMA_EC_STATUS = 0xA5,
+	CPL_TX_DMA_ACK        = 0xA0,
+	CPL_RDMA_READ_REQ     = 0xA1,
+	CPL_RDMA_TERMINATE    = 0xA2,
+	CPL_TRACE_PKT         = 0xA3,
+	CPL_RDMA_EC_STATUS    = 0xA5,
+	CPL_SGE_EC_CR_RETURN  = 0xA6,
 
-	NUM_CPL_CMDS		/* must be last and previous entries must be sorted */
+	NUM_CPL_CMDS    /* must be last and previous entries must be sorted */
 };
 
 enum CPL_error {
-	CPL_ERR_NONE = 0,
-	CPL_ERR_TCAM_PARITY = 1,
-	CPL_ERR_TCAM_FULL = 3,
-	CPL_ERR_CONN_RESET = 20,
-	CPL_ERR_CONN_EXIST = 22,
-	CPL_ERR_ARP_MISS = 23,
-	CPL_ERR_BAD_SYN = 24,
-	CPL_ERR_CONN_TIMEDOUT = 30,
-	CPL_ERR_XMIT_TIMEDOUT = 31,
-	CPL_ERR_PERSIST_TIMEDOUT = 32,
-	CPL_ERR_FINWAIT2_TIMEDOUT = 33,
+	CPL_ERR_NONE               = 0,
+	CPL_ERR_TCAM_PARITY        = 1,
+	CPL_ERR_TCAM_FULL          = 3,
+	CPL_ERR_CONN_RESET         = 20,
+	CPL_ERR_CONN_EXIST         = 22,
+	CPL_ERR_ARP_MISS           = 23,
+	CPL_ERR_BAD_SYN            = 24,
+	CPL_ERR_CONN_TIMEDOUT      = 30,
+	CPL_ERR_XMIT_TIMEDOUT      = 31,
+	CPL_ERR_PERSIST_TIMEDOUT   = 32,
+	CPL_ERR_FINWAIT2_TIMEDOUT  = 33,
 	CPL_ERR_KEEPALIVE_TIMEDOUT = 34,
-	CPL_ERR_RTX_NEG_ADVICE = 35,
+	CPL_ERR_RTX_NEG_ADVICE     = 35,
 	CPL_ERR_PERSIST_NEG_ADVICE = 36,
-	CPL_ERR_ABORT_FAILED = 42,
-	CPL_ERR_GENERAL = 99
+	CPL_ERR_ABORT_FAILED       = 42,
+	CPL_ERR_GENERAL            = 99
 };
 
 enum {
 	CPL_CONN_POLICY_AUTO = 0,
-	CPL_CONN_POLICY_ASK = 1,
+	CPL_CONN_POLICY_ASK  = 1,
+	CPL_CONN_POLICY_FILTER = 2,
 	CPL_CONN_POLICY_DENY = 3
 };
 
 enum {
-	ULP_MODE_NONE = 0,
-	ULP_MODE_ISCSI = 2,
-	ULP_MODE_RDMA = 4,
-	ULP_MODE_TCPDDP = 5
+	ULP_MODE_NONE          = 0,
+	ULP_MODE_TCP_DDP       = 1,
+	ULP_MODE_ISCSI         = 2,
+	ULP_MODE_RDMA          = 4,
+	ULP_MODE_TCPDDP        = 5
 };
 
 enum {
 	ULP_CRC_HEADER = 1 << 0,
-	ULP_CRC_DATA = 1 << 1
+	ULP_CRC_DATA   = 1 << 1
 };
 
 enum {
 	CPL_PASS_OPEN_ACCEPT,
-	CPL_PASS_OPEN_REJECT
+	CPL_PASS_OPEN_REJECT,
+	CPL_PASS_OPEN_ACCEPT_TNL
 };
 
 enum {
@@ -160,21 +146,21 @@
 	CPL_ABORT_POST_CLOSE_REQ = 2
 };
 
-enum {				/* TX_PKT_LSO ethernet types */
+enum {                     /* TX_PKT_LSO ethernet types */
 	CPL_ETH_II,
 	CPL_ETH_II_VLAN,
 	CPL_ETH_802_3,
 	CPL_ETH_802_3_VLAN
 };
 
-enum {				/* TCP congestion control algorithms */
+enum {                     /* TCP congestion control algorithms */
 	CONG_ALG_RENO,
 	CONG_ALG_TAHOE,
 	CONG_ALG_NEWRENO,
 	CONG_ALG_HIGHSPEED
 };
 
-enum {			/* RSS hash type */
+enum {                     /* RSS hash type */
 	RSS_HASH_NONE = 0,
 	RSS_HASH_2_TUPLE = 1,
 	RSS_HASH_4_TUPLE = 2,
@@ -191,13 +177,6 @@
 #define G_OPCODE(x) (((x) >> S_OPCODE) & 0xFF)
 #define G_TID(x)    ((x) & 0xFFFFFF)
 
-#define S_QNUM 0
-#define G_QNUM(x) (((x) >> S_QNUM) & 0xFFFF)
-
-#define S_HASHTYPE 22
-#define M_HASHTYPE 0x3
-#define G_HASHTYPE(x) (((x) >> S_HASHTYPE) & M_HASHTYPE)
-
 /* tid is assumed to be 24-bits */
 #define MK_OPCODE_TID(opcode, tid) (V_OPCODE(opcode) | (tid))
 
@@ -210,7 +189,7 @@
 	__be16 mss;
 	__u8 wsf;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	 __u8:5;
+	__u8 :5;
 	__u8 ecn:1;
 	__u8 sack:1;
 	__u8 tstamp:1;
@@ -218,7 +197,7 @@
 	__u8 tstamp:1;
 	__u8 sack:1;
 	__u8 ecn:1;
-	 __u8:5;
+	__u8 :5;
 #endif
 };
 
@@ -235,6 +214,14 @@
 	__be32 rss_hash_val;
 };
 
+#define S_HASHTYPE 22
+#define M_HASHTYPE 0x3
+#define G_HASHTYPE(x) (((x) >> S_HASHTYPE) & M_HASHTYPE)
+
+#define S_QNUM 0
+#define M_QNUM 0xFFFF
+#define G_QNUM(x) (((x) >> S_QNUM) & M_QNUM)
+
 #ifndef CHELSIO_FW
 struct work_request_hdr {
 	__be32 wr_hi;
@@ -257,6 +244,30 @@
 #define V_WR_BCNTLFLT(x) ((x) << S_WR_BCNTLFLT)
 #define G_WR_BCNTLFLT(x) (((x) >> S_WR_BCNTLFLT) & M_WR_BCNTLFLT)
 
+/*
+ * Applicable to BYPASS WRs only: the uP will add a CPL_BARRIER before
+ * and after the BYPASS WR if the ATOMIC bit is set.
+ */
+#define S_WR_ATOMIC	16
+#define V_WR_ATOMIC(x)	((x) << S_WR_ATOMIC)
+#define F_WR_ATOMIC	V_WR_ATOMIC(1U)
+
+/*
+ * Applicable to BYPASS WRs only: the uP will flush buffered non abort
+ * related WRs.
+ */
+#define S_WR_FLUSH	17
+#define V_WR_FLUSH(x)	((x) << S_WR_FLUSH)
+#define F_WR_FLUSH	V_WR_FLUSH(1U)
+
+#define S_WR_CHN	18
+#define V_WR_CHN(x)	((x) << S_WR_CHN)
+#define F_WR_CHN	V_WR_CHN(1U)
+
+#define S_WR_CHN_VLD	19
+#define V_WR_CHN_VLD(x)	((x) << S_WR_CHN_VLD)
+#define F_WR_CHN_VLD	V_WR_CHN_VLD(1U)
+
 #define S_WR_DATATYPE    20
 #define V_WR_DATATYPE(x) ((x) << S_WR_DATATYPE)
 #define F_WR_DATATYPE    V_WR_DATATYPE(1U)
@@ -402,6 +413,11 @@
 #define V_CPU_IDX(x) ((x) << S_CPU_IDX)
 #define G_CPU_IDX(x) (((x) >> S_CPU_IDX) & M_CPU_IDX)
 
+#define S_OPT1_VLAN    6
+#define M_OPT1_VLAN    0xFFF
+#define V_OPT1_VLAN(x) ((x) << S_OPT1_VLAN)
+#define G_OPT1_VLAN(x) (((x) >> S_OPT1_VLAN) & M_OPT1_VLAN)
+
 #define S_MAC_MATCH_VALID    18
 #define V_MAC_MATCH_VALID(x) ((x) << S_MAC_MATCH_VALID)
 #define F_MAC_MATCH_VALID    V_MAC_MATCH_VALID(1U)
@@ -489,7 +505,8 @@
 };
 
 struct cpl_pass_open_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 local_port;
 	__be16 peer_port;
 	__be32 local_ip;
@@ -499,7 +516,8 @@
 };
 
 struct cpl_pass_establish {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 local_port;
 	__be16 peer_port;
 	__be32 local_ip;
@@ -536,28 +554,29 @@
 #define G_TCPOPT_MSS(x)        (((x) >> 12) & 0xf)
 
 struct cpl_pass_accept_req {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 local_port;
 	__be16 peer_port;
 	__be32 local_ip;
 	__be32 peer_ip;
 	__be32 tos_tid;
 	struct tcp_options tcp_options;
-	__u8 dst_mac[6];
+	__u8  dst_mac[6];
 	__be16 vlan_tag;
-	__u8 src_mac[6];
+	__u8  src_mac[6];
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	 __u8:3;
-	__u8 addr_idx:3;
-	__u8 port_idx:1;
-	__u8 exact_match:1;
+	__u8  :3;
+	__u8  addr_idx:3;
+	__u8  port_idx:1;
+	__u8  exact_match:1;
 #else
-	__u8 exact_match:1;
-	__u8 port_idx:1;
-	__u8 addr_idx:3;
-	 __u8:3;
+	__u8  exact_match:1;
+	__u8  port_idx:1;
+	__u8  addr_idx:3;
+	__u8  :3;
 #endif
-	__u8 rsvd;
+	__u8  rsvd;
 	__be32 rcv_isn;
 	__be32 rsvd2;
 };
@@ -615,18 +634,20 @@
 #define G_AOPEN_IFF_VLAN(x) (((x) >> S_AOPEN_IFF_VLAN) & M_AOPEN_IFF_VLAN)
 
 struct cpl_act_open_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 local_port;
 	__be16 peer_port;
 	__be32 local_ip;
 	__be32 peer_ip;
 	__be32 atid;
-	__u8 rsvd[3];
-	__u8 status;
+	__u8  rsvd[3];
+	__u8  status;
 };
 
 struct cpl_act_establish {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 local_port;
 	__be16 peer_port;
 	__be32 local_ip;
@@ -646,7 +667,8 @@
 };
 
 struct cpl_get_tcb_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 rsvd;
 	__u8 status;
 	__be16 len;
@@ -655,8 +677,8 @@
 struct cpl_set_tcb {
 	WR_HDR;
 	union opcode_tid ot;
-	__u8 reply;
-	__u8 cpu_idx;
+	__u8  reply;
+	__u8  cpu_idx;
 	__be16 len;
 };
 
@@ -668,15 +690,16 @@
 struct cpl_set_tcb_field {
 	WR_HDR;
 	union opcode_tid ot;
-	__u8 reply;
-	__u8 cpu_idx;
+	__u8  reply;
+	__u8  cpu_idx;
 	__be16 word;
 	__be64 mask;
 	__be64 val;
 };
 
 struct cpl_set_tcb_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 rsvd[3];
 	__u8 status;
 };
@@ -689,9 +712,9 @@
 	__u8 src:1;
 	__u8 bundle:1;
 	__u8 channel:1;
-	 __u8:5;
+	__u8 :5;
 #else
-	 __u8:5;
+	__u8 :5;
 	__u8 channel:1;
 	__u8 bundle:1;
 	__u8 src:1;
@@ -700,9 +723,10 @@
 };
 
 struct cpl_pcmd_reply {
-	RSS_HDR union opcode_tid ot;
-	__u8 status;
-	__u8 rsvd;
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  status;
+	__u8  rsvd;
 	__be16 len;
 };
 
@@ -713,9 +737,10 @@
 };
 
 struct cpl_close_con_rpl {
-	RSS_HDR union opcode_tid ot;
-	__u8 rsvd[3];
-	__u8 status;
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  rsvd[3];
+	__u8  status;
 	__be32 snd_nxt;
 	__be32 rcv_nxt;
 };
@@ -723,53 +748,57 @@
 struct cpl_close_listserv_req {
 	WR_HDR;
 	union opcode_tid ot;
-	__u8 rsvd0;
-	__u8 cpu_idx;
+	__u8  rsvd0;
+	__u8  cpu_idx;
 	__be16 rsvd1;
 };
 
 struct cpl_close_listserv_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 rsvd[3];
 	__u8 status;
 };
 
 struct cpl_abort_req_rss {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 rsvd0;
-	__u8 rsvd1;
-	__u8 status;
-	__u8 rsvd2[6];
+	__u8  rsvd1;
+	__u8  status;
+	__u8  rsvd2[6];
 };
 
 struct cpl_abort_req {
 	WR_HDR;
 	union opcode_tid ot;
 	__be32 rsvd0;
-	__u8 rsvd1;
-	__u8 cmd;
-	__u8 rsvd2[6];
+	__u8  rsvd1;
+	__u8  cmd;
+	__u8  rsvd2[6];
 };
 
 struct cpl_abort_rpl_rss {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 rsvd0;
-	__u8 rsvd1;
-	__u8 status;
-	__u8 rsvd2[6];
+	__u8  rsvd1;
+	__u8  status;
+	__u8  rsvd2[6];
 };
 
 struct cpl_abort_rpl {
 	WR_HDR;
 	union opcode_tid ot;
 	__be32 rsvd0;
-	__u8 rsvd1;
-	__u8 cmd;
-	__u8 rsvd2[6];
+	__u8  rsvd1;
+	__u8  cmd;
+	__u8  rsvd2[6];
 };
 
 struct cpl_peer_close {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 rcv_nxt;
 };
 
@@ -783,8 +812,8 @@
 };
 
 /* tx_data_wr.flags fields */
-#define S_TX_ACK_PAGES	21
-#define M_TX_ACK_PAGES	0x7
+#define S_TX_ACK_PAGES    21
+#define M_TX_ACK_PAGES    0x7
 #define V_TX_ACK_PAGES(x) ((x) << S_TX_ACK_PAGES)
 #define G_TX_ACK_PAGES(x) (((x) >> S_TX_ACK_PAGES) & M_TX_ACK_PAGES)
 
@@ -863,45 +892,73 @@
 #define F_TX_IMM_DMA    V_TX_IMM_DMA(1U)
 
 struct cpl_tx_data_ack {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 ack_seq;
 };
 
 struct cpl_wr_ack {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 credits;
 	__be16 rsvd;
 	__be32 snd_nxt;
 	__be32 snd_una;
 };
 
+struct cpl_sge_ec_cr_return {
+	RSS_HDR
+	union opcode_tid ot;
+	__be16 sge_ec_id;
+	__u8 cr;
+	__u8 rsvd;
+};
+
 struct cpl_rdma_ec_status {
-	RSS_HDR union opcode_tid ot;
-	__u8 rsvd[3];
-	__u8 status;
+	RSS_HDR
+	union opcode_tid ot;
+	__u8  rsvd[3];
+	__u8  status;
 };
 
 struct mngt_pktsched_wr {
 	__be32 wr_hi;
 	__be32 wr_lo;
-	__u8 mngt_opcode;
-	__u8 rsvd[7];
-	__u8 sched;
-	__u8 idx;
-	__u8 min;
-	__u8 max;
-	__u8 binding;
-	__u8 rsvd1[3];
+	__u8  mngt_opcode;
+	__u8  rsvd[7];
+	__u8  sched;
+	__u8  idx;
+	__u8  min;
+	__u8  max;
+	__u8  binding;
+	__u8  rsvd1[3];
+};
+
+enum {
+ 	GPIO_EN_0 = 0x01,
+ 	PCIE_LINKDOWN = 0x02,
+ 	FW_EXCEPTION = 0x04
+};
+
+struct mngt_watchdog_wr {
+ 	__be32 wr_hi;
+ 	__be32 wr_lo;
+ 	__u8 mngt_opcode;
+ 	__u8 rsvd[7];
+ 	__u16 enable;
+ 	__be16 ac_or_rsvd1;
+ 	__be32 tval_or_rsvd2;
 };
 
 struct cpl_iscsi_hdr {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 pdu_len_ddp;
 	__be16 len;
 	__be32 seq;
 	__be16 urg;
-	__u8 rsvd;
-	__u8 status;
+	__u8  rsvd;
+	__u8  status;
 };
 
 /* cpl_iscsi_hdr.pdu_len_ddp fields */
@@ -915,23 +972,26 @@
 #define F_ISCSI_DDP    V_ISCSI_DDP(1U)
 
 struct cpl_rx_data {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 rsvd;
 	__be16 len;
 	__be32 seq;
 	__be16 urg;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	__u8 dack_mode:2;
-	__u8 psh:1;
-	__u8 heartbeat:1;
-	 __u8:4;
+	__u8  dack_mode:2;
+	__u8  psh:1;
+	__u8  heartbeat:1;
+	__u8  ddp_off:1;
+	__u8  :3;
 #else
-	 __u8:4;
-	__u8 heartbeat:1;
-	__u8 psh:1;
-	__u8 dack_mode:2;
+	__u8  :3;
+	__u8  ddp_off:1;
+	__u8  heartbeat:1;
+	__u8  psh:1;
+	__u8  dack_mode:2;
 #endif
-	__u8 status;
+	__u8  status;
 };
 
 struct cpl_rx_data_ack {
@@ -964,17 +1024,20 @@
 #define F_RX_DACK_CHANGE    V_RX_DACK_CHANGE(1U)
 
 struct cpl_rx_urg_notify {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 seq;
 };
 
 struct cpl_rx_ddp_complete {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 ddp_report;
 };
 
 struct cpl_rx_data_ddp {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be16 urg;
 	__be16 len;
 	__be32 seq;
@@ -1060,6 +1123,11 @@
 #define V_DDP_OFFSET(x) ((x) << S_DDP_OFFSET)
 #define G_DDP_OFFSET(x) (((x) >> S_DDP_OFFSET) & M_DDP_OFFSET)
 
+#define S_DDP_DACK_MODE    22
+#define M_DDP_DACK_MODE    0x3
+#define V_DDP_DACK_MODE(x) ((x) << S_DDP_DACK_MODE)
+#define G_DDP_DACK_MODE(x) (((x) >> S_DDP_DACK_MODE) & M_DDP_DACK_MODE)
+
 #define S_DDP_URG    24
 #define V_DDP_URG(x) ((x) << S_DDP_URG)
 #define F_DDP_URG    V_DDP_URG(1U)
@@ -1086,6 +1154,17 @@
 	__be32 len;
 };
 
+struct cpl_tx_pkt_coalesce {
+	__be32 cntrl;
+	__be32 len;
+	__be64 addr;
+};
+
+struct tx_pkt_coalesce_wr {
+	WR_HDR;
+	struct cpl_tx_pkt_coalesce cpl[0];
+};
+
 struct cpl_tx_pkt_lso {
 	WR_HDR;
 	__be32 cntrl;
@@ -1157,36 +1236,37 @@
 	__u8 rss_opcode;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u8 err:1;
-	 __u8:7;
+	__u8 :7;
 #else
-	 __u8:7;
+	__u8 :7;
 	__u8 err:1;
 #endif
 	__u8 rsvd0;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u8 qid:4;
-	 __u8:4;
+	__u8 :4;
 #else
-	 __u8:4;
+	__u8 :4;
 	__u8 qid:4;
 #endif
 	__be32 tstamp;
-#endif				/* CHELSIO_FW */
+#endif /* CHELSIO_FW */
 
-	__u8 opcode;
+	__u8  opcode;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	__u8 iff:4;
-	 __u8:4;
+	__u8  iff:4;
+	__u8  :4;
 #else
-	 __u8:4;
-	__u8 iff:4;
+	__u8  :4;
+	__u8  iff:4;
 #endif
-	__u8 rsvd[4];
+	__u8  rsvd[4];
 	__be16 len;
 };
 
 struct cpl_rx_pkt {
-	RSS_HDR __u8 opcode;
+	RSS_HDR
+	__u8 opcode;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u8 iff:4;
 	__u8 csum_valid:1;
@@ -1209,8 +1289,9 @@
 	WR_HDR;
 	union opcode_tid ot;
 	__be32 params;
-	__u8 rsvd[2];
-	__u8 dst_mac[6];
+	__u8  rsvd;
+	__u8  port_idx;
+	__u8  dst_mac[6];
 };
 
 /* cpl_l2t_write_req.params fields */
@@ -1235,7 +1316,8 @@
 #define G_L2T_W_PRIO(x) (((x) >> S_L2T_W_PRIO) & M_L2T_W_PRIO)
 
 struct cpl_l2t_write_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 	__u8 rsvd[3];
 };
@@ -1248,7 +1330,8 @@
 };
 
 struct cpl_l2t_read_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__be32 params;
 	__u8 rsvd[2];
 	__u8 dst_mac[6];
@@ -1288,13 +1371,14 @@
 #endif
 	__be16 rsvd2;
 	__be16 rsvd3;
-	__u8 src_mac1[6];
+	__u8  src_mac1[6];
 	__be16 rsvd4;
-	__u8 src_mac0[6];
+	__u8  src_mac0[6];
 };
 
 struct cpl_smt_write_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 	__u8 rsvd[3];
 };
@@ -1304,30 +1388,31 @@
 	union opcode_tid ot;
 	__u8 rsvd0;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	 __u8:4;
+	__u8 :4;
 	__u8 iff:4;
 #else
 	__u8 iff:4;
-	 __u8:4;
+	__u8 :4;
 #endif
 	__be16 rsvd2;
 };
 
 struct cpl_smt_read_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u8 mtu_idx:4;
-	 __u8:4;
+	__u8 :4;
 #else
-	 __u8:4;
+	__u8 :4;
 	__u8 mtu_idx:4;
 #endif
 	__be16 rsvd2;
 	__be16 rsvd3;
-	__u8 src_mac1[6];
+	__u8  src_mac1[6];
 	__be16 rsvd4;
-	__u8 src_mac0[6];
+	__u8  src_mac0[6];
 };
 
 struct cpl_rte_delete_req {
@@ -1352,7 +1437,8 @@
 #define F_RTE_READ_REQ_SELECT    V_RTE_READ_REQ_SELECT(1U)
 
 struct cpl_rte_delete_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 	__u8 rsvd[3];
 };
@@ -1361,13 +1447,13 @@
 	WR_HDR;
 	union opcode_tid ot;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	 __u8:6;
+	__u8 :6;
 	__u8 write_tcam:1;
 	__u8 write_l2t_lut:1;
 #else
 	__u8 write_l2t_lut:1;
 	__u8 write_tcam:1;
-	 __u8:6;
+	__u8 :6;
 #endif
 	__u8 rsvd[3];
 	__be32 lut_params;
@@ -1389,7 +1475,8 @@
 #define G_RTE_WRITE_REQ_LUT_BASE(x) (((x) >> S_RTE_WRITE_REQ_LUT_BASE) & M_RTE_WRITE_REQ_LUT_BASE)
 
 struct cpl_rte_write_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 	__u8 rsvd[3];
 };
@@ -1401,16 +1488,17 @@
 };
 
 struct cpl_rte_read_rpl {
-	RSS_HDR union opcode_tid ot;
+	RSS_HDR
+	union opcode_tid ot;
 	__u8 status;
 	__u8 rsvd0;
 	__be16 l2t_idx;
 #if defined(__LITTLE_ENDIAN_BITFIELD)
-	 __u8:7;
+	__u8 :7;
 	__u8 select:1;
 #else
 	__u8 select:1;
-	 __u8:7;
+	__u8 :7;
 #endif
 	__u8 rsvd2[3];
 	__be32 addr;
@@ -1439,16 +1527,16 @@
 	__u8 rsvd[2];
 #if defined(__LITTLE_ENDIAN_BITFIELD)
 	__u8 rspq:3;
-	 __u8:5;
+	__u8 :5;
 #else
-	 __u8:5;
+	__u8 :5;
 	__u8 rspq:3;
 #endif
 	__be32 tid_len;
 #endif
 	__be32 msn;
 	__be32 mo;
-	__u8 data[0];
+	__u8  data[0];
 };
 
 /* cpl_rdma_terminate.tid_len fields */
@@ -1465,12 +1553,12 @@
 /* ULP_TX opcodes */
 enum { ULP_MEM_READ = 2, ULP_MEM_WRITE = 3, ULP_TXPKT = 4 };
 
-#define S_ULPTX_CMD	28
-#define M_ULPTX_CMD	0xF
-#define V_ULPTX_CMD(x)	((x) << S_ULPTX_CMD)
+#define S_ULPTX_CMD    28
+#define M_ULPTX_CMD    0xF
+#define V_ULPTX_CMD(x) ((x) << S_ULPTX_CMD)
 
-#define S_ULPTX_NFLITS	0
-#define M_ULPTX_NFLITS	0xFF
+#define S_ULPTX_NFLITS    0
+#define M_ULPTX_NFLITS    0xFF
 #define V_ULPTX_NFLITS(x) ((x) << S_ULPTX_NFLITS)
 
 struct ulp_mem_io {
@@ -1480,16 +1568,32 @@
 };
 
 /* ulp_mem_io.cmd_lock_addr fields */
-#define S_ULP_MEMIO_ADDR	0
-#define M_ULP_MEMIO_ADDR	0x7FFFFFF
-#define V_ULP_MEMIO_ADDR(x)	((x) << S_ULP_MEMIO_ADDR)
-#define S_ULP_MEMIO_LOCK	27
-#define V_ULP_MEMIO_LOCK(x)	((x) << S_ULP_MEMIO_LOCK)
-#define F_ULP_MEMIO_LOCK	V_ULP_MEMIO_LOCK(1U)
+#define S_ULP_MEMIO_ADDR    0
+#define M_ULP_MEMIO_ADDR    0x7FFFFFF
+#define V_ULP_MEMIO_ADDR(x) ((x) << S_ULP_MEMIO_ADDR)
+
+#define S_ULP_MEMIO_LOCK    27
+#define V_ULP_MEMIO_LOCK(x) ((x) << S_ULP_MEMIO_LOCK)
+#define F_ULP_MEMIO_LOCK    V_ULP_MEMIO_LOCK(1U)
 
 /* ulp_mem_io.len fields */
-#define S_ULP_MEMIO_DATA_LEN	28
-#define M_ULP_MEMIO_DATA_LEN	0xF
-#define V_ULP_MEMIO_DATA_LEN(x)	((x) << S_ULP_MEMIO_DATA_LEN)
+#define S_ULP_MEMIO_DATA_LEN    28
+#define M_ULP_MEMIO_DATA_LEN    0xF
+#define V_ULP_MEMIO_DATA_LEN(x) ((x) << S_ULP_MEMIO_DATA_LEN)
 
-#endif				/* T3_CPL_H */
+struct ulp_txpkt {
+	__be32 cmd_dest;
+	__be32 len;
+};
+
+/* ulp_txpkt.cmd_dest fields */
+#define S_ULP_TXPKT_DEST    24
+#define M_ULP_TXPKT_DEST    0xF
+#define V_ULP_TXPKT_DEST(x) ((x) << S_ULP_TXPKT_DEST)
+
+/* Firmware watchdog enable/disable fields */
+#define S_FW_WR_WD_EN           15
+#define M_FW_WR_WD_EN           0x8000
+#define V_FW_WR_WD_EN(x)        ((x) << S_FW_WR_WD_EN)
+
+#endif  /* T3_CPL_H */
diff --git a/drivers/net/cxgb3/t3_firmware.h b/drivers/net/cxgb3/t3_firmware.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/t3_firmware.h
@@ -0,0 +1,381 @@
+
+/*
+ * Copyright (c) 2003-2011 Chelsio, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+/*
+ * dump of firmware files generated using:
+ *	hexdump -v -e '"\t" 16/1 "0x%02x, " "\n"'
+*/
+
+/* contents of ael2005_opt_edc.bin */
+/* from microInit_mdio_SR_AEL2005C_16.txt */
+static u8 ael2005_opt_edc_data[] = {
+	0xcc, 0x00, 0x2f, 0xf4, 0xcc, 0x01, 0x3c, 0xd4, 0xcc, 0x02, 0x20, 0x15, 0xcc, 0x03, 0x31, 0x05,
+	0xcc, 0x04, 0x65, 0x24, 0xcc, 0x05, 0x27, 0xff, 0xcc, 0x06, 0x30, 0x0f, 0xcc, 0x07, 0x2c, 0x8b,
+	0xcc, 0x08, 0x30, 0x0b, 0xcc, 0x09, 0x40, 0x09, 0xcc, 0x0a, 0x40, 0x0e, 0xcc, 0x0b, 0x2f, 0x72,
+	0xcc, 0x0c, 0x30, 0x02, 0xcc, 0x0d, 0x10, 0x02, 0xcc, 0x0e, 0x21, 0x72, 0xcc, 0x0f, 0x30, 0x12,
+	0xcc, 0x10, 0x10, 0x02, 0xcc, 0x11, 0x25, 0xd2, 0xcc, 0x12, 0x30, 0x12, 0xcc, 0x13, 0x10, 0x02,
+	0xcc, 0x14, 0xd0, 0x1e, 0xcc, 0x15, 0x27, 0xd2, 0xcc, 0x16, 0x30, 0x12, 0xcc, 0x17, 0x10, 0x02,
+	0xcc, 0x18, 0x20, 0x04, 0xcc, 0x19, 0x3c, 0x84, 0xcc, 0x1a, 0x64, 0x36, 0xcc, 0x1b, 0x20, 0x07,
+	0xcc, 0x1c, 0x3f, 0x87, 0xcc, 0x1d, 0x86, 0x76, 0xcc, 0x1e, 0x40, 0xb7, 0xcc, 0x1f, 0xa7, 0x46,
+	0xcc, 0x20, 0x40, 0x47, 0xcc, 0x21, 0x56, 0x73, 0xcc, 0x22, 0x29, 0x82, 0xcc, 0x23, 0x30, 0x02,
+	0xcc, 0x24, 0x13, 0xd2, 0xcc, 0x25, 0x8b, 0xbd, 0xcc, 0x26, 0x28, 0x62, 0xcc, 0x27, 0x30, 0x12,
+	0xcc, 0x28, 0x10, 0x02, 0xcc, 0x29, 0x20, 0x92, 0xcc, 0x2a, 0x30, 0x12, 0xcc, 0x2b, 0x10, 0x02,
+	0xcc, 0x2c, 0x5c, 0xc3, 0xcc, 0x2d, 0x03, 0x14, 0xcc, 0x2e, 0x29, 0x42, 0xcc, 0x2f, 0x30, 0x02,
+	0xcc, 0x30, 0x10, 0x02, 0xcc, 0x31, 0xd0, 0x19, 0xcc, 0x32, 0x20, 0x32, 0xcc, 0x33, 0x30, 0x12,
+	0xcc, 0x34, 0x10, 0x02, 0xcc, 0x35, 0x2a, 0x04, 0xcc, 0x36, 0x3c, 0x74, 0xcc, 0x37, 0x64, 0x35,
+	0xcc, 0x38, 0x2f, 0xa4, 0xcc, 0x39, 0x3c, 0xd4, 0xcc, 0x3a, 0x66, 0x24, 0xcc, 0x3b, 0x55, 0x63,
+	0xcc, 0x3c, 0x2d, 0x42, 0xcc, 0x3d, 0x30, 0x02, 0xcc, 0x3e, 0x13, 0xd2, 0xcc, 0x3f, 0x46, 0x4d,
+	0xcc, 0x40, 0x28, 0x62, 0xcc, 0x41, 0x30, 0x12, 0xcc, 0x42, 0x10, 0x02, 0xcc, 0x43, 0x20, 0x32,
+	0xcc, 0x44, 0x30, 0x12, 0xcc, 0x45, 0x10, 0x02, 0xcc, 0x46, 0x2f, 0xb4, 0xcc, 0x47, 0x3c, 0xd4,
+	0xcc, 0x48, 0x66, 0x24, 0xcc, 0x49, 0x55, 0x63, 0xcc, 0x4a, 0x2d, 0x42, 0xcc, 0x4b, 0x30, 0x02,
+	0xcc, 0x4c, 0x13, 0xd2, 0xcc, 0x4d, 0x2e, 0xd2, 0xcc, 0x4e, 0x30, 0x02, 0xcc, 0x4f, 0x10, 0x02,
+	0xcc, 0x50, 0x2f, 0xd2, 0xcc, 0x51, 0x30, 0x02, 0xcc, 0x52, 0x10, 0x02, 0xcc, 0x53, 0x00, 0x04,
+	0xcc, 0x54, 0x29, 0x42, 0xcc, 0x55, 0x30, 0x02, 0xcc, 0x56, 0x10, 0x02, 0xcc, 0x57, 0x20, 0x92,
+	0xcc, 0x58, 0x30, 0x12, 0xcc, 0x59, 0x10, 0x02, 0xcc, 0x5a, 0x5c, 0xc3, 0xcc, 0x5b, 0x03, 0x17,
+	0xcc, 0x5c, 0x2f, 0x72, 0xcc, 0x5d, 0x30, 0x02, 0xcc, 0x5e, 0x10, 0x02, 0xcc, 0x5f, 0x29, 0x42,
+	0xcc, 0x60, 0x30, 0x02, 0xcc, 0x61, 0x10, 0x02, 0xcc, 0x62, 0x22, 0xcd, 0xcc, 0x63, 0x30, 0x1d,
+	0xcc, 0x64, 0x28, 0x62, 0xcc, 0x65, 0x30, 0x12, 0xcc, 0x66, 0x10, 0x02, 0xcc, 0x67, 0x2e, 0xd2,
+	0xcc, 0x68, 0x30, 0x02, 0xcc, 0x69, 0x10, 0x02, 0xcc, 0x6a, 0x2d, 0x72, 0xcc, 0x6b, 0x30, 0x02,
+	0xcc, 0x6c, 0x10, 0x02, 0xcc, 0x6d, 0x62, 0x8f, 0xcc, 0x6e, 0x21, 0x12, 0xcc, 0x6f, 0x30, 0x12,
+	0xcc, 0x70, 0x10, 0x02, 0xcc, 0x71, 0x5a, 0xa3, 0xcc, 0x72, 0x2d, 0xc2, 0xcc, 0x73, 0x30, 0x02,
+	0xcc, 0x74, 0x13, 0x12, 0xcc, 0x75, 0x6f, 0x72, 0xcc, 0x76, 0x10, 0x02, 0xcc, 0x77, 0x28, 0x07,
+	0xcc, 0x78, 0x31, 0xa7, 0xcc, 0x79, 0x20, 0xc4, 0xcc, 0x7a, 0x3c, 0x24, 0xcc, 0x7b, 0x67, 0x24,
+	0xcc, 0x7c, 0x10, 0x02, 0xcc, 0x7d, 0x28, 0x07, 0xcc, 0x7e, 0x31, 0x87, 0xcc, 0x7f, 0x20, 0xc4,
+	0xcc, 0x80, 0x3c, 0x24, 0xcc, 0x81, 0x67, 0x24, 0xcc, 0x82, 0x10, 0x02, 0xcc, 0x83, 0x25, 0x14,
+	0xcc, 0x84, 0x3c, 0x64, 0xcc, 0x85, 0x64, 0x36, 0xcc, 0x86, 0xdf, 0xf4, 0xcc, 0x87, 0x64, 0x36,
+	0xcc, 0x88, 0x10, 0x02, 0xcc, 0x89, 0x40, 0xa4, 0xcc, 0x8a, 0x64, 0x3c, 0xcc, 0x8b, 0x40, 0x16,
+	0xcc, 0x8c, 0x8c, 0x6c, 0xcc, 0x8d, 0x2b, 0x24, 0xcc, 0x8e, 0x3c, 0x24, 0xcc, 0x8f, 0x64, 0x35,
+	0xcc, 0x90, 0x10, 0x02, 0xcc, 0x91, 0x2b, 0x24, 0xcc, 0x92, 0x3c, 0x24, 0xcc, 0x93, 0x64, 0x3a,
+	0xcc, 0x94, 0x40, 0x25, 0xcc, 0x95, 0x8a, 0x5a, 0xcc, 0x96, 0x10, 0x02, 0xcc, 0x97, 0x27, 0x31,
+	0xcc, 0x98, 0x30, 0x11, 0xcc, 0x99, 0x10, 0x01, 0xcc, 0x9a, 0xc7, 0xa0, 0xcc, 0x9b, 0x01, 0x00,
+	0xcc, 0x9c, 0xc5, 0x02, 0xcc, 0x9d, 0x53, 0xac, 0xcc, 0x9e, 0xc5, 0x03, 0xcc, 0x9f, 0xd5, 0xd5,
+	0xcc, 0xa0, 0xc6, 0x00, 0xcc, 0xa1, 0x2a, 0x6d, 0xcc, 0xa2, 0xc6, 0x01, 0xcc, 0xa3, 0x2a, 0x4c,
+	0xcc, 0xa4, 0xc6, 0x02, 0xcc, 0xa5, 0x01, 0x11, 0xcc, 0xa6, 0xc6, 0x0c, 0xcc, 0xa7, 0x59, 0x00,
+	0xcc, 0xa8, 0xc7, 0x10, 0xcc, 0xa9, 0x07, 0x00, 0xcc, 0xaa, 0xc7, 0x18, 0xcc, 0xab, 0x07, 0x00,
+	0xcc, 0xac, 0xc7, 0x20, 0xcc, 0xad, 0x47, 0x00, 0xcc, 0xae, 0xc8, 0x01, 0xcc, 0xaf, 0x7f, 0x50,
+	0xcc, 0xb0, 0xc8, 0x02, 0xcc, 0xb1, 0x77, 0x60, 0xcc, 0xb2, 0xc8, 0x03, 0xcc, 0xb3, 0x7f, 0xce,
+	0xcc, 0xb4, 0xc8, 0x04, 0xcc, 0xb5, 0x57, 0x00, 0xcc, 0xb6, 0xc8, 0x05, 0xcc, 0xb7, 0x5f, 0x11,
+	0xcc, 0xb8, 0xc8, 0x06, 0xcc, 0xb9, 0x47, 0x51, 0xcc, 0xba, 0xc8, 0x07, 0xcc, 0xbb, 0x57, 0xe1,
+	0xcc, 0xbc, 0xc8, 0x08, 0xcc, 0xbd, 0x27, 0x00, 0xcc, 0xbe, 0xc8, 0x09, 0xcc, 0xbf, 0x00, 0x00,
+	0xcc, 0xc0, 0xc8, 0x21, 0xcc, 0xc1, 0x00, 0x02, 0xcc, 0xc2, 0xc8, 0x22, 0xcc, 0xc3, 0x00, 0x14,
+	0xcc, 0xc4, 0xc8, 0x32, 0xcc, 0xc5, 0x11, 0x86, 0xcc, 0xc6, 0xc8, 0x47, 0xcc, 0xc7, 0x1e, 0x02,
+	0xcc, 0xc8, 0xc0, 0x13, 0xcc, 0xc9, 0xf3, 0x41, 0xcc, 0xca, 0xc0, 0x1a, 0xcc, 0xcb, 0x04, 0x46,
+	0xcc, 0xcc, 0xc0, 0x24, 0xcc, 0xcd, 0x10, 0x00, 0xcc, 0xce, 0xc0, 0x25, 0xcc, 0xcf, 0x0a, 0x00,
+	0xcc, 0xd0, 0xc0, 0x26, 0xcc, 0xd1, 0x0c, 0x0c, 0xcc, 0xd2, 0xc0, 0x27, 0xcc, 0xd3, 0x0c, 0x0c,
+	0xcc, 0xd4, 0xc0, 0x29, 0xcc, 0xd5, 0x00, 0xa0, 0xcc, 0xd6, 0xc0, 0x30, 0xcc, 0xd7, 0x0a, 0x00,
+	0xcc, 0xd8, 0xc0, 0x3c, 0xcc, 0xd9, 0x00, 0x1c, 0xcc, 0xda, 0xc0, 0x05, 0xcc, 0xdb, 0x7a, 0x06,
+	0xcc, 0xdc, 0x00, 0x00, 0xcc, 0xdd, 0x27, 0x31, 0xcc, 0xde, 0x30, 0x11, 0xcc, 0xdf, 0x10, 0x01,
+	0xcc, 0xe0, 0xc6, 0x20, 0xcc, 0xe1, 0x00, 0x00, 0xcc, 0xe2, 0xc6, 0x21, 0xcc, 0xe3, 0x00, 0x3f,
+	0xcc, 0xe4, 0xc6, 0x22, 0xcc, 0xe5, 0x00, 0x00, 0xcc, 0xe6, 0xc6, 0x23, 0xcc, 0xe7, 0x00, 0x00,
+	0xcc, 0xe8, 0xc6, 0x24, 0xcc, 0xe9, 0x00, 0x00, 0xcc, 0xea, 0xc6, 0x25, 0xcc, 0xeb, 0x00, 0x00,
+	0xcc, 0xec, 0xc6, 0x27, 0xcc, 0xed, 0x00, 0x00, 0xcc, 0xee, 0xc6, 0x28, 0xcc, 0xef, 0x00, 0x00,
+	0xcc, 0xf0, 0xc6, 0x2c, 0xcc, 0xf1, 0x00, 0x00, 0xcc, 0xf2, 0x00, 0x00, 0xcc, 0xf3, 0x28, 0x06,
+	0xcc, 0xf4, 0x3c, 0xb6, 0xcc, 0xf5, 0xc1, 0x61, 0xcc, 0xf6, 0x61, 0x34, 0xcc, 0xf7, 0x61, 0x35,
+	0xcc, 0xf8, 0x54, 0x43, 0xcc, 0xf9, 0x03, 0x03, 0xcc, 0xfa, 0x65, 0x24, 0xcc, 0xfb, 0x00, 0x0b,
+	0xcc, 0xfc, 0x10, 0x02, 0xcc, 0xfd, 0x21, 0x04, 0xcc, 0xfe, 0x3c, 0x24, 0xcc, 0xff, 0x21, 0x05,
+	0xcd, 0x00, 0x38, 0x05, 0xcd, 0x01, 0x65, 0x24, 0xcd, 0x02, 0xdf, 0xf4, 0xcd, 0x03, 0x40, 0x05,
+	0xcd, 0x04, 0x65, 0x24, 0xcd, 0x05, 0x10, 0x02, 0xcd, 0x06, 0x5d, 0xd3, 0xcd, 0x07, 0x03, 0x06,
+	0xcd, 0x08, 0x2f, 0xf7, 0xcd, 0x09, 0x38, 0xf7, 0xcd, 0x0a, 0x60, 0xb7, 0xcd, 0x0b, 0xdf, 0xfd,
+	0xcd, 0x0c, 0x00, 0x0a, 0xcd, 0x0d, 0x10, 0x02, 0xcd, 0x0e, 0x00, 0x00, 0x7c, 0xc7, 0xae, 0x59
+};
+
+
+/* contents of ael2005_twx_edc.bin */
+/* from microInit_mdio_TWINAX_AEL2005C_20.txt */
+static u8 ael2005_twx_edc_data[] = {
+	0xcc, 0x00, 0x40, 0x09, 0xcc, 0x01, 0x27, 0xff, 0xcc, 0x02, 0x30, 0x0f, 0xcc, 0x03, 0x40, 0xaa,
+	0xcc, 0x04, 0x40, 0x1c, 0xcc, 0x05, 0x40, 0x1e, 0xcc, 0x06, 0x2f, 0xf4, 0xcc, 0x07, 0x3c, 0xd4,
+	0xcc, 0x08, 0x20, 0x35, 0xcc, 0x09, 0x31, 0x45, 0xcc, 0x0a, 0x65, 0x24, 0xcc, 0x0b, 0x26, 0xa2,
+	0xcc, 0x0c, 0x30, 0x12, 0xcc, 0x0d, 0x10, 0x02, 0xcc, 0x0e, 0x29, 0xc2, 0xcc, 0x0f, 0x30, 0x02,
+	0xcc, 0x10, 0x10, 0x02, 0xcc, 0x11, 0x20, 0x72, 0xcc, 0x12, 0x30, 0x12, 0xcc, 0x13, 0x10, 0x02,
+	0xcc, 0x14, 0x22, 0xcd, 0xcc, 0x15, 0x30, 0x1d, 0xcc, 0x16, 0x2e, 0x52, 0xcc, 0x17, 0x30, 0x12,
+	0xcc, 0x18, 0x10, 0x02, 0xcc, 0x19, 0x28, 0xe2, 0xcc, 0x1a, 0x30, 0x02, 0xcc, 0x1b, 0x10, 0x02,
+	0xcc, 0x1c, 0x62, 0x8f, 0xcc, 0x1d, 0x2a, 0xc2, 0xcc, 0x1e, 0x30, 0x12, 0xcc, 0x1f, 0x10, 0x02,
+	0xcc, 0x20, 0x55, 0x53, 0xcc, 0x21, 0x2a, 0xe2, 0xcc, 0x22, 0x30, 0x02, 0xcc, 0x23, 0x13, 0x02,
+	0xcc, 0x24, 0x40, 0x1e, 0xcc, 0x25, 0x2b, 0xe2, 0xcc, 0x26, 0x30, 0x12, 0xcc, 0x27, 0x10, 0x02,
+	0xcc, 0x28, 0x2d, 0xa2, 0xcc, 0x29, 0x30, 0x12, 0xcc, 0x2a, 0x10, 0x02, 0xcc, 0x2b, 0x2b, 0xa2,
+	0xcc, 0x2c, 0x30, 0x02, 0xcc, 0x2d, 0x10, 0x02, 0xcc, 0x2e, 0x5e, 0xe3, 0xcc, 0x2f, 0x03, 0x05,
+	0xcc, 0x30, 0x40, 0x0e, 0xcc, 0x31, 0x2b, 0xc2, 0xcc, 0x32, 0x30, 0x02, 0xcc, 0x33, 0x10, 0x02,
+	0xcc, 0x34, 0x2b, 0x82, 0xcc, 0x35, 0x30, 0x12, 0xcc, 0x36, 0x10, 0x02, 0xcc, 0x37, 0x56, 0x63,
+	0xcc, 0x38, 0x03, 0x02, 0xcc, 0x39, 0x40, 0x1e, 0xcc, 0x3a, 0x6f, 0x72, 0xcc, 0x3b, 0x10, 0x02,
+	0xcc, 0x3c, 0x62, 0x8f, 0xcc, 0x3d, 0x2b, 0xe2, 0xcc, 0x3e, 0x30, 0x12, 0xcc, 0x3f, 0x10, 0x02,
+	0xcc, 0x40, 0x22, 0xcd, 0xcc, 0x41, 0x30, 0x1d, 0xcc, 0x42, 0x2e, 0x52, 0xcc, 0x43, 0x30, 0x12,
+	0xcc, 0x44, 0x10, 0x02, 0xcc, 0x45, 0x25, 0x22, 0xcc, 0x46, 0x30, 0x12, 0xcc, 0x47, 0x10, 0x02,
+	0xcc, 0x48, 0x2d, 0xa2, 0xcc, 0x49, 0x30, 0x12, 0xcc, 0x4a, 0x10, 0x02, 0xcc, 0x4b, 0x2c, 0xa2,
+	0xcc, 0x4c, 0x30, 0x12, 0xcc, 0x4d, 0x10, 0x02, 0xcc, 0x4e, 0x2f, 0xa4, 0xcc, 0x4f, 0x3c, 0xd4,
+	0xcc, 0x50, 0x66, 0x24, 0xcc, 0x51, 0x41, 0x0b, 0xcc, 0x52, 0x56, 0xb3, 0xcc, 0x53, 0x03, 0xc4,
+	0xcc, 0x54, 0x2f, 0xb2, 0xcc, 0x55, 0x30, 0x02, 0xcc, 0x56, 0x10, 0x02, 0xcc, 0x57, 0x22, 0x0b,
+	0xcc, 0x58, 0x30, 0x3b, 0xcc, 0x59, 0x56, 0xb3, 0xcc, 0x5a, 0x03, 0xc3, 0xcc, 0x5b, 0x86, 0x6b,
+	0xcc, 0x5c, 0x40, 0x0c, 0xcc, 0x5d, 0x23, 0xa2, 0xcc, 0x5e, 0x30, 0x12, 0xcc, 0x5f, 0x10, 0x02,
+	0xcc, 0x60, 0x2d, 0xa2, 0xcc, 0x61, 0x30, 0x12, 0xcc, 0x62, 0x10, 0x02, 0xcc, 0x63, 0x2c, 0xa2,
+	0xcc, 0x64, 0x30, 0x12, 0xcc, 0x65, 0x10, 0x02, 0xcc, 0x66, 0x2f, 0xb4, 0xcc, 0x67, 0x3c, 0xd4,
+	0xcc, 0x68, 0x66, 0x24, 0xcc, 0x69, 0x56, 0xb3, 0xcc, 0x6a, 0x03, 0xc3, 0xcc, 0x6b, 0x86, 0x6b,
+	0xcc, 0x6c, 0x40, 0x1c, 0xcc, 0x6d, 0x22, 0x05, 0xcc, 0x6e, 0x30, 0x35, 0xcc, 0x6f, 0x5b, 0x53,
+	0xcc, 0x70, 0x2c, 0x52, 0xcc, 0x71, 0x30, 0x02, 0xcc, 0x72, 0x13, 0xc2, 0xcc, 0x73, 0x5c, 0xc3,
+	0xcc, 0x74, 0x03, 0x17, 0xcc, 0x75, 0x25, 0x22, 0xcc, 0x76, 0x30, 0x12, 0xcc, 0x77, 0x10, 0x02,
+	0xcc, 0x78, 0x2d, 0xa2, 0xcc, 0x79, 0x30, 0x12, 0xcc, 0x7a, 0x10, 0x02, 0xcc, 0x7b, 0x2b, 0x82,
+	0xcc, 0x7c, 0x30, 0x12, 0xcc, 0x7d, 0x10, 0x02, 0xcc, 0x7e, 0x56, 0x63, 0xcc, 0x7f, 0x03, 0x03,
+	0xcc, 0x80, 0x40, 0x1e, 0xcc, 0x81, 0x00, 0x04, 0xcc, 0x82, 0x2c, 0x42, 0xcc, 0x83, 0x30, 0x12,
+	0xcc, 0x84, 0x10, 0x02, 0xcc, 0x85, 0x6f, 0x72, 0xcc, 0x86, 0x10, 0x02, 0xcc, 0x87, 0x62, 0x8f,
+	0xcc, 0x88, 0x23, 0x04, 0xcc, 0x89, 0x3c, 0x84, 0xcc, 0x8a, 0x64, 0x36, 0xcc, 0x8b, 0xdf, 0xf4,
+	0xcc, 0x8c, 0x64, 0x36, 0xcc, 0x8d, 0x2f, 0xf5, 0xcc, 0x8e, 0x30, 0x05, 0xcc, 0x8f, 0x86, 0x56,
+	0xcc, 0x90, 0xdf, 0xba, 0xcc, 0x91, 0x56, 0xa3, 0xcc, 0x92, 0xd0, 0x5a, 0xcc, 0x93, 0x21, 0xc2,
+	0xcc, 0x94, 0x30, 0x12, 0xcc, 0x95, 0x13, 0x92, 0xcc, 0x96, 0xd0, 0x5a, 0xcc, 0x97, 0x56, 0xa3,
+	0xcc, 0x98, 0xdf, 0xba, 0xcc, 0x99, 0x03, 0x83, 0xcc, 0x9a, 0x6f, 0x72, 0xcc, 0x9b, 0x10, 0x02,
+	0xcc, 0x9c, 0x28, 0xc5, 0xcc, 0x9d, 0x30, 0x05, 0xcc, 0x9e, 0x41, 0x78, 0xcc, 0x9f, 0x56, 0x53,
+	0xcc, 0xa0, 0x03, 0x84, 0xcc, 0xa1, 0x22, 0xb2, 0xcc, 0xa2, 0x30, 0x12, 0xcc, 0xa3, 0x10, 0x02,
+	0xcc, 0xa4, 0x2b, 0xe5, 0xcc, 0xa5, 0x30, 0x05, 0xcc, 0xa6, 0x41, 0xe8, 0xcc, 0xa7, 0x56, 0x53,
+	0xcc, 0xa8, 0x03, 0x82, 0xcc, 0xa9, 0x00, 0x02, 0xcc, 0xaa, 0x42, 0x58, 0xcc, 0xab, 0x24, 0x74,
+	0xcc, 0xac, 0x3c, 0x84, 0xcc, 0xad, 0x64, 0x37, 0xcc, 0xae, 0xdf, 0xf4, 0xcc, 0xaf, 0x64, 0x37,
+	0xcc, 0xb0, 0x2f, 0xf5, 0xcc, 0xb1, 0x3c, 0x05, 0xcc, 0xb2, 0x87, 0x57, 0xcc, 0xb3, 0xb8, 0x88,
+	0xcc, 0xb4, 0x97, 0x87, 0xcc, 0xb5, 0xdf, 0xf4, 0xcc, 0xb6, 0x67, 0x24, 0xcc, 0xb7, 0x86, 0x6a,
+	0xcc, 0xb8, 0x6f, 0x72, 0xcc, 0xb9, 0x10, 0x02, 0xcc, 0xba, 0x2d, 0x01, 0xcc, 0xbb, 0x30, 0x11,
+	0xcc, 0xbc, 0x10, 0x01, 0xcc, 0xbd, 0xc6, 0x20, 0xcc, 0xbe, 0x14, 0xe5, 0xcc, 0xbf, 0xc6, 0x21,
+	0xcc, 0xc0, 0xc5, 0x3d, 0xcc, 0xc1, 0xc6, 0x22, 0xcc, 0xc2, 0x3c, 0xbe, 0xcc, 0xc3, 0xc6, 0x23,
+	0xcc, 0xc4, 0x44, 0x52, 0xcc, 0xc5, 0xc6, 0x24, 0xcc, 0xc6, 0xc5, 0xc5, 0xcc, 0xc7, 0xc6, 0x25,
+	0xcc, 0xc8, 0xe0, 0x1e, 0xcc, 0xc9, 0xc6, 0x27, 0xcc, 0xca, 0x00, 0x00, 0xcc, 0xcb, 0xc6, 0x28,
+	0xcc, 0xcc, 0x00, 0x00, 0xcc, 0xcd, 0xc6, 0x2b, 0xcc, 0xce, 0x00, 0x00, 0xcc, 0xcf, 0xc6, 0x2c,
+	0xcc, 0xd0, 0x00, 0x00, 0xcc, 0xd1, 0x00, 0x00, 0xcc, 0xd2, 0x2d, 0x01, 0xcc, 0xd3, 0x30, 0x11,
+	0xcc, 0xd4, 0x10, 0x01, 0xcc, 0xd5, 0xc6, 0x20, 0xcc, 0xd6, 0x00, 0x00, 0xcc, 0xd7, 0xc6, 0x21,
+	0xcc, 0xd8, 0x00, 0x00, 0xcc, 0xd9, 0xc6, 0x22, 0xcc, 0xda, 0x00, 0xce, 0xcc, 0xdb, 0xc6, 0x23,
+	0xcc, 0xdc, 0x00, 0x7f, 0xcc, 0xdd, 0xc6, 0x24, 0xcc, 0xde, 0x00, 0x32, 0xcc, 0xdf, 0xc6, 0x25,
+	0xcc, 0xe0, 0x00, 0x00, 0xcc, 0xe1, 0xc6, 0x27, 0xcc, 0xe2, 0x00, 0x00, 0xcc, 0xe3, 0xc6, 0x28,
+	0xcc, 0xe4, 0x00, 0x00, 0xcc, 0xe5, 0xc6, 0x2b, 0xcc, 0xe6, 0x00, 0x00, 0xcc, 0xe7, 0xc6, 0x2c,
+	0xcc, 0xe8, 0x00, 0x00, 0xcc, 0xe9, 0x00, 0x00, 0xcc, 0xea, 0x2d, 0x01, 0xcc, 0xeb, 0x30, 0x11,
+	0xcc, 0xec, 0x10, 0x01, 0xcc, 0xed, 0xc5, 0x02, 0xcc, 0xee, 0x60, 0x9f, 0xcc, 0xef, 0xc6, 0x00,
+	0xcc, 0xf0, 0x2a, 0x6e, 0xcc, 0xf1, 0xc6, 0x01, 0xcc, 0xf2, 0x2a, 0x2c, 0xcc, 0xf3, 0xc6, 0x0c,
+	0xcc, 0xf4, 0x54, 0x00, 0xcc, 0xf5, 0xc7, 0x10, 0xcc, 0xf6, 0x07, 0x00, 0xcc, 0xf7, 0xc7, 0x18,
+	0xcc, 0xf8, 0x07, 0x00, 0xcc, 0xf9, 0xc7, 0x20, 0xcc, 0xfa, 0x47, 0x00, 0xcc, 0xfb, 0xc7, 0x28,
+	0xcc, 0xfc, 0x07, 0x00, 0xcc, 0xfd, 0xc7, 0x29, 0xcc, 0xfe, 0x12, 0x07, 0xcc, 0xff, 0xc8, 0x01,
+	0xcd, 0x00, 0x7f, 0x50, 0xcd, 0x01, 0xc8, 0x02, 0xcd, 0x02, 0x77, 0x60, 0xcd, 0x03, 0xc8, 0x03,
+	0xcd, 0x04, 0x7f, 0xce, 0xcd, 0x05, 0xc8, 0x04, 0xcd, 0x06, 0x52, 0x0e, 0xcd, 0x07, 0xc8, 0x05,
+	0xcd, 0x08, 0x5c, 0x11, 0xcd, 0x09, 0xc8, 0x06, 0xcd, 0x0a, 0x3c, 0x51, 0xcd, 0x0b, 0xc8, 0x07,
+	0xcd, 0x0c, 0x40, 0x61, 0xcd, 0x0d, 0xc8, 0x08, 0xcd, 0x0e, 0x49, 0xc1, 0xcd, 0x0f, 0xc8, 0x09,
+	0xcd, 0x10, 0x38, 0x40, 0xcd, 0x11, 0xc8, 0x0a, 0xcd, 0x12, 0x00, 0x00, 0xcd, 0x13, 0xc8, 0x21,
+	0xcd, 0x14, 0x00, 0x02, 0xcd, 0x15, 0xc8, 0x22, 0xcd, 0x16, 0x00, 0x46, 0xcd, 0x17, 0xc8, 0x44,
+	0xcd, 0x18, 0x18, 0x2f, 0xcd, 0x19, 0xc0, 0x13, 0xcd, 0x1a, 0xf3, 0x41, 0xcd, 0x1b, 0xc0, 0x1a,
+	0xcd, 0x1c, 0x04, 0x46, 0xcd, 0x1d, 0xc0, 0x24, 0xcd, 0x1e, 0x10, 0x00, 0xcd, 0x1f, 0xc0, 0x25,
+	0xcd, 0x20, 0x0a, 0x00, 0xcd, 0x21, 0xc0, 0x26, 0xcd, 0x22, 0x0c, 0x0c, 0xcd, 0x23, 0xc0, 0x27,
+	0xcd, 0x24, 0x0c, 0x0c, 0xcd, 0x25, 0xc0, 0x29, 0xcd, 0x26, 0x00, 0xa0, 0xcd, 0x27, 0xc0, 0x30,
+	0xcd, 0x28, 0x0a, 0x00, 0xcd, 0x29, 0xc0, 0x3c, 0xcd, 0x2a, 0x00, 0x1c, 0xcd, 0x2b, 0x00, 0x00,
+	0xcd, 0x2c, 0x2b, 0x84, 0xcd, 0x2d, 0x3c, 0x74, 0xcd, 0x2e, 0x64, 0x35, 0xcd, 0x2f, 0xdf, 0xf4,
+	0xcd, 0x30, 0x64, 0x35, 0xcd, 0x31, 0x28, 0x06, 0xcd, 0x32, 0x30, 0x06, 0xcd, 0x33, 0x85, 0x65,
+	0xcd, 0x34, 0x2b, 0x24, 0xcd, 0x35, 0x3c, 0x24, 0xcd, 0x36, 0x64, 0x36, 0xcd, 0x37, 0x10, 0x02,
+	0xcd, 0x38, 0x2b, 0x24, 0xcd, 0x39, 0x3c, 0x24, 0xcd, 0x3a, 0x64, 0x36, 0xcd, 0x3b, 0x40, 0x45,
+	0xcd, 0x3c, 0x86, 0x56, 0xcd, 0x3d, 0x10, 0x02, 0xcd, 0x3e, 0x28, 0x07, 0xcd, 0x3f, 0x31, 0xa7,
+	0xcd, 0x40, 0x20, 0xc4, 0xcd, 0x41, 0x3c, 0x24, 0xcd, 0x42, 0x67, 0x24, 0xcd, 0x43, 0x10, 0x02,
+	0xcd, 0x44, 0x28, 0x07, 0xcd, 0x45, 0x31, 0x87, 0xcd, 0x46, 0x20, 0xc4, 0xcd, 0x47, 0x3c, 0x24,
+	0xcd, 0x48, 0x67, 0x24, 0xcd, 0x49, 0x10, 0x02, 0xcd, 0x4a, 0x25, 0x14, 0xcd, 0x4b, 0x3c, 0x64,
+	0xcd, 0x4c, 0x64, 0x36, 0xcd, 0x4d, 0xdf, 0xf4, 0xcd, 0x4e, 0x64, 0x36, 0xcd, 0x4f, 0x10, 0x02,
+	0xcd, 0x50, 0x28, 0x06, 0xcd, 0x51, 0x3c, 0xb6, 0xcd, 0x52, 0xc1, 0x61, 0xcd, 0x53, 0x61, 0x34,
+	0xcd, 0x54, 0x61, 0x35, 0xcd, 0x55, 0x54, 0x43, 0xcd, 0x56, 0x03, 0x03, 0xcd, 0x57, 0x65, 0x24,
+	0xcd, 0x58, 0x00, 0x0b, 0xcd, 0x59, 0x10, 0x02, 0xcd, 0x5a, 0xd0, 0x19, 0xcd, 0x5b, 0x21, 0x04,
+	0xcd, 0x5c, 0x3c, 0x24, 0xcd, 0x5d, 0x21, 0x05, 0xcd, 0x5e, 0x38, 0x05, 0xcd, 0x5f, 0x65, 0x24,
+	0xcd, 0x60, 0xdf, 0xf4, 0xcd, 0x61, 0x40, 0x05, 0xcd, 0x62, 0x65, 0x24, 0xcd, 0x63, 0x2e, 0x8d,
+	0xcd, 0x64, 0x30, 0x3d, 0xcd, 0x65, 0x5d, 0xd3, 0xcd, 0x66, 0x03, 0x06, 0xcd, 0x67, 0x2f, 0xf7,
+	0xcd, 0x68, 0x38, 0xf7, 0xcd, 0x69, 0x60, 0xb7, 0xcd, 0x6a, 0xdf, 0xfd, 0xcd, 0x6b, 0x00, 0x0a,
+	0xcd, 0x6c, 0x10, 0x02, 0xcd, 0x6d, 0x00, 0x00, 0x52, 0xa7, 0x6b, 0x0e };
+
+
+/* contents of ael2020_twx_edc.bin */
+/* from microInit_mdio_TWINAX_AEL2020C_10.txt */
+static u8 ael2020_twx_edc_data[] = {
+	0xd8, 0x00, 0x40, 0x09, 0xd8, 0x01, 0x2f, 0xff, 0xd8, 0x02, 0x30, 0x0f, 0xd8, 0x03, 0x40, 0xaa,
+	0xd8, 0x04, 0x40, 0x1c, 0xd8, 0x05, 0x40, 0x1e, 0xd8, 0x06, 0x20, 0xc5, 0xd8, 0x07, 0x3c, 0x05,
+	0xd8, 0x08, 0x65, 0x36, 0xd8, 0x09, 0x2f, 0xe4, 0xd8, 0x0a, 0x3d, 0xc4, 0xd8, 0x0b, 0x66, 0x24,
+	0xd8, 0x0c, 0x2f, 0xf4, 0xd8, 0x0d, 0x3d, 0xc4, 0xd8, 0x0e, 0x20, 0x35, 0xd8, 0x0f, 0x30, 0xa5,
+	0xd8, 0x10, 0x65, 0x24, 0xd8, 0x11, 0x2c, 0xa2, 0xd8, 0x12, 0x30, 0x12, 0xd8, 0x13, 0x10, 0x02,
+	0xd8, 0x14, 0x27, 0xe2, 0xd8, 0x15, 0x30, 0x22, 0xd8, 0x16, 0x10, 0x02, 0xd8, 0x17, 0x28, 0xd2,
+	0xd8, 0x18, 0x30, 0x22, 0xd8, 0x19, 0x10, 0x02, 0xd8, 0x1a, 0x28, 0x92, 0xd8, 0x1b, 0x30, 0x12,
+	0xd8, 0x1c, 0x10, 0x02, 0xd8, 0x1d, 0x24, 0xe2, 0xd8, 0x1e, 0x30, 0x22, 0xd8, 0x1f, 0x10, 0x02,
+	0xd8, 0x20, 0x27, 0xe2, 0xd8, 0x21, 0x30, 0x12, 0xd8, 0x22, 0x10, 0x02, 0xd8, 0x23, 0x24, 0x22,
+	0xd8, 0x24, 0x30, 0x22, 0xd8, 0x25, 0x10, 0x02, 0xd8, 0x26, 0x22, 0xcd, 0xd8, 0x27, 0x30, 0x1d,
+	0xd8, 0x28, 0x28, 0xf2, 0xd8, 0x29, 0x30, 0x22, 0xd8, 0x2a, 0x10, 0x02, 0xd8, 0x2b, 0x55, 0x53,
+	0xd8, 0x2c, 0x03, 0x07, 0xd8, 0x2d, 0x25, 0x72, 0xd8, 0x2e, 0x30, 0x22, 0xd8, 0x2f, 0x10, 0x02,
+	0xd8, 0x30, 0x21, 0xa2, 0xd8, 0x31, 0x30, 0x12, 0xd8, 0x32, 0x10, 0x02, 0xd8, 0x33, 0x40, 0x16,
+	0xd8, 0x34, 0x5e, 0x63, 0xd8, 0x35, 0x03, 0x44, 0xd8, 0x36, 0x21, 0xa2, 0xd8, 0x37, 0x30, 0x12,
+	0xd8, 0x38, 0x10, 0x02, 0xd8, 0x39, 0x40, 0x0e, 0xd8, 0x3a, 0x25, 0x72, 0xd8, 0x3b, 0x30, 0x22,
+	0xd8, 0x3c, 0x10, 0x02, 0xd8, 0x3d, 0x2b, 0x22, 0xd8, 0x3e, 0x30, 0x12, 0xd8, 0x3f, 0x10, 0x02,
+	0xd8, 0x40, 0x28, 0x42, 0xd8, 0x41, 0x30, 0x22, 0xd8, 0x42, 0x10, 0x02, 0xd8, 0x43, 0x26, 0xe2,
+	0xd8, 0x44, 0x30, 0x22, 0xd8, 0x45, 0x10, 0x02, 0xd8, 0x46, 0x2f, 0xa4, 0xd8, 0x47, 0x3d, 0xc4,
+	0xd8, 0x48, 0x66, 0x24, 0xd8, 0x49, 0x2e, 0x8b, 0xd8, 0x4a, 0x30, 0x3b, 0xd8, 0x4b, 0x56, 0xb3,
+	0xd8, 0x4c, 0x03, 0xc6, 0xd8, 0x4d, 0x86, 0x6b, 0xd8, 0x4e, 0x40, 0x0c, 0xd8, 0x4f, 0x27, 0x82,
+	0xd8, 0x50, 0x30, 0x12, 0xd8, 0x51, 0x10, 0x02, 0xd8, 0x52, 0x2c, 0x4b, 0xd8, 0x53, 0x30, 0x9b,
+	0xd8, 0x54, 0x56, 0xb3, 0xd8, 0x55, 0x03, 0xc3, 0xd8, 0x56, 0x86, 0x6b, 0xd8, 0x57, 0x40, 0x0c,
+	0xd8, 0x58, 0x22, 0xa2, 0xd8, 0x59, 0x30, 0x22, 0xd8, 0x5a, 0x10, 0x02, 0xd8, 0x5b, 0x28, 0x42,
+	0xd8, 0x5c, 0x30, 0x22, 0xd8, 0x5d, 0x10, 0x02, 0xd8, 0x5e, 0x26, 0xe2, 0xd8, 0x5f, 0x30, 0x22,
+	0xd8, 0x60, 0x10, 0x02, 0xd8, 0x61, 0x2f, 0xb4, 0xd8, 0x62, 0x3d, 0xc4, 0xd8, 0x63, 0x66, 0x24,
+	0xd8, 0x64, 0x56, 0xb3, 0xd8, 0x65, 0x03, 0xc3, 0xd8, 0x66, 0x86, 0x6b, 0xd8, 0x67, 0x40, 0x1c,
+	0xd8, 0x68, 0x2c, 0x45, 0xd8, 0x69, 0x30, 0x95, 0xd8, 0x6a, 0x5b, 0x53, 0xd8, 0x6b, 0x23, 0xd2,
+	0xd8, 0x6c, 0x30, 0x12, 0xd8, 0x6d, 0x13, 0xc2, 0xd8, 0x6e, 0x5c, 0xc3, 0xd8, 0x6f, 0x27, 0x82,
+	0xd8, 0x70, 0x30, 0x12, 0xd8, 0x71, 0x13, 0x12, 0xd8, 0x72, 0x2b, 0x22, 0xd8, 0x73, 0x30, 0x12,
+	0xd8, 0x74, 0x10, 0x02, 0xd8, 0x75, 0x28, 0x42, 0xd8, 0x76, 0x30, 0x22, 0xd8, 0x77, 0x10, 0x02,
+	0xd8, 0x78, 0x26, 0x22, 0xd8, 0x79, 0x30, 0x22, 0xd8, 0x7a, 0x10, 0x02, 0xd8, 0x7b, 0x21, 0xa2,
+	0xd8, 0x7c, 0x30, 0x12, 0xd8, 0x7d, 0x10, 0x02, 0xd8, 0x7e, 0x62, 0x8f, 0xd8, 0x7f, 0x29, 0x85,
+	0xd8, 0x80, 0x33, 0xa5, 0xd8, 0x81, 0x26, 0xe2, 0xd8, 0x82, 0x30, 0x22, 0xd8, 0x83, 0x10, 0x02,
+	0xd8, 0x84, 0x56, 0x53, 0xd8, 0x85, 0x03, 0xd2, 0xd8, 0x86, 0x40, 0x1e, 0xd8, 0x87, 0x6f, 0x72,
+	0xd8, 0x88, 0x10, 0x02, 0xd8, 0x89, 0x62, 0x8f, 0xd8, 0x8a, 0x23, 0x04, 0xd8, 0x8b, 0x3c, 0x84,
+	0xd8, 0x8c, 0x64, 0x36, 0xd8, 0x8d, 0xdf, 0xf4, 0xd8, 0x8e, 0x64, 0x36, 0xd8, 0x8f, 0x2f, 0xf5,
+	0xd8, 0x90, 0x30, 0x05, 0xd8, 0x91, 0x86, 0x56, 0xd8, 0x92, 0xdf, 0xba, 0xd8, 0x93, 0x56, 0xa3,
+	0xd8, 0x94, 0xd0, 0x5a, 0xd8, 0x95, 0x29, 0xe2, 0xd8, 0x96, 0x30, 0x12, 0xd8, 0x97, 0x13, 0x92,
+	0xd8, 0x98, 0xd0, 0x5a, 0xd8, 0x99, 0x56, 0xa3, 0xd8, 0x9a, 0xdf, 0xba, 0xd8, 0x9b, 0x03, 0x83,
+	0xd8, 0x9c, 0x6f, 0x72, 0xd8, 0x9d, 0x10, 0x02, 0xd8, 0x9e, 0x2a, 0x64, 0xd8, 0x9f, 0x30, 0x14,
+	0xd8, 0xa0, 0x20, 0x05, 0xd8, 0xa1, 0x3d, 0x75, 0xd8, 0xa2, 0xc4, 0x51, 0xd8, 0xa3, 0x29, 0xa2,
+	0xd8, 0xa4, 0x30, 0x22, 0xd8, 0xa5, 0x10, 0x02, 0xd8, 0xa6, 0x17, 0x8c, 0xd8, 0xa7, 0x18, 0x98,
+	0xd8, 0xa8, 0x19, 0xa4, 0xd8, 0xa9, 0x1a, 0xb0, 0xd8, 0xaa, 0x1b, 0xbc, 0xd8, 0xab, 0x1c, 0xc8,
+	0xd8, 0xac, 0x1d, 0xd3, 0xd8, 0xad, 0x1e, 0xde, 0xd8, 0xae, 0x1f, 0xe9, 0xd8, 0xaf, 0x20, 0xf4,
+	0xd8, 0xb0, 0x21, 0xff, 0xd8, 0xb1, 0x00, 0x00, 0xd8, 0xb2, 0x27, 0x41, 0xd8, 0xb3, 0x30, 0x21,
+	0xd8, 0xb4, 0x10, 0x01, 0xd8, 0xb5, 0xc6, 0x20, 0xd8, 0xb6, 0x00, 0x00, 0xd8, 0xb7, 0xc6, 0x21,
+	0xd8, 0xb8, 0x00, 0x00, 0xd8, 0xb9, 0xc6, 0x22, 0xd8, 0xba, 0x00, 0xe2, 0xd8, 0xbb, 0xc6, 0x23,
+	0xd8, 0xbc, 0x00, 0x7f, 0xd8, 0xbd, 0xc6, 0x24, 0xd8, 0xbe, 0x00, 0xce, 0xd8, 0xbf, 0xc6, 0x25,
+	0xd8, 0xc0, 0x00, 0x00, 0xd8, 0xc1, 0xc6, 0x27, 0xd8, 0xc2, 0x00, 0x00, 0xd8, 0xc3, 0xc6, 0x28,
+	0xd8, 0xc4, 0x00, 0x00, 0xd8, 0xc5, 0xc9, 0x0a, 0xd8, 0xc6, 0x3a, 0x7c, 0xd8, 0xc7, 0xc6, 0x2c,
+	0xd8, 0xc8, 0x00, 0x00, 0xd8, 0xc9, 0x00, 0x00, 0xd8, 0xca, 0x27, 0x41, 0xd8, 0xcb, 0x30, 0x21,
+	0xd8, 0xcc, 0x10, 0x01, 0xd8, 0xcd, 0xc5, 0x02, 0xd8, 0xce, 0x53, 0xac, 0xd8, 0xcf, 0xc5, 0x03,
+	0xd8, 0xd0, 0x2c, 0xd3, 0xd8, 0xd1, 0xc6, 0x00, 0xd8, 0xd2, 0x2a, 0x6e, 0xd8, 0xd3, 0xc6, 0x01,
+	0xd8, 0xd4, 0x2a, 0x2c, 0xd8, 0xd5, 0xc6, 0x05, 0xd8, 0xd6, 0x55, 0x57, 0xd8, 0xd7, 0xc6, 0x0c,
+	0xd8, 0xd8, 0x54, 0x00, 0xd8, 0xd9, 0xc7, 0x10, 0xd8, 0xda, 0x07, 0x00, 0xd8, 0xdb, 0xc7, 0x11,
+	0xd8, 0xdc, 0x0f, 0x06, 0xd8, 0xdd, 0xc7, 0x18, 0xd8, 0xde, 0x07, 0x00, 0xd8, 0xdf, 0xc7, 0x19,
+	0xd8, 0xe0, 0x0f, 0x06, 0xd8, 0xe1, 0xc7, 0x20, 0xd8, 0xe2, 0x47, 0x00, 0xd8, 0xe3, 0xc7, 0x21,
+	0xd8, 0xe4, 0x0f, 0x06, 0xd8, 0xe5, 0xc7, 0x28, 0xd8, 0xe6, 0x07, 0x00, 0xd8, 0xe7, 0xc7, 0x29,
+	0xd8, 0xe8, 0x12, 0x07, 0xd8, 0xe9, 0xc8, 0x01, 0xd8, 0xea, 0x7f, 0x50, 0xd8, 0xeb, 0xc8, 0x02,
+	0xd8, 0xec, 0x77, 0x60, 0xd8, 0xed, 0xc8, 0x03, 0xd8, 0xee, 0x7f, 0xce, 0xd8, 0xef, 0xc8, 0x04,
+	0xd8, 0xf0, 0x52, 0x0e, 0xd8, 0xf1, 0xc8, 0x05, 0xd8, 0xf2, 0x5c, 0x11, 0xd8, 0xf3, 0xc8, 0x06,
+	0xd8, 0xf4, 0x3c, 0x51, 0xd8, 0xf5, 0xc8, 0x07, 0xd8, 0xf6, 0x40, 0x61, 0xd8, 0xf7, 0xc8, 0x08,
+	0xd8, 0xf8, 0x49, 0xc1, 0xd8, 0xf9, 0xc8, 0x09, 0xd8, 0xfa, 0x38, 0x40, 0xd8, 0xfb, 0xc8, 0x0a,
+	0xd8, 0xfc, 0x00, 0x00, 0xd8, 0xfd, 0xc8, 0x21, 0xd8, 0xfe, 0x00, 0x02, 0xd8, 0xff, 0xc8, 0x22,
+	0xd9, 0x00, 0x00, 0x46, 0xd9, 0x01, 0xc8, 0x44, 0xd9, 0x02, 0x18, 0x2f, 0xd9, 0x03, 0xc8, 0x49,
+	0xd9, 0x04, 0x04, 0x00, 0xd9, 0x05, 0xc8, 0x4a, 0xd9, 0x06, 0x00, 0x02, 0xd9, 0x07, 0xc0, 0x13,
+	0xd9, 0x08, 0xf3, 0x41, 0xd9, 0x09, 0xc0, 0x84, 0xd9, 0x0a, 0x00, 0x30, 0xd9, 0x0b, 0xc9, 0x04,
+	0xd9, 0x0c, 0x14, 0x01, 0xd9, 0x0d, 0xcb, 0x0c, 0xd9, 0x0e, 0x00, 0x04, 0xd9, 0x0f, 0xcb, 0x0e,
+	0xd9, 0x10, 0xa0, 0x0a, 0xd9, 0x11, 0xcb, 0x0f, 0xd9, 0x12, 0xc0, 0xc0, 0xd9, 0x13, 0xcb, 0x10,
+	0xd9, 0x14, 0xc0, 0xc0, 0xd9, 0x15, 0xcb, 0x11, 0xd9, 0x16, 0x00, 0xa0, 0xd9, 0x17, 0xcb, 0x12,
+	0xd9, 0x18, 0x00, 0x07, 0xd9, 0x19, 0xc2, 0x41, 0xd9, 0x1a, 0xa0, 0x00, 0xd9, 0x1b, 0xc2, 0x43,
+	0xd9, 0x1c, 0x7f, 0xe0, 0xd9, 0x1d, 0xc6, 0x04, 0xd9, 0x1e, 0x00, 0x0e, 0xd9, 0x1f, 0xc6, 0x09,
+	0xd9, 0x20, 0x00, 0xf5, 0xd9, 0x21, 0xc6, 0x11, 0xd9, 0x22, 0x00, 0x0e, 0xd9, 0x23, 0xc6, 0x60,
+	0xd9, 0x24, 0x96, 0x00, 0xd9, 0x25, 0xc6, 0x87, 0xd9, 0x26, 0x00, 0x04, 0xd9, 0x27, 0xc6, 0x0a,
+	0xd9, 0x28, 0x04, 0xf5, 0xd9, 0x29, 0x00, 0x00, 0xd9, 0x2a, 0x27, 0x41, 0xd9, 0x2b, 0x30, 0x21,
+	0xd9, 0x2c, 0x10, 0x01, 0xd9, 0x2d, 0xc6, 0x20, 0xd9, 0x2e, 0x14, 0xe5, 0xd9, 0x2f, 0xc6, 0x21,
+	0xd9, 0x30, 0xc5, 0x3d, 0xd9, 0x31, 0xc6, 0x22, 0xd9, 0x32, 0x3c, 0xbe, 0xd9, 0x33, 0xc6, 0x23,
+	0xd9, 0x34, 0x44, 0x52, 0xd9, 0x35, 0xc6, 0x24, 0xd9, 0x36, 0xc5, 0xc5, 0xd9, 0x37, 0xc6, 0x25,
+	0xd9, 0x38, 0xe0, 0x1e, 0xd9, 0x39, 0xc6, 0x27, 0xd9, 0x3a, 0x00, 0x00, 0xd9, 0x3b, 0xc6, 0x28,
+	0xd9, 0x3c, 0x00, 0x00, 0xd9, 0x3d, 0xc6, 0x2c, 0xd9, 0x3e, 0x00, 0x00, 0xd9, 0x3f, 0xc9, 0x0a,
+	0xd9, 0x40, 0x3a, 0x7c, 0xd9, 0x41, 0x00, 0x00, 0xd9, 0x42, 0x2b, 0x84, 0xd9, 0x43, 0x3c, 0x74,
+	0xd9, 0x44, 0x64, 0x35, 0xd9, 0x45, 0xdf, 0xf4, 0xd9, 0x46, 0x64, 0x35, 0xd9, 0x47, 0x28, 0x06,
+	0xd9, 0x48, 0x30, 0x06, 0xd9, 0x49, 0x85, 0x65, 0xd9, 0x4a, 0x2b, 0x24, 0xd9, 0x4b, 0x3c, 0x24,
+	0xd9, 0x4c, 0x64, 0x36, 0xd9, 0x4d, 0x10, 0x02, 0xd9, 0x4e, 0x2b, 0x24, 0xd9, 0x4f, 0x3c, 0x24,
+	0xd9, 0x50, 0x64, 0x36, 0xd9, 0x51, 0x40, 0x45, 0xd9, 0x52, 0x86, 0x56, 0xd9, 0x53, 0x56, 0x63,
+	0xd9, 0x54, 0x03, 0x02, 0xd9, 0x55, 0x40, 0x1e, 0xd9, 0x56, 0x10, 0x02, 0xd9, 0x57, 0x28, 0x07,
+	0xd9, 0x58, 0x31, 0xa7, 0xd9, 0x59, 0x20, 0xc4, 0xd9, 0x5a, 0x3c, 0x24, 0xd9, 0x5b, 0x67, 0x24,
+	0xd9, 0x5c, 0x2f, 0xf7, 0xd9, 0x5d, 0x30, 0xf7, 0xd9, 0x5e, 0x20, 0xc4, 0xd9, 0x5f, 0x3c, 0x04,
+	0xd9, 0x60, 0x67, 0x24, 0xd9, 0x61, 0x10, 0x02, 0xd9, 0x62, 0x28, 0x07, 0xd9, 0x63, 0x31, 0x87,
+	0xd9, 0x64, 0x20, 0xc4, 0xd9, 0x65, 0x3c, 0x24, 0xd9, 0x66, 0x67, 0x24, 0xd9, 0x67, 0x2f, 0xe4,
+	0xd9, 0x68, 0x3d, 0xc4, 0xd9, 0x69, 0x64, 0x37, 0xd9, 0x6a, 0x20, 0xc4, 0xd9, 0x6b, 0x3c, 0x04,
+	0xd9, 0x6c, 0x67, 0x24, 0xd9, 0x6d, 0x10, 0x02, 0xd9, 0x6e, 0x24, 0xf4, 0xd9, 0x6f, 0x3c, 0x64,
+	0xd9, 0x70, 0x64, 0x36, 0xd9, 0x71, 0xdf, 0xf4, 0xd9, 0x72, 0x64, 0x36, 0xd9, 0x73, 0x10, 0x02,
+	0xd9, 0x74, 0x20, 0x06, 0xd9, 0x75, 0x3d, 0x76, 0xd9, 0x76, 0xc1, 0x61, 0xd9, 0x77, 0x61, 0x34,
+	0xd9, 0x78, 0x61, 0x35, 0xd9, 0x79, 0x54, 0x43, 0xd9, 0x7a, 0x03, 0x03, 0xd9, 0x7b, 0x65, 0x24,
+	0xd9, 0x7c, 0x00, 0xfb, 0xd9, 0x7d, 0x10, 0x02, 0xd9, 0x7e, 0x20, 0xd4, 0xd9, 0x7f, 0x3c, 0x24,
+	0xd9, 0x80, 0x20, 0x25, 0xd9, 0x81, 0x30, 0x05, 0xd9, 0x82, 0x65, 0x24, 0xd9, 0x83, 0x10, 0x02,
+	0xd9, 0x84, 0xd0, 0x19, 0xd9, 0x85, 0x21, 0x04, 0xd9, 0x86, 0x3c, 0x24, 0xd9, 0x87, 0x21, 0x05,
+	0xd9, 0x88, 0x38, 0x05, 0xd9, 0x89, 0x65, 0x24, 0xd9, 0x8a, 0xdf, 0xf4, 0xd9, 0x8b, 0x40, 0x05,
+	0xd9, 0x8c, 0x65, 0x24, 0xd9, 0x8d, 0x2e, 0x8d, 0xd9, 0x8e, 0x30, 0x3d, 0xd9, 0x8f, 0x24, 0x08,
+	0xd9, 0x90, 0x35, 0xd8, 0xd9, 0x91, 0x5d, 0xd3, 0xd9, 0x92, 0x03, 0x07, 0xd9, 0x93, 0x88, 0x87,
+	0xd9, 0x94, 0x63, 0xa7, 0xd9, 0x95, 0x88, 0x87, 0xd9, 0x96, 0x63, 0xa7, 0xd9, 0x97, 0xdf, 0xfd,
+	0xd9, 0x98, 0x00, 0xf9, 0xd9, 0x99, 0x10, 0x02, 0xd9, 0x9a, 0x86, 0x6a, 0xd9, 0x9b, 0x61, 0x38,
+	0xd9, 0x9c, 0x58, 0x83, 0xd9, 0x9d, 0x2a, 0xa2, 0xd9, 0x9e, 0x30, 0x22, 0xd9, 0x9f, 0x13, 0x02,
+	0xd9, 0xa0, 0x2f, 0xf7, 0xd9, 0xa1, 0x30, 0x07, 0xd9, 0xa2, 0x87, 0x85, 0xd9, 0xa3, 0xb8, 0x87,
+	0xd9, 0xa4, 0x87, 0x86, 0xd9, 0xa5, 0xb8, 0xc6, 0xd9, 0xa6, 0x5a, 0x53, 0xd9, 0xa7, 0x29, 0xb2,
+	0xd9, 0xa8, 0x30, 0x22, 0xd9, 0xa9, 0x13, 0xc2, 0xd9, 0xaa, 0x24, 0x74, 0xd9, 0xab, 0x3c, 0x84,
+	0xd9, 0xac, 0x64, 0xd7, 0xd9, 0xad, 0x64, 0xd7, 0xd9, 0xae, 0x2f, 0xf5, 0xd9, 0xaf, 0x3c, 0x05,
+	0xd9, 0xb0, 0x87, 0x57, 0xd9, 0xb1, 0xb8, 0x86, 0xd9, 0xb2, 0x97, 0x67, 0xd9, 0xb3, 0x67, 0xc4,
+	0xd9, 0xb4, 0x6f, 0x72, 0xd9, 0xb5, 0x10, 0x02, 0xd9, 0xb6, 0x00, 0x00, 0x1f, 0xea, 0x7e, 0x84
+};
+
+struct t3_firmware_map {
+	char		name[128];
+	u8		*data;	
+	int		size;
+	struct firmware	*fw;
+};
+
+/* this matches up the firmware names with the raw data files */
+static struct t3_firmware_map fw_map[] = {
+	{ "ael2005_opt_edc.bin", ael2005_opt_edc_data, sizeof(ael2005_opt_edc_data), NULL},
+	{ "ael2005_twx_edc.bin", ael2005_twx_edc_data, sizeof(ael2005_twx_edc_data), NULL },
+	{ "ael2020_twx_edc.bin", ael2020_twx_edc_data, sizeof(ael2020_twx_edc_data), NULL },
+};
+
+static inline int t3_local_firmware_free(const struct firmware *firmware)
+{
+	int i;
+
+	for (i=0; i < (sizeof(fw_map) / sizeof(struct t3_firmware_map)); i++)
+		if (fw_map[i].fw == firmware) {
+			kfree(firmware);
+			fw_map[i].fw = NULL;
+			return 0;
+		}
+	return -ENODEV;
+}
+
+/* one function to pass the data out when a filename is requested */
+static inline int t3_local_firmware_load(const struct firmware **firmware, const char *name)
+{
+	int i; 
+	struct firmware *fw;
+	struct t3_firmware_map *t3fw_entry = NULL;
+
+	/* check to see if we've got that entry in our table */
+	for (i=0; i < (sizeof(fw_map) / sizeof(struct t3_firmware_map)); i++) {
+		if (strstr(name, fw_map[i].name)) {
+			t3fw_entry = &fw_map[i];
+		}
+	}
+
+	/* if we've got a hit, allocate the memory and populate the pointer */
+	if (t3fw_entry) {
+		*firmware = fw = kzalloc(sizeof(*fw), GFP_KERNEL);
+		if (!fw) {
+			printk(KERN_ERR "%s: kmalloc(struct firmware) failed\n",
+					__FUNCTION__);
+			return -ENOMEM;
+		}
+		fw->data = t3fw_entry->data;
+		fw->size = t3fw_entry->size;
+		t3fw_entry->fw = fw;
+		printk("firmware: trying embedded firmware since %s unavailable.\n", name);
+	}
+
+	return 0;
+}
diff --git a/drivers/net/cxgb3/t3_hw.c b/drivers/net/cxgb3/t3_hw.c
--- a/drivers/net/cxgb3/t3_hw.c
+++ b/drivers/net/cxgb3/t3_hw.c
@@ -1,34 +1,14 @@
 /*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include "common.h"
 #include "regs.h"
 #include "sge_defs.h"
@@ -47,11 +27,10 @@
  *	Wait until an operation is completed by checking a bit in a register
  *	up to @attempts times.  If @valp is not NULL the value of the register
  *	at the time it indicated completion is stored there.  Returns 0 if the
- *	operation completes and -EAGAIN otherwise.
+ *	operation completes and	-EAGAIN	otherwise.
  */
-
-int t3_wait_op_done_val(struct adapter *adapter, int reg, u32 mask,
-			int polarity, int attempts, int delay, u32 *valp)
+int t3_wait_op_done_val(adapter_t *adapter, int reg, u32 mask, int polarity,
+			int attempts, int delay, u32 *valp)
 {
 	while (1) {
 		u32 val = t3_read_reg(adapter, reg);
@@ -79,8 +58,8 @@
  *	value to the corresponding register.  Register addresses are adjusted
  *	by the supplied offset.
  */
-void t3_write_regs(struct adapter *adapter, const struct addr_val_pair *p,
-		   int n, unsigned int offset)
+void t3_write_regs(adapter_t *adapter, const struct addr_val_pair *p, int n,
+		   unsigned int offset)
 {
 	while (n--) {
 		t3_write_reg(adapter, p->reg_addr + offset, p->val);
@@ -98,13 +77,12 @@
  *	Sets a register field specified by the supplied mask to the
  *	given value.
  */
-void t3_set_reg_field(struct adapter *adapter, unsigned int addr, u32 mask,
-		      u32 val)
+void t3_set_reg_field(adapter_t *adapter, unsigned int addr, u32 mask, u32 val)
 {
 	u32 v = t3_read_reg(adapter, addr) & ~mask;
 
 	t3_write_reg(adapter, addr, v | val);
-	t3_read_reg(adapter, addr);	/* flush */
+	(void) t3_read_reg(adapter, addr);      /* flush */
 }
 
 /**
@@ -119,9 +97,9 @@
  *	Reads registers that are accessed indirectly through an address/data
  *	register pair.
  */
-static void t3_read_indirect(struct adapter *adap, unsigned int addr_reg,
-			     unsigned int data_reg, u32 *vals,
-			     unsigned int nregs, unsigned int start_idx)
+void t3_read_indirect(adapter_t *adap, unsigned int addr_reg,
+		      unsigned int data_reg, u32 *vals, unsigned int nregs,
+		      unsigned int start_idx)
 {
 	while (nregs--) {
 		t3_write_reg(adap, addr_reg, start_idx);
@@ -141,13 +119,13 @@
  *	accesses.
  */
 int t3_mc7_bd_read(struct mc7 *mc7, unsigned int start, unsigned int n,
-		   u64 *buf)
+                   u64 *buf)
 {
-	static const int shift[] = { 0, 0, 16, 24 };
-	static const int step[] = { 0, 32, 16, 8 };
-
-	unsigned int size64 = mc7->size / 8;	/* # of 64-bit words */
-	struct adapter *adap = mc7->adapter;
+	static int shift[] = { 0, 0, 16, 24 };
+	static int step[]  = { 0, 32, 16, 8 };
+
+	unsigned int size64 = mc7->size / 8;  /* # of 64-bit words */
+	adapter_t *adap = mc7->adapter;
 
 	if (start >= size64 || start + n > size64)
 		return -EINVAL;
@@ -161,7 +139,8 @@
 			int attempts = 10;
 			u32 val;
 
-			t3_write_reg(adap, mc7->offset + A_MC7_BD_ADDR, start);
+			t3_write_reg(adap, mc7->offset + A_MC7_BD_ADDR,
+				       start);
 			t3_write_reg(adap, mc7->offset + A_MC7_BD_OP, 0);
 			val = t3_read_reg(adap, mc7->offset + A_MC7_BD_OP);
 			while ((val & F_BUSY) && attempts--)
@@ -173,13 +152,12 @@
 			val = t3_read_reg(adap, mc7->offset + A_MC7_BD_DATA1);
 			if (mc7->width == 0) {
 				val64 = t3_read_reg(adap,
-						    mc7->offset +
-						    A_MC7_BD_DATA0);
-				val64 |= (u64) val << 32;
+						mc7->offset + A_MC7_BD_DATA0);
+				val64 |= (u64)val << 32;
 			} else {
 				if (mc7->width > 1)
 					val >>= shift[mc7->width];
-				val64 |= (u64) val << (step[mc7->width] * i);
+				val64 |= (u64)val << (step[mc7->width] * i);
 			}
 			start += 8;
 		}
@@ -191,12 +169,12 @@
 /*
  * Initialize MI1.
  */
-static void mi1_init(struct adapter *adap, const struct adapter_info *ai)
+static void mi1_init(adapter_t *adap, const struct adapter_info *ai)
 {
-	u32 clkdiv = adap->params.vpd.cclk / (2 * adap->params.vpd.mdc) - 1;
-	u32 val = F_PREEN | V_CLKDIV(clkdiv);
-
-	t3_write_reg(adap, A_MI1_CFG, val);
+        u32 clkdiv = adap->params.vpd.cclk / (2 * adap->params.vpd.mdc) - 1;
+        u32 val = F_PREEN | V_CLKDIV(clkdiv);
+
+        t3_write_reg(adap, A_MI1_CFG, val);
 }
 
 #define MDIO_ATTEMPTS 20
@@ -204,112 +182,101 @@
 /*
  * MI1 read/write operations for clause 22 PHYs.
  */
-static int t3_mi1_read(struct net_device *dev, int phy_addr, int mmd_addr,
-		       u16 reg_addr)
+int t3_mi1_read(adapter_t *adapter, int phy_addr, int mmd_addr,
+		int reg_addr, unsigned int *valp)
 {
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
 	int ret;
 	u32 addr = V_REGADDR(reg_addr) | V_PHYADDR(phy_addr);
 
-	mutex_lock(&adapter->mdio_lock);
+	if (mmd_addr)
+		return -EINVAL;
+
+	MDIO_LOCK(adapter);
 	t3_set_reg_field(adapter, A_MI1_CFG, V_ST(M_ST), V_ST(1));
 	t3_write_reg(adapter, A_MI1_ADDR, addr);
 	t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(2));
 	ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0, MDIO_ATTEMPTS, 10);
 	if (!ret)
-		ret = t3_read_reg(adapter, A_MI1_DATA);
-	mutex_unlock(&adapter->mdio_lock);
+		*valp = t3_read_reg(adapter, A_MI1_DATA);
+	MDIO_UNLOCK(adapter);
 	return ret;
 }
 
-static int t3_mi1_write(struct net_device *dev, int phy_addr, int mmd_addr,
-			u16 reg_addr, u16 val)
+int t3_mi1_write(adapter_t *adapter, int phy_addr, int mmd_addr,
+		 int reg_addr, unsigned int val)
 {
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
 	int ret;
 	u32 addr = V_REGADDR(reg_addr) | V_PHYADDR(phy_addr);
 
-	mutex_lock(&adapter->mdio_lock);
+	if (mmd_addr)
+		return -EINVAL;
+
+	MDIO_LOCK(adapter);
 	t3_set_reg_field(adapter, A_MI1_CFG, V_ST(M_ST), V_ST(1));
 	t3_write_reg(adapter, A_MI1_ADDR, addr);
 	t3_write_reg(adapter, A_MI1_DATA, val);
 	t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(1));
 	ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0, MDIO_ATTEMPTS, 10);
-	mutex_unlock(&adapter->mdio_lock);
+	MDIO_UNLOCK(adapter);
 	return ret;
 }
 
-static const struct mdio_ops mi1_mdio_ops = {
-	.read = t3_mi1_read,
-	.write = t3_mi1_write,
-	.mode_support = MDIO_SUPPORTS_C22
+static struct mdio_ops mi1_mdio_ops = {
+	t3_mi1_read,
+	t3_mi1_write
 };
 
 /*
- * Performs the address cycle for clause 45 PHYs.
- * Must be called with the MDIO_LOCK held.
+ * MI1 read/write operations for clause 45 PHYs.
  */
-static int mi1_wr_addr(struct adapter *adapter, int phy_addr, int mmd_addr,
-		       int reg_addr)
+static int mi1_ext_read(adapter_t *adapter, int phy_addr, int mmd_addr,
+			int reg_addr, unsigned int *valp)
 {
+	int ret;
 	u32 addr = V_REGADDR(mmd_addr) | V_PHYADDR(phy_addr);
 
+	MDIO_LOCK(adapter);
 	t3_set_reg_field(adapter, A_MI1_CFG, V_ST(M_ST), 0);
 	t3_write_reg(adapter, A_MI1_ADDR, addr);
 	t3_write_reg(adapter, A_MI1_DATA, reg_addr);
 	t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(0));
-	return t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0,
-			       MDIO_ATTEMPTS, 10);
-}
-
-/*
- * MI1 read/write operations for indirect-addressed PHYs.
- */
-static int mi1_ext_read(struct net_device *dev, int phy_addr, int mmd_addr,
-			u16 reg_addr)
-{
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
-	int ret;
-
-	mutex_lock(&adapter->mdio_lock);
-	ret = mi1_wr_addr(adapter, phy_addr, mmd_addr, reg_addr);
+	ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0, MDIO_ATTEMPTS, 10);
 	if (!ret) {
 		t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(3));
 		ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0,
 				      MDIO_ATTEMPTS, 10);
 		if (!ret)
-			ret = t3_read_reg(adapter, A_MI1_DATA);
+			*valp = t3_read_reg(adapter, A_MI1_DATA);
 	}
-	mutex_unlock(&adapter->mdio_lock);
+	MDIO_UNLOCK(adapter);
 	return ret;
 }
 
-static int mi1_ext_write(struct net_device *dev, int phy_addr, int mmd_addr,
-			 u16 reg_addr, u16 val)
+static int mi1_ext_write(adapter_t *adapter, int phy_addr, int mmd_addr,
+			 int reg_addr, unsigned int val)
 {
-	struct port_info *pi = netdev_priv(dev);
-	struct adapter *adapter = pi->adapter;
 	int ret;
-
-	mutex_lock(&adapter->mdio_lock);
-	ret = mi1_wr_addr(adapter, phy_addr, mmd_addr, reg_addr);
+	u32 addr = V_REGADDR(mmd_addr) | V_PHYADDR(phy_addr);
+
+	MDIO_LOCK(adapter);
+	t3_set_reg_field(adapter, A_MI1_CFG, V_ST(M_ST), 0);
+	t3_write_reg(adapter, A_MI1_ADDR, addr);
+	t3_write_reg(adapter, A_MI1_DATA, reg_addr);
+	t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(0));
+	ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0, MDIO_ATTEMPTS, 10);
 	if (!ret) {
 		t3_write_reg(adapter, A_MI1_DATA, val);
 		t3_write_reg(adapter, A_MI1_OP, V_MDI_OP(1));
 		ret = t3_wait_op_done(adapter, A_MI1_OP, F_BUSY, 0,
 				      MDIO_ATTEMPTS, 10);
 	}
-	mutex_unlock(&adapter->mdio_lock);
+	MDIO_UNLOCK(adapter);
 	return ret;
 }
 
-static const struct mdio_ops mi1_mdio_ext_ops = {
-	.read = mi1_ext_read,
-	.write = mi1_ext_write,
-	.mode_support = MDIO_SUPPORTS_C45 | MDIO_EMULATE_C22
+static struct mdio_ops mi1_mdio_ext_ops = {
+	mi1_ext_read,
+	mi1_ext_write
 };
 
 /**
@@ -329,10 +296,10 @@
 	int ret;
 	unsigned int val;
 
-	ret = t3_mdio_read(phy, mmd, reg, &val);
+	ret = mdio_read(phy, mmd, reg, &val);
 	if (!ret) {
 		val &= ~clear;
-		ret = t3_mdio_write(phy, mmd, reg, val | set);
+		ret = mdio_write(phy, mmd, reg, val | set);
 	}
 	return ret;
 }
@@ -352,16 +319,15 @@
 	int err;
 	unsigned int ctl;
 
-	err = t3_mdio_change_bits(phy, mmd, MDIO_CTRL1, MDIO_CTRL1_LPOWER,
-				  MDIO_CTRL1_RESET);
+	err = t3_mdio_change_bits(phy, mmd, MII_BMCR, BMCR_PDOWN, BMCR_RESET);
 	if (err || !wait)
 		return err;
 
 	do {
-		err = t3_mdio_read(phy, mmd, MDIO_CTRL1, &ctl);
+		err = mdio_read(phy, mmd, MII_BMCR, &ctl);
 		if (err)
 			return err;
-		ctl &= MDIO_CTRL1_RESET;
+		ctl &= BMCR_RESET;
 		if (ctl)
 			msleep(1);
 	} while (ctl && --wait);
@@ -382,7 +348,7 @@
 	int err;
 	unsigned int val = 0;
 
-	err = t3_mdio_read(phy, MDIO_DEVAD_NONE, MII_CTRL1000, &val);
+	err = mdio_read(phy, 0, MII_CTRL1000, &val);
 	if (err)
 		return err;
 
@@ -392,7 +358,7 @@
 	if (advert & ADVERTISED_1000baseT_Full)
 		val |= ADVERTISE_1000FULL;
 
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, MII_CTRL1000, val);
+	err = mdio_write(phy, 0, MII_CTRL1000, val);
 	if (err)
 		return err;
 
@@ -409,7 +375,7 @@
 		val |= ADVERTISE_PAUSE_CAP;
 	if (advert & ADVERTISED_Asym_Pause)
 		val |= ADVERTISE_PAUSE_ASYM;
-	return t3_mdio_write(phy, MDIO_DEVAD_NONE, MII_ADVERTISE, val);
+	return mdio_write(phy, 0, MII_ADVERTISE, val);
 }
 
 /**
@@ -432,7 +398,7 @@
 		val |= ADVERTISE_1000XPAUSE;
 	if (advert & ADVERTISED_Asym_Pause)
 		val |= ADVERTISE_1000XPSE_ASYM;
-	return t3_mdio_write(phy, MDIO_DEVAD_NONE, MII_ADVERTISE, val);
+	return mdio_write(phy, 0, MII_ADVERTISE, val);
 }
 
 /**
@@ -449,7 +415,7 @@
 	int err;
 	unsigned int ctl;
 
-	err = t3_mdio_read(phy, MDIO_DEVAD_NONE, MII_BMCR, &ctl);
+	err = mdio_read(phy, 0, MII_BMCR, &ctl);
 	if (err)
 		return err;
 
@@ -465,72 +431,74 @@
 		if (duplex == DUPLEX_FULL)
 			ctl |= BMCR_FULLDPLX;
 	}
-	if (ctl & BMCR_SPEED1000) /* auto-negotiation required for GigE */
+	if (ctl & BMCR_SPEED1000)  /* auto-negotiation required for GigE */
 		ctl |= BMCR_ANENABLE;
-	return t3_mdio_write(phy, MDIO_DEVAD_NONE, MII_BMCR, ctl);
+	return mdio_write(phy, 0, MII_BMCR, ctl);
 }
 
 int t3_phy_lasi_intr_enable(struct cphy *phy)
 {
-	return t3_mdio_write(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_CTRL,
-			     MDIO_PMA_LASI_LSALARM);
+	return mdio_write(phy, MDIO_DEV_PMA_PMD, LASI_CTRL, 1);
 }
 
 int t3_phy_lasi_intr_disable(struct cphy *phy)
 {
-	return t3_mdio_write(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_CTRL, 0);
+	return mdio_write(phy, MDIO_DEV_PMA_PMD, LASI_CTRL, 0);
 }
 
 int t3_phy_lasi_intr_clear(struct cphy *phy)
 {
 	u32 val;
 
-	return t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_STAT, &val);
+	return mdio_read(phy, MDIO_DEV_PMA_PMD, LASI_STAT, &val);
 }
 
 int t3_phy_lasi_intr_handler(struct cphy *phy)
 {
 	unsigned int status;
-	int err = t3_mdio_read(phy, MDIO_MMD_PMAPMD, MDIO_PMA_LASI_STAT,
-			       &status);
+	int err = mdio_read(phy, MDIO_DEV_PMA_PMD, LASI_STAT, &status);
 
 	if (err)
 		return err;
-	return (status & MDIO_PMA_LASI_LSALARM) ? cphy_cause_link_change : 0;
+	return (status & 1) ?  cphy_cause_link_change : 0;
 }
 
-static const struct adapter_info t3_adap_info[] = {
-	{1, 1, 0,
-	 F_GPIO2_OEN | F_GPIO4_OEN |
-	 F_GPIO2_OUT_VAL | F_GPIO4_OUT_VAL, { S_GPIO3, S_GPIO5 }, 0,
-	 &mi1_mdio_ops, "Chelsio PE9000"},
-	{1, 1, 0,
-	 F_GPIO2_OEN | F_GPIO4_OEN |
-	 F_GPIO2_OUT_VAL | F_GPIO4_OUT_VAL, { S_GPIO3, S_GPIO5 }, 0,
-	 &mi1_mdio_ops, "Chelsio T302"},
-	{1, 0, 0,
-	 F_GPIO1_OEN | F_GPIO6_OEN | F_GPIO7_OEN | F_GPIO10_OEN |
-	 F_GPIO11_OEN | F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
-	 { 0 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
-	 &mi1_mdio_ext_ops, "Chelsio T310"},
-	{1, 1, 0,
-	 F_GPIO1_OEN | F_GPIO2_OEN | F_GPIO4_OEN | F_GPIO5_OEN | F_GPIO6_OEN |
-	 F_GPIO7_OEN | F_GPIO10_OEN | F_GPIO11_OEN | F_GPIO1_OUT_VAL |
-	 F_GPIO5_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
-	 { S_GPIO9, S_GPIO3 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
-	 &mi1_mdio_ext_ops, "Chelsio T320"},
-	{},
-	{},
-	{1, 0, 0,
-	 F_GPIO1_OEN | F_GPIO2_OEN | F_GPIO4_OEN | F_GPIO6_OEN | F_GPIO7_OEN |
-	 F_GPIO10_OEN | F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
-	 { S_GPIO9 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
-	 &mi1_mdio_ext_ops, "Chelsio T310" },
-	{1, 0, 0,
-	 F_GPIO1_OEN | F_GPIO6_OEN | F_GPIO7_OEN |
-	 F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL,
-	 { S_GPIO9 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
-	 &mi1_mdio_ext_ops, "Chelsio N320E-G2" },
+static struct adapter_info t3_adap_info[] = {
+	{ 1, 1, 0,
+	  F_GPIO2_OEN | F_GPIO4_OEN |
+	  F_GPIO2_OUT_VAL | F_GPIO4_OUT_VAL, { S_GPIO3, S_GPIO5 }, 0,
+	  &mi1_mdio_ops, "Chelsio PE9000" },
+	{ 1, 1, 0,
+	  F_GPIO2_OEN | F_GPIO4_OEN |
+	  F_GPIO2_OUT_VAL | F_GPIO4_OUT_VAL, { S_GPIO3, S_GPIO5 }, 0,
+	  &mi1_mdio_ops, "Chelsio T302" },
+	{ 1, 0, 0,
+	  F_GPIO1_OEN | F_GPIO6_OEN | F_GPIO7_OEN | F_GPIO10_OEN |
+	  F_GPIO11_OEN | F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
+	  { 0 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
+	  &mi1_mdio_ext_ops, "Chelsio T310" },
+	{ 1, 1, 0,
+	  F_GPIO1_OEN | F_GPIO2_OEN | F_GPIO4_OEN | F_GPIO5_OEN | F_GPIO6_OEN |
+	  F_GPIO7_OEN | F_GPIO10_OEN | F_GPIO11_OEN | F_GPIO1_OUT_VAL |
+	  F_GPIO5_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
+	  { S_GPIO9, S_GPIO3 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
+	  &mi1_mdio_ext_ops, "Chelsio T320" },
+	{ 4, 0, 0,
+	  F_GPIO5_OEN | F_GPIO6_OEN | F_GPIO7_OEN | F_GPIO5_OUT_VAL |
+	  F_GPIO6_OUT_VAL | F_GPIO7_OUT_VAL,
+	  { S_GPIO1, S_GPIO2, S_GPIO3, S_GPIO4 }, SUPPORTED_AUI,
+	  &mi1_mdio_ops, "Chelsio T304" },
+	{ },
+	{ 1, 0, 0,
+	  F_GPIO1_OEN | F_GPIO2_OEN | F_GPIO4_OEN | F_GPIO6_OEN | F_GPIO7_OEN |
+	  F_GPIO10_OEN | F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL | F_GPIO10_OUT_VAL,
+	  { S_GPIO9 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
+	  &mi1_mdio_ext_ops, "Chelsio T310" },
+	{ 1, 0, 0,
+	  F_GPIO1_OEN | F_GPIO6_OEN | F_GPIO7_OEN | 
+	  F_GPIO1_OUT_VAL | F_GPIO6_OUT_VAL,
+	  { S_GPIO9 }, SUPPORTED_10000baseT_Full | SUPPORTED_AUI,
+	  &mi1_mdio_ext_ops, "Chelsio N320E-G2" },
 };
 
 /*
@@ -543,20 +511,20 @@
 }
 
 struct port_type_info {
-	int (*phy_prep)(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *ops);
+	int (*phy_prep)(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *ops);
 };
 
-static const struct port_type_info port_types[] = {
+static struct port_type_info port_types[] = {
 	{ NULL },
 	{ t3_ael1002_phy_prep },
 	{ t3_vsc8211_phy_prep },
-	{ NULL},
+	{ t3_mv88e1xxx_phy_prep },
 	{ t3_xaui_direct_phy_prep },
 	{ t3_ael2005_phy_prep },
 	{ t3_qt2045_phy_prep },
 	{ t3_ael1006_phy_prep },
-	{ NULL },
+	{ t3_tn1010_phy_prep },
 	{ t3_aq100x_phy_prep },
 	{ t3_ael2020_phy_prep },
 };
@@ -569,28 +537,28 @@
  * VPD-R sections.
  */
 struct t3_vpd {
-	u8 id_tag;
-	u8 id_len[2];
-	u8 id_data[16];
-	u8 vpdr_tag;
-	u8 vpdr_len[2];
-	VPD_ENTRY(pn, 16);	/* part number */
-	VPD_ENTRY(ec, 16);	/* EC level */
-	VPD_ENTRY(sn, SERNUM_LEN); /* serial number */
-	VPD_ENTRY(na, 12);	/* MAC address base */
-	VPD_ENTRY(cclk, 6);	/* core clock */
-	VPD_ENTRY(mclk, 6);	/* mem clock */
-	VPD_ENTRY(uclk, 6);	/* uP clk */
-	VPD_ENTRY(mdc, 6);	/* MDIO clk */
-	VPD_ENTRY(mt, 2);	/* mem timing */
-	VPD_ENTRY(xaui0cfg, 6);	/* XAUI0 config */
-	VPD_ENTRY(xaui1cfg, 6);	/* XAUI1 config */
-	VPD_ENTRY(port0, 2);	/* PHY0 complex */
-	VPD_ENTRY(port1, 2);	/* PHY1 complex */
-	VPD_ENTRY(port2, 2);	/* PHY2 complex */
-	VPD_ENTRY(port3, 2);	/* PHY3 complex */
-	VPD_ENTRY(rv, 1);	/* csum */
-	u32 pad;		/* for multiple-of-4 sizing and alignment */
+	u8  id_tag;
+	u8  id_len[2];
+	u8  id_data[16];
+	u8  vpdr_tag;
+	u8  vpdr_len[2];
+	VPD_ENTRY(pn, 16);                     /* part number */
+	VPD_ENTRY(ec, ECNUM_LEN);              /* EC level */
+	VPD_ENTRY(sn, SERNUM_LEN);             /* serial number */
+	VPD_ENTRY(na, 12);                     /* MAC address base */
+	VPD_ENTRY(cclk, 6);                    /* core clock */
+	VPD_ENTRY(mclk, 6);                    /* mem clock */
+	VPD_ENTRY(uclk, 6);                    /* uP clk */
+	VPD_ENTRY(mdc, 6);                     /* MDIO clk */
+	VPD_ENTRY(mt, 2);                      /* mem timing */
+	VPD_ENTRY(xaui0cfg, 6);                /* XAUI0 config */
+	VPD_ENTRY(xaui1cfg, 6);                /* XAUI1 config */
+	VPD_ENTRY(port0, 2);                   /* PHY0 complex */
+	VPD_ENTRY(port1, 2);                   /* PHY1 complex */
+	VPD_ENTRY(port2, 2);                   /* PHY2 complex */
+	VPD_ENTRY(port3, 2);                   /* PHY3 complex */
+	VPD_ENTRY(rv, 1);                      /* csum */
+	u32 pad;                  /* for multiple-of-4 sizing and alignment */
 };
 
 #define EEPROM_MAX_POLL   40
@@ -608,28 +576,27 @@
  *	addres is written to the control register.  The hardware device will
  *	set the flag to 1 when 4 bytes have been read into the data register.
  */
-int t3_seeprom_read(struct adapter *adapter, u32 addr, __le32 *data)
+int t3_seeprom_read(adapter_t *adapter, u32 addr, u32 *data)
 {
 	u16 val;
 	int attempts = EEPROM_MAX_POLL;
-	u32 v;
 	unsigned int base = adapter->params.pci.vpd_cap_addr;
 
 	if ((addr >= EEPROMSIZE && addr != EEPROM_STAT_ADDR) || (addr & 3))
 		return -EINVAL;
 
-	pci_write_config_word(adapter->pdev, base + PCI_VPD_ADDR, addr);
+	t3_os_pci_write_config_2(adapter, base + PCI_VPD_ADDR, (u16)addr);
 	do {
 		udelay(10);
-		pci_read_config_word(adapter->pdev, base + PCI_VPD_ADDR, &val);
+		t3_os_pci_read_config_2(adapter, base + PCI_VPD_ADDR, &val);
 	} while (!(val & PCI_VPD_ADDR_F) && --attempts);
 
 	if (!(val & PCI_VPD_ADDR_F)) {
 		CH_ERR(adapter, "reading EEPROM address 0x%x failed\n", addr);
 		return -EIO;
 	}
-	pci_read_config_dword(adapter->pdev, base + PCI_VPD_DATA, &v);
-	*data = cpu_to_le32(v);
+	t3_os_pci_read_config_4(adapter, base + PCI_VPD_DATA, data);
+	*data = le32_to_cpu(*data);
 	return 0;
 }
 
@@ -642,7 +609,7 @@
  *	Write a 32-bit word to a location in VPD EEPROM using the card's PCI
  *	VPD ROM capability.
  */
-int t3_seeprom_write(struct adapter *adapter, u32 addr, __le32 data)
+int t3_seeprom_write(adapter_t *adapter, u32 addr, u32 data)
 {
 	u16 val;
 	int attempts = EEPROM_MAX_POLL;
@@ -651,13 +618,13 @@
 	if ((addr >= EEPROMSIZE && addr != EEPROM_STAT_ADDR) || (addr & 3))
 		return -EINVAL;
 
-	pci_write_config_dword(adapter->pdev, base + PCI_VPD_DATA,
-			       le32_to_cpu(data));
-	pci_write_config_word(adapter->pdev,base + PCI_VPD_ADDR,
-			      addr | PCI_VPD_ADDR_F);
+	t3_os_pci_write_config_4(adapter, base + PCI_VPD_DATA,
+				 cpu_to_le32(data));
+	t3_os_pci_write_config_2(adapter, base + PCI_VPD_ADDR,
+				 (u16)addr | PCI_VPD_ADDR_F);
 	do {
 		msleep(1);
-		pci_read_config_word(adapter->pdev, base + PCI_VPD_ADDR, &val);
+		t3_os_pci_read_config_2(adapter, base + PCI_VPD_ADDR, &val);
 	} while ((val & PCI_VPD_ADDR_F) && --attempts);
 
 	if (val & PCI_VPD_ADDR_F) {
@@ -674,7 +641,7 @@
  *
  *	Enables or disables write protection on the serial EEPROM.
  */
-int t3_seeprom_wp(struct adapter *adapter, int enable)
+int t3_seeprom_wp(adapter_t *adapter, int enable)
 {
 	return t3_seeprom_write(adapter, EEPROM_STAT_ADDR, enable ? 0xc : 0);
 }
@@ -688,13 +655,132 @@
 }
 
 /**
+ * 	get_desc_len - get the length of a vpd descriptor.
+ *	@adapter: the adapter
+ *	@offset: first byte offset of the vpd descriptor
+ *
+ *	Retrieves the length of the small/large resource
+ *	data type starting at offset.
+ */
+static int get_desc_len(adapter_t *adapter, u32 offset)
+{
+	u32 read_offset, tmp, shift, len = 0;
+	u8 tag, buf[8];
+	int ret;
+
+	read_offset = offset & 0xfffffffc;
+	shift = offset & 0x03;
+
+	ret = t3_seeprom_read(adapter, read_offset, &tmp);
+	if (ret < 0)
+		return ret;
+
+	*((u32 *)buf) = cpu_to_le32(tmp);
+
+	tag = buf[shift];
+	if (tag & 0x80) {
+		ret = t3_seeprom_read(adapter, read_offset + 4, &tmp);
+		if (ret < 0)
+			return ret;
+
+		*((u32 *)(&buf[4])) = cpu_to_le32(tmp);
+		len = (buf[shift + 1] & 0xff) +
+		      ((buf[shift+2] << 8) & 0xff00) + 3;
+	} else
+		len = (tag & 0x07) + 1;
+
+	return len;
+}
+
+/**
+ *	is_end_tag - Check if a vpd tag is the end tag.
+ *	@adapter: the adapter
+ *	@offset: first byte offset of the tag
+ *
+ *	Checks if the tag located at offset is the end tag.
+ */
+static int is_end_tag(adapter_t * adapter, u32 offset)
+{
+	u32 read_offset, shift, ret, tmp;
+	u8 buf[4];
+
+	read_offset = offset & 0xfffffffc;
+	shift = offset & 0x03;
+
+	ret = t3_seeprom_read(adapter, read_offset, &tmp);
+	if (ret)
+		return ret;
+	*((u32 *)buf) = cpu_to_le32(tmp);
+
+	if (buf[shift] == 0x78)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ *	t3_get_vpd_len - computes the length of a vpd structure
+ *	@adapter: the adapter
+ *	@vpd: contains the offset of first byte of vpd
+ *
+ *	Computes the lentgh of the vpd structure starting at vpd->offset.
+ */
+
+int t3_get_vpd_len(adapter_t * adapter, struct generic_vpd *vpd)
+{
+	u32 len=0, offset;
+	int inc, ret;
+
+	offset = vpd->offset;
+
+	while (offset < (vpd->offset + MAX_VPD_BYTES)) {
+		ret = is_end_tag(adapter, offset);
+		if (ret < 0)
+			return ret;
+		else if (ret == 1)
+			break;
+
+		inc = get_desc_len(adapter, offset);
+		if (inc < 0)
+			return inc;
+		len += inc;
+		offset += inc;
+	}
+	return (len + 1);
+}
+
+/**
+ *	t3_read_vpd - reads the stream of bytes containing a vpd structure
+ *	@adapter: the adapter
+ *	@vpd: contains a buffer that would hold the stream of bytes
+ *
+ *	Reads the vpd structure starting at vpd->offset into vpd->data,
+ *	the length of the byte stream to read is vpd->len.
+ */
+
+int t3_read_vpd(adapter_t *adapter, struct generic_vpd *vpd)
+{
+	u32 i, ret;
+
+	for (i = 0; i < vpd->len; i += 4) {
+		ret = t3_seeprom_read(adapter, vpd->offset + i,
+				      (u32 *) &(vpd->data[i]));
+		if (ret)
+			return ret;
+	}
+
+	return 0;
+}
+
+
+/**
  *	get_vpd_params - read VPD parameters from VPD EEPROM
  *	@adapter: adapter to read
  *	@p: where to store the parameters
  *
  *	Reads card parameters stored in VPD EEPROM.
  */
-static int get_vpd_params(struct adapter *adapter, struct vpd_params *p)
+static int get_vpd_params(adapter_t *adapter, struct vpd_params *p)
 {
 	int i, addr, ret;
 	struct t3_vpd vpd;
@@ -703,14 +789,14 @@
 	 * Card information is normally at VPD_BASE but some early cards had
 	 * it at 0.
 	 */
-	ret = t3_seeprom_read(adapter, VPD_BASE, (__le32 *)&vpd);
+	ret = t3_seeprom_read(adapter, VPD_BASE, (u32 *)&vpd);
 	if (ret)
 		return ret;
 	addr = vpd.id_tag == 0x82 ? VPD_BASE : 0;
 
 	for (i = 0; i < sizeof(vpd); i += 4) {
 		ret = t3_seeprom_read(adapter, addr + i,
-				      (__le32 *)((u8 *)&vpd + i));
+				      (u32 *)((u8 *)&vpd + i));
 		if (ret)
 			return ret;
 	}
@@ -721,14 +807,17 @@
 	p->mdc = simple_strtoul(vpd.mdc_data, NULL, 10);
 	p->mem_timing = simple_strtoul(vpd.mt_data, NULL, 10);
 	memcpy(p->sn, vpd.sn_data, SERNUM_LEN);
+	memcpy(p->ec, vpd.ec_data, ECNUM_LEN);
 
 	/* Old eeproms didn't have port information */
 	if (adapter->params.rev == 0 && !vpd.port0_data[0]) {
 		p->port_type[0] = uses_xaui(adapter) ? 1 : 2;
 		p->port_type[1] = uses_xaui(adapter) ? 6 : 2;
 	} else {
-		p->port_type[0] = hex2int(vpd.port0_data[0]);
-		p->port_type[1] = hex2int(vpd.port1_data[0]);
+		p->port_type[0] = (u8)hex2int(vpd.port0_data[0]);
+		p->port_type[1] = (u8)hex2int(vpd.port1_data[0]);
+		p->port_type[2] = (u8)hex2int(vpd.port2_data[0]);
+		p->port_type[3] = (u8)hex2int(vpd.port3_data[0]);
 		p->xauicfg[0] = simple_strtoul(vpd.xaui0cfg_data, NULL, 16);
 		p->xauicfg[1] = simple_strtoul(vpd.xaui1cfg_data, NULL, 16);
 	}
@@ -736,26 +825,78 @@
 	for (i = 0; i < 6; i++)
 		p->eth_base[i] = hex2int(vpd.na_data[2 * i]) * 16 +
 				 hex2int(vpd.na_data[2 * i + 1]);
+
+	p->rlen = le16_to_cpu(*(u16 *)vpd.vpdr_len);
+	p->csum = vpd.rv_data[0];
+	
 	return 0;
 }
 
+int t3_check_vpd_checksum(adapter_t *adapter)
+{
+	int i, addr, ret, offset;
+	struct t3_vpd vpd;
+	u8 csum = 0, *p;
+
+	ret = t3_seeprom_read(adapter, VPD_BASE, (u32 *)&vpd);
+	if (ret)
+		return ret;
+	addr = vpd.id_tag == 0x82 ? VPD_BASE : 0;
+
+	for (i = 0; i < sizeof(vpd); i += 4) {
+		ret = t3_seeprom_read(adapter, addr + i,
+				      (u32 *)((u8 *)&vpd + i));
+		if (ret)
+			return ret;
+	}
+
+	p = (u8 *)&vpd;
+	offset = offsetof(struct t3_vpd, rv_data);
+	for (i = 0; i < offset; i++)
+		csum += *p++;
+	
+	csum = ~csum;
+	if (csum != vpd.rv_data[0])
+		CH_WARN(adapter, "Incorrect VPD checksum");
+
+	return csum != vpd.rv_data[0];
+}
+
+/* BIOS boot header */
+typedef struct boot_header_s {
+	u8	signature[2];	/* signature */
+	u8	length;		/* image length (include header) */
+	u8	offset[4];	/* initialization vector */
+	u8	reserved[19];	/* reserved */
+	u8	exheader[2];	/* offset to expansion header */
+} boot_header_t;
+
 /* serial flash and firmware constants */
 enum {
-	SF_ATTEMPTS = 5,	/* max retries for SF1 operations */
-	SF_SEC_SIZE = 64 * 1024,	/* serial flash sector size */
-	SF_SIZE = SF_SEC_SIZE * 8,	/* serial flash size */
+	SF_ATTEMPTS = 5,           /* max retries for SF1 operations */
+	SF_SEC_SIZE = 64 * 1024,   /* serial flash sector size */
+	SF_SIZE = SF_SEC_SIZE * 8, /* serial flash size */
 
 	/* flash command opcodes */
-	SF_PROG_PAGE = 2,	/* program page */
-	SF_WR_DISABLE = 4,	/* disable writes */
-	SF_RD_STATUS = 5,	/* read status register */
-	SF_WR_ENABLE = 6,	/* enable writes */
-	SF_RD_DATA_FAST = 0xb,	/* read flash */
-	SF_ERASE_SECTOR = 0xd8,	/* erase sector */
-
-	FW_FLASH_BOOT_ADDR = 0x70000,	/* start address of FW in flash */
+	SF_PROG_PAGE    = 2,       /* program page */
+	SF_WR_DISABLE   = 4,       /* disable writes */
+	SF_RD_STATUS    = 5,       /* read status register */
+	SF_WR_ENABLE    = 6,       /* enable writes */
+	SF_RD_DATA_FAST = 0xb,     /* read flash */
+	SF_ERASE_SECTOR = 0xd8,    /* erase sector */
+
+	FW_FLASH_BOOT_ADDR = 0x70000, /* start address of FW in flash */
 	FW_VERS_ADDR = 0x7fffc,    /* flash address holding FW version */
-	FW_MIN_SIZE = 8            /* at least version and csum */
+	FW_VERS_ADDR_PRE8 = 0x77ffc,/* flash address holding FW version pre8 */
+	FW_MIN_SIZE = 8,           /* at least version and csum */
+	FW_MAX_SIZE = FW_VERS_ADDR - FW_FLASH_BOOT_ADDR,
+	FW_MAX_SIZE_PRE8 = FW_VERS_ADDR_PRE8 - FW_FLASH_BOOT_ADDR,
+
+	BOOT_FLASH_BOOT_ADDR = 0x0,/* start address of boot image in flash */
+	BOOT_SIGNATURE = 0xaa55,   /* signature of BIOS boot ROM */
+	BOOT_SIZE_INC = 512,       /* image size measured in 512B chunks */
+	BOOT_MIN_SIZE = sizeof(boot_header_t), /* at least basic header */
+	BOOT_MAX_SIZE = 1024*BOOT_SIZE_INC /* 1 byte * length increment  */
 };
 
 /**
@@ -769,7 +910,7 @@
  *	the read needs to be specified prior to calling this by issuing the
  *	appropriate commands to the serial flash.
  */
-static int sf1_read(struct adapter *adapter, unsigned int byte_cnt, int cont,
+static int sf1_read(adapter_t *adapter, unsigned int byte_cnt, int cont,
 		    u32 *valp)
 {
 	int ret;
@@ -796,7 +937,7 @@
  *	the write needs to be specified prior to calling this by issuing the
  *	appropriate commands to the serial flash.
  */
-static int sf1_write(struct adapter *adapter, unsigned int byte_cnt, int cont,
+static int sf1_write(adapter_t *adapter, unsigned int byte_cnt, int cont,
 		     u32 val)
 {
 	if (!byte_cnt || byte_cnt > 4)
@@ -817,7 +958,7 @@
  *
  *	Wait for a flash operation to complete by polling the status register.
  */
-static int flash_wait_op(struct adapter *adapter, int attempts, int delay)
+static int flash_wait_op(adapter_t *adapter, int attempts, int delay)
 {
 	int ret;
 	u32 status;
@@ -848,8 +989,8 @@
  *	(i.e., big-endian), otherwise as 32-bit words in the platform's
  *	natural endianess.
  */
-int t3_read_flash(struct adapter *adapter, unsigned int addr,
-		  unsigned int nwords, u32 *data, int byte_oriented)
+int t3_read_flash(adapter_t *adapter, unsigned int addr, unsigned int nwords,
+		  u32 *data, int byte_oriented)
 {
 	int ret;
 
@@ -862,7 +1003,7 @@
 	    (ret = sf1_read(adapter, 1, 1, data)) != 0)
 		return ret;
 
-	for (; nwords; nwords--, data++) {
+	for ( ; nwords; nwords--, data++) {
 		ret = sf1_read(adapter, 4, nwords > 1, data);
 		if (ret)
 			return ret;
@@ -878,16 +1019,21 @@
  *	@addr: the start address to write
  *	@n: length of data to write
  *	@data: the data to write
+ *	@byte_oriented: whether to store data as bytes or as words
  *
  *	Writes up to a page of data (256 bytes) to the serial flash starting
  *	at the given address.
+ *	If @byte_oriented is set the write data is stored as a 32-bit
+ *	big-endian array, otherwise in the processor's native endianess.
+ *
  */
-static int t3_write_flash(struct adapter *adapter, unsigned int addr,
-			  unsigned int n, const u8 *data)
+static int t3_write_flash(adapter_t *adapter, unsigned int addr,
+			  unsigned int n, const u8 *data,
+			  int byte_oriented)
 {
 	int ret;
 	u32 buf[64];
-	unsigned int i, c, left, val, offset = addr & 0xff;
+	unsigned int c, left, val, offset = addr & 0xff;
 
 	if (addr + n > SF_SIZE || offset + n > 256)
 		return -EINVAL;
@@ -900,8 +1046,10 @@
 
 	for (left = n; left; left -= c) {
 		c = min(left, 4U);
-		for (val = 0, i = 0; i < c; ++i)
-			val = (val << 8) + *data++;
+		val = *(u32*)data;
+		data += c;
+		if (byte_oriented)
+			val = htonl(val);
 
 		ret = sf1_write(adapter, c, c != left, val);
 		if (ret)
@@ -911,11 +1059,12 @@
 		return ret;
 
 	/* Read the page to verify the write succeeded */
-	ret = t3_read_flash(adapter, addr & ~0xff, ARRAY_SIZE(buf), buf, 1);
+	ret = t3_read_flash(adapter, addr & ~0xff, ARRAY_SIZE(buf), buf,
+			    byte_oriented);
 	if (ret)
 		return ret;
 
-	if (memcmp(data - n, (u8 *) buf + offset, n))
+	if (memcmp(data - n, (u8 *)buf + offset, n))
 		return -EIO;
 	return 0;
 }
@@ -927,7 +1076,7 @@
  *
  *	Reads the protocol sram version from sram.
  */
-int t3_get_tp_version(struct adapter *adapter, u32 *vers)
+int t3_get_tp_version(adapter_t *adapter, u32 *vers)
 {
 	int ret;
 
@@ -947,31 +1096,39 @@
  *	t3_check_tpsram_version - read the tp sram version
  *	@adapter: the adapter
  *
- *	Reads the protocol sram version from flash.
  */
-int t3_check_tpsram_version(struct adapter *adapter)
+int t3_check_tpsram_version(adapter_t *adapter)
 {
 	int ret;
 	u32 vers;
-	unsigned int major, minor;
+	unsigned int major, minor, maj, min;
 
 	if (adapter->params.rev == T3_REV_A)
 		return 0;
+	else if (adapter->params.rev == T3_REV_B) {
+		maj = TP_VERSION_MAJOR_T3B;
+		min = TP_VERSION_MINOR_T3B;
+	} else {
+		maj = TP_VERSION_MAJOR;
+		min = TP_VERSION_MINOR;
+	}
 
 
 	ret = t3_get_tp_version(adapter, &vers);
 	if (ret)
 		return ret;
 
+	vers = t3_read_reg(adapter, A_TP_EMBED_OP_FIELD1);
+
 	major = G_TP_VERSION_MAJOR(vers);
 	minor = G_TP_VERSION_MINOR(vers);
 
-	if (major == TP_VERSION_MAJOR && minor == TP_VERSION_MINOR)
+	if (major == maj && minor == min)
 		return 0;
 	else {
 		CH_ERR(adapter, "found wrong TP version (%u.%u), "
 		       "driver compiled for version %d.%d\n", major, minor,
-		       TP_VERSION_MAJOR, TP_VERSION_MINOR);
+		       maj, min);
 	}
 	return -EINVAL;
 }
@@ -986,12 +1143,11 @@
  *	Checks if an adapter's tp sram is compatible with the driver.
  *	Returns 0 if the versions are compatible, a negative error otherwise.
  */
-int t3_check_tpsram(struct adapter *adapter, const u8 *tp_sram,
-		    unsigned int size)
+int t3_check_tpsram(adapter_t *adapter, const u8 *tp_sram, unsigned int size)
 {
 	u32 csum;
 	unsigned int i;
-	const __be32 *p = (const __be32 *)tp_sram;
+	const u32 *p = (const u32 *)tp_sram;
 
 	/* Verify checksum */
 	for (csum = 0, i = 0; i < size / sizeof(csum); i++)
@@ -1015,11 +1171,17 @@
  *	@adapter: the adapter
  *	@vers: where to place the version
  *
- *	Reads the FW version from flash.
+ *	Reads the FW version from flash. Note that we had to move the version
+ *	due to FW size. If we don't find a valid FW version in the new location
+ *	we fall back and read the old location.
  */
-int t3_get_fw_version(struct adapter *adapter, u32 *vers)
+int t3_get_fw_version(adapter_t *adapter, u32 *vers)
 {
-	return t3_read_flash(adapter, FW_VERS_ADDR, 1, vers, 0);
+	int ret = t3_read_flash(adapter, FW_VERS_ADDR, 1, vers, 0);
+	if (!ret && *vers != 0xffffffff)
+		return 0;
+	else
+		return t3_read_flash(adapter, FW_VERS_ADDR_PRE8, 1, vers, 0);
 }
 
 /**
@@ -1029,7 +1191,7 @@
  *	Checks if an adapter's FW is compatible with the driver.  Returns 0
  *	if the versions are compatible, a negative error otherwise.
  */
-int t3_check_fw_version(struct adapter *adapter)
+int t3_check_fw_version(adapter_t *adapter)
 {
 	int ret;
 	u32 vers;
@@ -1046,16 +1208,15 @@
 	if (type == FW_VERSION_T3 && major == FW_VERSION_MAJOR &&
 	    minor == FW_VERSION_MINOR)
 		return 0;
+
 	else if (major != FW_VERSION_MAJOR || minor < FW_VERSION_MINOR)
 		CH_WARN(adapter, "found old FW minor version(%u.%u), "
 		        "driver compiled for version %u.%u\n", major, minor,
 			FW_VERSION_MAJOR, FW_VERSION_MINOR);
-	else {
+	else
 		CH_WARN(adapter, "found newer FW version(%u.%u), "
 		        "driver compiled for version %u.%u\n", major, minor,
 			FW_VERSION_MAJOR, FW_VERSION_MINOR);
-			return 0;
-	}
 	return -EINVAL;
 }
 
@@ -1067,7 +1228,7 @@
  *
  *	Erases the sectors in the given range.
  */
-static int t3_flash_erase_sectors(struct adapter *adapter, int start, int end)
+static int t3_flash_erase_sectors(adapter_t *adapter, int start, int end)
 {
 	while (start <= end) {
 		int ret;
@@ -1093,18 +1254,28 @@
  *	data, followed by 4 bytes of FW version, followed by the 32-bit
  *	1's complement checksum of the whole image.
  */
-int t3_load_fw(struct adapter *adapter, const u8 *fw_data, unsigned int size)
+int t3_load_fw(adapter_t *adapter, const u8 *fw_data, unsigned int size)
 {
-	u32 csum;
+	u32 version, csum, fw_version_addr;
 	unsigned int i;
-	const __be32 *p = (const __be32 *)fw_data;
+	const u32 *p = (const u32 *)fw_data;
 	int ret, addr, fw_sector = FW_FLASH_BOOT_ADDR >> 16;
 
 	if ((size & 3) || size < FW_MIN_SIZE)
 		return -EINVAL;
-	if (size > FW_VERS_ADDR + 8 - FW_FLASH_BOOT_ADDR)
+	if (size - 8 > FW_MAX_SIZE)
 		return -EFBIG;
 
+	version = ntohl(*(u32 *)(fw_data + size - 8));
+	if (G_FW_VERSION_MAJOR(version) < 8) {
+
+		fw_version_addr = FW_VERS_ADDR_PRE8;
+
+		if (size - 8 > FW_MAX_SIZE_PRE8)
+			return -EFBIG;
+	} else
+		fw_version_addr = FW_VERS_ADDR;
+
 	for (csum = 0, i = 0; i < size / sizeof(csum); i++)
 		csum += ntohl(p[i]);
 	if (csum != 0xffffffff) {
@@ -1117,11 +1288,11 @@
 	if (ret)
 		goto out;
 
-	size -= 8;		/* trim off version and checksum */
-	for (addr = FW_FLASH_BOOT_ADDR; size;) {
+	size -= 8;  /* trim off version and checksum */
+	for (addr = FW_FLASH_BOOT_ADDR; size; ) {
 		unsigned int chunk_size = min(size, 256U);
 
-		ret = t3_write_flash(adapter, addr, chunk_size, fw_data);
+		ret = t3_write_flash(adapter, addr, chunk_size, fw_data, 1);
 		if (ret)
 			goto out;
 
@@ -1130,27 +1301,78 @@
 		size -= chunk_size;
 	}
 
-	ret = t3_write_flash(adapter, FW_VERS_ADDR, 4, fw_data);
+	ret = t3_write_flash(adapter, fw_version_addr, 4, fw_data, 1);
 out:
 	if (ret)
 		CH_ERR(adapter, "firmware download failed, error %d\n", ret);
 	return ret;
 }
 
-#define CIM_CTL_BASE 0x2000
+/*
+ *	t3_load_boot - download boot flash
+ *	@adapter: the adapter
+ *	@boot_data: the boot image to write
+ *	@size: image size
+ *
+ *	Write the supplied boot image to the card's serial flash.
+ *	The boot image has the following sections: a 28-byte header and the
+ *	boot image.
+ */
+int t3_load_boot(adapter_t *adapter, const u8 *boot_data, unsigned int size)
+{
+	boot_header_t *header = (boot_header_t *)boot_data;
+	int ret;
+	unsigned int addr;
+	unsigned int boot_sector = BOOT_FLASH_BOOT_ADDR >> 16;
+	unsigned int boot_end = (BOOT_FLASH_BOOT_ADDR + size - 1) >> 16;
+
+	/*
+	 * Perform some primitive sanity testing to avoid accidentally
+	 * writing garbage over the boot sectors.  We ought to check for
+	 * more but it's not worth it for now ...
+	 */
+	if (size < BOOT_MIN_SIZE || size > BOOT_MAX_SIZE) {
+		CH_ERR(adapter, "boot image too small/large\n");
+		return -EFBIG;
+	}
+	if (le16_to_cpu(*(u16*)header->signature) != BOOT_SIGNATURE) {
+		CH_ERR(adapter, "boot image missing signature\n");
+		return -EINVAL;
+	}
+
+	ret = t3_flash_erase_sectors(adapter, boot_sector, boot_end);
+	if (ret)
+		goto out;
+
+	for (addr = BOOT_FLASH_BOOT_ADDR; size; ) {
+		unsigned int chunk_size = min(size, 256U);
+
+		ret = t3_write_flash(adapter, addr, chunk_size, boot_data, 0);
+		if (ret)
+			goto out;
+
+		addr += chunk_size;
+		boot_data += chunk_size;
+		size -= chunk_size;
+	}
+
+out:
+	if (ret)
+		CH_ERR(adapter, "boot image download failed, error %d\n", ret);
+	return ret;
+}
 
 /**
- *      t3_cim_ctl_blk_read - read a block from CIM control region
+ *	t3_cim_ctl_blk_read - read a block from CIM control region
+ *	@adap: the adapter
+ *	@addr: the start address within the CIM control region
+ *	@n: number of words to read
+ *	@valp: where to store the result
  *
- *      @adap: the adapter
- *      @addr: the start address within the CIM control region
- *      @n: number of words to read
- *      @valp: where to store the result
- *
- *      Reads a block of 4-byte words from the CIM control region.
+ *	Reads a block of 4-byte words from the CIM control region.
  */
-int t3_cim_ctl_blk_read(struct adapter *adap, unsigned int addr,
-			unsigned int n, unsigned int *valp)
+int t3_cim_ctl_blk_read(adapter_t *adap, unsigned int addr, unsigned int n,
+			unsigned int *valp)
 {
 	int ret = 0;
 
@@ -1158,7 +1380,7 @@
 		return -EBUSY;
 
 	for ( ; !ret && n--; addr += 4) {
-		t3_write_reg(adap, A_CIM_HOST_ACC_CTRL, CIM_CTL_BASE + addr);
+		t3_write_reg(adap, A_CIM_HOST_ACC_CTRL, A_CIM_CTL_BASE + addr);
 		ret = t3_wait_op_done(adap, A_CIM_HOST_ACC_CTRL, F_HOSTBUSY,
 				      0, 5, 2);
 		if (!ret)
@@ -1167,37 +1389,79 @@
 	return ret;
 }
 
-static void t3_gate_rx_traffic(struct cmac *mac, u32 *rx_cfg,
+void t3_gate_rx_traffic(struct cmac *mac, u32 *rx_cfg,
 			       u32 *rx_hash_high, u32 *rx_hash_low)
 {
 	/* stop Rx unicast traffic */
 	t3_mac_disable_exact_filters(mac);
 
 	/* stop broadcast, multicast, promiscuous mode traffic */
-	*rx_cfg = t3_read_reg(mac->adapter, A_XGM_RX_CFG);
-	t3_set_reg_field(mac->adapter, A_XGM_RX_CFG,
+	*rx_cfg = t3_read_reg(mac->adapter, A_XGM_RX_CFG + mac->offset);
+	t3_set_reg_field(mac->adapter, A_XGM_RX_CFG + mac->offset, 
 			 F_ENHASHMCAST | F_DISBCAST | F_COPYALLFRAMES,
 			 F_DISBCAST);
 
-	*rx_hash_high = t3_read_reg(mac->adapter, A_XGM_RX_HASH_HIGH);
-	t3_write_reg(mac->adapter, A_XGM_RX_HASH_HIGH, 0);
-
-	*rx_hash_low = t3_read_reg(mac->adapter, A_XGM_RX_HASH_LOW);
-	t3_write_reg(mac->adapter, A_XGM_RX_HASH_LOW, 0);
+	*rx_hash_high = t3_read_reg(mac->adapter, A_XGM_RX_HASH_HIGH +
+	    mac->offset);
+	t3_write_reg(mac->adapter, A_XGM_RX_HASH_HIGH + mac->offset, 0);
+
+	*rx_hash_low = t3_read_reg(mac->adapter, A_XGM_RX_HASH_LOW +
+	    mac->offset);
+	t3_write_reg(mac->adapter, A_XGM_RX_HASH_LOW + mac->offset, 0);
 
 	/* Leave time to drain max RX fifo */
 	msleep(1);
 }
 
-static void t3_open_rx_traffic(struct cmac *mac, u32 rx_cfg,
+void t3_open_rx_traffic(struct cmac *mac, u32 rx_cfg,
 			       u32 rx_hash_high, u32 rx_hash_low)
 {
 	t3_mac_enable_exact_filters(mac);
-	t3_set_reg_field(mac->adapter, A_XGM_RX_CFG,
+	t3_set_reg_field(mac->adapter, A_XGM_RX_CFG + mac->offset, 
 			 F_ENHASHMCAST | F_DISBCAST | F_COPYALLFRAMES,
 			 rx_cfg);
-	t3_write_reg(mac->adapter, A_XGM_RX_HASH_HIGH, rx_hash_high);
-	t3_write_reg(mac->adapter, A_XGM_RX_HASH_LOW, rx_hash_low);
+	t3_write_reg(mac->adapter, A_XGM_RX_HASH_HIGH + mac->offset,
+	    rx_hash_high);
+	t3_write_reg(mac->adapter, A_XGM_RX_HASH_LOW + mac->offset,
+	    rx_hash_low);
+}
+
+static int t3_detect_link_fault(adapter_t *adapter, int port_id)
+{
+	struct port_info *pi = adap2pinfo(adapter, port_id);
+	struct cmac *mac = &pi->mac;
+	uint32_t rx_cfg, rx_hash_high, rx_hash_low;
+	int link_fault;
+
+	/* stop rx */
+	t3_gate_rx_traffic(mac, &rx_cfg, &rx_hash_high, &rx_hash_low);
+	t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset, 0);
+
+	/* clear status and make sure intr is enabled */
+	(void) t3_read_reg(adapter, A_XGM_INT_STATUS + mac->offset);
+	t3_xgm_intr_enable(adapter, port_id);
+
+	/* restart rx */
+	t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset, F_RXEN);
+	t3_open_rx_traffic(mac, rx_cfg, rx_hash_high, rx_hash_low);
+
+	link_fault = t3_read_reg(adapter, A_XGM_INT_STATUS + mac->offset);
+	return (link_fault & F_LINKFAULTCHANGE ? 1 : 0);
+}
+
+static void t3_clear_faults(adapter_t *adapter, int port_id)
+{
+	struct port_info *pi = adap2pinfo(adapter, port_id);
+	struct cmac *mac = &pi->mac;
+
+	if (adapter->params.nports <= 2) {
+		t3_xgm_intr_disable(adapter, pi->port_id);
+		t3_read_reg(adapter, A_XGM_INT_STATUS + mac->offset);
+		t3_write_reg(adapter, A_XGM_INT_CAUSE + mac->offset, F_XGM_INT);
+		t3_set_reg_field(adapter, A_XGM_INT_ENABLE + mac->offset,
+				 F_XGM_INT, F_XGM_INT);
+		t3_xgm_intr_enable(adapter, pi->port_id);
+	}
 }
 
 /**
@@ -1209,114 +1473,164 @@
  *	to the associated PHY and MAC.  After performing the common tasks it
  *	invokes an OS-specific handler.
  */
-void t3_link_changed(struct adapter *adapter, int port_id)
+void t3_link_changed(adapter_t *adapter, int port_id)
 {
-	int link_ok, speed, duplex, fc;
+	int link_ok, speed, duplex, fc, link_fault, us_delay;
 	struct port_info *pi = adap2pinfo(adapter, port_id);
 	struct cphy *phy = &pi->phy;
 	struct cmac *mac = &pi->mac;
 	struct link_config *lc = &pi->link_config;
 
+	link_ok = lc->link_ok;
+	speed = lc->speed;
+	duplex = lc->duplex;
+	fc = lc->fc;
+	link_fault = 0;
+
 	phy->ops->get_link_status(phy, &link_ok, &speed, &duplex, &fc);
 
-	if (!lc->link_ok && link_ok) {
-		u32 rx_cfg, rx_hash_high, rx_hash_low;
-		u32 status;
-
-		t3_xgm_intr_enable(adapter, port_id);
-		t3_gate_rx_traffic(mac, &rx_cfg, &rx_hash_high, &rx_hash_low);
-		t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset, 0);
-		t3_mac_enable(mac, MAC_DIRECTION_RX);
-
-		status = t3_read_reg(adapter, A_XGM_INT_STATUS + mac->offset);
-		if (status & F_LINKFAULTCHANGE) {
-			mac->stats.link_faults++;
-			pi->link_fault = 1;
-		}
-		t3_open_rx_traffic(mac, rx_cfg, rx_hash_high, rx_hash_low);
-	}
+	/* Need to clear link_fault on 4port cards */
+	if (adapter->params.nports > 2)
+		pi->link_fault = LF_NO;
 
 	if (lc->requested_fc & PAUSE_AUTONEG)
 		fc &= lc->requested_fc;
 	else
 		fc = lc->requested_fc & (PAUSE_RX | PAUSE_TX);
 
+	/* Update mac speed before checking for link fault. */
+	if (link_ok && speed >= 0 && lc->autoneg == AUTONEG_ENABLE &&
+	    (speed != lc->speed || duplex != lc->duplex || fc != lc->fc))
+		t3_mac_set_speed_duplex_fc(mac, speed, duplex, fc);
+
+	/*
+	 * Check for link faults if any of these is true:
+	 * a) A link fault is suspected, and PHY says link ok
+	 * b) PHY link transitioned from down -> up
+	 */
+	if (adapter->params.nports <= 2 &&
+	    ((pi->link_fault && link_ok) || (!lc->link_ok && link_ok))) {
+
+		link_fault = t3_detect_link_fault(adapter, port_id);
+		if (link_fault) {
+			if (pi->link_fault != LF_YES) {
+				mac->stats.link_faults++;
+				pi->link_fault = LF_YES;
+			}
+
+			if (uses_xaui(adapter)) {
+				if (adapter->params.rev >= T3_REV_C)
+					t3c_pcs_force_los(mac);
+				else
+					t3b_pcs_reset(mac);
+			}
+
+			/* Don't report link up */
+			link_ok = 0;
+		} else {
+			/* clear faults here if this was a false alarm. */
+			if (pi->link_fault == LF_MAYBE &&
+			    link_ok && lc->link_ok)
+				t3_clear_faults(adapter, port_id);
+
+			pi->link_fault = LF_NO;
+		}
+	}
+
 	if (link_ok == lc->link_ok && speed == lc->speed &&
 	    duplex == lc->duplex && fc == lc->fc)
 		return;                            /* nothing changed */
 
-	if (link_ok != lc->link_ok && adapter->params.rev > 0 &&
-	    uses_xaui(adapter)) {
-		if (link_ok)
-			t3b_pcs_reset(mac);
-		t3_write_reg(adapter, A_XGM_XAUI_ACT_CTRL + mac->offset,
-			     link_ok ? F_TXACTENABLE | F_RXEN : 0);
-	}
-	lc->link_ok = link_ok;
+	lc->link_ok = (unsigned char)link_ok;
 	lc->speed = speed < 0 ? SPEED_INVALID : speed;
 	lc->duplex = duplex < 0 ? DUPLEX_INVALID : duplex;
-
-	if (link_ok && speed >= 0 && lc->autoneg == AUTONEG_ENABLE) {
-		/* Set MAC speed, duplex, and flow control to match PHY. */
-		t3_mac_set_speed_duplex_fc(mac, speed, duplex, fc);
-		lc->fc = fc;
-	}
-
-	t3_os_link_changed(adapter, port_id, link_ok && !pi->link_fault,
-			   speed, duplex, fc);
-}
-
-void t3_link_fault(struct adapter *adapter, int port_id)
-{
-	struct port_info *pi = adap2pinfo(adapter, port_id);
-	struct cmac *mac = &pi->mac;
-	struct cphy *phy = &pi->phy;
-	struct link_config *lc = &pi->link_config;
-	int link_ok, speed, duplex, fc, link_fault;
-	u32 rx_cfg, rx_hash_high, rx_hash_low;
-
-	t3_gate_rx_traffic(mac, &rx_cfg, &rx_hash_high, &rx_hash_low);
-
-	if (adapter->params.rev > 0 && uses_xaui(adapter))
-		t3_write_reg(adapter, A_XGM_XAUI_ACT_CTRL + mac->offset, 0);
-
-	t3_write_reg(adapter, A_XGM_RX_CTRL + mac->offset, 0);
-	t3_mac_enable(mac, MAC_DIRECTION_RX);
-
-	t3_open_rx_traffic(mac, rx_cfg, rx_hash_high, rx_hash_low);
-
-	link_fault = t3_read_reg(adapter,
-				 A_XGM_INT_STATUS + mac->offset);
-	link_fault &= F_LINKFAULTCHANGE;
-
-	link_ok = lc->link_ok;
-	speed = lc->speed;
-	duplex = lc->duplex;
-	fc = lc->fc;
-
-	phy->ops->get_link_status(phy, &link_ok, &speed, &duplex, &fc);
-
-	if (link_fault) {
-		lc->link_ok = 0;
-		lc->speed = SPEED_INVALID;
-		lc->duplex = DUPLEX_INVALID;
-
-		t3_os_link_fault(adapter, port_id, 0);
-
-		/* Account link faults only when the phy reports a link up */
-		if (link_ok)
-			mac->stats.link_faults++;
-	} else {
-		if (link_ok)
+	lc->fc = fc;
+
+	if (link_ok) {
+
+		/* down -> up, or up -> up with changed settings */
+
+		if (adapter->params.rev > 0 && uses_xaui(adapter)) {
+
+			if (adapter->params.rev >= T3_REV_C)
+				t3c_pcs_force_los(mac);
+			else
+				t3b_pcs_reset(mac);
+
 			t3_write_reg(adapter, A_XGM_XAUI_ACT_CTRL + mac->offset,
 				     F_TXACTENABLE | F_RXEN);
-
-		pi->link_fault = 0;
-		lc->link_ok = (unsigned char)link_ok;
-		lc->speed = speed < 0 ? SPEED_INVALID : speed;
-		lc->duplex = duplex < 0 ? DUPLEX_INVALID : duplex;
-		t3_os_link_fault(adapter, port_id, link_ok);
+		}
+
+		/* disable TX FIFO drain */
+		t3_set_reg_field(adapter, A_XGM_TXFIFO_CFG + mac->offset,
+				 F_ENDROPPKT, 0);
+
+		t3_mac_enable(mac, MAC_DIRECTION_TX | MAC_DIRECTION_RX);
+		t3_set_reg_field(adapter, A_XGM_STAT_CTRL + mac->offset,
+				 F_CLRSTATS, 1);
+		t3_clear_faults(adapter, port_id);
+
+	} else {
+
+		/* up -> down */
+
+		if (adapter->params.rev > 0 && uses_xaui(adapter)) {
+			t3_write_reg(adapter,
+				     A_XGM_XAUI_ACT_CTRL + mac->offset, 0);
+		}
+
+		t3_xgm_intr_disable(adapter, pi->port_id);
+		if (adapter->params.nports <= 2) {
+			t3_set_reg_field(adapter,
+					 A_XGM_INT_ENABLE + mac->offset,
+					 F_XGM_INT, 0);
+		
+
+			/* Do not disable the XGMAC's rx if in
+			 * loopback mode */
+			if (!pi->loopback)
+				t3_mac_disable(mac, MAC_DIRECTION_RX);
+
+			/*
+			 * Make sure Tx FIFO continues to drain, even as
+			 * rxen is left high to help detect and indicate
+			 * remote faults.
+			 */
+
+			t3_set_reg_field(adapter,
+					 A_XGM_TXFIFO_CFG + mac->offset, 0,
+					 F_ENDROPPKT);
+			t3_write_reg(adapter,
+					 A_XGM_RX_CTRL + mac->offset, 0);
+			t3_write_reg(adapter,
+					 A_XGM_TX_CTRL + mac->offset, F_TXEN);
+			/*
+			 * Delay has to be added before F_RXEN to give sufficient
+			 * time to flush the packet in pipe so that the xgmac drop 
+			 * mechanism kicks in. Delay of 0xffff*512 bit time is 
+			 * required to allow worst case rxpause to be cleared.
+			 */
+			switch (speed) {
+				case SPEED_10000:
+					us_delay = 3500; /* In micro seconds */
+					break;
+				case SPEED_1000:
+					us_delay = 35000; /* In micro seconds */
+					break;
+				case SPEED_100:
+				default:
+					us_delay = 350000; /* In micro seconds */
+					break;
+			}
+			udelay(us_delay);
+			t3_write_reg(adapter,
+					 A_XGM_RX_CTRL + mac->offset, F_RXEN);
+		}
 	}
+
+	t3_os_link_changed(adapter, port_id, link_ok, speed, duplex, fc,
+	    mac->was_reset);
+	mac->was_reset = 0;
 }
 
 /**
@@ -1344,6 +1658,7 @@
 			if (fc & PAUSE_RX)
 				lc->advertising |= ADVERTISED_Pause;
 		}
+
 		phy->ops->advertise(phy, lc->advertising);
 
 		if (lc->autoneg == AUTONEG_DISABLE) {
@@ -1354,6 +1669,9 @@
 						   fc);
 			/* Also disables autoneg */
 			phy->ops->set_speed_duplex(phy, lc->speed, lc->duplex);
+			/* PR 5666. Power phy up when doing an ifup */
+			if (!is_10G(phy->adapter))
+				phy->ops->power_down(phy, 0);
 		} else
 			phy->ops->autoneg_enable(phy);
 	} else {
@@ -1372,7 +1690,7 @@
  *
  *	Enables or disables HW extraction of VLAN tags for the given port.
  */
-void t3_set_vlan_accel(struct adapter *adapter, unsigned int ports, int on)
+void t3_set_vlan_accel(adapter_t *adapter, unsigned int ports, int on)
 {
 	t3_set_reg_field(adapter, A_TP_OUT_CONFIG,
 			 ports << S_VLANEXTRACTIONENABLE,
@@ -1380,10 +1698,10 @@
 }
 
 struct intr_info {
-	unsigned int mask;	/* bits to check in interrupt status */
-	const char *msg;	/* message to print or NULL */
-	short stat_idx;		/* stat counter to increment or -1 */
-	unsigned short fatal;	/* whether the condition reported is fatal */
+	unsigned int mask;       /* bits to check in interrupt status */
+	const char *msg;         /* message to print or NULL */
+	short stat_idx;          /* stat counter to increment or -1 */
+	unsigned short fatal;    /* whether the condition reported is fatal */
 };
 
 /**
@@ -1401,7 +1719,7 @@
  *	incrementing a stat counter.  The table is terminated by an entry
  *	specifying mask 0.  Returns the number of fatal interrupt conditions.
  */
-static int t3_handle_intr_status(struct adapter *adapter, unsigned int reg,
+static int t3_handle_intr_status(adapter_t *adapter, unsigned int reg,
 				 unsigned int mask,
 				 const struct intr_info *acts,
 				 unsigned long *stats)
@@ -1409,20 +1727,20 @@
 	int fatal = 0;
 	unsigned int status = t3_read_reg(adapter, reg) & mask;
 
-	for (; acts->mask; ++acts) {
-		if (!(status & acts->mask))
-			continue;
+	for ( ; acts->mask; ++acts) {
+		if (!(status & acts->mask)) continue;
 		if (acts->fatal) {
 			fatal++;
 			CH_ALERT(adapter, "%s (0x%x)\n",
 				 acts->msg, status & acts->mask);
+			status &= ~acts->mask;
 		} else if (acts->msg)
 			CH_WARN(adapter, "%s (0x%x)\n",
 				acts->msg, status & acts->mask);
 		if (acts->stat_idx >= 0)
 			stats[acts->stat_idx]++;
 	}
-	if (status)		/* clear processed interrupts */
+	if (status)                           /* clear processed interrupts */
 		t3_write_reg(adapter, reg, status);
 	return fatal;
 }
@@ -1433,7 +1751,10 @@
 		       F_IRPARITYERROR | V_ITPARITYERROR(M_ITPARITYERROR) | \
 		       V_FLPARITYERROR(M_FLPARITYERROR) | F_LODRBPARITYERROR | \
 		       F_HIDRBPARITYERROR | F_LORCQPARITYERROR | \
-		       F_HIRCQPARITYERROR)
+		       F_HIRCQPARITYERROR | F_LOPRIORITYDBFULL | \
+		       F_HIPRIORITYDBFULL | F_LOPRIORITYDBEMPTY | \
+		       F_HIPRIORITYDBEMPTY | F_HIPIODRBDROPERR | \
+		       F_LOPIODRBDROPERR)
 #define MC5_INTR_MASK (F_PARITYERR | F_ACTRGNFULL | F_UNKNOWNCMD | \
 		       F_REQQPARERR | F_DISPQPARERR | F_DELACTEMPTY | \
 		       F_NFASRCHFAIL)
@@ -1466,7 +1787,8 @@
 		       F_DRAMPARERR | F_ICACHEPARERR | F_DCACHEPARERR | \
 		       F_OBQSGEPARERR | F_OBQULPHIPARERR | F_OBQULPLOPARERR | \
 		       F_IBQSGELOPARERR | F_IBQSGEHIPARERR | F_IBQULPPARERR | \
-		       F_IBQTPPARERR | F_ITAGPARERR | F_DTAGPARERR)
+		       F_IBQTPPARERR | F_ITAGPARERR | F_DTAGPARERR | \
+		       F_TIMER1INTEN)
 #define PMTX_INTR_MASK (F_ZERO_C_CMD_ERROR | ICSPI_FRM_ERR | OESPI_FRM_ERR | \
 			V_ICSPI_PAR_ERROR(M_ICSPI_PAR_ERROR) | \
 			V_OESPI_PAR_ERROR(M_OESPI_PAR_ERROR))
@@ -1485,32 +1807,32 @@
 /*
  * Interrupt handler for the PCIX1 module.
  */
-static void pci_intr_handler(struct adapter *adapter)
+static void pci_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info pcix1_intr_info[] = {
-		{F_MSTDETPARERR, "PCI master detected parity error", -1, 1},
-		{F_SIGTARABT, "PCI signaled target abort", -1, 1},
-		{F_RCVTARABT, "PCI received target abort", -1, 1},
-		{F_RCVMSTABT, "PCI received master abort", -1, 1},
-		{F_SIGSYSERR, "PCI signaled system error", -1, 1},
-		{F_DETPARERR, "PCI detected parity error", -1, 1},
-		{F_SPLCMPDIS, "PCI split completion discarded", -1, 1},
-		{F_UNXSPLCMP, "PCI unexpected split completion error", -1, 1},
-		{F_RCVSPLCMPERR, "PCI received split completion error", -1,
-		 1},
-		{F_DETCORECCERR, "PCI correctable ECC error",
-		 STAT_PCI_CORR_ECC, 0},
-		{F_DETUNCECCERR, "PCI uncorrectable ECC error", -1, 1},
-		{F_PIOPARERR, "PCI PIO FIFO parity error", -1, 1},
-		{V_WFPARERR(M_WFPARERR), "PCI write FIFO parity error", -1,
-		 1},
-		{V_RFPARERR(M_RFPARERR), "PCI read FIFO parity error", -1,
-		 1},
-		{V_CFPARERR(M_CFPARERR), "PCI command FIFO parity error", -1,
-		 1},
-		{V_MSIXPARERR(M_MSIXPARERR), "PCI MSI-X table/PBA parity "
-		 "error", -1, 1},
-		{0}
+	static struct intr_info pcix1_intr_info[] = {
+		{ F_MSTDETPARERR, "PCI master detected parity error", -1, 1 },
+		{ F_SIGTARABT, "PCI signaled target abort", -1, 1 },
+		{ F_RCVTARABT, "PCI received target abort", -1, 1 },
+		{ F_RCVMSTABT, "PCI received master abort", -1, 1 },
+		{ F_SIGSYSERR, "PCI signaled system error", -1, 1 },
+		{ F_DETPARERR, "PCI detected parity error", -1, 1 },
+		{ F_SPLCMPDIS, "PCI split completion discarded", -1, 1 },
+		{ F_UNXSPLCMP, "PCI unexpected split completion error", -1, 1 },
+		{ F_RCVSPLCMPERR, "PCI received split completion error", -1,
+		  1 },
+		{ F_DETCORECCERR, "PCI correctable ECC error",
+		  STAT_PCI_CORR_ECC, 0 },
+		{ F_DETUNCECCERR, "PCI uncorrectable ECC error", -1, 1 },
+		{ F_PIOPARERR, "PCI PIO FIFO parity error", -1, 1 },
+		{ V_WFPARERR(M_WFPARERR), "PCI write FIFO parity error", -1,
+		  1 },
+		{ V_RFPARERR(M_RFPARERR), "PCI read FIFO parity error", -1,
+		  1 },
+		{ V_CFPARERR(M_CFPARERR), "PCI command FIFO parity error", -1,
+		  1 },
+		{ V_MSIXPARERR(M_MSIXPARERR), "PCI MSI-X table/PBA parity "
+		  "error", -1, 1 },
+		{ 0 }
 	};
 
 	if (t3_handle_intr_status(adapter, A_PCIX_INT_CAUSE, PCIX_INTR_MASK,
@@ -1521,26 +1843,26 @@
 /*
  * Interrupt handler for the PCIE module.
  */
-static void pcie_intr_handler(struct adapter *adapter)
+static void pcie_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info pcie_intr_info[] = {
-		{F_PEXERR, "PCI PEX error", -1, 1},
-		{F_UNXSPLCPLERRR,
-		 "PCI unexpected split completion DMA read error", -1, 1},
-		{F_UNXSPLCPLERRC,
-		 "PCI unexpected split completion DMA command error", -1, 1},
-		{F_PCIE_PIOPARERR, "PCI PIO FIFO parity error", -1, 1},
-		{F_PCIE_WFPARERR, "PCI write FIFO parity error", -1, 1},
-		{F_PCIE_RFPARERR, "PCI read FIFO parity error", -1, 1},
-		{F_PCIE_CFPARERR, "PCI command FIFO parity error", -1, 1},
-		{V_PCIE_MSIXPARERR(M_PCIE_MSIXPARERR),
-		 "PCI MSI-X table/PBA parity error", -1, 1},
-		{F_RETRYBUFPARERR, "PCI retry buffer parity error", -1, 1},
-		{F_RETRYLUTPARERR, "PCI retry LUT parity error", -1, 1},
-		{F_RXPARERR, "PCI Rx parity error", -1, 1},
-		{F_TXPARERR, "PCI Tx parity error", -1, 1},
-		{V_BISTERR(M_BISTERR), "PCI BIST error", -1, 1},
-		{0}
+	static struct intr_info pcie_intr_info[] = {
+		{ F_PEXERR, "PCI PEX error", -1, 1 },
+		{ F_UNXSPLCPLERRR,
+		  "PCI unexpected split completion DMA read error", -1, 1 },
+		{ F_UNXSPLCPLERRC,
+		  "PCI unexpected split completion DMA command error", -1, 1 },
+		{ F_PCIE_PIOPARERR, "PCI PIO FIFO parity error", -1, 1 },
+		{ F_PCIE_WFPARERR, "PCI write FIFO parity error", -1, 1 },
+		{ F_PCIE_RFPARERR, "PCI read FIFO parity error", -1, 1 },
+		{ F_PCIE_CFPARERR, "PCI command FIFO parity error", -1, 1 },
+		{ V_PCIE_MSIXPARERR(M_PCIE_MSIXPARERR),
+		  "PCI MSI-X table/PBA parity error", -1, 1 },
+		{ F_RETRYBUFPARERR, "PCI retry buffer parity error", -1, 1 },
+		{ F_RETRYLUTPARERR, "PCI retry LUT parity error", -1, 1 },
+		{ F_RXPARERR, "PCI Rx parity error", -1, 1 },
+		{ F_TXPARERR, "PCI Tx parity error", -1, 1 },
+		{ V_BISTERR(M_BISTERR), "PCI BIST error", -1, 1 },
+		{ 0 }
 	};
 
 	if (t3_read_reg(adapter, A_PCIE_INT_CAUSE) & F_PEXERR)
@@ -1555,62 +1877,63 @@
 /*
  * TP interrupt handler.
  */
-static void tp_intr_handler(struct adapter *adapter)
+static void tp_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info tp_intr_info[] = {
-		{0xffffff, "TP parity error", -1, 1},
-		{0x1000000, "TP out of Rx pages", -1, 1},
-		{0x2000000, "TP out of Tx pages", -1, 1},
-		{0}
+	static struct intr_info tp_intr_info[] = {
+		{ 0xffffff,  "TP parity error", -1, 1 },
+		{ 0x1000000, "TP out of Rx pages", -1, 1 },
+		{ 0x2000000, "TP out of Tx pages", -1, 1 },
+		{ 0 }
 	};
-
 	static struct intr_info tp_intr_info_t3c[] = {
-		{0x1fffffff, "TP parity error", -1, 1},
-		{F_FLMRXFLSTEMPTY, "TP out of Rx pages", -1, 1},
-		{F_FLMTXFLSTEMPTY, "TP out of Tx pages", -1, 1},
-		{0}
+		{ 0x1fffffff,  "TP parity error", -1, 1 },
+		{ F_FLMRXFLSTEMPTY, "TP out of Rx pages", -1, 1 },
+		{ F_FLMTXFLSTEMPTY, "TP out of Tx pages", -1, 1 },
+		{ 0 }
 	};
 
 	if (t3_handle_intr_status(adapter, A_TP_INT_CAUSE, 0xffffffff,
 				  adapter->params.rev < T3_REV_C ?
-				  tp_intr_info : tp_intr_info_t3c, NULL))
+					tp_intr_info : tp_intr_info_t3c, NULL))
 		t3_fatal_err(adapter);
 }
 
 /*
  * CIM interrupt handler.
  */
-static void cim_intr_handler(struct adapter *adapter)
+static void cim_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info cim_intr_info[] = {
-		{F_RSVDSPACEINT, "CIM reserved space write", -1, 1},
-		{F_SDRAMRANGEINT, "CIM SDRAM address out of range", -1, 1},
-		{F_FLASHRANGEINT, "CIM flash address out of range", -1, 1},
-		{F_BLKWRBOOTINT, "CIM block write to boot space", -1, 1},
-		{F_WRBLKFLASHINT, "CIM write to cached flash space", -1, 1},
-		{F_SGLWRFLASHINT, "CIM single write to flash space", -1, 1},
-		{F_BLKRDFLASHINT, "CIM block read from flash space", -1, 1},
-		{F_BLKWRFLASHINT, "CIM block write to flash space", -1, 1},
-		{F_BLKRDCTLINT, "CIM block read from CTL space", -1, 1},
-		{F_BLKWRCTLINT, "CIM block write to CTL space", -1, 1},
-		{F_BLKRDPLINT, "CIM block read from PL space", -1, 1},
-		{F_BLKWRPLINT, "CIM block write to PL space", -1, 1},
-		{F_DRAMPARERR, "CIM DRAM parity error", -1, 1},
-		{F_ICACHEPARERR, "CIM icache parity error", -1, 1},
-		{F_DCACHEPARERR, "CIM dcache parity error", -1, 1},
-		{F_OBQSGEPARERR, "CIM OBQ SGE parity error", -1, 1},
-		{F_OBQULPHIPARERR, "CIM OBQ ULPHI parity error", -1, 1},
-		{F_OBQULPLOPARERR, "CIM OBQ ULPLO parity error", -1, 1},
-		{F_IBQSGELOPARERR, "CIM IBQ SGELO parity error", -1, 1},
-		{F_IBQSGEHIPARERR, "CIM IBQ SGEHI parity error", -1, 1},
-		{F_IBQULPPARERR, "CIM IBQ ULP parity error", -1, 1},
-		{F_IBQTPPARERR, "CIM IBQ TP parity error", -1, 1},
-		{F_ITAGPARERR, "CIM itag parity error", -1, 1},
-		{F_DTAGPARERR, "CIM dtag parity error", -1, 1},
-		{0}
-	};
-
-	if (t3_handle_intr_status(adapter, A_CIM_HOST_INT_CAUSE, 0xffffffff,
+	static struct intr_info cim_intr_info[] = {
+		{ F_RSVDSPACEINT, "CIM reserved space write", -1, 1 },
+		{ F_SDRAMRANGEINT, "CIM SDRAM address out of range", -1, 1 },
+		{ F_FLASHRANGEINT, "CIM flash address out of range", -1, 1 },
+		{ F_BLKWRBOOTINT, "CIM block write to boot space", -1, 1 },
+		{ F_WRBLKFLASHINT, "CIM write to cached flash space", -1, 1 },
+		{ F_SGLWRFLASHINT, "CIM single write to flash space", -1, 1 },
+		{ F_BLKRDFLASHINT, "CIM block read from flash space", -1, 1 },
+		{ F_BLKWRFLASHINT, "CIM block write to flash space", -1, 1 },
+		{ F_BLKRDCTLINT, "CIM block read from CTL space", -1, 1 },
+		{ F_BLKWRCTLINT, "CIM block write to CTL space", -1, 1 },
+		{ F_BLKRDPLINT, "CIM block read from PL space", -1, 1 },
+		{ F_BLKWRPLINT, "CIM block write to PL space", -1, 1 },
+		{ F_DRAMPARERR, "CIM DRAM parity error", -1, 1 },
+		{ F_ICACHEPARERR, "CIM icache parity error", -1, 1 },
+		{ F_DCACHEPARERR, "CIM dcache parity error", -1, 1 },
+		{ F_OBQSGEPARERR, "CIM OBQ SGE parity error", -1, 1 },
+		{ F_OBQULPHIPARERR, "CIM OBQ ULPHI parity error", -1, 1 },
+		{ F_OBQULPLOPARERR, "CIM OBQ ULPLO parity error", -1, 1 },
+		{ F_IBQSGELOPARERR, "CIM IBQ SGELO parity error", -1, 1 },
+		{ F_IBQSGEHIPARERR, "CIM IBQ SGEHI parity error", -1, 1 },
+		{ F_IBQULPPARERR, "CIM IBQ ULP parity error", -1, 1 },
+		{ F_IBQTPPARERR, "CIM IBQ TP parity error", -1, 1 },
+		{ F_ITAGPARERR, "CIM itag parity error", -1, 1 },
+		{ F_DTAGPARERR, "CIM dtag parity error", -1, 1 },
+		{ F_TIMER1INTEN, "CIM Timer1 expired interrupt. "
+			"Fimrware crashed?", -1, 0},
+		{ 0 }
+        };
+
+	if (t3_handle_intr_status(adapter, A_CIM_HOST_INT_CAUSE, CIM_INTR_MASK,
 				  cim_intr_info, NULL))
 		t3_fatal_err(adapter);
 }
@@ -1618,19 +1941,19 @@
 /*
  * ULP RX interrupt handler.
  */
-static void ulprx_intr_handler(struct adapter *adapter)
+static void ulprx_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info ulprx_intr_info[] = {
-		{F_PARERRDATA, "ULP RX data parity error", -1, 1},
-		{F_PARERRPCMD, "ULP RX command parity error", -1, 1},
-		{F_ARBPF1PERR, "ULP RX ArbPF1 parity error", -1, 1},
-		{F_ARBPF0PERR, "ULP RX ArbPF0 parity error", -1, 1},
-		{F_ARBFPERR, "ULP RX ArbF parity error", -1, 1},
-		{F_PCMDMUXPERR, "ULP RX PCMDMUX parity error", -1, 1},
-		{F_DATASELFRAMEERR1, "ULP RX frame error", -1, 1},
-		{F_DATASELFRAMEERR0, "ULP RX frame error", -1, 1},
-		{0}
-	};
+	static struct intr_info ulprx_intr_info[] = {
+		{ F_PARERRDATA, "ULP RX data parity error", -1, 1 },
+		{ F_PARERRPCMD, "ULP RX command parity error", -1, 1 },
+		{ F_ARBPF1PERR, "ULP RX ArbPF1 parity error", -1, 1 },
+		{ F_ARBPF0PERR, "ULP RX ArbPF0 parity error", -1, 1 },
+		{ F_ARBFPERR, "ULP RX ArbF parity error", -1, 1 },
+		{ F_PCMDMUXPERR, "ULP RX PCMDMUX parity error", -1, 1 },
+		{ F_DATASELFRAMEERR1, "ULP RX frame error", -1, 1 },
+		{ F_DATASELFRAMEERR0, "ULP RX frame error", -1, 1 },
+		{ 0 }
+        };
 
 	if (t3_handle_intr_status(adapter, A_ULPRX_INT_CAUSE, 0xffffffff,
 				  ulprx_intr_info, NULL))
@@ -1640,16 +1963,16 @@
 /*
  * ULP TX interrupt handler.
  */
-static void ulptx_intr_handler(struct adapter *adapter)
+static void ulptx_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info ulptx_intr_info[] = {
-		{F_PBL_BOUND_ERR_CH0, "ULP TX channel 0 PBL out of bounds",
-		 STAT_ULP_CH0_PBL_OOB, 0},
-		{F_PBL_BOUND_ERR_CH1, "ULP TX channel 1 PBL out of bounds",
-		 STAT_ULP_CH1_PBL_OOB, 0},
-		{0xfc, "ULP TX parity error", -1, 1},
-		{0}
-	};
+	static struct intr_info ulptx_intr_info[] = {
+		{ F_PBL_BOUND_ERR_CH0, "ULP TX channel 0 PBL out of bounds",
+		  STAT_ULP_CH0_PBL_OOB, 0 },
+		{ F_PBL_BOUND_ERR_CH1, "ULP TX channel 1 PBL out of bounds",
+		  STAT_ULP_CH1_PBL_OOB, 0 },
+		{ 0xfc, "ULP TX parity error", -1, 1 },
+		{ 0 }
+        };
 
 	if (t3_handle_intr_status(adapter, A_ULPTX_INT_CAUSE, 0xffffffff,
 				  ulptx_intr_info, adapter->irq_stats))
@@ -1668,18 +1991,18 @@
 /*
  * PM TX interrupt handler.
  */
-static void pmtx_intr_handler(struct adapter *adapter)
+static void pmtx_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info pmtx_intr_info[] = {
-		{F_ZERO_C_CMD_ERROR, "PMTX 0-length pcmd", -1, 1},
-		{ICSPI_FRM_ERR, "PMTX ispi framing error", -1, 1},
-		{OESPI_FRM_ERR, "PMTX ospi framing error", -1, 1},
-		{V_ICSPI_PAR_ERROR(M_ICSPI_PAR_ERROR),
-		 "PMTX ispi parity error", -1, 1},
-		{V_OESPI_PAR_ERROR(M_OESPI_PAR_ERROR),
-		 "PMTX ospi parity error", -1, 1},
-		{0}
-	};
+	static struct intr_info pmtx_intr_info[] = {
+		{ F_ZERO_C_CMD_ERROR, "PMTX 0-length pcmd", -1, 1 },
+		{ ICSPI_FRM_ERR, "PMTX ispi framing error", -1, 1 },
+		{ OESPI_FRM_ERR, "PMTX ospi framing error", -1, 1 },
+		{ V_ICSPI_PAR_ERROR(M_ICSPI_PAR_ERROR),
+		  "PMTX ispi parity error", -1, 1 },
+		{ V_OESPI_PAR_ERROR(M_OESPI_PAR_ERROR),
+		  "PMTX ospi parity error", -1, 1 },
+		{ 0 }
+        };
 
 	if (t3_handle_intr_status(adapter, A_PM1_TX_INT_CAUSE, 0xffffffff,
 				  pmtx_intr_info, NULL))
@@ -1698,18 +2021,18 @@
 /*
  * PM RX interrupt handler.
  */
-static void pmrx_intr_handler(struct adapter *adapter)
+static void pmrx_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info pmrx_intr_info[] = {
-		{F_ZERO_E_CMD_ERROR, "PMRX 0-length pcmd", -1, 1},
-		{IESPI_FRM_ERR, "PMRX ispi framing error", -1, 1},
-		{OCSPI_FRM_ERR, "PMRX ospi framing error", -1, 1},
-		{V_IESPI_PAR_ERROR(M_IESPI_PAR_ERROR),
-		 "PMRX ispi parity error", -1, 1},
-		{V_OCSPI_PAR_ERROR(M_OCSPI_PAR_ERROR),
-		 "PMRX ospi parity error", -1, 1},
-		{0}
-	};
+	static struct intr_info pmrx_intr_info[] = {
+		{ F_ZERO_E_CMD_ERROR, "PMRX 0-length pcmd", -1, 1 },
+		{ IESPI_FRM_ERR, "PMRX ispi framing error", -1, 1 },
+		{ OCSPI_FRM_ERR, "PMRX ospi framing error", -1, 1 },
+		{ V_IESPI_PAR_ERROR(M_IESPI_PAR_ERROR),
+		  "PMRX ispi parity error", -1, 1 },
+		{ V_OCSPI_PAR_ERROR(M_OCSPI_PAR_ERROR),
+		  "PMRX ospi parity error", -1, 1 },
+		{ 0 }
+        };
 
 	if (t3_handle_intr_status(adapter, A_PM1_RX_INT_CAUSE, 0xffffffff,
 				  pmrx_intr_info, NULL))
@@ -1719,17 +2042,17 @@
 /*
  * CPL switch interrupt handler.
  */
-static void cplsw_intr_handler(struct adapter *adapter)
+static void cplsw_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info cplsw_intr_info[] = {
-		{F_CIM_OP_MAP_PERR, "CPL switch CIM parity error", -1, 1},
-		{F_CIM_OVFL_ERROR, "CPL switch CIM overflow", -1, 1},
-		{F_TP_FRAMING_ERROR, "CPL switch TP framing error", -1, 1},
-		{F_SGE_FRAMING_ERROR, "CPL switch SGE framing error", -1, 1},
-		{F_CIM_FRAMING_ERROR, "CPL switch CIM framing error", -1, 1},
-		{F_ZERO_SWITCH_ERROR, "CPL switch no-switch error", -1, 1},
-		{0}
-	};
+	static struct intr_info cplsw_intr_info[] = {
+		{ F_CIM_OP_MAP_PERR, "CPL switch CIM parity error", -1, 1 },
+		{ F_CIM_OVFL_ERROR, "CPL switch CIM overflow", -1, 1 },
+		{ F_TP_FRAMING_ERROR, "CPL switch TP framing error", -1, 1 },
+		{ F_SGE_FRAMING_ERROR, "CPL switch SGE framing error", -1, 1 },
+		{ F_CIM_FRAMING_ERROR, "CPL switch CIM framing error", -1, 1 },
+		{ F_ZERO_SWITCH_ERROR, "CPL switch no-switch error", -1, 1 },
+		{ 0 }
+        };
 
 	if (t3_handle_intr_status(adapter, A_CPL_INTR_CAUSE, 0xffffffff,
 				  cplsw_intr_info, NULL))
@@ -1739,11 +2062,11 @@
 /*
  * MPS interrupt handler.
  */
-static void mps_intr_handler(struct adapter *adapter)
+static void mps_intr_handler(adapter_t *adapter)
 {
-	static const struct intr_info mps_intr_info[] = {
-		{0x1ff, "MPS parity error", -1, 1},
-		{0}
+	static struct intr_info mps_intr_info[] = {
+		{ 0x1ff, "MPS parity error", -1, 1 },
+		{ 0 }
 	};
 
 	if (t3_handle_intr_status(adapter, A_MPS_INT_CAUSE, 0xffffffff,
@@ -1758,7 +2081,7 @@
  */
 static void mc7_intr_handler(struct mc7 *mc7)
 {
-	struct adapter *adapter = mc7->adapter;
+	adapter_t *adapter = mc7->adapter;
 	u32 cause = t3_read_reg(adapter, mc7->offset + A_MC7_INT_CAUSE);
 
 	if (cause & F_CE) {
@@ -1809,17 +2132,22 @@
 /*
  * XGMAC interrupt handler.
  */
-static int mac_intr_handler(struct adapter *adap, unsigned int idx)
+static int mac_intr_handler(adapter_t *adap, unsigned int idx)
 {
-	struct cmac *mac = &adap2pinfo(adap, idx)->mac;
+	u32 cause;
+	struct cmac *mac;
+
+	idx = idx == 0 ? 0 : adapter_info(adap)->nports0; /* MAC idx -> port */
+	mac = &adap2pinfo(adap, idx)->mac;
+
 	/*
 	 * We mask out interrupt causes for which we're not taking interrupts.
 	 * This allows us to use polling logic to monitor some of the other
 	 * conditions when taking interrupts would impose too much load on the
 	 * system.
 	 */
-	u32 cause = t3_read_reg(adap, A_XGM_INT_CAUSE + mac->offset) &
-		    ~F_RXFIFO_OVERFLOW;
+	cause = (t3_read_reg(adap, A_XGM_INT_CAUSE + mac->offset)
+		 & ~(F_RXFIFO_OVERFLOW));
 
 	if (cause & V_TXFIFO_PRTY_ERR(M_TXFIFO_PRTY_ERR)) {
 		mac->stats.tx_fifo_parity_err++;
@@ -1843,23 +2171,22 @@
 		t3_set_reg_field(adap,
 				 A_XGM_INT_ENABLE + mac->offset,
 				 F_XGM_INT, 0);
-		mac->stats.link_faults++;
-
+
+		/* link fault suspected */
 		t3_os_link_fault_handler(adap, idx);
 	}
 
-	t3_write_reg(adap, A_XGM_INT_CAUSE + mac->offset, cause);
-
 	if (cause & XGM_INTR_FATAL)
 		t3_fatal_err(adap);
 
+	t3_write_reg(adap, A_XGM_INT_CAUSE + mac->offset, cause);
 	return cause != 0;
 }
 
 /*
  * Interrupt handler for PHY events.
  */
-int t3_phy_intr_handler(struct adapter *adapter)
+int t3_phy_intr_handler(adapter_t *adapter)
 {
 	u32 i, cause = t3_read_reg(adapter, A_T3DBG_INT_CAUSE);
 
@@ -1873,11 +2200,15 @@
 			int phy_cause = p->phy.ops->intr_handler(&p->phy);
 
 			if (phy_cause & cphy_cause_link_change)
-				t3_link_changed(adapter, i);
+				t3_os_link_fault_handler(adapter, i);
 			if (phy_cause & cphy_cause_fifo_error)
 				p->phy.fifo_errors++;
 			if (phy_cause & cphy_cause_module_change)
 				t3_os_phymod_changed(adapter, i);
+			if (phy_cause & cphy_cause_alarm)
+				CH_WARN(adapter, "Operation affected due to "
+				    "adverse environment.  Check the spec "
+				    "sheet for corrective action.");
 		}
 	}
 
@@ -1885,10 +2216,15 @@
 	return 0;
 }
 
-/*
- * T3 slow path (non-data) interrupt handler.
+/**
+ *	t3_slow_intr_handler - control path interrupt handler
+ *	@adapter: the adapter
+ *
+ *	T3 interrupt handler for non-data interrupt events, e.g., errors.
+ *	The designation 'slow' is because it involves register reads, while
+ *	data interrupts typically don't involve any MMIOs.
  */
-int t3_slow_intr_handler(struct adapter *adapter)
+int t3_slow_intr_handler(adapter_t *adapter)
 {
 	u32 cause = t3_read_reg(adapter, A_PL_INT_CAUSE0);
 
@@ -1936,11 +2272,11 @@
 
 	/* Clear the interrupts just processed. */
 	t3_write_reg(adapter, A_PL_INT_CAUSE0, cause);
-	t3_read_reg(adapter, A_PL_INT_CAUSE0);	/* flush */
+	(void) t3_read_reg(adapter, A_PL_INT_CAUSE0); /* flush */
 	return 1;
 }
 
-static unsigned int calc_gpio_intr(struct adapter *adap)
+static unsigned int calc_gpio_intr(adapter_t *adap)
 {
 	unsigned int i, gpi_intr = 0;
 
@@ -1959,21 +2295,20 @@
  *	various HW modules and then enabling the top-level interrupt
  *	concentrator.
  */
-void t3_intr_enable(struct adapter *adapter)
+void t3_intr_enable(adapter_t *adapter)
 {
-	static const struct addr_val_pair intr_en_avp[] = {
-		{A_SG_INT_ENABLE, SGE_INTR_MASK},
-		{A_MC7_INT_ENABLE, MC7_INTR_MASK},
-		{A_MC7_INT_ENABLE - MC7_PMRX_BASE_ADDR + MC7_PMTX_BASE_ADDR,
-		 MC7_INTR_MASK},
-		{A_MC7_INT_ENABLE - MC7_PMRX_BASE_ADDR + MC7_CM_BASE_ADDR,
-		 MC7_INTR_MASK},
-		{A_MC5_DB_INT_ENABLE, MC5_INTR_MASK},
-		{A_ULPRX_INT_ENABLE, ULPRX_INTR_MASK},
-		{A_PM1_TX_INT_ENABLE, PMTX_INTR_MASK},
-		{A_PM1_RX_INT_ENABLE, PMRX_INTR_MASK},
-		{A_CIM_HOST_INT_ENABLE, CIM_INTR_MASK},
-		{A_MPS_INT_ENABLE, MPS_INTR_MASK},
+	static struct addr_val_pair intr_en_avp[] = {
+		{ A_MC7_INT_ENABLE, MC7_INTR_MASK },
+		{ A_MC7_INT_ENABLE - MC7_PMRX_BASE_ADDR + MC7_PMTX_BASE_ADDR,
+			MC7_INTR_MASK },
+		{ A_MC7_INT_ENABLE - MC7_PMRX_BASE_ADDR + MC7_CM_BASE_ADDR,
+			MC7_INTR_MASK },
+		{ A_MC5_DB_INT_ENABLE, MC5_INTR_MASK },
+		{ A_ULPRX_INT_ENABLE, ULPRX_INTR_MASK },
+		{ A_PM1_TX_INT_ENABLE, PMTX_INTR_MASK },
+		{ A_PM1_RX_INT_ENABLE, PMRX_INTR_MASK },
+		{ A_CIM_HOST_INT_ENABLE, CIM_INTR_MASK },
+		{ A_MPS_INT_ENABLE, MPS_INTR_MASK },
 	};
 
 	adapter->slow_intr_mask = PL_INTR_MASK;
@@ -1981,6 +2316,7 @@
 	t3_write_regs(adapter, intr_en_avp, ARRAY_SIZE(intr_en_avp), 0);
 	t3_write_reg(adapter, A_TP_INT_ENABLE,
 		     adapter->params.rev >= T3_REV_C ? 0x2bfffff : 0x3bfffff);
+	t3_write_reg(adapter, A_SG_INT_ENABLE, SGE_INTR_MASK);
 
 	if (adapter->params.rev > 0) {
 		t3_write_reg(adapter, A_CPL_INTR_ENABLE,
@@ -2000,7 +2336,7 @@
 	else
 		t3_write_reg(adapter, A_PCIX_INT_ENABLE, PCIX_INTR_MASK);
 	t3_write_reg(adapter, A_PL_INT_ENABLE0, adapter->slow_intr_mask);
-	t3_read_reg(adapter, A_PL_INT_ENABLE0);	/* flush */
+	(void) t3_read_reg(adapter, A_PL_INT_ENABLE0);          /* flush */
 }
 
 /**
@@ -2010,10 +2346,10 @@
  *	Disable interrupts.  We only disable the top-level interrupt
  *	concentrator and the SGE data interrupts.
  */
-void t3_intr_disable(struct adapter *adapter)
+void t3_intr_disable(adapter_t *adapter)
 {
 	t3_write_reg(adapter, A_PL_INT_ENABLE0, 0);
-	t3_read_reg(adapter, A_PL_INT_ENABLE0);	/* flush */
+	(void) t3_read_reg(adapter, A_PL_INT_ENABLE0);  /* flush */
 	adapter->slow_intr_mask = 0;
 }
 
@@ -2023,7 +2359,7 @@
  *
  *	Clears all interrupts.
  */
-void t3_intr_clear(struct adapter *adapter)
+void t3_intr_clear(adapter_t *adapter)
 {
 	static const unsigned int cause_reg_addr[] = {
 		A_SG_INT_CAUSE,
@@ -2047,7 +2383,7 @@
 
 	/* Clear PHY and MAC interrupts for each port. */
 	for_each_port(adapter, i)
-	    t3_port_intr_clear(adapter, i);
+		t3_port_intr_clear(adapter, i);
 
 	for (i = 0; i < ARRAY_SIZE(cause_reg_addr); ++i)
 		t3_write_reg(adapter, cause_reg_addr[i], 0xffffffff);
@@ -2055,10 +2391,10 @@
 	if (is_pcie(adapter))
 		t3_write_reg(adapter, A_PCIE_PEX_ERR, 0xffffffff);
 	t3_write_reg(adapter, A_PL_INT_CAUSE0, 0xffffffff);
-	t3_read_reg(adapter, A_PL_INT_CAUSE0);	/* flush */
+	(void) t3_read_reg(adapter, A_PL_INT_CAUSE0);          /* flush */
 }
 
-void t3_xgm_intr_enable(struct adapter *adapter, int idx)
+void t3_xgm_intr_enable(adapter_t *adapter, int idx)
 {
 	struct port_info *pi = adap2pinfo(adapter, idx);
 
@@ -2066,7 +2402,7 @@
 		     XGM_EXTRA_INTR_MASK);
 }
 
-void t3_xgm_intr_disable(struct adapter *adapter, int idx)
+void t3_xgm_intr_disable(adapter_t *adapter, int idx)
 {
 	struct port_info *pi = adap2pinfo(adapter, idx);
 
@@ -2082,13 +2418,12 @@
  *	Enable port-specific (i.e., MAC and PHY) interrupts for the given
  *	adapter port.
  */
-void t3_port_intr_enable(struct adapter *adapter, int idx)
+void t3_port_intr_enable(adapter_t *adapter, int idx)
 {
-	struct cphy *phy = &adap2pinfo(adapter, idx)->phy;
-
-	t3_write_reg(adapter, XGM_REG(A_XGM_INT_ENABLE, idx), XGM_INTR_MASK);
-	t3_read_reg(adapter, XGM_REG(A_XGM_INT_ENABLE, idx)); /* flush */
-	phy->ops->intr_enable(phy);
+	struct port_info *pi = adap2pinfo(adapter, idx);
+
+	t3_write_reg(adapter, A_XGM_INT_ENABLE + pi->mac.offset, XGM_INTR_MASK);
+	pi->phy.ops->intr_enable(&pi->phy);
 }
 
 /**
@@ -2099,13 +2434,12 @@
  *	Disable port-specific (i.e., MAC and PHY) interrupts for the given
  *	adapter port.
  */
-void t3_port_intr_disable(struct adapter *adapter, int idx)
+void t3_port_intr_disable(adapter_t *adapter, int idx)
 {
-	struct cphy *phy = &adap2pinfo(adapter, idx)->phy;
-
-	t3_write_reg(adapter, XGM_REG(A_XGM_INT_ENABLE, idx), 0);
-	t3_read_reg(adapter, XGM_REG(A_XGM_INT_ENABLE, idx)); /* flush */
-	phy->ops->intr_disable(phy);
+	struct port_info *pi = adap2pinfo(adapter, idx);
+
+	t3_write_reg(adapter, A_XGM_INT_ENABLE + pi->mac.offset, 0);
+	pi->phy.ops->intr_disable(&pi->phy);
 }
 
 /**
@@ -2116,13 +2450,12 @@
  *	Clear port-specific (i.e., MAC and PHY) interrupts for the given
  *	adapter port.
  */
-void t3_port_intr_clear(struct adapter *adapter, int idx)
+void t3_port_intr_clear(adapter_t *adapter, int idx)
 {
-	struct cphy *phy = &adap2pinfo(adapter, idx)->phy;
-
-	t3_write_reg(adapter, XGM_REG(A_XGM_INT_CAUSE, idx), 0xffffffff);
-	t3_read_reg(adapter, XGM_REG(A_XGM_INT_CAUSE, idx)); /* flush */
-	phy->ops->intr_clear(phy);
+	struct port_info *pi = adap2pinfo(adapter, idx);
+
+	t3_write_reg(adapter, A_XGM_INT_CAUSE + pi->mac.offset, 0xffffffff);
+	pi->phy.ops->intr_clear(&pi->phy);
 }
 
 #define SG_CONTEXT_CMD_ATTEMPTS 100
@@ -2136,7 +2469,7 @@
  * 	Program an SGE context with the values already loaded in the
  * 	CONTEXT_DATA? registers.
  */
-static int t3_sge_write_context(struct adapter *adapter, unsigned int id,
+static int t3_sge_write_context(adapter_t *adapter, unsigned int id,
 				unsigned int type)
 {
 	if (type == F_RESPONSEQ) {
@@ -2173,8 +2506,7 @@
  *	"sensitive bits" in the contexts the way that t3_sge_write_context()
  *	does ...
  */
-static int clear_sge_ctxt(struct adapter *adap, unsigned int id,
-			  unsigned int type)
+static int clear_sge_ctxt(adapter_t *adap, unsigned int id, unsigned int type)
 {
 	t3_write_reg(adap, A_SG_CONTEXT_DATA0, 0);
 	t3_write_reg(adap, A_SG_CONTEXT_DATA1, 0);
@@ -2207,14 +2539,14 @@
  *	platform allows concurrent context operations, the caller is
  *	responsible for appropriate locking.
  */
-int t3_sge_init_ecntxt(struct adapter *adapter, unsigned int id, int gts_enable,
+int t3_sge_init_ecntxt(adapter_t *adapter, unsigned int id, int gts_enable,
 		       enum sge_context_type type, int respq, u64 base_addr,
 		       unsigned int size, unsigned int token, int gen,
 		       unsigned int cidx)
 {
 	unsigned int credits = type == SGE_CNTXT_OFLD ? 0 : FW_WR_NUM;
 
-	if (base_addr & 0xfff)	/* must be 4K aligned */
+	if (base_addr & 0xfff)     /* must be 4K aligned */
 		return -EINVAL;
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2223,12 +2555,12 @@
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA0, V_EC_INDEX(cidx) |
 		     V_EC_CREDITS(credits) | V_EC_GTS(gts_enable));
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA1, V_EC_SIZE(size) |
-		     V_EC_BASE_LO(base_addr & 0xffff));
+		     V_EC_BASE_LO((u32)base_addr & 0xffff));
 	base_addr >>= 16;
-	t3_write_reg(adapter, A_SG_CONTEXT_DATA2, base_addr);
+	t3_write_reg(adapter, A_SG_CONTEXT_DATA2, (u32)base_addr);
 	base_addr >>= 32;
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA3,
-		     V_EC_BASE_HI(base_addr & 0xf) | V_EC_RESPQ(respq) |
+		     V_EC_BASE_HI((u32)base_addr & 0xf) | V_EC_RESPQ(respq) |
 		     V_EC_TYPE(type) | V_EC_GEN(gen) | V_EC_UP_TOKEN(token) |
 		     F_EC_VALID);
 	return t3_sge_write_context(adapter, id, F_EGRESS);
@@ -2250,21 +2582,20 @@
  *	caller is responsible for ensuring only one context operation occurs
  *	at a time.
  */
-int t3_sge_init_flcntxt(struct adapter *adapter, unsigned int id,
-			int gts_enable, u64 base_addr, unsigned int size,
-			unsigned int bsize, unsigned int cong_thres, int gen,
-			unsigned int cidx)
+int t3_sge_init_flcntxt(adapter_t *adapter, unsigned int id, int gts_enable,
+			u64 base_addr, unsigned int size, unsigned int bsize,
+			unsigned int cong_thres, int gen, unsigned int cidx)
 {
-	if (base_addr & 0xfff)	/* must be 4K aligned */
+	if (base_addr & 0xfff)     /* must be 4K aligned */
 		return -EINVAL;
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
 
 	base_addr >>= 12;
-	t3_write_reg(adapter, A_SG_CONTEXT_DATA0, base_addr);
+	t3_write_reg(adapter, A_SG_CONTEXT_DATA0, (u32)base_addr);
 	base_addr >>= 32;
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA1,
-		     V_FL_BASE_HI((u32) base_addr) |
+		     V_FL_BASE_HI((u32)base_addr) |
 		     V_FL_INDEX_LO(cidx & M_FL_INDEX_LO));
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA2, V_FL_SIZE(size) |
 		     V_FL_GEN(gen) | V_FL_INDEX_HI(cidx >> 12) |
@@ -2290,13 +2621,13 @@
  *	The caller is responsible for ensuring only one context operation
  *	occurs at a time.
  */
-int t3_sge_init_rspcntxt(struct adapter *adapter, unsigned int id,
-			 int irq_vec_idx, u64 base_addr, unsigned int size,
+int t3_sge_init_rspcntxt(adapter_t *adapter, unsigned int id, int irq_vec_idx,
+			 u64 base_addr, unsigned int size,
 			 unsigned int fl_thres, int gen, unsigned int cidx)
 {
-	unsigned int intr = 0;
-
-	if (base_addr & 0xfff)	/* must be 4K aligned */
+	unsigned int ctrl, intr = 0;
+
+	if (base_addr & 0xfff)     /* must be 4K aligned */
 		return -EINVAL;
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2304,12 +2635,17 @@
 	base_addr >>= 12;
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA0, V_CQ_SIZE(size) |
 		     V_CQ_INDEX(cidx));
-	t3_write_reg(adapter, A_SG_CONTEXT_DATA1, base_addr);
+	t3_write_reg(adapter, A_SG_CONTEXT_DATA1, (u32)base_addr);
 	base_addr >>= 32;
-	if (irq_vec_idx >= 0)
-		intr = V_RQ_MSI_VEC(irq_vec_idx) | F_RQ_INTR_EN;
+        ctrl = t3_read_reg(adapter, A_SG_CONTROL);
+        if ((irq_vec_idx > 0) ||
+		((irq_vec_idx == 0) && !(ctrl & F_ONEINTMULTQ)))
+                	intr = F_RQ_INTR_EN;
+
+        if (irq_vec_idx >= 0)
+                intr |= V_RQ_MSI_VEC(irq_vec_idx);
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA2,
-		     V_CQ_BASE_HI((u32) base_addr) | intr | V_RQ_GEN(gen));
+		     V_CQ_BASE_HI((u32)base_addr) | intr | V_RQ_GEN(gen));
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA3, fl_thres);
 	return t3_sge_write_context(adapter, id, F_RESPONSEQ);
 }
@@ -2329,21 +2665,21 @@
  *	The caller is responsible for ensuring only one context operation
  *	occurs at a time.
  */
-int t3_sge_init_cqcntxt(struct adapter *adapter, unsigned int id, u64 base_addr,
+int t3_sge_init_cqcntxt(adapter_t *adapter, unsigned int id, u64 base_addr,
 			unsigned int size, int rspq, int ovfl_mode,
 			unsigned int credits, unsigned int credit_thres)
 {
-	if (base_addr & 0xfff)	/* must be 4K aligned */
+	if (base_addr & 0xfff)     /* must be 4K aligned */
 		return -EINVAL;
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
 
 	base_addr >>= 12;
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA0, V_CQ_SIZE(size));
-	t3_write_reg(adapter, A_SG_CONTEXT_DATA1, base_addr);
+	t3_write_reg(adapter, A_SG_CONTEXT_DATA1, (u32)base_addr);
 	base_addr >>= 32;
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA2,
-		     V_CQ_BASE_HI((u32) base_addr) | V_CQ_RSPQ(rspq) |
+		     V_CQ_BASE_HI((u32)base_addr) | V_CQ_RSPQ(rspq) |
 		     V_CQ_GEN(1) | V_CQ_OVERFLOW_MODE(ovfl_mode) |
 		     V_CQ_ERR(ovfl_mode));
 	t3_write_reg(adapter, A_SG_CONTEXT_DATA3, V_CQ_CREDITS(credits) |
@@ -2360,7 +2696,7 @@
  *	Enable or disable an SGE egress context.  The caller is responsible for
  *	ensuring only one context operation occurs at a time.
  */
-int t3_sge_enable_ecntxt(struct adapter *adapter, unsigned int id, int enable)
+int t3_sge_enable_ecntxt(adapter_t *adapter, unsigned int id, int enable)
 {
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2384,7 +2720,7 @@
  *	Disable an SGE free-buffer list.  The caller is responsible for
  *	ensuring only one context operation occurs at a time.
  */
-int t3_sge_disable_fl(struct adapter *adapter, unsigned int id)
+int t3_sge_disable_fl(adapter_t *adapter, unsigned int id)
 {
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2408,7 +2744,7 @@
  *	Disable an SGE response queue.  The caller is responsible for
  *	ensuring only one context operation occurs at a time.
  */
-int t3_sge_disable_rspcntxt(struct adapter *adapter, unsigned int id)
+int t3_sge_disable_rspcntxt(adapter_t *adapter, unsigned int id)
 {
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2432,7 +2768,7 @@
  *	Disable an SGE completion queue.  The caller is responsible for
  *	ensuring only one context operation occurs at a time.
  */
-int t3_sge_disable_cqcntxt(struct adapter *adapter, unsigned int id)
+int t3_sge_disable_cqcntxt(adapter_t *adapter, unsigned int id)
 {
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -2453,12 +2789,16 @@
  *	@adapter: the adapter
  *	@id: the context id
  *	@op: the operation to perform
+ *	@credits: credits to return to the CQ
  *
  *	Perform the selected operation on an SGE completion queue context.
  *	The caller is responsible for ensuring only one context operation
  *	occurs at a time.
+ *
+ *	For most operations the function returns the current HW position in
+ *	the completion queue.
  */
-int t3_sge_cqcntxt_op(struct adapter *adapter, unsigned int id, unsigned int op,
+int t3_sge_cqcntxt_op(adapter_t *adapter, unsigned int id, unsigned int op,
 		      unsigned int credits)
 {
 	u32 val;
@@ -2498,7 +2838,7 @@
  * 	Read an SGE egress context.  The caller is responsible for ensuring
  * 	only one context operation occurs at a time.
  */
-static int t3_sge_read_context(unsigned int type, struct adapter *adapter,
+static int t3_sge_read_context(unsigned int type, adapter_t *adapter,
 			       unsigned int id, u32 data[4])
 {
 	if (t3_read_reg(adapter, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
@@ -2525,7 +2865,7 @@
  * 	Read an SGE egress context.  The caller is responsible for ensuring
  * 	only one context operation occurs at a time.
  */
-int t3_sge_read_ecntxt(struct adapter *adapter, unsigned int id, u32 data[4])
+int t3_sge_read_ecntxt(adapter_t *adapter, unsigned int id, u32 data[4])
 {
 	if (id >= 65536)
 		return -EINVAL;
@@ -2541,7 +2881,7 @@
  * 	Read an SGE CQ context.  The caller is responsible for ensuring
  * 	only one context operation occurs at a time.
  */
-int t3_sge_read_cq(struct adapter *adapter, unsigned int id, u32 data[4])
+int t3_sge_read_cq(adapter_t *adapter, unsigned int id, u32 data[4])
 {
 	if (id >= 65536)
 		return -EINVAL;
@@ -2557,7 +2897,7 @@
  * 	Read an SGE free-list context.  The caller is responsible for ensuring
  * 	only one context operation occurs at a time.
  */
-int t3_sge_read_fl(struct adapter *adapter, unsigned int id, u32 data[4])
+int t3_sge_read_fl(adapter_t *adapter, unsigned int id, u32 data[4])
 {
 	if (id >= SGE_QSETS * 2)
 		return -EINVAL;
@@ -2573,7 +2913,7 @@
  * 	Read an SGE response queue context.  The caller is responsible for
  * 	ensuring only one context operation occurs at a time.
  */
-int t3_sge_read_rspq(struct adapter *adapter, unsigned int id, u32 data[4])
+int t3_sge_read_rspq(adapter_t *adapter, unsigned int id, u32 data[4])
 {
 	if (id >= SGE_QSETS)
 		return -EINVAL;
@@ -2592,8 +2932,8 @@
  *	provide fewer values than the size of the tables the supplied values
  *	are used repeatedly until the tables are fully populated.
  */
-void t3_config_rss(struct adapter *adapter, unsigned int rss_config,
-		   const u8 * cpus, const u16 *rspq)
+void t3_config_rss(adapter_t *adapter, unsigned int rss_config, const u8 *cpus,
+		   const u16 *rspq)
 {
 	int i, j, cpu_idx = 0, q_idx = 0;
 
@@ -2628,7 +2968,7 @@
  *
  *	Reads the contents of the receive packet steering tables.
  */
-int t3_read_rss(struct adapter *adapter, u8 * lkup, u16 *map)
+int t3_read_rss(adapter_t *adapter, u8 *lkup, u16 *map)
 {
 	int i;
 	u32 val;
@@ -2640,8 +2980,8 @@
 			val = t3_read_reg(adapter, A_TP_RSS_LKP_TABLE);
 			if (!(val & 0x80000000))
 				return -EAGAIN;
-			*lkup++ = val;
-			*lkup++ = (val >> 8);
+			*lkup++ = (u8)val;
+			*lkup++ = (u8)(val >> 8);
 		}
 
 	if (map)
@@ -2651,7 +2991,7 @@
 			val = t3_read_reg(adapter, A_TP_RSS_MAP_TABLE);
 			if (!(val & 0x80000000))
 				return -EAGAIN;
-			*map++ = val;
+			*map++ = (u16)val;
 		}
 	return 0;
 }
@@ -2663,7 +3003,7 @@
  *
  *	Switches TP to NIC/offload mode.
  */
-void t3_tp_set_offload_mode(struct adapter *adap, int enable)
+void t3_tp_set_offload_mode(adapter_t *adap, int enable)
 {
 	if (is_offload(adap) || !enable)
 		t3_set_reg_field(adap, A_TP_IN_CONFIG, F_NICMODE,
@@ -2671,6 +3011,52 @@
 }
 
 /**
+ *	tp_wr_bits_indirect - set/clear bits in an indirect TP register
+ *	@adap: the adapter
+ *	@addr: the indirect TP register address
+ *	@mask: specifies the field within the register to modify
+ *	@val: new value for the field
+ *
+ *	Sets a field of an indirect TP register to the given value.
+ */
+static void tp_wr_bits_indirect(adapter_t *adap, unsigned int addr,
+				unsigned int mask, unsigned int val)
+{
+	t3_write_reg(adap, A_TP_PIO_ADDR, addr);
+	val |= t3_read_reg(adap, A_TP_PIO_DATA) & ~mask;
+	t3_write_reg(adap, A_TP_PIO_DATA, val);
+}
+
+/**
+ *	t3_enable_filters - enable the HW filters
+ *	@adap: the adapter
+ *
+ *	Enables the HW filters for NIC traffic.
+ */
+void t3_enable_filters(adapter_t *adap)
+{
+	t3_set_reg_field(adap, A_TP_IN_CONFIG, F_NICMODE, 0);
+	t3_set_reg_field(adap, A_MC5_DB_CONFIG, 0, F_FILTEREN);
+	t3_set_reg_field(adap, A_TP_GLOBAL_CONFIG, 0, V_FIVETUPLELOOKUP(3));
+	tp_wr_bits_indirect(adap, A_TP_INGRESS_CONFIG, 0, F_LOOKUPEVERYPKT);
+}
+
+/**
+ *	t3_disable_filters - disable the HW filters
+ *	@adap: the adapter
+ *
+ *	Disables the HW filters for NIC traffic.
+ */
+void t3_disable_filters(adapter_t *adap)
+{
+	/* note that we don't want to revert to NIC-only mode */
+	t3_set_reg_field(adap, A_MC5_DB_CONFIG, F_FILTEREN, 0);
+	t3_set_reg_field(adap, A_TP_GLOBAL_CONFIG,
+			 V_FIVETUPLELOOKUP(M_FIVETUPLELOOKUP), 0);
+	tp_wr_bits_indirect(adap, A_TP_INGRESS_CONFIG, F_LOOKUPEVERYPKT, 0);
+}
+
+/**
  *	pm_num_pages - calculate the number of pages of the payload memory
  *	@mem_size: the size of the payload memory
  *	@pg_size: the size of each payload memory page
@@ -2699,7 +3085,7 @@
  *	Partitions context and payload memory and configures TP's memory
  *	registers.
  */
-static void partition_mem(struct adapter *adap, const struct tp_params *p)
+static void partition_mem(adapter_t *adap, const struct tp_params *p)
 {
 	unsigned int m, pstructs, tids = t3_mc5_size(&adap->mc5);
 	unsigned int timers = 0, timers_shift = 22;
@@ -2757,14 +3143,19 @@
 		adap->params.mc5.nservers += m - tids;
 }
 
-static inline void tp_wr_indirect(struct adapter *adap, unsigned int addr,
-				  u32 val)
+static inline void tp_wr_indirect(adapter_t *adap, unsigned int addr, u32 val)
 {
 	t3_write_reg(adap, A_TP_PIO_ADDR, addr);
 	t3_write_reg(adap, A_TP_PIO_DATA, val);
 }
 
-static void tp_config(struct adapter *adap, const struct tp_params *p)
+static inline u32 tp_rd_indirect(adapter_t *adap, unsigned int addr)
+{
+	t3_write_reg(adap, A_TP_PIO_ADDR, addr);
+	return t3_read_reg(adap, A_TP_PIO_DATA);
+}
+
+static void tp_config(adapter_t *adap, const struct tp_params *p)
 {
 	t3_write_reg(adap, A_TP_GLOBAL_CONFIG, F_TXPACINGENABLE | F_PATHMTU |
 		     F_IPCHECKSUMOFFLOAD | F_UDPCHECKSUMOFFLOAD |
@@ -2782,8 +3173,7 @@
 	t3_write_reg(adap, A_TP_PARA_REG4, 0x5050105);
 	t3_set_reg_field(adap, A_TP_PARA_REG6, 0,
 			 adap->params.rev > 0 ? F_ENABLEESND :
-			 F_T3A_ENABLEESND);
-
+			 			F_T3A_ENABLEESND);
 	t3_set_reg_field(adap, A_TP_PC_CONFIG,
 			 F_ENABLEEPCMDAFULL,
 			 F_ENABLEOCSPIFULL |F_TXDEFERENABLE | F_HEARBEATDACK |
@@ -2796,10 +3186,12 @@
 
 	if (adap->params.rev > 0) {
 		tp_wr_indirect(adap, A_TP_EGRESS_CONFIG, F_REWRITEFORCETOSIZE);
-		t3_set_reg_field(adap, A_TP_PARA_REG3, F_TXPACEAUTO,
-				 F_TXPACEAUTO);
+		t3_set_reg_field(adap, A_TP_PARA_REG3, 0,
+				 F_TXPACEAUTO | F_TXPACEAUTOSTRICT);
 		t3_set_reg_field(adap, A_TP_PC_CONFIG, F_LOCKTID, F_LOCKTID);
-		t3_set_reg_field(adap, A_TP_PARA_REG3, 0, F_TXPACEAUTOSTRICT);
+		tp_wr_indirect(adap, A_TP_VLAN_PRI_MAP, 0xfa50);
+		tp_wr_indirect(adap, A_TP_MAC_MATCH_MAP0, 0xfac688);
+		tp_wr_indirect(adap, A_TP_MAC_MATCH_MAP1, 0xfac688);
 	} else
 		t3_set_reg_field(adap, A_TP_PARA_REG3, 0, F_TXPACEFIXED);
 
@@ -2812,11 +3204,23 @@
 	t3_write_reg(adap, A_TP_TX_MOD_QUEUE_WEIGHT0, 0);
 	t3_write_reg(adap, A_TP_MOD_CHANNEL_WEIGHT, 0);
 	t3_write_reg(adap, A_TP_MOD_RATE_LIMIT, 0xf2200000);
+
+	if (adap->params.nports > 2) {
+		t3_set_reg_field(adap, A_TP_PC_CONFIG2, 0,
+				 F_ENABLETXPORTFROMDA2 | F_ENABLETXPORTFROMDA |
+				 F_ENABLERXPORTFROMADDR);
+		tp_wr_bits_indirect(adap, A_TP_QOS_RX_MAP_MODE,
+				    V_RXMAPMODE(M_RXMAPMODE), 0);
+		tp_wr_indirect(adap, A_TP_INGRESS_CONFIG, V_BITPOS0(48) |
+			       V_BITPOS1(49) | V_BITPOS2(50) | V_BITPOS3(51) |
+			       F_ENABLEEXTRACT | F_ENABLEEXTRACTIONSFD |
+			       F_ENABLEINSERTION | F_ENABLEINSERTIONSFD);
+		tp_wr_indirect(adap, A_TP_PREAMBLE_MSB, 0xfb000000);
+		tp_wr_indirect(adap, A_TP_PREAMBLE_LSB, 0xd5);
+		tp_wr_indirect(adap, A_TP_INTF_FROM_TX_PKT, F_INTFFROMTXPKT);
+	}
 }
 
-/* Desired TP timer resolution in usec */
-#define TP_TMR_RES 50
-
 /* TCP timer values in ms */
 #define TP_DACK_TIMER 50
 #define TP_RTO_MIN    250
@@ -2829,11 +3233,11 @@
  *	Set TP's timing parameters, such as the various timer resolutions and
  *	the TCP timer values.
  */
-static void tp_set_timers(struct adapter *adap, unsigned int core_clk)
+static void tp_set_timers(adapter_t *adap, unsigned int core_clk)
 {
-	unsigned int tre = fls(core_clk / (1000000 / TP_TMR_RES)) - 1;
-	unsigned int dack_re = fls(core_clk / 5000) - 1;	/* 200us */
-	unsigned int tstamp_re = fls(core_clk / 1000);	/* 1ms, at least */
+	unsigned int tre = adap->params.tp.tre;
+	unsigned int dack_re = adap->params.tp.dack_re;
+	unsigned int tstamp_re = fls(core_clk / 1000);     /* 1ms, at least */
 	unsigned int tps = core_clk >> tre;
 
 	t3_write_reg(adap, A_TP_TIMER_RESOLUTION, V_TIMERRESOLUTION(tre) |
@@ -2852,7 +3256,8 @@
 
 #define SECONDS * tps
 
-	t3_write_reg(adap, A_TP_MSL, adap->params.rev > 0 ? 0 : 2 SECONDS);
+	t3_write_reg(adap, A_TP_MSL,
+		     adap->params.rev > 0 ? 0 : 2 SECONDS);
 	t3_write_reg(adap, A_TP_RXT_MIN, tps / (1000 / TP_RTO_MIN));
 	t3_write_reg(adap, A_TP_RXT_MAX, 64 SECONDS);
 	t3_write_reg(adap, A_TP_PERS_MIN, 5 SECONDS);
@@ -2865,6 +3270,7 @@
 #undef SECONDS
 }
 
+#ifdef CONFIG_CHELSIO_T3_CORE
 /**
  *	t3_tp_set_coalescing_size - set receive coalescing size
  *	@adap: the adapter
@@ -2873,7 +3279,7 @@
  *
  *	Set the receive coalescing size and PSH bit handling.
  */
-int t3_tp_set_coalescing_size(struct adapter *adap, unsigned int size, int psh)
+int t3_tp_set_coalescing_size(adapter_t *adap, unsigned int size, int psh)
 {
 	u32 val;
 
@@ -2903,13 +3309,13 @@
  *	Set TP's max receive size.  This is the limit that applies when
  *	receive coalescing is disabled.
  */
-void t3_tp_set_max_rxsize(struct adapter *adap, unsigned int size)
+void t3_tp_set_max_rxsize(adapter_t *adap, unsigned int size)
 {
 	t3_write_reg(adap, A_TP_PARA_REG7,
 		     V_PMMAXXFERLEN0(size) | V_PMMAXXFERLEN1(size));
 }
 
-static void init_mtus(unsigned short mtus[])
+static void __devinit init_mtus(unsigned short mtus[])
 {
 	/*
 	 * See draft-mathis-plpmtud-00.txt for the values.  The min is 88 so
@@ -2934,10 +3340,14 @@
 	mtus[15] = 9600;
 }
 
-/*
- * Initial congestion control parameters.
+/**
+ *	init_cong_ctrl - initialize congestion control parameters
+ *	@a: the alpha values for congestion control
+ *	@b: the beta values for congestion control
+ *
+ *	Initialize the congestion control parameters.
  */
-static void init_cong_ctrl(unsigned short *a, unsigned short *b)
+static void __devinit init_cong_ctrl(unsigned short *a, unsigned short *b)
 {
 	a[0] = a[1] = a[2] = a[3] = a[4] = a[5] = a[6] = a[7] = a[8] = 1;
 	a[9] = 2;
@@ -2981,7 +3391,7 @@
  *	t3_load_mtus - write the MTU and congestion control HW tables
  *	@adap: the adapter
  *	@mtus: the unrestricted values for the MTU table
- *	@alphs: the values for the congestion control alpha parameter
+ *	@alpha: the values for the congestion control alpha parameter
  *	@beta: the values for the congestion control beta parameter
  *	@mtu_cap: the maximum permitted effective MTU
  *
@@ -2989,15 +3399,14 @@
  *	Update the high-speed congestion control table with the supplied alpha,
  * 	beta, and MTUs.
  */
-void t3_load_mtus(struct adapter *adap, unsigned short mtus[NMTUS],
+void t3_load_mtus(adapter_t *adap, unsigned short mtus[NMTUS],
 		  unsigned short alpha[NCCTRL_WIN],
 		  unsigned short beta[NCCTRL_WIN], unsigned short mtu_cap)
 {
 	static const unsigned int avg_pkts[NCCTRL_WIN] = {
 		2, 6, 10, 14, 20, 28, 40, 56, 80, 112, 160, 224, 320, 448, 640,
 		896, 1281, 1792, 2560, 3584, 5120, 7168, 10240, 14336, 20480,
-		28672, 40960, 57344, 81920, 114688, 163840, 229376
-	};
+		28672, 40960, 57344, 81920, 114688, 163840, 229376 };
 
 	unsigned int i, w;
 
@@ -3005,7 +3414,7 @@
 		unsigned int mtu = min(mtus[i], mtu_cap);
 		unsigned int log2 = fls(mtu);
 
-		if (!(mtu & ((1 << log2) >> 2)))	/* round */
+		if (!(mtu & ((1 << log2) >> 2)))     /* round */
 			log2--;
 		t3_write_reg(adap, A_TP_MTU_TABLE,
 			     (i << 24) | (log2 << 16) | mtu);
@@ -3029,7 +3438,7 @@
  *
  *	Reads the HW MTU table.
  */
-void t3_read_hw_mtus(struct adapter *adap, unsigned short mtus[NMTUS])
+void t3_read_hw_mtus(adapter_t *adap, unsigned short mtus[NMTUS])
 {
 	int i;
 
@@ -3050,7 +3459,7 @@
  *	Reads the additive increments programmed into the HW congestion
  *	control table.
  */
-void t3_get_cong_cntl_tab(struct adapter *adap,
+void t3_get_cong_cntl_tab(adapter_t *adap,
 			  unsigned short incr[NMTUS][NCCTRL_WIN])
 {
 	unsigned int mtu, w;
@@ -3059,8 +3468,8 @@
 		for (w = 0; w < NCCTRL_WIN; ++w) {
 			t3_write_reg(adap, A_TP_CCTRL_TABLE,
 				     0xffff0000 | (mtu << 5) | w);
-			incr[mtu][w] = t3_read_reg(adap, A_TP_CCTRL_TABLE) &
-				       0x1fff;
+			incr[mtu][w] = (unsigned short)t3_read_reg(adap,
+				        A_TP_CCTRL_TABLE) & 0x1fff;
 		}
 }
 
@@ -3071,12 +3480,48 @@
  *
  *	Returns the values of TP's MIB counters.
  */
-void t3_tp_get_mib_stats(struct adapter *adap, struct tp_mib_stats *tps)
+void t3_tp_get_mib_stats(adapter_t *adap, struct tp_mib_stats *tps)
 {
-	t3_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_RDATA, (u32 *) tps,
+	t3_read_indirect(adap, A_TP_MIB_INDEX, A_TP_MIB_RDATA, (u32 *)tps,
 			 sizeof(*tps) / sizeof(u32), 0);
 }
 
+/**
+ *	t3_read_pace_tbl - read the pace table
+ *	@adap: the adapter
+ *	@pace_vals: holds the returned values
+ *
+ *	Returns the values of TP's pace table in nanoseconds.
+ */
+void t3_read_pace_tbl(adapter_t *adap, unsigned int pace_vals[NTX_SCHED])
+{
+	unsigned int i, tick_ns = dack_ticks_to_usec(adap, 1000);
+
+	for (i = 0; i < NTX_SCHED; i++) {
+		t3_write_reg(adap, A_TP_PACE_TABLE, 0xffff0000 + i);
+		pace_vals[i] = t3_read_reg(adap, A_TP_PACE_TABLE) * tick_ns;
+	}
+}
+
+/**
+ *	t3_set_pace_tbl - set the pace table
+ *	@adap: the adapter
+ *	@pace_vals: the pace values in nanoseconds
+ *	@start: index of the first entry in the HW pace table to set
+ *	@n: how many entries to set
+ *
+ *	Sets (a subset of the) HW pace table.
+ */
+void t3_set_pace_tbl(adapter_t *adap, unsigned int *pace_vals,
+		     unsigned int start, unsigned int n)
+{
+	unsigned int tick_ns = dack_ticks_to_usec(adap, 1000);
+
+	for ( ; n; n--, start++, pace_vals++)
+		t3_write_reg(adap, A_TP_PACE_TABLE, (start << 16) |
+			     ((*pace_vals + tick_ns / 2) / tick_ns));
+}
+
 #define ulp_region(adap, name, start, len) \
 	t3_write_reg((adap), A_ULPRX_ ## name ## _LLIMIT, (start)); \
 	t3_write_reg((adap), A_ULPRX_ ## name ## _ULIMIT, \
@@ -3088,7 +3533,7 @@
 	t3_write_reg((adap), A_ULPTX_ ## name ## _ULIMIT, \
 		     (start) + (len) - 1)
 
-static void ulp_config(struct adapter *adap, const struct tp_params *p)
+static void ulp_config(adapter_t *adap, const struct tp_params *p)
 {
 	unsigned int m = p->chan_rx_size;
 
@@ -3102,6 +3547,7 @@
 	t3_write_reg(adap, A_ULPRX_TDDP_TAGMASK, 0xffffffff);
 }
 
+
 /**
  *	t3_set_proto_sram - set the contents of the protocol sram
  *	@adapter: the adapter
@@ -3109,30 +3555,38 @@
  *
  *	Write the contents of the protocol SRAM.
  */
-int t3_set_proto_sram(struct adapter *adap, const u8 *data)
+int t3_set_proto_sram(adapter_t *adap, const u8 *data)
 {
 	int i;
-	const __be32 *buf = (const __be32 *)data;
+	u32 *buf = (u32 *)data;
 
 	for (i = 0; i < PROTO_SRAM_LINES; i++) {
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD5, be32_to_cpu(*buf++));
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD4, be32_to_cpu(*buf++));
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD3, be32_to_cpu(*buf++));
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD2, be32_to_cpu(*buf++));
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD1, be32_to_cpu(*buf++));
-
-		t3_write_reg(adap, A_TP_EMBED_OP_FIELD0, i << 1 | 1 << 31);
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD5, cpu_to_be32(*buf++));
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD4, cpu_to_be32(*buf++));
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD3, cpu_to_be32(*buf++));
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD2, cpu_to_be32(*buf++));
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD1, cpu_to_be32(*buf++));
+
+		t3_write_reg(adap, A_TP_EMBED_OP_FIELD0, i << 1 | 1U << 31);
 		if (t3_wait_op_done(adap, A_TP_EMBED_OP_FIELD0, 1, 1, 5, 1))
 			return -EIO;
 	}
-	t3_write_reg(adap, A_TP_EMBED_OP_FIELD0, 0);
-
 	return 0;
 }
-
-void t3_config_trace_filter(struct adapter *adapter,
-			    const struct trace_params *tp, int filter_index,
-			    int invert, int enable)
+#endif
+
+/**
+ *	t3_config_trace_filter - configure one of the tracing filters
+ *	@adapter: the adapter
+ *	@tp: the desired trace filter parameters
+ *	@filter_index: which filter to configure
+ *	@invert: if set non-matching packets are traced instead of matching ones
+ *	@enable: whether to enable or disable the filter
+ *
+ *	Configures one of the tracing filters available in HW.
+ */
+void t3_config_trace_filter(adapter_t *adapter, const struct trace_params *tp,
+			    int filter_index, int invert, int enable)
 {
 	u32 addr, key[4], mask[4];
 
@@ -3159,8 +3613,53 @@
 	tp_wr_indirect(adapter, addr++, key[2]);
 	tp_wr_indirect(adapter, addr++, mask[2]);
 	tp_wr_indirect(adapter, addr++, key[3]);
-	tp_wr_indirect(adapter, addr, mask[3]);
-	t3_read_reg(adapter, A_TP_PIO_DATA);
+	tp_wr_indirect(adapter, addr,   mask[3]);
+	(void) t3_read_reg(adapter, A_TP_PIO_DATA);
+}
+
+/**
+ *	t3_query_trace_filter - query a tracing filter
+ *	@adapter: the adapter
+ *	@tp: the current trace filter parameters
+ *	@filter_index: which filter to query
+ *	@inverted: non-zero if the filter is inverted
+ *	@enabled: non-zero if the filter is enabled
+ *
+ *	Returns the current settings of the specified HW tracing filter.
+ */
+void t3_query_trace_filter(adapter_t *adapter, struct trace_params *tp,
+			   int filter_index, int *inverted, int *enabled)
+{
+	u32 addr, key[4], mask[4];
+
+	addr = filter_index ? A_TP_RX_TRC_KEY0 : A_TP_TX_TRC_KEY0;
+	key[0]  = tp_rd_indirect(adapter, addr++);
+	mask[0] = tp_rd_indirect(adapter, addr++);
+	key[1]  = tp_rd_indirect(adapter, addr++);
+	mask[1] = tp_rd_indirect(adapter, addr++);
+	key[2]  = tp_rd_indirect(adapter, addr++);
+	mask[2] = tp_rd_indirect(adapter, addr++);
+	key[3]  = tp_rd_indirect(adapter, addr++);
+	mask[3] = tp_rd_indirect(adapter, addr);
+
+	tp->sport = key[0] & 0xffff;
+	tp->sip   = (key[0] >> 16) | ((key[1] & 0xffff) << 16);
+	tp->dport = key[1] >> 16;
+	tp->dip   = key[2];
+	tp->proto = key[3] & 0xff;
+	tp->vlan  = key[3] >> 8;
+	tp->intf  = key[3] >> 20;
+
+	tp->sport_mask = mask[0] & 0xffff;
+	tp->sip_mask   = (mask[0] >> 16) | ((mask[1] & 0xffff) << 16);
+	tp->dport_mask = mask[1] >> 16;
+	tp->dip_mask   = mask[2];
+	tp->proto_mask = mask[3] & 0xff;
+	tp->vlan_mask  = mask[3] >> 8;
+	tp->intf_mask  = mask[3] >> 20;
+
+	*inverted = key[3] & (1 << 29);
+	*enabled  = key[3] & (1 << 28);
 }
 
 /**
@@ -3169,23 +3668,23 @@
  *	@kbps: target rate in Kbps
  *	@sched: the scheduler index
  *
- *	Configure a HW scheduler for the target rate
+ *	Configure a Tx HW scheduler for the target rate.
  */
-int t3_config_sched(struct adapter *adap, unsigned int kbps, int sched)
+int t3_config_sched(adapter_t *adap, unsigned int kbps, int sched)
 {
-	unsigned int v, tps, cpt, bpt, delta, mindelta = ~0;
+	unsigned int v, tps, cpt, bpt, delta, mindelta = ~0U;
 	unsigned int clk = adap->params.vpd.cclk * 1000;
 	unsigned int selected_cpt = 0, selected_bpt = 0;
 
 	if (kbps > 0) {
-		kbps *= 125;	/* -> bytes */
+		kbps *= 125;     /* -> bytes */
 		for (cpt = 1; cpt <= 255; cpt++) {
 			tps = clk / cpt;
 			bpt = (kbps + tps / 2) / tps;
 			if (bpt > 0 && bpt <= 255) {
 				v = bpt * tps;
 				delta = v >= kbps ? v - kbps : kbps - v;
-				if (delta <= mindelta) {
+				if (delta < mindelta) {
 					mindelta = delta;
 					selected_cpt = cpt;
 					selected_bpt = bpt;
@@ -3207,7 +3706,83 @@
 	return 0;
 }
 
-static int tp_init(struct adapter *adap, const struct tp_params *p)
+/**
+ *	t3_set_sched_ipg - set the IPG for a Tx HW packet rate scheduler
+ *	@adap: the adapter
+ *	@sched: the scheduler index
+ *	@ipg: the interpacket delay in tenths of nanoseconds
+ *
+ *	Set the interpacket delay for a HW packet rate scheduler.
+ */
+int t3_set_sched_ipg(adapter_t *adap, int sched, unsigned int ipg)
+{
+	unsigned int v, addr = A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR - sched / 2;
+
+	/* convert ipg to nearest number of core clocks */
+	ipg *= core_ticks_per_usec(adap);
+	ipg = (ipg + 5000) / 10000;
+	if (ipg > 0xffff)
+		return -EINVAL;
+
+	t3_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+	v = t3_read_reg(adap, A_TP_TM_PIO_DATA);
+	if (sched & 1)
+		v = (v & 0xffff) | (ipg << 16);
+	else
+		v = (v & 0xffff0000) | ipg;
+	t3_write_reg(adap, A_TP_TM_PIO_DATA, v);
+	t3_read_reg(adap, A_TP_TM_PIO_DATA);
+	return 0;
+}
+
+/**
+ *	t3_get_tx_sched - get the configuration of a Tx HW traffic scheduler
+ *	@adap: the adapter
+ *	@sched: the scheduler index
+ *	@kbps: the byte rate in Kbps
+ *	@ipg: the interpacket delay in tenths of nanoseconds
+ *
+ *	Return the current configuration of a HW Tx scheduler.
+ */
+void t3_get_tx_sched(adapter_t *adap, unsigned int sched, unsigned int *kbps,
+		     unsigned int *ipg)
+{
+	unsigned int v, addr, bpt, cpt;
+
+	if (kbps) {
+		addr = A_TP_TX_MOD_Q1_Q0_RATE_LIMIT - sched / 2;
+		t3_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+		v = t3_read_reg(adap, A_TP_TM_PIO_DATA);
+		if (sched & 1)
+			v >>= 16;
+		bpt = (v >> 8) & 0xff;
+		cpt = v & 0xff;
+		if (!cpt)
+			*kbps = 0;        /* scheduler disabled */
+		else {
+			v = (adap->params.vpd.cclk * 1000) / cpt;
+			*kbps = (v * bpt) / 125;
+		}
+	}
+	if (ipg) {
+		addr = A_TP_TX_MOD_Q1_Q0_TIMER_SEPARATOR - sched / 2;
+		t3_write_reg(adap, A_TP_TM_PIO_ADDR, addr);
+		v = t3_read_reg(adap, A_TP_TM_PIO_DATA);
+		if (sched & 1)
+			v >>= 16;
+		v &= 0xffff;
+		*ipg = (10000 * v) / core_ticks_per_usec(adap);
+	}
+}
+
+/**
+ *	tp_init - configure TP
+ *	@adap: the adapter
+ *	@p: TP configuration parameters
+ *
+ *	Initializes the TP HW module.
+ */
+static int tp_init(adapter_t *adap, const struct tp_params *p)
 {
 	int busy = 0;
 
@@ -3228,7 +3803,14 @@
 	return busy;
 }
 
-int t3_mps_set_active_ports(struct adapter *adap, unsigned int port_mask)
+/**
+ *	t3_mps_set_active_ports - configure port failover
+ *	@adap: the adapter
+ *	@port_mask: bitmap of active ports
+ *
+ *	Sets the active ports according to the supplied bitmap.
+ */
+int t3_mps_set_active_ports(adapter_t *adap, unsigned int port_mask)
 {
 	if (port_mask & ~((1 << adap->params.nports) - 1))
 		return -EINVAL;
@@ -3237,11 +3819,15 @@
 	return 0;
 }
 
-/*
- * Perform the bits of HW initialization that are dependent on the Tx
- * channels being used.
+/**
+ * 	chan_init_hw - channel-dependent HW initialization
+ *	@adap: the adapter
+ *	@chan_map: bitmap of Tx channels being used
+ *
+ *	Perform the bits of HW initialization that are dependent on the Tx
+ *	channels being used.
  */
-static void chan_init_hw(struct adapter *adap, unsigned int chan_map)
+static void chan_init_hw(adapter_t *adap, unsigned int chan_map)
 {
 	int i;
 
@@ -3253,6 +3839,11 @@
 					      F_TPTXPORT1EN | F_PORT1ACTIVE));
 		t3_write_reg(adap, A_PM1_TX_CFG,
 			     chan_map == 1 ? 0xffffffff : 0);
+		if (chan_map == 2)
+			t3_write_reg(adap, A_TP_TX_MOD_QUEUE_REQ_MAP,
+				     V_TX_MOD_QUEUE_REQ_MAP(0xff));
+		t3_write_reg(adap, A_TP_TX_MOD_QUE_TABLE, (12 << 16) | 0xd9c8);
+		t3_write_reg(adap, A_TP_TX_MOD_QUE_TABLE, (13 << 16) | 0xfbea);
 	} else {                                             /* two channels */
 		t3_set_reg_field(adap, A_ULPRX_CTL, 0, F_ROUND_ROBIN);
 		t3_set_reg_field(adap, A_ULPTX_CONFIG, 0, F_CFG_RR_ARB);
@@ -3268,17 +3859,19 @@
 		for (i = 0; i < 16; i++)
 			t3_write_reg(adap, A_TP_TX_MOD_QUE_TABLE,
 				     (i << 16) | 0x1010);
+		t3_write_reg(adap, A_TP_TX_MOD_QUE_TABLE, (12 << 16) | 0xba98);
+		t3_write_reg(adap, A_TP_TX_MOD_QUE_TABLE, (13 << 16) | 0xfedc);
 	}
 }
 
-static int calibrate_xgm(struct adapter *adapter)
+static int calibrate_xgm(adapter_t *adapter)
 {
 	if (uses_xaui(adapter)) {
 		unsigned int v, i;
 
 		for (i = 0; i < 5; ++i) {
 			t3_write_reg(adapter, A_XGM_XAUI_IMP, 0);
-			t3_read_reg(adapter, A_XGM_XAUI_IMP);
+			(void) t3_read_reg(adapter, A_XGM_XAUI_IMP);
 			msleep(1);
 			v = t3_read_reg(adapter, A_XGM_XAUI_IMP);
 			if (!(v & (F_XGM_CALFAULT | F_CALBUSY))) {
@@ -3298,7 +3891,7 @@
 	return 0;
 }
 
-static void calibrate_xgm_t3b(struct adapter *adapter)
+static void calibrate_xgm_t3b(adapter_t *adapter)
 {
 	if (!uses_xaui(adapter)) {
 		t3_write_reg(adapter, A_XGM_RGMII_IMP, F_CALRESET |
@@ -3328,10 +3921,10 @@
  * writes normally complete in a cycle or two, so one read should suffice.
  * The very first read exists to flush the posted write to the device.
  */
-static int wrreg_wait(struct adapter *adapter, unsigned int addr, u32 val)
+static int wrreg_wait(adapter_t *adapter, unsigned int addr, u32 val)
 {
-	t3_write_reg(adapter, addr, val);
-	t3_read_reg(adapter, addr);	/* flush */
+	t3_write_reg(adapter,	addr, val);
+	(void) t3_read_reg(adapter, addr);                   /* flush */
 	if (!(t3_read_reg(adapter, addr) & F_BUSY))
 		return 0;
 	CH_ERR(adapter, "write to MC7 register 0x%x timed out\n", addr);
@@ -3344,16 +3937,16 @@
 		0x632, 0x642, 0x652, 0x432, 0x442
 	};
 	static const struct mc7_timing_params mc7_timings[] = {
-		{12, 3, 4, {20, 28, 34, 52, 0}, 15, 6, 4},
-		{12, 4, 5, {20, 28, 34, 52, 0}, 16, 7, 4},
-		{12, 5, 6, {20, 28, 34, 52, 0}, 17, 8, 4},
-		{9, 3, 4, {15, 21, 26, 39, 0}, 12, 6, 4},
-		{9, 4, 5, {15, 21, 26, 39, 0}, 13, 7, 4}
+		{ 12, 3, 4, { 20, 28, 34, 52, 0 }, 15, 6, 4 },
+		{ 12, 4, 5, { 20, 28, 34, 52, 0 }, 16, 7, 4 },
+		{ 12, 5, 6, { 20, 28, 34, 52, 0 }, 17, 8, 4 },
+		{ 9,  3, 4, { 15, 21, 26, 39, 0 }, 12, 6, 4 },
+		{ 9,  4, 5, { 15, 21, 26, 39, 0 }, 13, 7, 4 }
 	};
 
 	u32 val;
 	unsigned int width, density, slow, attempts;
-	struct adapter *adapter = mc7->adapter;
+	adapter_t *adapter = mc7->adapter;
 	const struct mc7_timing_params *p = &mc7_timings[mem_type];
 
 	if (!mc7->size)
@@ -3365,12 +3958,12 @@
 	density = G_DEN(val);
 
 	t3_write_reg(adapter, mc7->offset + A_MC7_CFG, val | F_IFEN);
-	val = t3_read_reg(adapter, mc7->offset + A_MC7_CFG);	/* flush */
+	val = t3_read_reg(adapter, mc7->offset + A_MC7_CFG);  /* flush */
 	msleep(1);
 
 	if (!slow) {
 		t3_write_reg(adapter, mc7->offset + A_MC7_CAL, F_SGL_CAL_EN);
-		t3_read_reg(adapter, mc7->offset + A_MC7_CAL);
+		(void) t3_read_reg(adapter, mc7->offset + A_MC7_CAL);
 		msleep(1);
 		if (t3_read_reg(adapter, mc7->offset + A_MC7_CAL) &
 		    (F_BUSY | F_SGL_CAL_EN | F_CAL_FAULT)) {
@@ -3388,7 +3981,7 @@
 
 	t3_write_reg(adapter, mc7->offset + A_MC7_CFG,
 		     val | F_CLKEN | F_TERM150);
-	t3_read_reg(adapter, mc7->offset + A_MC7_CFG);	/* flush */
+	(void) t3_read_reg(adapter, mc7->offset + A_MC7_CFG); /* flush */
 
 	if (!slow)
 		t3_set_reg_field(adapter, mc7->offset + A_MC7_DLL, F_DLLENB,
@@ -3404,7 +3997,8 @@
 
 	if (!slow) {
 		t3_write_reg(adapter, mc7->offset + A_MC7_MODE, 0x100);
-		t3_set_reg_field(adapter, mc7->offset + A_MC7_DLL, F_DLLRST, 0);
+		t3_set_reg_field(adapter, mc7->offset + A_MC7_DLL,
+				 F_DLLRST, 0);
 		udelay(5);
 	}
 
@@ -3418,20 +4012,21 @@
 		goto out_fail;
 
 	/* clock value is in KHz */
-	mc7_clock = mc7_clock * 7812 + mc7_clock / 2;	/* ns */
-	mc7_clock /= 1000000;	/* KHz->MHz, ns->us */
+	mc7_clock = mc7_clock * 7812 + mc7_clock / 2;  /* ns */
+	mc7_clock /= 1000000;                          /* KHz->MHz, ns->us */
 
 	t3_write_reg(adapter, mc7->offset + A_MC7_REF,
 		     F_PERREFEN | V_PREREFDIV(mc7_clock));
-	t3_read_reg(adapter, mc7->offset + A_MC7_REF);	/* flush */
-
-	t3_write_reg(adapter, mc7->offset + A_MC7_ECC, F_ECCGENEN | F_ECCCHKEN);
+	(void) t3_read_reg(adapter, mc7->offset + A_MC7_REF); /* flush */
+
+	t3_write_reg(adapter, mc7->offset + A_MC7_ECC,
+		     F_ECCGENEN | F_ECCCHKEN);
 	t3_write_reg(adapter, mc7->offset + A_MC7_BIST_DATA, 0);
 	t3_write_reg(adapter, mc7->offset + A_MC7_BIST_ADDR_BEG, 0);
 	t3_write_reg(adapter, mc7->offset + A_MC7_BIST_ADDR_END,
 		     (mc7->size << width) - 1);
 	t3_write_reg(adapter, mc7->offset + A_MC7_BIST_OP, V_OP(1));
-	t3_read_reg(adapter, mc7->offset + A_MC7_BIST_OP);	/* flush */
+	(void) t3_read_reg(adapter, mc7->offset + A_MC7_BIST_OP); /* flush */
 
 	attempts = 50;
 	do {
@@ -3447,54 +4042,56 @@
 	t3_set_reg_field(adapter, mc7->offset + A_MC7_CFG, 0, F_RDY);
 	return 0;
 
-out_fail:
+ out_fail:
 	return -1;
 }
 
-static void config_pcie(struct adapter *adap)
+static void config_pcie(adapter_t *adap)
 {
 	static const u16 ack_lat[4][6] = {
-		{237, 416, 559, 1071, 2095, 4143},
-		{128, 217, 289, 545, 1057, 2081},
-		{73, 118, 154, 282, 538, 1050},
-		{67, 107, 86, 150, 278, 534}
+		{ 237, 416, 559, 1071, 2095, 4143 },
+		{ 128, 217, 289, 545, 1057, 2081 },
+		{ 73, 118, 154, 282, 538, 1050 },
+		{ 67, 107, 86, 150, 278, 534 }
 	};
 	static const u16 rpl_tmr[4][6] = {
-		{711, 1248, 1677, 3213, 6285, 12429},
-		{384, 651, 867, 1635, 3171, 6243},
-		{219, 354, 462, 846, 1614, 3150},
-		{201, 321, 258, 450, 834, 1602}
+		{ 711, 1248, 1677, 3213, 6285, 12429 },
+		{ 384, 651, 867, 1635, 3171, 6243 },
+		{ 219, 354, 462, 846, 1614, 3150 },
+		{ 201, 321, 258, 450, 834, 1602 }
 	};
 
 	u16 val, devid;
 	unsigned int log2_width, pldsize;
 	unsigned int fst_trn_rx, fst_trn_tx, acklat, rpllmt;
 
-	pci_read_config_word(adap->pdev,
-			     adap->params.pci.pcie_cap_addr + PCI_EXP_DEVCTL,
-			     &val);
+	t3_os_pci_read_config_2(adap,
+				adap->params.pci.pcie_cap_addr + PCI_EXP_DEVCTL,
+				&val);
 	pldsize = (val & PCI_EXP_DEVCTL_PAYLOAD) >> 5;
 
-	pci_read_config_word(adap->pdev, 0x2, &devid);
+	/*
+	 * Gen2 adapter pcie bridge compatibility requires minimum
+	 * Max_Read_Request_size
+	 */
+	t3_os_pci_read_config_2(adap, 0x2, &devid);
 	if (devid == 0x37) {
-		pci_write_config_word(adap->pdev,
-				      adap->params.pci.pcie_cap_addr +
-				      PCI_EXP_DEVCTL,
-				      val & ~PCI_EXP_DEVCTL_READRQ &
-				      ~PCI_EXP_DEVCTL_PAYLOAD);
+		t3_os_pci_write_config_2(adap,
+		    adap->params.pci.pcie_cap_addr + PCI_EXP_DEVCTL,
+		    val & ~PCI_EXP_DEVCTL_READRQ & ~PCI_EXP_DEVCTL_PAYLOAD);
 		pldsize = 0;
 	}
 
-	pci_read_config_word(adap->pdev,
-			     adap->params.pci.pcie_cap_addr + PCI_EXP_LNKCTL,
-			     &val);
+	t3_os_pci_read_config_2(adap,
+				adap->params.pci.pcie_cap_addr + PCI_EXP_LNKCTL,
+			       	&val);
 
 	fst_trn_tx = G_NUMFSTTRNSEQ(t3_read_reg(adap, A_PCIE_PEX_CTRL0));
 	fst_trn_rx = adap->params.rev == 0 ? fst_trn_tx :
-	    G_NUMFSTTRNSEQRX(t3_read_reg(adap, A_PCIE_MODE));
+			G_NUMFSTTRNSEQRX(t3_read_reg(adap, A_PCIE_MODE));
 	log2_width = fls(adap->params.pci.width) - 1;
 	acklat = ack_lat[log2_width][pldsize];
-	if (val & 1)		/* check LOsEnable */
+	if (val & 1)                            /* check LOsEnable */
 		acklat += fst_trn_tx * 4;
 	rpllmt = rpl_tmr[log2_width][pldsize] + fst_trn_rx * 4;
 
@@ -3515,15 +4112,20 @@
 			 F_PCIE_DMASTOPEN | F_PCIE_CLIDECEN);
 }
 
-/*
- * Initialize and configure T3 HW modules.  This performs the
- * initialization steps that need to be done once after a card is reset.
- * MAC and PHY initialization is handled separarely whenever a port is enabled.
+/**
+ * 	t3_init_hw - initialize and configure T3 HW modules
+ * 	@adapter: the adapter
+ * 	@fw_params: initial parameters to pass to firmware (optional)
  *
- * fw_params are passed to FW and their value is platform dependent.  Only the
- * top 8 bits are available for use, the rest must be 0.
+ *	Initialize and configure T3 HW modules.  This performs the
+ *	initialization steps that need to be done once after a card is reset.
+ *	MAC and PHY initialization is handled separarely whenever a port is
+ *	enabled.
+ *
+ *	@fw_params are passed to FW and their value is platform dependent.
+ *	Only the top 8 bits are available for use, the rest must be 0.
  */
-int t3_init_hw(struct adapter *adapter, u32 fw_params)
+int t3_init_hw(adapter_t *adapter, u32 fw_params)
 {
 	int err = -EIO, attempts, i;
 	const struct vpd_params *vpd = &adapter->params.vpd;
@@ -3533,6 +4135,9 @@
 	else if (calibrate_xgm(adapter))
 		goto out_err;
 
+	if (adapter->params.nports > 2)
+		t3_mac_init(&adap2pinfo(adapter, 0)->mac);
+
 	if (vpd->mclk) {
 		partition_mem(adapter, &adapter->params.tp);
 
@@ -3540,8 +4145,8 @@
 		    mc7_init(&adapter->pmtx, vpd->mclk, vpd->mem_timing) ||
 		    mc7_init(&adapter->cm, vpd->mclk, vpd->mem_timing) ||
 		    t3_mc5_init(&adapter->mc5, adapter->params.mc5.nservers,
-				adapter->params.mc5.nfilters,
-				adapter->params.mc5.nroutes))
+			        adapter->params.mc5.nfilters,
+			       	adapter->params.mc5.nroutes))
 			goto out_err;
 
 		for (i = 0; i < 32; i++)
@@ -3552,13 +4157,14 @@
 	if (tp_init(adapter, &adapter->params.tp))
 		goto out_err;
 
+#ifdef CONFIG_CHELSIO_T3_CORE
 	t3_tp_set_coalescing_size(adapter,
 				  min(adapter->params.sge.max_pkt_size,
 				      MAX_RX_COALESCING_LEN), 1);
 	t3_tp_set_max_rxsize(adapter,
 			     min(adapter->params.sge.max_pkt_size, 16384U));
 	ulp_config(adapter, &adapter->params.tp);
-
+#endif
 	if (is_pcie(adapter))
 		config_pcie(adapter);
 	else
@@ -3574,16 +4180,17 @@
 	t3_write_reg(adapter, A_PM1_TX_MODE, 0);
 	chan_init_hw(adapter, adapter->params.chan_map);
 	t3_sge_init(adapter, &adapter->params.sge);
+	t3_set_reg_field(adapter, A_PL_RST, 0, F_FATALPERREN);
 
 	t3_write_reg(adapter, A_T3DBG_GPIO_ACT_LOW, calc_gpio_intr(adapter));
 
 	t3_write_reg(adapter, A_CIM_HOST_ACC_DATA, vpd->uclk | fw_params);
 	t3_write_reg(adapter, A_CIM_BOOT_CFG,
 		     V_BOOTADDR(FW_FLASH_BOOT_ADDR >> 2));
-	t3_read_reg(adapter, A_CIM_BOOT_CFG);	/* flush */
+	(void) t3_read_reg(adapter, A_CIM_BOOT_CFG);    /* flush */
 
 	attempts = 100;
-	do {			/* wait for uP to initialize */
+	do {                          /* wait for uP to initialize */
 		msleep(20);
 	} while (t3_read_reg(adapter, A_CIM_HOST_ACC_DATA) && --attempts);
 	if (!attempts) {
@@ -3592,7 +4199,7 @@
 	}
 
 	err = 0;
-out_err:
+ out_err:
 	return err;
 }
 
@@ -3604,18 +4211,18 @@
  *	Determines a card's PCI mode and associated parameters, such as speed
  *	and width.
  */
-static void get_pci_mode(struct adapter *adapter, struct pci_params *p)
+static void __devinit get_pci_mode(adapter_t *adapter, struct pci_params *p)
 {
 	static unsigned short speed_map[] = { 33, 66, 100, 133 };
 	u32 pci_mode, pcie_cap;
 
-	pcie_cap = pci_find_capability(adapter->pdev, PCI_CAP_ID_EXP);
+	pcie_cap = t3_os_find_pci_capability(adapter, PCI_CAP_ID_EXP);
 	if (pcie_cap) {
 		u16 val;
 
 		p->variant = PCI_VARIANT_PCIE;
 		p->pcie_cap_addr = pcie_cap;
-		pci_read_config_word(adapter->pdev, pcie_cap + PCI_EXP_LNKSTA,
+		t3_os_pci_read_config_2(adapter, pcie_cap + PCI_EXP_LNKSTA,
 					&val);
 		p->width = (val >> 4) & 0x3f;
 		return;
@@ -3638,13 +4245,14 @@
 /**
  *	init_link_config - initialize a link's SW state
  *	@lc: structure holding the link state
- *	@ai: information about the current card
+ *	@caps: link capabilities
  *
  *	Initializes the SW state maintained for each link, including the link's
  *	capabilities and default speed/duplex/flow-control/autonegotiation
  *	settings.
  */
-static void init_link_config(struct link_config *lc, unsigned int caps)
+static void __devinit init_link_config(struct link_config *lc,
+				       unsigned int caps)
 {
 	lc->supported = caps;
 	lc->requested_speed = lc->speed = SPEED_INVALID;
@@ -3667,7 +4275,7 @@
  *	Calculates the size of an MC7 memory in bytes from the value of its
  *	configuration register.
  */
-static unsigned int mc7_calc_size(u32 cfg)
+static unsigned int __devinit mc7_calc_size(u32 cfg)
 {
 	unsigned int width = G_WIDTH(cfg);
 	unsigned int banks = !!(cfg & F_BKS) + 1;
@@ -3678,8 +4286,8 @@
 	return MBs << 20;
 }
 
-static void mc7_prep(struct adapter *adapter, struct mc7 *mc7,
-		     unsigned int base_addr, const char *name)
+static void __devinit mc7_prep(adapter_t *adapter, struct mc7 *mc7,
+			       unsigned int base_addr, const char *name)
 {
 	u32 cfg;
 
@@ -3687,21 +4295,32 @@
 	mc7->name = name;
 	mc7->offset = base_addr - MC7_PMRX_BASE_ADDR;
 	cfg = t3_read_reg(adapter, mc7->offset + A_MC7_CFG);
-	mc7->size = mc7->size = G_DEN(cfg) == M_DEN ? 0 : mc7_calc_size(cfg);
+	mc7->size = G_DEN(cfg) == M_DEN ? 0 : mc7_calc_size(cfg);
 	mc7->width = G_WIDTH(cfg);
 }
 
-void mac_prep(struct cmac *mac, struct adapter *adapter, int index)
+void mac_prep(struct cmac *mac, adapter_t *adapter, int index)
 {
 	u16 devid;
 
 	mac->adapter = adapter;
-	pci_read_config_word(adapter->pdev, 0x2, &devid);
-
-	if (devid == 0x37 && !adapter->params.vpd.xauicfg[1])
-		index = 0;
+	mac->multiport = adapter->params.nports > 2;
+	if (mac->multiport) {
+		mac->ext_port = (unsigned char)index;
+		mac->nucast = 8;
+	} else
+		mac->nucast = 1;
+
+	/* Gen2 adapter uses VPD xauicfg[] to notify driver which MAC
+	   is connected to each port, its suppose to be using xgmac0 for both ports
+	 */
+	t3_os_pci_read_config_2(adapter, 0x2, &devid);
+
+	if (mac->multiport ||
+		(!adapter->params.vpd.xauicfg[1] && (devid==0x37)))
+			index  = 0;
+
 	mac->offset = (XGMAC0_1_BASE_ADDR - XGMAC0_0_BASE_ADDR) * index;
-	mac->nucast = 1;
 
 	if (adapter->params.rev == 0 && uses_xaui(adapter)) {
 		t3_write_reg(adapter, A_XGM_SERDES_CTRL + mac->offset,
@@ -3711,15 +4330,47 @@
 	}
 }
 
-void early_hw_init(struct adapter *adapter, const struct adapter_info *ai)
+/*
+ * First stash of BT adapters have GPIO6 polarity inverted.
+ * Apply this ugly patch to get them working.
+ * This does not guarantee any forward compatibility.
+ */
+int is_demo_bt(adapter_t *adap)
 {
-	u32 val = V_PORTSPEED(is_10G(adapter) ? 3 : 2);
+	const struct port_type_info *pti;
+	int j = -1;
+
+	while (!adap->params.vpd.port_type[++j])
+		j++;
+	pti = &port_types[adap->params.vpd.port_type[j]];
+
+	return (pti->phy_prep == t3_tn1010_phy_prep);
+}
+
+/**
+ *	early_hw_init - HW initialization done at card detection time
+ *	@adapter: the adapter
+ *	@ai: contains information about the adapter type and properties
+ *
+ *	Perfoms the part of HW initialization that is done early on when the
+ *	driver first detecs the card.  Most of the HW state is initialized
+ *	lazily later on when a port or an offload function are first used.
+ */
+void early_hw_init(adapter_t *adapter, const struct adapter_info *ai)
+{
+	u32 val = V_PORTSPEED(is_10G(adapter) || adapter->params.nports > 2 ?
+			      3 : 2);
+	u32 gpio_out = ai->gpio_out;
 
 	mi1_init(adapter, ai);
-	t3_write_reg(adapter, A_I2C_CFG,	/* set for 80KHz */
+	t3_write_reg(adapter, A_I2C_CFG,                  /* set for 80KHz */
 		     V_I2C_CLKDIV(adapter->params.vpd.cclk / 80 - 1));
+
+	if (is_demo_bt(adapter))
+		gpio_out &= ~F_GPIO6_OUT_VAL;
+
 	t3_write_reg(adapter, A_T3DBG_GPIO_EN,
-		     ai->gpio_out | F_GPIO0_OEN | F_GPIO0_OUT_VAL);
+		     gpio_out | F_GPIO0_OEN | F_GPIO0_OUT_VAL);
 	t3_write_reg(adapter, A_MC5_DB_SERVER_INDEX, 0);
 	t3_write_reg(adapter, A_SG_OCO_BASE, V_BASE1(0xfff));
 
@@ -3728,37 +4379,38 @@
 
 	/* Enable MAC clocks so we can access the registers */
 	t3_write_reg(adapter, A_XGM_PORT_CFG, val);
-	t3_read_reg(adapter, A_XGM_PORT_CFG);
+	(void) t3_read_reg(adapter, A_XGM_PORT_CFG);
 
 	val |= F_CLKDIVRESET_;
 	t3_write_reg(adapter, A_XGM_PORT_CFG, val);
-	t3_read_reg(adapter, A_XGM_PORT_CFG);
+	(void) t3_read_reg(adapter, A_XGM_PORT_CFG);
 	t3_write_reg(adapter, XGM_REG(A_XGM_PORT_CFG, 1), val);
-	t3_read_reg(adapter, A_XGM_PORT_CFG);
+	(void) t3_read_reg(adapter, A_XGM_PORT_CFG);
 }
 
-/*
- * Reset the adapter.
- * Older PCIe cards lose their config space during reset, PCI-X
- * ones don't.
+/**
+ *	t3_reset_adapter - reset the adapter
+ *	@adapter: the adapter
+ *
+ * 	Reset the adapter.
  */
-int t3_reset_adapter(struct adapter *adapter)
+int t3_reset_adapter(adapter_t *adapter)
 {
 	int i, save_and_restore_pcie =
 	    adapter->params.rev < T3_REV_B2 && is_pcie(adapter);
 	uint16_t devid = 0;
 
 	if (save_and_restore_pcie)
-		pci_save_state(adapter->pdev);
+		t3_os_pci_save_state(adapter);
 	t3_write_reg(adapter, A_PL_RST, F_CRSTWRM | F_CRSTWRMMODE);
 
-	/*
+ 	/*
 	 * Delay. Give Some time to device to reset fully.
 	 * XXX The delay time should be modified.
 	 */
 	for (i = 0; i < 10; i++) {
 		msleep(50);
-		pci_read_config_word(adapter->pdev, 0x00, &devid);
+		t3_os_pci_read_config_2(adapter, 0x00, &devid);
 		if (devid == 0x1425)
 			break;
 	}
@@ -3767,13 +4419,13 @@
 		return -1;
 
 	if (save_and_restore_pcie)
-		pci_restore_state(adapter->pdev);
+		t3_os_pci_restore_state(adapter);
 	return 0;
 }
 
-static int init_parity(struct adapter *adap)
+static int init_parity(adapter_t *adap)
 {
-		int i, err, addr;
+	int i, err, addr;
 
 	if (t3_read_reg(adap, A_SG_CONTEXT_CMD) & F_CONTEXT_CMD_BUSY)
 		return -EBUSY;
@@ -3801,16 +4453,20 @@
 	return 0;
 }
 
-/*
- * Initialize adapter SW state for the various HW modules, set initial values
- * for some adapter tunables, take PHYs out of reset, and initialize the MDIO
- * interface.
+/**
+ *	t3_prep_adapter - prepare SW and HW for operation
+ *	@adapter: the adapter
+ *	@ai: contains information about the adapter type and properties
+ *
+ *	Initialize adapter SW state for the various HW modules, set initial
+ *	values for some adapter tunables, take PHYs out of reset, and
+ *	initialize the MDIO interface.
  */
-int t3_prep_adapter(struct adapter *adapter, const struct adapter_info *ai,
-		    int reset)
+int __devinit t3_prep_adapter(adapter_t *adapter,
+			      const struct adapter_info *ai, int reset)
 {
 	int ret;
-	unsigned int i, j = -1;
+	unsigned int i, j = 0;
 
 	get_pci_mode(adapter, &adapter->params.pci);
 
@@ -3818,19 +4474,25 @@
 	adapter->params.nports = ai->nports0 + ai->nports1;
 	adapter->params.chan_map = (!!ai->nports0) | (!!ai->nports1 << 1);
 	adapter->params.rev = t3_read_reg(adapter, A_PL_REV);
+
 	/*
 	 * We used to only run the "adapter check task" once a second if
 	 * we had PHYs which didn't support interrupts (we would check
 	 * their link status once a second).  Now we check other conditions
-	 * in that routine which could potentially impose a very high
+	 * in that routine which would [potentially] impose a very high
 	 * interrupt load on the system.  As such, we now always scan the
 	 * adapter state once a second ...
 	 */
 	adapter->params.linkpoll_period = 10;
-	adapter->params.stats_update_period = is_10G(adapter) ?
-	    MAC_STATS_ACCUM_SECS : (MAC_STATS_ACCUM_SECS * 10);
+
+	if (adapter->params.nports > 2)
+		adapter->params.stats_update_period = VSC_STATS_ACCUM_SECS;
+	else
+		adapter->params.stats_update_period = is_10G(adapter) ?
+			MAC_STATS_ACCUM_SECS : (MAC_STATS_ACCUM_SECS * 10);
 	adapter->params.pci.vpd_cap_addr =
-	    pci_find_capability(adapter->pdev, PCI_CAP_ID_VPD);
+		t3_os_find_pci_capability(adapter, PCI_CAP_ID_VPD);
+
 	ret = get_vpd_params(adapter, &adapter->params.vpd);
 	if (ret < 0)
 		return ret;
@@ -3851,14 +4513,17 @@
 		p->pmrx_size = t3_mc7_size(&adapter->pmrx);
 		p->pmtx_size = t3_mc7_size(&adapter->pmtx);
 		p->cm_size = t3_mc7_size(&adapter->cm);
-		p->chan_rx_size = p->pmrx_size / 2;	/* only 1 Rx channel */
+		p->chan_rx_size = p->pmrx_size / 2;     /* only 1 Rx channel */
 		p->chan_tx_size = p->pmtx_size / p->nchan;
 		p->rx_pg_size = 64 * 1024;
 		p->tx_pg_size = is_10G(adapter) ? 64 * 1024 : 16 * 1024;
 		p->rx_num_pgs = pm_num_pages(p->chan_rx_size, p->rx_pg_size);
 		p->tx_num_pgs = pm_num_pages(p->chan_tx_size, p->tx_pg_size);
 		p->ntimer_qs = p->cm_size >= (128 << 20) ||
-		    adapter->params.rev > 0 ? 12 : 6;
+			       adapter->params.rev > 0 ? 12 : 6;
+		p->tre = fls(adapter->params.vpd.cclk / (1000 / TP_TMR_RES)) -
+			 1;
+		p->dack_re = fls(adapter->params.vpd.cclk / 10) - 1; /* 100us */
 	}
 
 	adapter->params.offload = t3_mc7_size(&adapter->pmrx) &&
@@ -3867,13 +4532,15 @@
 
 	if (is_offload(adapter)) {
 		adapter->params.mc5.nservers = DEFAULT_NSERVERS;
-		adapter->params.mc5.nfilters = adapter->params.rev > 0 ?
-		    DEFAULT_NFILTERS : 0;
+		/* PR 6487. TOE and filtering are mutually exclusive */
+		adapter->params.mc5.nfilters = 0;
 		adapter->params.mc5.nroutes = 0;
 		t3_mc5_prep(adapter, &adapter->mc5, MC5_MODE_144_BIT);
 
+#ifdef CONFIG_CHELSIO_T3_CORE
 		init_mtus(adapter->params.mtus);
 		init_cong_ctrl(adapter->params.a_wnd, adapter->params.b_wnd);
+#endif
 	}
 
 	early_hw_init(adapter, ai);
@@ -3881,27 +4548,34 @@
 	if (ret)
 		return ret;
 
+	if (adapter->params.nports > 2 &&
+	    (ret = t3_vsc7323_init(adapter, adapter->params.nports)))
+		return ret;
+
 	for_each_port(adapter, i) {
 		u8 hw_addr[6];
 		const struct port_type_info *pti;
 		struct port_info *p = adap2pinfo(adapter, i);
 
-		while (!adapter->params.vpd.port_type[++j])
-			;
-
-		pti = &port_types[adapter->params.vpd.port_type[j]];
-		if (!pti->phy_prep) {
-			CH_ALERT(adapter, "Invalid port type index %d\n",
-				 adapter->params.vpd.port_type[j]);
-			return -EINVAL;
+		for (;;) {
+			unsigned port_type = adapter->params.vpd.port_type[j];
+			if (port_type) {
+				if (port_type < ARRAY_SIZE(port_types)) {
+					pti = &port_types[port_type];
+					break;
+				} else
+					return -EINVAL;
+			}
+			j++;
+			if (j >= ARRAY_SIZE(adapter->params.vpd.port_type))
+				return -EINVAL;
 		}
-
-		p->phy.mdio.dev = adapter->port[i];
-		ret = pti->phy_prep(&p->phy, adapter, ai->phy_base_addr + j,
+		ret = pti->phy_prep(p, ai->phy_base_addr + j,
 				    ai->mdio_ops);
 		if (ret)
 			return ret;
 		mac_prep(&p->mac, adapter, j);
+		++j;
 
 		/*
 		 * The VPD EEPROM stores the base Ethernet address for the
@@ -3911,10 +4585,7 @@
 		memcpy(hw_addr, adapter->params.vpd.eth_base, 5);
 		hw_addr[5] = adapter->params.vpd.eth_base[5] + i;
 
-		memcpy(adapter->port[i]->dev_addr, hw_addr,
-		       ETH_ALEN);
-		memcpy(adapter->port[i]->perm_addr, hw_addr,
-		       ETH_ALEN);
+		t3_os_set_hw_addr(adapter, i, hw_addr);
 		init_link_config(&p->link_config, p->phy.caps);
 		p->phy.ops->power_down(&p->phy, 1);
 
@@ -3931,37 +4602,204 @@
 	return 0;
 }
 
-void t3_led_ready(struct adapter *adapter)
+/**
+ *	t3_reinit_adapter - prepare HW for operation again
+ *	@adapter: the adapter
+ *
+ *	Put HW in the same state as @t3_prep_adapter without any changes to
+ *	SW state.  This is a cut down version of @t3_prep_adapter intended
+ *	to be used after events that wipe out HW state but preserve SW state,
+ *	e.g., EEH.  The device must be reset before calling this.
+ */
+int t3_reinit_adapter(adapter_t *adap)
+{
+	unsigned int i;
+	int ret, j = 0;
+
+	early_hw_init(adap, adap->params.info);
+	ret = init_parity(adap);
+	if (ret)
+		return ret;
+
+	if (adap->params.nports > 2 &&
+	    (ret = t3_vsc7323_init(adap, adap->params.nports)))
+		return ret;
+
+	for_each_port(adap, i) {
+		const struct port_type_info *pti;
+		struct port_info *p = adap2pinfo(adap, i);
+
+		for (;;) {
+			unsigned port_type = adap->params.vpd.port_type[j];
+			if (port_type) {
+				if (port_type < ARRAY_SIZE(port_types)) {
+					pti = &port_types[port_type];
+					break;
+				} else
+					return -EINVAL;
+			}
+			j++;
+			if (j >= ARRAY_SIZE(adap->params.vpd.port_type))
+				return -EINVAL;
+		}
+		ret = pti->phy_prep(p, p->phy.addr, NULL);
+		if (ret)
+			return ret;
+		p->phy.ops->power_down(&p->phy, 1);
+	}
+	return 0;
+}
+
+void t3_led_ready(adapter_t *adapter)
 {
 	t3_set_reg_field(adapter, A_T3DBG_GPIO_EN, F_GPIO0_OUT_VAL,
 			 F_GPIO0_OUT_VAL);
 }
 
-int t3_replay_prep_adapter(struct adapter *adapter)
+void t3_port_failover(adapter_t *adapter, int port)
 {
-	const struct adapter_info *ai = adapter->params.info;
-	unsigned int i, j = -1;
-	int ret;
-
-	early_hw_init(adapter, ai);
-	ret = init_parity(adapter);
+	u32 val;
+
+	val = port ? F_PORT1ACTIVE : F_PORT0ACTIVE;
+	t3_set_reg_field(adapter, A_MPS_CFG, F_PORT0ACTIVE | F_PORT1ACTIVE,
+			 val);
+}
+
+void t3_failover_done(adapter_t *adapter, int port)
+{
+	t3_set_reg_field(adapter, A_MPS_CFG, F_PORT0ACTIVE | F_PORT1ACTIVE,
+			 F_PORT0ACTIVE | F_PORT1ACTIVE);
+}
+
+void t3_failover_clear(adapter_t *adapter)
+{
+	t3_set_reg_field(adapter, A_MPS_CFG, F_PORT0ACTIVE | F_PORT1ACTIVE,
+			 F_PORT0ACTIVE | F_PORT1ACTIVE);
+}
+
+int t3_cim_hac_read(adapter_t *adapter, u32 addr, u32 *val)
+{
+	u32 v;
+
+	t3_write_reg(adapter, A_CIM_HOST_ACC_CTRL, addr);
+	if (t3_wait_op_done_val(adapter, A_CIM_HOST_ACC_CTRL,
+				F_HOSTBUSY, 0, 10, 10, &v))
+		return -EIO;
+
+	*val = t3_read_reg(adapter, A_CIM_HOST_ACC_DATA);
+
+	return 0;
+}
+
+int t3_cim_hac_write(adapter_t *adapter, u32 addr, u32 val)
+{
+	u32 v;
+
+	t3_write_reg(adapter, A_CIM_HOST_ACC_DATA, val);
+
+	addr |= F_HOSTWRITE;
+	t3_write_reg(adapter, A_CIM_HOST_ACC_CTRL, addr);
+
+	if (t3_wait_op_done_val(adapter, A_CIM_HOST_ACC_CTRL,
+				F_HOSTBUSY, 0, 10, 5, &v))
+		return -EIO;
+	return 0;
+}
+
+int t3_get_up_la(adapter_t *adapter, u32 *stopped, u32 *index,
+		 u32 *size, void *data)
+{
+	u32 v, *buf = data;
+	int i, cnt,  ret;
+
+	if (*size < LA_ENTRIES * 4)
+		return -EINVAL;
+
+	ret = t3_cim_hac_read(adapter, LA_CTRL, &v);
 	if (ret)
-		return ret;
-
-	for_each_port(adapter, i) {
-		const struct port_type_info *pti;
-		struct port_info *p = adap2pinfo(adapter, i);
-
-		while (!adapter->params.vpd.port_type[++j])
-			;
-
-		pti = &port_types[adapter->params.vpd.port_type[j]];
-		ret = pti->phy_prep(&p->phy, adapter, p->phy.mdio.prtad, NULL);
+		goto out;
+
+	*stopped = !(v & 1);
+
+	/* Freeze LA */
+	if (!*stopped) {
+		ret = t3_cim_hac_write(adapter, LA_CTRL, 0);
 		if (ret)
-			return ret;
-		p->phy.ops->power_down(&p->phy, 1);
+			goto out;
 	}
 
-return 0;
+	for (i = 0; i < LA_ENTRIES; i++) {
+		v = (i << 2) | (1 << 1);
+		ret = t3_cim_hac_write(adapter, LA_CTRL, v);
+		if (ret)
+			goto out;
+
+		ret = t3_cim_hac_read(adapter, LA_CTRL, &v);
+		if (ret)
+			goto out;
+
+		cnt = 20;
+		while ((v & (1 << 1)) && cnt) {
+			udelay(5);
+			--cnt;
+			ret = t3_cim_hac_read(adapter, LA_CTRL, &v);
+			if (ret)
+				goto out;
+		}
+
+		if (v & (1 << 1))
+			return -EIO;
+
+		ret = t3_cim_hac_read(adapter, LA_DATA, &v);
+		if (ret)
+			goto out;
+
+		*buf++ = v;
+	}
+
+	ret = t3_cim_hac_read(adapter, LA_CTRL, &v);
+	if (ret)
+		goto out;
+
+	*index = (v >> 16) + 4;
+	*size = LA_ENTRIES * 4;
+out:
+	/* Unfreeze LA */
+	t3_cim_hac_write(adapter, LA_CTRL, 1);
+	return ret;
 }
 
+int t3_get_up_ioqs(adapter_t *adapter, u32 *size, void *data)
+{
+	u32 v, *buf = data;
+	int i, j, ret;
+
+	if (*size < IOQ_ENTRIES * sizeof(struct t3_ioq_entry))
+		return -EINVAL;
+
+	for (i = 0; i < 4; i++) {
+		ret = t3_cim_hac_read(adapter, (4 * i), &v);
+		if (ret)
+			goto out;
+
+		*buf++ = v;
+	}
+
+	for (i = 0; i < IOQ_ENTRIES; i++) {
+		u32 base_addr = 0x10 * (i + 1);		
+
+		for (j = 0; j < 4; j++) {
+			ret = t3_cim_hac_read(adapter, base_addr + 4 * j, &v);
+			if (ret)
+				goto out;
+
+			*buf++ = v;
+		}
+	}
+	
+	*size = IOQ_ENTRIES * sizeof(struct t3_ioq_entry);
+
+out:
+	return ret;
+}
+	
diff --git a/drivers/net/cxgb3/t3cdev.h b/drivers/net/cxgb3/t3cdev.h
old mode 100644
new mode 100755
--- a/drivers/net/cxgb3/t3cdev.h
+++ b/drivers/net/cxgb3/t3cdev.h
@@ -1,33 +1,10 @@
 /*
- * Copyright (C) 2006-2008 Chelsio Communications.  All rights reserved.
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
 #ifndef _T3CDEV_H_
 #define _T3CDEV_H_
@@ -50,21 +27,21 @@
 };
 
 struct t3cdev {
-	char name[T3CNAMSIZ];	/* T3C device name */
+	char name[T3CNAMSIZ];		    /* T3C device name */
 	enum t3ctype type;
-	struct list_head ofld_dev_list;	/* for list linking */
-	struct net_device *lldev;	/* LL dev associated with T3C messages */
-	struct proc_dir_entry *proc_dir;	/* root of proc dir for this T3C */
+	struct list_head ofld_dev_list;	    /* for list linking */
+	struct net_device *lldev;     /* LL dev associated with T3C messages */
+	struct proc_dir_entry *proc_dir;    /* root of proc dir for this T3C */
 	int (*send)(struct t3cdev *dev, struct sk_buff *skb);
 	int (*recv)(struct t3cdev *dev, struct sk_buff **skb, int n);
 	int (*ctl)(struct t3cdev *dev, unsigned int req, void *data);
 	void (*neigh_update)(struct t3cdev *dev, struct neighbour *neigh);
-	void *priv;		/* driver private data */
-	void *l2opt;		/* optional layer 2 data */
-	void *l3opt;		/* optional layer 3 data */
-	void *l4opt;		/* optional layer 4 data */
-	void *ulp;		/* ulp stuff */
-	void *ulp_iscsi;	/* ulp iscsi */
+	void *priv;                         /* driver private data */
+	void *l2opt;                        /* optional layer 2 data */
+	void *l3opt;                        /* optional layer 3 data */
+	void *l4opt;                        /* optional layer 4 data */
+	void *ulp;			    /* ulp stuff */
+	void *ulp_iscsi;		    /* ulp iscsi */
 };
 
-#endif				/* _T3CDEV_H_ */
+#endif /* _T3CDEV_H_ */
diff --git a/drivers/net/cxgb3/tcb.h b/drivers/net/cxgb3/tcb.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/tcb.h
@@ -0,0 +1,646 @@
+/* This file is automatically generated --- do not edit */
+
+#ifndef _TCB_DEFS_H
+#define _TCB_DEFS_H
+
+#define W_TCB_T_STATE    0
+#define S_TCB_T_STATE    0
+#define M_TCB_T_STATE    0xfULL
+#define V_TCB_T_STATE(x) ((x) << S_TCB_T_STATE)
+
+#define W_TCB_TIMER    0
+#define S_TCB_TIMER    4
+#define M_TCB_TIMER    0x1ULL
+#define V_TCB_TIMER(x) ((x) << S_TCB_TIMER)
+
+#define W_TCB_DACK_TIMER    0
+#define S_TCB_DACK_TIMER    5
+#define M_TCB_DACK_TIMER    0x1ULL
+#define V_TCB_DACK_TIMER(x) ((x) << S_TCB_DACK_TIMER)
+
+#define W_TCB_DEL_FLAG    0
+#define S_TCB_DEL_FLAG    6
+#define M_TCB_DEL_FLAG    0x1ULL
+#define V_TCB_DEL_FLAG(x) ((x) << S_TCB_DEL_FLAG)
+
+#define W_TCB_L2T_IX    0
+#define S_TCB_L2T_IX    7
+#define M_TCB_L2T_IX    0x7ffULL
+#define V_TCB_L2T_IX(x) ((x) << S_TCB_L2T_IX)
+
+#define W_TCB_SMAC_SEL    0
+#define S_TCB_SMAC_SEL    18
+#define M_TCB_SMAC_SEL    0x3ULL
+#define V_TCB_SMAC_SEL(x) ((x) << S_TCB_SMAC_SEL)
+
+#define W_TCB_TOS    0
+#define S_TCB_TOS    20
+#define M_TCB_TOS    0x3fULL
+#define V_TCB_TOS(x) ((x) << S_TCB_TOS)
+
+#define W_TCB_MAX_RT    0
+#define S_TCB_MAX_RT    26
+#define M_TCB_MAX_RT    0xfULL
+#define V_TCB_MAX_RT(x) ((x) << S_TCB_MAX_RT)
+
+#define W_TCB_T_RXTSHIFT    0
+#define S_TCB_T_RXTSHIFT    30
+#define M_TCB_T_RXTSHIFT    0xfULL
+#define V_TCB_T_RXTSHIFT(x) ((x) << S_TCB_T_RXTSHIFT)
+
+#define W_TCB_T_DUPACKS    1
+#define S_TCB_T_DUPACKS    2
+#define M_TCB_T_DUPACKS    0xfULL
+#define V_TCB_T_DUPACKS(x) ((x) << S_TCB_T_DUPACKS)
+
+#define W_TCB_T_MAXSEG    1
+#define S_TCB_T_MAXSEG    6
+#define M_TCB_T_MAXSEG    0xfULL
+#define V_TCB_T_MAXSEG(x) ((x) << S_TCB_T_MAXSEG)
+
+#define W_TCB_T_FLAGS1    1
+#define S_TCB_T_FLAGS1    10
+#define M_TCB_T_FLAGS1    0xffffffffULL
+#define V_TCB_T_FLAGS1(x) ((x) << S_TCB_T_FLAGS1)
+
+#define W_TCB_T_FLAGS2    2
+#define S_TCB_T_FLAGS2    10
+#define M_TCB_T_FLAGS2    0x7fULL
+#define V_TCB_T_FLAGS2(x) ((x) << S_TCB_T_FLAGS2)
+
+#define W_TCB_SND_SCALE    2
+#define S_TCB_SND_SCALE    17
+#define M_TCB_SND_SCALE    0xfULL
+#define V_TCB_SND_SCALE(x) ((x) << S_TCB_SND_SCALE)
+
+#define W_TCB_RCV_SCALE    2
+#define S_TCB_RCV_SCALE    21
+#define M_TCB_RCV_SCALE    0xfULL
+#define V_TCB_RCV_SCALE(x) ((x) << S_TCB_RCV_SCALE)
+
+#define W_TCB_SND_UNA_RAW    2
+#define S_TCB_SND_UNA_RAW    25
+#define M_TCB_SND_UNA_RAW    0x7ffffffULL
+#define V_TCB_SND_UNA_RAW(x) ((x) << S_TCB_SND_UNA_RAW)
+
+#define W_TCB_SND_NXT_RAW    3
+#define S_TCB_SND_NXT_RAW    20
+#define M_TCB_SND_NXT_RAW    0x7ffffffULL
+#define V_TCB_SND_NXT_RAW(x) ((x) << S_TCB_SND_NXT_RAW)
+
+#define W_TCB_RCV_NXT    4
+#define S_TCB_RCV_NXT    15
+#define M_TCB_RCV_NXT    0xffffffffULL
+#define V_TCB_RCV_NXT(x) ((x) << S_TCB_RCV_NXT)
+
+#define W_TCB_RCV_ADV    5
+#define S_TCB_RCV_ADV    15
+#define M_TCB_RCV_ADV    0xffffULL
+#define V_TCB_RCV_ADV(x) ((x) << S_TCB_RCV_ADV)
+
+#define W_TCB_SND_MAX_RAW    5
+#define S_TCB_SND_MAX_RAW    31
+#define M_TCB_SND_MAX_RAW    0x7ffffffULL
+#define V_TCB_SND_MAX_RAW(x) ((x) << S_TCB_SND_MAX_RAW)
+
+#define W_TCB_SND_CWND    6
+#define S_TCB_SND_CWND    26
+#define M_TCB_SND_CWND    0x7ffffffULL
+#define V_TCB_SND_CWND(x) ((x) << S_TCB_SND_CWND)
+
+#define W_TCB_SND_SSTHRESH    7
+#define S_TCB_SND_SSTHRESH    21
+#define M_TCB_SND_SSTHRESH    0x7ffffffULL
+#define V_TCB_SND_SSTHRESH(x) ((x) << S_TCB_SND_SSTHRESH)
+
+#define W_TCB_T_RTT_TS_RECENT_AGE    8
+#define S_TCB_T_RTT_TS_RECENT_AGE    16
+#define M_TCB_T_RTT_TS_RECENT_AGE    0xffffffffULL
+#define V_TCB_T_RTT_TS_RECENT_AGE(x) ((x) << S_TCB_T_RTT_TS_RECENT_AGE)
+
+#define W_TCB_T_RTSEQ_RECENT    9
+#define S_TCB_T_RTSEQ_RECENT    16
+#define M_TCB_T_RTSEQ_RECENT    0xffffffffULL
+#define V_TCB_T_RTSEQ_RECENT(x) ((x) << S_TCB_T_RTSEQ_RECENT)
+
+#define W_TCB_T_SRTT    10
+#define S_TCB_T_SRTT    16
+#define M_TCB_T_SRTT    0xffffULL
+#define V_TCB_T_SRTT(x) ((x) << S_TCB_T_SRTT)
+
+#define W_TCB_T_RTTVAR    11
+#define S_TCB_T_RTTVAR    0
+#define M_TCB_T_RTTVAR    0xffffULL
+#define V_TCB_T_RTTVAR(x) ((x) << S_TCB_T_RTTVAR)
+
+#define W_TCB_TS_LAST_ACK_SENT_RAW    11
+#define S_TCB_TS_LAST_ACK_SENT_RAW    16
+#define M_TCB_TS_LAST_ACK_SENT_RAW    0x7ffffffULL
+#define V_TCB_TS_LAST_ACK_SENT_RAW(x) ((x) << S_TCB_TS_LAST_ACK_SENT_RAW)
+
+#define W_TCB_DIP    12
+#define S_TCB_DIP    11
+#define M_TCB_DIP    0xffffffffULL
+#define V_TCB_DIP(x) ((x) << S_TCB_DIP)
+
+#define W_TCB_SIP    13
+#define S_TCB_SIP    11
+#define M_TCB_SIP    0xffffffffULL
+#define V_TCB_SIP(x) ((x) << S_TCB_SIP)
+
+#define W_TCB_DP    14
+#define S_TCB_DP    11
+#define M_TCB_DP    0xffffULL
+#define V_TCB_DP(x) ((x) << S_TCB_DP)
+
+#define W_TCB_SP    14
+#define S_TCB_SP    27
+#define M_TCB_SP    0xffffULL
+#define V_TCB_SP(x) ((x) << S_TCB_SP)
+
+#define W_TCB_TIMESTAMP    15
+#define S_TCB_TIMESTAMP    11
+#define M_TCB_TIMESTAMP    0xffffffffULL
+#define V_TCB_TIMESTAMP(x) ((x) << S_TCB_TIMESTAMP)
+
+#define W_TCB_TIMESTAMP_OFFSET    16
+#define S_TCB_TIMESTAMP_OFFSET    11
+#define M_TCB_TIMESTAMP_OFFSET    0xfULL
+#define V_TCB_TIMESTAMP_OFFSET(x) ((x) << S_TCB_TIMESTAMP_OFFSET)
+
+#define W_TCB_TX_MAX    16
+#define S_TCB_TX_MAX    15
+#define M_TCB_TX_MAX    0xffffffffULL
+#define V_TCB_TX_MAX(x) ((x) << S_TCB_TX_MAX)
+
+#define W_TCB_TX_HDR_PTR_RAW    17
+#define S_TCB_TX_HDR_PTR_RAW    15
+#define M_TCB_TX_HDR_PTR_RAW    0x1ffffULL
+#define V_TCB_TX_HDR_PTR_RAW(x) ((x) << S_TCB_TX_HDR_PTR_RAW)
+
+#define W_TCB_TX_LAST_PTR_RAW    18
+#define S_TCB_TX_LAST_PTR_RAW    0
+#define M_TCB_TX_LAST_PTR_RAW    0x1ffffULL
+#define V_TCB_TX_LAST_PTR_RAW(x) ((x) << S_TCB_TX_LAST_PTR_RAW)
+
+#define W_TCB_TX_COMPACT    18
+#define S_TCB_TX_COMPACT    17
+#define M_TCB_TX_COMPACT    0x1ULL
+#define V_TCB_TX_COMPACT(x) ((x) << S_TCB_TX_COMPACT)
+
+#define W_TCB_RX_COMPACT    18
+#define S_TCB_RX_COMPACT    18
+#define M_TCB_RX_COMPACT    0x1ULL
+#define V_TCB_RX_COMPACT(x) ((x) << S_TCB_RX_COMPACT)
+
+#define W_TCB_RCV_WND    18
+#define S_TCB_RCV_WND    19
+#define M_TCB_RCV_WND    0x7ffffffULL
+#define V_TCB_RCV_WND(x) ((x) << S_TCB_RCV_WND)
+
+#define W_TCB_RX_HDR_OFFSET    19
+#define S_TCB_RX_HDR_OFFSET    14
+#define M_TCB_RX_HDR_OFFSET    0x7ffffffULL
+#define V_TCB_RX_HDR_OFFSET(x) ((x) << S_TCB_RX_HDR_OFFSET)
+
+#define W_TCB_RX_FRAG0_START_IDX_RAW    20
+#define S_TCB_RX_FRAG0_START_IDX_RAW    9
+#define M_TCB_RX_FRAG0_START_IDX_RAW    0x7ffffffULL
+#define V_TCB_RX_FRAG0_START_IDX_RAW(x) ((x) << S_TCB_RX_FRAG0_START_IDX_RAW)
+
+#define W_TCB_RX_FRAG1_START_IDX_OFFSET    21
+#define S_TCB_RX_FRAG1_START_IDX_OFFSET    4
+#define M_TCB_RX_FRAG1_START_IDX_OFFSET    0x7ffffffULL
+#define V_TCB_RX_FRAG1_START_IDX_OFFSET(x) ((x) << S_TCB_RX_FRAG1_START_IDX_OFFSET)
+
+#define W_TCB_RX_FRAG0_LEN    21
+#define S_TCB_RX_FRAG0_LEN    31
+#define M_TCB_RX_FRAG0_LEN    0x7ffffffULL
+#define V_TCB_RX_FRAG0_LEN(x) ((x) << S_TCB_RX_FRAG0_LEN)
+
+#define W_TCB_RX_FRAG1_LEN    22
+#define S_TCB_RX_FRAG1_LEN    26
+#define M_TCB_RX_FRAG1_LEN    0x7ffffffULL
+#define V_TCB_RX_FRAG1_LEN(x) ((x) << S_TCB_RX_FRAG1_LEN)
+
+#define W_TCB_NEWRENO_RECOVER    23
+#define S_TCB_NEWRENO_RECOVER    21
+#define M_TCB_NEWRENO_RECOVER    0x7ffffffULL
+#define V_TCB_NEWRENO_RECOVER(x) ((x) << S_TCB_NEWRENO_RECOVER)
+
+#define W_TCB_PDU_HAVE_LEN    24
+#define S_TCB_PDU_HAVE_LEN    16
+#define M_TCB_PDU_HAVE_LEN    0x1ULL
+#define V_TCB_PDU_HAVE_LEN(x) ((x) << S_TCB_PDU_HAVE_LEN)
+
+#define W_TCB_PDU_LEN    24
+#define S_TCB_PDU_LEN    17
+#define M_TCB_PDU_LEN    0xffffULL
+#define V_TCB_PDU_LEN(x) ((x) << S_TCB_PDU_LEN)
+
+#define W_TCB_RX_QUIESCE    25
+#define S_TCB_RX_QUIESCE    1
+#define M_TCB_RX_QUIESCE    0x1ULL
+#define V_TCB_RX_QUIESCE(x) ((x) << S_TCB_RX_QUIESCE)
+
+#define W_TCB_RX_PTR_RAW    25
+#define S_TCB_RX_PTR_RAW    2
+#define M_TCB_RX_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_PTR_RAW(x) ((x) << S_TCB_RX_PTR_RAW)
+
+#define W_TCB_CPU_NO    25
+#define S_TCB_CPU_NO    19
+#define M_TCB_CPU_NO    0x7fULL
+#define V_TCB_CPU_NO(x) ((x) << S_TCB_CPU_NO)
+
+#define W_TCB_ULP_TYPE    25
+#define S_TCB_ULP_TYPE    26
+#define M_TCB_ULP_TYPE    0xfULL
+#define V_TCB_ULP_TYPE(x) ((x) << S_TCB_ULP_TYPE)
+
+#define W_TCB_RX_FRAG1_PTR_RAW    25
+#define S_TCB_RX_FRAG1_PTR_RAW    30
+#define M_TCB_RX_FRAG1_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG1_PTR_RAW(x) ((x) << S_TCB_RX_FRAG1_PTR_RAW)
+
+#define W_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    26
+#define S_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    15
+#define M_TCB_RX_FRAG2_START_IDX_OFFSET_RAW    0x7ffffffULL
+#define V_TCB_RX_FRAG2_START_IDX_OFFSET_RAW(x) ((x) << S_TCB_RX_FRAG2_START_IDX_OFFSET_RAW)
+
+#define W_TCB_RX_FRAG2_PTR_RAW    27
+#define S_TCB_RX_FRAG2_PTR_RAW    10
+#define M_TCB_RX_FRAG2_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG2_PTR_RAW(x) ((x) << S_TCB_RX_FRAG2_PTR_RAW)
+
+#define W_TCB_RX_FRAG2_LEN_RAW    27
+#define S_TCB_RX_FRAG2_LEN_RAW    27
+#define M_TCB_RX_FRAG2_LEN_RAW    0x7ffffffULL
+#define V_TCB_RX_FRAG2_LEN_RAW(x) ((x) << S_TCB_RX_FRAG2_LEN_RAW)
+
+#define W_TCB_RX_FRAG3_PTR_RAW    28
+#define S_TCB_RX_FRAG3_PTR_RAW    22
+#define M_TCB_RX_FRAG3_PTR_RAW    0x1ffffULL
+#define V_TCB_RX_FRAG3_PTR_RAW(x) ((x) << S_TCB_RX_FRAG3_PTR_RAW)
+
+#define W_TCB_RX_FRAG3_LEN_RAW    29
+#define S_TCB_RX_FRAG3_LEN_RAW    7
+#define M_TCB_RX_FRAG3_LEN_RAW    0x7ffffffULL
+#define V_TCB_RX_FRAG3_LEN_RAW(x) ((x) << S_TCB_RX_FRAG3_LEN_RAW)
+
+#define W_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    30
+#define S_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    2
+#define M_TCB_RX_FRAG3_START_IDX_OFFSET_RAW    0x7ffffffULL
+#define V_TCB_RX_FRAG3_START_IDX_OFFSET_RAW(x) ((x) << S_TCB_RX_FRAG3_START_IDX_OFFSET_RAW)
+
+#define W_TCB_PDU_HDR_LEN    30
+#define S_TCB_PDU_HDR_LEN    29
+#define M_TCB_PDU_HDR_LEN    0xffULL
+#define V_TCB_PDU_HDR_LEN(x) ((x) << S_TCB_PDU_HDR_LEN)
+
+#define W_TCB_SLUSH1    31
+#define S_TCB_SLUSH1    5
+#define M_TCB_SLUSH1    0x7ffffULL
+#define V_TCB_SLUSH1(x) ((x) << S_TCB_SLUSH1)
+
+#define W_TCB_ULP_RAW    31
+#define S_TCB_ULP_RAW    24
+#define M_TCB_ULP_RAW    0xffULL
+#define V_TCB_ULP_RAW(x) ((x) << S_TCB_ULP_RAW)
+
+#define W_TCB_DDP_RDMAP_VERSION    25
+#define S_TCB_DDP_RDMAP_VERSION    30
+#define M_TCB_DDP_RDMAP_VERSION    0x1ULL
+#define V_TCB_DDP_RDMAP_VERSION(x) ((x) << S_TCB_DDP_RDMAP_VERSION)
+
+#define W_TCB_MARKER_ENABLE_RX    25
+#define S_TCB_MARKER_ENABLE_RX    31
+#define M_TCB_MARKER_ENABLE_RX    0x1ULL
+#define V_TCB_MARKER_ENABLE_RX(x) ((x) << S_TCB_MARKER_ENABLE_RX)
+
+#define W_TCB_MARKER_ENABLE_TX    26
+#define S_TCB_MARKER_ENABLE_TX    0
+#define M_TCB_MARKER_ENABLE_TX    0x1ULL
+#define V_TCB_MARKER_ENABLE_TX(x) ((x) << S_TCB_MARKER_ENABLE_TX)
+
+#define W_TCB_CRC_ENABLE    26
+#define S_TCB_CRC_ENABLE    1
+#define M_TCB_CRC_ENABLE    0x1ULL
+#define V_TCB_CRC_ENABLE(x) ((x) << S_TCB_CRC_ENABLE)
+
+#define W_TCB_IRS_ULP    26
+#define S_TCB_IRS_ULP    2
+#define M_TCB_IRS_ULP    0x1ffULL
+#define V_TCB_IRS_ULP(x) ((x) << S_TCB_IRS_ULP)
+
+#define W_TCB_ISS_ULP    26
+#define S_TCB_ISS_ULP    11
+#define M_TCB_ISS_ULP    0x1ffULL
+#define V_TCB_ISS_ULP(x) ((x) << S_TCB_ISS_ULP)
+
+#define W_TCB_TX_PDU_LEN    26
+#define S_TCB_TX_PDU_LEN    20
+#define M_TCB_TX_PDU_LEN    0x3fffULL
+#define V_TCB_TX_PDU_LEN(x) ((x) << S_TCB_TX_PDU_LEN)
+
+#define W_TCB_TX_PDU_OUT    27
+#define S_TCB_TX_PDU_OUT    2
+#define M_TCB_TX_PDU_OUT    0x1ULL
+#define V_TCB_TX_PDU_OUT(x) ((x) << S_TCB_TX_PDU_OUT)
+
+#define W_TCB_CQ_IDX_SQ    27
+#define S_TCB_CQ_IDX_SQ    3
+#define M_TCB_CQ_IDX_SQ    0xffffULL
+#define V_TCB_CQ_IDX_SQ(x) ((x) << S_TCB_CQ_IDX_SQ)
+
+#define W_TCB_CQ_IDX_RQ    27
+#define S_TCB_CQ_IDX_RQ    19
+#define M_TCB_CQ_IDX_RQ    0xffffULL
+#define V_TCB_CQ_IDX_RQ(x) ((x) << S_TCB_CQ_IDX_RQ)
+
+#define W_TCB_QP_ID    28
+#define S_TCB_QP_ID    3
+#define M_TCB_QP_ID    0xffffULL
+#define V_TCB_QP_ID(x) ((x) << S_TCB_QP_ID)
+
+#define W_TCB_PD_ID    28
+#define S_TCB_PD_ID    19
+#define M_TCB_PD_ID    0xffffULL
+#define V_TCB_PD_ID(x) ((x) << S_TCB_PD_ID)
+
+#define W_TCB_STAG    29
+#define S_TCB_STAG    3
+#define M_TCB_STAG    0xffffffffULL
+#define V_TCB_STAG(x) ((x) << S_TCB_STAG)
+
+#define W_TCB_RQ_START    30
+#define S_TCB_RQ_START    3
+#define M_TCB_RQ_START    0x3ffffffULL
+#define V_TCB_RQ_START(x) ((x) << S_TCB_RQ_START)
+
+#define W_TCB_RQ_MSN    30
+#define S_TCB_RQ_MSN    29
+#define M_TCB_RQ_MSN    0x3ffULL
+#define V_TCB_RQ_MSN(x) ((x) << S_TCB_RQ_MSN)
+
+#define W_TCB_RQ_MAX_OFFSET    31
+#define S_TCB_RQ_MAX_OFFSET    7
+#define M_TCB_RQ_MAX_OFFSET    0xfULL
+#define V_TCB_RQ_MAX_OFFSET(x) ((x) << S_TCB_RQ_MAX_OFFSET)
+
+#define W_TCB_RQ_WRITE_PTR    31
+#define S_TCB_RQ_WRITE_PTR    11
+#define M_TCB_RQ_WRITE_PTR    0x3ffULL
+#define V_TCB_RQ_WRITE_PTR(x) ((x) << S_TCB_RQ_WRITE_PTR)
+
+#define W_TCB_INB_WRITE_PERM    31
+#define S_TCB_INB_WRITE_PERM    21
+#define M_TCB_INB_WRITE_PERM    0x1ULL
+#define V_TCB_INB_WRITE_PERM(x) ((x) << S_TCB_INB_WRITE_PERM)
+
+#define W_TCB_INB_READ_PERM    31
+#define S_TCB_INB_READ_PERM    22
+#define M_TCB_INB_READ_PERM    0x1ULL
+#define V_TCB_INB_READ_PERM(x) ((x) << S_TCB_INB_READ_PERM)
+
+#define W_TCB_ORD_L_BIT_VLD    31
+#define S_TCB_ORD_L_BIT_VLD    23
+#define M_TCB_ORD_L_BIT_VLD    0x1ULL
+#define V_TCB_ORD_L_BIT_VLD(x) ((x) << S_TCB_ORD_L_BIT_VLD)
+
+#define W_TCB_RDMAP_OPCODE    31
+#define S_TCB_RDMAP_OPCODE    24
+#define M_TCB_RDMAP_OPCODE    0xfULL
+#define V_TCB_RDMAP_OPCODE(x) ((x) << S_TCB_RDMAP_OPCODE)
+
+#define W_TCB_TX_FLUSH    31
+#define S_TCB_TX_FLUSH    28
+#define M_TCB_TX_FLUSH    0x1ULL
+#define V_TCB_TX_FLUSH(x) ((x) << S_TCB_TX_FLUSH)
+
+#define W_TCB_TX_OOS_RXMT    31
+#define S_TCB_TX_OOS_RXMT    29
+#define M_TCB_TX_OOS_RXMT    0x1ULL
+#define V_TCB_TX_OOS_RXMT(x) ((x) << S_TCB_TX_OOS_RXMT)
+
+#define W_TCB_TX_OOS_TXMT    31
+#define S_TCB_TX_OOS_TXMT    30
+#define M_TCB_TX_OOS_TXMT    0x1ULL
+#define V_TCB_TX_OOS_TXMT(x) ((x) << S_TCB_TX_OOS_TXMT)
+
+#define W_TCB_SLUSH_AUX2    31
+#define S_TCB_SLUSH_AUX2    31
+#define M_TCB_SLUSH_AUX2    0x1ULL
+#define V_TCB_SLUSH_AUX2(x) ((x) << S_TCB_SLUSH_AUX2)
+
+#define W_TCB_RX_FRAG1_PTR_RAW2    25
+#define S_TCB_RX_FRAG1_PTR_RAW2    30
+#define M_TCB_RX_FRAG1_PTR_RAW2    0x1ffffULL
+#define V_TCB_RX_FRAG1_PTR_RAW2(x) ((x) << S_TCB_RX_FRAG1_PTR_RAW2)
+
+#define W_TCB_RX_DDP_FLAGS    26
+#define S_TCB_RX_DDP_FLAGS    15
+#define M_TCB_RX_DDP_FLAGS    0xffffULL
+#define V_TCB_RX_DDP_FLAGS(x) ((x) << S_TCB_RX_DDP_FLAGS)
+
+#define W_TCB_SLUSH_AUX3    26
+#define S_TCB_SLUSH_AUX3    31
+#define M_TCB_SLUSH_AUX3    0x1ffULL
+#define V_TCB_SLUSH_AUX3(x) ((x) << S_TCB_SLUSH_AUX3)
+
+#define W_TCB_RX_DDP_BUF0_OFFSET    27
+#define S_TCB_RX_DDP_BUF0_OFFSET    8
+#define M_TCB_RX_DDP_BUF0_OFFSET    0x3fffffULL
+#define V_TCB_RX_DDP_BUF0_OFFSET(x) ((x) << S_TCB_RX_DDP_BUF0_OFFSET)
+
+#define W_TCB_RX_DDP_BUF0_LEN    27
+#define S_TCB_RX_DDP_BUF0_LEN    30
+#define M_TCB_RX_DDP_BUF0_LEN    0x3fffffULL
+#define V_TCB_RX_DDP_BUF0_LEN(x) ((x) << S_TCB_RX_DDP_BUF0_LEN)
+
+#define W_TCB_RX_DDP_BUF1_OFFSET    28
+#define S_TCB_RX_DDP_BUF1_OFFSET    20
+#define M_TCB_RX_DDP_BUF1_OFFSET    0x3fffffULL
+#define V_TCB_RX_DDP_BUF1_OFFSET(x) ((x) << S_TCB_RX_DDP_BUF1_OFFSET)
+
+#define W_TCB_RX_DDP_BUF1_LEN    29
+#define S_TCB_RX_DDP_BUF1_LEN    10
+#define M_TCB_RX_DDP_BUF1_LEN    0x3fffffULL
+#define V_TCB_RX_DDP_BUF1_LEN(x) ((x) << S_TCB_RX_DDP_BUF1_LEN)
+
+#define W_TCB_RX_DDP_BUF0_TAG    30
+#define S_TCB_RX_DDP_BUF0_TAG    0
+#define M_TCB_RX_DDP_BUF0_TAG    0xffffffffULL
+#define V_TCB_RX_DDP_BUF0_TAG(x) ((x) << S_TCB_RX_DDP_BUF0_TAG)
+
+#define W_TCB_RX_DDP_BUF1_TAG    31
+#define S_TCB_RX_DDP_BUF1_TAG    0
+#define M_TCB_RX_DDP_BUF1_TAG    0xffffffffULL
+#define V_TCB_RX_DDP_BUF1_TAG(x) ((x) << S_TCB_RX_DDP_BUF1_TAG)
+
+#define S_TF_DACK    10
+#define V_TF_DACK(x) ((x) << S_TF_DACK)
+
+#define S_TF_NAGLE    11
+#define V_TF_NAGLE(x) ((x) << S_TF_NAGLE)
+
+#define S_TF_RECV_SCALE    12
+#define V_TF_RECV_SCALE(x) ((x) << S_TF_RECV_SCALE)
+
+#define S_TF_RECV_TSTMP    13
+#define V_TF_RECV_TSTMP(x) ((x) << S_TF_RECV_TSTMP)
+
+#define S_TF_RECV_SACK    14
+#define V_TF_RECV_SACK(x) ((x) << S_TF_RECV_SACK)
+
+#define S_TF_TURBO    15
+#define V_TF_TURBO(x) ((x) << S_TF_TURBO)
+
+#define S_TF_KEEPALIVE    16
+#define V_TF_KEEPALIVE(x) ((x) << S_TF_KEEPALIVE)
+
+#define S_TF_TCAM_BYPASS    17
+#define V_TF_TCAM_BYPASS(x) ((x) << S_TF_TCAM_BYPASS)
+
+#define S_TF_CORE_FIN    18
+#define V_TF_CORE_FIN(x) ((x) << S_TF_CORE_FIN)
+
+#define S_TF_CORE_MORE    19
+#define V_TF_CORE_MORE(x) ((x) << S_TF_CORE_MORE)
+
+#define S_TF_MIGRATING    20
+#define V_TF_MIGRATING(x) ((x) << S_TF_MIGRATING)
+
+#define S_TF_ACTIVE_OPEN    21
+#define V_TF_ACTIVE_OPEN(x) ((x) << S_TF_ACTIVE_OPEN)
+
+#define S_TF_ASK_MODE    22
+#define V_TF_ASK_MODE(x) ((x) << S_TF_ASK_MODE)
+
+#define S_TF_NON_OFFLOAD    23
+#define V_TF_NON_OFFLOAD(x) ((x) << S_TF_NON_OFFLOAD)
+
+#define S_TF_MOD_SCHD    24
+#define V_TF_MOD_SCHD(x) ((x) << S_TF_MOD_SCHD)
+
+#define S_TF_MOD_SCHD_REASON0    25
+#define V_TF_MOD_SCHD_REASON0(x) ((x) << S_TF_MOD_SCHD_REASON0)
+
+#define S_TF_MOD_SCHD_REASON1    26
+#define V_TF_MOD_SCHD_REASON1(x) ((x) << S_TF_MOD_SCHD_REASON1)
+
+#define S_TF_MOD_SCHD_RX    27
+#define V_TF_MOD_SCHD_RX(x) ((x) << S_TF_MOD_SCHD_RX)
+
+#define S_TF_CORE_PUSH    28
+#define V_TF_CORE_PUSH(x) ((x) << S_TF_CORE_PUSH)
+
+#define S_TF_RCV_COALESCE_ENABLE    29
+#define V_TF_RCV_COALESCE_ENABLE(x) ((x) << S_TF_RCV_COALESCE_ENABLE)
+
+#define S_TF_RCV_COALESCE_PUSH    30
+#define V_TF_RCV_COALESCE_PUSH(x) ((x) << S_TF_RCV_COALESCE_PUSH)
+
+#define S_TF_RCV_COALESCE_LAST_PSH    31
+#define V_TF_RCV_COALESCE_LAST_PSH(x) ((x) << S_TF_RCV_COALESCE_LAST_PSH)
+
+#define S_TF_RCV_COALESCE_HEARTBEAT    32
+#define V_TF_RCV_COALESCE_HEARTBEAT(x) ((x) << S_TF_RCV_COALESCE_HEARTBEAT)
+
+#define S_TF_LOCK_TID    33
+#define V_TF_LOCK_TID(x) ((x) << S_TF_LOCK_TID)
+
+#define S_TF_DACK_MSS    34
+#define V_TF_DACK_MSS(x) ((x) << S_TF_DACK_MSS)
+
+#define S_TF_CCTRL_SEL0    35
+#define V_TF_CCTRL_SEL0(x) ((x) << S_TF_CCTRL_SEL0)
+
+#define S_TF_CCTRL_SEL1    36
+#define V_TF_CCTRL_SEL1(x) ((x) << S_TF_CCTRL_SEL1)
+
+#define S_TF_TCP_NEWRENO_FAST_RECOVERY    37
+#define V_TF_TCP_NEWRENO_FAST_RECOVERY(x) ((x) << S_TF_TCP_NEWRENO_FAST_RECOVERY)
+
+#define S_TF_TX_PACE_AUTO    38
+#define V_TF_TX_PACE_AUTO(x) ((x) << S_TF_TX_PACE_AUTO)
+
+#define S_TF_PEER_FIN_HELD    39
+#define V_TF_PEER_FIN_HELD(x) ((x) << S_TF_PEER_FIN_HELD)
+
+#define S_TF_CORE_URG    40
+#define V_TF_CORE_URG(x) ((x) << S_TF_CORE_URG)
+
+#define S_TF_RDMA_ERROR    41
+#define V_TF_RDMA_ERROR(x) ((x) << S_TF_RDMA_ERROR)
+
+#define S_TF_SSWS_DISABLED    42
+#define V_TF_SSWS_DISABLED(x) ((x) << S_TF_SSWS_DISABLED)
+
+#define S_TF_DUPACK_COUNT_ODD    43
+#define V_TF_DUPACK_COUNT_ODD(x) ((x) << S_TF_DUPACK_COUNT_ODD)
+
+#define S_TF_TX_CHANNEL    44
+#define V_TF_TX_CHANNEL(x) ((x) << S_TF_TX_CHANNEL)
+
+#define S_TF_RX_CHANNEL    45
+#define V_TF_RX_CHANNEL(x) ((x) << S_TF_RX_CHANNEL)
+
+#define S_TF_TX_PACE_FIXED    46
+#define V_TF_TX_PACE_FIXED(x) ((x) << S_TF_TX_PACE_FIXED)
+
+#define S_TF_RDMA_FLM_ERROR    47
+#define V_TF_RDMA_FLM_ERROR(x) ((x) << S_TF_RDMA_FLM_ERROR)
+
+#define S_TF_RX_FLOW_CONTROL_DISABLE    48
+#define V_TF_RX_FLOW_CONTROL_DISABLE(x) ((x) << S_TF_RX_FLOW_CONTROL_DISABLE)
+
+#define S_TF_DDP_INDICATE_OUT    15
+#define V_TF_DDP_INDICATE_OUT(x) ((x) << S_TF_DDP_INDICATE_OUT)
+
+#define S_TF_DDP_ACTIVE_BUF    16
+#define V_TF_DDP_ACTIVE_BUF(x) ((x) << S_TF_DDP_ACTIVE_BUF)
+
+#define S_TF_DDP_BUF0_VALID    17
+#define V_TF_DDP_BUF0_VALID(x) ((x) << S_TF_DDP_BUF0_VALID)
+
+#define S_TF_DDP_BUF1_VALID    18
+#define V_TF_DDP_BUF1_VALID(x) ((x) << S_TF_DDP_BUF1_VALID)
+
+#define S_TF_DDP_BUF0_INDICATE    19
+#define V_TF_DDP_BUF0_INDICATE(x) ((x) << S_TF_DDP_BUF0_INDICATE)
+
+#define S_TF_DDP_BUF1_INDICATE    20
+#define V_TF_DDP_BUF1_INDICATE(x) ((x) << S_TF_DDP_BUF1_INDICATE)
+
+#define S_TF_DDP_PUSH_DISABLE_0    21
+#define V_TF_DDP_PUSH_DISABLE_0(x) ((x) << S_TF_DDP_PUSH_DISABLE_0)
+
+#define S_TF_DDP_PUSH_DISABLE_1    22
+#define V_TF_DDP_PUSH_DISABLE_1(x) ((x) << S_TF_DDP_PUSH_DISABLE_1)
+
+#define S_TF_DDP_OFF    23
+#define V_TF_DDP_OFF(x) ((x) << S_TF_DDP_OFF)
+
+#define S_TF_DDP_WAIT_FRAG    24
+#define V_TF_DDP_WAIT_FRAG(x) ((x) << S_TF_DDP_WAIT_FRAG)
+
+#define S_TF_DDP_BUF_INF    25
+#define V_TF_DDP_BUF_INF(x) ((x) << S_TF_DDP_BUF_INF)
+
+#define S_TF_DDP_RX2TX    26
+#define V_TF_DDP_RX2TX(x) ((x) << S_TF_DDP_RX2TX)
+
+#define S_TF_DDP_BUF0_FLUSH    27
+#define V_TF_DDP_BUF0_FLUSH(x) ((x) << S_TF_DDP_BUF0_FLUSH)
+
+#define S_TF_DDP_BUF1_FLUSH    28
+#define V_TF_DDP_BUF1_FLUSH(x) ((x) << S_TF_DDP_BUF1_FLUSH)
+
+#define S_TF_DDP_PSH_NO_INVALIDATE0    29
+#define V_TF_DDP_PSH_NO_INVALIDATE0(x) ((x) << S_TF_DDP_PSH_NO_INVALIDATE0)
+
+#define S_TF_DDP_PSH_NO_INVALIDATE1    30
+#define V_TF_DDP_PSH_NO_INVALIDATE1(x) ((x) << S_TF_DDP_PSH_NO_INVALIDATE1)
+
+#endif /* _TCB_DEFS_H */
diff --git a/drivers/net/cxgb3/tn1010.c b/drivers/net/cxgb3/tn1010.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/tn1010.c
@@ -0,0 +1,197 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver.
+ *
+ * Copyright (C) 2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include "common.h"
+
+/* TN1010 PHY specific registers. */
+enum {
+	TN1010_VEND1_STAT = 1,
+};
+
+/* IEEE auto-negotiation 10GBASE-T registers */
+enum {
+	ANEG_ADVER    = 16,
+	ANEG_LPA      = 19,
+	ANEG_10G_CTRL = 32,
+	ANEG_10G_STAT = 33
+};
+
+#define ADVERTISE_ENPAGE      (1 << 12)
+#define ADVERTISE_10000FULL   (1 << 12)
+#define ADVERTISE_LOOP_TIMING (1 << 0)
+
+/* vendor specific status register fields */
+#define F_XS_LANE_ALIGN_STAT (1 << 0)
+#define F_PCS_BLK_LOCK       (1 << 1)
+#define F_PMD_SIGNAL_OK      (1 << 2)
+#define F_LINK_STAT          (1 << 3)
+#define F_ANEG_SPEED_1G      (1 << 4)
+#define F_ANEG_MASTER        (1 << 5)
+
+#define S_ANEG_STAT    6
+#define M_ANEG_STAT    0x3
+#define G_ANEG_STAT(x) (((x) >> S_ANEG_STAT) & M_ANEG_STAT)
+
+enum {                        /* autonegotiation status */
+	ANEG_IN_PROGR = 0,
+	ANEG_COMPLETE = 1,
+	ANEG_FAILED   = 3
+};
+
+/*
+ * Reset the PHY.  May take up to 500ms to complete.
+ */
+static int tn1010_reset(struct cphy *phy, int wait)
+{
+	int err = t3_phy_reset(phy, MDIO_DEV_PMA_PMD, wait);
+	msleep(500);
+	return err;
+}
+
+static int tn1010_power_down(struct cphy *phy, int enable)
+{
+	return t3_mdio_change_bits(phy, MDIO_DEV_PMA_PMD, MII_BMCR,
+				   BMCR_PDOWN, enable ? BMCR_PDOWN : 0);
+}
+
+static int tn1010_autoneg_enable(struct cphy *phy)
+{
+	int err;
+
+	err = tn1010_power_down(phy, 0);
+	if (!err)
+		err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, MII_BMCR, 0,
+					  BMCR_ANENABLE | BMCR_ANRESTART);
+	return err;
+}
+
+static int tn1010_autoneg_restart(struct cphy *phy)
+{
+	int err;
+
+	err = tn1010_power_down(phy, 0);
+	if (!err)
+		err = t3_mdio_change_bits(phy, MDIO_DEV_ANEG, MII_BMCR, 0,
+					  BMCR_ANRESTART);
+	return err;
+}
+
+static int tn1010_advertise(struct cphy *phy, unsigned int advert)
+{
+	int err, val;
+
+	if (!(advert & ADVERTISED_1000baseT_Full))
+		return -EINVAL;               /* PHY can't disable 1000BASE-T */
+
+	val = ADVERTISE_CSMA | ADVERTISE_ENPAGE | ADVERTISE_NPAGE;
+	if (advert & ADVERTISED_Pause)
+		val |= ADVERTISE_PAUSE_CAP;
+	if (advert & ADVERTISED_Asym_Pause)
+		val |= ADVERTISE_PAUSE_ASYM;
+	err = mdio_write(phy, MDIO_DEV_ANEG, ANEG_ADVER, val);
+	if (err)
+		return err;
+
+	val = (advert & ADVERTISED_10000baseT_Full) ? ADVERTISE_10000FULL : 0;
+	return mdio_write(phy, MDIO_DEV_ANEG, ANEG_10G_CTRL, val |
+			  ADVERTISE_LOOP_TIMING);
+}
+
+static int tn1010_get_link_status(struct cphy *phy, int *link_ok,
+				  int *speed, int *duplex, int *fc)
+{
+	unsigned int status, lpa, adv;
+	int err, sp = -1, pause = 0;
+
+	err = mdio_read(phy, MDIO_DEV_VEND1, TN1010_VEND1_STAT, &status);
+	if (err)
+		return err;
+
+	if (link_ok)
+		*link_ok = (status & F_LINK_STAT) != 0;
+
+	if (G_ANEG_STAT(status) == ANEG_COMPLETE) {
+		sp = (status & F_ANEG_SPEED_1G) ? SPEED_1000 : SPEED_10000;
+
+		if (fc) {
+			err = mdio_read(phy, MDIO_DEV_ANEG, ANEG_LPA, &lpa);
+			if (!err)
+				err = mdio_read(phy, MDIO_DEV_ANEG, ANEG_ADVER,
+						&adv);
+			if (err)
+				return err;
+
+			if (lpa & adv & ADVERTISE_PAUSE_CAP)
+				pause = PAUSE_RX | PAUSE_TX;
+			else if ((lpa & ADVERTISE_PAUSE_CAP) &&
+				 (lpa & ADVERTISE_PAUSE_ASYM) &&
+				 (adv & ADVERTISE_PAUSE_ASYM))
+				pause = PAUSE_TX;
+			else if ((lpa & ADVERTISE_PAUSE_ASYM) &&
+				 (adv & ADVERTISE_PAUSE_CAP))
+				pause = PAUSE_RX;
+		}
+	}
+	if (speed)
+		*speed = sp;
+	if (duplex)
+		*duplex = DUPLEX_FULL;
+	if (fc)
+		*fc = pause;
+	return 0;
+}
+
+int tn1010_set_speed_duplex(struct cphy *phy, int speed, int duplex)
+{
+	return -EINVAL;    /* require autoneg */
+}
+
+#ifdef C99_NOT_SUPPORTED
+static struct cphy_ops tn1010_ops = {
+	tn1010_reset,
+	t3_phy_lasi_intr_enable,
+	t3_phy_lasi_intr_disable,
+	t3_phy_lasi_intr_clear,
+	t3_phy_lasi_intr_handler,
+	tn1010_autoneg_enable,
+	tn1010_autoneg_restart,
+	tn1010_advertise,
+	NULL,
+	tn1010_set_speed_duplex,
+	tn1010_get_link_status,
+	tn1010_power_down,
+};
+#else
+static struct cphy_ops tn1010_ops = {
+	.reset             = tn1010_reset,
+	.intr_enable       = t3_phy_lasi_intr_enable,
+	.intr_disable      = t3_phy_lasi_intr_disable,
+	.intr_clear        = t3_phy_lasi_intr_clear,
+	.intr_handler      = t3_phy_lasi_intr_handler,
+	.autoneg_enable    = tn1010_autoneg_enable,
+	.autoneg_restart   = tn1010_autoneg_restart,
+	.advertise         = tn1010_advertise,
+	.set_speed_duplex  = tn1010_set_speed_duplex,
+	.get_link_status   = tn1010_get_link_status,
+	.power_down        = tn1010_power_down,
+};
+#endif
+
+int t3_tn1010_phy_prep(pinfo_t *pinfo, int phy_addr,
+		       const struct mdio_ops *mdio_ops)
+{
+	cphy_init(&pinfo->phy, pinfo->adapter, pinfo, phy_addr, &tn1010_ops, mdio_ops,
+		  SUPPORTED_1000baseT_Full | SUPPORTED_10000baseT_Full |
+		  SUPPORTED_Autoneg | SUPPORTED_AUI | SUPPORTED_TP,
+		  "1000/10GBASE-T");
+	msleep(500);    /* PHY needs up to 500ms to start responding to MDIO */
+	return 0;
+}
diff --git a/drivers/net/cxgb3/trace.c b/drivers/net/cxgb3/trace.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/trace.c
@@ -0,0 +1,161 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/*
+ *      Routines to allocate and free T3 trace buffers.
+ *
+ *      Authors:
+ *              Felix Marti <felix@chelsio.com>
+ *
+ *      The code suffers from a trace buffer count increment race, which might
+ *      lead to entries being overwritten. I don't really care about this,
+ *      because the trace buffer is a simple debug/perfomance tuning aid.
+ *
+ *      Trace buffers are created in /proc, which needs to be fixed.
+ */
+
+#include "trace.h"
+
+#ifdef T3_TRACE
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include "cxgb3_compat.h"
+
+/*
+ * SEQ OPS
+ */
+static void *t3_trace_seq_start(struct seq_file *seq, loff_t *pos)
+{
+        struct trace_buf *tb = seq->private;
+        struct trace_entry *e = NULL;
+        unsigned int start, count;
+
+        if (tb->idx > tb->capacity) {
+                start = tb->idx & (tb->capacity - 1);
+                count = tb->capacity;
+        } else {
+                start = 0;
+                count = tb->idx;
+        }
+
+        if (*pos < count)
+                e = &tb->ep[(start + *pos) & (tb->capacity - 1)];
+
+        return e;
+}
+
+static void *t3_trace_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+        struct trace_buf *tb = seq->private;
+        struct trace_entry *e = v;
+        unsigned int count = min(tb->idx, tb->capacity);
+
+        if (++*pos < count) {
+                e++;
+                if (e >= &tb->ep[tb->capacity])
+                        e = tb->ep;
+        } else
+                e = NULL;
+
+        return e;
+}
+
+static void t3_trace_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static int t3_trace_seq_show(struct seq_file *seq, void *v)
+{
+        struct trace_entry *ep = v;
+
+        seq_printf(seq, "%016llx ", (unsigned long long) ep->tsc);
+        seq_printf(seq, ep->fmt, ep->param[0], ep->param[1], ep->param[2],
+                ep->param[3], ep->param[4], ep->param[5]);
+        seq_printf(seq, "\n");
+
+        return 0;
+}
+
+static struct seq_operations t3_trace_seq_ops = {
+	.start = t3_trace_seq_start,
+	.next  = t3_trace_seq_next,
+	.stop  = t3_trace_seq_stop,
+	.show  = t3_trace_seq_show
+};
+
+/*
+ * FILE OPS
+ */
+static int t3_trace_seq_open(struct inode *inode, struct file *file)
+{
+        int rc = seq_open(file, &t3_trace_seq_ops);
+
+        if (!rc) {
+                struct seq_file *seq = file->private_data;
+
+                seq->private = inode->i_private;
+        }
+
+        return rc;
+}
+
+static struct file_operations t3_trace_seq_fops = {
+	.owner   = THIS_MODULE,
+	.open    = t3_trace_seq_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = seq_release
+};
+
+/*
+ * TRACEBUFFER API
+ */
+struct trace_buf *t3_trace_alloc(struct dentry *root, const char *name,
+			      unsigned int capacity)
+{
+	struct trace_buf *tb;
+	unsigned int size;
+
+	if (!name || !capacity)
+		return NULL;
+	if (capacity & (capacity - 1))     /* require power of 2 */
+		return NULL;
+
+	size = sizeof(*tb) + sizeof(struct trace_entry) * capacity;
+	tb = kmalloc(size, GFP_KERNEL);
+	if (!tb)
+		return NULL;
+
+	memset(tb, 0, size);
+	tb->capacity = capacity;
+	tb->debugfs_dentry = debugfs_create_file(name, S_IFREG | S_IRUGO, root,
+						 tb, &t3_trace_seq_fops);
+	if (!tb->debugfs_dentry) {
+		kfree(tb);
+		return NULL;
+	}
+
+        return tb;
+}
+
+void t3_trace_free(struct trace_buf *tb)
+{
+        if (tb) {
+		if (tb->debugfs_dentry)
+			debugfs_remove(tb->debugfs_dentry);
+                kfree(tb);
+        }
+}
+EXPORT_SYMBOL(t3_trace_alloc);
+EXPORT_SYMBOL(t3_trace_free);
+#endif /* T3_TRACE */
diff --git a/drivers/net/cxgb3/trace.h b/drivers/net/cxgb3/trace.h
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/trace.h
@@ -0,0 +1,122 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver for Linux.
+ *
+ * Copyright (C) 2003-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+/*
+ *      Definitions and inline functions for the T3 trace buffers.
+ *
+ *      Authors:
+ *              Felix Marti <felix@chelsio.com>
+ */
+
+#ifndef __T3_TRACE_H__
+#define __T3_TRACE_H__
+
+#if defined(T3_TRACE) || defined(T3_TRACE_TOM)
+
+#include <linux/time.h>
+#include <linux/timex.h>
+
+#define T3_TRACE_NUM_PARAM 6
+
+typedef unsigned long tracearg_t;
+
+#define T3_TRACE0(b, s) \
+        if ((b) != NULL) \
+                (void) t3_trace((b), (s));
+#define	T3_TRACE1(b, s, p0) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+        }
+#define T3_TRACE2(b, s, p0, p1) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+                _p[1] = (tracearg_t) (p1); \
+        }
+#define T3_TRACE3(b, s, p0, p1, p2) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+                _p[1] = (tracearg_t) (p1); \
+                _p[2] = (tracearg_t) (p2); \
+        }
+#define T3_TRACE4(b, s, p0, p1, p2, p3) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+                _p[1] = (tracearg_t) (p1); \
+                _p[2] = (tracearg_t) (p2); \
+                _p[3] = (tracearg_t) (p3); \
+        }
+#define T3_TRACE5(b, s, p0, p1, p2, p3, p4) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+                _p[1] = (tracearg_t) (p1); \
+                _p[2] = (tracearg_t) (p2); \
+                _p[3] = (tracearg_t) (p3); \
+                _p[4] = (tracearg_t) (p4); \
+        }
+#define T3_TRACE6(b, s, p0, p1, p2, p3, p4, p5) \
+        if ((b) != NULL) { \
+                tracearg_t *_p = t3_trace((b), (s)); \
+                _p[0] = (tracearg_t) (p0); \
+                _p[1] = (tracearg_t) (p1); \
+                _p[2] = (tracearg_t) (p2); \
+                _p[3] = (tracearg_t) (p3); \
+                _p[4] = (tracearg_t) (p4); \
+                _p[5] = (tracearg_t) (p5); \
+        }
+
+struct trace_entry {
+        cycles_t   tsc;
+        char      *fmt;
+        tracearg_t param[T3_TRACE_NUM_PARAM];
+};
+
+struct dentry;
+
+struct trace_buf {
+	unsigned int capacity;          /* size of ring buffer */
+	unsigned int idx;               /* index of next entry to write */
+	struct dentry *debugfs_dentry;
+	struct trace_entry ep[0];       /* the ring buffer */
+};
+
+static inline unsigned long *t3_trace(struct trace_buf *tb, char *fmt)
+{
+        struct trace_entry *ep = &tb->ep[tb->idx++ & (tb->capacity - 1)];
+
+        ep->fmt = fmt;
+        ep->tsc = get_cycles();
+
+        return (unsigned long *) &ep->param[0];
+}
+
+struct trace_buf *t3_trace_alloc(struct dentry *root, const char *name,
+			      unsigned int capacity);
+void t3_trace_free(struct trace_buf *tb);
+
+#else
+#define T3_TRACE0(b, s)
+#define T3_TRACE1(b, s, p0)
+#define T3_TRACE2(b, s, p0, p1)
+#define T3_TRACE3(b, s, p0, p1, p2)
+#define T3_TRACE4(b, s, p0, p1, p2, p3)
+#define T3_TRACE5(b, s, p0, p1, p2, p3, p4)
+#define T3_TRACE6(b, s, p0, p1, p2, p3, p4, p5)
+
+#define t3_trace_alloc(root, name, capacity) NULL
+#define t3_trace_free(tb)
+#endif
+
+#endif /* __T3_TRACE_H__ */
diff --git a/drivers/net/cxgb3/version.h b/drivers/net/cxgb3/version.h
--- a/drivers/net/cxgb3/version.h
+++ b/drivers/net/cxgb3/version.h
@@ -1,44 +1,26 @@
-/*
- * Copyright (c) 2003-2008 Chelsio, Inc. All rights reserved.
- *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
- *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
- */
-/* $Date: 2006/10/31 18:57:51 $ $RCSfile: version.h,v $ $Revision: 1.3 $ */
+/*****************************************************************************
+ *                                                                           *
+ * File:                                                                     *
+ *  version.h                                                                *
+ *                                                                           *
+ * Description:                                                              *
+ *  Chelsio driver version defines.                                          *
+ *                                                                           *
+ * Copyright (c) 2003 - 2009 Chelsio Communications, Inc.                    *
+ * All rights reserved.                                                      *
+ *                                                                           *
+ * Maintainers: maintainers@chelsio.com                                      *
+ *                                                                           *
+ * http://www.chelsio.com                                                    *
+ *                                                                           *
+ ****************************************************************************/
+/* $Date: 2008/08/07 02:39:03 $ $RCSfile: version.h,v $ $Revision: 1.5.4.1 $ */
 #ifndef __CHELSIO_VERSION_H
 #define __CHELSIO_VERSION_H
-#define DRV_DESC "Chelsio T3 Network Driver"
-#define DRV_NAME "cxgb3"
-/* Driver version */
-#define DRV_VERSION "1.1.3-ko"
-
-/* Firmware version */
-#define FW_VERSION_MAJOR 7
-#define FW_VERSION_MINOR 4
-#define FW_VERSION_MICRO 0
-#endif				/* __CHELSIO_VERSION_H */
+#define DRIVER_DESC "Chelsio T3 Network Driver"
+#define DRIVER_NAME "cxgb3"
+// Driver version
+#ifndef DRIVER_VERSION
+#define DRIVER_VERSION "1.6.0.3-xen"
+#endif
+#endif //__CHELSIO_VERSION_H
diff --git a/drivers/net/cxgb3/vsc7323.c b/drivers/net/cxgb3/vsc7323.c
new file mode 100644
--- /dev/null
+++ b/drivers/net/cxgb3/vsc7323.c
@@ -0,0 +1,339 @@
+/*
+ * This file is part of the Chelsio T3 Ethernet driver.
+ *
+ * Copyright (C) 2007-2009 Chelsio Communications.  All rights reserved.
+ *
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
+ */
+
+#include "common.h"
+
+enum {
+	ELMR_ADDR    = 0,
+	ELMR_STAT    = 1,
+	ELMR_DATA_LO = 2,
+	ELMR_DATA_HI = 3,
+
+	ELMR_THRES0  = 0xe000,
+	ELMR_BW      = 0xe00c,
+	ELMR_FIFO_SZ = 0xe00d,
+	ELMR_STATS   = 0xf000,
+
+	ELMR_MDIO_ADDR = 10
+};
+
+#define VSC_REG(block, subblock, reg) \
+	((reg) | ((subblock) << 8) | ((block) << 12))
+
+int t3_elmr_blk_write(adapter_t *adap, int start, const u32 *vals, int n)
+{
+	int ret;
+	const struct mdio_ops *mo = adapter_info(adap)->mdio_ops;
+
+	ELMR_LOCK(adap);
+	ret = mo->write(adap, ELMR_MDIO_ADDR, 0, ELMR_ADDR, start);
+	for ( ; !ret && n; n--, vals++) {
+		ret = mo->write(adap, ELMR_MDIO_ADDR, 0, ELMR_DATA_LO,
+				*vals & 0xffff);
+		if (!ret)
+			ret = mo->write(adap, ELMR_MDIO_ADDR, 0, ELMR_DATA_HI,
+					*vals >> 16);
+	}
+	ELMR_UNLOCK(adap);
+	return ret;
+}
+
+static int elmr_write(adapter_t *adap, int addr, u32 val)
+{
+	return t3_elmr_blk_write(adap, addr, &val, 1);
+}
+
+int t3_elmr_blk_read(adapter_t *adap, int start, u32 *vals, int n)
+{
+	int i, ret;
+	unsigned int v;
+	const struct mdio_ops *mo = adapter_info(adap)->mdio_ops;
+
+	ELMR_LOCK(adap);
+
+	ret = mo->write(adap, ELMR_MDIO_ADDR, 0, ELMR_ADDR, start);
+	if (ret)
+		goto out;
+
+	for (i = 0; i < 5; i++) {
+		ret = mo->read(adap, ELMR_MDIO_ADDR, 0, ELMR_STAT, &v);
+		if (ret)
+			goto out;
+		if (v == 1)
+			break;
+		udelay(5);
+	}
+	if (v != 1) {
+		ret = -ETIME;
+		goto out;
+	}
+
+	for ( ; !ret && n; n--, vals++) {
+		ret = mo->read(adap, ELMR_MDIO_ADDR, 0, ELMR_DATA_LO, vals);
+		if (!ret) {
+			ret = mo->read(adap, ELMR_MDIO_ADDR, 0, ELMR_DATA_HI,
+				       &v);
+			*vals |= v << 16;
+		}
+	}
+out:	ELMR_UNLOCK(adap);
+	return ret;
+}
+
+int t3_vsc7323_init(adapter_t *adap, int nports)
+{
+	static struct addr_val_pair sys_avp[] = {
+		{ VSC_REG(7, 15, 0xf),  2 },
+		{ VSC_REG(7, 15, 0x19), 0xd6 },
+		{ VSC_REG(7, 15, 7),    0xc },
+		{ VSC_REG(7, 1, 0),     0x220 },
+	};
+	static struct addr_val_pair fifo_avp[] = {
+		{ VSC_REG(2, 0, 0x2f), 0 },
+		{ VSC_REG(2, 0, 0xf),  0xa0010291 },
+		{ VSC_REG(2, 1, 0x2f), 1 },
+		{ VSC_REG(2, 1, 0xf),  0xa026301 }
+	};
+	static struct addr_val_pair xg_avp[] = {
+		{ VSC_REG(1, 10, 0),    0x600b },
+		{ VSC_REG(1, 10, 1),    0x70600 }, //QUANTA = 96*1024*8/512
+		{ VSC_REG(1, 10, 2),    0x2710 },
+		{ VSC_REG(1, 10, 5),    0x65 },
+		{ VSC_REG(1, 10, 7),    0x23 },
+		{ VSC_REG(1, 10, 0x23), 0x800007bf },
+		{ VSC_REG(1, 10, 0x23), 0x000007bf },
+		{ VSC_REG(1, 10, 0x23), 0x800007bf },
+		{ VSC_REG(1, 10, 0x24), 4 }
+	};
+
+	int i, ret, ing_step, egr_step, ing_bot, egr_bot;
+
+	for (i = 0; i < ARRAY_SIZE(sys_avp); i++)
+		if ((ret = t3_elmr_blk_write(adap, sys_avp[i].reg_addr,
+					     &sys_avp[i].val, 1)))
+			return ret;
+
+	ing_step = 0xc0 / nports;
+	egr_step = 0x40 / nports;
+	ing_bot = egr_bot = 0;
+//	ing_wm = ing_step * 64;
+//	egr_wm = egr_step * 64;
+
+	/* {ING,EGR}_CONTROL.CLR = 1 here */
+	for (i = 0; i < nports; i++) {
+		if (
+		    (ret = elmr_write(adap, VSC_REG(2, 0, 0x10 + i),
+				((ing_bot + ing_step) << 16) | ing_bot)) ||
+		    (ret = elmr_write(adap, VSC_REG(2, 0, 0x40 + i),
+				0x6000bc0)) ||
+		    (ret = elmr_write(adap, VSC_REG(2, 0, 0x50 + i), 1)) ||
+		    (ret = elmr_write(adap, VSC_REG(2, 1, 0x10 + i),
+				((egr_bot + egr_step) << 16) | egr_bot)) ||
+		    (ret = elmr_write(adap, VSC_REG(2, 1, 0x40 + i),
+				0x2000280)) ||
+		    (ret = elmr_write(adap, VSC_REG(2, 1, 0x50 + i), 0)))
+			return ret;
+		ing_bot += ing_step;
+		egr_bot += egr_step;
+	}
+
+	for (i = 0; i < ARRAY_SIZE(fifo_avp); i++)
+		if ((ret = t3_elmr_blk_write(adap, fifo_avp[i].reg_addr,
+					     &fifo_avp[i].val, 1)))
+			return ret;
+
+	for (i = 0; i < ARRAY_SIZE(xg_avp); i++)
+		if ((ret = t3_elmr_blk_write(adap, xg_avp[i].reg_addr,
+					     &xg_avp[i].val, 1)))
+			return ret;
+
+	for (i = 0; i < nports; i++)
+		if ((ret = elmr_write(adap, VSC_REG(1, i, 0), 0xa59c)) ||
+		    (ret = elmr_write(adap, VSC_REG(1, i, 5),
+				 (i << 12) | 0x63)) ||
+		    (ret = elmr_write(adap, VSC_REG(1, i, 0xb), 0x96)) ||
+		    (ret = elmr_write(adap, VSC_REG(1, i, 0x15), 0x21)) ||
+		    (ret = elmr_write(adap, ELMR_THRES0 + i, 768)))
+			return ret;
+
+	if ((ret = elmr_write(adap, ELMR_BW, 7)))
+		return ret;
+
+	return ret;
+}
+
+int t3_vsc7323_set_speed_fc(adapter_t *adap, int speed, int fc, int port)
+{
+	int mode, clk, r;
+
+	if (speed >= 0) {
+		if (speed == SPEED_10)
+			mode = clk = 1;
+		else if (speed == SPEED_100)
+			mode = 1, clk = 2;
+		else if (speed == SPEED_1000)
+			mode = clk = 3;
+		else
+			return -EINVAL;
+
+		if ((r = elmr_write(adap, VSC_REG(1, port, 0),
+				    0xa590 | (mode << 2))) ||
+		    (r = elmr_write(adap, VSC_REG(1, port, 0xb),
+				    0x91 | (clk << 1))) ||
+		    (r = elmr_write(adap, VSC_REG(1, port, 0xb),
+				    0x90 | (clk << 1))) ||
+		    (r = elmr_write(adap, VSC_REG(1, port, 0),
+				    0xa593 | (mode << 2))))
+			return r;
+	}
+
+	r = (fc & PAUSE_RX) ? 0x60200 : 0x20200; //QUANTA = 32*1024*8/512
+	if (fc & PAUSE_TX)
+		r |= (1 << 19);
+	return elmr_write(adap, VSC_REG(1, port, 1), r);
+}
+
+int t3_vsc7323_set_mtu(adapter_t *adap, unsigned int mtu, int port)
+{
+	return elmr_write(adap, VSC_REG(1, port, 2), mtu);
+}
+
+int t3_vsc7323_set_addr(adapter_t *adap, u8 addr[6], int port)
+{
+	int ret;
+
+	ret = elmr_write(adap, VSC_REG(1, port, 3),
+			 (addr[0] << 16) | (addr[1] << 8) | addr[2]);
+	if (!ret)
+		ret = elmr_write(adap, VSC_REG(1, port, 4),
+				 (addr[3] << 16) | (addr[4] << 8) | addr[5]);
+	return ret;
+}
+
+int t3_vsc7323_enable(adapter_t *adap, int port, int which)
+{
+	int ret;
+	unsigned int v, orig;
+
+	ret = t3_elmr_blk_read(adap, VSC_REG(1, port, 0), &v, 1);
+	if (!ret) {
+		orig = v;
+		if (which & MAC_DIRECTION_TX)
+			v |= 1;
+		if (which & MAC_DIRECTION_RX)
+			v |= 2;
+		if (v != orig)
+			ret = elmr_write(adap, VSC_REG(1, port, 0), v);
+	}
+	return ret;
+}
+
+int t3_vsc7323_disable(adapter_t *adap, int port, int which)
+{
+	int ret;
+	unsigned int v, orig;
+
+	ret = t3_elmr_blk_read(adap, VSC_REG(1, port, 0), &v, 1);
+	if (!ret) {
+		orig = v;
+		if (which & MAC_DIRECTION_TX)
+			v &= ~1;
+		if (which & MAC_DIRECTION_RX)
+			v &= ~2;
+		if (v != orig)
+			ret = elmr_write(adap, VSC_REG(1, port, 0), v);
+	}
+	return ret;
+}
+
+#define STATS0_START 1
+#define STATS1_START 0x24
+#define NSTATS0 (0x1d - STATS0_START + 1)
+#define NSTATS1 (0x2a - STATS1_START + 1)
+
+#define ELMR_STAT(port, reg) (ELMR_STATS + port * 0x40 + reg)
+
+const struct mac_stats *t3_vsc7323_update_stats(struct cmac *mac)
+{
+	int ret;
+	u64 rx_ucast, tx_ucast;
+	u32 stats0[NSTATS0], stats1[NSTATS1];
+
+	ret = t3_elmr_blk_read(mac->adapter,
+			       ELMR_STAT(mac->ext_port, STATS0_START),
+			       stats0, NSTATS0);
+	if (!ret)
+		ret = t3_elmr_blk_read(mac->adapter,
+				       ELMR_STAT(mac->ext_port, STATS1_START),
+				       stats1, NSTATS1);
+	if (ret)
+		goto out;
+
+	/*
+	 * HW counts Rx/Tx unicast frames but we want all the frames.
+	 */
+	rx_ucast = mac->stats.rx_frames - mac->stats.rx_mcast_frames -
+		   mac->stats.rx_bcast_frames;
+	rx_ucast += (u64)(stats0[6 - STATS0_START] - (u32)rx_ucast);
+	tx_ucast = mac->stats.tx_frames - mac->stats.tx_mcast_frames -
+		   mac->stats.tx_bcast_frames;
+	tx_ucast += (u64)(stats0[27 - STATS0_START] - (u32)tx_ucast);
+
+#define RMON_UPDATE(mac, name, hw_stat) \
+	mac->stats.name += (u64)((hw_stat) - (u32)(mac->stats.name))
+
+	RMON_UPDATE(mac, rx_octets, stats0[4 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames, stats0[6 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames, stats0[7 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames, stats0[8 - STATS0_START]);
+	RMON_UPDATE(mac, rx_mcast_frames, stats0[7 - STATS0_START]);
+	RMON_UPDATE(mac, rx_bcast_frames, stats0[8 - STATS0_START]);
+	RMON_UPDATE(mac, rx_fcs_errs, stats0[9 - STATS0_START]);
+	RMON_UPDATE(mac, rx_pause, stats0[2 - STATS0_START]);
+	RMON_UPDATE(mac, rx_jabber, stats0[16 - STATS0_START]);
+	RMON_UPDATE(mac, rx_short, stats0[11 - STATS0_START]);
+	RMON_UPDATE(mac, rx_symbol_errs, stats0[1 - STATS0_START]);
+	RMON_UPDATE(mac, rx_too_long, stats0[15 - STATS0_START]);
+
+	RMON_UPDATE(mac, rx_frames_64,        stats0[17 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_65_127,    stats0[18 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_128_255,   stats0[19 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_256_511,   stats0[20 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_512_1023,  stats0[21 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_1024_1518, stats0[22 - STATS0_START]);
+	RMON_UPDATE(mac, rx_frames_1519_max,  stats0[23 - STATS0_START]);
+
+	RMON_UPDATE(mac, tx_octets, stats0[26 - STATS0_START]);
+	RMON_UPDATE(mac, tx_frames, stats0[27 - STATS0_START]);
+	RMON_UPDATE(mac, tx_frames, stats0[28 - STATS0_START]);
+	RMON_UPDATE(mac, tx_frames, stats0[29 - STATS0_START]);
+	RMON_UPDATE(mac, tx_mcast_frames, stats0[28 - STATS0_START]);
+	RMON_UPDATE(mac, tx_bcast_frames, stats0[29 - STATS0_START]);
+	RMON_UPDATE(mac, tx_pause, stats0[25 - STATS0_START]);
+
+	RMON_UPDATE(mac, tx_underrun, 0);
+
+	RMON_UPDATE(mac, tx_frames_64,        stats1[36 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_65_127,    stats1[37 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_128_255,   stats1[38 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_256_511,   stats1[39 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_512_1023,  stats1[40 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_1024_1518, stats1[41 - STATS1_START]);
+	RMON_UPDATE(mac, tx_frames_1519_max,  stats1[42 - STATS1_START]);
+
+#undef RMON_UPDATE
+
+	mac->stats.rx_frames = rx_ucast + mac->stats.rx_mcast_frames +
+			       mac->stats.rx_bcast_frames;
+	mac->stats.tx_frames = tx_ucast + mac->stats.tx_mcast_frames +
+			       mac->stats.tx_bcast_frames;
+out:    return &mac->stats;
+}
diff --git a/drivers/net/cxgb3/vsc8211.c b/drivers/net/cxgb3/vsc8211.c
--- a/drivers/net/cxgb3/vsc8211.c
+++ b/drivers/net/cxgb3/vsc8211.c
@@ -1,63 +1,45 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include "common.h"
 
 /* VSC8211 PHY specific registers. */
 enum {
-	VSC8211_SIGDET_CTRL = 19,
-	VSC8211_EXT_CTRL = 23,
-	VSC8211_INTR_ENABLE = 25,
-	VSC8211_INTR_STATUS = 26,
-	VSC8211_LED_CTRL = 27,
+	VSC8211_SIGDET_CTRL   = 19,
+	VSC8211_EXT_CTRL_3    = 20,
+	VSC8211_EXT_CTRL      = 23,
+	VSC8211_PHY_CTRL      = 24,
+	VSC8211_INTR_ENABLE   = 25,
+	VSC8211_INTR_STATUS   = 26,
+	VSC8211_LED_CTRL      = 27,
 	VSC8211_AUX_CTRL_STAT = 28,
-	VSC8211_EXT_PAGE_AXS = 31,
+	VSC8211_EXT_PAGE_AXS  = 31,
 };
 
 enum {
-	VSC_INTR_RX_ERR = 1 << 0,
-	VSC_INTR_MS_ERR = 1 << 1,  /* master/slave resolution error */
-	VSC_INTR_CABLE = 1 << 2,  /* cable impairment */
+	VSC_INTR_RX_ERR     = 1 << 0,
+	VSC_INTR_MS_ERR     = 1 << 1,  /* master/slave resolution error */
+	VSC_INTR_CABLE      = 1 << 2,  /* cable impairment */
 	VSC_INTR_FALSE_CARR = 1 << 3,  /* false carrier */
-	VSC_INTR_MEDIA_CHG = 1 << 4,  /* AMS media change */
-	VSC_INTR_RX_FIFO = 1 << 5,  /* Rx FIFO over/underflow */
-	VSC_INTR_TX_FIFO = 1 << 6,  /* Tx FIFO over/underflow */
-	VSC_INTR_DESCRAMBL = 1 << 7,  /* descrambler lock-lost */
+	VSC_INTR_MEDIA_CHG  = 1 << 4,  /* AMS media change */
+	VSC_INTR_RX_FIFO    = 1 << 5,  /* Rx FIFO over/underflow */
+	VSC_INTR_TX_FIFO    = 1 << 6,  /* Tx FIFO over/underflow */
+	VSC_INTR_DESCRAMBL  = 1 << 7,  /* descrambler lock-lost */
 	VSC_INTR_SYMBOL_ERR = 1 << 8,  /* symbol error */
-	VSC_INTR_NEG_DONE = 1 << 10, /* autoneg done */
-	VSC_INTR_NEG_ERR = 1 << 11, /* autoneg error */
-	VSC_INTR_DPLX_CHG = 1 << 12, /* duplex change */
-	VSC_INTR_LINK_CHG = 1 << 13, /* link change */
-	VSC_INTR_SPD_CHG = 1 << 14, /* speed change */
-	VSC_INTR_ENABLE = 1 << 15, /* interrupt enable */
+	VSC_INTR_NEG_DONE   = 1 << 10, /* autoneg done */
+	VSC_INTR_NEG_ERR    = 1 << 11, /* autoneg error */
+	VSC_INTR_DPLX_CHG   = 1 << 12, /* duplex change */
+	VSC_INTR_LINK_CHG   = 1 << 13, /* link change */
+	VSC_INTR_SPD_CHG    = 1 << 14, /* speed change */
+	VSC_INTR_ENABLE     = 1 << 15, /* interrupt enable */
 };
 
 enum {
@@ -91,18 +73,17 @@
  */
 static int vsc8211_reset(struct cphy *cphy, int wait)
 {
-	return t3_phy_reset(cphy, MDIO_DEVAD_NONE, 0);
+	return t3_phy_reset(cphy, 0, 0);
 }
 
 static int vsc8211_intr_enable(struct cphy *cphy)
 {
-	return t3_mdio_write(cphy, MDIO_DEVAD_NONE, VSC8211_INTR_ENABLE,
-			     INTR_MASK);
+	return mdio_write(cphy, 0, VSC8211_INTR_ENABLE, INTR_MASK);
 }
 
 static int vsc8211_intr_disable(struct cphy *cphy)
 {
-	return t3_mdio_write(cphy, MDIO_DEVAD_NONE, VSC8211_INTR_ENABLE, 0);
+	return mdio_write(cphy, 0, VSC8211_INTR_ENABLE, 0);
 }
 
 static int vsc8211_intr_clear(struct cphy *cphy)
@@ -110,32 +91,30 @@
 	u32 val;
 
 	/* Clear PHY interrupts by reading the register. */
-	return t3_mdio_read(cphy, MDIO_DEVAD_NONE, VSC8211_INTR_STATUS, &val);
+	return mdio_read(cphy, 0, VSC8211_INTR_STATUS, &val);
 }
 
 static int vsc8211_autoneg_enable(struct cphy *cphy)
 {
-	return t3_mdio_change_bits(cphy, MDIO_DEVAD_NONE, MII_BMCR,
-				   BMCR_PDOWN | BMCR_ISOLATE,
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_PDOWN | BMCR_ISOLATE,
 				   BMCR_ANENABLE | BMCR_ANRESTART);
 }
 
 static int vsc8211_autoneg_restart(struct cphy *cphy)
 {
-	return t3_mdio_change_bits(cphy, MDIO_DEVAD_NONE, MII_BMCR,
-				   BMCR_PDOWN | BMCR_ISOLATE,
+	return t3_mdio_change_bits(cphy, 0, MII_BMCR, BMCR_PDOWN | BMCR_ISOLATE,
 				   BMCR_ANRESTART);
 }
 
 static int vsc8211_get_link_status(struct cphy *cphy, int *link_ok,
-				   int *speed, int *duplex, int *fc)
+				     int *speed, int *duplex, int *fc)
 {
 	unsigned int bmcr, status, lpa, adv;
 	int err, sp = -1, dplx = -1, pause = 0;
 
-	err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMCR, &bmcr);
+	err = mdio_read(cphy, 0, MII_BMCR, &bmcr);
 	if (!err)
-		err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMSR, &status);
+		err = mdio_read(cphy, 0, MII_BMSR, &status);
 	if (err)
 		return err;
 
@@ -145,8 +124,7 @@
 		 * once more to get the current link state.
 		 */
 		if (!(status & BMSR_LSTATUS))
-			err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMSR,
-					   &status);
+			err = mdio_read(cphy, 0, MII_BMSR, &status);
 		if (err)
 			return err;
 		*link_ok = (status & BMSR_LSTATUS) != 0;
@@ -160,8 +138,7 @@
 		else
 			sp = SPEED_10;
 	} else if (status & BMSR_ANEGCOMPLETE) {
-		err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, VSC8211_AUX_CTRL_STAT,
-				   &status);
+		err = mdio_read(cphy, 0, VSC8211_AUX_CTRL_STAT, &status);
 		if (err)
 			return err;
 
@@ -175,11 +152,9 @@
 			sp = SPEED_1000;
 
 		if (fc && dplx == DUPLEX_FULL) {
-			err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_LPA,
-					   &lpa);
+			err = mdio_read(cphy, 0, MII_LPA, &lpa);
 			if (!err)
-				err = t3_mdio_read(cphy, MDIO_DEVAD_NONE,
-						   MII_ADVERTISE, &adv);
+				err = mdio_read(cphy, 0, MII_ADVERTISE, &adv);
 			if (err)
 				return err;
 
@@ -209,9 +184,9 @@
 	unsigned int bmcr, status, lpa, adv;
 	int err, sp = -1, dplx = -1, pause = 0;
 
-	err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMCR, &bmcr);
+	err = mdio_read(cphy, 0, MII_BMCR, &bmcr);
 	if (!err)
-		err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMSR, &status);
+		err = mdio_read(cphy, 0, MII_BMSR, &status);
 	if (err)
 		return err;
 
@@ -221,8 +196,7 @@
 		 * once more to get the current link state.
 		 */
 		if (!(status & BMSR_LSTATUS))
-			err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_BMSR,
-					   &status);
+			err = mdio_read(cphy, 0, MII_BMSR, &status);
 		if (err)
 			return err;
 		*link_ok = (status & BMSR_LSTATUS) != 0;
@@ -236,10 +210,9 @@
 		else
 			sp = SPEED_10;
 	} else if (status & BMSR_ANEGCOMPLETE) {
-		err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_LPA, &lpa);
+		err = mdio_read(cphy, 0, MII_LPA, &lpa);
 		if (!err)
-			err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, MII_ADVERTISE,
-					   &adv);
+			err = mdio_read(cphy, 0, MII_ADVERTISE, &adv);
 		if (err)
 			return err;
 
@@ -271,7 +244,6 @@
 	return 0;
 }
 
-#ifdef UNUSED
 /*
  * Enable/disable auto MDI/MDI-X in forced link speed mode.
  */
@@ -279,26 +251,12 @@
 {
 	int err;
 
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_EXT_PAGE_AXS, 0x52b5);
-	if (err)
+	if ((err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 0x52b5)) != 0 ||
+	    (err = mdio_write(phy, 0, 18, 0x12)) != 0 ||
+	    (err = mdio_write(phy, 0, 17, enable ? 0x2803 : 0x3003)) != 0 ||
+	    (err = mdio_write(phy, 0, 16, 0x87fa)) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 0)) != 0)
 		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, 18, 0x12);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, 17, enable ? 0x2803 : 0x3003);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, 16, 0x87fa);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_EXT_PAGE_AXS, 0);
-	if (err)
-		return err;
-
 	return 0;
 }
 
@@ -311,7 +269,6 @@
 		err = vsc8211_set_automdi(phy, 1);
 	return err;
 }
-#endif /* UNUSED */
 
 static int vsc8211_power_down(struct cphy *cphy, int enable)
 {
@@ -324,7 +281,7 @@
 	unsigned int cause;
 	int err, cphy_cause = 0;
 
-	err = t3_mdio_read(cphy, MDIO_DEVAD_NONE, VSC8211_INTR_STATUS, &cause);
+	err = mdio_read(cphy, 0, VSC8211_INTR_STATUS, &cause);
 	if (err)
 		return err;
 
@@ -336,53 +293,145 @@
 	return cphy_cause;
 }
 
+#ifdef C99_NOT_SUPPORTED
 static struct cphy_ops vsc8211_ops = {
-	.reset = vsc8211_reset,
-	.intr_enable = vsc8211_intr_enable,
-	.intr_disable = vsc8211_intr_disable,
-	.intr_clear = vsc8211_intr_clear,
-	.intr_handler = vsc8211_intr_handler,
-	.autoneg_enable = vsc8211_autoneg_enable,
-	.autoneg_restart = vsc8211_autoneg_restart,
-	.advertise = t3_phy_advertise,
-	.set_speed_duplex = t3_set_phy_speed_duplex,
-	.get_link_status = vsc8211_get_link_status,
-	.power_down = vsc8211_power_down,
+	vsc8211_reset,
+	vsc8211_intr_enable,
+	vsc8211_intr_disable,
+	vsc8211_intr_clear,
+	vsc8211_intr_handler,
+	vsc8211_autoneg_enable,
+	vsc8211_autoneg_restart,
+	t3_phy_advertise,
+	NULL,
+	vsc8211_set_speed_duplex,
+	vsc8211_get_link_status,
+	vsc8211_power_down,
 };
 
 static struct cphy_ops vsc8211_fiber_ops = {
-	.reset = vsc8211_reset,
-	.intr_enable = vsc8211_intr_enable,
-	.intr_disable = vsc8211_intr_disable,
-	.intr_clear = vsc8211_intr_clear,
-	.intr_handler = vsc8211_intr_handler,
-	.autoneg_enable = vsc8211_autoneg_enable,
-	.autoneg_restart = vsc8211_autoneg_restart,
-	.advertise = t3_phy_advertise_fiber,
-	.set_speed_duplex = t3_set_phy_speed_duplex,
-	.get_link_status = vsc8211_get_link_status_fiber,
-	.power_down = vsc8211_power_down,
+	vsc8211_reset,
+	vsc8211_intr_enable,
+	vsc8211_intr_disable,
+	vsc8211_intr_clear,
+	vsc8211_intr_handler,
+	vsc8211_autoneg_enable,
+	vsc8211_autoneg_restart,
+	t3_phy_advertise_fiber,
+	NULL,
+	t3_set_phy_speed_duplex,
+	vsc8211_get_link_status_fiber,
+	vsc8211_power_down,
+};
+#else
+static struct cphy_ops vsc8211_ops = {
+	.reset             = vsc8211_reset,
+	.intr_enable       = vsc8211_intr_enable,
+	.intr_disable      = vsc8211_intr_disable,
+	.intr_clear        = vsc8211_intr_clear,
+	.intr_handler      = vsc8211_intr_handler,
+	.autoneg_enable    = vsc8211_autoneg_enable,
+	.autoneg_restart   = vsc8211_autoneg_restart,
+	.advertise         = t3_phy_advertise,
+	.set_speed_duplex  = vsc8211_set_speed_duplex,
+	.get_link_status   = vsc8211_get_link_status,
+	.power_down        = vsc8211_power_down,
 };
 
-int t3_vsc8211_phy_prep(struct cphy *phy, struct adapter *adapter,
-			int phy_addr, const struct mdio_ops *mdio_ops)
+static struct cphy_ops vsc8211_fiber_ops = {
+	.reset             = vsc8211_reset,
+	.intr_enable       = vsc8211_intr_enable,
+	.intr_disable      = vsc8211_intr_disable,
+	.intr_clear        = vsc8211_intr_clear,
+	.intr_handler      = vsc8211_intr_handler,
+	.autoneg_enable    = vsc8211_autoneg_enable,
+	.autoneg_restart   = vsc8211_autoneg_restart,
+	.advertise         = t3_phy_advertise_fiber,
+	.set_speed_duplex  = t3_set_phy_speed_duplex,
+	.get_link_status   = vsc8211_get_link_status_fiber,
+	.power_down        = vsc8211_power_down,
+};
+#endif
+
+#define VSC8211_PHY_CTRL 24
+
+#define S_VSC8211_TXFIFODEPTH    7
+#define M_VSC8211_TXFIFODEPTH    0x7
+#define V_VSC8211_TXFIFODEPTH(x) ((x) << S_VSC8211_TXFIFODEPTH)
+#define G_VSC8211_TXFIFODEPTH(x) (((x) >> S_VSC8211_TXFIFODEPTH) & M_VSC8211_TXFIFODEPTH)
+
+#define S_VSC8211_RXFIFODEPTH    4
+#define M_VSC8211_RXFIFODEPTH    0x7
+#define V_VSC8211_RXFIFODEPTH(x) ((x) << S_VSC8211_RXFIFODEPTH)
+#define G_VSC8211_RXFIFODEPTH(x) (((x) >> S_VSC8211_RXFIFODEPTH) & M_VSC8211_RXFIFODEPTH)
+
+int t3_vsc8211_fifo_depth(adapter_t *adap, unsigned int mtu, int port)
 {
+	/* TX FIFO Depth set bits 9:7 to 100 (IEEE mode) */
+	unsigned int val = 4;
+	unsigned int currentregval;
+	unsigned int regval;
+	int err;
+
+	/* Retrieve the port info structure from adater_t */
+	struct port_info *portinfo = adap2pinfo(adap, port);
+
+	/* What phy is this */
+	struct cphy *phy = &portinfo->phy;
+
+	/* Read the current value of the PHY control Register */
+	err = mdio_read(phy, 0, VSC8211_PHY_CTRL, &currentregval);
+
+	if (err)
+		return err;
+
+	/* IEEE mode supports up to 1518 bytes */
+	/* mtu does not contain the header + FCS (18 bytes) */
+	if (mtu > 1500)
+		/* 
+		 * If using a packet size > 1500  set TX FIFO Depth bits 
+		 * 9:7 to 011 (Jumbo packet mode) 
+		 */
+		val = 3;
+
+	regval = V_VSC8211_TXFIFODEPTH(val) | V_VSC8211_RXFIFODEPTH(val) | 
+		(currentregval & ~V_VSC8211_TXFIFODEPTH(M_VSC8211_TXFIFODEPTH) &
+		~V_VSC8211_RXFIFODEPTH(M_VSC8211_RXFIFODEPTH));
+
+	return  mdio_write(phy, 0, VSC8211_PHY_CTRL, regval);
+}
+
+#define S_LNKSPEEDDSENABLE    4 /* Enable Link Speed Auto-Downshift */
+#define V_LNKSPEEDDSENABLE(x) ((x) << S_LNKSPEEDDSENABLE)
+#define F_LNKSPEEDDSENABLE    V_LNKSPEEDDSENABLE(1U)
+
+int t3_vsc8211_phy_prep(pinfo_t *pinfo, int phy_addr,
+			const struct mdio_ops *mdio_ops)
+{
+	struct cphy *phy = &pinfo->phy;
 	int err;
 	unsigned int val;
 
-	cphy_init(phy, adapter, phy_addr, &vsc8211_ops, mdio_ops,
+	cphy_init(&pinfo->phy, pinfo->adapter, pinfo, phy_addr, &vsc8211_ops, mdio_ops,
 		  SUPPORTED_10baseT_Full | SUPPORTED_100baseT_Full |
 		  SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg | SUPPORTED_MII |
 		  SUPPORTED_TP | SUPPORTED_IRQ, "10/100/1000BASE-T");
 	msleep(20);       /* PHY needs ~10ms to start responding to MDIO */
 
-	err = t3_mdio_read(phy, MDIO_DEVAD_NONE, VSC8211_EXT_CTRL, &val);
+	/* Disable Link Speed Auto-Downshift */ 
+	if ((err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 1)) != 0 ||
+	    (err = mdio_read(phy, 0, VSC8211_EXT_CTRL_3, &val)) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_EXT_CTRL_3,
+			      (val & ~(F_LNKSPEEDDSENABLE)) )) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 0)) != 0 )
+		return err;
+
+	err = mdio_read(phy, 0, VSC8211_EXT_CTRL, &val);
 	if (err)
 		return err;
 	if (val & VSC_CTRL_MEDIA_MODE_HI) {
 		/* copper interface, just need to configure the LEDs */
-		return t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_LED_CTRL,
-				     0x100);
+		return mdio_write(phy, 0, VSC8211_LED_CTRL, 0x100);
 	}
 
 	phy->caps = SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg |
@@ -390,25 +439,12 @@
 	phy->desc = "1000BASE-X";
 	phy->ops = &vsc8211_fiber_ops;
 
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_EXT_PAGE_AXS, 1);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_SIGDET_CTRL, 1);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_EXT_PAGE_AXS, 0);
-	if (err)
-		return err;
-
-	err = t3_mdio_write(phy, MDIO_DEVAD_NONE, VSC8211_EXT_CTRL,
-			    val | VSC_CTRL_CLAUSE37_VIEW);
-	if (err)
-		return err;
-
-	err = vsc8211_reset(phy, 0);
-	if (err)
+	if ((err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 1)) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_SIGDET_CTRL, 1)) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_EXT_PAGE_AXS, 0)) != 0 ||
+	    (err = mdio_write(phy, 0, VSC8211_EXT_CTRL,
+			      val | VSC_CTRL_CLAUSE37_VIEW)) != 0 ||
+	    (err = vsc8211_reset(phy, 0)) != 0)
 		return err;
 
 	udelay(5); /* delay after reset before next SMI */
diff --git a/drivers/net/cxgb3/xgmac.c b/drivers/net/cxgb3/xgmac.c
--- a/drivers/net/cxgb3/xgmac.c
+++ b/drivers/net/cxgb3/xgmac.c
@@ -1,64 +1,60 @@
 /*
- * Copyright (c) 2005-2008 Chelsio, Inc. All rights reserved.
+ * This file is part of the Chelsio T3 Ethernet driver.
  *
- * This software is available to you under a choice of one of two
- * licenses.  You may choose to be licensed under the terms of the GNU
- * General Public License (GPL) Version 2, available from the file
- * COPYING in the main directory of this source tree, or the
- * OpenIB.org BSD license below:
+ * Copyright (C) 2005-2009 Chelsio Communications.  All rights reserved.
  *
- *     Redistribution and use in source and binary forms, with or
- *     without modification, are permitted provided that the following
- *     conditions are met:
- *
- *      - Redistributions of source code must retain the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer.
- *
- *      - Redistributions in binary form must reproduce the above
- *        copyright notice, this list of conditions and the following
- *        disclaimer in the documentation and/or other materials
- *        provided with the distribution.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
- * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
- * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
- * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
- * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
+ * This program is distributed in the hope that it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the LICENSE file included in this
+ * release for licensing terms and conditions.
  */
+
 #include "common.h"
 #include "regs.h"
 
-/*
- * # of exact address filters.  The first one is used for the station address,
- * the rest are available for multicast addresses.
- */
-#define EXACT_ADDR_FILTERS 8
-
 static inline int macidx(const struct cmac *mac)
 {
 	return mac->offset / (XGMAC0_1_BASE_ADDR - XGMAC0_0_BASE_ADDR);
 }
 
+/*
+ * Returns a reasonable A_XGM_RESET_CTRL value for the mac specified.
+ */
+static inline int xgm_reset_ctrl(const struct cmac *mac)
+{
+	adapter_t *adap = mac->adapter;
+	int val = F_MAC_RESET_ | F_XGMAC_STOP_EN;
+
+	if (is_10G(adap)) {
+		int cfg = t3_read_reg(adap, A_XGM_PORT_CFG + mac->offset);
+
+		val |= F_PCS_RESET_;
+		if (G_PORTSPEED(cfg) != 3)	/* not running at 10G */
+			val |= F_XG2G_RESET_;
+	} else if (uses_xaui(adap))
+		val |= F_PCS_RESET_ | F_XG2G_RESET_;
+	else
+		val |= F_RGMII_RESET_ | F_XG2G_RESET_;
+
+	return (val);
+}
+
 static void xaui_serdes_reset(struct cmac *mac)
 {
 	static const unsigned int clear[] = {
-		F_PWRDN0 | F_PWRDN1, F_RESETPLL01, F_RESET0 | F_RESET1,
-		F_PWRDN2 | F_PWRDN3, F_RESETPLL23, F_RESET2 | F_RESET3
+		F_PWRDN0 | F_PWRDN1,    F_RESETPLL01,    F_RESET0 | F_RESET1,
+	     	F_PWRDN2 | F_PWRDN3,    F_RESETPLL23,    F_RESET2 | F_RESET3
 	};
 
 	int i;
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
 	u32 ctrl = A_XGM_SERDES_CTRL0 + mac->offset;
 
 	t3_write_reg(adap, ctrl, adap->params.vpd.xauicfg[macidx(mac)] |
 		     F_RESET3 | F_RESET2 | F_RESET1 | F_RESET0 |
 		     F_PWRDN3 | F_PWRDN2 | F_PWRDN1 | F_PWRDN0 |
 		     F_RESETPLL23 | F_RESETPLL01);
-	t3_read_reg(adap, ctrl);
+	(void)t3_read_reg(adap, ctrl);
 	udelay(15);
 
 	for (i = 0; i < ARRAY_SIZE(clear); i++) {
@@ -67,40 +63,81 @@
 	}
 }
 
+/**
+ *	t3b_pcs_reset - reset the PCS on T3B+ adapters
+ *	@mac: the XGMAC handle
+ *
+ *	Reset the XGMAC PCS block on T3B+ adapters.
+ */
 void t3b_pcs_reset(struct cmac *mac)
 {
 	t3_set_reg_field(mac->adapter, A_XGM_RESET_CTRL + mac->offset,
 			 F_PCS_RESET_, 0);
-	udelay(20);
+
+	/* No delay required */
+
 	t3_set_reg_field(mac->adapter, A_XGM_RESET_CTRL + mac->offset, 0,
 			 F_PCS_RESET_);
 }
 
-int t3_mac_reset(struct cmac *mac)
+void t3c_pcs_force_los(struct cmac *mac)
 {
-	static const struct addr_val_pair mac_reset_avp[] = {
-		{A_XGM_TX_CTRL, 0},
-		{A_XGM_RX_CTRL, 0},
-		{A_XGM_RX_CFG, F_DISPAUSEFRAMES | F_EN1536BFRAMES |
-		 F_RMFCS | F_ENJUMBO | F_ENHASHMCAST},
-		{A_XGM_RX_HASH_LOW, 0},
-		{A_XGM_RX_HASH_HIGH, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_1, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_2, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_3, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_4, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_5, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_6, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_7, 0},
-		{A_XGM_RX_EXACT_MATCH_LOW_8, 0},
-		{A_XGM_STAT_CTRL, F_CLRSTATS}
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT0 + mac->offset,
+	    F_LOWSIGFORCEEN0 | F_LOWSIGFORCEVALUE0,
+	    F_LOWSIGFORCEEN0 | F_LOWSIGFORCEVALUE0);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT1 + mac->offset,
+	    F_LOWSIGFORCEEN1 | F_LOWSIGFORCEVALUE1,
+	    F_LOWSIGFORCEEN1 | F_LOWSIGFORCEVALUE1);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT2 + mac->offset,
+	    F_LOWSIGFORCEEN2 | F_LOWSIGFORCEVALUE2,
+	    F_LOWSIGFORCEEN2 | F_LOWSIGFORCEVALUE2);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT3 + mac->offset,
+	    F_LOWSIGFORCEEN3 | F_LOWSIGFORCEVALUE3,
+	    F_LOWSIGFORCEEN3 | F_LOWSIGFORCEVALUE3);
+
+	/* No delay required */
+
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT0 + mac->offset,
+	    F_LOWSIGFORCEEN0, 0);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT1 + mac->offset,
+	    F_LOWSIGFORCEEN1, 0);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT2 + mac->offset,
+	    F_LOWSIGFORCEEN2, 0);
+	t3_set_reg_field(mac->adapter, A_XGM_SERDES_STAT3 + mac->offset,
+	    F_LOWSIGFORCEEN3, 0);
+}
+
+/**
+ *	t3_mac_init - initialize a MAC
+ *	@mac: the MAC to initialize
+ *
+ *	Initialize the given MAC.
+ */
+int t3_mac_init(struct cmac *mac)
+{
+	static struct addr_val_pair mac_reset_avp[] = {
+		{ A_XGM_TX_CTRL, 0 },
+		{ A_XGM_RX_CTRL, 0 },
+		{ A_XGM_RX_CFG, F_DISPAUSEFRAMES | F_EN1536BFRAMES |
+		                F_RMFCS | F_ENJUMBO | F_ENHASHMCAST },
+		{ A_XGM_RX_HASH_LOW, 0 },
+		{ A_XGM_RX_HASH_HIGH, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_1, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_2, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_3, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_4, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_5, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_6, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_7, 0 },
+		{ A_XGM_RX_EXACT_MATCH_LOW_8, 0 },
+		{ A_XGM_STAT_CTRL, F_CLRSTATS }
 	};
 	u32 val;
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
 	unsigned int oft = mac->offset;
 
 	t3_write_reg(adap, A_XGM_RESET_CTRL + oft, F_MAC_RESET_);
-	t3_read_reg(adap, A_XGM_RESET_CTRL + oft);	/* flush */
+	(void) t3_read_reg(adap, A_XGM_RESET_CTRL + oft);    /* flush */
 
 	t3_write_regs(adap, mac_reset_avp, ARRAY_SIZE(mac_reset_avp), oft);
 	t3_set_reg_field(adap, A_XGM_RXFIFO_CFG + oft,
@@ -125,19 +162,28 @@
 			xaui_serdes_reset(mac);
 	}
 
+
+	if (mac->multiport) {
+		t3_write_reg(adap, A_XGM_RX_MAX_PKT_SIZE + oft,
+			     V_RXMAXPKTSIZE(MAX_FRAME_SIZE - 4));
+		t3_set_reg_field(adap, A_XGM_TXFIFO_CFG + oft, 0,
+				 F_DISPREAMBLE);
+		t3_set_reg_field(adap, A_XGM_RX_CFG + oft, 0, F_COPYPREAMBLE |
+				 F_ENNON802_3PREAMBLE);
+		t3_set_reg_field(adap, A_XGM_TXFIFO_CFG + oft,
+				 V_TXFIFOTHRESH(M_TXFIFOTHRESH),
+				 V_TXFIFOTHRESH(64));
+		t3_write_reg(adap, A_XGM_TX_CTRL + oft, F_TXEN);
+		t3_write_reg(adap, A_XGM_RX_CTRL + oft, F_RXEN);
+	}
+
 	t3_set_reg_field(adap, A_XGM_RX_MAX_PKT_SIZE + oft,
 			 V_RXMAXFRAMERSIZE(M_RXMAXFRAMERSIZE),
 			 V_RXMAXFRAMERSIZE(MAX_FRAME_SIZE) | F_RXENFRAMER);
-	val = F_MAC_RESET_ | F_XGMAC_STOP_EN;
 
-	if (is_10G(adap))
-		val |= F_PCS_RESET_;
-	else if (uses_xaui(adap))
-		val |= F_PCS_RESET_ | F_XG2G_RESET_;
-	else
-		val |= F_RGMII_RESET_ | F_XG2G_RESET_;
+	val = xgm_reset_ctrl(mac);
 	t3_write_reg(adap, A_XGM_RESET_CTRL + oft, val);
-	t3_read_reg(adap, A_XGM_RESET_CTRL + oft);	/* flush */
+	(void) t3_read_reg(adap, A_XGM_RESET_CTRL + oft);  /* flush */
 	if ((val & F_PCS_RESET_) && adap->params.rev) {
 		msleep(1);
 		t3b_pcs_reset(mac);
@@ -147,29 +193,34 @@
 	return 0;
 }
 
-static int t3b2_mac_reset(struct cmac *mac)
+static int t3_mac_reset(struct cmac *mac, int portspeed)
 {
-	struct adapter *adap = mac->adapter;
-	unsigned int oft = mac->offset, store;
+	u32 val, store_mps;
+	adapter_t *adap = mac->adapter;
+	unsigned int oft = mac->offset;
 	int idx = macidx(mac);
-	u32 val;
+	unsigned int store;
 
-	if (!macidx(mac))
+	/* Stop egress traffic to xgm*/
+	store_mps = t3_read_reg(adap, A_MPS_CFG);
+	if (!idx)
 		t3_set_reg_field(adap, A_MPS_CFG, F_PORT0ACTIVE, 0);
 	else
 		t3_set_reg_field(adap, A_MPS_CFG, F_PORT1ACTIVE, 0);
 
-	/* Stop NIC traffic to reduce the number of TXTOGGLES */
+	/* This will reduce the number of TXTOGGLES */
+	/* Clear: to stop the NIC traffic */
 	t3_set_reg_field(adap, A_MPS_CFG, F_ENFORCEPKT, 0);
 	/* Ensure TX drains */
 	t3_set_reg_field(adap, A_XGM_TX_CFG + oft, F_TXPAUSEEN, 0);
 
+	/* PCS in reset */
 	t3_write_reg(adap, A_XGM_RESET_CTRL + oft, F_MAC_RESET_);
-	t3_read_reg(adap, A_XGM_RESET_CTRL + oft);    /* flush */
+	(void) t3_read_reg(adap, A_XGM_RESET_CTRL + oft);    /* flush */
 
 	/* Store A_TP_TX_DROP_CFG_CH0 */
 	t3_write_reg(adap, A_TP_PIO_ADDR, A_TP_TX_DROP_CFG_CH0 + idx);
-	store = t3_read_reg(adap, A_TP_TX_DROP_CFG_CH0 + idx);
+	store = t3_read_reg(adap, A_TP_PIO_DATA);
 
 	msleep(10);
 
@@ -180,46 +231,55 @@
 	/* Check for xgm Rx fifo empty */
 	/* Increased loop count to 1000 from 5 cover 1G and 100Mbps case */
 	if (t3_wait_op_done(adap, A_XGM_RX_MAX_PKT_SIZE_ERR_CNT + oft,
-			    0x80000000, 1, 1000, 2)) {
-		CH_ERR(adap, "MAC %d Rx fifo drain failed\n",
-		       macidx(mac));
+			    0x80000000, 1, 1000, 2) && portspeed < 0) {
+		CH_ERR(adap, "MAC %d Rx fifo drain failed\n", idx);
 		return -1;
 	}
 
-	t3_write_reg(adap, A_XGM_RESET_CTRL + oft, 0);
-	t3_read_reg(adap, A_XGM_RESET_CTRL + oft);    /* flush */
+	if (portspeed >= 0) {
+		u32 intr = t3_read_reg(adap, A_XGM_INT_ENABLE + oft);
 
-	val = F_MAC_RESET_;
-	if (is_10G(adap))
-		val |= F_PCS_RESET_;
-	else if (uses_xaui(adap))
-		val |= F_PCS_RESET_ | F_XG2G_RESET_;
-	else
-		val |= F_RGMII_RESET_ | F_XG2G_RESET_;
-	t3_write_reg(adap, A_XGM_RESET_CTRL + oft, val);
-	t3_read_reg(adap, A_XGM_RESET_CTRL + oft);  /* flush */
-	if ((val & F_PCS_RESET_) && adap->params.rev) {
-		msleep(1);
-		t3b_pcs_reset(mac);
+		/*
+		 * safespeedchange: wipes out pretty much all XGMAC registers.
+		 */
+
+		t3_set_reg_field(adap, A_XGM_PORT_CFG + oft,
+		    V_PORTSPEED(M_PORTSPEED) | F_SAFESPEEDCHANGE,
+		    portspeed | F_SAFESPEEDCHANGE);
+		(void) t3_read_reg(adap, A_XGM_PORT_CFG + oft);
+		t3_set_reg_field(adap, A_XGM_PORT_CFG + oft,
+		    F_SAFESPEEDCHANGE, 0);
+		(void) t3_read_reg(adap, A_XGM_PORT_CFG + oft);
+		t3_mac_init(mac);
+		
+		t3_write_reg(adap, A_XGM_INT_ENABLE + oft, intr);
+	} else {
+
+		t3_write_reg(adap, A_XGM_RESET_CTRL + oft, 0); /*MAC in reset*/
+		(void) t3_read_reg(adap, A_XGM_RESET_CTRL + oft);    /* flush */
+
+		val = xgm_reset_ctrl(mac);
+		t3_write_reg(adap, A_XGM_RESET_CTRL + oft, val);
+		(void) t3_read_reg(adap, A_XGM_RESET_CTRL + oft);  /* flush */
+		if ((val & F_PCS_RESET_) && adap->params.rev) {
+			msleep(1);
+			t3b_pcs_reset(mac);
+		}
+		t3_write_reg(adap, A_XGM_RX_CFG + oft,
+			 F_DISPAUSEFRAMES | F_EN1536BFRAMES |
+					F_RMFCS | F_ENJUMBO | F_ENHASHMCAST );
 	}
-	t3_write_reg(adap, A_XGM_RX_CFG + oft,
-		     F_DISPAUSEFRAMES | F_EN1536BFRAMES |
-		     F_RMFCS | F_ENJUMBO | F_ENHASHMCAST);
 
 	/* Restore the DROP_CFG */
 	t3_write_reg(adap, A_TP_PIO_ADDR, A_TP_TX_DROP_CFG_CH0 + idx);
 	t3_write_reg(adap, A_TP_PIO_DATA, store);
 
-	if (!idx)
-		t3_set_reg_field(adap, A_MPS_CFG, 0, F_PORT0ACTIVE);
-	else
-		t3_set_reg_field(adap, A_MPS_CFG, 0, F_PORT1ACTIVE);
-
-	/* re-enable nic traffic */
-	t3_set_reg_field(adap, A_MPS_CFG, F_ENFORCEPKT, 1);
+	/* Resume egress traffic to xgm */
+	t3_set_reg_field(adap, A_MPS_CFG, F_PORT1ACTIVE | F_PORT0ACTIVE,
+			 store_mps);
 
 	/*  Set: re-enable NIC traffic */
-	t3_set_reg_field(adap, A_MPS_CFG, F_ENFORCEPKT, 1);
+	t3_set_reg_field(adap, A_MPS_CFG, F_ENFORCEPKT, F_ENFORCEPKT);
 
 	return 0;
 }
@@ -227,7 +287,7 @@
 /*
  * Set the exact match register 'idx' to recognize the given Ethernet address.
  */
-static void set_addr_filter(struct cmac *mac, int idx, const u8 * addr)
+static void set_addr_filter(struct cmac *mac, int idx, const u8 *addr)
 {
 	u32 addr_lo, addr_hi;
 	unsigned int oft = mac->offset + idx * 8;
@@ -239,21 +299,39 @@
 	t3_write_reg(mac->adapter, A_XGM_RX_EXACT_MATCH_HIGH_1 + oft, addr_hi);
 }
 
-/* Set one of the station's unicast MAC addresses. */
+/**
+ *	t3_mac_set_address - set one of the station's unicast MAC addresses
+ *	@mac: the MAC handle
+ *	@idx: index of the exact address match filter to use
+ *	@addr: the Ethernet address
+ *
+ *	Set one of the station's unicast MAC addresses.
+ */
 int t3_mac_set_address(struct cmac *mac, unsigned int idx, u8 addr[6])
 {
+	if (mac->multiport)
+		idx = mac->ext_port + idx * mac->adapter->params.nports;
 	if (idx >= mac->nucast)
 		return -EINVAL;
 	set_addr_filter(mac, idx, addr);
+	if (mac->multiport && idx < mac->adapter->params.nports)
+		t3_vsc7323_set_addr(mac->adapter, addr, idx);
 	return 0;
 }
 
-/*
- * Specify the number of exact address filters that should be reserved for
- * unicast addresses.  Caller should reload the unicast and multicast addresses
- * after calling this.
+/**
+ *	t3_mac_set_num_ucast - set the number of unicast addresses needed
+ *	@mac: the MAC handle
+ *	@n: number of unicast addresses needed
+ *
+ *	Specify the number of exact address filters that should be reserved for
+ *	unicast addresses.  Caller should reload the unicast and multicast
+ *	addresses after calling this.
+ *
+ *	Generally, this is 1 with the first one used for the station address,
+ *	and the rest are available for multicast addresses.
  */
-int t3_mac_set_num_ucast(struct cmac *mac, int n)
+int t3_mac_set_num_ucast(struct cmac *mac, unsigned char n)
 {
 	if (n > EXACT_ADDR_FILTERS)
 		return -EINVAL;
@@ -269,7 +347,7 @@
 		u32 v = t3_read_reg(mac->adapter, reg);
 		t3_write_reg(mac->adapter, reg, v);
 	}
-	t3_read_reg(mac->adapter, A_XGM_RX_EXACT_MATCH_LOW_1);	/* flush */
+	t3_read_reg(mac->adapter, A_XGM_RX_EXACT_MATCH_LOW_1); /* flush */
 }
 
 void t3_mac_enable_exact_filters(struct cmac *mac)
@@ -280,11 +358,11 @@
 		u32 v = t3_read_reg(mac->adapter, reg);
 		t3_write_reg(mac->adapter, reg, v);
 	}
-	t3_read_reg(mac->adapter, A_XGM_RX_EXACT_MATCH_LOW_1);	/* flush */
+	t3_read_reg(mac->adapter, A_XGM_RX_EXACT_MATCH_LOW_1); /* flush */
 }
 
 /* Calculate the RX hash filter index of an Ethernet address */
-static int hash_hw_addr(const u8 * addr)
+static int hash_hw_addr(const u8 *addr)
 {
 	int hash = 0, octet, bit, i = 0, c;
 
@@ -297,18 +375,28 @@
 	return hash;
 }
 
+/**
+ *	t3_mac_set_rx_mode - set the Rx mode and address filters
+ *	@mac: the MAC to configure
+ *	@rm: structure containing the Rx mode and MAC addresses needed
+ *
+ *	Configures the MAC Rx mode (promiscuity, etc) and exact and hash
+ *	address filters.
+ */
 int t3_mac_set_rx_mode(struct cmac *mac, struct t3_rx_mode *rm)
 {
-	u32 val, hash_lo, hash_hi;
-	struct adapter *adap = mac->adapter;
+	u32 hash_lo, hash_hi;
+	adapter_t *adap = mac->adapter;
 	unsigned int oft = mac->offset;
 
-	val = t3_read_reg(adap, A_XGM_RX_CFG + oft) & ~F_COPYALLFRAMES;
-	if (rm->dev->flags & IFF_PROMISC)
-		val |= F_COPYALLFRAMES;
-	t3_write_reg(adap, A_XGM_RX_CFG + oft, val);
+	if (promisc_rx_mode(rm))
+		mac->promisc_map |= 1 << mac->ext_port;
+	else
+		mac->promisc_map &= ~(1 << mac->ext_port);
+	t3_set_reg_field(adap, A_XGM_RX_CFG + oft, F_COPYALLFRAMES,
+			 mac->promisc_map ? F_COPYALLFRAMES : 0);
 
-	if (rm->dev->flags & IFF_ALLMULTI)
+	if (allmulti_rx_mode(rm) || mac->multiport)
 		hash_lo = hash_hi = 0xffffffff;
 	else {
 		u8 *addr;
@@ -341,24 +429,49 @@
 	return min(hwm, MAC_RXFIFO_SIZE - 8192);
 }
 
+/**
+ *	t3_mac_set_mtu - set the MAC MTU
+ *	@mac: the MAC to configure
+ *	@mtu: the MTU
+ *
+ *	Sets the MAC MTU and adjusts the FIFO PAUSE watermarks accordingly.
+ */
 int t3_mac_set_mtu(struct cmac *mac, unsigned int mtu)
 {
-	int hwm, lwm, divisor;
+	int hwm, lwm;
 	int ipg;
 	unsigned int thres, v, reg;
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
+	unsigned port_type = adap->params.vpd.port_type[macidx(mac)];
+	unsigned int orig_mtu=mtu;
+	unsigned int drain_time = 0;
 
 	/*
 	 * MAX_FRAME_SIZE inludes header + FCS, mtu doesn't.  The HW max
-	 * packet size register includes header, but not FCS.
+	 * packet size register includes header, but not FCS.  An interesting
+	 * "feature" of the chip is that it automatically handle frame size
+	 * inflation for VLAN tags but this feature only works for certain MTU
+	 * sizes.  It's easier to simply add 4 to accommodate possible VLAN
+	 * Tags ...
 	 */
 	mtu += 14;
-	if (mtu > 1536)
+	if (mac->multiport)
+		mtu += 8;                             /* for preamble */
+	else
 		mtu += 4;
 
 	if (mtu > MAX_FRAME_SIZE - 4)
 		return -EINVAL;
-	t3_write_reg(adap, A_XGM_RX_MAX_PKT_SIZE + mac->offset, mtu);
+	if (mac->multiport)
+		return t3_vsc7323_set_mtu(adap, mtu - 4, mac->ext_port);
+
+	/* Modify the TX and RX fifo depth only if the card has a vsc8211 phy */
+	if (port_type == 2) {
+		int err = t3_vsc8211_fifo_depth(adap,orig_mtu,macidx(mac));
+
+		if (err)
+			return err;
+	}
 
 	if (adap->params.rev >= T3_REV_B2 &&
 	    (t3_read_reg(adap, A_XGM_RX_CTRL + mac->offset) & F_RXEN)) {
@@ -370,12 +483,20 @@
 		reg = adap->params.rev == T3_REV_B2 ?
 			A_XGM_RX_MAX_PKT_SIZE_ERR_CNT : A_XGM_RXFIFO_CFG;
 
+		/* Time to drain max Rx fifo */
+		msleep(1);
+		drain_time = 1;
+
 		/* drain RX FIFO */
-		if (t3_wait_op_done(adap, reg + mac->offset,
-				    F_RXFIFO_EMPTY, 1, 20, 5)) {
-			t3_write_reg(adap, A_XGM_RX_CFG + mac->offset, v);
-			t3_mac_enable_exact_filters(mac);
-			return -EIO;
+		while (t3_wait_op_done(adap, reg + mac->offset,
+				       F_RXFIFO_EMPTY, 1, 20, 5)) {
+			msleep(5);
+			drain_time += 5;
+			if (drain_time > 500) {
+				t3_write_reg(adap, A_XGM_RX_CFG + mac->offset, v);
+				t3_mac_enable_exact_filters(mac);
+				return -EIO;
+			}
 		}
 		t3_set_reg_field(adap, A_XGM_RX_MAX_PKT_SIZE + mac->offset,
 				 V_RXMAXPKTSIZE(M_RXMAXPKTSIZE),
@@ -386,13 +507,12 @@
 		t3_set_reg_field(adap, A_XGM_RX_MAX_PKT_SIZE + mac->offset,
 				 V_RXMAXPKTSIZE(M_RXMAXPKTSIZE),
 				 V_RXMAXPKTSIZE(mtu));
-
 	/*
 	 * Adjust the PAUSE frame watermarks.  We always set the LWM, and the
 	 * HWM only if flow-control is enabled.
 	 */
 	hwm = rx_fifo_hwm(mtu);
-	lwm = min(3 * (int)mtu, MAC_RXFIFO_SIZE / 4);
+	lwm = min(3 * (int) mtu, MAC_RXFIFO_SIZE /4);
 	v = t3_read_reg(adap, A_XGM_RXFIFO_CFG + mac->offset);
 	v &= ~V_RXFIFOPAUSELWM(M_RXFIFOPAUSELWM);
 	v |= V_RXFIFOPAUSELWM(lwm / 8);
@@ -408,30 +528,89 @@
 	if (is_10G(adap))
 		thres /= 10;
 	thres = mtu > thres ? (mtu - thres + 7) / 8 : 0;
-	thres = max(thres, 8U);	/* need at least 8 */
+	thres = max(thres, 8U);                          /* need at least 8 */
 	ipg = (adap->params.rev == T3_REV_C) ? 0 : 1;
+
+	/* Ugly hack */
+	if (is_demo_bt(adap))
+		ipg = 1;
+
 	t3_set_reg_field(adap, A_XGM_TXFIFO_CFG + mac->offset,
 			 V_TXFIFOTHRESH(M_TXFIFOTHRESH) | V_TXIPG(M_TXIPG),
 			 V_TXFIFOTHRESH(thres) | V_TXIPG(ipg));
-
-	if (adap->params.rev > 0) {
-		divisor = (adap->params.rev == T3_REV_C) ? 64 : 8;
-		t3_write_reg(adap, A_XGM_PAUSE_TIMER + mac->offset,
-			     (hwm - lwm) * 4 / divisor);
-	}
-	t3_write_reg(adap, A_XGM_TX_PAUSE_QUANTA + mac->offset,
-		     MAC_RXFIFO_SIZE * 4 * 8 / 512);
 	return 0;
 }
 
+/**
+ *	t3_mac_set_speed_duplex_fc - set MAC speed, duplex and flow control
+ *	@mac: the MAC to configure
+ *	@speed: the desired speed (10/100/1000/10000)
+ *	@duplex: the desired duplex
+ *	@fc: desired Tx/Rx PAUSE configuration
+ *
+ *	Set the MAC speed, duplex (actually only full-duplex is supported), and
+ *	flow control.  If a parameter value is negative the corresponding
+ *	MAC setting is left at its current value.
+ */
 int t3_mac_set_speed_duplex_fc(struct cmac *mac, int speed, int duplex, int fc)
 {
 	u32 val;
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
 	unsigned int oft = mac->offset;
+	unsigned int pause_bits;
 
 	if (duplex >= 0 && duplex != DUPLEX_FULL)
 		return -EINVAL;
+
+	/*
+	 * Set TX Pause Quanta and Timer registers.  The Pause Quanta will be
+	 * sent in our TX Pause Frames and is in terms of 512bit units.  We
+	 * arbitrarily set our Pause Quanta to be 4 times our RX FIFO Size.
+	 * We want the Pause Frame Timer to fire off a new Pause Frame every
+	 * 1/2 Passe Quanta time as long as the XGMAC is experiencing back-
+	 * pressure.  The Pause Timer is in terms of MAC Clock Ticks.  Thus
+	 * we have:
+	 *
+	 *           Quanta * 512 bits / 2   MAC Clock Ticks
+	 *   Timer = --------------------- x ---------------
+	 *            Link Speed bits/us          us
+	 *
+	 * Since:
+	 *
+	 *   MAC Clock Ticks/us = 0.015625 Ticks/bit * Link Speed bits/us
+	 *
+	 * The calculation for the TX Pause Timer simplifies to:
+	 *
+	 *   Timer = Quanta * 512 bits / 2 * 15625/10^6
+	 *
+	 * Note that 15625 = 5^6 and 10^6 = 2^6 * 5^6 so we can perform an
+	 * arithmatic simplification which helps us avoid integer overflow:
+	 *
+	 *   Timer = Quanta * 512 bits / 2 / 2^6
+	 *
+	 * Also note that the TX_PAUSE_TIMER for T3C runs 8 times slower than
+	 * earlier revisions which changes the "2^6" above to "2^9" when we're
+	 * dealing with a T3C adapter ...
+	 */
+	pause_bits = MAC_RXFIFO_SIZE * 4 * 8;
+	t3_write_reg(adap, A_XGM_TX_PAUSE_QUANTA + mac->offset,
+		     pause_bits / 512);
+	t3_write_reg(adap, A_XGM_PAUSE_TIMER + mac->offset,
+		     (pause_bits >> (adap->params.rev == T3_REV_C ? 10 : 7)));
+
+	if (mac->multiport) {
+		u32 rx_max_pkt_size =
+		    G_RXMAXPKTSIZE(t3_read_reg(adap,
+					       A_XGM_RX_MAX_PKT_SIZE + oft));
+		val = t3_read_reg(adap, A_XGM_RXFIFO_CFG + oft);
+		val &= ~V_RXFIFOPAUSEHWM(M_RXFIFOPAUSEHWM);
+		val |= V_RXFIFOPAUSEHWM(rx_fifo_hwm(rx_max_pkt_size) / 8);
+		t3_write_reg(adap, A_XGM_RXFIFO_CFG + oft, val);
+		t3_set_reg_field(adap, A_XGM_TX_CFG + oft, F_TXPAUSEEN,
+			  		F_TXPAUSEEN);
+
+		return t3_vsc7323_set_speed_fc(adap, speed, fc, mac->ext_port);
+	}
 	if (speed >= 0) {
 		if (speed == SPEED_10)
 			val = V_PORTSPEED(0);
@@ -444,8 +623,17 @@
 		else
 			return -EINVAL;
 
-		t3_set_reg_field(adap, A_XGM_PORT_CFG + oft,
-				 V_PORTSPEED(M_PORTSPEED), val);
+		if (!uses_xaui(adap)) /* T302 */
+			t3_set_reg_field(adap, A_XGM_PORT_CFG + oft,
+			    V_PORTSPEED(M_PORTSPEED), val);
+		else {
+			u32 old = t3_read_reg(adap, A_XGM_PORT_CFG + oft);
+
+			if ((old & V_PORTSPEED(M_PORTSPEED)) != val) {
+				t3_mac_reset(mac, val);
+				mac->was_reset = 1;
+			}
+		}
 	}
 
 	val = t3_read_reg(adap, A_XGM_RXFIFO_CFG + oft);
@@ -459,17 +647,29 @@
 	t3_write_reg(adap, A_XGM_RXFIFO_CFG + oft, val);
 
 	t3_set_reg_field(adap, A_XGM_TX_CFG + oft, F_TXPAUSEEN,
-			 (fc & PAUSE_RX) ? F_TXPAUSEEN : 0);
+			(fc & PAUSE_RX) ? F_TXPAUSEEN : 0);
 	return 0;
 }
 
+/**
+ *	t3_mac_enable - enable the MAC in the given directions
+ *	@mac: the MAC to configure
+ *	@which: bitmap indicating which directions to enable
+ *
+ *	Enables the MAC for operation in the given directions.
+ *	%MAC_DIRECTION_TX enables the Tx direction, and %MAC_DIRECTION_RX
+ *	enables the Rx one.
+ */
 int t3_mac_enable(struct cmac *mac, int which)
 {
 	int idx = macidx(mac);
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
 	unsigned int oft = mac->offset;
 	struct mac_stats *s = &mac->stats;
 
+	if (mac->multiport)
+		return t3_vsc7323_enable(adap, mac->ext_port, which);
+
 	if (which & MAC_DIRECTION_TX) {
 		t3_write_reg(adap, A_TP_PIO_ADDR, A_TP_TX_DROP_CFG_CH0 + idx);
 		t3_write_reg(adap, A_TP_PIO_DATA,
@@ -477,14 +677,15 @@
 			     0xc4ffff01 : 0xc0ede401);
 		t3_write_reg(adap, A_TP_PIO_ADDR, A_TP_TX_DROP_MODE);
 		t3_set_reg_field(adap, A_TP_PIO_DATA, 1 << idx,
-				 adap->params.rev == T3_REV_C ? 0 : 1 << idx);
+				 adap->params.rev == T3_REV_C ?
+				 0 : 1 << idx);
 
 		t3_write_reg(adap, A_XGM_TX_CTRL + oft, F_TXEN);
 
 		t3_write_reg(adap, A_TP_PIO_ADDR, A_TP_TX_DROP_CNT_CH0 + idx);
 		mac->tx_mcnt = s->tx_frames;
 		mac->tx_tcnt = (G_TXDROPCNTCH0RCVD(t3_read_reg(adap,
-							A_TP_PIO_DATA)));
+							       A_TP_PIO_DATA)));
 		mac->tx_xcnt = (G_TXSPI4SOPCNT(t3_read_reg(adap,
 						A_XGM_TX_SPI4_SOP_EOP_CNT +
 						oft)));
@@ -502,27 +703,33 @@
 	return 0;
 }
 
+/**
+ *	t3_mac_disable - disable the MAC in the given directions
+ *	@mac: the MAC to configure
+ *	@which: bitmap indicating which directions to disable
+ *
+ *	Disables the MAC in the given directions.
+ *	%MAC_DIRECTION_TX disables the Tx direction, and %MAC_DIRECTION_RX
+ *	disables the Rx one.
+ */
 int t3_mac_disable(struct cmac *mac, int which)
 {
-	struct adapter *adap = mac->adapter;
+	adapter_t *adap = mac->adapter;
+
+	if (mac->multiport)
+		return t3_vsc7323_disable(adap, mac->ext_port, which);
 
 	if (which & MAC_DIRECTION_TX) {
 		t3_write_reg(adap, A_XGM_TX_CTRL + mac->offset, 0);
 		mac->txen = 0;
 	}
 	if (which & MAC_DIRECTION_RX) {
-		int val = F_MAC_RESET_;
+		int val = xgm_reset_ctrl(mac);
 
 		t3_set_reg_field(mac->adapter, A_XGM_RESET_CTRL + mac->offset,
 				 F_PCS_RESET_, 0);
 		msleep(100);
 		t3_write_reg(adap, A_XGM_RX_CTRL + mac->offset, 0);
-		if (is_10G(adap))
-			val |= F_PCS_RESET_;
-		else if (uses_xaui(adap))
-			val |= F_PCS_RESET_ | F_XG2G_RESET_;
-		else
-			val |= F_RGMII_RESET_ | F_XG2G_RESET_;
 		t3_write_reg(mac->adapter, A_XGM_RESET_CTRL + mac->offset, val);
 	}
 	return 0;
@@ -530,27 +737,35 @@
 
 int t3b2_mac_watchdog_task(struct cmac *mac)
 {
-	struct adapter *adap = mac->adapter;
+	int status;
+	unsigned int tx_tcnt, tx_xcnt;
+	adapter_t *adap = mac->adapter;
 	struct mac_stats *s = &mac->stats;
-	unsigned int tx_tcnt, tx_xcnt;
 	u64 tx_mcnt = s->tx_frames;
-	int status;
+
+	if (mac->multiport)
+		tx_mcnt = t3_read_reg(adap, A_XGM_STAT_TX_FRAME_LOW);
 
 	status = 0;
-	tx_xcnt = 1;		/* By default tx_xcnt is making progress */
-	tx_tcnt = mac->tx_tcnt;	/* If tx_mcnt is progressing ignore tx_tcnt */
+	tx_xcnt = 1; /* By default tx_xcnt is making progress*/
+	tx_tcnt = mac->tx_tcnt; /* If tx_mcnt is progressing ignore tx_tcnt*/
 	if (tx_mcnt == mac->tx_mcnt && mac->rx_pause == s->rx_pause) {
+		u32 cfg, active, enforcepkt;
+
 		tx_xcnt = (G_TXSPI4SOPCNT(t3_read_reg(adap,
-						A_XGM_TX_SPI4_SOP_EOP_CNT +
-					       	mac->offset)));
-		if (tx_xcnt == 0) {
+						      A_XGM_TX_SPI4_SOP_EOP_CNT +
+						      mac->offset)));
+		cfg = t3_read_reg(adap, A_MPS_CFG);
+		active = macidx(mac) ? cfg & F_PORT1ACTIVE : cfg & F_PORT0ACTIVE;
+		enforcepkt = cfg & F_ENFORCEPKT;	
+		if (active && enforcepkt && (tx_xcnt == 0)) {
 			t3_write_reg(adap, A_TP_PIO_ADDR,
-				     A_TP_TX_DROP_CNT_CH0 + macidx(mac));
+			     	A_TP_TX_DROP_CNT_CH0 + macidx(mac));
 			tx_tcnt = (G_TXDROPCNTCH0RCVD(t3_read_reg(adap,
-						      A_TP_PIO_DATA)));
-		} else {
+			      	A_TP_PIO_DATA)));
+		} else
 			goto out;
-		}
+
 	} else {
 		mac->toggle_cnt = 0;
 		goto out;
@@ -581,18 +796,21 @@
 		t3_read_reg(adap, A_XGM_TX_CTRL + mac->offset);  /* flush */
 		mac->toggle_cnt++;
 	} else if (status == 2) {
-		t3b2_mac_reset(mac);
+		t3_mac_reset(mac, -1);
 		mac->toggle_cnt = 0;
 	}
 	return status;
 }
 
-/*
- * This function is called periodically to accumulate the current values of the
- * RMON counters into the port statistics.  Since the packet counters are only
- * 32 bits they can overflow in ~286 secs at 10G, so the function should be
- * called more frequently than that.  The byte counters are 45-bit wide, they
- * would overflow in ~7.8 hours.
+/**
+ *	t3_mac_update_stats - accumulate MAC statistics
+ *	@mac: the MAC handle
+ *
+ *	This function is called periodically to accumulate the current values
+ *	of the RMON counters into the port statistics.  Since the packet
+ *	counters are only 32 bits they can overflow in ~286 secs at 10G, so the
+ *	function should be called more frequently than that.  The byte counters
+ *	are 45-bit wide, they would overflow in ~7.8 hours.
  */
 const struct mac_stats *t3_mac_update_stats(struct cmac *mac)
 {
@@ -605,6 +823,9 @@
 
 	u32 v, lo;
 
+	if (mac->multiport)
+		return t3_vsc7323_update_stats(mac);
+
 	RMON_UPDATE64(mac, rx_octets, RX_BYTES_LOW, RX_BYTES_HIGH);
 	RMON_UPDATE64(mac, rx_frames, RX_FRAMES_LOW, RX_FRAMES_HIGH);
 	RMON_UPDATE(mac, rx_mcast_frames, RX_MCAST_FRAMES);
@@ -622,13 +843,13 @@
 		v &= 0x7fffffff;
 	mac->stats.rx_too_long += v;
 
-	RMON_UPDATE(mac, rx_frames_64, RX_64B_FRAMES);
-	RMON_UPDATE(mac, rx_frames_65_127, RX_65_127B_FRAMES);
-	RMON_UPDATE(mac, rx_frames_128_255, RX_128_255B_FRAMES);
-	RMON_UPDATE(mac, rx_frames_256_511, RX_256_511B_FRAMES);
-	RMON_UPDATE(mac, rx_frames_512_1023, RX_512_1023B_FRAMES);
+	RMON_UPDATE(mac, rx_frames_64,        RX_64B_FRAMES);
+	RMON_UPDATE(mac, rx_frames_65_127,    RX_65_127B_FRAMES);
+	RMON_UPDATE(mac, rx_frames_128_255,   RX_128_255B_FRAMES);
+	RMON_UPDATE(mac, rx_frames_256_511,   RX_256_511B_FRAMES);
+	RMON_UPDATE(mac, rx_frames_512_1023,  RX_512_1023B_FRAMES);
 	RMON_UPDATE(mac, rx_frames_1024_1518, RX_1024_1518B_FRAMES);
-	RMON_UPDATE(mac, rx_frames_1519_max, RX_1519_MAXB_FRAMES);
+	RMON_UPDATE(mac, rx_frames_1519_max,  RX_1519_MAXB_FRAMES);
 
 	RMON_UPDATE64(mac, tx_octets, TX_BYTE_LOW, TX_BYTE_HIGH);
 	RMON_UPDATE64(mac, tx_frames, TX_FRAME_LOW, TX_FRAME_HIGH);
@@ -638,19 +859,19 @@
 	/* This counts error frames in general (bad FCS, underrun, etc). */
 	RMON_UPDATE(mac, tx_underrun, TX_ERR_FRAMES);
 
-	RMON_UPDATE(mac, tx_frames_64, TX_64B_FRAMES);
-	RMON_UPDATE(mac, tx_frames_65_127, TX_65_127B_FRAMES);
-	RMON_UPDATE(mac, tx_frames_128_255, TX_128_255B_FRAMES);
-	RMON_UPDATE(mac, tx_frames_256_511, TX_256_511B_FRAMES);
-	RMON_UPDATE(mac, tx_frames_512_1023, TX_512_1023B_FRAMES);
+	RMON_UPDATE(mac, tx_frames_64,        TX_64B_FRAMES);
+	RMON_UPDATE(mac, tx_frames_65_127,    TX_65_127B_FRAMES);
+	RMON_UPDATE(mac, tx_frames_128_255,   TX_128_255B_FRAMES);
+	RMON_UPDATE(mac, tx_frames_256_511,   TX_256_511B_FRAMES);
+	RMON_UPDATE(mac, tx_frames_512_1023,  TX_512_1023B_FRAMES);
 	RMON_UPDATE(mac, tx_frames_1024_1518, TX_1024_1518B_FRAMES);
-	RMON_UPDATE(mac, tx_frames_1519_max, TX_1519_MAXB_FRAMES);
+	RMON_UPDATE(mac, tx_frames_1519_max,  TX_1519_MAXB_FRAMES);
 
 	/* The next stat isn't clear-on-read. */
 	t3_write_reg(mac->adapter, A_TP_MIB_INDEX, mac->offset ? 51 : 50);
 	v = t3_read_reg(mac->adapter, A_TP_MIB_RDATA);
-	lo = (u32) mac->stats.rx_cong_drops;
-	mac->stats.rx_cong_drops += (u64) (v - lo);
+	lo = (u32)mac->stats.rx_cong_drops;
+	mac->stats.rx_cong_drops += (u64)(v - lo);
 
 	return &mac->stats;
 }
