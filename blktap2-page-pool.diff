blktap: Manage segment buffers in mempools.

 - Replaces the request free list with a (mempooled) slab.

 - Replaces request buckets with a mempool. No buckets, because
   we're doing full s/g on page granularity anyway, so can gfp()
   independent pages everywhere. Allocations are 1-11 page-sized
   segments.

 - Adds support for multiple page pools.

 - Adds pools to sysfs. Linked as a 'pools' kset to blktap-control.

 - Makes the per-tap pool selectable.
   Attribute 'pool' on the tap device.

 - Make pools online-resizeable.
   Attributes free/size on the pool kobj.

Signed-off-by: Daniel Stodden <daniel.stodden@citrix.com>

diff -r 7b618e7dfdd1 drivers/xen/blktap2/blktap.h
--- a/drivers/xen/blktap2/blktap.h	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/blktap.h	Mon Jul 09 15:48:16 2012 +0100
@@ -121,17 +121,19 @@
 };
 
 struct blktap_request {
+	struct blktap                 *tap;
 	struct request                *rq;
 	uint16_t                       usr_idx;
 
 	uint8_t                        status;
 	atomic_t                       pendcnt;
-	uint8_t                        nr_pages;
 	unsigned short                 operation;
 
 	struct timeval                 time;
 	struct grant_handle_pair       handles[BLKIF_MAX_SEGMENTS_PER_REQUEST];
-	struct list_head               free_list;
+
+	struct page                   *pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	int                            nr_pages;
 };
 
 struct blktap {
@@ -140,6 +142,7 @@
 
 	struct blktap_ring             ring;
 	struct blktap_device           device;
+	struct blktap_page_pool       *pool;
 
 	int                            pending_cnt;
 	struct blktap_request         *pending_requests[MAX_PENDING_REQS];
@@ -152,6 +155,13 @@
 	struct blktap_statistics       stats;
 };
 
+struct blktap_page_pool {
+	struct mempool_s              *_mem;
+	spinlock_t                     lock;
+	struct kobject                 kobj;
+	wait_queue_head_t              wait;
+};
+
 extern struct mutex blktap_lock;
 extern struct blktap **blktaps;
 extern int blktap_max_minor;
@@ -165,7 +175,6 @@
 int blktap_ring_create(struct blktap *);
 int blktap_ring_destroy(struct blktap *);
 void blktap_ring_kick_user(struct blktap *);
-void blktap_ring_kick_all(void);
 
 int blktap_sysfs_init(void);
 void blktap_sysfs_exit(void);
@@ -181,19 +190,23 @@
 int blktap_device_run_queue(struct blktap *);
 void blktap_device_end_request(struct blktap *, struct blktap_request *, int);
 
-int blktap_request_pool_init(void);
-void blktap_request_pool_free(void);
-int blktap_request_pool_grow(void);
-int blktap_request_pool_shrink(void);
-struct blktap_request *blktap_request_allocate(struct blktap *);
+int blktap_page_pool_init(struct kobject *);
+void blktap_page_pool_exit(void);
+struct blktap_page_pool *blktap_page_pool_get(const char *);
+
+size_t blktap_request_debug(struct blktap *, char *, size_t);
+struct blktap_request *blktap_request_alloc(struct blktap *);
+int blktap_request_get_pages(struct blktap *, struct blktap_request *, int);
 void blktap_request_free(struct blktap *, struct blktap_request *);
-struct page *request_to_page(struct blktap_request *, int);
+void blktap_request_bounce(struct blktap *, struct blktap_request *, int, int);
 
 static inline unsigned long
 request_to_kaddr(struct blktap_request *req, int seg)
 {
-	unsigned long pfn = page_to_pfn(request_to_page(req, seg));
-	return (unsigned long)pfn_to_kaddr(pfn);
+	return (unsigned long)page_address(req->pages[seg]);
 }
 
+#define request_to_page(_request, _seg) ((_request)->pages[_seg])
+
+
 #endif
diff -r 7b618e7dfdd1 drivers/xen/blktap2/control.c
--- a/drivers/xen/blktap2/control.c	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/control.c	Mon Jul 09 15:48:16 2012 +0100
@@ -1,7 +1,7 @@
 #include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/miscdevice.h>
-
+#include <linux/device.h>
 #include <asm/uaccess.h>
 
 #include "blktap.h"
@@ -10,6 +10,7 @@
 
 struct blktap **blktaps;
 int blktap_max_minor;
+static struct blktap_page_pool *default_pool;
 
 static struct blktap *
 blktap_control_get_minor(void)
@@ -83,6 +84,9 @@
 	if (!tap)
 		return NULL;
 
+	kobject_get(&default_pool->kobj);
+	tap->pool = default_pool;
+
 	err = blktap_ring_create(tap);
 	if (err)
 		goto fail_tap;
@@ -110,6 +114,8 @@
 	if (err)
 		return err;
 
+	kobject_put(&tap->pool->kobj);
+
 	blktap_sysfs_destroy(tap);
 
 	blktap_control_put_minor(tap);
@@ -166,12 +172,43 @@
 	.ioctl    = blktap_control_ioctl,
 };
 
-static struct miscdevice blktap_misc = {
+static struct miscdevice blktap_control = {
 	.minor    = MISC_DYNAMIC_MINOR,
 	.name     = "blktap-control",
 	.fops     = &blktap_control_file_operations,
 };
 
+static struct device *control_device;
+
+static ssize_t
+blktap_control_show_default_pool(struct device *device,
+				 struct device_attribute *attr,
+				 char *buf)
+{
+	return sprintf(buf, "%s\n", kobject_name(&default_pool->kobj));
+}
+
+static ssize_t
+blktap_control_store_default_pool(struct device *device,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	struct blktap_page_pool *pool, *tmp = default_pool;
+
+	pool = blktap_page_pool_get(buf);
+	if (IS_ERR(pool))
+		return PTR_ERR(pool);
+
+	default_pool = pool;
+	kobject_put(&tmp->kobj);
+
+	return size;
+}
+
+static DEVICE_ATTR(default_pool, S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH,
+		   blktap_control_show_default_pool,
+		   blktap_control_store_default_pool);
+
 size_t
 blktap_control_debug(struct blktap *tap, char *buf, size_t size)
 {
@@ -190,12 +227,11 @@
 {
 	int err;
 
-	err = misc_register(&blktap_misc);
-	if (err) {
-		blktap_misc.minor = MISC_DYNAMIC_MINOR;
-		BTERR("misc_register failed for control device");
+	err = misc_register(&blktap_control);
+	if (err)
 		return err;
-	}
+
+	control_device = blktap_control.this_device;
 
 	blktap_max_minor = min(64, MAX_BLKTAP_DEVICE);
 	blktaps = kzalloc(blktap_max_minor * sizeof(blktaps[0]), GFP_KERNEL);
@@ -204,20 +240,39 @@
 		return -ENOMEM;
 	}
 
+	err = blktap_page_pool_init(&control_device->kobj);
+	if (err)
+		return err;
+
+	default_pool = blktap_page_pool_get("default");
+	if (!default_pool)
+		return -ENOMEM;
+
+	err = device_create_file(control_device, &dev_attr_default_pool);
+	if (err)
+		return err;
+
 	return 0;
 }
 
 static void
 blktap_control_exit(void)
 {
+	if (default_pool) {
+		kobject_put(&default_pool->kobj);
+		default_pool = NULL;
+	}
+
+	blktap_page_pool_exit();
+
 	if (blktaps) {
 		kfree(blktaps);
 		blktaps = NULL;
 	}
 
-	if (blktap_misc.minor != MISC_DYNAMIC_MINOR) {
-		misc_deregister(&blktap_misc);
-		blktap_misc.minor = MISC_DYNAMIC_MINOR;
+	if (control_device) {
+		misc_deregister(&blktap_control);
+		control_device = NULL;
 	}
 }
 
@@ -228,7 +283,6 @@
 	blktap_ring_exit();
 	blktap_sysfs_exit();
 	blktap_device_exit();
-	blktap_request_pool_free();
 }
 
 static int __init
@@ -239,10 +293,6 @@
 	if (!xen_pv_domain())
 		return -ENODEV;
 
-	err = blktap_request_pool_init();
-	if (err)
-		return err;
-
 	err = blktap_device_init();
 	if (err)
 		goto fail;
diff -r 7b618e7dfdd1 drivers/xen/blktap2/device.c
--- a/drivers/xen/blktap2/device.c	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/device.c	Mon Jul 09 15:48:16 2012 +0100
@@ -607,7 +607,7 @@
 			break;
 		}
 
-		request = blktap_request_allocate(tap);
+		request = blktap_request_alloc(tap);
 		if (!request) {
 			tap->stats.st_oo_req++;
 			goto wait;
diff -r 7b618e7dfdd1 drivers/xen/blktap2/request.c
--- a/drivers/xen/blktap2/request.c	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/request.c	Mon Jul 09 15:48:16 2012 +0100
@@ -1,297 +1,401 @@
+#include <linux/mempool.h>
 #include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/device.h>
 #include <xen/balloon.h>
-#include <linux/sched.h>
 
 #include "blktap.h"
 
-#define MAX_BUCKETS                      8
-#define BUCKET_SIZE                      MAX_PENDING_REQS
+/* max pages per shared pool. just to prevent accidental dos. */
+#define POOL_MAX_PAGES           (256*BLKIF_MAX_SEGMENTS_PER_REQUEST)
 
-#define BLKTAP_POOL_CLOSING              1
+/* default page pool size. when considering to shrink a shared pool,
+ * note that paused tapdisks may grab a whole lot of pages for a long
+ * time. */
+#define POOL_DEFAULT_PAGES       (2 * MMAP_PAGES)
 
-struct blktap_request_bucket;
+/* max number of pages allocatable per request. */
+#define POOL_MAX_REQUEST_PAGES   BLKIF_MAX_SEGMENTS_PER_REQUEST
 
-struct blktap_request_handle {
-	int                              slot;
-	uint8_t                          inuse;
-	struct blktap_request            request;
-	struct blktap_request_bucket    *bucket;
-};
+/* min request structs per pool. These grow dynamically. */
+#define POOL_MIN_REQS            BLK_RING_SIZE
 
-struct blktap_request_bucket {
-	atomic_t                         reqs_in_use;
-	struct blktap_request_handle     handles[BUCKET_SIZE];
-	struct page                    **foreign_pages;
-};
+static struct kset *pool_set;
 
-struct blktap_request_pool {
-	spinlock_t                       lock;
-	uint8_t                          status;
-	struct list_head                 free_list;
-	atomic_t                         reqs_in_use;
-	wait_queue_head_t                wait_queue;
-	struct blktap_request_bucket    *buckets[MAX_BUCKETS];
-};
+#define kobj_to_pool(_kobj) \
+	container_of(_kobj, struct blktap_page_pool, kobj)
 
-static struct blktap_request_pool pool;
+static struct kmem_cache *request_cache;
+static mempool_t *request_pool;
 
-static inline struct blktap_request_handle *
-blktap_request_to_handle(struct blktap_request *req)
+static void
+__page_pool_wake(struct blktap_page_pool *pool)
 {
-	return container_of(req, struct blktap_request_handle, request);
+	mempool_t *mem = pool->_mem;
+
+	/*
+	  NB. slightly wasteful to always wait for a full segment
+	  set. but this ensures the next disk makes
+	  progress. presently, the repeated request struct
+	  alloc/release cycles would otherwise keep everyone spinning.
+	*/
+
+	if (mem->curr_nr >= POOL_MAX_REQUEST_PAGES)
+		wake_up(&pool->wait);
+}
+
+int
+blktap_request_get_pages(struct blktap *tap,
+			 struct blktap_request *request, int nr_pages)
+{
+	struct blktap_page_pool *pool = tap->pool;
+	mempool_t *mem = pool->_mem;
+	struct page *page;
+
+	BUG_ON(request->nr_pages != 0);
+	BUG_ON(nr_pages > POOL_MAX_REQUEST_PAGES);
+
+	if (mem->curr_nr < nr_pages)
+		return -ENOMEM;
+
+	spin_lock(&pool->lock);
+
+	if (mem->curr_nr < nr_pages) {
+		spin_unlock(&pool->lock);
+		return -ENOMEM;
+	}
+
+	while (request->nr_pages < nr_pages) {
+		page = mempool_alloc(mem, GFP_NOWAIT);
+		BUG_ON(!page);
+		request->pages[request->nr_pages++] = page;
+	}
+
+	spin_unlock(&pool->lock);
+
+	return 0;
 }
 
 static void
-blktap_request_pool_init_request(struct blktap_request *request)
+blktap_request_put_pages(struct blktap *tap,
+			 struct blktap_request *request)
 {
-	int i;
+	struct blktap_page_pool *pool = tap->pool;
+	struct page *page;
 
-	request->usr_idx  = -1;
+	while (request->nr_pages) {
+		page = request->pages[--request->nr_pages];
+		mempool_free(page, pool->_mem);
+	}
+
 	request->nr_pages = 0;
-	request->status   = BLKTAP_REQUEST_FREE;
-	INIT_LIST_HEAD(&request->free_list);
-	for (i = 0; i < ARRAY_SIZE(request->handles); i++) {
-		request->handles[i].user   = INVALID_GRANT_HANDLE;
-		request->handles[i].kernel = INVALID_GRANT_HANDLE;
-	}
 }
 
-static int
-blktap_request_pool_allocate_bucket(void)
+size_t
+blktap_request_debug(struct blktap *tap, char *buf, size_t size)
 {
-	int i, idx;
-	unsigned long flags;
-	struct blktap_request *request;
-	struct blktap_request_handle *handle;
-	struct blktap_request_bucket *bucket;
+	struct blktap_page_pool *pool = tap->pool;
+	mempool_t *mem = pool->_mem;
+	char *s = buf, *end = buf + size;
 
-	bucket = kzalloc(sizeof(struct blktap_request_bucket), GFP_KERNEL);
-	if (!bucket)
-		goto fail;
+	s += snprintf(buf, end - s,
+		      "pool:%s pages:%d free:%d\n",
+		      kobject_name(&pool->kobj),
+		      mem->min_nr, mem->curr_nr);
 
-	bucket->foreign_pages = alloc_empty_pages_and_pagevec(MMAP_PAGES);
-	if (!bucket->foreign_pages)
-		goto fail;
-
-	spin_lock_irqsave(&pool.lock, flags);
-
-	idx = -1;
-	for (i = 0; i < MAX_BUCKETS; i++) {
-		if (!pool.buckets[i]) {
-			idx = i;
-			pool.buckets[idx] = bucket;
-			break;
-		}
-	}
-
-	if (idx == -1) {
-		spin_unlock_irqrestore(&pool.lock, flags);
-		goto fail;
-	}
-
-	for (i = 0; i < BUCKET_SIZE; i++) {
-		handle  = bucket->handles + i;
-		request = &handle->request;
-
-		handle->slot   = i;
-		handle->inuse  = 0;
-		handle->bucket = bucket;
-
-		blktap_request_pool_init_request(request);
-		list_add_tail(&request->free_list, &pool.free_list);
-	}
-
-	spin_unlock_irqrestore(&pool.lock, flags);
-
-	return 0;
-
-fail:
-	if (bucket && bucket->foreign_pages)
-		free_empty_pages_and_pagevec(bucket->foreign_pages, MMAP_PAGES);
-	kfree(bucket);
-	return -ENOMEM;
+	return s - buf;
 }
 
-static void
-blktap_request_pool_free_bucket(struct blktap_request_bucket *bucket)
+struct blktap_request*
+blktap_request_alloc(struct blktap *tap)
 {
-	if (!bucket)
-		return;
-
-	BTDBG("freeing bucket %p\n", bucket);
-
-	free_empty_pages_and_pagevec(bucket->foreign_pages, MMAP_PAGES);
-	kfree(bucket);
-}
-
-struct page *
-request_to_page(struct blktap_request *req, int seg)
-{
-	struct blktap_request_handle *handle = blktap_request_to_handle(req);
-	int idx = handle->slot * BLKIF_MAX_SEGMENTS_PER_REQUEST + seg;
-	return handle->bucket->foreign_pages[idx];
-}
-
-int
-blktap_request_pool_shrink(void)
-{
-	int i, err;
-	unsigned long flags;
-	struct blktap_request_bucket *bucket;
-
-	err = -EAGAIN;
-
-	spin_lock_irqsave(&pool.lock, flags);
-
-	/* always keep at least one bucket */
-	for (i = 1; i < MAX_BUCKETS; i++) {
-		bucket = pool.buckets[i];
-		if (!bucket)
-			continue;
-
-		if (atomic_read(&bucket->reqs_in_use))
-			continue;
-
-		blktap_request_pool_free_bucket(bucket);
-		pool.buckets[i] = NULL;
-		err = 0;
-		break;
-	}
-
-	spin_unlock_irqrestore(&pool.lock, flags);
-
-	return err;
-}
-
-int
-blktap_request_pool_grow(void)
-{
-	return blktap_request_pool_allocate_bucket();
-}
-
-struct blktap_request *
-blktap_request_allocate(struct blktap *tap)
-{
-	int i;
-	uint16_t usr_idx;
-	unsigned long flags;
 	struct blktap_request *request;
 
-	usr_idx = -1;
-	request = NULL;
+	request = mempool_alloc(request_pool, GFP_NOWAIT);
+	if (request)
+		request->tap = tap;
 
-	spin_lock_irqsave(&pool.lock, flags);
-
-	if (pool.status == BLKTAP_POOL_CLOSING)
-		goto out;
-
-	for (i = 0; i < ARRAY_SIZE(tap->pending_requests); i++)
-		if (!tap->pending_requests[i]) {
-			usr_idx = i;
-			break;
-		}
-
-	if (usr_idx == (uint16_t)-1)
-		goto out;
-
-	if (!list_empty(&pool.free_list)) {
-		request = list_entry(pool.free_list.next,
-				     struct blktap_request, free_list);
-		list_del(&request->free_list);
-	}
-
-	if (request) {
-		struct blktap_request_handle *handle;
-
-		atomic_inc(&pool.reqs_in_use);
-
-		handle = blktap_request_to_handle(request);
-		atomic_inc(&handle->bucket->reqs_in_use);
-		handle->inuse = 1;
-
-		request->usr_idx = usr_idx;
-
-		tap->pending_requests[usr_idx] = request;
-		tap->pending_cnt++;
-	}
-
-out:
-	spin_unlock_irqrestore(&pool.lock, flags);
 	return request;
 }
 
 void
-blktap_request_free(struct blktap *tap, struct blktap_request *request)
+blktap_request_free(struct blktap *tap,
+		    struct blktap_request *request)
 {
-	int free;
-	unsigned long flags;
-	struct blktap_request_handle *handle;
+	blktap_request_put_pages(tap, request);
 
-	BUG_ON(request->usr_idx >= ARRAY_SIZE(tap->pending_requests));
-	handle = blktap_request_to_handle(request);
+	mempool_free(request, request_pool);
 
-	spin_lock_irqsave(&pool.lock, flags);
+	__page_pool_wake(tap->pool);
+}
 
-	handle->inuse = 0;
-	tap->pending_requests[request->usr_idx] = NULL;
-	blktap_request_pool_init_request(request);
-	list_add(&request->free_list, &pool.free_list);
-	atomic_dec(&handle->bucket->reqs_in_use);
-	free = atomic_dec_and_test(&pool.reqs_in_use);
-	tap->pending_cnt--;
+static void
+blktap_request_ctor(void *obj)
+{
+	struct blktap_request *request = obj;
 
-	spin_unlock_irqrestore(&pool.lock, flags);
+	memset(request, 0, sizeof(*request));
+	sg_init_table(request->sg_table, ARRAY_SIZE(request->sg_table));
+}
 
-	if (free)
-		wake_up(&pool.wait_queue);
+static int
+blktap_page_pool_resize(struct blktap_page_pool *pool, int target)
+{
+	mempool_t *mem = pool->_mem;
+	int err;
 
-	blktap_ring_kick_all();
+	/* NB. mempool asserts min_nr >= 1 */
+	target = max(1, target);
+
+	err = mempool_resize(mem, target, GFP_KERNEL);
+	if (err)
+		return err;
+
+	__page_pool_wake(pool);
+
+	return 0;
+}
+
+struct pool_attribute {
+	struct attribute attr;
+
+	ssize_t (*show)(struct blktap_page_pool *pool,
+			char *buf);
+
+	ssize_t (*store)(struct blktap_page_pool *pool,
+			 const char *buf, size_t count);
+};
+
+#define kattr_to_pool_attr(_kattr) \
+	container_of(_kattr, struct pool_attribute, attr)
+
+static ssize_t
+blktap_page_pool_show_size(struct blktap_page_pool *pool,
+			   char *buf)
+{
+	mempool_t *mem = pool->_mem;
+	return sprintf(buf, "%d\n", mem->min_nr);
+}
+
+static ssize_t
+blktap_page_pool_store_size(struct blktap_page_pool *pool,
+			    const char *buf, size_t size)
+{
+	int target;
+
+	/*
+	 * NB. target fixup to avoid undesired results. less than a
+	 * full segment set can wedge the disk. much more than a
+	 * couple times the physical queue depth is rarely useful.
+	 */
+
+	target = simple_strtoul(buf, NULL, 0);
+	target = max(POOL_MAX_REQUEST_PAGES, target);
+	target = min(target, POOL_MAX_PAGES);
+
+	return blktap_page_pool_resize(pool, target) ? : size;
+}
+
+static struct pool_attribute blktap_page_pool_attr_size =
+	__ATTR(size, S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH,
+	       blktap_page_pool_show_size,
+	       blktap_page_pool_store_size);
+
+static ssize_t
+blktap_page_pool_show_free(struct blktap_page_pool *pool,
+			   char *buf)
+{
+	mempool_t *mem = pool->_mem;
+	return sprintf(buf, "%d\n", mem->curr_nr);
+}
+
+static struct pool_attribute blktap_page_pool_attr_free =
+	__ATTR(free, S_IRUSR|S_IRGRP|S_IROTH,
+	       blktap_page_pool_show_free,
+	       NULL);
+
+static struct attribute *blktap_page_pool_attrs[] = {
+	&blktap_page_pool_attr_size.attr,
+	&blktap_page_pool_attr_free.attr,
+	NULL,
+};
+
+static inline struct kobject*
+__blktap_kset_find_obj(struct kset *kset, const char *name)
+{
+	struct kobject *k;
+	struct kobject *ret = NULL;
+
+	spin_lock(&kset->list_lock);
+	list_for_each_entry(k, &kset->list, entry) {
+		if (kobject_name(k) && !strcmp(kobject_name(k), name)) {
+			ret = kobject_get(k);
+			break;
+		}
+	}
+	spin_unlock(&kset->list_lock);
+	return ret;
+}
+
+static ssize_t
+blktap_page_pool_show_attr(struct kobject *kobj, struct attribute *kattr,
+			   char *buf)
+{
+	struct blktap_page_pool *pool = kobj_to_pool(kobj);
+	struct pool_attribute *attr = kattr_to_pool_attr(kattr);
+
+	if (attr->show)
+		return attr->show(pool, buf);
+
+	return -EIO;
+}
+
+static ssize_t
+blktap_page_pool_store_attr(struct kobject *kobj, struct attribute *kattr,
+			    const char *buf, size_t size)
+{
+	struct blktap_page_pool *pool = kobj_to_pool(kobj);
+	struct pool_attribute *attr = kattr_to_pool_attr(kattr);
+
+	if (attr->show)
+		return attr->store(pool, buf, size);
+
+	return -EIO;
+}
+
+static struct sysfs_ops blktap_page_pool_sysfs_ops = {
+	.show		= blktap_page_pool_show_attr,
+	.store		= blktap_page_pool_store_attr,
+};
+
+static void
+blktap_page_pool_release(struct kobject *kobj)
+{
+	struct blktap_page_pool *pool = kobj_to_pool(kobj);
+	mempool_destroy(pool->_mem);
+	kfree(pool);
+}
+
+struct kobj_type blktap_page_pool_ktype = {
+	.release       = blktap_page_pool_release,
+	.sysfs_ops     = &blktap_page_pool_sysfs_ops,
+	.default_attrs = blktap_page_pool_attrs,
+};
+
+static void*
+__mempool_page_alloc(gfp_t gfp_mask, void *pool_data)
+{
+	struct page *page;
+
+	if (!(gfp_mask & __GFP_WAIT))
+		return NULL;
+
+	page = alloc_page(gfp_mask);
+	if (page)
+		SetPageReserved(page);
+
+	return page;
+}
+
+static void
+__mempool_page_free(void *element, void *pool_data)
+{
+	struct page *page = element;
+
+	ClearPageReserved(page);
+	put_page(page);
+}
+
+static struct kobject*
+blktap_page_pool_create(const char *name, int nr_pages)
+{
+	struct blktap_page_pool *pool;
+	int err;
+
+	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
+	if (!pool)
+		goto fail;
+
+	spin_lock_init(&pool->lock);
+	init_waitqueue_head(&pool->wait);
+
+	pool->_mem = mempool_create(nr_pages,
+				    __mempool_page_alloc, __mempool_page_free,
+				    pool);
+	if (!pool->_mem)
+		goto fail_pool;
+
+	kobject_init(&pool->kobj, &blktap_page_pool_ktype);
+	pool->kobj.kset = pool_set;
+	err = kobject_add(&pool->kobj, &pool_set->kobj, "%s", name);
+	if (err)
+		goto fail_mem;
+
+	return &pool->kobj;
+
+	kobject_del(&pool->kobj);
+fail_mem:
+	mempool_destroy(pool->_mem);
+fail_pool:
+	kfree(pool);
+fail:
+	return NULL;
+}
+
+struct blktap_page_pool*
+blktap_page_pool_get(const char *name)
+{
+	struct kobject *kobj;
+
+	kobj = __blktap_kset_find_obj(pool_set, name);
+	if (!kobj)
+		kobj = blktap_page_pool_create(name,
+					       POOL_DEFAULT_PAGES);
+	if (!kobj)
+		return ERR_PTR(-ENOMEM);
+
+	return kobj_to_pool(kobj);
+}
+
+int __init
+blktap_page_pool_init(struct kobject *parent)
+{
+	request_cache =
+		kmem_cache_create("blktap-request",
+				  sizeof(struct blktap_request), 0,
+				  0, blktap_request_ctor);
+	if (!request_cache)
+		return -ENOMEM;
+
+	request_pool =
+		mempool_create_slab_pool(POOL_MIN_REQS, request_cache);
+	if (!request_pool)
+		return -ENOMEM;
+
+	pool_set = kset_create_and_add("pools", NULL, parent);
+	if (!pool_set)
+		return -ENOMEM;
+
+	return 0;
 }
 
 void
-blktap_request_pool_free(void)
+blktap_page_pool_exit(void)
 {
-	int i;
-	unsigned long flags;
-
-	spin_lock_irqsave(&pool.lock, flags);
-
-	pool.status = BLKTAP_POOL_CLOSING;
-	while (atomic_read(&pool.reqs_in_use)) {
-		spin_unlock_irqrestore(&pool.lock, flags);
-		wait_event(pool.wait_queue, !atomic_read(&pool.reqs_in_use));
-		spin_lock_irqsave(&pool.lock, flags);
+	if (pool_set) {
+		BUG_ON(!list_empty(&pool_set->list));
+		kset_unregister(pool_set);
+		pool_set = NULL;
 	}
 
-	for (i = 0; i < MAX_BUCKETS; i++) {
-		blktap_request_pool_free_bucket(pool.buckets[i]);
-		pool.buckets[i] = NULL;
+	if (request_pool) {
+		mempool_destroy(request_pool);
+		request_pool = NULL;
 	}
 
-	spin_unlock_irqrestore(&pool.lock, flags);
+	if (request_cache) {
+		kmem_cache_destroy(request_cache);
+		request_cache = NULL;
+	}
 }
-
-int __init
-blktap_request_pool_init(void)
-{
-	int i, err;
-
-	memset(&pool, 0, sizeof(pool));
-
-	spin_lock_init(&pool.lock);
-	INIT_LIST_HEAD(&pool.free_list);
-	atomic_set(&pool.reqs_in_use, 0);
-	init_waitqueue_head(&pool.wait_queue);
-
-	for (i = 0; i < 2; i++) {
-		err = blktap_request_pool_allocate_bucket();
-		if (err)
-			goto fail;
-	}
-
-	return 0;
-
-fail:
-	blktap_request_pool_free();
-	return err;
-}
diff -r 7b618e7dfdd1 drivers/xen/blktap2/ring.c
--- a/drivers/xen/blktap2/ring.c	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/ring.c	Mon Jul 09 15:48:16 2012 +0100
@@ -17,8 +17,6 @@
 int blktap_ring_major;
 static struct cdev blktap_ring_cdev;
 
-static DECLARE_WAIT_QUEUE_HEAD(blktap_poll_wait);
-
 static inline struct blktap *
 vma_to_blktap(struct vm_area_struct *vma)
 {
@@ -412,7 +410,7 @@
 	struct blktap_ring *ring = &tap->ring;
 	int work = 0;
 
-	poll_wait(filp, &blktap_poll_wait, wait);
+	poll_wait(filp, &tap->pool->wait, wait);
 	poll_wait(filp, &ring->poll_wait, wait);
 
 	down_read(&current->mm->mmap_sem);
@@ -443,12 +441,6 @@
 	wake_up(&tap->ring.poll_wait);
 }
 
-void
-blktap_ring_kick_all(void)
-{
-	wake_up(&blktap_poll_wait);
-}
-
 int
 blktap_ring_destroy(struct blktap *tap)
 {
diff -r 7b618e7dfdd1 drivers/xen/blktap2/sysfs.c
--- a/drivers/xen/blktap2/sysfs.c	Mon Jul 09 15:37:12 2012 +0100
+++ b/drivers/xen/blktap2/sysfs.c	Mon Jul 09 15:48:16 2012 +0100
@@ -104,6 +104,8 @@
 
 	s += blktap_control_debug(tap, s, end - s);
 
+	s += blktap_request_debug(tap, s, end - s);
+
 	s += blktap_device_debug(tap, s, end - s);
 
 	s += blktap_ring_debug(tap, s, end - s);
@@ -129,6 +131,38 @@
 }
 static DEVICE_ATTR(task, S_IRUGO, blktap_sysfs_show_task, NULL);
 
+static ssize_t
+blktap_sysfs_show_pool(struct device *dev,
+		       struct device_attribute *attr,
+		       char *buf)
+{
+	struct blktap *tap = dev_get_drvdata(dev);
+	return sprintf(buf, "%s\n", kobject_name(&tap->pool->kobj));
+}
+
+static ssize_t
+blktap_sysfs_store_pool(struct device *dev,
+			struct device_attribute *attr,
+			const char *buf, size_t size)
+{
+	struct blktap *tap = dev_get_drvdata(dev);
+	struct blktap_page_pool *pool, *tmp = tap->pool;
+
+	if (tap->device.gd)
+		return -EBUSY;
+
+	pool = blktap_page_pool_get(buf);
+	if (IS_ERR(pool))
+		return PTR_ERR(pool);
+
+	tap->pool = pool;
+	kobject_put(&tmp->kobj);
+
+	return size;
+}
+DEVICE_ATTR(pool, S_IRUSR|S_IWUSR,
+	    blktap_sysfs_show_pool, blktap_sysfs_store_pool);
+
 int
 blktap_sysfs_create(struct blktap *tap)
 {
@@ -151,6 +185,8 @@
 	if (!err)
 		err = device_create_file(dev, &dev_attr_task);
 	if (!err)
+		err = device_create_file(dev, &dev_attr_pool);
+	if (!err)
 		ring->dev = dev;
 	else
 		device_unregister(dev);
